{
    "query": "For the automated evaluation of GPT-4 generated scientific academic writing, what are the effectiveness and bias considerations when using GPT-4 as its own evaluator compared to using a different LLM (either more or less capable), and what best practices should guide the choice?",
    "user_id": "lib_user",
    "task_id": "6bd61a97-ce78-4292-b2ad-f3b991f4e808",
    "timestamp": "2025-06-23T21:14:48.736295",
    "n_retrieval": 256,
    "n_retrieved": 260,
    "n_candidates": 42,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.3705900000000001,
    "decomposed_query": {
        "rewritten_query": "Effectiveness and bias considerations when using GPT-4 as its own evaluator for scientific academic writing compared to using a different LLM, and best practices for choosing evaluators.",
        "keyword_query": "GPT-4 evaluator scientific academic writing bias effectiveness different LLM best practices",
        "search_filters": {
            "fieldsOfStudy": "Computer Science,Education"
        },
        "cost": 0.010185,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "From RAGs to riches: Utilizing large language models to write documents for clinical trials.",
            "venue": "Clinical Trials",
            "year": 2024,
            "reference_count": 17,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2402.16406",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.16406, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283902403",
                    "name": "Nigel Markey"
                },
                {
                    "authorId": "2283902153",
                    "name": "Ilyass El-Mansouri"
                },
                {
                    "authorId": "2348265827",
                    "name": "Ga\u00ebtan Rensonnet"
                },
                {
                    "authorId": "2348262714",
                    "name": "Casper van Langen"
                },
                {
                    "authorId": "2283902581",
                    "name": "Christoph Meier"
                }
            ],
            "abstract": "BACKGROUND/AIMS\nClinical trials require numerous documents to be written: Protocols, consent forms, clinical study reports, and many others. Large language models offer the potential to rapidly generate first-draft versions of these documents; however, there are concerns about the quality of their output. Here, we report an evaluation of how good large language models are at generating sections of one such document, clinical trial protocols.\n\n\nMETHODS\nUsing an off-the-shelf large language model, we generated protocol sections for a broad range of diseases and clinical trial phases. Each of these document sections we assessed across four dimensions: Clinical thinking and logic; Transparency and references; Medical and clinical terminology; and Content relevance and suitability. To improve performance, we used the retrieval-augmented generation method to enhance the large language model with accurate up-to-date information, including regulatory guidance documents and data from ClinicalTrials.gov. Using this retrieval-augmented generation large language model, we regenerated the same protocol sections and assessed them across the same four dimensions.\n\n\nRESULTS\nWe find that the off-the-shelf large language model delivers reasonable results, especially when assessing content relevance and the correct use of medical and clinical terminology, with scores of over 80%. However, the off-the-shelf large language model shows limited performance in clinical thinking and logic and transparency and references, with assessment scores of \u224840% or less. The use of retrieval-augmented generation substantially improves the writing quality of the large language model, with clinical thinking and logic and transparency and references scores increasing to \u224880%. The retrieval-augmented generation method thus greatly improves the practical usability of large language models for clinical trial-related writing.\n\n\nDISCUSSION\nOur results suggest that hybrid large language model architectures, such as the retrieval-augmented generation method we utilized, offer strong potential for clinical trial-related writing, including a wide variety of documents. This is potentially transformative, since it addresses several major bottlenecks of drug development.",
            "corpus_id": 268033311,
            "sentences": [
                {
                    "corpus_id": "268033311",
                    "title": "From RAGs to riches: Utilizing large language models to write documents for clinical trials.",
                    "text": "Our assessment focused on GPT-4, one of the leading LLMs available today 12 and utilizing it to generate key sections of clinical trial protocols. The LLM output was subsequently assessed in terms of writing quality. Specifically we analyzed four dimensions: Clinical thinking and logic, which measures how closely recommendations from regulatory guidance documents were included in the generated section; Transparency and references, which verifies the presence and relevance of cited scientific sources in the generated text; Medical and clinical terminology, which assesses the use of appropriate jargon and scales of measurements; Content relevance and suitability, which measures, among others, whether the generated protocol section was specific to the disease and trial phase. An overview of the methodology is given in Figure 1, and a full description of both the generation and evaluation process, including the criteria and requirements used, is provided in the Supplementary Information. \n\nOur analysis does not make direct comparisons between LLM-written text and fully human-written text. This is because, from our experience in the field, there is often substantial variability between individual human writers. It is therefore challenging to establish a single, objective ''ground truth'' to compare against. Our evaluation framework, with its four dimensions described above, addresses this challenge by breaking down the assessment into discrete sub-dimensions which can be assessed objectively. Our assessment targeted two key sections of a clinical trial protocol document: the endpoints section and the eligibility criteria section. Two LLM models were evaluated: off-the-shelf GPT-4 via its commercially available application programming interface, 6,12 used as a baseline; and a retrieval-augmented generation (RAG) GPT-4 as an alternative to the off-the-shelf version (see Figure 1(b)). RAG is a methodology for incorporating knowledge from external databases, 13 and involves providing the LLM with external sources of knowledge to supplement the model's internal representation of information. 14 The RAG-augmented LLM was configured as follows: based on a user input query below, an LLM-powered decision agent automatically decided which tools to use to fetch relevant context and feed it to an LLM for final summarization and document generation.",
                    "score": 0.413461288391448,
                    "section_title": "Methods",
                    "char_start_offset": 2454,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 146
                        },
                        {
                            "start": 147,
                            "end": 216
                        },
                        {
                            "start": 217,
                            "end": 783
                        },
                        {
                            "start": 784,
                            "end": 998
                        },
                        {
                            "start": 1001,
                            "end": 1101
                        },
                        {
                            "start": 1102,
                            "end": 1225
                        },
                        {
                            "start": 1226,
                            "end": 1323
                        },
                        {
                            "start": 1324,
                            "end": 1512
                        },
                        {
                            "start": 1513,
                            "end": 1652
                        },
                        {
                            "start": 1653,
                            "end": 1909
                        },
                        {
                            "start": 1910,
                            "end": 2121
                        },
                        {
                            "start": 2122,
                            "end": 2373
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 2119,
                            "end": 2121,
                            "matchedPaperCorpusId": "218869575"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93798828125
                }
            ],
            "relevance_judgement": 0.93798828125,
            "relevance_judgment_input_expanded": "# Title: From RAGs to riches: Utilizing large language models to write documents for clinical trials.\n# Venue: Clinical Trials\n# Authors: Nigel Markey, Ilyass El-Mansouri, Ga\u00ebtan Rensonnet, Casper van Langen, Christoph Meier\n## Abstract\nBACKGROUND/AIMS\nClinical trials require numerous documents to be written: Protocols, consent forms, clinical study reports, and many others. Large language models offer the potential to rapidly generate first-draft versions of these documents; however, there are concerns about the quality of their output. Here, we report an evaluation of how good large language models are at generating sections of one such document, clinical trial protocols.\n\n\nMETHODS\nUsing an off-the-shelf large language model, we generated protocol sections for a broad range of diseases and clinical trial phases. Each of these document sections we assessed across four dimensions: Clinical thinking and logic; Transparency and references; Medical and clinical terminology; and Content relevance and suitability. To improve performance, we used the retrieval-augmented generation method to enhance the large language model with accurate up-to-date information, including regulatory guidance documents and data from ClinicalTrials.gov. Using this retrieval-augmented generation large language model, we regenerated the same protocol sections and assessed them across the same four dimensions.\n\n\nRESULTS\nWe find that the off-the-shelf large language model delivers reasonable results, especially when assessing content relevance and the correct use of medical and clinical terminology, with scores of over 80%. However, the off-the-shelf large language model shows limited performance in clinical thinking and logic and transparency and references, with assessment scores of \u224840% or less. The use of retrieval-augmented generation substantially improves the writing quality of the large language model, with clinical thinking and logic and transparency and references scores increasing to \u224880%. The retrieval-augmented generation method thus greatly improves the practical usability of large language models for clinical trial-related writing.\n\n\nDISCUSSION\nOur results suggest that hybrid large language model architectures, such as the retrieval-augmented generation method we utilized, offer strong potential for clinical trial-related writing, including a wide variety of documents. This is potentially transformative, since it addresses several major bottlenecks of drug development.\n## Methods\nOur assessment focused on GPT-4, one of the leading LLMs available today 12 and utilizing it to generate key sections of clinical trial protocols. The LLM output was subsequently assessed in terms of writing quality. Specifically we analyzed four dimensions: Clinical thinking and logic, which measures how closely recommendations from regulatory guidance documents were included in the generated section; Transparency and references, which verifies the presence and relevance of cited scientific sources in the generated text; Medical and clinical terminology, which assesses the use of appropriate jargon and scales of measurements; Content relevance and suitability, which measures, among others, whether the generated protocol section was specific to the disease and trial phase. An overview of the methodology is given in Figure 1, and a full description of both the generation and evaluation process, including the criteria and requirements used, is provided in the Supplementary Information. \n\nOur analysis does not make direct comparisons between LLM-written text and fully human-written text. This is because, from our experience in the field, there is often substantial variability between individual human writers. It is therefore challenging to establish a single, objective ''ground truth'' to compare against. Our evaluation framework, with its four dimensions described above, addresses this challenge by breaking down the assessment into discrete sub-dimensions which can be assessed objectively. Our assessment targeted two key sections of a clinical trial protocol document: the endpoints section and the eligibility criteria section. Two LLM models were evaluated: off-the-shelf GPT-4 via its commercially available application programming interface, 6,12 used as a baseline; and a retrieval-augmented generation (RAG) GPT-4 as an alternative to the off-the-shelf version (see Figure 1(b)). RAG is a methodology for incorporating knowledge from external databases, 13 and involves providing the LLM with external sources of knowledge to supplement the model's internal representation of information. 14 The RAG-augmented LLM was configured as follows: based on a user input query below, an LLM-powered decision agent automatically decided which tools to use to fetch relevant context and feed it to an LLM for final summarization and document generation.",
            "reference_string": "[268033311 | Markey et al. | 2024 | Citations: 9]"
        },
        {
            "title": "AI-Driven Review Systems: Evaluating LLMs in Scalable and Bias-Aware Academic Reviews",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 22,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.10365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2220094389",
                    "name": "Keith Tyser"
                },
                {
                    "authorId": "2316490107",
                    "name": "Ben Segev"
                },
                {
                    "authorId": "2345005539",
                    "name": "Gaston Longhitano"
                },
                {
                    "authorId": "2316857633",
                    "name": "Xin-Yu Zhang"
                },
                {
                    "authorId": "2316489211",
                    "name": "Zachary Meeks"
                },
                {
                    "authorId": "2316515465",
                    "name": "Jason Lee"
                },
                {
                    "authorId": "2316488233",
                    "name": "Uday Garg"
                },
                {
                    "authorId": "2316488208",
                    "name": "Nicholas Belsten"
                },
                {
                    "authorId": "2142463930",
                    "name": "A. Shporer"
                },
                {
                    "authorId": "2316489270",
                    "name": "Madeleine Udell"
                },
                {
                    "authorId": "1397529070",
                    "name": "Dov Te\u2019eni"
                },
                {
                    "authorId": "2861627",
                    "name": "Iddo Drori"
                }
            ],
            "abstract": "Automatic reviewing helps handle a large volume of papers, provides early feedback and quality control, reduces bias, and allows the analysis of trends. We evaluate the alignment of automatic paper reviews with human reviews using an arena of human preferences by pairwise comparisons. Gathering human preference may be time-consuming; therefore, we also use an LLM to automatically evaluate reviews to increase sample efficiency while reducing bias. In addition to evaluating human and LLM preferences among LLM reviews, we fine-tune an LLM to predict human preferences, predicting which reviews humans will prefer in a head-to-head battle between LLMs. We artificially introduce errors into papers and analyze the LLM's responses to identify limitations, use adaptive review questions, meta prompting, role-playing, integrate visual and textual analysis, use venue-specific reviewing materials, and predict human preferences, improving upon the limitations of the traditional review processes. We make the reviews of publicly available arXiv and open-access Nature journal papers available online, along with a free service which helps authors review and revise their research papers and improve their quality. This work develops proof-of-concept LLM reviewing systems that quickly deliver consistent, high-quality reviews and evaluate their quality. We mitigate the risks of misuse, inflated review scores, overconfident ratings, and skewed score distributions by augmenting the LLM with multiple documents, including the review form, reviewer guide, code of ethics and conduct, area chair guidelines, and previous year statistics, by finding which errors and shortcomings of the paper may be detected by automated reviews, and evaluating pairwise reviewer preferences. This work identifies and addresses the limitations of using LLMs as reviewers and evaluators and enhances the quality of the reviewing process.",
            "corpus_id": 271909821,
            "sentences": [
                {
                    "corpus_id": "271909821",
                    "title": "AI-Driven Review Systems: Evaluating LLMs in Scalable and Bias-Aware Academic Reviews",
                    "text": "Our aim is to improve scientific writing, research, and communication by providing fast and reliable in-depth reviews on demand. This work evaluates the limitations and capabilities of GPT-4 to review papers and suggest revisions. LLMgenerated reviews align well with human reviewers when evaluated by blind human evaluation and an automatic GPT-4 comparison. We present our LLM reviewer system, Open-Reviewer, and the associated Papers with the Reviews site. \n\nTo our knowledge, we are the first to report on such a largescale empirical evaluation of LLM reviewing. \n\nUsing human reviews as a baseline, we evaluated value alignment and the process alignment of LLM reviews, i.e., we compared the quality of reviews and the adherence of the reviewing process to conference guidelines and scientific norms of practice. Prior work on LLM academic capabilities suggests that LLMs are now ready for specific reviewing tasks and appear to be more effective for some academic domains and less effective for others (Checco et al. 2021;Schulz et al. 2022;Liu and Shah 2023;Lu et al. 2024). Therefore, we conducted ablation studies and determined the types of errors and shortcomings the LLM can detect and review. When supplied with information about previous editorial decisions, the LLM aligns well with human reviewers. Furthermore, the LLM performs well in detecting specific errors and shortcomings, such as overclaiming, but not others, such as detecting cases in which the authors needed to follow expected norms. We find that iterative design and large-scale empirical evaluation are essential to calibrate the application of LLMs. \n\nThis work leverages LLMs in the review process, addressing challenges and offering proof-of-concept LLM review tools. We introduce and evaluate systems designed to streamline handling tens of thousands of academic papers, from initial collection to reviewing and evaluation. Our methods offer novel approaches to automating academic reviews, improving upon traditional reviews. Our analysis reveals that the system facilitates a more efficient review process and enhances the accessibility and quality of academic literature for both authors and the broader scholarly community. Using papers from arXiv and open-access Nature, coupled with our methods, shows promise in identifying highquality papers and emerging research trends.",
                    "score": 0.4586881087101804,
                    "section_title": "Conclusions",
                    "char_start_offset": 25809,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 230
                        },
                        {
                            "start": 231,
                            "end": 359
                        },
                        {
                            "start": 360,
                            "end": 459
                        },
                        {
                            "start": 462,
                            "end": 566
                        },
                        {
                            "start": 569,
                            "end": 817
                        },
                        {
                            "start": 818,
                            "end": 1081
                        },
                        {
                            "start": 1082,
                            "end": 1205
                        },
                        {
                            "start": 1206,
                            "end": 1314
                        },
                        {
                            "start": 1315,
                            "end": 1512
                        },
                        {
                            "start": 1513,
                            "end": 1631
                        },
                        {
                            "start": 1634,
                            "end": 1751
                        },
                        {
                            "start": 1752,
                            "end": 1908
                        },
                        {
                            "start": 1909,
                            "end": 2011
                        },
                        {
                            "start": 2012,
                            "end": 2212
                        },
                        {
                            "start": 2213,
                            "end": 2364
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92431640625
                }
            ],
            "relevance_judgement": 0.92431640625,
            "relevance_judgment_input_expanded": "# Title: AI-Driven Review Systems: Evaluating LLMs in Scalable and Bias-Aware Academic Reviews\n# Venue: arXiv.org\n# Authors: Keith Tyser, Ben Segev, Gaston Longhitano, Xin-Yu Zhang, Zachary Meeks, Jason Lee, Uday Garg, Nicholas Belsten, A. Shporer, Madeleine Udell, Dov Te\u2019eni, Iddo Drori\n## Abstract\nAutomatic reviewing helps handle a large volume of papers, provides early feedback and quality control, reduces bias, and allows the analysis of trends. We evaluate the alignment of automatic paper reviews with human reviews using an arena of human preferences by pairwise comparisons. Gathering human preference may be time-consuming; therefore, we also use an LLM to automatically evaluate reviews to increase sample efficiency while reducing bias. In addition to evaluating human and LLM preferences among LLM reviews, we fine-tune an LLM to predict human preferences, predicting which reviews humans will prefer in a head-to-head battle between LLMs. We artificially introduce errors into papers and analyze the LLM's responses to identify limitations, use adaptive review questions, meta prompting, role-playing, integrate visual and textual analysis, use venue-specific reviewing materials, and predict human preferences, improving upon the limitations of the traditional review processes. We make the reviews of publicly available arXiv and open-access Nature journal papers available online, along with a free service which helps authors review and revise their research papers and improve their quality. This work develops proof-of-concept LLM reviewing systems that quickly deliver consistent, high-quality reviews and evaluate their quality. We mitigate the risks of misuse, inflated review scores, overconfident ratings, and skewed score distributions by augmenting the LLM with multiple documents, including the review form, reviewer guide, code of ethics and conduct, area chair guidelines, and previous year statistics, by finding which errors and shortcomings of the paper may be detected by automated reviews, and evaluating pairwise reviewer preferences. This work identifies and addresses the limitations of using LLMs as reviewers and evaluators and enhances the quality of the reviewing process.\n## Conclusions\nOur aim is to improve scientific writing, research, and communication by providing fast and reliable in-depth reviews on demand. This work evaluates the limitations and capabilities of GPT-4 to review papers and suggest revisions. LLMgenerated reviews align well with human reviewers when evaluated by blind human evaluation and an automatic GPT-4 comparison. We present our LLM reviewer system, Open-Reviewer, and the associated Papers with the Reviews site. \n\nTo our knowledge, we are the first to report on such a largescale empirical evaluation of LLM reviewing. \n\nUsing human reviews as a baseline, we evaluated value alignment and the process alignment of LLM reviews, i.e., we compared the quality of reviews and the adherence of the reviewing process to conference guidelines and scientific norms of practice. Prior work on LLM academic capabilities suggests that LLMs are now ready for specific reviewing tasks and appear to be more effective for some academic domains and less effective for others (Checco et al. 2021;Schulz et al. 2022;Liu and Shah 2023;Lu et al. 2024). Therefore, we conducted ablation studies and determined the types of errors and shortcomings the LLM can detect and review. When supplied with information about previous editorial decisions, the LLM aligns well with human reviewers. Furthermore, the LLM performs well in detecting specific errors and shortcomings, such as overclaiming, but not others, such as detecting cases in which the authors needed to follow expected norms. We find that iterative design and large-scale empirical evaluation are essential to calibrate the application of LLMs. \n\nThis work leverages LLMs in the review process, addressing challenges and offering proof-of-concept LLM review tools. We introduce and evaluate systems designed to streamline handling tens of thousands of academic papers, from initial collection to reviewing and evaluation. Our methods offer novel approaches to automating academic reviews, improving upon traditional reviews. Our analysis reveals that the system facilitates a more efficient review process and enhances the accessibility and quality of academic literature for both authors and the broader scholarly community. Using papers from arXiv and open-access Nature, coupled with our methods, shows promise in identifying highquality papers and emerging research trends.",
            "reference_string": "[271909821 | Tyser et al. | 2024 | Citations: 22]"
        },
        {
            "title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
            "venue": "NEJM AI",
            "year": 2023,
            "reference_count": 69,
            "citation_count": 145,
            "influential_citation_count": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.01783",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.01783, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "151253861",
                    "name": "Weixin Liang"
                },
                {
                    "authorId": "49889860",
                    "name": "Yuhui Zhang"
                },
                {
                    "authorId": "2256613375",
                    "name": "Hancheng Cao"
                },
                {
                    "authorId": "2253999923",
                    "name": "Binglu Wang"
                },
                {
                    "authorId": "2253469362",
                    "name": "Daisy Ding"
                },
                {
                    "authorId": "2216295251",
                    "name": "Xinyu Yang"
                },
                {
                    "authorId": "52224284",
                    "name": "Kailas Vodrahalli"
                },
                {
                    "authorId": "2257314770",
                    "name": "Siyu He"
                },
                {
                    "authorId": "2199559560",
                    "name": "D. Smith"
                },
                {
                    "authorId": "2256439343",
                    "name": "Yian Yin"
                },
                {
                    "authorId": "2251233257",
                    "name": "Daniel A. McFarland"
                },
                {
                    "authorId": "2244005287",
                    "name": "James Zou"
                }
            ],
            "abstract": "Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.",
            "corpus_id": 263608784,
            "sentences": [
                {
                    "corpus_id": "263608784",
                    "title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                    "text": "Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.",
                    "score": 0.4226805297905569,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.888671875
                },
                {
                    "corpus_id": "263608784",
                    "title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                    "text": "Given these challenges, there is an urgent need for crafting scalable and efficient feedback mechanisms that can enrich and streamline the scientific feedback process. Adopting such advancements holds the promise of not just elevating the quality and scope of scientific research, given the concerning deceleration in scientific advancements 15,16 , but also of democratizing its access across the scientific community. \n\nLarge language models (LLMs) [17][18][19] , especially those powered by Transformer-based architectures and pretrained at immense scales, have opened up great potential in various applications [20][21][22][23] . While LLMs have made remarkable strides in various domains, the promises and perils of leveraging LLMs for scientific feedback remain largely unknown. Despite recent attempts that explore the potential uses of such tools in areas such as automating paper screening 24 , error identification 25 , and checklist verification 26 1 , we lack large-scale empirical evidence on whether and how LLMs may be used to facilitate scientific feedback and augment current academic practices. \n\nIn this work, we present the first large-scale systematic analysis characterizing the potential reliability and credibility of leveraging LLM for generating scientific feedback. Specifically, we developed a GPT-4 based scientific feedback generation pipeline that takes the raw PDF of a paper and produces structured feedback (Fig. 1a). The system is designed to generate constructive feedback across various key aspects, mirroring the review structure of leading interdisciplinary journals 27,28 and conferences [29][30][31][32][33] , including: 1) Significance and novelty, 2) Potential reasons for acceptance, 3) Potential reasons for rejection, and 4) Suggestions for improvement. \n\nTo characterize the informativeness of GPT-4 generated feedback, we conducted both a retrospective analysis and a prospective user study. In the retrospective analysis, we applied our pipeline on papers that had previously been assessed by human reviewers. We then compared the LLM feedback with the human feedback. We assessed the degree of overlap between key points raised by both sources to gauge the effectiveness and reliability of LLM feedback. Furthermore, we compared the topic distributions of LLM feedback and human feedback.",
                    "score": 0.44059211996991066,
                    "section_title": "Introduction",
                    "char_start_offset": 2314,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 419
                        },
                        {
                            "start": 422,
                            "end": 633
                        },
                        {
                            "start": 634,
                            "end": 784
                        },
                        {
                            "start": 785,
                            "end": 1112
                        },
                        {
                            "start": 1115,
                            "end": 1292
                        },
                        {
                            "start": 1293,
                            "end": 1451
                        },
                        {
                            "start": 1452,
                            "end": 1799
                        },
                        {
                            "start": 1802,
                            "end": 1939
                        },
                        {
                            "start": 1940,
                            "end": 2058
                        },
                        {
                            "start": 2059,
                            "end": 2117
                        },
                        {
                            "start": 2118,
                            "end": 2253
                        },
                        {
                            "start": 2254,
                            "end": 2338
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 342,
                            "end": 345,
                            "matchedPaperCorpusId": "126306204"
                        },
                        {
                            "start": 345,
                            "end": 347,
                            "matchedPaperCorpusId": "3703088"
                        },
                        {
                            "start": 451,
                            "end": 455,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 455,
                            "end": 459,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 615,
                            "end": 619,
                            "matchedPaperCorpusId": "258375371"
                        },
                        {
                            "start": 623,
                            "end": 627,
                            "matchedPaperCorpusId": "254876189"
                        },
                        {
                            "start": 899,
                            "end": 901,
                            "matchedPaperCorpusId": "249579676"
                        },
                        {
                            "start": 1644,
                            "end": 1648,
                            "matchedPaperCorpusId": "85728208"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82470703125
                },
                {
                    "corpus_id": "263608784",
                    "title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                    "text": "We conduct a prospective user study to further validate the effectiveness of leveraging LLMs to generate scientific feedback. To facilitate our user study, we launched an online Gradio demo 55 of the aforementioned generation pipeline, accessible at a public URL (Supp. Fig. 11). Users are prompted to upload a research paper in its original PDF format, after which the system delivers the review to user's email. We ask users to only upload papers published after 9/2021 to ensure the papers are never seen by GPT-4 during training (the cutdown date of GPT-4 training corpora is 9/2021). We have also incorporated an ethics statement to discourage the direct use of LLM content for any 10/39 review-related tasks. After the review is generated and sent to users, users are asked to fill a 6-page survey (Figure 4), which includes 1) author background information, 2) review situation in author's area, 3) general impression of LLM review, 4) detailed evaluation of LLM review, 5) comparison with human review, and 6) additional questions and feedback, which systematically investigates human evaluations of different aspects of LLM reviews. The survey takes around 15-20 minutes and users will be compensated with $20. We recruit the participants through 1) relevant institute mailing lists, and 2) reaching out to all authors who have published at least one preprint on arXiv in the field of computer science and computational biology during January to March, 2023, provided their email contact information is available in the first three pages of the PDF. The study has been approved by Stanford University's Institutional Review Board.",
                    "score": 0.41131144666355063,
                    "section_title": "Prospective User Study and Survey",
                    "char_start_offset": 42178,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 269
                        },
                        {
                            "start": 270,
                            "end": 279
                        },
                        {
                            "start": 280,
                            "end": 413
                        },
                        {
                            "start": 414,
                            "end": 588
                        },
                        {
                            "start": 589,
                            "end": 714
                        },
                        {
                            "start": 715,
                            "end": 1141
                        },
                        {
                            "start": 1142,
                            "end": 1219
                        },
                        {
                            "start": 1220,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1639
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 190,
                            "end": 192,
                            "matchedPaperCorpusId": "213713921"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53076171875
                }
            ],
            "relevance_judgement": 0.888671875,
            "relevance_judgment_input_expanded": "# Title: Can large language models provide useful feedback on research papers? A large-scale empirical analysis\n# Venue: NEJM AI\n# Authors: Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, D. Smith, Yian Yin, Daniel A. McFarland, James Zou\n## Abstract\nExpert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.\n## Introduction\nGiven these challenges, there is an urgent need for crafting scalable and efficient feedback mechanisms that can enrich and streamline the scientific feedback process. Adopting such advancements holds the promise of not just elevating the quality and scope of scientific research, given the concerning deceleration in scientific advancements 15,16 , but also of democratizing its access across the scientific community. \n\nLarge language models (LLMs) [17][18][19] , especially those powered by Transformer-based architectures and pretrained at immense scales, have opened up great potential in various applications [20][21][22][23] . While LLMs have made remarkable strides in various domains, the promises and perils of leveraging LLMs for scientific feedback remain largely unknown. Despite recent attempts that explore the potential uses of such tools in areas such as automating paper screening 24 , error identification 25 , and checklist verification 26 1 , we lack large-scale empirical evidence on whether and how LLMs may be used to facilitate scientific feedback and augment current academic practices. \n\nIn this work, we present the first large-scale systematic analysis characterizing the potential reliability and credibility of leveraging LLM for generating scientific feedback. Specifically, we developed a GPT-4 based scientific feedback generation pipeline that takes the raw PDF of a paper and produces structured feedback (Fig. 1a). The system is designed to generate constructive feedback across various key aspects, mirroring the review structure of leading interdisciplinary journals 27,28 and conferences [29][30][31][32][33] , including: 1) Significance and novelty, 2) Potential reasons for acceptance, 3) Potential reasons for rejection, and 4) Suggestions for improvement. \n\nTo characterize the informativeness of GPT-4 generated feedback, we conducted both a retrospective analysis and a prospective user study. In the retrospective analysis, we applied our pipeline on papers that had previously been assessed by human reviewers. We then compared the LLM feedback with the human feedback. We assessed the degree of overlap between key points raised by both sources to gauge the effectiveness and reliability of LLM feedback. Furthermore, we compared the topic distributions of LLM feedback and human feedback.\n\n## Prospective User Study and Survey\nWe conduct a prospective user study to further validate the effectiveness of leveraging LLMs to generate scientific feedback. To facilitate our user study, we launched an online Gradio demo 55 of the aforementioned generation pipeline, accessible at a public URL (Supp. Fig. 11). Users are prompted to upload a research paper in its original PDF format, after which the system delivers the review to user's email. We ask users to only upload papers published after 9/2021 to ensure the papers are never seen by GPT-4 during training (the cutdown date of GPT-4 training corpora is 9/2021). We have also incorporated an ethics statement to discourage the direct use of LLM content for any 10/39 review-related tasks. After the review is generated and sent to users, users are asked to fill a 6-page survey (Figure 4), which includes 1) author background information, 2) review situation in author's area, 3) general impression of LLM review, 4) detailed evaluation of LLM review, 5) comparison with human review, and 6) additional questions and feedback, which systematically investigates human evaluations of different aspects of LLM reviews. The survey takes around 15-20 minutes and users will be compensated with $20. We recruit the participants through 1) relevant institute mailing lists, and 2) reaching out to all authors who have published at least one preprint on arXiv in the field of computer science and computational biology during January to March, 2023, provided their email contact information is available in the first three pages of the PDF. The study has been approved by Stanford University's Institutional Review Board.",
            "reference_string": "[263608784 | Liang et al. | 2023 | Citations: 145]"
        },
        {
            "title": "Guiding Large Language Models via External Attention Prompting for Scientific Extreme Summarization",
            "venue": "SDP",
            "year": 2024,
            "reference_count": 32,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.sdp-1.22, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2315438513",
                    "name": "Yuan Chang"
                },
                {
                    "authorId": "2315367785",
                    "name": "Ziyue Li"
                },
                {
                    "authorId": "2315309245",
                    "name": "Xiaoqiu Le"
                }
            ],
            "abstract": "Scientific extreme summarization, the task of generating concise one-sentence summaries (TLDRs) for scientific papers, presents significant challenges due to the need for deep domain-specific understanding and the ability to distill salient information. This study identifies the critical role of titles and keywords in enhancing TLDR generation through quantitative analysis. We propose a novel method, External Attention Prompting (EAP), which leverages LLMs by guiding them to focus on the most critical parts of the source text through varying degrees of attention signals. Our method employs Markdown emphasis syntax to annotate attention levels, enabling LLMs to prioritize salient information effectively. Extensive experiments demonstrate that EAP significantly outperforms baseline methods across various LLMs and metrics in both zero-shot and few-shot settings. Further evaluations by GPT-4 demonstrate that EAP can enable LLMs to generate TLDRs of higher human-aligned quality.",
            "corpus_id": 271769421,
            "sentences": [
                {
                    "corpus_id": "271769421",
                    "title": "Guiding Large Language Models via External Attention Prompting for Scientific Extreme Summarization",
                    "text": "Setup. In addition to using traditional automatic metrics, we explore the use of LLMs as evaluators to assess the quality of generated TLDRs. This approach aims to address the limitations of automatic metrics, which may not fully capture the nuances of human preferences and the overall quality of the summaries (Zheng et al., 2023). By leveraging LLMs, we seek to provide a more comprehensive and human-aligned evaluation of the generated TL-DRs. \n\nTo this end, we employ GPT-4 (version gpt-4-0613), which is currently recognized as one of the most advanced LLMs, to assess the quality of TL-DRs generated by our method compared to a baseline method. \n\nWe randomly sample 100 data points and collect TLDRs generated by GPT-3.5 using both the baseline SP method and our EAP method under a fewshot setting. The evaluation criteria focus on the TLDR's ability to concisely capture the key aspects of a scientific paper while maintaining faithfulness to the source. To mitigate position bias (Zheng et al., 2023), each pair is evaluated twice with the positions of the two TLDRs swapped. A method is considered the winner for a sample only if both evaluations favored the same TLDR. In cases of conflicting results or ties, the sample is marked as a tie. The above process is repeated with three different random seeds, resulting in three rounds of evaluation. \n\nResults. The results of GPT-4 evaluation are summarized in Table 3, showing that the proposed EAP method consistently outperforms the baseline SP method, achieving an average win rate of 37.33 % compared to the baseline's 26.00 %. The average tie rate is 36.67 %, with an average conflict rate of 23.67 %. These results indicate that the EAP method significantly enhances the performance of LLMs in generating TLDRs, as judged by GPT-4.",
                    "score": 0.41313477039992375,
                    "section_title": "LLM as Evaluator",
                    "char_start_offset": 19188,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 6
                        },
                        {
                            "start": 7,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 447
                        },
                        {
                            "start": 450,
                            "end": 651
                        },
                        {
                            "start": 654,
                            "end": 805
                        },
                        {
                            "start": 806,
                            "end": 962
                        },
                        {
                            "start": 963,
                            "end": 1084
                        },
                        {
                            "start": 1085,
                            "end": 1179
                        },
                        {
                            "start": 1180,
                            "end": 1251
                        },
                        {
                            "start": 1252,
                            "end": 1357
                        },
                        {
                            "start": 1360,
                            "end": 1368
                        },
                        {
                            "start": 1369,
                            "end": 1590
                        },
                        {
                            "start": 1591,
                            "end": 1665
                        },
                        {
                            "start": 1666,
                            "end": 1796
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8828125
                }
            ],
            "relevance_judgement": 0.8828125,
            "relevance_judgment_input_expanded": "# Title: Guiding Large Language Models via External Attention Prompting for Scientific Extreme Summarization\n# Venue: SDP\n# Authors: Yuan Chang, Ziyue Li, Xiaoqiu Le\n## Abstract\nScientific extreme summarization, the task of generating concise one-sentence summaries (TLDRs) for scientific papers, presents significant challenges due to the need for deep domain-specific understanding and the ability to distill salient information. This study identifies the critical role of titles and keywords in enhancing TLDR generation through quantitative analysis. We propose a novel method, External Attention Prompting (EAP), which leverages LLMs by guiding them to focus on the most critical parts of the source text through varying degrees of attention signals. Our method employs Markdown emphasis syntax to annotate attention levels, enabling LLMs to prioritize salient information effectively. Extensive experiments demonstrate that EAP significantly outperforms baseline methods across various LLMs and metrics in both zero-shot and few-shot settings. Further evaluations by GPT-4 demonstrate that EAP can enable LLMs to generate TLDRs of higher human-aligned quality.\n## LLM as Evaluator\nSetup. In addition to using traditional automatic metrics, we explore the use of LLMs as evaluators to assess the quality of generated TLDRs. This approach aims to address the limitations of automatic metrics, which may not fully capture the nuances of human preferences and the overall quality of the summaries (Zheng et al., 2023). By leveraging LLMs, we seek to provide a more comprehensive and human-aligned evaluation of the generated TL-DRs. \n\nTo this end, we employ GPT-4 (version gpt-4-0613), which is currently recognized as one of the most advanced LLMs, to assess the quality of TL-DRs generated by our method compared to a baseline method. \n\nWe randomly sample 100 data points and collect TLDRs generated by GPT-3.5 using both the baseline SP method and our EAP method under a fewshot setting. The evaluation criteria focus on the TLDR's ability to concisely capture the key aspects of a scientific paper while maintaining faithfulness to the source. To mitigate position bias (Zheng et al., 2023), each pair is evaluated twice with the positions of the two TLDRs swapped. A method is considered the winner for a sample only if both evaluations favored the same TLDR. In cases of conflicting results or ties, the sample is marked as a tie. The above process is repeated with three different random seeds, resulting in three rounds of evaluation. \n\nResults. The results of GPT-4 evaluation are summarized in Table 3, showing that the proposed EAP method consistently outperforms the baseline SP method, achieving an average win rate of 37.33 % compared to the baseline's 26.00 %. The average tie rate is 36.67 %, with an average conflict rate of 23.67 %. These results indicate that the EAP method significantly enhances the performance of LLMs in generating TLDRs, as judged by GPT-4.",
            "reference_string": "[271769421 | Chang et al. | 2024 | Citations: 1]"
        },
        {
            "title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
            "venue": "ACM/IEEE Joint Conference on Digital Libraries",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.18812, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2360502238",
                    "name": "Hamed Babaei Giglou"
                },
                {
                    "authorId": "2261403432",
                    "name": "Jennifer D'Souza"
                },
                {
                    "authorId": "2268415251",
                    "name": "S. Auer"
                }
            ],
            "abstract": "In response to the growing complexity and volume of scientific literature, this paper introduces the LLMs4Synthesis framework, designed to enhance the capabilities of Large Language Models (LLMs) to synthesize the key insights from scientific texts as high-quality and concise summaries. This framework addresses the need for rapid, coherent, and contextually rich integration of key scientific insights, leveraging both open-source and proprietary LLMs. It also examines the effectiveness of LLMs in evaluating the integrity and reliability of these syntheses, alleviating inadequacies in current quantitative metrics. The contributions of this study are a novel methodology for synthesizing key scientific insights, definition of new synthesis types, and establishing nine detailed quality criteria for evaluating syntheses. The implementation fits LLMs with reinforcement learning to optimize for synthesis quality by alignment with our established quality criteria. The LLMs4Synthesis framework and its components are available, promising to improve the generation and evaluation of scientific research synthesis.",
            "corpus_id": 272968899,
            "sentences": [
                {
                    "corpus_id": "272968899",
                    "title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                    "text": "Prior work has demonstrated the effectiveness of LLMs as automatic evaluators for scientific synthesis [6,17,19], arguing that they are becoming both the tools and the standards for language assessment. Thus, for the evaluation task, we utilized GPT-4-Turbo, the most advanced LLM available at the time of writing. The evaluation prompt and script are available athttps://github.com/jd-coderepos/ scisynthesis/tree/main/gpt-4%20synthesis-evaluator. The prompt includes all nine evaluation criteria along with their corresponding questions. Each evaluation criteria was assessed based on a rating scale from 1 to 5 as follows: 1. Very Bad, 2. Bad, 3. Moderate, 4. Good, and 5. Very Good. Each rating level was specifically tailored with a description to aid in the assessment of each criterion. For instance, consider the rating scale description for the first criteria i.e. \"1. Relevancy: is the information in the answer relevant to the problem?\" as follows: Rating 1. Very bad: The information provided does not relate to the research problem, showing a lack of understanding or connection to the topic. Rating 2. Bad: The information occasionally relates to the research problem but lacks direct and consistent relevance. Rating 3. Moderate: The information is generally related to the research problem, with occasional lapses in direct relevance. Rating 4. Good: The information is consistently relevant to the research problem, with only minor exceptions. Rating 5. Very good: The synthesis is directly and consistently relevant to the research problem, demonstrating a deep understanding of the topic and its nuances. For the full prompts, including detailed descriptions of the rating scale for all criteria, please refer to the system prompt. Finally, similar to 3-fold cross-validation, the GPT-4 synthesis evaluator was run thrice on the same data samples to ensure consistent assessment. The resulting scores, which did not show significant divergence largely due to our detailed prompt specification of the expected evaluation task leaving little room for ambiguity in interpretation, were averaged and reported.",
                    "score": 0.4388387508618159,
                    "section_title": "LLM Evaluation of Synthesis Quality",
                    "char_start_offset": 21573,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 202
                        },
                        {
                            "start": 203,
                            "end": 314
                        },
                        {
                            "start": 315,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 686
                        },
                        {
                            "start": 687,
                            "end": 793
                        },
                        {
                            "start": 794,
                            "end": 877
                        },
                        {
                            "start": 878,
                            "end": 1105
                        },
                        {
                            "start": 1106,
                            "end": 1224
                        },
                        {
                            "start": 1225,
                            "end": 1350
                        },
                        {
                            "start": 1351,
                            "end": 1460
                        },
                        {
                            "start": 1461,
                            "end": 1623
                        },
                        {
                            "start": 1624,
                            "end": 1750
                        },
                        {
                            "start": 1751,
                            "end": 1898
                        },
                        {
                            "start": 1899,
                            "end": 2124
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 103,
                            "end": 106,
                            "matchedPaperCorpusId": "259095491"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.837890625
                },
                {
                    "corpus_id": "272968899",
                    "title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                    "text": "Mistral's syntheses tend to be general and lack depth, with both models critiqued for insufficient insight into the research problem. 5. Integration. GPT-4 effectively integrates sources into a structured synthesis, whereas Mistral tends to list sources without transitions. GPT-4 is more consistent in creating a unified narrative, though both could improve paragraph organization. 6. Cohesion. GPT-4 offers more cohesive syntheses with smoother transitions and well-connected ideas, whereas Mistral's work often reads like separate summaries. GPT-4 effectively groups studies thematically, enhancing synthesis cohesion. 7. Coherence. GPT-4 shows higher overall coherence, while Mistral's syntheses can seem disjointed. Both models could strengthen connections between ideas, but GPT-4 is closer to achieving this. 8. Readability. Both models produce clear syntheses, but Mistral's style is slightly preferred for its simplicity. GPT-4 adheres well to stylistic norms but can be overly complex. Despite structural issues, Mistral's writing quality is praised. 9. Conciseness. GPT-4 generally adheres to word limits better, while Mistral often exceeds or falls short of the 200-word target. Both are praised for being non-repetitive. \n\nFinally, the domain-specific survey evaluations reveal that GPT-4 performs best in Earth Science and Computer Science, while Mistral excels in Earth Science and Chemistry. Both LLMs show weaker performance in Linguistics and Sociology, suggesting a better fit for engineering and hard sciences. For more insights, visit our repository. \n\nThis first part of the paper has introduced the scientific synthesis task, LLM synthesis generators, the multidisciplinary ORKG dataset, and insights from both an automatic LLM evaluator and a human survey. These elements contribute to defining the LLMs4Synthesis framework, which optimizes open-source LLMs for synthesis generation. The second part evaluates this framework, particularly against Mistral-7B, focusing on improvements in output format and addressing weaknesses in the nine evaluation criteria.",
                    "score": 0.4323257599694882,
                    "section_title": "Survey Results",
                    "char_start_offset": 30843,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 274
                        },
                        {
                            "start": 275,
                            "end": 382
                        },
                        {
                            "start": 383,
                            "end": 395
                        },
                        {
                            "start": 396,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 621
                        },
                        {
                            "start": 622,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 720
                        },
                        {
                            "start": 721,
                            "end": 815
                        },
                        {
                            "start": 816,
                            "end": 831
                        },
                        {
                            "start": 832,
                            "end": 930
                        },
                        {
                            "start": 931,
                            "end": 995
                        },
                        {
                            "start": 996,
                            "end": 1060
                        },
                        {
                            "start": 1061,
                            "end": 1076
                        },
                        {
                            "start": 1077,
                            "end": 1190
                        },
                        {
                            "start": 1191,
                            "end": 1233
                        },
                        {
                            "start": 1236,
                            "end": 1407
                        },
                        {
                            "start": 1408,
                            "end": 1530
                        },
                        {
                            "start": 1531,
                            "end": 1571
                        },
                        {
                            "start": 1574,
                            "end": 1780
                        },
                        {
                            "start": 1781,
                            "end": 1907
                        },
                        {
                            "start": 1908,
                            "end": 2083
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7119140625
                }
            ],
            "relevance_judgement": 0.837890625,
            "relevance_judgment_input_expanded": "# Title: LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis\n# Venue: ACM/IEEE Joint Conference on Digital Libraries\n# Authors: Hamed Babaei Giglou, Jennifer D'Souza, S. Auer\n## Abstract\nIn response to the growing complexity and volume of scientific literature, this paper introduces the LLMs4Synthesis framework, designed to enhance the capabilities of Large Language Models (LLMs) to synthesize the key insights from scientific texts as high-quality and concise summaries. This framework addresses the need for rapid, coherent, and contextually rich integration of key scientific insights, leveraging both open-source and proprietary LLMs. It also examines the effectiveness of LLMs in evaluating the integrity and reliability of these syntheses, alleviating inadequacies in current quantitative metrics. The contributions of this study are a novel methodology for synthesizing key scientific insights, definition of new synthesis types, and establishing nine detailed quality criteria for evaluating syntheses. The implementation fits LLMs with reinforcement learning to optimize for synthesis quality by alignment with our established quality criteria. The LLMs4Synthesis framework and its components are available, promising to improve the generation and evaluation of scientific research synthesis.\n## LLM Evaluation of Synthesis Quality\nPrior work has demonstrated the effectiveness of LLMs as automatic evaluators for scientific synthesis [6,17,19], arguing that they are becoming both the tools and the standards for language assessment. Thus, for the evaluation task, we utilized GPT-4-Turbo, the most advanced LLM available at the time of writing. The evaluation prompt and script are available athttps://github.com/jd-coderepos/ scisynthesis/tree/main/gpt-4%20synthesis-evaluator. The prompt includes all nine evaluation criteria along with their corresponding questions. Each evaluation criteria was assessed based on a rating scale from 1 to 5 as follows: 1. Very Bad, 2. Bad, 3. Moderate, 4. Good, and 5. Very Good. Each rating level was specifically tailored with a description to aid in the assessment of each criterion. For instance, consider the rating scale description for the first criteria i.e. \"1. Relevancy: is the information in the answer relevant to the problem?\" as follows: Rating 1. Very bad: The information provided does not relate to the research problem, showing a lack of understanding or connection to the topic. Rating 2. Bad: The information occasionally relates to the research problem but lacks direct and consistent relevance. Rating 3. Moderate: The information is generally related to the research problem, with occasional lapses in direct relevance. Rating 4. Good: The information is consistently relevant to the research problem, with only minor exceptions. Rating 5. Very good: The synthesis is directly and consistently relevant to the research problem, demonstrating a deep understanding of the topic and its nuances. For the full prompts, including detailed descriptions of the rating scale for all criteria, please refer to the system prompt. Finally, similar to 3-fold cross-validation, the GPT-4 synthesis evaluator was run thrice on the same data samples to ensure consistent assessment. The resulting scores, which did not show significant divergence largely due to our detailed prompt specification of the expected evaluation task leaving little room for ambiguity in interpretation, were averaged and reported.\n\n## Survey Results\nMistral's syntheses tend to be general and lack depth, with both models critiqued for insufficient insight into the research problem. 5. Integration. GPT-4 effectively integrates sources into a structured synthesis, whereas Mistral tends to list sources without transitions. GPT-4 is more consistent in creating a unified narrative, though both could improve paragraph organization. 6. Cohesion. GPT-4 offers more cohesive syntheses with smoother transitions and well-connected ideas, whereas Mistral's work often reads like separate summaries. GPT-4 effectively groups studies thematically, enhancing synthesis cohesion. 7. Coherence. GPT-4 shows higher overall coherence, while Mistral's syntheses can seem disjointed. Both models could strengthen connections between ideas, but GPT-4 is closer to achieving this. 8. Readability. Both models produce clear syntheses, but Mistral's style is slightly preferred for its simplicity. GPT-4 adheres well to stylistic norms but can be overly complex. Despite structural issues, Mistral's writing quality is praised. 9. Conciseness. GPT-4 generally adheres to word limits better, while Mistral often exceeds or falls short of the 200-word target. Both are praised for being non-repetitive. \n\nFinally, the domain-specific survey evaluations reveal that GPT-4 performs best in Earth Science and Computer Science, while Mistral excels in Earth Science and Chemistry. Both LLMs show weaker performance in Linguistics and Sociology, suggesting a better fit for engineering and hard sciences. For more insights, visit our repository. \n\nThis first part of the paper has introduced the scientific synthesis task, LLM synthesis generators, the multidisciplinary ORKG dataset, and insights from both an automatic LLM evaluator and a human survey. These elements contribute to defining the LLMs4Synthesis framework, which optimizes open-source LLMs for synthesis generation. The second part evaluates this framework, particularly against Mistral-7B, focusing on improvements in output format and addressing weaknesses in the nine evaluation criteria.",
            "reference_string": "[272968899 | Giglou et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Large Language Models as Evaluators for Scientific Synthesis",
            "venue": "Conference on Natural Language Processing",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.02977, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280931544",
                    "name": "Julia Evans"
                },
                {
                    "authorId": "2300091096",
                    "name": "Jennifer D'Souza"
                },
                {
                    "authorId": "2268415251",
                    "name": "S. Auer"
                }
            ],
            "abstract": "Our study explores how well the state-of-the-art Large Language Models (LLMs), like GPT-4 and Mistral, can assess the quality of scientific summaries or, more fittingly, scientific syntheses, comparing their evaluations to those of human annotators. We used a dataset of 100 research questions and their syntheses made by GPT-4 from abstracts of five related papers, checked against human quality ratings. The study evaluates both the closed-source GPT-4 and the open-source Mistral model's ability to rate these summaries and provide reasons for their judgments. Preliminary results show that LLMs can offer logical explanations that somewhat match the quality ratings, yet a deeper statistical analysis shows a weak correlation between LLM and human ratings, suggesting the potential and current limitations of LLMs in scientific synthesis evaluation.",
            "corpus_id": 270924255,
            "sentences": [
                {
                    "corpus_id": "270924255",
                    "title": "Large Language Models as Evaluators for Scientific Synthesis",
                    "text": "Large Language Models (LLMs) have made a significant impact on natural language processing (NLP), demonstrating exceptional performance in tasks like text generation, sentiment analysis, machine translation, and question answering, with outputs that often rival human-created content (Huang et al., 2023).In addition to their direct applications, LLMs offer substantial benefits in streamlining machine learning model development, particularly in evaluation processes.They reduce the dependency on human-generated ground truth data and the necessity for human evaluators (Bai et al., 2023) in two key ways: by facilitating the generation of synthetic ground truth data and by serving as evaluators for model predictions themselves.This approach not only speeds up the evaluation process but also broadens the scope of evaluation criteria to include factors such as diversity and coverage, enhancing the efficiency and comprehensiveness of model assessments.\n\nThis study investigates the use of LLMs as evaluators to streamline the evaluation process, moving away from traditional reliance on human evaluators and human-generated ground truth data.It specifically examines the effectiveness of LLMs in synthesizing scientific abstracts seen generally as multi-document summarization tasks.The main focus of this research is to assess how two state-ofthe-art LLMs-the proprietary GPT-4 Turbo (Ope-nAI, 2023) and the open-source Mistral-7B (Jiang et al., 2023)-perform in evaluating scientific syntheses.Furthermore, leveraging LLMs meant better versatility in evaluation considerations, which meant that the evaluations tested varied dimensions of syntheses quality, viz.comprehensiveness, trustworthiness, and utility.\n\nThis paper is structured as follows.First, section 2 presents a review of related work in the fields of text summarization and LLM evaluation.In section 3, we show our approach to using LLMs for scientific synthesis evaluation, wherein subsection 3.1 describes the LLM output, while subsection 3.2 presents a qualitative evaluation of this output.In subsection 3.3, we analyze the correlation between LLM ratings and human judgments.A discussion of our findings and final conclusions is described in section 4.",
                    "score": 0.39192076446627655,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 305
                        },
                        {
                            "start": 305,
                            "end": 468
                        },
                        {
                            "start": 468,
                            "end": 731
                        },
                        {
                            "start": 731,
                            "end": 957
                        },
                        {
                            "start": 959,
                            "end": 1147
                        },
                        {
                            "start": 1147,
                            "end": 1288
                        },
                        {
                            "start": 1288,
                            "end": 1501
                        },
                        {
                            "start": 1501,
                            "end": 1669
                        },
                        {
                            "start": 1669,
                            "end": 1717
                        },
                        {
                            "start": 1719,
                            "end": 1755
                        },
                        {
                            "start": 1755,
                            "end": 1861
                        },
                        {
                            "start": 1861,
                            "end": 2066
                        },
                        {
                            "start": 2066,
                            "end": 2152
                        },
                        {
                            "start": 2152,
                            "end": 2229
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 284,
                            "end": 304,
                            "matchedPaperCorpusId": "256868854"
                        },
                        {
                            "start": 571,
                            "end": 589,
                            "matchedPaperCorpusId": "259095491"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82080078125
                }
            ],
            "relevance_judgement": 0.82080078125,
            "relevance_judgment_input_expanded": "# Title: Large Language Models as Evaluators for Scientific Synthesis\n# Venue: Conference on Natural Language Processing\n# Authors: Julia Evans, Jennifer D'Souza, S. Auer\n## Abstract\nOur study explores how well the state-of-the-art Large Language Models (LLMs), like GPT-4 and Mistral, can assess the quality of scientific summaries or, more fittingly, scientific syntheses, comparing their evaluations to those of human annotators. We used a dataset of 100 research questions and their syntheses made by GPT-4 from abstracts of five related papers, checked against human quality ratings. The study evaluates both the closed-source GPT-4 and the open-source Mistral model's ability to rate these summaries and provide reasons for their judgments. Preliminary results show that LLMs can offer logical explanations that somewhat match the quality ratings, yet a deeper statistical analysis shows a weak correlation between LLM and human ratings, suggesting the potential and current limitations of LLMs in scientific synthesis evaluation.\n## Introduction\nLarge Language Models (LLMs) have made a significant impact on natural language processing (NLP), demonstrating exceptional performance in tasks like text generation, sentiment analysis, machine translation, and question answering, with outputs that often rival human-created content (Huang et al., 2023).In addition to their direct applications, LLMs offer substantial benefits in streamlining machine learning model development, particularly in evaluation processes.They reduce the dependency on human-generated ground truth data and the necessity for human evaluators (Bai et al., 2023) in two key ways: by facilitating the generation of synthetic ground truth data and by serving as evaluators for model predictions themselves.This approach not only speeds up the evaluation process but also broadens the scope of evaluation criteria to include factors such as diversity and coverage, enhancing the efficiency and comprehensiveness of model assessments.\n\nThis study investigates the use of LLMs as evaluators to streamline the evaluation process, moving away from traditional reliance on human evaluators and human-generated ground truth data.It specifically examines the effectiveness of LLMs in synthesizing scientific abstracts seen generally as multi-document summarization tasks.The main focus of this research is to assess how two state-ofthe-art LLMs-the proprietary GPT-4 Turbo (Ope-nAI, 2023) and the open-source Mistral-7B (Jiang et al., 2023)-perform in evaluating scientific syntheses.Furthermore, leveraging LLMs meant better versatility in evaluation considerations, which meant that the evaluations tested varied dimensions of syntheses quality, viz.comprehensiveness, trustworthiness, and utility.\n\nThis paper is structured as follows.First, section 2 presents a review of related work in the fields of text summarization and LLM evaluation.In section 3, we show our approach to using LLMs for scientific synthesis evaluation, wherein subsection 3.1 describes the LLM output, while subsection 3.2 presents a qualitative evaluation of this output.In subsection 3.3, we analyze the correlation between LLM ratings and human judgments.A discussion of our findings and final conclusions is described in section 4.",
            "reference_string": "[270924255 | Evans et al. | 2024 | Citations: 4]"
        },
        {
            "title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 92,
            "citation_count": 76,
            "influential_citation_count": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.00622",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.00622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2112309079",
                    "name": "Ryan Liu"
                },
                {
                    "authorId": "1737249",
                    "name": "Nihar B. Shah"
                }
            ],
            "abstract": "Given the rapid ascent of large language models (LLMs), we study the question: (How) can large language models help in reviewing of scientific papers or proposals? We first conduct some pilot studies where we find that (i) GPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to identify errors) outperforms prompting to simply write a review. With these insights, we study the use of LLMs (specifically, GPT-4) for three tasks: 1. Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers. We observe that the LLM finds errors in 7 of them, spanning both mathematical and conceptual errors. 2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist questions in the respective sections of 15 NeurIPS 2022 papers. We find that across 119 {checklist question, paper} pairs, the LLM had an 86.6% accuracy. 3. Choosing the\"better\"paper: We generate 10 pairs of abstracts, deliberately designing each pair in such a way that one abstract was clearly superior than the other. The LLM, however, struggled to discern these relatively straightforward distinctions accurately, committing errors in its evaluations for 6 out of the 10 pairs. Based on these experiments, we think that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not (yet) for complete evaluations of papers or proposals.",
            "corpus_id": 258999338,
            "sentences": [
                {
                    "corpus_id": "258999338",
                    "title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing",
                    "text": "Given the rapid ascent of large language models (LLMs), we study the question: (How) can large language models help in reviewing of scientific papers or proposals? We first conduct some pilot studies where we find that (i) GPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to identify errors) outperforms prompting to simply write a review. With these insights, we study the use of LLMs (specifically, GPT-4) for three tasks: 1. Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers. We observe that the LLM finds errors in 7 of them, spanning both mathematical and conceptual errors. 2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist questions in the respective sections of 15 NeurIPS 2022 papers. We find that across 119 {checklist question, paper} pairs, the LLM had an 86.6% accuracy. 3. Choosing the\"better\"paper: We generate 10 pairs of abstracts, deliberately designing each pair in such a way that one abstract was clearly superior than the other. The LLM, however, struggled to discern these relatively straightforward distinctions accurately, committing errors in its evaluations for 6 out of the 10 pairs. Based on these experiments, we think that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not (yet) for complete evaluations of papers or proposals.",
                    "score": 0.3826519984850574,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81982421875
                },
                {
                    "corpus_id": "258999338",
                    "title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing",
                    "text": "We queried GPT-4 for three responses per {question, paper} pair, taking the majority vote as the answer and evaluating its correctness against the ground truth label. If all three responses were different, we marked the answer as incorrect. In Table 2, we provide the results for our checklist experiment. We found that compared to the hand-labeled ground truth, GPT-4 achieves 86.6% accuracy across 119 examples.\n\nIn some more detail, author-submitted checklists also match the ground truth 86.6% of the time, although the mismatches may potentially be due to later paper revisions. We find that there is little overlap between LLM and author mismatches: On one hand, GPT-4 answers disagree with 12 out of 16 (75%) of mismatched author responses, allowing for further examination. On the other hand, 9 out of 16 (56.3%) of GPT-4's incorrect answers had correct author responses. Furthermore, 8 out of 16 of GPT-4's incorrect answers were  Table 2: Accuracy of GPT-4 on papers and checklist questions. The notations \u2713 and \u00d7 correspond to whether the majority label across three responses by GPT-4 was correct or incorrect respectively, as measured against the hand-labeled ground truth. Grey cells indicate the author had initially marked \"no\" or \"N/A\".\n\ndue to there being no textual evidence of the correct answer (e.g., requires access to figures in the paper, which is beyond its current capabilities). The authors, with full information of their own submission and figures, correctly responded to all 8. Removing this set of questions increases GPT-4's accuracy to 92.8%, but the 86.6% statistic is more representative across the problems that current LLMs may face when deployed into checklist-verifying roles.",
                    "score": 0.4235185059480855,
                    "section_title": "Summary of results",
                    "char_start_offset": 94299,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61181640625
                }
            ],
            "relevance_judgement": 0.81982421875,
            "relevance_judgment_input_expanded": "# Title: ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing\n# Venue: arXiv.org\n# Authors: Ryan Liu, Nihar B. Shah\n## Abstract\nGiven the rapid ascent of large language models (LLMs), we study the question: (How) can large language models help in reviewing of scientific papers or proposals? We first conduct some pilot studies where we find that (i) GPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to identify errors) outperforms prompting to simply write a review. With these insights, we study the use of LLMs (specifically, GPT-4) for three tasks: 1. Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers. We observe that the LLM finds errors in 7 of them, spanning both mathematical and conceptual errors. 2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist questions in the respective sections of 15 NeurIPS 2022 papers. We find that across 119 {checklist question, paper} pairs, the LLM had an 86.6% accuracy. 3. Choosing the\"better\"paper: We generate 10 pairs of abstracts, deliberately designing each pair in such a way that one abstract was clearly superior than the other. The LLM, however, struggled to discern these relatively straightforward distinctions accurately, committing errors in its evaluations for 6 out of the 10 pairs. Based on these experiments, we think that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not (yet) for complete evaluations of papers or proposals.\n## Summary of results\nWe queried GPT-4 for three responses per {question, paper} pair, taking the majority vote as the answer and evaluating its correctness against the ground truth label. If all three responses were different, we marked the answer as incorrect. In Table 2, we provide the results for our checklist experiment. We found that compared to the hand-labeled ground truth, GPT-4 achieves 86.6% accuracy across 119 examples.\n\nIn some more detail, author-submitted checklists also match the ground truth 86.6% of the time, although the mismatches may potentially be due to later paper revisions. We find that there is little overlap between LLM and author mismatches: On one hand, GPT-4 answers disagree with 12 out of 16 (75%) of mismatched author responses, allowing for further examination. On the other hand, 9 out of 16 (56.3%) of GPT-4's incorrect answers had correct author responses. Furthermore, 8 out of 16 of GPT-4's incorrect answers were  Table 2: Accuracy of GPT-4 on papers and checklist questions. The notations \u2713 and \u00d7 correspond to whether the majority label across three responses by GPT-4 was correct or incorrect respectively, as measured against the hand-labeled ground truth. Grey cells indicate the author had initially marked \"no\" or \"N/A\".\n\ndue to there being no textual evidence of the correct answer (e.g., requires access to figures in the paper, which is beyond its current capabilities). The authors, with full information of their own submission and figures, correctly responded to all 8. Removing this set of questions increases GPT-4's accuracy to 92.8%, but the 86.6% statistic is more representative across the problems that current LLMs may face when deployed into checklist-verifying roles.",
            "reference_string": "[258999338 | Liu et al. | 2023 | Citations: 76]"
        },
        {
            "title": "Using Large Language Models for Automated Grading of Student Writing about Science",
            "venue": "International Journal of Artificial Intelligence in Education",
            "year": 2024,
            "reference_count": 75,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2412.18719",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.18719, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2282957473",
                    "name": "Christopher Impey"
                },
                {
                    "authorId": "2282961815",
                    "name": "Matthew C. Wenger"
                },
                {
                    "authorId": "2306959829",
                    "name": "Nikhil Garuda"
                },
                {
                    "authorId": "65754049",
                    "name": "Shahriar Golchin"
                },
                {
                    "authorId": "2337688775",
                    "name": "Sarah Stamer"
                }
            ],
            "abstract": "Assessing writing in large classes for formal or informal learners presents a significant challenge. Consequently, most large classes, particularly in science, rely on objective assessment tools such as multiple-choice quizzes, which have a single correct answer. The rapid development of AI has introduced the possibility of using large language models (LLMs) to evaluate student writing. An experiment was conducted using GPT-4 to determine if machine learning methods based on LLMs can match or exceed the reliability of instructor grading in evaluating short writing assignments on topics in astronomy. The audience consisted of adult learners in three massive open online courses (MOOCs) offered through Coursera. One course was on astronomy, the second was on astrobiology, and the third was on the history and philosophy of astronomy. The results should also be applicable to non-science majors in university settings, where the content and modes of evaluation are similar. The data comprised answers from 120 students to 12 questions across the three courses. GPT-4 was provided with total grades, model answers, and rubrics from an instructor for all three courses. In addition to evaluating how reliably the LLM reproduced instructor grades, the LLM was also tasked with generating its own rubrics. Overall, the LLM was more reliable than peer grading, both in aggregate and by individual student, and approximately matched instructor grades for all three online courses. The implication is that LLMs may soon be used for automated, reliable, and scalable grading of student science writing.",
            "corpus_id": 275119013,
            "sentences": [
                {
                    "corpus_id": "275119013",
                    "title": "Using Large Language Models for Automated Grading of Student Writing about Science",
                    "text": "Assessing writing in large classes for formal or informal learners presents a significant challenge. Consequently, most large classes, particularly in science, rely on objective assessment tools such as multiple-choice quizzes, which have a single correct answer. The rapid development of AI has introduced the possibility of using large language models (LLMs) to evaluate student writing. An experiment was conducted using GPT-4 to determine if machine learning methods based on LLMs can match or exceed the reliability of instructor grading in evaluating short writing assignments on topics in astronomy. The audience consisted of adult learners in three massive open online courses (MOOCs) offered through Coursera. One course was on astronomy, the second was on astrobiology, and the third was on the history and philosophy of astronomy. The results should also be applicable to non-science majors in university settings, where the content and modes of evaluation are similar. The data comprised answers from 120 students to 12 questions across the three courses. GPT-4 was provided with total grades, model answers, and rubrics from an instructor for all three courses. In addition to evaluating how reliably the LLM reproduced instructor grades, the LLM was also tasked with generating its own rubrics. Overall, the LLM was more reliable than peer grading, both in aggregate and by individual student, and approximately matched instructor grades for all three online courses. The implication is that LLMs may soon be used for automated, reliable, and scalable grading of student science writing.",
                    "score": 0.405953183873833,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.802734375
                }
            ],
            "relevance_judgement": 0.802734375,
            "relevance_judgment_input_expanded": "# Title: Using Large Language Models for Automated Grading of Student Writing about Science\n# Venue: International Journal of Artificial Intelligence in Education\n# Authors: Christopher Impey, Matthew C. Wenger, Nikhil Garuda, Shahriar Golchin, Sarah Stamer\n## Abstract\nAssessing writing in large classes for formal or informal learners presents a significant challenge. Consequently, most large classes, particularly in science, rely on objective assessment tools such as multiple-choice quizzes, which have a single correct answer. The rapid development of AI has introduced the possibility of using large language models (LLMs) to evaluate student writing. An experiment was conducted using GPT-4 to determine if machine learning methods based on LLMs can match or exceed the reliability of instructor grading in evaluating short writing assignments on topics in astronomy. The audience consisted of adult learners in three massive open online courses (MOOCs) offered through Coursera. One course was on astronomy, the second was on astrobiology, and the third was on the history and philosophy of astronomy. The results should also be applicable to non-science majors in university settings, where the content and modes of evaluation are similar. The data comprised answers from 120 students to 12 questions across the three courses. GPT-4 was provided with total grades, model answers, and rubrics from an instructor for all three courses. In addition to evaluating how reliably the LLM reproduced instructor grades, the LLM was also tasked with generating its own rubrics. Overall, the LLM was more reliable than peer grading, both in aggregate and by individual student, and approximately matched instructor grades for all three online courses. The implication is that LLMs may soon be used for automated, reliable, and scalable grading of student science writing.\n",
            "reference_string": "[275119013 | Impey et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Enhancing Clinical Reasoning with Virtual Patients: A Hybrid Systematic Review Combining Human Reviewers and ChatGPT",
            "venue": "Healthcare",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11594149, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2311772509",
                    "name": "Daniel Garc\u00eda-Torres"
                },
                {
                    "authorId": "2165787803",
                    "name": "Mar\u00eda Asunci\u00f3n Vicente Ripoll"
                },
                {
                    "authorId": "2228013807",
                    "name": "C\u00e9sar Fern\u00e1ndez Peris"
                },
                {
                    "authorId": "116702418",
                    "name": "J. J. Mira Solves"
                }
            ],
            "abstract": "Objectives: This study presents a systematic review aimed at evaluating the effectiveness of virtual patients in enhancing clinical reasoning skills in medical education. A hybrid methodology was used, combining human reviewers and ChatGPT to assess the impact of conversational virtual patients on student learning outcomes and satisfaction. Methods: Various studies involving conversational virtual patients were analyzed to determine the effect of these digital tools on clinical competencies. The hybrid review process incorporated both human assessments and AI-driven reviews, allowing a comparison of accuracy between the two approaches. Results: Consistent with previous systematic reviews, our findings suggest that conversational virtual patients can improve clinical competencies, particularly in history-taking and clinical reasoning. Regarding student feedback, satisfaction tends to be higher when virtual patients\u2019 interactions are more realistic, often due to the use of artificial intelligence (AI) and natural language processing (NLP) in the simulators. Furthermore, the study compares the accuracy of AI-driven reviews with human assessments, revealing comparable results. Conclusions: This research highlights AI\u2019s potential to complement human expertise in academic evaluations, contributing to more efficient and consistent systematic reviews in rapidly evolving educational fields.",
            "corpus_id": 273982985,
            "sentences": [
                {
                    "corpus_id": "273982985",
                    "title": "Enhancing Clinical Reasoning with Virtual Patients: A Hybrid Systematic Review Combining Human Reviewers and ChatGPT",
                    "text": "In recent years, artificial intelligence (AI) has emerged as a valuable tool in various fields of academic and medical research. Large language models (LLMs), such as ChatGPT developed by OpenAI, exemplify how AI can contribute to the systematic review process of scientific articles. These models, particularly those based on the GPT-4 architecture, possess the capability to analyze large volumes of text, identify patterns, and provide detailed feedback in significantly less time than human reviewers. For instance, in the work of Katz et al. [18], the remarkable advancement of language models, particularly GPT-4, is highlighted. This research evaluates GPT-4's performance on the Bar Exam, including the multiple-choice, essay, and practical tests. GPT-4 significantly outperformed its predecessors and even achieved higher scores than the average human examinee in several areas. \n\nThe use of LLMs like ChatGPT as article reviewers offers several advantages. First, AI can efficiently handle repetitive tasks and data analysis, allowing human reviewers to focus on more complex and critical aspects of the review process. Second, ChatGPT can assist in identifying grammatical errors, coherence issues, and other technical aspects of the manuscript, thereby enhancing the overall quality of the text prior to final human review. \n\nMoreover, integrating ChatGPT into the review process ensures greater consistency and objectivity in manuscript evaluation. AI can apply the same evaluation criteria uniformly, reducing potential bias that may arise from the differing perceptions and experiences of human reviewers. This not only improves the quality of the review but also expedites the process, which is particularly valuable in rapidly evolving fields such as medical education and health technologies [19]. \n\nIt is crucial to acknowledge, however, that the role of ChatGPT should be complementary to that of human reviewers. While AI can provide valuable preliminary analysis, the expertise and clinical judgment of human experts remain irreplaceable for assessing the scientific relevance, innovation, and clinical applicability of articles. Therefore, a collaborative approach that combines the strengths of AI and human review can yield the best results in the evaluation of scientific literature [20].",
                    "score": 0.39961900940566253,
                    "section_title": "Use of ChatGPT or Other LLMs as Article Reviewers",
                    "char_start_offset": 4922,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 284
                        },
                        {
                            "start": 285,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 755
                        },
                        {
                            "start": 756,
                            "end": 887
                        },
                        {
                            "start": 890,
                            "end": 966
                        },
                        {
                            "start": 967,
                            "end": 1129
                        },
                        {
                            "start": 1130,
                            "end": 1335
                        },
                        {
                            "start": 1338,
                            "end": 1461
                        },
                        {
                            "start": 1462,
                            "end": 1620
                        },
                        {
                            "start": 1621,
                            "end": 1815
                        },
                        {
                            "start": 1818,
                            "end": 1933
                        },
                        {
                            "start": 1934,
                            "end": 2151
                        },
                        {
                            "start": 2152,
                            "end": 2314
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 547,
                            "end": 551,
                            "matchedPaperCorpusId": "257572753"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7998046875
                }
            ],
            "relevance_judgement": 0.7998046875,
            "relevance_judgment_input_expanded": "# Title: Enhancing Clinical Reasoning with Virtual Patients: A Hybrid Systematic Review Combining Human Reviewers and ChatGPT\n# Venue: Healthcare\n# Authors: Daniel Garc\u00eda-Torres, Mar\u00eda Asunci\u00f3n Vicente Ripoll, C\u00e9sar Fern\u00e1ndez Peris, J. J. Mira Solves\n## Abstract\nObjectives: This study presents a systematic review aimed at evaluating the effectiveness of virtual patients in enhancing clinical reasoning skills in medical education. A hybrid methodology was used, combining human reviewers and ChatGPT to assess the impact of conversational virtual patients on student learning outcomes and satisfaction. Methods: Various studies involving conversational virtual patients were analyzed to determine the effect of these digital tools on clinical competencies. The hybrid review process incorporated both human assessments and AI-driven reviews, allowing a comparison of accuracy between the two approaches. Results: Consistent with previous systematic reviews, our findings suggest that conversational virtual patients can improve clinical competencies, particularly in history-taking and clinical reasoning. Regarding student feedback, satisfaction tends to be higher when virtual patients\u2019 interactions are more realistic, often due to the use of artificial intelligence (AI) and natural language processing (NLP) in the simulators. Furthermore, the study compares the accuracy of AI-driven reviews with human assessments, revealing comparable results. Conclusions: This research highlights AI\u2019s potential to complement human expertise in academic evaluations, contributing to more efficient and consistent systematic reviews in rapidly evolving educational fields.\n## Use of ChatGPT or Other LLMs as Article Reviewers\nIn recent years, artificial intelligence (AI) has emerged as a valuable tool in various fields of academic and medical research. Large language models (LLMs), such as ChatGPT developed by OpenAI, exemplify how AI can contribute to the systematic review process of scientific articles. These models, particularly those based on the GPT-4 architecture, possess the capability to analyze large volumes of text, identify patterns, and provide detailed feedback in significantly less time than human reviewers. For instance, in the work of Katz et al. [18], the remarkable advancement of language models, particularly GPT-4, is highlighted. This research evaluates GPT-4's performance on the Bar Exam, including the multiple-choice, essay, and practical tests. GPT-4 significantly outperformed its predecessors and even achieved higher scores than the average human examinee in several areas. \n\nThe use of LLMs like ChatGPT as article reviewers offers several advantages. First, AI can efficiently handle repetitive tasks and data analysis, allowing human reviewers to focus on more complex and critical aspects of the review process. Second, ChatGPT can assist in identifying grammatical errors, coherence issues, and other technical aspects of the manuscript, thereby enhancing the overall quality of the text prior to final human review. \n\nMoreover, integrating ChatGPT into the review process ensures greater consistency and objectivity in manuscript evaluation. AI can apply the same evaluation criteria uniformly, reducing potential bias that may arise from the differing perceptions and experiences of human reviewers. This not only improves the quality of the review but also expedites the process, which is particularly valuable in rapidly evolving fields such as medical education and health technologies [19]. \n\nIt is crucial to acknowledge, however, that the role of ChatGPT should be complementary to that of human reviewers. While AI can provide valuable preliminary analysis, the expertise and clinical judgment of human experts remain irreplaceable for assessing the scientific relevance, innovation, and clinical applicability of articles. Therefore, a collaborative approach that combines the strengths of AI and human review can yield the best results in the evaluation of scientific literature [20].",
            "reference_string": "[273982985 | Garcia-Torres et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Automated title and abstract screening for scoping reviews using the GPT-4 Large Language Model",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 27,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.07918, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2266464909",
                    "name": "David Wilkins"
                }
            ],
            "abstract": "Scoping reviews, a type of literature review, require intensive human effort to screen large numbers of scholarly sources for their relevance to the review objectives. This manuscript introduces GPTscreenR, a package for the R statistical programming language that uses the GPT-4 Large Language Model (LLM) to automatically screen sources. The package makes use of the chain-of-thought technique with the goal of maximising performance on complex screening tasks. In validation against consensus human reviewer decisions, GPTscreenR performed similarly to an alternative zero-shot technique, with a sensitivity of 71%, specificity of 89%, and overall accuracy of 84%. Neither method achieved perfect accuracy nor human levels of intraobserver agreement. GPTscreenR demonstrates the potential for LLMs to support scholarly work and provides a user-friendly software framework that can be integrated into existing review processes.",
            "corpus_id": 265157492,
            "sentences": [
                {
                    "corpus_id": "265157492",
                    "title": "Automated title and abstract screening for scoping reviews using the GPT-4 Large Language Model",
                    "text": "They also systematically evaluated prompts given to the LLM to identify a prompt that performed best at the screening task; their chosen prompt, like that of Guo et al., placed the LLM in the role of an academic reviewer. \n\nBoth of these approaches made use of a single, fixed text prompt template, which the LLM then completes with additional text representing its response (the decision to include or exclude a source), a method sometimes called 'zero-shot prompting'. Recent work has identified a number of methods which can be superior to zero-shot prompting when using LLMs for tasks that require complex or multistep reasoning. These methods include 'chain-ofthought prompting' [21], in which a complex task is broken down into a series of intermediate steps, and the 'tree of thoughts' strategy [23], in which multiple parallel chains of thought are generated, compared, and integrated. The LLM is induced to follow these complex reasoning strategies either by being given examples of multi-step reasoning on similar tasks, or by being lead through the process with a series of intermediate prompts. \n\nIn this paper, I introduce a package for the R programming language [14] called GPTscreenR that implements a chain-of-thought approach to using GPT-4 for scoping review title and abstract screening, and evaluate its performance by comparison to human reviewers. The purpose of this package is to assist and augment rather than replace human reviewers in performing scoping reviews. This package represents the first LLM-based screening tool designed specifically for scoping reviews. Further, this study provides the first report on the accuracy of LLM-based screening using the most recent iteration of the GPT model lineage, GPT-4, and using the recently developed chain-of-thought approach.",
                    "score": 0.3715397080007158,
                    "section_title": "Introduction",
                    "char_start_offset": 3851,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 221
                        },
                        {
                            "start": 224,
                            "end": 470
                        },
                        {
                            "start": 471,
                            "end": 633
                        },
                        {
                            "start": 634,
                            "end": 893
                        },
                        {
                            "start": 894,
                            "end": 1106
                        },
                        {
                            "start": 1109,
                            "end": 1370
                        },
                        {
                            "start": 1371,
                            "end": 1490
                        },
                        {
                            "start": 1491,
                            "end": 1592
                        },
                        {
                            "start": 1593,
                            "end": 1802
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7958984375
                }
            ],
            "relevance_judgement": 0.7958984375,
            "relevance_judgment_input_expanded": "# Title: Automated title and abstract screening for scoping reviews using the GPT-4 Large Language Model\n# Venue: arXiv.org\n# Authors: David Wilkins\n## Abstract\nScoping reviews, a type of literature review, require intensive human effort to screen large numbers of scholarly sources for their relevance to the review objectives. This manuscript introduces GPTscreenR, a package for the R statistical programming language that uses the GPT-4 Large Language Model (LLM) to automatically screen sources. The package makes use of the chain-of-thought technique with the goal of maximising performance on complex screening tasks. In validation against consensus human reviewer decisions, GPTscreenR performed similarly to an alternative zero-shot technique, with a sensitivity of 71%, specificity of 89%, and overall accuracy of 84%. Neither method achieved perfect accuracy nor human levels of intraobserver agreement. GPTscreenR demonstrates the potential for LLMs to support scholarly work and provides a user-friendly software framework that can be integrated into existing review processes.\n## Introduction\nThey also systematically evaluated prompts given to the LLM to identify a prompt that performed best at the screening task; their chosen prompt, like that of Guo et al., placed the LLM in the role of an academic reviewer. \n\nBoth of these approaches made use of a single, fixed text prompt template, which the LLM then completes with additional text representing its response (the decision to include or exclude a source), a method sometimes called 'zero-shot prompting'. Recent work has identified a number of methods which can be superior to zero-shot prompting when using LLMs for tasks that require complex or multistep reasoning. These methods include 'chain-ofthought prompting' [21], in which a complex task is broken down into a series of intermediate steps, and the 'tree of thoughts' strategy [23], in which multiple parallel chains of thought are generated, compared, and integrated. The LLM is induced to follow these complex reasoning strategies either by being given examples of multi-step reasoning on similar tasks, or by being lead through the process with a series of intermediate prompts. \n\nIn this paper, I introduce a package for the R programming language [14] called GPTscreenR that implements a chain-of-thought approach to using GPT-4 for scoping review title and abstract screening, and evaluate its performance by comparison to human reviewers. The purpose of this package is to assist and augment rather than replace human reviewers in performing scoping reviews. This package represents the first LLM-based screening tool designed specifically for scoping reviews. Further, this study provides the first report on the accuracy of LLM-based screening using the most recent iteration of the GPT model lineage, GPT-4, and using the recently developed chain-of-thought approach.",
            "reference_string": "[265157492 | Wilkins | 2023 | Citations: 3]"
        },
        {
            "title": "CycleResearcher: Improving Automated Research via Automated Review",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 164,
            "citation_count": 25,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.00816, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2142839441",
                    "name": "Yixuan Weng"
                },
                {
                    "authorId": "2316827669",
                    "name": "Minjun Zhu"
                },
                {
                    "authorId": "1993226927",
                    "name": "Guangsheng Bao"
                },
                {
                    "authorId": "2329135444",
                    "name": "Hongbo Zhang"
                },
                {
                    "authorId": "2273553706",
                    "name": "Jindong Wang"
                },
                {
                    "authorId": "2325943212",
                    "name": "Yue Zhang"
                },
                {
                    "authorId": "2145500840",
                    "name": "Linyi Yang"
                }
            ],
            "abstract": "The automation of scientific discovery has been a long-standing goal within the research community, driven by the potential to accelerate knowledge creation. While significant progress has been made using commercial large language models (LLMs) as research assistants or idea generators, the possibility of automating the entire research process with open-source LLMs remains largely unexplored. This paper explores the feasibility of using open-source post-trained LLMs as autonomous agents capable of performing the full cycle of automated research and review, from literature review and manuscript preparation to peer review and paper refinement. Our iterative preference training framework consists of CycleResearcher, which conducts research tasks, and CycleReviewer, which simulates the peer review process, providing iterative feedback via reinforcement learning. To train these models, we develop two new datasets, Review-5k and Research-14k, reflecting real-world machine learning research and peer review dynamics. Our results demonstrate that CycleReviewer achieves promising performance with a 26.89\\% reduction in mean absolute error (MAE) compared to individual human reviewers in predicting paper scores, indicating the potential of LLMs to effectively assist expert-level research evaluation. In research, the papers generated by the CycleResearcher model achieved a score of 5.36 in simulated peer reviews, showing some competitiveness in terms of simulated review scores compared to the preprint level of 5.24 from human experts, while still having room for improvement compared to the accepted paper level of 5.69. This work represents a significant step toward fully automated scientific inquiry, providing ethical safeguards and exploring AI-driven research capabilities. The code, dataset and model weight are released at https://wengsyx.github.io/Researcher/.",
            "corpus_id": 273811997,
            "sentences": [
                {
                    "corpus_id": "273811997",
                    "title": "CycleResearcher: Improving Automated Research via Automated Review",
                    "text": "The key challenge is enabling AI to go beyond analysis and actively contribute to generating new research ideas, which demands advanced reasoning and creativity. Our work builds on AI's historical role in science, aiming to shift AI from a supporting tool to a leader in scientific discovery. \n\nAutomated Evaluation of Research Papers. The use of AI tools in the scientific publishing process has garnered widespread attention (Bao et al., 2021;Liu & Shah, 2023;Liang et al., 2024;D'Arcy et al., 2024;Jin et al., 2024), including summarizing research paper content (Collins et al., 2017), detecting inaccuracies (Nuijten et al., 2016), and identifying fairness disparities (Zhang et al., 2022). Hosseini & Horbach (2023) conducted small-scale qualitative experiments to evaluate the effectiveness of ChatGPT in the peer review process, while Robertson (2023) invited 10 participants to assess the benefits of GPT-4 in assisting with peer review. Lu et al. (2024) and Tyser et al. (2024) used GPT-4 to evaluate full-text PDFs of scientific papers. However, when LLMs act as judges, even the most advanced models, such as GPT-4 (Achiam et al., 2023) and Gemini (Reid et al., 2024), still lag behind reward models specifically trained for the task, as seen in RewardBench (Lambert et al., 2024). This gap highlights the challenge of achieving human-level judgment and reasoning in AI-driven peer reviews. In contrast, we train a Generative Reward Model (Zhang et al., 2024) to simulate a comprehensive peer review. Our CycleReviewer simulates reviewers with varying perspectives, documenting summaries, strengths, and weaknesses. In the final stage, a primary reviewer consolidates these insights to deliver the final decision.",
                    "score": 0.3838882065532446,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 25924,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 292
                        },
                        {
                            "start": 295,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 694
                        },
                        {
                            "start": 695,
                            "end": 945
                        },
                        {
                            "start": 946,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1292
                        },
                        {
                            "start": 1293,
                            "end": 1401
                        },
                        {
                            "start": 1402,
                            "end": 1511
                        },
                        {
                            "start": 1512,
                            "end": 1626
                        },
                        {
                            "start": 1627,
                            "end": 1724
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 427,
                            "end": 445,
                            "matchedPaperCorpusId": "235324633"
                        },
                        {
                            "start": 462,
                            "end": 481,
                            "matchedPaperCorpusId": "263608784"
                        },
                        {
                            "start": 612,
                            "end": 634,
                            "matchedPaperCorpusId": "265505419"
                        },
                        {
                            "start": 695,
                            "end": 720,
                            "matchedPaperCorpusId": "257310820"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75634765625
                }
            ],
            "relevance_judgement": 0.75634765625,
            "relevance_judgment_input_expanded": "# Title: CycleResearcher: Improving Automated Research via Automated Review\n# Venue: arXiv.org\n# Authors: Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang\n## Abstract\nThe automation of scientific discovery has been a long-standing goal within the research community, driven by the potential to accelerate knowledge creation. While significant progress has been made using commercial large language models (LLMs) as research assistants or idea generators, the possibility of automating the entire research process with open-source LLMs remains largely unexplored. This paper explores the feasibility of using open-source post-trained LLMs as autonomous agents capable of performing the full cycle of automated research and review, from literature review and manuscript preparation to peer review and paper refinement. Our iterative preference training framework consists of CycleResearcher, which conducts research tasks, and CycleReviewer, which simulates the peer review process, providing iterative feedback via reinforcement learning. To train these models, we develop two new datasets, Review-5k and Research-14k, reflecting real-world machine learning research and peer review dynamics. Our results demonstrate that CycleReviewer achieves promising performance with a 26.89\\% reduction in mean absolute error (MAE) compared to individual human reviewers in predicting paper scores, indicating the potential of LLMs to effectively assist expert-level research evaluation. In research, the papers generated by the CycleResearcher model achieved a score of 5.36 in simulated peer reviews, showing some competitiveness in terms of simulated review scores compared to the preprint level of 5.24 from human experts, while still having room for improvement compared to the accepted paper level of 5.69. This work represents a significant step toward fully automated scientific inquiry, providing ethical safeguards and exploring AI-driven research capabilities. The code, dataset and model weight are released at https://wengsyx.github.io/Researcher/.\n## RELATED WORK\nThe key challenge is enabling AI to go beyond analysis and actively contribute to generating new research ideas, which demands advanced reasoning and creativity. Our work builds on AI's historical role in science, aiming to shift AI from a supporting tool to a leader in scientific discovery. \n\nAutomated Evaluation of Research Papers. The use of AI tools in the scientific publishing process has garnered widespread attention (Bao et al., 2021;Liu & Shah, 2023;Liang et al., 2024;D'Arcy et al., 2024;Jin et al., 2024), including summarizing research paper content (Collins et al., 2017), detecting inaccuracies (Nuijten et al., 2016), and identifying fairness disparities (Zhang et al., 2022). Hosseini & Horbach (2023) conducted small-scale qualitative experiments to evaluate the effectiveness of ChatGPT in the peer review process, while Robertson (2023) invited 10 participants to assess the benefits of GPT-4 in assisting with peer review. Lu et al. (2024) and Tyser et al. (2024) used GPT-4 to evaluate full-text PDFs of scientific papers. However, when LLMs act as judges, even the most advanced models, such as GPT-4 (Achiam et al., 2023) and Gemini (Reid et al., 2024), still lag behind reward models specifically trained for the task, as seen in RewardBench (Lambert et al., 2024). This gap highlights the challenge of achieving human-level judgment and reasoning in AI-driven peer reviews. In contrast, we train a Generative Reward Model (Zhang et al., 2024) to simulate a comprehensive peer review. Our CycleReviewer simulates reviewers with varying perspectives, documenting summaries, strengths, and weaknesses. In the final stage, a primary reviewer consolidates these insights to deliver the final decision.",
            "reference_string": "[273811997 | Weng et al. | 2024 | Citations: 25]"
        },
        {
            "title": "Strengths and Weaknesses of ChatGPT Models for Scientific Writing About Medical Vitamin B12: Mixed Methods Study",
            "venue": "JMIR Formative Research",
            "year": 2023,
            "reference_count": 29,
            "citation_count": 15,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://formative.jmir.org/2023/1/e49459/PDF",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10674142, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "8176300",
                    "name": "Omar Abuyaman"
                }
            ],
            "abstract": "Background ChatGPT is a large language model developed by OpenAI designed to generate human-like responses to prompts. Objective This study aims to evaluate the ability of GPT-4 to generate scientific content and assist in scientific writing using medical vitamin B12 as the topic. Furthermore, the study will compare the performance of GPT-4 to its predecessor, GPT-3.5. Methods The study examined responses from GPT-4 and GPT-3.5 to vitamin B12\u2013related prompts, focusing on their quality and characteristics and comparing them to established scientific literature. Results The results indicated that GPT-4 can potentially streamline scientific writing through its ability to edit language and write abstracts, keywords, and abbreviation lists. However, significant limitations of ChatGPT were revealed, including its inability to identify and address bias, inability to include recent information, lack of transparency, and inclusion of inaccurate information. Additionally, it cannot check for plagiarism or provide proper references. The accuracy of GPT-4\u2019s answers was found to be superior to GPT-3.5. Conclusions ChatGPT can be considered a helpful assistant in the writing process but not a replacement for a scientist\u2019s expertise. Researchers must remain aware of its limitations and use it appropriately. The improvements in consecutive ChatGPT versions suggest the possibility of overcoming some present limitations in the near future.",
            "corpus_id": 265103287,
            "sentences": [
                {
                    "corpus_id": "265103287",
                    "title": "Strengths and Weaknesses of ChatGPT Models for Scientific Writing About Medical Vitamin B12: Mixed Methods Study",
                    "text": "Background ChatGPT is a large language model developed by OpenAI designed to generate human-like responses to prompts. Objective This study aims to evaluate the ability of GPT-4 to generate scientific content and assist in scientific writing using medical vitamin B12 as the topic. Furthermore, the study will compare the performance of GPT-4 to its predecessor, GPT-3.5. Methods The study examined responses from GPT-4 and GPT-3.5 to vitamin B12\u2013related prompts, focusing on their quality and characteristics and comparing them to established scientific literature. Results The results indicated that GPT-4 can potentially streamline scientific writing through its ability to edit language and write abstracts, keywords, and abbreviation lists. However, significant limitations of ChatGPT were revealed, including its inability to identify and address bias, inability to include recent information, lack of transparency, and inclusion of inaccurate information. Additionally, it cannot check for plagiarism or provide proper references. The accuracy of GPT-4\u2019s answers was found to be superior to GPT-3.5. Conclusions ChatGPT can be considered a helpful assistant in the writing process but not a replacement for a scientist\u2019s expertise. Researchers must remain aware of its limitations and use it appropriately. The improvements in consecutive ChatGPT versions suggest the possibility of overcoming some present limitations in the near future.",
                    "score": 0.4714148805594768,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7490234375
                },
                {
                    "corpus_id": "265103287",
                    "title": "Strengths and Weaknesses of ChatGPT Models for Scientific Writing About Medical Vitamin B12: Mixed Methods Study",
                    "text": "Additionally, it could theoretically reduce errors and inconsistencies that are often found in scientific writing. A recent release of the ChatGPT model, GPT-4, boasts enhanced accuracy and heightened attention to details in user prompts, as demonstrated by OpenAI's internal test results on an array of professional and academic exams [19]. However, its performance in the realm of scientific writing remains untested, and the results of OpenAI's evaluations have not been verified through peer-reviewed publications. In this study, an evaluation was performed on GPT-4's ability to generate scientific text on the specific topic of vitamin B12 treatment. In addition, the performance of GPT-4 was compared to that of its predecessor, GPT-3.5.",
                    "score": 0.4556620464421468,
                    "section_title": "Introduction",
                    "char_start_offset": 2096,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 114
                        },
                        {
                            "start": 115,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 518
                        },
                        {
                            "start": 519,
                            "end": 656
                        },
                        {
                            "start": 657,
                            "end": 744
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57177734375
                }
            ],
            "relevance_judgement": 0.7490234375,
            "relevance_judgment_input_expanded": "# Title: Strengths and Weaknesses of ChatGPT Models for Scientific Writing About Medical Vitamin B12: Mixed Methods Study\n# Venue: JMIR Formative Research\n# Authors: Omar Abuyaman\n## Abstract\nBackground ChatGPT is a large language model developed by OpenAI designed to generate human-like responses to prompts. Objective This study aims to evaluate the ability of GPT-4 to generate scientific content and assist in scientific writing using medical vitamin B12 as the topic. Furthermore, the study will compare the performance of GPT-4 to its predecessor, GPT-3.5. Methods The study examined responses from GPT-4 and GPT-3.5 to vitamin B12\u2013related prompts, focusing on their quality and characteristics and comparing them to established scientific literature. Results The results indicated that GPT-4 can potentially streamline scientific writing through its ability to edit language and write abstracts, keywords, and abbreviation lists. However, significant limitations of ChatGPT were revealed, including its inability to identify and address bias, inability to include recent information, lack of transparency, and inclusion of inaccurate information. Additionally, it cannot check for plagiarism or provide proper references. The accuracy of GPT-4\u2019s answers was found to be superior to GPT-3.5. Conclusions ChatGPT can be considered a helpful assistant in the writing process but not a replacement for a scientist\u2019s expertise. Researchers must remain aware of its limitations and use it appropriately. The improvements in consecutive ChatGPT versions suggest the possibility of overcoming some present limitations in the near future.\n## Introduction\nAdditionally, it could theoretically reduce errors and inconsistencies that are often found in scientific writing. A recent release of the ChatGPT model, GPT-4, boasts enhanced accuracy and heightened attention to details in user prompts, as demonstrated by OpenAI's internal test results on an array of professional and academic exams [19]. However, its performance in the realm of scientific writing remains untested, and the results of OpenAI's evaluations have not been verified through peer-reviewed publications. In this study, an evaluation was performed on GPT-4's ability to generate scientific text on the specific topic of vitamin B12 treatment. In addition, the performance of GPT-4 was compared to that of its predecessor, GPT-3.5.",
            "reference_string": "[265103287 | Abuyaman | 2023 | Citations: 15]"
        },
        {
            "title": "Harnessing LLMs for multi-dimensional writing assessment: Reliability and alignment with human judgments",
            "venue": "Heliyon",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1016/j.heliyon.2024.e34262",
                "status": "GOLD",
                "license": "CCBYNC",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11305227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118488377",
                    "name": "Xiaoyi Tang"
                },
                {
                    "authorId": "2272635431",
                    "name": "Hongwei Chen"
                },
                {
                    "authorId": "2302524052",
                    "name": "Daoyu Lin"
                },
                {
                    "authorId": "2301924234",
                    "name": "Kexin Li"
                }
            ],
            "abstract": null,
            "corpus_id": 271126365,
            "sentences": [
                {
                    "corpus_id": "271126365",
                    "title": "Harnessing LLMs for multi-dimensional writing assessment: Reliability and alignment with human judgments",
                    "text": "To analyze the efficacy of LLM in scoring across various rating dimensions, we contrasted the performance of GPT-4 across different prompt strategies with human scoring benchmarks, yielding the results depicted in the subsequent Fig. 11. With more specific and sophisticated prompt strategies, the scoring accuracy of GPT-4 increased steadily, showing that prompt engineering is of vital importance for multi-dimensional writing assessment and instruction. \n\nAs shown in Table 4, for the rating dimension of Ideas, GPT-4 attained its highest QWK coefficient of 0.551 at the Criteria & Sample-Referenced Justification prompt, closely approximating the inter-rater reliability observed among human raters, which stands at 0.605. Pertaining to the rating dimension of Organization, GPT-4 demonstrated optimal scoring effectiveness with a QWK of 0.584 under the same prompt, exceeding the inter-rater reliability of 0.541. This accentuates the model's capability effectively discern the structural components of essays when augmented with insights derived from human raters. \n\nAs for the rating dimension of Style, alignment with human evaluative judgments was most pronounced when applying the Criteria & Sample-Referenced Justification prompt, achieving a QWK score of 0.474, albeit not reaching the inter-rater reliability of 0.565. This indicates that while the model exhibits a capacity to recognize stylistic elements of text when informed by annotated exemplars from human raters, there is a discernible margin for enhancement. Assessing Conventions posed a considerable challenge for GPT-4, with the most specific and sophisticated prompt, Criteria & Sample-Referenced Justification, attaining a QWK of merely 0.216, which is substantially below the human inter-rater reliability of 0.498. \n\nThe observed disparities in GPT-4's scoring proficiency across different dimensions may be ascribed to a multitude of factors. Dimensions that necessitate a more profound comprehension of content and context, such as Ideas and Organization, are advantaged by prompts that incorporate criteria and exemplars crafted by human evaluators.",
                    "score": 0.39332605052069014,
                    "section_title": "Experiment 3: analysis of LLM effectiveness across scoring dimensions",
                    "char_start_offset": 38653,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 456
                        },
                        {
                            "start": 459,
                            "end": 726
                        },
                        {
                            "start": 727,
                            "end": 918
                        },
                        {
                            "start": 919,
                            "end": 1070
                        },
                        {
                            "start": 1073,
                            "end": 1331
                        },
                        {
                            "start": 1332,
                            "end": 1530
                        },
                        {
                            "start": 1531,
                            "end": 1793
                        },
                        {
                            "start": 1796,
                            "end": 1922
                        },
                        {
                            "start": 1923,
                            "end": 2131
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7041015625
                },
                {
                    "corpus_id": "271126365",
                    "title": "Harnessing LLMs for multi-dimensional writing assessment: Reliability and alignment with human judgments",
                    "text": "In essence, the matrices indicate that although the GPT-4 model can emulate human scoring to a certain degree, the extent of agreement fluctuates depending on the prompt engineering. The most pronounced levels of agreement are observed when scores are attributed across multiple dimensions (Dim. Score) and when criteria-based scoring is enacted. The integration of justifications and sample references does not substantially amplify the level of concordance. The inter-rater reliability matrix reveals that the agreement between GPT-4 and a human rater falls within a tolerable scope, acknowledging the intrinsic variability in human judgment. Additionally, it can be discerned that aside from the Criteria & Sample-Referenced Justification Prompt, the scores assigned by the GPT-4 model are consistently lower than those of Rater 1. \n\nThe experimental outcomes corroborate that the types of prompt engineering and the models employed markedly influence the dependability of LLMs in AES tasks. The evidence suggests that more elaborated and specific prompts, encompassing criteria-based scoring with justifications, facilitate LLMs to approximate human-level consistency more closely. GPT-4, as the most advanced model evaluated, exhibited a heightened concordance with human raters, especially when justifications were integrated into the scoring processes. In line with the previous studies related to prompting engineering in LLMs, the present study also offers evidence for the idea that prompt engineering is integral to optimizing the performance of LLMs [55,60]. \n\nTo summarize, exploring the potential of prompt engineering in LLMs can be fruitful for writing assessment and education. For example, Fig. 7 illustrates the multi-dimensional feedback that GPT-4 could offer when grading an essay based on the rating rubrics and Criteria & Sample-Referenced Justification Prompt utilized in the present study. With the help of LLMs, Students and learners can receive instant, diagnostic, and customized feedback on writing quality by integrating AES with LLMs. Besides, teachers can reduce the strain of correcting students' writing and mitigating evaluation bias while focusing on other essential parts of writing instruction regarding the organization, coherence, and logical flow of writing processes [3]. \n\nWe also explored a deeper analysis of the variability and consistency observed across different LLMs.",
                    "score": 0.40302863774101194,
                    "section_title": "Experiment 1: prompt engineering",
                    "char_start_offset": 29331,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 295
                        },
                        {
                            "start": 296,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 644
                        },
                        {
                            "start": 645,
                            "end": 834
                        },
                        {
                            "start": 837,
                            "end": 994
                        },
                        {
                            "start": 995,
                            "end": 1185
                        },
                        {
                            "start": 1186,
                            "end": 1359
                        },
                        {
                            "start": 1360,
                            "end": 1570
                        },
                        {
                            "start": 1573,
                            "end": 1694
                        },
                        {
                            "start": 1695,
                            "end": 1915
                        },
                        {
                            "start": 1916,
                            "end": 2066
                        },
                        {
                            "start": 2067,
                            "end": 2314
                        },
                        {
                            "start": 2317,
                            "end": 2418
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1566,
                            "end": 1569,
                            "matchedPaperCorpusId": "261441498"
                        },
                        {
                            "start": 2310,
                            "end": 2313,
                            "matchedPaperCorpusId": "257277710"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6943359375
                }
            ],
            "relevance_judgement": 0.7041015625,
            "relevance_judgment_input_expanded": "# Title: Harnessing LLMs for multi-dimensional writing assessment: Reliability and alignment with human judgments\n# Venue: Heliyon\n# Authors: Xiaoyi Tang, Hongwei Chen, Daoyu Lin, Kexin Li\n## Abstract\nNone\n## Experiment 1: prompt engineering\nIn essence, the matrices indicate that although the GPT-4 model can emulate human scoring to a certain degree, the extent of agreement fluctuates depending on the prompt engineering. The most pronounced levels of agreement are observed when scores are attributed across multiple dimensions (Dim. Score) and when criteria-based scoring is enacted. The integration of justifications and sample references does not substantially amplify the level of concordance. The inter-rater reliability matrix reveals that the agreement between GPT-4 and a human rater falls within a tolerable scope, acknowledging the intrinsic variability in human judgment. Additionally, it can be discerned that aside from the Criteria & Sample-Referenced Justification Prompt, the scores assigned by the GPT-4 model are consistently lower than those of Rater 1. \n\nThe experimental outcomes corroborate that the types of prompt engineering and the models employed markedly influence the dependability of LLMs in AES tasks. The evidence suggests that more elaborated and specific prompts, encompassing criteria-based scoring with justifications, facilitate LLMs to approximate human-level consistency more closely. GPT-4, as the most advanced model evaluated, exhibited a heightened concordance with human raters, especially when justifications were integrated into the scoring processes. In line with the previous studies related to prompting engineering in LLMs, the present study also offers evidence for the idea that prompt engineering is integral to optimizing the performance of LLMs [55,60]. \n\nTo summarize, exploring the potential of prompt engineering in LLMs can be fruitful for writing assessment and education. For example, Fig. 7 illustrates the multi-dimensional feedback that GPT-4 could offer when grading an essay based on the rating rubrics and Criteria & Sample-Referenced Justification Prompt utilized in the present study. With the help of LLMs, Students and learners can receive instant, diagnostic, and customized feedback on writing quality by integrating AES with LLMs. Besides, teachers can reduce the strain of correcting students' writing and mitigating evaluation bias while focusing on other essential parts of writing instruction regarding the organization, coherence, and logical flow of writing processes [3]. \n\nWe also explored a deeper analysis of the variability and consistency observed across different LLMs.\n\n## Experiment 3: analysis of LLM effectiveness across scoring dimensions\nTo analyze the efficacy of LLM in scoring across various rating dimensions, we contrasted the performance of GPT-4 across different prompt strategies with human scoring benchmarks, yielding the results depicted in the subsequent Fig. 11. With more specific and sophisticated prompt strategies, the scoring accuracy of GPT-4 increased steadily, showing that prompt engineering is of vital importance for multi-dimensional writing assessment and instruction. \n\nAs shown in Table 4, for the rating dimension of Ideas, GPT-4 attained its highest QWK coefficient of 0.551 at the Criteria & Sample-Referenced Justification prompt, closely approximating the inter-rater reliability observed among human raters, which stands at 0.605. Pertaining to the rating dimension of Organization, GPT-4 demonstrated optimal scoring effectiveness with a QWK of 0.584 under the same prompt, exceeding the inter-rater reliability of 0.541. This accentuates the model's capability effectively discern the structural components of essays when augmented with insights derived from human raters. \n\nAs for the rating dimension of Style, alignment with human evaluative judgments was most pronounced when applying the Criteria & Sample-Referenced Justification prompt, achieving a QWK score of 0.474, albeit not reaching the inter-rater reliability of 0.565. This indicates that while the model exhibits a capacity to recognize stylistic elements of text when informed by annotated exemplars from human raters, there is a discernible margin for enhancement. Assessing Conventions posed a considerable challenge for GPT-4, with the most specific and sophisticated prompt, Criteria & Sample-Referenced Justification, attaining a QWK of merely 0.216, which is substantially below the human inter-rater reliability of 0.498. \n\nThe observed disparities in GPT-4's scoring proficiency across different dimensions may be ascribed to a multitude of factors. Dimensions that necessitate a more profound comprehension of content and context, such as Ideas and Organization, are advantaged by prompts that incorporate criteria and exemplars crafted by human evaluators.",
            "reference_string": "[271126365 | Tang et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Distinguishing GPT-4-generated Radiology Abstracts from Original Abstracts: Performance of Blinded Human Observers and AI Content Detector",
            "venue": "medRxiv",
            "year": 2023,
            "reference_count": 5,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.medrxiv.org/content/medrxiv/early/2023/05/03/2023.04.28.23289283.full.pdf",
                "status": "GREEN",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2023.04.28.23289283?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2023.04.28.23289283, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "4955051",
                    "name": "F. Ufuk"
                },
                {
                    "authorId": "2163486794",
                    "name": "H. Peker"
                },
                {
                    "authorId": "4282049",
                    "name": "E. Sa\u011fta\u015f"
                },
                {
                    "authorId": "36553672",
                    "name": "A. Ya\u011fc\u0131"
                }
            ],
            "abstract": "Objective: To determine GPT-4's effectiveness in writing scientific radiology article abstracts and investigate human reviewers' and AI Content detectors' success in distinguishing these abstracts. Additionally, to determine the similarity scores of abstracts generated by GPT-4 to better understand its ability to create unique text. Methods: The study collected 250 original articles published between 2021 and 2023 in five radiology journals. The articles were randomly selected, and their abstracts were generated by GPT-4 using a specific prompt. Three experienced academic radiologists independently evaluated the GPT-4 generated and original abstracts to distinguish them as original or generated by GPT-4. All abstracts were also uploaded to an AI Content Detector and plagiarism detector to calculate similarity scores. Statistical analysis was performed to determine discrimination performance and similarity scores. Results: Out of 134 GPT-4 generated abstracts, average of 75 (56%) were detected by reviewers, and average of 50 (43%) original abstracts were falsely categorized as GPT-4 generated abstracts by reviewers. The sensitivity, specificity, accuracy, PPV, and NPV of observers in distinguishing GPT-4 written abstracts ranged from 51.5% to 55.6%, 56.1% to 70%, 54.8% to 60.8%, 41.2% to 76.7%, and 47% to 62.7%, respectively. No significant difference was observed between observers in discrimination performance. Conclusion: GPT-4 can generate convincing scientific radiology article abstracts. However, human reviewers and AI Content detectors have difficulty in distinguishing GPT-4 generated abstracts from original ones.",
            "corpus_id": 258460621,
            "sentences": [
                {
                    "corpus_id": "258460621",
                    "title": "Distinguishing GPT-4-generated Radiology Abstracts from Original Abstracts: Performance of Blinded Human Observers and AI Content Detector",
                    "text": "Objective: To determine GPT-4's effectiveness in writing scientific radiology article abstracts and investigate human reviewers' and AI Content detectors' success in distinguishing these abstracts. Additionally, to determine the similarity scores of abstracts generated by GPT-4 to better understand its ability to create unique text. Methods: The study collected 250 original articles published between 2021 and 2023 in five radiology journals. The articles were randomly selected, and their abstracts were generated by GPT-4 using a specific prompt. Three experienced academic radiologists independently evaluated the GPT-4 generated and original abstracts to distinguish them as original or generated by GPT-4. All abstracts were also uploaded to an AI Content Detector and plagiarism detector to calculate similarity scores. Statistical analysis was performed to determine discrimination performance and similarity scores. Results: Out of 134 GPT-4 generated abstracts, average of 75 (56%) were detected by reviewers, and average of 50 (43%) original abstracts were falsely categorized as GPT-4 generated abstracts by reviewers. The sensitivity, specificity, accuracy, PPV, and NPV of observers in distinguishing GPT-4 written abstracts ranged from 51.5% to 55.6%, 56.1% to 70%, 54.8% to 60.8%, 41.2% to 76.7%, and 47% to 62.7%, respectively. No significant difference was observed between observers in discrimination performance. Conclusion: GPT-4 can generate convincing scientific radiology article abstracts. However, human reviewers and AI Content detectors have difficulty in distinguishing GPT-4 generated abstracts from original ones.",
                    "score": 0.39647233178800956,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70361328125
                }
            ],
            "relevance_judgement": 0.70361328125,
            "relevance_judgment_input_expanded": "# Title: Distinguishing GPT-4-generated Radiology Abstracts from Original Abstracts: Performance of Blinded Human Observers and AI Content Detector\n# Venue: medRxiv\n# Authors: F. Ufuk, H. Peker, E. Sa\u011fta\u015f, A. Ya\u011fc\u0131\n## Abstract\nObjective: To determine GPT-4's effectiveness in writing scientific radiology article abstracts and investigate human reviewers' and AI Content detectors' success in distinguishing these abstracts. Additionally, to determine the similarity scores of abstracts generated by GPT-4 to better understand its ability to create unique text. Methods: The study collected 250 original articles published between 2021 and 2023 in five radiology journals. The articles were randomly selected, and their abstracts were generated by GPT-4 using a specific prompt. Three experienced academic radiologists independently evaluated the GPT-4 generated and original abstracts to distinguish them as original or generated by GPT-4. All abstracts were also uploaded to an AI Content Detector and plagiarism detector to calculate similarity scores. Statistical analysis was performed to determine discrimination performance and similarity scores. Results: Out of 134 GPT-4 generated abstracts, average of 75 (56%) were detected by reviewers, and average of 50 (43%) original abstracts were falsely categorized as GPT-4 generated abstracts by reviewers. The sensitivity, specificity, accuracy, PPV, and NPV of observers in distinguishing GPT-4 written abstracts ranged from 51.5% to 55.6%, 56.1% to 70%, 54.8% to 60.8%, 41.2% to 76.7%, and 47% to 62.7%, respectively. No significant difference was observed between observers in discrimination performance. Conclusion: GPT-4 can generate convincing scientific radiology article abstracts. However, human reviewers and AI Content detectors have difficulty in distinguishing GPT-4 generated abstracts from original ones.\n",
            "reference_string": "[258460621 | Ufuk et al. | 2023 | Citations: 3]"
        },
        {
            "title": "PRE: A Peer Review Based Large Language Model Evaluator",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 21,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.15641, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1381470019",
                    "name": "Zhumin Chu"
                },
                {
                    "authorId": "2256982003",
                    "name": "Qingyao Ai"
                },
                {
                    "authorId": "2275628230",
                    "name": "Yiteng Tu"
                },
                {
                    "authorId": "2108590438",
                    "name": "Haitao Li"
                },
                {
                    "authorId": "2260835922",
                    "name": "Yiqun Liu"
                }
            ],
            "abstract": "The impressive performance of large language models (LLMs) has attracted considerable attention from the academic and industrial communities. Besides how to construct and train LLMs, how to effectively evaluate and compare the capacity of LLMs has also been well recognized as an important yet difficult problem. Existing paradigms rely on either human annotators or model-based evaluators to evaluate the performance of LLMs on different tasks. However, these paradigms often suffer from high cost, low generalizability, and inherited biases in practice, which make them incapable of supporting the sustainable development of LLMs in long term. In order to address these issues, inspired by the peer review systems widely used in academic publication process, we propose a novel framework that can automatically evaluate LLMs through a peer-review process. Specifically, for the evaluation of a specific task, we first construct a small qualification exam to select\"reviewers\"from a couple of powerful LLMs. Then, to actually evaluate the\"submissions\"written by different candidate LLMs, i.e., the evaluatees, we use the reviewer LLMs to rate or compare the submissions. The final ranking of evaluatee LLMs is generated based on the results provided by all reviewers. We conducted extensive experiments on text summarization tasks with eleven LLMs including GPT-4. The results demonstrate the existence of biasness when evaluating using a single LLM. Also, our PRE model outperforms all the baselines, illustrating the effectiveness of the peer review mechanism.",
            "corpus_id": 267311508,
            "sentences": [
                {
                    "corpus_id": "267311508",
                    "title": "PRE: A Peer Review Based Large Language Model Evaluator",
                    "text": "Recently, many studies have adopted the state-of-the-art LLM, GPT-4, as their evaluation tools [19,26]. Although several works have demonstrated that GPT-4 has decent evaluation capabilities [19,26], we found that GPT-4, so as other LLMs, often prefers responses of LLMs from its own series (i.e., the GPT models) over other LLMs despite of the actual quality of the responses. In other words, if we use GPT-4 as the evaluator, its inherent bias may make it difficult, if possible, to develop an LLM outside the GPT family that outperforms GPT-4. \n\nTo address the aforementioned issues, we propose a novel framework, Peer Review Evaluator (PRE) 1 , to evaluate the performance of LLMs automatically. Inspired by the peer review mechanism in academic community, we propose to use LLMs as reviewers to evaluate the performance of LLMs directly. Specifically, we first develop a qualification exam to filter out LLMs that fail to provide reliable evaluation results. Then, qualified reviewer LLMs are required to assess the outputs of the evaluatee LLMs, and the final evaluation results are aggregated from all reviewer LLMs' ratings or preferences. To verified the effectiveness of our framework, we conducted extensive experiments on two representative text generation tasks, i.e., document summarization and non-factoid question answering. The experimental results show that the results of PRE model have the highest consistency with human preferences (ground truth) compared to all the baseline models including GPT-4. Comparing to previous evaluation methods, PRE can easily be generalized to different tasks and is highly cost efficient. Also, experiment results show that PRE provides much more robust evaluation results than methods that rely on specific model structures or LLMs. \n\nIn summary, our contributions can be summarized as follows: \n\n\u2022 We propose a novel and automatic LLM evaluation framework PRE that incorporates peer review mechanisms. \u2022 Through the use of qualification exams and result fusions, PRE can achieve effective LLM evaluation while being robust to potential model bias, which has been widely observed in existing automatic evaluation methods. \u2022 We conducted extensive experiments with both the document summarization and non-factoid QA task to demonstrate the potential of PRE.",
                    "score": 0.5270457653983353,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 3722,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 103
                        },
                        {
                            "start": 104,
                            "end": 377
                        },
                        {
                            "start": 378,
                            "end": 546
                        },
                        {
                            "start": 549,
                            "end": 699
                        },
                        {
                            "start": 700,
                            "end": 842
                        },
                        {
                            "start": 843,
                            "end": 963
                        },
                        {
                            "start": 964,
                            "end": 1147
                        },
                        {
                            "start": 1148,
                            "end": 1340
                        },
                        {
                            "start": 1341,
                            "end": 1520
                        },
                        {
                            "start": 1521,
                            "end": 1641
                        },
                        {
                            "start": 1642,
                            "end": 1786
                        },
                        {
                            "start": 1789,
                            "end": 1848
                        },
                        {
                            "start": 1851,
                            "end": 1956
                        },
                        {
                            "start": 1957,
                            "end": 2175
                        },
                        {
                            "start": 2176,
                            "end": 2310
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68798828125
                }
            ],
            "relevance_judgement": 0.68798828125,
            "relevance_judgment_input_expanded": "# Title: PRE: A Peer Review Based Large Language Model Evaluator\n# Venue: arXiv.org\n# Authors: Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, Yiqun Liu\n## Abstract\nThe impressive performance of large language models (LLMs) has attracted considerable attention from the academic and industrial communities. Besides how to construct and train LLMs, how to effectively evaluate and compare the capacity of LLMs has also been well recognized as an important yet difficult problem. Existing paradigms rely on either human annotators or model-based evaluators to evaluate the performance of LLMs on different tasks. However, these paradigms often suffer from high cost, low generalizability, and inherited biases in practice, which make them incapable of supporting the sustainable development of LLMs in long term. In order to address these issues, inspired by the peer review systems widely used in academic publication process, we propose a novel framework that can automatically evaluate LLMs through a peer-review process. Specifically, for the evaluation of a specific task, we first construct a small qualification exam to select\"reviewers\"from a couple of powerful LLMs. Then, to actually evaluate the\"submissions\"written by different candidate LLMs, i.e., the evaluatees, we use the reviewer LLMs to rate or compare the submissions. The final ranking of evaluatee LLMs is generated based on the results provided by all reviewers. We conducted extensive experiments on text summarization tasks with eleven LLMs including GPT-4. The results demonstrate the existence of biasness when evaluating using a single LLM. Also, our PRE model outperforms all the baselines, illustrating the effectiveness of the peer review mechanism.\n## INTRODUCTION\nRecently, many studies have adopted the state-of-the-art LLM, GPT-4, as their evaluation tools [19,26]. Although several works have demonstrated that GPT-4 has decent evaluation capabilities [19,26], we found that GPT-4, so as other LLMs, often prefers responses of LLMs from its own series (i.e., the GPT models) over other LLMs despite of the actual quality of the responses. In other words, if we use GPT-4 as the evaluator, its inherent bias may make it difficult, if possible, to develop an LLM outside the GPT family that outperforms GPT-4. \n\nTo address the aforementioned issues, we propose a novel framework, Peer Review Evaluator (PRE) 1 , to evaluate the performance of LLMs automatically. Inspired by the peer review mechanism in academic community, we propose to use LLMs as reviewers to evaluate the performance of LLMs directly. Specifically, we first develop a qualification exam to filter out LLMs that fail to provide reliable evaluation results. Then, qualified reviewer LLMs are required to assess the outputs of the evaluatee LLMs, and the final evaluation results are aggregated from all reviewer LLMs' ratings or preferences. To verified the effectiveness of our framework, we conducted extensive experiments on two representative text generation tasks, i.e., document summarization and non-factoid question answering. The experimental results show that the results of PRE model have the highest consistency with human preferences (ground truth) compared to all the baseline models including GPT-4. Comparing to previous evaluation methods, PRE can easily be generalized to different tasks and is highly cost efficient. Also, experiment results show that PRE provides much more robust evaluation results than methods that rely on specific model structures or LLMs. \n\nIn summary, our contributions can be summarized as follows: \n\n\u2022 We propose a novel and automatic LLM evaluation framework PRE that incorporates peer review mechanisms. \u2022 Through the use of qualification exams and result fusions, PRE can achieve effective LLM evaluation while being robust to potential model bias, which has been widely observed in existing automatic evaluation methods. \u2022 We conducted extensive experiments with both the document summarization and non-factoid QA task to demonstrate the potential of PRE.",
            "reference_string": "[267311508 | Chu et al. | 2024 | Citations: 21]"
        },
        {
            "title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.07108, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115773025",
                    "name": "Yongqiang Ma"
                },
                {
                    "authorId": "2296088772",
                    "name": "Lizhi Qin"
                },
                {
                    "authorId": "48210950",
                    "name": "Jiawei Liu"
                },
                {
                    "authorId": "153818687",
                    "name": "Yangyang Kang"
                },
                {
                    "authorId": "2296706290",
                    "name": "Yue Zhang"
                },
                {
                    "authorId": "2260835264",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "2238387483",
                    "name": "Xiaozhong Liu"
                },
                {
                    "authorId": "2283732",
                    "name": "Qikai Cheng"
                }
            ],
            "abstract": "Evaluating large language models (LLMs) is fundamental, particularly in the context of practical applications. Conventional evaluation methods, typically designed primarily for LLM development, yield numerical scores that ignore the user experience. Therefore, our study shifts the focus from model-centered to human-centered evaluation in the context of AI-powered writing assistance applications. Our proposed metric, termed ``Revision Distance,'' utilizes LLMs to suggest revision edits that mimic the human writing process. It is determined by counting the revision edits generated by LLMs. Benefiting from the generated revision edit details, our metric can provide a self-explained text evaluation result in a human-understandable manner beyond the context-independent score. Our results show that for the easy-writing task, ``Revision Distance'' is consistent with established metrics (ROUGE, Bert-score, and GPT-score), but offers more insightful, detailed feedback and better distinguishes between texts. Moreover, in the context of challenging academic writing tasks, our metric still delivers reliable evaluations where other metrics tend to struggle. Furthermore, our metric also holds significant potential for scenarios lacking reference texts.",
            "corpus_id": 269032822,
            "sentences": [
                {
                    "corpus_id": "269032822",
                    "title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
                    "text": "This paper introduces a metric that leverages GPT-4, specifically applied to evaluating LLM-powered writing assistants.However, the computational and financial costs of using GPT-4 are considerable.Exploring the use of a smaller, specialized model to generate initial edits could reduce costs and improve efficiency.\n\nLLMs have a wide array of applications, and for this study, we have chosen the \"Related Work\" section generation task as a testbed for challenging writing scenarios.As a knowledge-intensive cognitive task, writing the \"Related Work\" section requires writers to integrate multi-source knowledge into the manuscript.Therefore, writing a comprehensive \"Related Work\" section is a laborintensive and time-consuming endeavor.Future studies could explore the application of our metric in longer text generation tasks, such as code generation and scientific reports, to validate its effectiveness and applicability across different domains.\n\nIn this study, each generated revision item is assigned equal weight.Future research should focus on developing a dynamic revision edit weighting method to evaluate textual differences more finely.",
                    "score": 0.3978186067801772,
                    "section_title": "Limitations",
                    "char_start_offset": 11713,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 119,
                            "end": 198
                        },
                        {
                            "start": 198,
                            "end": 316
                        },
                        {
                            "start": 318,
                            "end": 483
                        },
                        {
                            "start": 483,
                            "end": 632
                        },
                        {
                            "start": 632,
                            "end": 738
                        },
                        {
                            "start": 738,
                            "end": 951
                        },
                        {
                            "start": 953,
                            "end": 1022
                        },
                        {
                            "start": 1022,
                            "end": 1150
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68603515625
                }
            ],
            "relevance_judgement": 0.68603515625,
            "relevance_judgment_input_expanded": "# Title: From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Yongqiang Ma, Lizhi Qin, Jiawei Liu, Yangyang Kang, Yue Zhang, Wei Lu, Xiaozhong Liu, Qikai Cheng\n## Abstract\nEvaluating large language models (LLMs) is fundamental, particularly in the context of practical applications. Conventional evaluation methods, typically designed primarily for LLM development, yield numerical scores that ignore the user experience. Therefore, our study shifts the focus from model-centered to human-centered evaluation in the context of AI-powered writing assistance applications. Our proposed metric, termed ``Revision Distance,'' utilizes LLMs to suggest revision edits that mimic the human writing process. It is determined by counting the revision edits generated by LLMs. Benefiting from the generated revision edit details, our metric can provide a self-explained text evaluation result in a human-understandable manner beyond the context-independent score. Our results show that for the easy-writing task, ``Revision Distance'' is consistent with established metrics (ROUGE, Bert-score, and GPT-score), but offers more insightful, detailed feedback and better distinguishes between texts. Moreover, in the context of challenging academic writing tasks, our metric still delivers reliable evaluations where other metrics tend to struggle. Furthermore, our metric also holds significant potential for scenarios lacking reference texts.\n## Limitations\nThis paper introduces a metric that leverages GPT-4, specifically applied to evaluating LLM-powered writing assistants.However, the computational and financial costs of using GPT-4 are considerable.Exploring the use of a smaller, specialized model to generate initial edits could reduce costs and improve efficiency.\n\nLLMs have a wide array of applications, and for this study, we have chosen the \"Related Work\" section generation task as a testbed for challenging writing scenarios.As a knowledge-intensive cognitive task, writing the \"Related Work\" section requires writers to integrate multi-source knowledge into the manuscript.Therefore, writing a comprehensive \"Related Work\" section is a laborintensive and time-consuming endeavor.Future studies could explore the application of our metric in longer text generation tasks, such as code generation and scientific reports, to validate its effectiveness and applicability across different domains.\n\nIn this study, each generated revision item is assigned equal weight.Future research should focus on developing a dynamic revision edit weighting method to evaluate textual differences more finely.",
            "reference_string": "[269032822 | Ma et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Rating Short L2 Essays on the CEFR Scale with GPT-4",
            "venue": "Workshop on Innovative Use of NLP for Building Educational Applications",
            "year": 2023,
            "reference_count": 38,
            "citation_count": 65,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.bea-1.49.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.bea-1.49, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "41136452",
                    "name": "K. Yancey"
                },
                {
                    "authorId": "2279237573",
                    "name": "Geoffrey T. LaFlair"
                },
                {
                    "authorId": "2221319070",
                    "name": "Anthony Verardi"
                },
                {
                    "authorId": "2526345",
                    "name": "J. Burstein"
                }
            ],
            "abstract": "Essay scoring is a critical task used to evaluate second-language (L2) writing proficiency on high-stakes language assessments. While automated scoring approaches are mature and have been around for decades, human scoring is still considered the gold standard, despite its high costs and well-known issues such as human rater fatigue and bias. The recent introduction of large language models (LLMs) brings new opportunities for automated scoring. In this paper, we evaluate how well GPT-3.5 and GPT-4 can rate short essay responses written by L2 English learners on a high-stakes language assessment, computing inter-rater agreement with human ratings. Results show that when calibration examples are provided, GPT-4 can perform almost as well as modern Automatic Writing Evaluation (AWE) methods, but agreement with human ratings can vary depending on the test-taker\u2019s first language (L1).",
            "corpus_id": 259376840,
            "sentences": [
                {
                    "corpus_id": "259376840",
                    "title": "Rating Short L2 Essays on the CEFR Scale with GPT-4",
                    "text": "Automated writing evaluation (AWE) systems are commonly used to evaluate test-taker writing. AWE systems are deployed on large-scale, highstakes writing assessments used for admissions to higher education institutions, and for lower-stakes US state writing assessments that provide information about K-12 students' academic writing performance. These systems typically use featureengineering approaches that include rule-based and statistical natural language processing (NLP) methods. NLP is used to extract features from essay writing responses that are characteristic of writing quality. Features may include errors in grammar and spelling, discourse structure, discourse coherence, vocabulary usage, and sentence variety. Features may be rule-based or statistically derived. Statistical model methods, such as straightforward linear regression, are used to train (build) AWE scoring models for high-stakes scoring of writing assessments. Detailed descriptions of systems are avail-able for major systems, including e-rater\u00ae, Intelligent Essay Assessor\u2122, Intellimetric\u00ae, and PEG (Shermis and Burstein, 2013), and Cambium's automated essay scoring system (Lottridge, in press). \n\nRecent advances in language modeling with neural transformer architectures (OpenAI, 2023;Brown et al., 2020) have the potential to revolutionize AWE. These large language models (LLMs) demonstrate an incredible potential to analyze and evaluate text which has implications for the future of AWE. In addition, GPT's intuitive, text-based interface lowers barriers for use, potentially increasing accessibility and adoption of these tools for AWE. The assumptions about how LLMs -specifically GPT-4 -can be used for AWE tasks, such as automated scoring and feedback need to be evaluated to determine how we can use them beneficially, and particularly to ensure that they can be used in a fair and ethical manner (Burstein, 2023). \n\nPrevious research evaluated GPT-3.5 for essay scoring tasks in an L2 context (Mizumoto and Eguchi, 2023). In this paper, we evaluate GPT-4 for a similar task, comparing it to GPT-3.5, human judgement, and a strong baseline using current AWE methods. We also explore various aspects that affect the accuracy of GPT's ratings, and its fairness across gender and L1.",
                    "score": 0.37174235344900947,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 92
                        },
                        {
                            "start": 93,
                            "end": 344
                        },
                        {
                            "start": 345,
                            "end": 485
                        },
                        {
                            "start": 486,
                            "end": 590
                        },
                        {
                            "start": 591,
                            "end": 725
                        },
                        {
                            "start": 726,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 941
                        },
                        {
                            "start": 942,
                            "end": 1179
                        },
                        {
                            "start": 1182,
                            "end": 1331
                        },
                        {
                            "start": 1332,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1627
                        },
                        {
                            "start": 1628,
                            "end": 1909
                        },
                        {
                            "start": 1912,
                            "end": 2017
                        },
                        {
                            "start": 2018,
                            "end": 2161
                        },
                        {
                            "start": 2162,
                            "end": 2275
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1271,
                            "end": 1290,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68212890625
                },
                {
                    "corpus_id": "259376840",
                    "title": "Rating Short L2 Essays on the CEFR Scale with GPT-4",
                    "text": "We showed that unlike GPT-3.5, GPT-4 is able to attain performance similar to conventional Automated Writing Evaluation (AWE) models when rating short L2 essays. GPT-4 only required one calibration example per rating category to achieve near optimal performance, but other prompt engineering techniques we tried were not very helpful. Furthermore, when assessing fairness with respect to the test-taker's gender or L1, we found that while GPT-4 did not show bias in favor of any one group, it showed significantly less agreement with human ratings for some L1s. It is unclear whether this is due to the reliability of GPT-4 or that of the human ratings themselves. More research is needed to understand this discrepancy and its implications for fairness. Future research may also explore other prompt engineering strategies for improving GPT-4's performance at this task, or potentially finetuning GPT-3.5, enabling one to leverage dramatically more training data than what can be provided in a prompt. Perhaps most excitingly, future work may explore GPT-4's potential for providing feedback aligned to essay scoring: a task for which GPT-4 seems particularly well suited.",
                    "score": 0.4024631443003993,
                    "section_title": "Conclusion",
                    "char_start_offset": 13183,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 334
                        },
                        {
                            "start": 335,
                            "end": 561
                        },
                        {
                            "start": 562,
                            "end": 664
                        },
                        {
                            "start": 665,
                            "end": 754
                        },
                        {
                            "start": 755,
                            "end": 1002
                        },
                        {
                            "start": 1003,
                            "end": 1173
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6357421875
                }
            ],
            "relevance_judgement": 0.68212890625,
            "relevance_judgment_input_expanded": "# Title: Rating Short L2 Essays on the CEFR Scale with GPT-4\n# Venue: Workshop on Innovative Use of NLP for Building Educational Applications\n# Authors: K. Yancey, Geoffrey T. LaFlair, Anthony Verardi, J. Burstein\n## Abstract\nEssay scoring is a critical task used to evaluate second-language (L2) writing proficiency on high-stakes language assessments. While automated scoring approaches are mature and have been around for decades, human scoring is still considered the gold standard, despite its high costs and well-known issues such as human rater fatigue and bias. The recent introduction of large language models (LLMs) brings new opportunities for automated scoring. In this paper, we evaluate how well GPT-3.5 and GPT-4 can rate short essay responses written by L2 English learners on a high-stakes language assessment, computing inter-rater agreement with human ratings. Results show that when calibration examples are provided, GPT-4 can perform almost as well as modern Automatic Writing Evaluation (AWE) methods, but agreement with human ratings can vary depending on the test-taker\u2019s first language (L1).\n## Introduction\nAutomated writing evaluation (AWE) systems are commonly used to evaluate test-taker writing. AWE systems are deployed on large-scale, highstakes writing assessments used for admissions to higher education institutions, and for lower-stakes US state writing assessments that provide information about K-12 students' academic writing performance. These systems typically use featureengineering approaches that include rule-based and statistical natural language processing (NLP) methods. NLP is used to extract features from essay writing responses that are characteristic of writing quality. Features may include errors in grammar and spelling, discourse structure, discourse coherence, vocabulary usage, and sentence variety. Features may be rule-based or statistically derived. Statistical model methods, such as straightforward linear regression, are used to train (build) AWE scoring models for high-stakes scoring of writing assessments. Detailed descriptions of systems are avail-able for major systems, including e-rater\u00ae, Intelligent Essay Assessor\u2122, Intellimetric\u00ae, and PEG (Shermis and Burstein, 2013), and Cambium's automated essay scoring system (Lottridge, in press). \n\nRecent advances in language modeling with neural transformer architectures (OpenAI, 2023;Brown et al., 2020) have the potential to revolutionize AWE. These large language models (LLMs) demonstrate an incredible potential to analyze and evaluate text which has implications for the future of AWE. In addition, GPT's intuitive, text-based interface lowers barriers for use, potentially increasing accessibility and adoption of these tools for AWE. The assumptions about how LLMs -specifically GPT-4 -can be used for AWE tasks, such as automated scoring and feedback need to be evaluated to determine how we can use them beneficially, and particularly to ensure that they can be used in a fair and ethical manner (Burstein, 2023). \n\nPrevious research evaluated GPT-3.5 for essay scoring tasks in an L2 context (Mizumoto and Eguchi, 2023). In this paper, we evaluate GPT-4 for a similar task, comparing it to GPT-3.5, human judgement, and a strong baseline using current AWE methods. We also explore various aspects that affect the accuracy of GPT's ratings, and its fairness across gender and L1.\n\n## Conclusion\nWe showed that unlike GPT-3.5, GPT-4 is able to attain performance similar to conventional Automated Writing Evaluation (AWE) models when rating short L2 essays. GPT-4 only required one calibration example per rating category to achieve near optimal performance, but other prompt engineering techniques we tried were not very helpful. Furthermore, when assessing fairness with respect to the test-taker's gender or L1, we found that while GPT-4 did not show bias in favor of any one group, it showed significantly less agreement with human ratings for some L1s. It is unclear whether this is due to the reliability of GPT-4 or that of the human ratings themselves. More research is needed to understand this discrepancy and its implications for fairness. Future research may also explore other prompt engineering strategies for improving GPT-4's performance at this task, or potentially finetuning GPT-3.5, enabling one to leverage dramatically more training data than what can be provided in a prompt. Perhaps most excitingly, future work may explore GPT-4's potential for providing feedback aligned to essay scoring: a task for which GPT-4 seems particularly well suited.",
            "reference_string": "[259376840 | Yancey et al. | 2023 | Citations: 65]"
        },
        {
            "title": "Evaluating AI and human authorship quality in academic writing through physics essays",
            "venue": "European journal of physics",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1088/1361-6404/ad669d",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.05458, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2290487695",
                    "name": "Will Yeadon"
                },
                {
                    "authorId": "2290487629",
                    "name": "Elise Agra"
                },
                {
                    "authorId": "2197779028",
                    "name": "O. Inyang"
                },
                {
                    "authorId": "2290484460",
                    "name": "Paul Mackay"
                },
                {
                    "authorId": "9799803",
                    "name": "Arin Mizouri"
                }
            ],
            "abstract": "This study aims to compare the academic writing quality and detectability of authorship between human and AI-generated texts by evaluating n = 300 short-form physics essay submissions, equally divided between student work submitted before the introduction of ChatGPT and those generated by OpenAI\u2019s GPT-4. In blinded evaluations conducted by five independent markers who were unaware of the origin of the essays, we observed no statistically significant differences in scores between essays authored by humans and those produced by AI (p-value = 0.107, \u03b1 = 0.05). Additionally, when the markers subsequently attempted to identify the authorship of the essays on a 4-point Likert scale\u2014from \u2018Definitely AI\u2019 to \u2018Definitely Human\u2019\u2014their performance was only marginally better than random chance. This outcome not only underscores the convergence of AI and human authorship quality but also highlights the difficulty of discerning AI-generated content solely through human judgment. Furthermore, the effectiveness of five commercially available software tools for identifying essay authorship was evaluated. Among these, ZeroGPT was the most accurate, achieving a 98% accuracy rate and a precision score of 1.0 when its classifications were reduced to binary outcomes. This result is a source of potential optimism for maintaining assessment integrity. Finally, we propose that texts with \u226450% AI-generated content should be considered the upper limit for classification as human-authored, a boundary inclusive of a future with ubiquitous AI assistance whilst also respecting human-authorship.",
            "corpus_id": 268296847,
            "sentences": [
                {
                    "corpus_id": "268296847",
                    "title": "Evaluating AI and human authorship quality in academic writing through physics essays",
                    "text": "There are many powerful LLMs available, and benchmarking them is a comprehensive field of study aimed at evaluating the capabilities of these models across a broad spectrum of cognitive and linguistic tasks [6]. As of this writing, OpenAI's GPT-4 consistently ranks at or near the top of most LLM benchmark leaderboards, scoring highly on prominent benchmarks such as the Massive Multitask Language Understanding (MMLU), according to its technical report [7]. \n\nThe MMLU benchmark, known for its extensive range of subjects from humanities to hard sciences, provides a rigorous test of a model's ability to understand and generate responses across diverse knowledge areas. Similarly, the SuperGLUE benchmark [8], which focuses on tasks that require a deep understanding of language, such as question answering, inference, and reasoning, challenges models to demonstrate advanced levels of comprehension and the ability to handle nuanced linguistic constructs. GPT-4's performance, independently supported by scoring over 90% on SuperGLUE [9], underscores its capabilities. \n\nGiven this, our study uses GPT-4 as a representation of current LLM essay writing abilities. While in principle other LLMs could perform better, GPT-4's proficiency ensures that our test is a valid measure of the state of the art in AI-driven essay composition.",
                    "score": 0.4078932588254143,
                    "section_title": "State of the Art",
                    "char_start_offset": 2311,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 459
                        },
                        {
                            "start": 462,
                            "end": 672
                        },
                        {
                            "start": 673,
                            "end": 959
                        },
                        {
                            "start": 960,
                            "end": 1072
                        },
                        {
                            "start": 1075,
                            "end": 1167
                        },
                        {
                            "start": 1168,
                            "end": 1336
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 708,
                            "end": 711,
                            "matchedPaperCorpusId": "143424870"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68115234375
                }
            ],
            "relevance_judgement": 0.68115234375,
            "relevance_judgment_input_expanded": "# Title: Evaluating AI and human authorship quality in academic writing through physics essays\n# Venue: European journal of physics\n# Authors: Will Yeadon, Elise Agra, O. Inyang, Paul Mackay, Arin Mizouri\n## Abstract\nThis study aims to compare the academic writing quality and detectability of authorship between human and AI-generated texts by evaluating n = 300 short-form physics essay submissions, equally divided between student work submitted before the introduction of ChatGPT and those generated by OpenAI\u2019s GPT-4. In blinded evaluations conducted by five independent markers who were unaware of the origin of the essays, we observed no statistically significant differences in scores between essays authored by humans and those produced by AI (p-value = 0.107, \u03b1 = 0.05). Additionally, when the markers subsequently attempted to identify the authorship of the essays on a 4-point Likert scale\u2014from \u2018Definitely AI\u2019 to \u2018Definitely Human\u2019\u2014their performance was only marginally better than random chance. This outcome not only underscores the convergence of AI and human authorship quality but also highlights the difficulty of discerning AI-generated content solely through human judgment. Furthermore, the effectiveness of five commercially available software tools for identifying essay authorship was evaluated. Among these, ZeroGPT was the most accurate, achieving a 98% accuracy rate and a precision score of 1.0 when its classifications were reduced to binary outcomes. This result is a source of potential optimism for maintaining assessment integrity. Finally, we propose that texts with \u226450% AI-generated content should be considered the upper limit for classification as human-authored, a boundary inclusive of a future with ubiquitous AI assistance whilst also respecting human-authorship.\n## State of the Art\nThere are many powerful LLMs available, and benchmarking them is a comprehensive field of study aimed at evaluating the capabilities of these models across a broad spectrum of cognitive and linguistic tasks [6]. As of this writing, OpenAI's GPT-4 consistently ranks at or near the top of most LLM benchmark leaderboards, scoring highly on prominent benchmarks such as the Massive Multitask Language Understanding (MMLU), according to its technical report [7]. \n\nThe MMLU benchmark, known for its extensive range of subjects from humanities to hard sciences, provides a rigorous test of a model's ability to understand and generate responses across diverse knowledge areas. Similarly, the SuperGLUE benchmark [8], which focuses on tasks that require a deep understanding of language, such as question answering, inference, and reasoning, challenges models to demonstrate advanced levels of comprehension and the ability to handle nuanced linguistic constructs. GPT-4's performance, independently supported by scoring over 90% on SuperGLUE [9], underscores its capabilities. \n\nGiven this, our study uses GPT-4 as a representation of current LLM essay writing abilities. While in principle other LLMs could perform better, GPT-4's proficiency ensures that our test is a valid measure of the state of the art in AI-driven essay composition.",
            "reference_string": "[268296847 | Yeadon et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Eliciting Informative Text Evaluations with Large Language Models",
            "venue": "ACM Conference on Economics and Computation",
            "year": 2024,
            "reference_count": 64,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2405.15077",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.15077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2140025107",
                    "name": "Yuxuan Lu"
                },
                {
                    "authorId": "2110923845",
                    "name": "Shengwei Xu"
                },
                {
                    "authorId": "2120311160",
                    "name": "Yichi Zhang"
                },
                {
                    "authorId": "2273919664",
                    "name": "Yuqing Kong"
                },
                {
                    "authorId": "2268317075",
                    "name": "Grant Schoenebeck"
                }
            ],
            "abstract": "Peer prediction mechanisms motivate high-quality feedback with provable guarantees. However, current methods only apply to rather simple reports, like multiple-choice or scalar numbers. We aim to broaden these techniques to the larger domain of text-based reports, drawing on the recent developments in large language models (LLMs). This vastly increases the applicability of peer prediction mechanisms as textual feedback is the norm in a large variety of feedback channels: peer reviews, e-commerce customer reviews, and comments on social media. We introduce two mechanisms, the Generative Peer Prediction Mechanism (GPPM) and the Generative Synopsis Peer Prediction Mechanism (GSPPM). These mechanisms utilize LLMs as predictors, mapping from one agent's report to a prediction of her peer's report. Theoretically, we show that when the LLM prediction is sufficiently accurate, our mechanisms can incentivize high effort and truth-telling as an (approximate) Bayesian Nash equilibrium. Empirically, we confirm the efficacy of our mechanisms through experiments conducted on two real datasets: the Yelp review dataset and the ICLR OpenReview dataset. We highlight the results that on the ICLR dataset, our mechanisms can differentiate three quality levels --- human-written reviews, GPT-4-generated reviews, and GPT-3.5-generated reviews in terms of expected scores. Additionally, GSPPM penalizes LLM-generated reviews more effectively than GPPM.",
            "corpus_id": 270045144,
            "sentences": [
                {
                    "corpus_id": "270045144",
                    "title": "Eliciting Informative Text Evaluations with Large Language Models",
                    "text": "Furthermore, we employ the LLMs to create synthetic text reports based on a given item, simulating the scenario of the creation of fictitious academic peer reviews. We conduct the experiment on the ICLR2020 OpenReview dataset4 . Specifically, we provide the paper  as input to both GPT-3.5 and GPT-4 [Achiam et al., 2023, Brown et al., 2020], requesting them to generate comprehensive reviews following Liang et al. [2023]'s method. The prompt for this task is presented in Appendix D.4. \n\nWe compare three types of reviews: the human-written review, the GPT-4-generated review, and the GPT-3.5-generated review. Given that GPT-4 is commonly considered a stronger AI than GPT-3.5, and both models are considered worse than human expert reviewers, we hypothesize that we can utilize the LLM-generated reviews to simulate low-quality human reviews, thereby simulating three levels of quality. \n\nIt is worth noting that, unlike the degradations discussed in section 6.1, the LLM-generated review does not only depend on the agent's signal   since the reviewed paper is input into the LLMs to create reviews. It can not be regarded as untruthfully reporting the original signal. Thus, our focus remains on assessing the effectiveness of differentiating various quality levels of reports across different mechanisms.",
                    "score": 0.36965041909499896,
                    "section_title": "LLM-generated Reviews.",
                    "char_start_offset": 48675,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 164
                        },
                        {
                            "start": 165,
                            "end": 228
                        },
                        {
                            "start": 229,
                            "end": 432
                        },
                        {
                            "start": 433,
                            "end": 487
                        },
                        {
                            "start": 490,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 612
                        },
                        {
                            "start": 613,
                            "end": 890
                        },
                        {
                            "start": 893,
                            "end": 1104
                        },
                        {
                            "start": 1105,
                            "end": 1174
                        },
                        {
                            "start": 1175,
                            "end": 1311
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 320,
                            "end": 341,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.677734375
                }
            ],
            "relevance_judgement": 0.677734375,
            "relevance_judgment_input_expanded": "# Title: Eliciting Informative Text Evaluations with Large Language Models\n# Venue: ACM Conference on Economics and Computation\n# Authors: Yuxuan Lu, Shengwei Xu, Yichi Zhang, Yuqing Kong, Grant Schoenebeck\n## Abstract\nPeer prediction mechanisms motivate high-quality feedback with provable guarantees. However, current methods only apply to rather simple reports, like multiple-choice or scalar numbers. We aim to broaden these techniques to the larger domain of text-based reports, drawing on the recent developments in large language models (LLMs). This vastly increases the applicability of peer prediction mechanisms as textual feedback is the norm in a large variety of feedback channels: peer reviews, e-commerce customer reviews, and comments on social media. We introduce two mechanisms, the Generative Peer Prediction Mechanism (GPPM) and the Generative Synopsis Peer Prediction Mechanism (GSPPM). These mechanisms utilize LLMs as predictors, mapping from one agent's report to a prediction of her peer's report. Theoretically, we show that when the LLM prediction is sufficiently accurate, our mechanisms can incentivize high effort and truth-telling as an (approximate) Bayesian Nash equilibrium. Empirically, we confirm the efficacy of our mechanisms through experiments conducted on two real datasets: the Yelp review dataset and the ICLR OpenReview dataset. We highlight the results that on the ICLR dataset, our mechanisms can differentiate three quality levels --- human-written reviews, GPT-4-generated reviews, and GPT-3.5-generated reviews in terms of expected scores. Additionally, GSPPM penalizes LLM-generated reviews more effectively than GPPM.\n## LLM-generated Reviews.\nFurthermore, we employ the LLMs to create synthetic text reports based on a given item, simulating the scenario of the creation of fictitious academic peer reviews. We conduct the experiment on the ICLR2020 OpenReview dataset4 . Specifically, we provide the paper  as input to both GPT-3.5 and GPT-4 [Achiam et al., 2023, Brown et al., 2020], requesting them to generate comprehensive reviews following Liang et al. [2023]'s method. The prompt for this task is presented in Appendix D.4. \n\nWe compare three types of reviews: the human-written review, the GPT-4-generated review, and the GPT-3.5-generated review. Given that GPT-4 is commonly considered a stronger AI than GPT-3.5, and both models are considered worse than human expert reviewers, we hypothesize that we can utilize the LLM-generated reviews to simulate low-quality human reviews, thereby simulating three levels of quality. \n\nIt is worth noting that, unlike the degradations discussed in section 6.1, the LLM-generated review does not only depend on the agent's signal   since the reviewed paper is input into the LLMs to create reviews. It can not be regarded as untruthfully reporting the original signal. Thus, our focus remains on assessing the effectiveness of differentiating various quality levels of reports across different mechanisms.",
            "reference_string": "[270045144 | Lu et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Is GPT-4 Alone Sufficient for Automated Essay Scoring?: A Comparative Judgment Approach Based on Rater Cognition",
            "venue": "ACM Conference on Learning @ Scale",
            "year": 2024,
            "reference_count": 29,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.05733, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2310489304",
                    "name": "Seungju Kim"
                },
                {
                    "authorId": "2310341618",
                    "name": "Meounggun Jo"
                }
            ],
            "abstract": "Large Language Models (LLMs) have shown promise in Automated Essay Scoring (AES), but their zero-shot and few-shot performance often falls short compared to state-of-the-art models and human raters. However, fine-tuning LLMs for each specific task is impractical due to the variety of essay prompts and rubrics used in real-world educational contexts. This study proposes a novel approach combining LLMs and Comparative Judgment (CJ) for AES, using zero-shot prompting to choose between two essays. We demonstrate that a CJ method surpasses traditional rubric-based scoring in essay scoring using LLMs.",
            "corpus_id": 271050803,
            "sentences": [
                {
                    "corpus_id": "271050803",
                    "title": "Is GPT-4 Alone Sufficient for Automated Essay Scoring?: A Comparative Judgment Approach Based on Rater Cognition",
                    "text": "This study contributes to the growing body of research on the application of Large Language Models (LLMs) in Automated Essay Scoring (AES) by investigating the effectiveness of combining LLMs with Comparative Judgment (CJ).The findings demonstrate that the CJ-based scoring strategy, particularly when combined with elaborated rubrics and fine-grained scores using GPT-4, is more effective than the traditional rubric-based scoring in enabling LLMs to imitate human rater scores.This study shows that while GPT-4 is a powerful tool for AES, it is not sufficient on its own, as many factors influence both human raters and LLMs in essay scoring.\n\nThe results have significant implications not just for the advancement and utilization of LLMs in AES, but also for several research domains that entail generating multi-trait scoring data with a hierarchy.The insight gained from this study can guide the development of automated scoring systems in various fields, emphasizing the significance of taking into account elements such as scoring criteria, scoring methods, and the specific language model used.This work highlights the significance of interdisciplinary collaboration among specialists in the areas of natural language processing, educational assessment, and cognitive psychology to further enhance the progress and implementation of LLMs in intricate educational problems.",
                    "score": 0.36952221322102197,
                    "section_title": "Conclusion",
                    "char_start_offset": 26909,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 223
                        },
                        {
                            "start": 223,
                            "end": 479
                        },
                        {
                            "start": 479,
                            "end": 644
                        },
                        {
                            "start": 646,
                            "end": 852
                        },
                        {
                            "start": 852,
                            "end": 1102
                        },
                        {
                            "start": 1102,
                            "end": 1380
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6640625
                }
            ],
            "relevance_judgement": 0.6640625,
            "relevance_judgment_input_expanded": "# Title: Is GPT-4 Alone Sufficient for Automated Essay Scoring?: A Comparative Judgment Approach Based on Rater Cognition\n# Venue: ACM Conference on Learning @ Scale\n# Authors: Seungju Kim, Meounggun Jo\n## Abstract\nLarge Language Models (LLMs) have shown promise in Automated Essay Scoring (AES), but their zero-shot and few-shot performance often falls short compared to state-of-the-art models and human raters. However, fine-tuning LLMs for each specific task is impractical due to the variety of essay prompts and rubrics used in real-world educational contexts. This study proposes a novel approach combining LLMs and Comparative Judgment (CJ) for AES, using zero-shot prompting to choose between two essays. We demonstrate that a CJ method surpasses traditional rubric-based scoring in essay scoring using LLMs.\n## Conclusion\nThis study contributes to the growing body of research on the application of Large Language Models (LLMs) in Automated Essay Scoring (AES) by investigating the effectiveness of combining LLMs with Comparative Judgment (CJ).The findings demonstrate that the CJ-based scoring strategy, particularly when combined with elaborated rubrics and fine-grained scores using GPT-4, is more effective than the traditional rubric-based scoring in enabling LLMs to imitate human rater scores.This study shows that while GPT-4 is a powerful tool for AES, it is not sufficient on its own, as many factors influence both human raters and LLMs in essay scoring.\n\nThe results have significant implications not just for the advancement and utilization of LLMs in AES, but also for several research domains that entail generating multi-trait scoring data with a hierarchy.The insight gained from this study can guide the development of automated scoring systems in various fields, emphasizing the significance of taking into account elements such as scoring criteria, scoring methods, and the specific language model used.This work highlights the significance of interdisciplinary collaboration among specialists in the areas of natural language processing, educational assessment, and cognitive psychology to further enhance the progress and implementation of LLMs in intricate educational problems.",
            "reference_string": "[271050803 | Kim et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 29,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03019, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309300543",
                    "name": "Sungduk Yu"
                },
                {
                    "authorId": "2309218122",
                    "name": "Man Luo"
                },
                {
                    "authorId": "115098946",
                    "name": "Avinash Madasu"
                },
                {
                    "authorId": "2309247409",
                    "name": "Vasudev Lal"
                },
                {
                    "authorId": "2253672147",
                    "name": "Phillip Howard"
                }
            ],
            "abstract": "Peer review is a critical process for ensuring the integrity of published scientific research. Confidence in this process is predicated on the assumption that experts in the relevant domain give careful consideration to the merits of manuscripts which are submitted for publication. With the recent rapid advancements in the linguistic capabilities of large language models (LLMs), a new potential risk to the peer review process is that negligent reviewers will rely on LLMs to perform the often time consuming process of reviewing a paper. In this study, we investigate the ability of existing AI text detection algorithms to distinguish between peer reviews written by humans and different state-of-the-art LLMs. Our analysis shows that existing approaches fail to identify many GPT-4o written reviews without also producing a high number of false positive classifications. To address this deficiency, we propose a new detection approach which surpasses existing methods in the identification of GPT-4o written peer reviews at low levels of false positive classifications. Our work reveals the difficulty of accurately identifying AI-generated text at the individual review level, highlighting the urgent need for new tools and methods to detect this type of unethical application of generative AI.",
            "corpus_id": 273162522,
            "sentences": [
                {
                    "corpus_id": "273162522",
                    "title": "Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review",
                    "text": "There has been substantial research examining the implications of using generative AI in conference peer review processes [1,[27][28][29][30][31][32]. Liang et al. [1] introduced a maximum likelihood method to assess the influence of large language models (LLMs) on reviewing practices at the corpus level. They also conducted a large-scale empirical study [27] to evaluate the effectiveness of LLM-generated feedback compared to human reviews, revealing insights into its utility and limitations in peer review. In another effort, Tan et al. [28] developed a comprehensive dataset that simulates the peer review process as a multi-turn dialogue, incorporating the roles of reviewers, authors, and meta-reviewers to explore the dynamics of review interactions. Zhou et al. [29] focused on evaluating the reviewing capabilities of LLMs such as GPT-3.5 and GPT-4o through 196 multiple-choice questions related to reviewing tasks, providing insights into their strengths and weaknesses in academic review settings. To enhance the quality of reviews, Tyser et al. [30] proposed the OpenReviewer tool, which allows authors or reviewers to submit papers and receive automatic reviews, enabling iterative improvements before actual submission or peer review. Kuznetsov et al. [31] explored the core challenges in the peer review process and argued for a more integrated, transparent use of LLMs in reviewing to overcome these limitations. Additionally, Mosca et al. [32] introduced a benchmark designed to distinguish between human-written and machine-generated scientific papers, helping to address concerns about the integrity of AI-generated content in academic settings. In contrast, our work focuses on the utility of AI text detection methods in identifying LLM generated content in the peer review process.",
                    "score": 0.37596071513112495,
                    "section_title": "AI-assisted peer review.",
                    "char_start_offset": 14808,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 306
                        },
                        {
                            "start": 307,
                            "end": 512
                        },
                        {
                            "start": 513,
                            "end": 760
                        },
                        {
                            "start": 761,
                            "end": 1011
                        },
                        {
                            "start": 1012,
                            "end": 1251
                        },
                        {
                            "start": 1252,
                            "end": 1431
                        },
                        {
                            "start": 1432,
                            "end": 1667
                        },
                        {
                            "start": 1668,
                            "end": 1806
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 125,
                            "end": 129,
                            "matchedPaperCorpusId": "263608784"
                        },
                        {
                            "start": 133,
                            "end": 137,
                            "matchedPaperCorpusId": "269803977"
                        },
                        {
                            "start": 145,
                            "end": 149,
                            "matchedPaperCorpusId": "260063149"
                        },
                        {
                            "start": 357,
                            "end": 361,
                            "matchedPaperCorpusId": "263608784"
                        },
                        {
                            "start": 773,
                            "end": 777,
                            "matchedPaperCorpusId": "269803977"
                        },
                        {
                            "start": 1459,
                            "end": 1463,
                            "matchedPaperCorpusId": "260063149"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66015625
                }
            ],
            "relevance_judgement": 0.66015625,
            "relevance_judgment_input_expanded": "# Title: Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review\n# Venue: arXiv.org\n# Authors: Sungduk Yu, Man Luo, Avinash Madasu, Vasudev Lal, Phillip Howard\n## Abstract\nPeer review is a critical process for ensuring the integrity of published scientific research. Confidence in this process is predicated on the assumption that experts in the relevant domain give careful consideration to the merits of manuscripts which are submitted for publication. With the recent rapid advancements in the linguistic capabilities of large language models (LLMs), a new potential risk to the peer review process is that negligent reviewers will rely on LLMs to perform the often time consuming process of reviewing a paper. In this study, we investigate the ability of existing AI text detection algorithms to distinguish between peer reviews written by humans and different state-of-the-art LLMs. Our analysis shows that existing approaches fail to identify many GPT-4o written reviews without also producing a high number of false positive classifications. To address this deficiency, we propose a new detection approach which surpasses existing methods in the identification of GPT-4o written peer reviews at low levels of false positive classifications. Our work reveals the difficulty of accurately identifying AI-generated text at the individual review level, highlighting the urgent need for new tools and methods to detect this type of unethical application of generative AI.\n## AI-assisted peer review.\nThere has been substantial research examining the implications of using generative AI in conference peer review processes [1,[27][28][29][30][31][32]. Liang et al. [1] introduced a maximum likelihood method to assess the influence of large language models (LLMs) on reviewing practices at the corpus level. They also conducted a large-scale empirical study [27] to evaluate the effectiveness of LLM-generated feedback compared to human reviews, revealing insights into its utility and limitations in peer review. In another effort, Tan et al. [28] developed a comprehensive dataset that simulates the peer review process as a multi-turn dialogue, incorporating the roles of reviewers, authors, and meta-reviewers to explore the dynamics of review interactions. Zhou et al. [29] focused on evaluating the reviewing capabilities of LLMs such as GPT-3.5 and GPT-4o through 196 multiple-choice questions related to reviewing tasks, providing insights into their strengths and weaknesses in academic review settings. To enhance the quality of reviews, Tyser et al. [30] proposed the OpenReviewer tool, which allows authors or reviewers to submit papers and receive automatic reviews, enabling iterative improvements before actual submission or peer review. Kuznetsov et al. [31] explored the core challenges in the peer review process and argued for a more integrated, transparent use of LLMs in reviewing to overcome these limitations. Additionally, Mosca et al. [32] introduced a benchmark designed to distinguish between human-written and machine-generated scientific papers, helping to address concerns about the integrity of AI-generated content in academic settings. In contrast, our work focuses on the utility of AI text detection methods in identifying LLM generated content in the peer review process.",
            "reference_string": "[273162522 | Yu et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Exploring Scientific Hypothesis Generation with Mamba",
            "venue": "NLP4SCIENCE",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 5,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.nlp4science-1.17, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2329735903",
                    "name": "Miaosen Chai"
                },
                {
                    "authorId": "2329737480",
                    "name": "Emily Herron"
                },
                {
                    "authorId": "2329736173",
                    "name": "Erick Cervantes"
                },
                {
                    "authorId": "2329737470",
                    "name": "Tirthankar Ghosal"
                }
            ],
            "abstract": "Generating scientifically grounded hypotheses is a challenging frontier task for generative AI models in science. The difficulty arises from the inherent subjectivity of the task and the extensive knowledge of prior work required to assess the validity of a generated hypothesis. Large Language Models (LLMs), trained on vast datasets from diverse sources, have shown a strong ability to utilize the knowledge embedded in their training data. Recent research has explored using transformer-based models for scientific hypothesis generation, leveraging their advanced capabilities. However, these models often require a significant number of parameters to manage Long sequences, which can be a limitation. State Space Models, such as Mamba, offer an alternative by effectively handling very Long sequences with fewer parameters than transformers. In this work, we investigate the use of Mamba for scientific hypothesis generation. Our preliminary findings indicate that Mamba achieves similar performance w.r.t. transformer-based models of similar sizes for a higher-order complex task like hypothesis generation. We have made our code available here: https://github.com/fglx-c/Exploring-Scientific-Hypothesis-Generation-with-Mamba",
            "corpus_id": 273901331,
            "sentences": [
                {
                    "corpus_id": "273901331",
                    "title": "Exploring Scientific Hypothesis Generation with Mamba",
                    "text": "To address the limitations of automatic evaluation metrics, we incorporate an LLM evaluation to assess the quality of the generated scientific hypotheses. Specifically, we employ Claude-3.5 instead of the more mainstream GPT-4 to mitigate potential self-enhancement bias, which occurs when a model is evaluated using its own framework (Xu et al., 2024b). We utilize a reference-guided style prompt, as introduced by previous research (Zheng et al., 2023), which has been shown to increase the agreement between human and machine evaluations by providing a reference solution. \n\nWe design a structured prompt that directs Claude-3.5 to judge the effectiveness of a hypothesis. Specifically, Claude-3.5 is asked to assess the relevance of the hypothesis to the background context and seed term, ensuring sufficient novelty without overly mirroring the provided information, verifying the scientific soundness, and evaluating the clarity and coherence of the language. See the detailed prompt(C). Based on these criteria, Claude-3.5 assigns a rating of either \"ineffective\" or \"effective\" to each generated hypothesis, along with a brief justification for its decision, formatted in JSON for consistent analysis. To assist the LLM in making more accurate assessments, we provide it with five examples of both \"ineffective\" and \"effective\" hypotheses.",
                    "score": 0.3684660510461264,
                    "section_title": "LLM As a Judge",
                    "char_start_offset": 12395,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 154
                        },
                        {
                            "start": 155,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 575
                        },
                        {
                            "start": 578,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 965
                        },
                        {
                            "start": 966,
                            "end": 993
                        },
                        {
                            "start": 994,
                            "end": 1209
                        },
                        {
                            "start": 1210,
                            "end": 1347
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6591796875
                }
            ],
            "relevance_judgement": 0.6591796875,
            "relevance_judgment_input_expanded": "# Title: Exploring Scientific Hypothesis Generation with Mamba\n# Venue: NLP4SCIENCE\n# Authors: Miaosen Chai, Emily Herron, Erick Cervantes, Tirthankar Ghosal\n## Abstract\nGenerating scientifically grounded hypotheses is a challenging frontier task for generative AI models in science. The difficulty arises from the inherent subjectivity of the task and the extensive knowledge of prior work required to assess the validity of a generated hypothesis. Large Language Models (LLMs), trained on vast datasets from diverse sources, have shown a strong ability to utilize the knowledge embedded in their training data. Recent research has explored using transformer-based models for scientific hypothesis generation, leveraging their advanced capabilities. However, these models often require a significant number of parameters to manage Long sequences, which can be a limitation. State Space Models, such as Mamba, offer an alternative by effectively handling very Long sequences with fewer parameters than transformers. In this work, we investigate the use of Mamba for scientific hypothesis generation. Our preliminary findings indicate that Mamba achieves similar performance w.r.t. transformer-based models of similar sizes for a higher-order complex task like hypothesis generation. We have made our code available here: https://github.com/fglx-c/Exploring-Scientific-Hypothesis-Generation-with-Mamba\n## LLM As a Judge\nTo address the limitations of automatic evaluation metrics, we incorporate an LLM evaluation to assess the quality of the generated scientific hypotheses. Specifically, we employ Claude-3.5 instead of the more mainstream GPT-4 to mitigate potential self-enhancement bias, which occurs when a model is evaluated using its own framework (Xu et al., 2024b). We utilize a reference-guided style prompt, as introduced by previous research (Zheng et al., 2023), which has been shown to increase the agreement between human and machine evaluations by providing a reference solution. \n\nWe design a structured prompt that directs Claude-3.5 to judge the effectiveness of a hypothesis. Specifically, Claude-3.5 is asked to assess the relevance of the hypothesis to the background context and seed term, ensuring sufficient novelty without overly mirroring the provided information, verifying the scientific soundness, and evaluating the clarity and coherence of the language. See the detailed prompt(C). Based on these criteria, Claude-3.5 assigns a rating of either \"ineffective\" or \"effective\" to each generated hypothesis, along with a brief justification for its decision, formatted in JSON for consistent analysis. To assist the LLM in making more accurate assessments, we provide it with five examples of both \"ineffective\" and \"effective\" hypotheses.",
            "reference_string": "[273901331 | Chai et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Check-Eval: A Checklist-based Approach for Evaluating Text Quality",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 8,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.14467, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257137831",
                    "name": "Jayr Pereira"
                },
                {
                    "authorId": "2256889299",
                    "name": "R.A. Lotufo"
                }
            ],
            "abstract": "Evaluating the quality of text generated by large language models (LLMs) remains a significant challenge. Traditional metrics often fail to align well with human judgments, particularly in tasks requiring creativity and nuance. In this paper, we propose \\textsc{Check-Eval}, a novel evaluation framework leveraging LLMs to assess the quality of generated text through a checklist-based approach. \\textsc{Check-Eval} can be employed as both a reference-free and reference-dependent evaluation method, providing a structured and interpretable assessment of text quality. The framework consists of two main stages: checklist generation and checklist evaluation. We validate \\textsc{Check-Eval} on two benchmark datasets: Portuguese Legal Semantic Textual Similarity and \\textsc{SummEval}. Our results demonstrate that \\textsc{Check-Eval} achieves higher correlations with human judgments compared to existing metrics, such as \\textsc{G-Eval} and \\textsc{GPTScore}, underscoring its potential as a more reliable and effective evaluation framework for natural language generation tasks. The code for our experiments is available at \\url{https://anonymous.4open.science/r/check-eval-0DB4}",
            "corpus_id": 271310142,
            "sentences": [
                {
                    "corpus_id": "271310142",
                    "title": "Check-Eval: A Checklist-based Approach for Evaluating Text Quality",
                    "text": "The evaluation of automatically generated text has been a persistent challenge in the NLG field. Traditional metrics such as BLEU, ROUGE, and METEOR have been extensively used but have shown limitations in aligning with human judgment, particularly in tasks requiring creativity and nuance [Fabbri et al. 2021]. In recent years, more sophisticated evaluation frameworks leveraging LLMs have been proposed to address these shortcomings. Two recent methods are GPTSCORE [Fu et al. 2023] and G-EVAL [Liu et al. 2023]. \n\nGPTSCORE is a framework that utilizes generative pre-trained transformers (GPTs) and other language models to evaluate NLG outputs without relying on reference texts. The core idea is to assess the probability that the LLM assigns to the generated text, under the assumption that higher probabilities indicate higher quality. Fu et al. (2023) [Fu et al. 2023] demonstrated that GPTSCORE could achieve better correlations with human judgments compared to traditional metrics, especially in open-ended tasks such as dialogue generation and creative writing. However, despite its advancements, the method lacks interpretability and may be biased towards texts similar to those seen during the model's training phase. \n\nG-EVAL is another recent approach that leverages the capabilities of LLMs, specifically GPT-4, to improve NLG evaluation. Proposed by Liu et al. (2023) [Liu et al. 2023], G-EVAL introduces a chain-of-thought (CoT) [Wei et al. 2022] paradigm where the evaluation process is guided by detailed intermediate steps generated by the LLM. This method has shown improvements in correlation with human evaluations, particularly in tasks such as text summarization and dialogue generation. To address issues with score distribution and variability, G-EVAL employs a self-consistency strategy. Specifically, it generates multiple samples (n = 20) using different decoding parameters and averages the evaluation scores across these samples. This approach helps mitigate two key problems: the dominance of a single score (such as 3 on a 1-5 scale) and the tendency of LLMs to output integer scores, even when decimal values are requested.",
                    "score": 0.37823057560128404,
                    "section_title": "Related Work",
                    "char_start_offset": 3484,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 96
                        },
                        {
                            "start": 97,
                            "end": 311
                        },
                        {
                            "start": 312,
                            "end": 435
                        },
                        {
                            "start": 436,
                            "end": 514
                        },
                        {
                            "start": 517,
                            "end": 683
                        },
                        {
                            "start": 684,
                            "end": 842
                        },
                        {
                            "start": 843,
                            "end": 1072
                        },
                        {
                            "start": 1073,
                            "end": 1230
                        },
                        {
                            "start": 1233,
                            "end": 1354
                        },
                        {
                            "start": 1355,
                            "end": 1565
                        },
                        {
                            "start": 1566,
                            "end": 1713
                        },
                        {
                            "start": 1714,
                            "end": 1816
                        },
                        {
                            "start": 1817,
                            "end": 1962
                        },
                        {
                            "start": 1963,
                            "end": 2159
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 290,
                            "end": 310,
                            "matchedPaperCorpusId": "220768873"
                        },
                        {
                            "start": 468,
                            "end": 483,
                            "matchedPaperCorpusId": "964287"
                        },
                        {
                            "start": 843,
                            "end": 859,
                            "matchedPaperCorpusId": "964287"
                        },
                        {
                            "start": 860,
                            "end": 875,
                            "matchedPaperCorpusId": "964287"
                        },
                        {
                            "start": 1447,
                            "end": 1464,
                            "matchedPaperCorpusId": "246411621"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65576171875
                }
            ],
            "relevance_judgement": 0.65576171875,
            "relevance_judgment_input_expanded": "# Title: Check-Eval: A Checklist-based Approach for Evaluating Text Quality\n# Venue: arXiv.org\n# Authors: Jayr Pereira, R.A. Lotufo\n## Abstract\nEvaluating the quality of text generated by large language models (LLMs) remains a significant challenge. Traditional metrics often fail to align well with human judgments, particularly in tasks requiring creativity and nuance. In this paper, we propose \\textsc{Check-Eval}, a novel evaluation framework leveraging LLMs to assess the quality of generated text through a checklist-based approach. \\textsc{Check-Eval} can be employed as both a reference-free and reference-dependent evaluation method, providing a structured and interpretable assessment of text quality. The framework consists of two main stages: checklist generation and checklist evaluation. We validate \\textsc{Check-Eval} on two benchmark datasets: Portuguese Legal Semantic Textual Similarity and \\textsc{SummEval}. Our results demonstrate that \\textsc{Check-Eval} achieves higher correlations with human judgments compared to existing metrics, such as \\textsc{G-Eval} and \\textsc{GPTScore}, underscoring its potential as a more reliable and effective evaluation framework for natural language generation tasks. The code for our experiments is available at \\url{https://anonymous.4open.science/r/check-eval-0DB4}\n## Related Work\nThe evaluation of automatically generated text has been a persistent challenge in the NLG field. Traditional metrics such as BLEU, ROUGE, and METEOR have been extensively used but have shown limitations in aligning with human judgment, particularly in tasks requiring creativity and nuance [Fabbri et al. 2021]. In recent years, more sophisticated evaluation frameworks leveraging LLMs have been proposed to address these shortcomings. Two recent methods are GPTSCORE [Fu et al. 2023] and G-EVAL [Liu et al. 2023]. \n\nGPTSCORE is a framework that utilizes generative pre-trained transformers (GPTs) and other language models to evaluate NLG outputs without relying on reference texts. The core idea is to assess the probability that the LLM assigns to the generated text, under the assumption that higher probabilities indicate higher quality. Fu et al. (2023) [Fu et al. 2023] demonstrated that GPTSCORE could achieve better correlations with human judgments compared to traditional metrics, especially in open-ended tasks such as dialogue generation and creative writing. However, despite its advancements, the method lacks interpretability and may be biased towards texts similar to those seen during the model's training phase. \n\nG-EVAL is another recent approach that leverages the capabilities of LLMs, specifically GPT-4, to improve NLG evaluation. Proposed by Liu et al. (2023) [Liu et al. 2023], G-EVAL introduces a chain-of-thought (CoT) [Wei et al. 2022] paradigm where the evaluation process is guided by detailed intermediate steps generated by the LLM. This method has shown improvements in correlation with human evaluations, particularly in tasks such as text summarization and dialogue generation. To address issues with score distribution and variability, G-EVAL employs a self-consistency strategy. Specifically, it generates multiple samples (n = 20) using different decoding parameters and averages the evaluation scores across these samples. This approach helps mitigate two key problems: the dominance of a single score (such as 3 on a 1-5 scale) and the tendency of LLMs to output integer scores, even when decimal values are requested.",
            "reference_string": "[271310142 | Pereira et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Testing the utility of GPT for title and abstract screening in environmental systematic evidence synthesis",
            "venue": "Environmental Evidence",
            "year": 2025,
            "reference_count": 25,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1186/s13750-025-00360-x",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12016299, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308353601",
                    "name": "Bj\u00f6rn Nykvist"
                },
                {
                    "authorId": "2138147563",
                    "name": "Biljana Macura"
                },
                {
                    "authorId": "2276059937",
                    "name": "Maria Xylia"
                },
                {
                    "authorId": "2357031170",
                    "name": "Erik Olsson"
                }
            ],
            "abstract": "In this paper we show that OpenAI\u2019s Large Language Model (LLM) GPT perform remarkably well when used for title and abstract eligibility screening of scientific articles and within a (systematic) literature review workflow. We evaluated GPT on screening data from a systematic review study on electric vehicle charging infrastructure demand with almost 12,000 records using the same eligibility criteria as human screeners. We tested 3 different versions of this model that were tasked to distinguishing between relevant and irrelevant content by responding with a relevance probability between 0 and 1. For the latest GPT-4 model (tested in November 2023) and probability cutoff 0.5 the recall rate is 100%, meaning no relevant papers were missed and using this mode for screening would have saved 50% of the time that would otherwise be spent on manual screening. Experimenting with a higher cut of threshold can save more time. With threshold chosen so that recall is still above 95% for GPT-4 (where up to 5% of relevant papers might be missed), the model could save 75% of the time spent on manual screening. If automation technologies can replicate manual screening by human experts with effectiveness, accuracy, and precision, the work and cost savings are significant. Furthermore, the value of a comprehensive list of relevant literature, rather quickly available at the start of a research project, is hard to understate. However, as this study only evaluated the performance on one systematic review and one prompt, we caution that more test and methodological development is needed, and outline the next steps to properly evaluate rigor and effectiveness of LLMs for eligibility screening. Supplementary Information The online version contains supplementary material available at 10.1186/s13750-025-00360-x.",
            "corpus_id": 278028562,
            "sentences": [
                {
                    "corpus_id": "278028562",
                    "title": "Testing the utility of GPT for title and abstract screening in environmental systematic evidence synthesis",
                    "text": "In this paper we show that OpenAI\u2019s Large Language Model (LLM) GPT perform remarkably well when used for title and abstract eligibility screening of scientific articles and within a (systematic) literature review workflow. We evaluated GPT on screening data from a systematic review study on electric vehicle charging infrastructure demand with almost 12,000 records using the same eligibility criteria as human screeners. We tested 3 different versions of this model that were tasked to distinguishing between relevant and irrelevant content by responding with a relevance probability between 0 and 1. For the latest GPT-4 model (tested in November 2023) and probability cutoff 0.5 the recall rate is 100%, meaning no relevant papers were missed and using this mode for screening would have saved 50% of the time that would otherwise be spent on manual screening. Experimenting with a higher cut of threshold can save more time. With threshold chosen so that recall is still above 95% for GPT-4 (where up to 5% of relevant papers might be missed), the model could save 75% of the time spent on manual screening. If automation technologies can replicate manual screening by human experts with effectiveness, accuracy, and precision, the work and cost savings are significant. Furthermore, the value of a comprehensive list of relevant literature, rather quickly available at the start of a research project, is hard to understate. However, as this study only evaluated the performance on one systematic review and one prompt, we caution that more test and methodological development is needed, and outline the next steps to properly evaluate rigor and effectiveness of LLMs for eligibility screening. Supplementary Information The online version contains supplementary material available at 10.1186/s13750-025-00360-x.",
                    "score": 0.39026227421158965,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65185546875
                }
            ],
            "relevance_judgement": 0.65185546875,
            "relevance_judgment_input_expanded": "# Title: Testing the utility of GPT for title and abstract screening in environmental systematic evidence synthesis\n# Venue: Environmental Evidence\n# Authors: Bj\u00f6rn Nykvist, Biljana Macura, Maria Xylia, Erik Olsson\n## Abstract\nIn this paper we show that OpenAI\u2019s Large Language Model (LLM) GPT perform remarkably well when used for title and abstract eligibility screening of scientific articles and within a (systematic) literature review workflow. We evaluated GPT on screening data from a systematic review study on electric vehicle charging infrastructure demand with almost 12,000 records using the same eligibility criteria as human screeners. We tested 3 different versions of this model that were tasked to distinguishing between relevant and irrelevant content by responding with a relevance probability between 0 and 1. For the latest GPT-4 model (tested in November 2023) and probability cutoff 0.5 the recall rate is 100%, meaning no relevant papers were missed and using this mode for screening would have saved 50% of the time that would otherwise be spent on manual screening. Experimenting with a higher cut of threshold can save more time. With threshold chosen so that recall is still above 95% for GPT-4 (where up to 5% of relevant papers might be missed), the model could save 75% of the time spent on manual screening. If automation technologies can replicate manual screening by human experts with effectiveness, accuracy, and precision, the work and cost savings are significant. Furthermore, the value of a comprehensive list of relevant literature, rather quickly available at the start of a research project, is hard to understate. However, as this study only evaluated the performance on one systematic review and one prompt, we caution that more test and methodological development is needed, and outline the next steps to properly evaluate rigor and effectiveness of LLMs for eligibility screening. Supplementary Information The online version contains supplementary material available at 10.1186/s13750-025-00360-x.\n",
            "reference_string": "[278028562 | Nykvist et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Style Over Substance: Evaluation Biases for Large Language Models",
            "venue": "International Conference on Computational Linguistics",
            "year": 2023,
            "reference_count": 51,
            "citation_count": 47,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.03025",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.03025, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2145209409",
                    "name": "Minghao Wu"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                }
            ],
            "abstract": "As large language models (LLMs) continue to advance, accurately and comprehensively evaluating their performance becomes increasingly challenging. Ranking the relative performance of LLMs based on Elo ratings, according to human judgment, is gaining more popularity. However, the extent to which humans and LLMs are capable evaluators remains uncertain. This study investigates the behavior of crowd-sourced and expert annotators, as well as LLMs, when comparing outputs from different models. To achieve this, we curate a dataset of intentionally flawed machine-generated answers. Our findings reveal a concerning bias in the evaluation process, as answers with factual errors are rated more favorably than answers that are too short or contained grammatical errors. To address this issue, we propose independently evaluating machine-generated text across multiple dimensions, rather than merging all the evaluation aspects into a single score. We instantiate this idea with the Elo rating system, resulting in the Multi-Elo Rating System (MERS). Empirical results from our study reveal that this proposed approach significantly enhances the quality of LLM-based evaluations, particularly in terms of factual accuracy. However, there is no significant improvement in crowd-sourced-based evaluations, indicating the need for further investigation.",
            "corpus_id": 259360998,
            "sentences": [
                {
                    "corpus_id": "259360998",
                    "title": "Style Over Substance: Evaluation Biases for Large Language Models",
                    "text": "As human evaluation can be costly and inefficient, there is an increase in the use of advanced LLMs, such as GPT-4, to evaluate model outputs. In our work, we also use LLMs as judges to assess answer quality. However, previous studies rely solely on GPT-4 as the LLM judge (Chiang et al., 2023;Li et al., 2023a;Zheng et al., 2023), which may not be appropriate for our work as our answers are refined by humans after being generated by GPT-4. This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4.5 By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study. \n\nWe utilize the evaluation prompt from Dettmers et al. ( 2023), as presented in Figure 1. The prompt assesses the answers based on their helpfulness, relevance, accuracy, and level of detail, while also aiming to avoid bias related to answer ordering.",
                    "score": 0.444141000168608,
                    "section_title": "LLM Evaluation",
                    "char_start_offset": 13307,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 208
                        },
                        {
                            "start": 209,
                            "end": 442
                        },
                        {
                            "start": 443,
                            "end": 572
                        },
                        {
                            "start": 573,
                            "end": 658
                        },
                        {
                            "start": 659,
                            "end": 797
                        },
                        {
                            "start": 800,
                            "end": 888
                        },
                        {
                            "start": 889,
                            "end": 1050
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64013671875
                }
            ],
            "relevance_judgement": 0.64013671875,
            "relevance_judgment_input_expanded": "# Title: Style Over Substance: Evaluation Biases for Large Language Models\n# Venue: International Conference on Computational Linguistics\n# Authors: Minghao Wu, Alham Fikri Aji\n## Abstract\nAs large language models (LLMs) continue to advance, accurately and comprehensively evaluating their performance becomes increasingly challenging. Ranking the relative performance of LLMs based on Elo ratings, according to human judgment, is gaining more popularity. However, the extent to which humans and LLMs are capable evaluators remains uncertain. This study investigates the behavior of crowd-sourced and expert annotators, as well as LLMs, when comparing outputs from different models. To achieve this, we curate a dataset of intentionally flawed machine-generated answers. Our findings reveal a concerning bias in the evaluation process, as answers with factual errors are rated more favorably than answers that are too short or contained grammatical errors. To address this issue, we propose independently evaluating machine-generated text across multiple dimensions, rather than merging all the evaluation aspects into a single score. We instantiate this idea with the Elo rating system, resulting in the Multi-Elo Rating System (MERS). Empirical results from our study reveal that this proposed approach significantly enhances the quality of LLM-based evaluations, particularly in terms of factual accuracy. However, there is no significant improvement in crowd-sourced-based evaluations, indicating the need for further investigation.\n## LLM Evaluation\nAs human evaluation can be costly and inefficient, there is an increase in the use of advanced LLMs, such as GPT-4, to evaluate model outputs. In our work, we also use LLMs as judges to assess answer quality. However, previous studies rely solely on GPT-4 as the LLM judge (Chiang et al., 2023;Li et al., 2023a;Zheng et al., 2023), which may not be appropriate for our work as our answers are refined by humans after being generated by GPT-4. This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4.5 By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study. \n\nWe utilize the evaluation prompt from Dettmers et al. ( 2023), as presented in Figure 1. The prompt assesses the answers based on their helpfulness, relevance, accuracy, and level of detail, while also aiming to avoid bias related to answer ordering.",
            "reference_string": "[259360998 | Wu et al. | 2023 | Citations: 47]"
        },
        {
            "title": "SC4ANM: Identifying optimal section combinations for automated novelty prediction in academic papers",
            "venue": "Expert systems with applications",
            "year": 2025,
            "reference_count": 61,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.16330, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2307769002",
                    "name": "Wenqing Wu"
                },
                {
                    "authorId": "2267229283",
                    "name": "Chengzhi Zhang"
                },
                {
                    "authorId": "2325373803",
                    "name": "Tong Bao"
                },
                {
                    "authorId": "2284551410",
                    "name": "Yi Zhao"
                }
            ],
            "abstract": null,
            "corpus_id": 276296422,
            "sentences": [
                {
                    "corpus_id": "276296422",
                    "title": "SC4ANM: Identifying optimal section combinations for automated novelty prediction in academic papers",
                    "text": "With the rapid advancement of artificial intelligence and natural language processing technologies, LLMs (Brown et al., 2020;Ouyang et al., 2022), particularly those supported by transformer-based architectures and pre-trained on massive datasets, have gradually come into the public eye. With the successive releases of ChatGPT and GPT-4 (OpenAI, 2024) by OpenAI, as well as Llama (Touvron et al., 2023a) and others (Meta, 2024;Touvron et al., 2023b;Chowdhery et al., 2023), these LLMs have demonstrated powerful language generation and understanding capabilities, attracting significant research interest from scholars in the academic community. (Patsakis et al., 2024;Shafee et al., 2024;Caruccio et al., 2024) Recently, research on using LLMs for peer review has been gaining increasing popularity. Liang et al. (Liang et al., 2023) conducted a large-scale empirical analysis to assess comments generated by GPT-4: they tag several metrics and investigated user satisfaction to measure comment quality. They found that while LLM-generated reviews were helpful, they might be non-generic and tended to focus on certain aspects of scientific feedback. Liu and Shah (Liu and Shah, 2023) validated the utility of LLMs across three tasks: identifying errors, verifying checklists, and choosing the \"better\" pa-per. They concluded that LLMs serve well as review assistants for specific reviewing tasks; however, they are not yet sufficient for conducting comprehensive evaluations of papers. Mike Thelwall (Thelwall, 2024) used GPT-4 to evaluate the quality of journal articles on a paper assessment dataset to test its effectiveness. The results indicated that GPT-4 appears to be insufficiently accurate for any formal or informal research quality assessment tasks. Zhou et al. (Zhou et al., 2024) conducted a comprehensive evaluation to determine whether LLMs can be qualified and reliable reviewers. They concluded that it is premature for LLMs to serve as automated scientific paper reviewers. Although there is potential for obtaining useful and accurate results, their current capabilities are not yet reliable enough.",
                    "score": 0.46559314104082405,
                    "section_title": "Large Language Models for Reviewing",
                    "char_start_offset": 16466,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 288
                        },
                        {
                            "start": 289,
                            "end": 647
                        },
                        {
                            "start": 648,
                            "end": 802
                        },
                        {
                            "start": 803,
                            "end": 1006
                        },
                        {
                            "start": 1007,
                            "end": 1153
                        },
                        {
                            "start": 1154,
                            "end": 1313
                        },
                        {
                            "start": 1314,
                            "end": 1489
                        },
                        {
                            "start": 1490,
                            "end": 1632
                        },
                        {
                            "start": 1633,
                            "end": 1765
                        },
                        {
                            "start": 1766,
                            "end": 1901
                        },
                        {
                            "start": 1902,
                            "end": 1996
                        },
                        {
                            "start": 1997,
                            "end": 2123
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 105,
                            "end": 125,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 125,
                            "end": 145,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 451,
                            "end": 474,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 648,
                            "end": 671,
                            "matchedPaperCorpusId": "269456918"
                        },
                        {
                            "start": 691,
                            "end": 713,
                            "matchedPaperCorpusId": "261038523"
                        },
                        {
                            "start": 816,
                            "end": 836,
                            "matchedPaperCorpusId": "263608784"
                        },
                        {
                            "start": 1504,
                            "end": 1520,
                            "matchedPaperCorpusId": "267547461"
                        },
                        {
                            "start": 1778,
                            "end": 1797,
                            "matchedPaperCorpusId": "269803977"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6328125
                }
            ],
            "relevance_judgement": 0.6328125,
            "relevance_judgment_input_expanded": "# Title: SC4ANM: Identifying optimal section combinations for automated novelty prediction in academic papers\n# Venue: Expert systems with applications\n# Authors: Wenqing Wu, Chengzhi Zhang, Tong Bao, Yi Zhao\n## Abstract\nNone\n## Large Language Models for Reviewing\nWith the rapid advancement of artificial intelligence and natural language processing technologies, LLMs (Brown et al., 2020;Ouyang et al., 2022), particularly those supported by transformer-based architectures and pre-trained on massive datasets, have gradually come into the public eye. With the successive releases of ChatGPT and GPT-4 (OpenAI, 2024) by OpenAI, as well as Llama (Touvron et al., 2023a) and others (Meta, 2024;Touvron et al., 2023b;Chowdhery et al., 2023), these LLMs have demonstrated powerful language generation and understanding capabilities, attracting significant research interest from scholars in the academic community. (Patsakis et al., 2024;Shafee et al., 2024;Caruccio et al., 2024) Recently, research on using LLMs for peer review has been gaining increasing popularity. Liang et al. (Liang et al., 2023) conducted a large-scale empirical analysis to assess comments generated by GPT-4: they tag several metrics and investigated user satisfaction to measure comment quality. They found that while LLM-generated reviews were helpful, they might be non-generic and tended to focus on certain aspects of scientific feedback. Liu and Shah (Liu and Shah, 2023) validated the utility of LLMs across three tasks: identifying errors, verifying checklists, and choosing the \"better\" pa-per. They concluded that LLMs serve well as review assistants for specific reviewing tasks; however, they are not yet sufficient for conducting comprehensive evaluations of papers. Mike Thelwall (Thelwall, 2024) used GPT-4 to evaluate the quality of journal articles on a paper assessment dataset to test its effectiveness. The results indicated that GPT-4 appears to be insufficiently accurate for any formal or informal research quality assessment tasks. Zhou et al. (Zhou et al., 2024) conducted a comprehensive evaluation to determine whether LLMs can be qualified and reliable reviewers. They concluded that it is premature for LLMs to serve as automated scientific paper reviewers. Although there is potential for obtaining useful and accurate results, their current capabilities are not yet reliable enough.",
            "reference_string": "[276296422 | Wu et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Automated literature research and review-generation method based on large language models",
            "venue": "National Science Review",
            "year": 2024,
            "reference_count": 80,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1093/nsr/nwaf169",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.20906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2142349414",
                    "name": "Shican Wu"
                },
                {
                    "authorId": "2313884864",
                    "name": "Xiao Ma"
                },
                {
                    "authorId": "2313733635",
                    "name": "Dehui Luo"
                },
                {
                    "authorId": "2277565237",
                    "name": "Lulu Li"
                },
                {
                    "authorId": "2289200637",
                    "name": "Xiangcheng Shi"
                },
                {
                    "authorId": "2289200103",
                    "name": "Xin Chang"
                },
                {
                    "authorId": "2152842807",
                    "name": "Xiaoyun Lin"
                },
                {
                    "authorId": "2075340202",
                    "name": "Ran Luo"
                },
                {
                    "authorId": "48471690",
                    "name": "Chunlei Pei"
                },
                {
                    "authorId": "12144921",
                    "name": "Zhijian Zhao"
                },
                {
                    "authorId": "2258427381",
                    "name": "Jinlong Gong"
                }
            ],
            "abstract": "Abstract Literature research, which is vital for scientific work, faces the challenge of surging information volumes that are exceeding researchers\u2019 processing capabilities. This paper describes an automated review-generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring user domain knowledge. Applied to propane dehydrogenation catalysts, our method demonstrated two aspects: first, generating comprehensive reviews from 343 articles spanning 35 topics; and, second, evaluating data-mining capabilities by using 1041 articles for experimental catalyst property analysis. Through multilayered quality control, we effectively mitigated the hallucinations of LLMs, with expert verification confirming accuracy and citation integrity, while demonstrating hallucination risks reduced to <0.5% with 95% confidence. The released software application enables one-click review generation, enhancing research productivity and literature-recommendation efficiency while facilitating broader scientific explorations.",
            "corpus_id": 271544398,
            "sentences": [
                {
                    "corpus_id": "271544398",
                    "title": "Automated literature research and review-generation method based on large language models",
                    "text": "Research demonstrates LLMs excel in evaluation tasks, with GPT-4 surpassing both crowdsourced workers [26] and experts [27] in text annotation accuracy and reliability, and bias control for complex tasks requiring contextual knowledge reasoning [27]. LLMs show comparable or superior performance to human annotation in persuasiveness, accuracy, and satisfaction [28]. Studies confirm the capability of LLMs to evaluate other LLMs, with verification abilities improving faster than generation quality [29]. GPT-4's evaluations exceeds 80% consistency with human reviewers [30] and exceeds 85% alignment in pairwise comparisons [31], reaching nearly 100% agreement when performance differences are significant [31]. Self-verification benefits LLM performance [32], though biases issues [33], such as position bias [34], length bias [30], and self-bias [35], persist but can be reduced through proper design [36]. \n\nGiven characteristic writing patterns of LLMs and potential human evaluation bias, this study employs LLM-based evaluation exclusively. Existing LLM-based LLM evaluation methods encompass scoring, comparison, selection judgment, and comprehensive description. This study introduces a dual-baseline review quality evaluation framework, to minimize potential LLM evaluation bias and quantitatively compare LLM-generated reviews with peer-reviewed expert-written content, validating reliability through statistical analysis. In our method, we segmented 14 published Q1 reviews into 89 fragments based on semantic content. Using extracted topics from these fragments and the literature cited in the original text, we generated comparative reviews using Qwen2-7b-Instruct, Qwen2-72b-Instruct, and Claude3.5Sonnet. This methodology enabled direct comparison between human experts and LLMs in writing reviews with identical literature background, establishing a rigorous benchmark for LLMs given their limitations in accessing both human accumulated domain expertise and pre-1970 undigitized literature. \n\nIn the evaluation process, we compared the performance of two models of different scales from the open-source Qwen2 series (Qwen2-7b-Instruct and Qwen2-72b-Instruct) and the closed-source model Claude3.5Sonnet (Fig. 1).",
                    "score": 0.4657512104917435,
                    "section_title": "Evaluation of generated review quality",
                    "char_start_offset": 8750,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 250
                        },
                        {
                            "start": 251,
                            "end": 367
                        },
                        {
                            "start": 368,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 713
                        },
                        {
                            "start": 714,
                            "end": 910
                        },
                        {
                            "start": 913,
                            "end": 1048
                        },
                        {
                            "start": 1049,
                            "end": 1172
                        },
                        {
                            "start": 1173,
                            "end": 1434
                        },
                        {
                            "start": 1435,
                            "end": 1531
                        },
                        {
                            "start": 1532,
                            "end": 1721
                        },
                        {
                            "start": 1722,
                            "end": 2009
                        },
                        {
                            "start": 2012,
                            "end": 2221
                        },
                        {
                            "start": 2222,
                            "end": 2231
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 102,
                            "end": 106,
                            "matchedPaperCorpusId": "257766307"
                        },
                        {
                            "start": 571,
                            "end": 575,
                            "matchedPaperCorpusId": "259129398"
                        },
                        {
                            "start": 626,
                            "end": 630,
                            "matchedPaperCorpusId": "259095491"
                        },
                        {
                            "start": 708,
                            "end": 712,
                            "matchedPaperCorpusId": "259095491"
                        },
                        {
                            "start": 830,
                            "end": 834,
                            "matchedPaperCorpusId": "259129398"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63232421875
                }
            ],
            "relevance_judgement": 0.63232421875,
            "relevance_judgment_input_expanded": "# Title: Automated literature research and review-generation method based on large language models\n# Venue: National Science Review\n# Authors: Shican Wu, Xiao Ma, Dehui Luo, Lulu Li, Xiangcheng Shi, Xin Chang, Xiaoyun Lin, Ran Luo, Chunlei Pei, Zhijian Zhao, Jinlong Gong\n## Abstract\nAbstract Literature research, which is vital for scientific work, faces the challenge of surging information volumes that are exceeding researchers\u2019 processing capabilities. This paper describes an automated review-generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring user domain knowledge. Applied to propane dehydrogenation catalysts, our method demonstrated two aspects: first, generating comprehensive reviews from 343 articles spanning 35 topics; and, second, evaluating data-mining capabilities by using 1041 articles for experimental catalyst property analysis. Through multilayered quality control, we effectively mitigated the hallucinations of LLMs, with expert verification confirming accuracy and citation integrity, while demonstrating hallucination risks reduced to <0.5% with 95% confidence. The released software application enables one-click review generation, enhancing research productivity and literature-recommendation efficiency while facilitating broader scientific explorations.\n## Evaluation of generated review quality\nResearch demonstrates LLMs excel in evaluation tasks, with GPT-4 surpassing both crowdsourced workers [26] and experts [27] in text annotation accuracy and reliability, and bias control for complex tasks requiring contextual knowledge reasoning [27]. LLMs show comparable or superior performance to human annotation in persuasiveness, accuracy, and satisfaction [28]. Studies confirm the capability of LLMs to evaluate other LLMs, with verification abilities improving faster than generation quality [29]. GPT-4's evaluations exceeds 80% consistency with human reviewers [30] and exceeds 85% alignment in pairwise comparisons [31], reaching nearly 100% agreement when performance differences are significant [31]. Self-verification benefits LLM performance [32], though biases issues [33], such as position bias [34], length bias [30], and self-bias [35], persist but can be reduced through proper design [36]. \n\nGiven characteristic writing patterns of LLMs and potential human evaluation bias, this study employs LLM-based evaluation exclusively. Existing LLM-based LLM evaluation methods encompass scoring, comparison, selection judgment, and comprehensive description. This study introduces a dual-baseline review quality evaluation framework, to minimize potential LLM evaluation bias and quantitatively compare LLM-generated reviews with peer-reviewed expert-written content, validating reliability through statistical analysis. In our method, we segmented 14 published Q1 reviews into 89 fragments based on semantic content. Using extracted topics from these fragments and the literature cited in the original text, we generated comparative reviews using Qwen2-7b-Instruct, Qwen2-72b-Instruct, and Claude3.5Sonnet. This methodology enabled direct comparison between human experts and LLMs in writing reviews with identical literature background, establishing a rigorous benchmark for LLMs given their limitations in accessing both human accumulated domain expertise and pre-1970 undigitized literature. \n\nIn the evaluation process, we compared the performance of two models of different scales from the open-source Qwen2 series (Qwen2-7b-Instruct and Qwen2-72b-Instruct) and the closed-source model Claude3.5Sonnet (Fig. 1).",
            "reference_string": "[271544398 | Wu et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Evaluating science: A comparison of human and AI reviewers",
            "venue": "Judgment and Decision Making",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1017/jdm.2024.24",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1017/jdm.2024.24?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1017/jdm.2024.24, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "119770927",
                    "name": "Anna Shcherbiak"
                },
                {
                    "authorId": "2164766266",
                    "name": "Hooman Habibnia"
                },
                {
                    "authorId": "2331981144",
                    "name": "Robert B\u00f6hm"
                },
                {
                    "authorId": "2282716321",
                    "name": "Susann Fiedler"
                }
            ],
            "abstract": "\n Scientists have started to explore whether novel artificial intelligence (AI) tools based on large language models, such as GPT-4, could support the scientific peer review process. We sought to understand (i) whether AI versus human reviewers are able to distinguish between made-up AI-generated and human-written conference abstracts reporting on actual research, and (ii) how the quality assessments by AI versus human reviewers of the reported research correspond to each other. We conducted a large-scale field experiment during a medium-sized scientific conference, relying on 305 human-written and 20 AI-written abstracts that were reviewed either by AI or 217 human reviewers. The results show that human reviewers and GPTZero were better in discerning (AI vs. human) authorship than GPT-4. Regarding quality assessments, there was rather low agreement between both human\u2013human and human\u2013AI reviewer pairs, but AI reviewers were more aligned with human reviewers in classifying the very best abstracts. This indicates that AI could become a prescreening tool for scientific abstracts. The results are discussed with regard to the future development and use of AI tools during the scientific peer review process.",
            "corpus_id": 274213776,
            "sentences": [
                {
                    "corpus_id": "274213776",
                    "title": "Evaluating science: A comparison of human and AI reviewers",
                    "text": "There are several limitations to our study. Our sample of participants suffers from self-selection bias since the results are based on responses from conference participants who consented to review AIgenerated work. 6 While the call for participation was open to all conference participants, it is quite likely that participants who opted into the study were already familiar with and interested in LLMs. Such reviewers may have been particularly knowledgeable in distinguishing between actual humangenerated abstracts and made-up AI-generated abstracts. Furthermore, we only compared purely AIgenerated and human-written abstracts. Our primary intention was to isolate the effects of LLMs in the abstract generation and review processes, thereby limiting the potential impact of human intervention. Researchers commonly employ LLMs to improve many parts of their writing, such as language refinement, clarity, and idea development (Liang et al., 2024a,b). We did not have data on the extent or method by which researchers used LLMs to improve their submissions. Therefore, our study cannot account for the complex ways AI may have contributed to the quality of human-written abstracts. However, we did evaluate the extent to which authors have potentially used ChatGPT using GPTZero, and the overall prediction was rather low (8.9%, see Model 2 in Table 2). Moreover, we have used the same prompt for both human reviewers and ChatGPT for evaluation. As described above, some prompts were quite abstract and lacked specific evaluation criteria, which could have been an issue for ChatGPT evaluations. A precise and unambiguous prompt may better demonstrate the limits and capabilities of LLMs, and some researchers (e.g., Liang et al., 2024;Liu and Shah, 2023) have either given very specific criteria or dynamically changed the prompt to probe for the most accurate answer. Finally, our results are naturally limited to the current technology (i.e., GPT-3.5 and GPT-4 as used in the present study) and knowledge of the sample of reviewers. 7 Improved LLMs, some of which are fine-tuned on scientific data (e.g., SciBERT Beltagy et al., 2019), may soon overcome some of the weaknesses of AI-based reviews.",
                    "score": 0.37671999937239253,
                    "section_title": "Limitations",
                    "char_start_offset": 25023,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 43
                        },
                        {
                            "start": 44,
                            "end": 217
                        },
                        {
                            "start": 218,
                            "end": 404
                        },
                        {
                            "start": 405,
                            "end": 554
                        },
                        {
                            "start": 555,
                            "end": 632
                        },
                        {
                            "start": 633,
                            "end": 799
                        },
                        {
                            "start": 800,
                            "end": 956
                        },
                        {
                            "start": 957,
                            "end": 1062
                        },
                        {
                            "start": 1063,
                            "end": 1186
                        },
                        {
                            "start": 1187,
                            "end": 1358
                        },
                        {
                            "start": 1359,
                            "end": 1450
                        },
                        {
                            "start": 1451,
                            "end": 1600
                        },
                        {
                            "start": 1601,
                            "end": 1874
                        },
                        {
                            "start": 1875,
                            "end": 2042
                        },
                        {
                            "start": 2043,
                            "end": 2205
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1722,
                            "end": 1741,
                            "matchedPaperCorpusId": "263608784"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.60693359375
                }
            ],
            "relevance_judgement": 0.60693359375,
            "relevance_judgment_input_expanded": "# Title: Evaluating science: A comparison of human and AI reviewers\n# Venue: Judgment and Decision Making\n# Authors: Anna Shcherbiak, Hooman Habibnia, Robert B\u00f6hm, Susann Fiedler\n## Abstract\n\n Scientists have started to explore whether novel artificial intelligence (AI) tools based on large language models, such as GPT-4, could support the scientific peer review process. We sought to understand (i) whether AI versus human reviewers are able to distinguish between made-up AI-generated and human-written conference abstracts reporting on actual research, and (ii) how the quality assessments by AI versus human reviewers of the reported research correspond to each other. We conducted a large-scale field experiment during a medium-sized scientific conference, relying on 305 human-written and 20 AI-written abstracts that were reviewed either by AI or 217 human reviewers. The results show that human reviewers and GPTZero were better in discerning (AI vs. human) authorship than GPT-4. Regarding quality assessments, there was rather low agreement between both human\u2013human and human\u2013AI reviewer pairs, but AI reviewers were more aligned with human reviewers in classifying the very best abstracts. This indicates that AI could become a prescreening tool for scientific abstracts. The results are discussed with regard to the future development and use of AI tools during the scientific peer review process.\n## Limitations\nThere are several limitations to our study. Our sample of participants suffers from self-selection bias since the results are based on responses from conference participants who consented to review AIgenerated work. 6 While the call for participation was open to all conference participants, it is quite likely that participants who opted into the study were already familiar with and interested in LLMs. Such reviewers may have been particularly knowledgeable in distinguishing between actual humangenerated abstracts and made-up AI-generated abstracts. Furthermore, we only compared purely AIgenerated and human-written abstracts. Our primary intention was to isolate the effects of LLMs in the abstract generation and review processes, thereby limiting the potential impact of human intervention. Researchers commonly employ LLMs to improve many parts of their writing, such as language refinement, clarity, and idea development (Liang et al., 2024a,b). We did not have data on the extent or method by which researchers used LLMs to improve their submissions. Therefore, our study cannot account for the complex ways AI may have contributed to the quality of human-written abstracts. However, we did evaluate the extent to which authors have potentially used ChatGPT using GPTZero, and the overall prediction was rather low (8.9%, see Model 2 in Table 2). Moreover, we have used the same prompt for both human reviewers and ChatGPT for evaluation. As described above, some prompts were quite abstract and lacked specific evaluation criteria, which could have been an issue for ChatGPT evaluations. A precise and unambiguous prompt may better demonstrate the limits and capabilities of LLMs, and some researchers (e.g., Liang et al., 2024;Liu and Shah, 2023) have either given very specific criteria or dynamically changed the prompt to probe for the most accurate answer. Finally, our results are naturally limited to the current technology (i.e., GPT-3.5 and GPT-4 as used in the present study) and knowledge of the sample of reviewers. 7 Improved LLMs, some of which are fine-tuned on scientific data (e.g., SciBERT Beltagy et al., 2019), may soon overcome some of the weaknesses of AI-based reviews.",
            "reference_string": "[274213776 | Shcherbiak et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Harnessing Large Language Models in Medical Research and Scientific Writing: A Closer Look to The Future",
            "venue": "High Yield Medical Reviews",
            "year": 2023,
            "reference_count": 0,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://hymr.highyieldmed.org/index.php/HYMR/article/download/15/49",
                "status": "HYBRID",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.59707/hymrfbya5348?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.59707/hymrfbya5348, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280673116",
                    "name": "Mohammad Abu-Jeyyab"
                },
                {
                    "authorId": "2198010313",
                    "name": "Sallam Alrosan"
                },
                {
                    "authorId": "2219157451",
                    "name": "I. Alkhawaldeh"
                }
            ],
            "abstract": "Large Language Models (LLMs), a form of artificial intelligence generating natural language responses based on user input, have demonstrated potential across various applications such as entertainment, education, and customer service. This review comprehensively highlights their current research status and potential applications within the medical domain, addressing the challenges and opportunities for future development and implementation. Key aspects covered include diverse data sources for training and testing, such as electronic health records and clinical trials; ethical considerations, including privacy and consent; evaluation techniques focusing on accuracy and coherence; and clinical applications ranging from diagnosis to patient education. The review concludes that LLMs hold significant promise for enhancing the quality and efficiency of medical research and scientific writing but also emphasize the need for careful design and regulation to ensure safety and reliability.",
            "corpus_id": 266153952,
            "sentences": [
                {
                    "corpus_id": "266153952",
                    "title": "Harnessing Large Language Models in Medical Research and Scientific Writing: A Closer Look to The Future",
                    "text": "Chatbot AI tools are artificial intelligence systems that can generate natural language responses based on text or image inputs. 19,34 They can potentially assist with various tasks in medical research and scientific writing, such as literature review, 35 data analysis, 36 draft generation, 37 summarization, 38 translation, 39 and proofreading. 27 However, they also pose challenges and risks, such as bias, 40 plagiarism, 41 inaccuracies, 42 and ethical issues. 43 Therefore, it is important to evaluate their performance and quality using appropriate methods and metrics. 22 ome possible methods that can be used to measure the performance and quality of LLMs for medical research and scientific writing are: \n\nThese methods have different strengths and limitations that need to be considered when evaluating LLMs for medical research and scientific writing. Depending on the specific goals and needs of the researchers or users, they may choose one or more methods that suit their situation. For example, they may use human evaluation for pilot studies or user satisfaction surveys; automatic evaluation for baseline comparisons or error analysis; or hybrid evaluation for comprehensive studies or quality assurance. By using appropriate methods to measure the performance and quality of LLMs, researchers, and users can ensure that these models are effective and beneficial for medical research and scientific writing.",
                    "score": 0.39795220771390466,
                    "section_title": "DIFFERENT METHODS CAN BE USED TO MEASURE THE PERFORMANCE AND QUALITY OF LLMS FOR MEDICAL RESEARCH AND SCIENTIFIC WRITING",
                    "char_start_offset": 16151,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 135,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 467
                        },
                        {
                            "start": 468,
                            "end": 578
                        },
                        {
                            "start": 579,
                            "end": 712
                        },
                        {
                            "start": 715,
                            "end": 862
                        },
                        {
                            "start": 863,
                            "end": 996
                        },
                        {
                            "start": 997,
                            "end": 1221
                        },
                        {
                            "start": 1222,
                            "end": 1424
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 129,
                            "end": 132,
                            "matchedPaperCorpusId": "254876189"
                        },
                        {
                            "start": 132,
                            "end": 134,
                            "matchedPaperCorpusId": "7961699"
                        },
                        {
                            "start": 253,
                            "end": 255,
                            "matchedPaperCorpusId": "6405271"
                        },
                        {
                            "start": 271,
                            "end": 273,
                            "matchedPaperCorpusId": "44134226"
                        },
                        {
                            "start": 292,
                            "end": 294,
                            "matchedPaperCorpusId": "15442925"
                        },
                        {
                            "start": 326,
                            "end": 328,
                            "matchedPaperCorpusId": "52167799"
                        },
                        {
                            "start": 347,
                            "end": 349,
                            "matchedPaperCorpusId": "33285731"
                        },
                        {
                            "start": 465,
                            "end": 467,
                            "matchedPaperCorpusId": "3519414"
                        },
                        {
                            "start": 576,
                            "end": 578,
                            "matchedPaperCorpusId": "10438447"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6025390625
                }
            ],
            "relevance_judgement": 0.6025390625,
            "relevance_judgment_input_expanded": "# Title: Harnessing Large Language Models in Medical Research and Scientific Writing: A Closer Look to The Future\n# Venue: High Yield Medical Reviews\n# Authors: Mohammad Abu-Jeyyab, Sallam Alrosan, I. Alkhawaldeh\n## Abstract\nLarge Language Models (LLMs), a form of artificial intelligence generating natural language responses based on user input, have demonstrated potential across various applications such as entertainment, education, and customer service. This review comprehensively highlights their current research status and potential applications within the medical domain, addressing the challenges and opportunities for future development and implementation. Key aspects covered include diverse data sources for training and testing, such as electronic health records and clinical trials; ethical considerations, including privacy and consent; evaluation techniques focusing on accuracy and coherence; and clinical applications ranging from diagnosis to patient education. The review concludes that LLMs hold significant promise for enhancing the quality and efficiency of medical research and scientific writing but also emphasize the need for careful design and regulation to ensure safety and reliability.\n## DIFFERENT METHODS CAN BE USED TO MEASURE THE PERFORMANCE AND QUALITY OF LLMS FOR MEDICAL RESEARCH AND SCIENTIFIC WRITING\nChatbot AI tools are artificial intelligence systems that can generate natural language responses based on text or image inputs. 19,34 They can potentially assist with various tasks in medical research and scientific writing, such as literature review, 35 data analysis, 36 draft generation, 37 summarization, 38 translation, 39 and proofreading. 27 However, they also pose challenges and risks, such as bias, 40 plagiarism, 41 inaccuracies, 42 and ethical issues. 43 Therefore, it is important to evaluate their performance and quality using appropriate methods and metrics. 22 ome possible methods that can be used to measure the performance and quality of LLMs for medical research and scientific writing are: \n\nThese methods have different strengths and limitations that need to be considered when evaluating LLMs for medical research and scientific writing. Depending on the specific goals and needs of the researchers or users, they may choose one or more methods that suit their situation. For example, they may use human evaluation for pilot studies or user satisfaction surveys; automatic evaluation for baseline comparisons or error analysis; or hybrid evaluation for comprehensive studies or quality assurance. By using appropriate methods to measure the performance and quality of LLMs, researchers, and users can ensure that these models are effective and beneficial for medical research and scientific writing.",
            "reference_string": "[266153952 | Abu-Jeyyab et al. | 2023 | Citations: 5]"
        },
        {
            "title": "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 46,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.12265, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326124996",
                    "name": "Junjie Chen"
                },
                {
                    "authorId": "2147219374",
                    "name": "Weihang Su"
                },
                {
                    "authorId": "1381470019",
                    "name": "Zhumin Chu"
                },
                {
                    "authorId": "2108590438",
                    "name": "Haitao Li"
                },
                {
                    "authorId": "2326111223",
                    "name": "Qinyao Ai"
                },
                {
                    "authorId": "1783406",
                    "name": "Yiqun Liu"
                },
                {
                    "authorId": "2258690221",
                    "name": "Min Zhang"
                },
                {
                    "authorId": "8093158",
                    "name": "Shaoping Ma"
                }
            ],
            "abstract": "With the rapid development of large language models (LLMs), how to efficiently evaluate them has become an important research question. Existing evaluation methods often suffer from high costs, limited test formats, the need of human references, and systematic evaluation biases. To address these limitations, our study introduces the Auto-PRE, an automatic LLM evaluation framework based on peer review. In contrast to previous studies that rely on human annotations, Auto-PRE selects evaluator LLMs automatically based on their inherent traits including consistency, self-confidence, and pertinence. We conduct extensive experiments on three tasks: summary generation, non-factoid question-answering, and dialogue generation. Experimental results indicate our Auto-PRE achieves state-of-the-art performance at a lower cost. Moreover, our study highlights the impact of prompt strategies and evaluation formats on evaluation performance, offering guidance for method optimization in the future.",
            "corpus_id": 273374964,
            "sentences": [
                {
                    "corpus_id": "273374964",
                    "title": "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation",
                    "text": "To demonstrate that Auto-PRE effectively reduces the bias in single evaluator LLM (like GPT-4), we conducted the following experiments: Previous research (Chu et al., 2024;Zeng et al., 2023) has shown that one of the biases present in GPT-4 is that it may prefer answers generated by LLMs in the GPT series, regardless of the actual quality of the answers. To clearly demonstrate the negative impact of this bias on experimental outcomes, we used GPT-3.5-turbo and ChatGLM2-6B as two LLMs generating the answers to be tested and conducted experiments on the Xsum and NF_CATS datasets in pairwise evaluation format. rate method = p method \u2212 p human p human \u00d7 100% (1) \n\nTable 4 presents the experimental results. Rate is calculated using Equation 1, where p denotes the proportion of evaluation results that favor GPT-3.5-turbo. The rate value refers to the percentage change in the method's preference for GPT-3.5turbo compared to human preference. It is noteworthy that the rate does not necessarily correlate with accuracy, as it does not consider the order of answers. However, it can, to some extent, reflect a method's preference bias for specific answers. \n\nFrom the results, we can observe a significant performance gap between GPT-4 and Auto-PRE, with GPT-4's rate much higher than that of other methods. This suggests that GPT-4's preference for answers from GPT-3.5-turbo compromises its performance and reliability. Conversely, Auto-PRE enhances overall performance and reliability by collaborating with various types of LLMs.",
                    "score": 0.39746640271373646,
                    "section_title": "Bias Analysis",
                    "char_start_offset": 21326,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 356
                        },
                        {
                            "start": 357,
                            "end": 460
                        },
                        {
                            "start": 461,
                            "end": 614
                        },
                        {
                            "start": 615,
                            "end": 666
                        },
                        {
                            "start": 669,
                            "end": 711
                        },
                        {
                            "start": 712,
                            "end": 827
                        },
                        {
                            "start": 828,
                            "end": 918
                        },
                        {
                            "start": 919,
                            "end": 948
                        },
                        {
                            "start": 949,
                            "end": 1071
                        },
                        {
                            "start": 1072,
                            "end": 1161
                        },
                        {
                            "start": 1164,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1426
                        },
                        {
                            "start": 1427,
                            "end": 1537
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5966796875
                },
                {
                    "corpus_id": "273374964",
                    "title": "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation",
                    "text": "ChatEval (Chan et al., 2023), for instance, directly selects strong LLMs like GPT-4 to build multiple agents to debate and collaborate for evaluation. However, since it only utilizes LLMs from the same series, it still suffers from systematic biases. PRE (Chu et al., 2024) proposes to build a peer-review system with different types of evaluator LLMs selected with a well-crafted exam, which selects qualified evaluator LLMs by calculating the accuracy of the candidate LLMs' results compared to the manual annotations. Although this method can achieve superior evaluation performance, it still relies on human-annotated data for the exam and thus is not fully automated. \n\nInspired by PRE, we propose the Auto-PRE by designing an automated qualification exam. Previous studies (Zhao et al., 2015;Zeng et al., 2023) have shown that whether a human evaluator is excellent in the academic peer review system is influenced by three important factors: (1) Consistency: whether the reviewer produces consistent evaluation; (2) Self-Confidence: whether the reviewers can correctly estimate their evaluation confidence based on task difficulty; (3) Pertinence: whether the reviewer can capture the key information that distinguishes different candidates without affected by superficial factors that are not important for evaluation. Based on the above observations, we design three selection methods to filter evaluator LLMs automatically for peer review. All these methods involve no human annotations, thus making the whole framework a fully automated one that is open-ended, reference-free, cost-efficient, and robust to systematic biases introduced by LLMs. \n\nExperiment results on three tasks, summary generation, non-factoid question-answering and dialogue generation indicate that our Auto-PRE can achieve state-of-the-art performance similar to PRE and GPT-4 at a much lower cost. Additionally, we analyze how prompt strategies and evaluation formats could affect our framework to improve the generalizability of our work and provide more insights for future automated evaluation methods based on LLMs. \n\nIn conclusion, the contributions of this paper are as follows:",
                    "score": 0.4629709121058305,
                    "section_title": "Introduction",
                    "char_start_offset": 2087,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 250
                        },
                        {
                            "start": 251,
                            "end": 520
                        },
                        {
                            "start": 521,
                            "end": 672
                        },
                        {
                            "start": 675,
                            "end": 761
                        },
                        {
                            "start": 762,
                            "end": 1326
                        },
                        {
                            "start": 1327,
                            "end": 1449
                        },
                        {
                            "start": 1450,
                            "end": 1655
                        },
                        {
                            "start": 1658,
                            "end": 1882
                        },
                        {
                            "start": 1883,
                            "end": 2104
                        },
                        {
                            "start": 2107,
                            "end": 2169
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.54296875
                }
            ],
            "relevance_judgement": 0.5966796875,
            "relevance_judgment_input_expanded": "# Title: An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation\n# Venue: arXiv.org\n# Authors: Junjie Chen, Weihang Su, Zhumin Chu, Haitao Li, Qinyao Ai, Yiqun Liu, Min Zhang, Shaoping Ma\n## Abstract\nWith the rapid development of large language models (LLMs), how to efficiently evaluate them has become an important research question. Existing evaluation methods often suffer from high costs, limited test formats, the need of human references, and systematic evaluation biases. To address these limitations, our study introduces the Auto-PRE, an automatic LLM evaluation framework based on peer review. In contrast to previous studies that rely on human annotations, Auto-PRE selects evaluator LLMs automatically based on their inherent traits including consistency, self-confidence, and pertinence. We conduct extensive experiments on three tasks: summary generation, non-factoid question-answering, and dialogue generation. Experimental results indicate our Auto-PRE achieves state-of-the-art performance at a lower cost. Moreover, our study highlights the impact of prompt strategies and evaluation formats on evaluation performance, offering guidance for method optimization in the future.\n## Introduction\nChatEval (Chan et al., 2023), for instance, directly selects strong LLMs like GPT-4 to build multiple agents to debate and collaborate for evaluation. However, since it only utilizes LLMs from the same series, it still suffers from systematic biases. PRE (Chu et al., 2024) proposes to build a peer-review system with different types of evaluator LLMs selected with a well-crafted exam, which selects qualified evaluator LLMs by calculating the accuracy of the candidate LLMs' results compared to the manual annotations. Although this method can achieve superior evaluation performance, it still relies on human-annotated data for the exam and thus is not fully automated. \n\nInspired by PRE, we propose the Auto-PRE by designing an automated qualification exam. Previous studies (Zhao et al., 2015;Zeng et al., 2023) have shown that whether a human evaluator is excellent in the academic peer review system is influenced by three important factors: (1) Consistency: whether the reviewer produces consistent evaluation; (2) Self-Confidence: whether the reviewers can correctly estimate their evaluation confidence based on task difficulty; (3) Pertinence: whether the reviewer can capture the key information that distinguishes different candidates without affected by superficial factors that are not important for evaluation. Based on the above observations, we design three selection methods to filter evaluator LLMs automatically for peer review. All these methods involve no human annotations, thus making the whole framework a fully automated one that is open-ended, reference-free, cost-efficient, and robust to systematic biases introduced by LLMs. \n\nExperiment results on three tasks, summary generation, non-factoid question-answering and dialogue generation indicate that our Auto-PRE can achieve state-of-the-art performance similar to PRE and GPT-4 at a much lower cost. Additionally, we analyze how prompt strategies and evaluation formats could affect our framework to improve the generalizability of our work and provide more insights for future automated evaluation methods based on LLMs. \n\nIn conclusion, the contributions of this paper are as follows:\n\n## Bias Analysis\nTo demonstrate that Auto-PRE effectively reduces the bias in single evaluator LLM (like GPT-4), we conducted the following experiments: Previous research (Chu et al., 2024;Zeng et al., 2023) has shown that one of the biases present in GPT-4 is that it may prefer answers generated by LLMs in the GPT series, regardless of the actual quality of the answers. To clearly demonstrate the negative impact of this bias on experimental outcomes, we used GPT-3.5-turbo and ChatGLM2-6B as two LLMs generating the answers to be tested and conducted experiments on the Xsum and NF_CATS datasets in pairwise evaluation format. rate method = p method \u2212 p human p human \u00d7 100% (1) \n\nTable 4 presents the experimental results. Rate is calculated using Equation 1, where p denotes the proportion of evaluation results that favor GPT-3.5-turbo. The rate value refers to the percentage change in the method's preference for GPT-3.5turbo compared to human preference. It is noteworthy that the rate does not necessarily correlate with accuracy, as it does not consider the order of answers. However, it can, to some extent, reflect a method's preference bias for specific answers. \n\nFrom the results, we can observe a significant performance gap between GPT-4 and Auto-PRE, with GPT-4's rate much higher than that of other methods. This suggests that GPT-4's preference for answers from GPT-3.5-turbo compromises its performance and reliability. Conversely, Auto-PRE enhances overall performance and reliability by collaborating with various types of LLMs.",
            "reference_string": "[273374964 | Chen et al. | 2024 | Citations: 3]"
        },
        {
            "title": "The transformative impact of large language models on medical writing and publishing: current applications, challenges and future directions",
            "venue": "Korean Journal of Physiology and Pharmacology",
            "year": 2024,
            "reference_count": 64,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.4196/kjpp.2024.28.5.393",
                "status": "GOLD",
                "license": "CCBYNC",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11362003, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237073435",
                    "name": "S. Ahn"
                }
            ],
            "abstract": "Large language models (LLMs) are rapidly transforming medical writing and publishing. This review article focuses on experimental evidence to provide a comprehensive overview of the current applications, challenges, and future implications of LLMs in various stages of academic research and publishing process. Global surveys reveal a high prevalence of LLM usage in scientific writing, with both potential benefits and challenges associated with its adoption. LLMs have been successfully applied in literature search, research design, writing assistance, quality assessment, citation generation, and data analysis. LLMs have also been used in peer review and publication processes, including manuscript screening, generating review comments, and identifying potential biases. To ensure the integrity and quality of scholarly work in the era of LLM-assisted research, responsible artificial intelligence (AI) use is crucial. Researchers should prioritize verifying the accuracy and reliability of AI-generated content, maintain transparency in the use of LLMs, and develop collaborative human-AI workflows. Reviewers should focus on higher-order reviewing skills and be aware of the potential use of LLMs in manuscripts. Editorial offices should develop clear policies and guidelines on AI use and foster open dialogue within the academic community. Future directions include addressing the limitations and biases of current LLMs, exploring innovative applications, and continuously updating policies and practices in response to technological advancements. Collaborative efforts among stakeholders are necessary to harness the transformative potential of LLMs while maintaining the integrity of medical writing and publishing.",
            "corpus_id": 272223451,
            "sentences": [
                {
                    "corpus_id": "272223451",
                    "title": "The transformative impact of large language models on medical writing and publishing: current applications, challenges and future directions",
                    "text": "LLMs have been extensively applied in various aspects of writing assistance, particularly in abstract generation (Table 1). ChatGPT-3.5 demonstrated the ability to generate high-quality abstracts from clinical trial keywords and data tables, showcasing impressive accuracy with minor errors [24]. However, its performance varied significantly when tasked with writing abstracts on broad, well-documented topics compared to more specific, recently published subjects [25]. The low plagiarism scores and difficult detection of AI-generated abstracts and the ethical boundaries of using such technology in academic writing have also been discussed [19]. Although ChatGPT-3.5 could generate abstracts that were challenging to distinguish from human-written ones in the arthroplasty field, the quality was notably better in those written by humans [26]. Using both ChatGPT-3.5 and ChatGPT-4 to write abstracts for randomized controlled trials revealed that, despite their potential, the quality was not satisfactory, highlighting the need for further development and refinement in generative AI tools [27]. \n\nIn addition to abstract generation, LLMs have been used to assist in various other writing tasks. For example, GPT-4 was used to generate introduction sections for randomized controlled trials, with non-inferiority confirmed and higher readability scores compared to human-written introductions [28]. ChatGPT was also used to write medical case reports [29] and to write a clinical summary containing patient situation, case evaluation and appropriate interventions [30]. In a study regarding human reproduction, ChatGPT could produce high-quality text and efficiently summarize information, but its ability to interpret data and answer scientific questions was limited [31]. \n\nLLMs have been employed to generate cover letters for abstracts, with non-inferiority confirmed by randomized trials and higher readability scores [32]. These tools have also been used to facilitate language learning and improve technical writing skills for non-native English speakers, which is particularly meaningful for scholars using English as a non-primary language [33]. However, it is important to note that the effectiveness of these tools may vary, as one study found that the free version of ChatGPT-3.5 was not an effective writing coach [34].",
                    "score": 0.3947036202072795,
                    "section_title": "Writing assistance and quality assessment",
                    "char_start_offset": 7220,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 123
                        },
                        {
                            "start": 124,
                            "end": 296
                        },
                        {
                            "start": 297,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 650
                        },
                        {
                            "start": 651,
                            "end": 848
                        },
                        {
                            "start": 849,
                            "end": 1101
                        },
                        {
                            "start": 1104,
                            "end": 1201
                        },
                        {
                            "start": 1202,
                            "end": 1404
                        },
                        {
                            "start": 1405,
                            "end": 1575
                        },
                        {
                            "start": 1576,
                            "end": 1779
                        },
                        {
                            "start": 1782,
                            "end": 1934
                        },
                        {
                            "start": 1935,
                            "end": 2160
                        },
                        {
                            "start": 2161,
                            "end": 2338
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 291,
                            "end": 295,
                            "matchedPaperCorpusId": "258508171"
                        },
                        {
                            "start": 645,
                            "end": 649,
                            "matchedPaperCorpusId": "258354722"
                        },
                        {
                            "start": 843,
                            "end": 847,
                            "matchedPaperCorpusId": "267633428"
                        },
                        {
                            "start": 1096,
                            "end": 1100,
                            "matchedPaperCorpusId": "267679187"
                        },
                        {
                            "start": 1399,
                            "end": 1403,
                            "matchedPaperCorpusId": "265331105"
                        },
                        {
                            "start": 1457,
                            "end": 1461,
                            "matchedPaperCorpusId": "258874036"
                        },
                        {
                            "start": 1570,
                            "end": 1574,
                            "matchedPaperCorpusId": "258157664"
                        },
                        {
                            "start": 1929,
                            "end": 1933,
                            "matchedPaperCorpusId": "265497952"
                        },
                        {
                            "start": 2155,
                            "end": 2159,
                            "matchedPaperCorpusId": "266330607"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.58203125
                }
            ],
            "relevance_judgement": 0.58203125,
            "relevance_judgment_input_expanded": "# Title: The transformative impact of large language models on medical writing and publishing: current applications, challenges and future directions\n# Venue: Korean Journal of Physiology and Pharmacology\n# Authors: S. Ahn\n## Abstract\nLarge language models (LLMs) are rapidly transforming medical writing and publishing. This review article focuses on experimental evidence to provide a comprehensive overview of the current applications, challenges, and future implications of LLMs in various stages of academic research and publishing process. Global surveys reveal a high prevalence of LLM usage in scientific writing, with both potential benefits and challenges associated with its adoption. LLMs have been successfully applied in literature search, research design, writing assistance, quality assessment, citation generation, and data analysis. LLMs have also been used in peer review and publication processes, including manuscript screening, generating review comments, and identifying potential biases. To ensure the integrity and quality of scholarly work in the era of LLM-assisted research, responsible artificial intelligence (AI) use is crucial. Researchers should prioritize verifying the accuracy and reliability of AI-generated content, maintain transparency in the use of LLMs, and develop collaborative human-AI workflows. Reviewers should focus on higher-order reviewing skills and be aware of the potential use of LLMs in manuscripts. Editorial offices should develop clear policies and guidelines on AI use and foster open dialogue within the academic community. Future directions include addressing the limitations and biases of current LLMs, exploring innovative applications, and continuously updating policies and practices in response to technological advancements. Collaborative efforts among stakeholders are necessary to harness the transformative potential of LLMs while maintaining the integrity of medical writing and publishing.\n## Writing assistance and quality assessment\nLLMs have been extensively applied in various aspects of writing assistance, particularly in abstract generation (Table 1). ChatGPT-3.5 demonstrated the ability to generate high-quality abstracts from clinical trial keywords and data tables, showcasing impressive accuracy with minor errors [24]. However, its performance varied significantly when tasked with writing abstracts on broad, well-documented topics compared to more specific, recently published subjects [25]. The low plagiarism scores and difficult detection of AI-generated abstracts and the ethical boundaries of using such technology in academic writing have also been discussed [19]. Although ChatGPT-3.5 could generate abstracts that were challenging to distinguish from human-written ones in the arthroplasty field, the quality was notably better in those written by humans [26]. Using both ChatGPT-3.5 and ChatGPT-4 to write abstracts for randomized controlled trials revealed that, despite their potential, the quality was not satisfactory, highlighting the need for further development and refinement in generative AI tools [27]. \n\nIn addition to abstract generation, LLMs have been used to assist in various other writing tasks. For example, GPT-4 was used to generate introduction sections for randomized controlled trials, with non-inferiority confirmed and higher readability scores compared to human-written introductions [28]. ChatGPT was also used to write medical case reports [29] and to write a clinical summary containing patient situation, case evaluation and appropriate interventions [30]. In a study regarding human reproduction, ChatGPT could produce high-quality text and efficiently summarize information, but its ability to interpret data and answer scientific questions was limited [31]. \n\nLLMs have been employed to generate cover letters for abstracts, with non-inferiority confirmed by randomized trials and higher readability scores [32]. These tools have also been used to facilitate language learning and improve technical writing skills for non-native English speakers, which is particularly meaningful for scholars using English as a non-primary language [33]. However, it is important to note that the effectiveness of these tools may vary, as one study found that the free version of ChatGPT-3.5 was not an effective writing coach [34].",
            "reference_string": "[272223451 | Ahn | 2024 | Citations: 7]"
        },
        {
            "title": "DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems",
            "venue": "Proceedings of the 4th International Workshop on Knowledge-Augmented Methods for Natural Language Processing",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.10701, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2187586628",
                    "name": "Anni Zou"
                },
                {
                    "authorId": "2265843326",
                    "name": "Wenhao Yu"
                },
                {
                    "authorId": "2254831297",
                    "name": "Hongming Zhang"
                },
                {
                    "authorId": "2311458574",
                    "name": "Kaixin Ma"
                },
                {
                    "authorId": "2286805668",
                    "name": "Deng Cai"
                },
                {
                    "authorId": "3322871",
                    "name": "Zhuosheng Zhang"
                },
                {
                    "authorId": "2287982566",
                    "name": "Hai Zhao"
                },
                {
                    "authorId": "2261392681",
                    "name": "Dong Yu"
                }
            ],
            "abstract": "Recently, there has been a growing interest among large language model (LLM) developers in LLM-based document reading systems, which enable users to upload their own documents and pose questions related to the document contents, going beyond simple reading comprehension tasks. Consequently, these systems have been carefully designed to tackle challenges such as file parsing, metadata extraction, multi-modal information understanding and long-context reading. However, no current benchmark exists to evaluate their performance in such scenarios, where a raw file and questions are provided as input, and a corresponding response is expected as output. In this paper, we introduce DocBench, a new benchmark designed to evaluate LLM-based document reading systems. Our benchmark involves a meticulously crafted process, including the recruitment of human annotators and the generation of synthetic questions. It includes 229 real documents and 1,102 questions, spanning across five different domains and four major types of questions. We evaluate both proprietary LLM-based systems accessible via web interfaces or APIs, and a parse-then-read pipeline employing open-source LLMs. Our evaluations reveal noticeable gaps between existing LLM-based document reading systems and human performance, underscoring the challenges of developing proficient systems. To summarize, DocBench aims to establish a standardized benchmark for evaluating LLM-based document reading systems under diverse real-world scenarios, thereby guiding future advancements in this research area.",
            "corpus_id": 271212285,
            "sentences": [
                {
                    "corpus_id": "271212285",
                    "title": "DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems",
                    "text": "Evaluation Process Our dataset diversity poses two major evaluation challenges: (i) The evaluation methods vary depending on the answer type.For example, for boolean or numerical answers, a fair evaluator only needs to verify the correctness of a binary yes/no response or a specific number using simple techniques like string matching or number extraction.In contrast, textual responses require more nuanced standards such as natural language generation (NLG) metrics.Thus, accurately determining the appropriate evaluation method becomes complex when the answer type is unknown.\n\n(ii) Different LLMs and systems exhibit substantial variations in the organization and style of their outputs, potentially leading to biases in traditional evaluation approaches.Therefore, we capitalize on the prowess of LLMs that have proven to be decent evaluators and can be easily adapted to the assessment of various answer types [14,24,39].Inspired by Liu et al. [24], we clearly define the evaluation criteria for various types within the instruction prompt and then instruct GPT-4 to assign a score of 0 (incorrect) or 1 (correct).After evaluating 200 examples by both human evaluators and GPT-4, we found that the GPT-4 automatic evaluator shows a 98% agreement with human annotators, significantly exceeding the traditional string matching approach.Details of this experiment is shown in Table 3, and details of evaluation instruction prompts are attached in Appendix A. Metrics As mentioned above, we instruct GPT-4 to assign a score of 0 (incorrect) or 1 (correct), thus using Accuracy (abbreviated as Acc.) to measure system performance.We report accuracy across all instances, as well as for each domain and QA-pair type in Table 4.\n\n3 Experiments and Analysis",
                    "score": 0.3790613114962421,
                    "section_title": "Evaluation Setup",
                    "char_start_offset": 10648,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 141,
                            "end": 357
                        },
                        {
                            "start": 357,
                            "end": 469
                        },
                        {
                            "start": 469,
                            "end": 580
                        },
                        {
                            "start": 582,
                            "end": 760
                        },
                        {
                            "start": 760,
                            "end": 928
                        },
                        {
                            "start": 928,
                            "end": 1121
                        },
                        {
                            "start": 1121,
                            "end": 1341
                        },
                        {
                            "start": 1341,
                            "end": 1632
                        },
                        {
                            "start": 1632,
                            "end": 1728
                        },
                        {
                            "start": 1730,
                            "end": 1756
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 921,
                            "end": 924,
                            "matchedPaperCorpusId": "257804696"
                        },
                        {
                            "start": 924,
                            "end": 927,
                            "matchedPaperCorpusId": "257378627"
                        },
                        {
                            "start": 951,
                            "end": 955,
                            "matchedPaperCorpusId": "257804696"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.580078125
                }
            ],
            "relevance_judgement": 0.580078125,
            "relevance_judgment_input_expanded": "# Title: DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems\n# Venue: Proceedings of the 4th International Workshop on Knowledge-Augmented Methods for Natural Language Processing\n# Authors: Anni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, Dong Yu\n## Abstract\nRecently, there has been a growing interest among large language model (LLM) developers in LLM-based document reading systems, which enable users to upload their own documents and pose questions related to the document contents, going beyond simple reading comprehension tasks. Consequently, these systems have been carefully designed to tackle challenges such as file parsing, metadata extraction, multi-modal information understanding and long-context reading. However, no current benchmark exists to evaluate their performance in such scenarios, where a raw file and questions are provided as input, and a corresponding response is expected as output. In this paper, we introduce DocBench, a new benchmark designed to evaluate LLM-based document reading systems. Our benchmark involves a meticulously crafted process, including the recruitment of human annotators and the generation of synthetic questions. It includes 229 real documents and 1,102 questions, spanning across five different domains and four major types of questions. We evaluate both proprietary LLM-based systems accessible via web interfaces or APIs, and a parse-then-read pipeline employing open-source LLMs. Our evaluations reveal noticeable gaps between existing LLM-based document reading systems and human performance, underscoring the challenges of developing proficient systems. To summarize, DocBench aims to establish a standardized benchmark for evaluating LLM-based document reading systems under diverse real-world scenarios, thereby guiding future advancements in this research area.\n## Evaluation Setup\nEvaluation Process Our dataset diversity poses two major evaluation challenges: (i) The evaluation methods vary depending on the answer type.For example, for boolean or numerical answers, a fair evaluator only needs to verify the correctness of a binary yes/no response or a specific number using simple techniques like string matching or number extraction.In contrast, textual responses require more nuanced standards such as natural language generation (NLG) metrics.Thus, accurately determining the appropriate evaluation method becomes complex when the answer type is unknown.\n\n(ii) Different LLMs and systems exhibit substantial variations in the organization and style of their outputs, potentially leading to biases in traditional evaluation approaches.Therefore, we capitalize on the prowess of LLMs that have proven to be decent evaluators and can be easily adapted to the assessment of various answer types [14,24,39].Inspired by Liu et al. [24], we clearly define the evaluation criteria for various types within the instruction prompt and then instruct GPT-4 to assign a score of 0 (incorrect) or 1 (correct).After evaluating 200 examples by both human evaluators and GPT-4, we found that the GPT-4 automatic evaluator shows a 98% agreement with human annotators, significantly exceeding the traditional string matching approach.Details of this experiment is shown in Table 3, and details of evaluation instruction prompts are attached in Appendix A. Metrics As mentioned above, we instruct GPT-4 to assign a score of 0 (incorrect) or 1 (correct), thus using Accuracy (abbreviated as Acc.) to measure system performance.We report accuracy across all instances, as well as for each domain and QA-pair type in Table 4.\n\n3 Experiments and Analysis",
            "reference_string": "[271212285 | Zou et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Learning the Bitter Lesson: Empirical Evidence from 20 Years of CVPR Proceedings",
            "venue": "NLP4SCIENCE",
            "year": 2024,
            "reference_count": 18,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.09649, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325908556",
                    "name": "Mojtaba Yousefi"
                },
                {
                    "authorId": "2325892124",
                    "name": "Jack Collins"
                }
            ],
            "abstract": "This study examines the alignment of Conference on Computer Vision and Pattern Recognition (CVPR) research with the principles of the \u201cbitter lesson\u201d proposed by Rich Sutton. We analyze two decades of CVPR abstracts and titles using large language models (LLMs) to assess the field\u2019s embracement of these principles. Our methodology leverages state-of-the-art natural language processing techniques to systematically evaluate the evolution of research approaches in computer vision. The results reveal significant trends in the adoption of general-purpose learning algorithms and the utilization of increased computational resources. We discuss the implications of these findings for the future direction of computer vision research and its potential impact on broader artificial intelligence development. This work contributes to the ongoing dialogue about the most effective strategies for advancing machine learning and computer vision, offering insights that may guide future research priorities and methodologies in the field.",
            "corpus_id": 273346449,
            "sentences": [
                {
                    "corpus_id": "273346449",
                    "title": "Learning the Bitter Lesson: Empirical Evidence from 20 Years of CVPR Proceedings",
                    "text": "The integration of Large Language Models (LLMs) into the evaluation of academic texts has emerged as a significant area of interest. LLMs, such as GPT-4, have demonstrated remarkable capabilities in processing and analyzing large volumes of information quickly, making them suitable for various applications, including the assessment of academic literature. For instance, research has shown that LLMs can effectively assist in title and abstract screening for literature reviews, which is crucial in the biomedical domain (Dennst\u00e4dt, 2024). Moreover, LLMs have been employed to perform qualitative data analysis, producing consistent results across multiple iterations (Tai et al., 2023). \n\nIn addition to their analytical capabilities, LLMs have been shown to possess a degree of human-like judgment in evaluating the quality of text. The G-EVAL framework, which utilizes LLMs to assess the quality of natural language generation outputs, demonstrates that LLMs can align closely with human evaluators in certain contexts (Liu, 2023). However, the deployment of LLMs in academic evaluation is not without challenges. LLMs can exhibit biases similar to those found in human judgments, which may affect the fairness and accuracy of their evaluations (Acerbi, 2023). Furthermore, the phenomenon of \"hallucination,\" where LLMs produce plausible but factually incorrect information, poses a risk in academic contexts (Buchanan, 2023). \n\nThe role of LLMs in answering questions and generating hypotheses also merits attention. Their ability to provide detailed responses to complex queries has been leveraged in various educational settings, enhancing learning experiences and fa-cilitating knowledge acquisition (Polverini, 2024). However, the tendency of LLMs to produce verbose outputs can sometimes obscure the clarity of their answers, necessitating careful prompt engineering (Yeadon, 2024). In the context of academic research, LLMs can assist in generating hypotheses and guiding exploratory studies, contributing to the advancement of knowledge in various fields (Irons et al., 2023). \n\nDespite the promising applications of LLMs in academic evaluation and research, it is crucial to establish ethical guidelines and best practices for their use.",
                    "score": 0.4447867327769749,
                    "section_title": "Large Language Models in Academic Evaluation",
                    "char_start_offset": 5394,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 132
                        },
                        {
                            "start": 133,
                            "end": 357
                        },
                        {
                            "start": 358,
                            "end": 540
                        },
                        {
                            "start": 541,
                            "end": 688
                        },
                        {
                            "start": 691,
                            "end": 835
                        },
                        {
                            "start": 836,
                            "end": 1035
                        },
                        {
                            "start": 1036,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1264
                        },
                        {
                            "start": 1265,
                            "end": 1430
                        },
                        {
                            "start": 1433,
                            "end": 1521
                        },
                        {
                            "start": 1522,
                            "end": 1726
                        },
                        {
                            "start": 1727,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 2088
                        },
                        {
                            "start": 2091,
                            "end": 2250
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1413,
                            "end": 1429,
                            "matchedPaperCorpusId": "265442718"
                        },
                        {
                            "start": 1708,
                            "end": 1725,
                            "matchedPaperCorpusId": "262083876"
                        },
                        {
                            "start": 1877,
                            "end": 1891,
                            "matchedPaperCorpusId": "261682569"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57763671875
                }
            ],
            "relevance_judgement": 0.57763671875,
            "relevance_judgment_input_expanded": "# Title: Learning the Bitter Lesson: Empirical Evidence from 20 Years of CVPR Proceedings\n# Venue: NLP4SCIENCE\n# Authors: Mojtaba Yousefi, Jack Collins\n## Abstract\nThis study examines the alignment of Conference on Computer Vision and Pattern Recognition (CVPR) research with the principles of the \u201cbitter lesson\u201d proposed by Rich Sutton. We analyze two decades of CVPR abstracts and titles using large language models (LLMs) to assess the field\u2019s embracement of these principles. Our methodology leverages state-of-the-art natural language processing techniques to systematically evaluate the evolution of research approaches in computer vision. The results reveal significant trends in the adoption of general-purpose learning algorithms and the utilization of increased computational resources. We discuss the implications of these findings for the future direction of computer vision research and its potential impact on broader artificial intelligence development. This work contributes to the ongoing dialogue about the most effective strategies for advancing machine learning and computer vision, offering insights that may guide future research priorities and methodologies in the field.\n## Large Language Models in Academic Evaluation\nThe integration of Large Language Models (LLMs) into the evaluation of academic texts has emerged as a significant area of interest. LLMs, such as GPT-4, have demonstrated remarkable capabilities in processing and analyzing large volumes of information quickly, making them suitable for various applications, including the assessment of academic literature. For instance, research has shown that LLMs can effectively assist in title and abstract screening for literature reviews, which is crucial in the biomedical domain (Dennst\u00e4dt, 2024). Moreover, LLMs have been employed to perform qualitative data analysis, producing consistent results across multiple iterations (Tai et al., 2023). \n\nIn addition to their analytical capabilities, LLMs have been shown to possess a degree of human-like judgment in evaluating the quality of text. The G-EVAL framework, which utilizes LLMs to assess the quality of natural language generation outputs, demonstrates that LLMs can align closely with human evaluators in certain contexts (Liu, 2023). However, the deployment of LLMs in academic evaluation is not without challenges. LLMs can exhibit biases similar to those found in human judgments, which may affect the fairness and accuracy of their evaluations (Acerbi, 2023). Furthermore, the phenomenon of \"hallucination,\" where LLMs produce plausible but factually incorrect information, poses a risk in academic contexts (Buchanan, 2023). \n\nThe role of LLMs in answering questions and generating hypotheses also merits attention. Their ability to provide detailed responses to complex queries has been leveraged in various educational settings, enhancing learning experiences and fa-cilitating knowledge acquisition (Polverini, 2024). However, the tendency of LLMs to produce verbose outputs can sometimes obscure the clarity of their answers, necessitating careful prompt engineering (Yeadon, 2024). In the context of academic research, LLMs can assist in generating hypotheses and guiding exploratory studies, contributing to the advancement of knowledge in various fields (Irons et al., 2023). \n\nDespite the promising applications of LLMs in academic evaluation and research, it is crucial to establish ethical guidelines and best practices for their use.",
            "reference_string": "[273346449 | Yousefi et al. | 2024 | Citations: 1]"
        },
        {
            "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 40,
            "citation_count": 1211,
            "influential_citation_count": 222,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2303.16634",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.16634, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "2110197273",
                    "name": "Yichong Xu"
                },
                {
                    "authorId": "2146294891",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "8233965",
                    "name": "Ruochen Xu"
                },
                {
                    "authorId": "8652308",
                    "name": "Chenguang Zhu"
                }
            ],
            "abstract": "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval",
            "corpus_id": 257804696,
            "sentences": [
                {
                    "corpus_id": "257804696",
                    "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                    "text": "The authors of the original paper found that inter-annotator agreement on judging human-written and LLM-generated summaries is very low, with Krippendorff's alpha at 0.07. \n\n2. G-EVAL may have a bias towards the LLMgenerated summaries because the model could share the same concept of evaluation criteria during generation and evaluation. \n\nOur work should be considered as a preliminary study on this issue, and more research is needed to fully understand the behavior of LLM-based evaluators to reduce its inherent bias towards LLMgenerated text. We highlight this concern in the context that LLM-based evaluators may lead to self-reinforcement of LLMs if the evaluation score is used as a reward signal for further tuning. And this could result in the over-fitting of the LLMs to their own evaluation criteria, rather than the true evaluation criteria of the NLG tasks. \n\nThe Effect of Chain-of-Thoughts We compare the performance of G-EVAL with and without chain-of-thoughts (CoT) on the SummEval benchmark. Table 1 shows that G-EVAL-4 with CoT has higher correlation than G-EVAL-4 without CoT on all dimensions, especially for fluency. This suggests that CoT can provide more context and guidance for the LLM to evaluate the generated text, and can also help to explain the evaluation process and results. And it is shown that CoT is more useful on consistency and fluency dimensions. We also provide results of G-EVAL with a simple prompting baseline on SummEval (only asking GPT-4 to score a summary from 1-5 on each dimension, without detailed task introduction, evaluation criteria and CoT). \n\nThe Effect of Probability Normalization We compare the performance of G-EVAL with and without probability normalization on the Sum-mEval benchmark. Table 1 shows that, on Kendall-Tau correlation, G-EVAL-4 with probabilities is inferior to G-EVAL-4 without probabilities on Sum-mEval. We believe this is related to the calculation of Kendall-Tau correlation, which is based on the number of concordant and discordant pairs.",
                    "score": 0.37190101367900774,
                    "section_title": "Analysis",
                    "char_start_offset": 16891,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 171
                        },
                        {
                            "start": 174,
                            "end": 338
                        },
                        {
                            "start": 341,
                            "end": 548
                        },
                        {
                            "start": 549,
                            "end": 725
                        },
                        {
                            "start": 726,
                            "end": 872
                        },
                        {
                            "start": 875,
                            "end": 1011
                        },
                        {
                            "start": 1012,
                            "end": 1140
                        },
                        {
                            "start": 1141,
                            "end": 1310
                        },
                        {
                            "start": 1311,
                            "end": 1389
                        },
                        {
                            "start": 1390,
                            "end": 1600
                        },
                        {
                            "start": 1603,
                            "end": 1750
                        },
                        {
                            "start": 1751,
                            "end": 1886
                        },
                        {
                            "start": 1887,
                            "end": 2025
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55078125
                }
            ],
            "relevance_judgement": 0.55078125,
            "relevance_judgment_input_expanded": "# Title: G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Yang Liu, Dan Iter, Yichong Xu, Shuo Wang, Ruochen Xu, Chenguang Zhu\n## Abstract\nThe quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval\n## Analysis\nThe authors of the original paper found that inter-annotator agreement on judging human-written and LLM-generated summaries is very low, with Krippendorff's alpha at 0.07. \n\n2. G-EVAL may have a bias towards the LLMgenerated summaries because the model could share the same concept of evaluation criteria during generation and evaluation. \n\nOur work should be considered as a preliminary study on this issue, and more research is needed to fully understand the behavior of LLM-based evaluators to reduce its inherent bias towards LLMgenerated text. We highlight this concern in the context that LLM-based evaluators may lead to self-reinforcement of LLMs if the evaluation score is used as a reward signal for further tuning. And this could result in the over-fitting of the LLMs to their own evaluation criteria, rather than the true evaluation criteria of the NLG tasks. \n\nThe Effect of Chain-of-Thoughts We compare the performance of G-EVAL with and without chain-of-thoughts (CoT) on the SummEval benchmark. Table 1 shows that G-EVAL-4 with CoT has higher correlation than G-EVAL-4 without CoT on all dimensions, especially for fluency. This suggests that CoT can provide more context and guidance for the LLM to evaluate the generated text, and can also help to explain the evaluation process and results. And it is shown that CoT is more useful on consistency and fluency dimensions. We also provide results of G-EVAL with a simple prompting baseline on SummEval (only asking GPT-4 to score a summary from 1-5 on each dimension, without detailed task introduction, evaluation criteria and CoT). \n\nThe Effect of Probability Normalization We compare the performance of G-EVAL with and without probability normalization on the Sum-mEval benchmark. Table 1 shows that, on Kendall-Tau correlation, G-EVAL-4 with probabilities is inferior to G-EVAL-4 without probabilities on Sum-mEval. We believe this is related to the calculation of Kendall-Tau correlation, which is based on the number of concordant and discordant pairs.",
            "reference_string": "[257804696 | Liu et al. | 2023 | Citations: 1211]"
        },
        {
            "title": "Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 17,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.00507, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256995981",
                    "name": "Shichao Sun"
                },
                {
                    "authorId": "2273661632",
                    "name": "Ruifeng Yuan"
                },
                {
                    "authorId": "2314396",
                    "name": "Ziqiang Cao"
                },
                {
                    "authorId": "2237591981",
                    "name": "Wenjie Li"
                },
                {
                    "authorId": "2256991660",
                    "name": "Pengfei Liu"
                }
            ],
            "abstract": "Large language models (LLMs) have demonstrated the capacity to improve summary quality by mirroring a human-like iterative process of critique and refinement starting from the initial draft. Two strategies are designed to perform this iterative process: Prompt Chaining and Stepwise Prompt. Prompt chaining orchestrates the drafting, critiquing, and refining phases through a series of three discrete prompts, while Stepwise prompt integrates these phases within a single prompt. However, the relative effectiveness of the two methods has not been extensively studied. This paper is dedicated to examining and comparing these two methods in the context of text summarization to ascertain which method stands out as the most effective. Experimental results show that the prompt chaining method can produce a more favorable outcome. This might be because stepwise prompt might produce a simulated refinement process according to our various experiments. Since refinement is adaptable to diverse tasks, our conclusions have the potential to be extrapolated to other applications, thereby offering insights that may contribute to the broader development of LLMs.",
            "corpus_id": 270218703,
            "sentences": [
                {
                    "corpus_id": "270218703",
                    "title": "Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization",
                    "text": "Refinement can be powered by various LLMs.In this paper, we choose the newest versions of GPT-3.5 (gpt-3.5-turbo-0125)and GPT-4 (gpt-4-0125-preview) models from OpenAI4 to draft, critique, and refine the outcomes due to their strong instruction-following capabilities.We also explore the performance of a strong open-source LLM (Mixtral 8\u00d77B (Jiang et al., 2024)).\n\nWe use the LLMCompare as our evaluation protocol, which compares two candidate outputs and then selects the better one (Zheng et al., 2023;Wang et al., 2023).This is because LLMCompare coupled with GPT-4 is the best evaluation protocol, as mentioned in Liu et al. (2023).The evaluation prompts are shown in Appendix A.\n\nWe evaluate the generated summaries from the three quality dimensions as introduced in Liu et al. ( 2023): (1) Overall Quality measures the overall excellence of the summary following the summary requirements.(2) Missing Information assesses whether the summary omits any essential article details pertinent to the summary requirements.\n\n(3) Irrelevant Information examines whether the summary contains extraneous information that falls outside the scope of the summary requirements.",
                    "score": 0.3932049090103612,
                    "section_title": "Models and Metrics",
                    "char_start_offset": 5615,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 42
                        },
                        {
                            "start": 42,
                            "end": 118
                        },
                        {
                            "start": 118,
                            "end": 268
                        },
                        {
                            "start": 268,
                            "end": 364
                        },
                        {
                            "start": 366,
                            "end": 524
                        },
                        {
                            "start": 524,
                            "end": 637
                        },
                        {
                            "start": 637,
                            "end": 684
                        },
                        {
                            "start": 686,
                            "end": 895
                        },
                        {
                            "start": 895,
                            "end": 1022
                        },
                        {
                            "start": 1024,
                            "end": 1169
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 485,
                            "end": 505,
                            "matchedPaperCorpusId": "259129398"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5498046875
                }
            ],
            "relevance_judgement": 0.5498046875,
            "relevance_judgment_input_expanded": "# Title: Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Shichao Sun, Ruifeng Yuan, Ziqiang Cao, Wenjie Li, Pengfei Liu\n## Abstract\nLarge language models (LLMs) have demonstrated the capacity to improve summary quality by mirroring a human-like iterative process of critique and refinement starting from the initial draft. Two strategies are designed to perform this iterative process: Prompt Chaining and Stepwise Prompt. Prompt chaining orchestrates the drafting, critiquing, and refining phases through a series of three discrete prompts, while Stepwise prompt integrates these phases within a single prompt. However, the relative effectiveness of the two methods has not been extensively studied. This paper is dedicated to examining and comparing these two methods in the context of text summarization to ascertain which method stands out as the most effective. Experimental results show that the prompt chaining method can produce a more favorable outcome. This might be because stepwise prompt might produce a simulated refinement process according to our various experiments. Since refinement is adaptable to diverse tasks, our conclusions have the potential to be extrapolated to other applications, thereby offering insights that may contribute to the broader development of LLMs.\n## Models and Metrics\nRefinement can be powered by various LLMs.In this paper, we choose the newest versions of GPT-3.5 (gpt-3.5-turbo-0125)and GPT-4 (gpt-4-0125-preview) models from OpenAI4 to draft, critique, and refine the outcomes due to their strong instruction-following capabilities.We also explore the performance of a strong open-source LLM (Mixtral 8\u00d77B (Jiang et al., 2024)).\n\nWe use the LLMCompare as our evaluation protocol, which compares two candidate outputs and then selects the better one (Zheng et al., 2023;Wang et al., 2023).This is because LLMCompare coupled with GPT-4 is the best evaluation protocol, as mentioned in Liu et al. (2023).The evaluation prompts are shown in Appendix A.\n\nWe evaluate the generated summaries from the three quality dimensions as introduced in Liu et al. ( 2023): (1) Overall Quality measures the overall excellence of the summary following the summary requirements.(2) Missing Information assesses whether the summary omits any essential article details pertinent to the summary requirements.\n\n(3) Irrelevant Information examines whether the summary contains extraneous information that falls outside the scope of the summary requirements.",
            "reference_string": "[270218703 | Sun et al. | 2024 | Citations: 20]"
        },
        {
            "title": "Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting",
            "venue": "Journal of Biomedical Informatics",
            "year": 2023,
            "reference_count": 45,
            "citation_count": 18,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.16275",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.16275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118898475",
                    "name": "Yiwen Shi"
                },
                {
                    "authorId": "2061530524",
                    "name": "Ping Ren"
                },
                {
                    "authorId": "2152438838",
                    "name": "Jing Wang"
                },
                {
                    "authorId": "2220800269",
                    "name": "Biao Han"
                },
                {
                    "authorId": "65986913",
                    "name": "Taha ValizadehAslani"
                },
                {
                    "authorId": "1997966680",
                    "name": "Felix Agbavor"
                },
                {
                    "authorId": "2153915340",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "144642598",
                    "name": "Meng Hu"
                },
                {
                    "authorId": "144010790",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "2220924999",
                    "name": "Hualou Liang"
                }
            ],
            "abstract": "Food effect summarization from New Drug Application (NDA) is an essential component of product-specific guidance (PSG) development and assessment, which provides the basis of recommendations for fasting and fed bioequivalence studies to guide the pharmaceutical industry for developing generic drug products. However, manual summarization of food effect from extensive drug application review documents is time-consuming. Therefore, there is a need to develop automated methods to generate food effect summary. Recent advances in natural language processing (NLP), particularly large language models (LLMs) such as ChatGPT and GPT-4, have demonstrated great potential in improving the effectiveness of automated text summarization, but its ability with regard to the accuracy in summarizing food effect for PSG assessment remains unclear. In this study, we introduce a simple yet effective approach,iterative prompting, which allows one to interact with ChatGPT or GPT-4 more effectively and efficiently through multi-turn interaction. Specifically, we propose a three-turn iterative prompting approach to food effect summarization in which the keyword-focused and length-controlled prompts are respectively provided in consecutive turns to refine the quality of the generated summary. We conduct a series of extensive evaluations, ranging from automated metrics to FDA professionals and even evaluation by GPT-4, on 100 NDA review documents selected over the past five years. We observe that the summary quality is progressively improved throughout the iterative prompting process. Moreover, we find that GPT-4 performs better than ChatGPT, as evaluated by FDA professionals (43% vs. 12%) and GPT-4 (64% vs. 35%). Importantly, all the FDA professionals unanimously rated that 85% of the summaries generated by GPT-4 are factually consistent with the golden reference summary, a finding further supported by GPT-4 rating of 72% consistency. Taken together, these results strongly suggest a great potential for GPT-4 to draft food effect summaries that could be reviewed by FDA professionals, thereby improving the efficiency of the PSG assessment cycle and promoting generic drug product development.",
            "corpus_id": 259274988,
            "sentences": [
                {
                    "corpus_id": "259274988",
                    "title": "Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting",
                    "text": "Can GPT-4 be used in automatic evaluation? \n\nSince the food effect summarization would be used to assist PSG assessment, it is imperative for FDA professionals to evaluate the quality of the summary, though the evaluation by human is quite laborious and expensive. In this work, we find that GPT-4 evaluation provides a cheap and reasonable alternative to human evaluation. When using GPT-4 to rate the performance of different models against the reference summary, there are a number of issues. First, there exists an inconsistency problem whereby there is no guarantee that an LLM will produce the identical output for the same input every time. A good practice to fix the problem is to set temperature = 0; the temperature parameter of the model controls the randomness of the text generated, with 0 being deterministic. Second, we find a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt. To mitigate such recency bias, we recommend reporting the mean score over both orders. Third, we observe from Figure 4 that GPT-4 assigns significantly higher scores to its own outputs compared to human ratings (64% vs. 43%), which represents the propensity of the model to favor its own suggestions. Future work should investigate the existence of potential biases in GPT-4 evaluation as well as possible mitigation strategies. \n\nDespite the potential of GPT-4 for automating evaluation, it is not as flexible as humans, particularly in situations where specific definitions of summary quality are not provided. The performance of GPT-4 hence may vary due to the lack of clarity in evaluation standards. GPT-4 aligns with the human evaluation, concluding that it generates better summaries than ChatGPT. However, GPT-4 encounters difficulty distinguishing scenarios where both models produce equally good summaries. In cases where human assessors perceive the summaries as equally good, GPT-4 may favor a summary that includes relevant contextual information does not present in the other, such as \"explicitly linking it to a high-fat, high-calorie meal\" or \"mention the lack of clinically significant differences\". It also may penalize certain aspects based on subjective criteria, such as \"less concise\".",
                    "score": 0.42768764564373063,
                    "section_title": "Which model performs better in the study?",
                    "char_start_offset": 33838,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 42
                        },
                        {
                            "start": 45,
                            "end": 264
                        },
                        {
                            "start": 265,
                            "end": 373
                        },
                        {
                            "start": 374,
                            "end": 495
                        },
                        {
                            "start": 496,
                            "end": 647
                        },
                        {
                            "start": 648,
                            "end": 823
                        },
                        {
                            "start": 824,
                            "end": 945
                        },
                        {
                            "start": 946,
                            "end": 1032
                        },
                        {
                            "start": 1033,
                            "end": 1246
                        },
                        {
                            "start": 1247,
                            "end": 1374
                        },
                        {
                            "start": 1377,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1650
                        },
                        {
                            "start": 1651,
                            "end": 1750
                        },
                        {
                            "start": 1751,
                            "end": 1862
                        },
                        {
                            "start": 1863,
                            "end": 2162
                        },
                        {
                            "start": 2163,
                            "end": 2253
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.54150390625
                }
            ],
            "relevance_judgement": 0.54150390625,
            "relevance_judgment_input_expanded": "# Title: Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting\n# Venue: Journal of Biomedical Informatics\n# Authors: Yiwen Shi, Ping Ren, Jing Wang, Biao Han, Taha ValizadehAslani, Felix Agbavor, Yi Zhang, Meng Hu, Liang Zhao, Hualou Liang\n## Abstract\nFood effect summarization from New Drug Application (NDA) is an essential component of product-specific guidance (PSG) development and assessment, which provides the basis of recommendations for fasting and fed bioequivalence studies to guide the pharmaceutical industry for developing generic drug products. However, manual summarization of food effect from extensive drug application review documents is time-consuming. Therefore, there is a need to develop automated methods to generate food effect summary. Recent advances in natural language processing (NLP), particularly large language models (LLMs) such as ChatGPT and GPT-4, have demonstrated great potential in improving the effectiveness of automated text summarization, but its ability with regard to the accuracy in summarizing food effect for PSG assessment remains unclear. In this study, we introduce a simple yet effective approach,iterative prompting, which allows one to interact with ChatGPT or GPT-4 more effectively and efficiently through multi-turn interaction. Specifically, we propose a three-turn iterative prompting approach to food effect summarization in which the keyword-focused and length-controlled prompts are respectively provided in consecutive turns to refine the quality of the generated summary. We conduct a series of extensive evaluations, ranging from automated metrics to FDA professionals and even evaluation by GPT-4, on 100 NDA review documents selected over the past five years. We observe that the summary quality is progressively improved throughout the iterative prompting process. Moreover, we find that GPT-4 performs better than ChatGPT, as evaluated by FDA professionals (43% vs. 12%) and GPT-4 (64% vs. 35%). Importantly, all the FDA professionals unanimously rated that 85% of the summaries generated by GPT-4 are factually consistent with the golden reference summary, a finding further supported by GPT-4 rating of 72% consistency. Taken together, these results strongly suggest a great potential for GPT-4 to draft food effect summaries that could be reviewed by FDA professionals, thereby improving the efficiency of the PSG assessment cycle and promoting generic drug product development.\n## Which model performs better in the study?\nCan GPT-4 be used in automatic evaluation? \n\nSince the food effect summarization would be used to assist PSG assessment, it is imperative for FDA professionals to evaluate the quality of the summary, though the evaluation by human is quite laborious and expensive. In this work, we find that GPT-4 evaluation provides a cheap and reasonable alternative to human evaluation. When using GPT-4 to rate the performance of different models against the reference summary, there are a number of issues. First, there exists an inconsistency problem whereby there is no guarantee that an LLM will produce the identical output for the same input every time. A good practice to fix the problem is to set temperature = 0; the temperature parameter of the model controls the randomness of the text generated, with 0 being deterministic. Second, we find a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt. To mitigate such recency bias, we recommend reporting the mean score over both orders. Third, we observe from Figure 4 that GPT-4 assigns significantly higher scores to its own outputs compared to human ratings (64% vs. 43%), which represents the propensity of the model to favor its own suggestions. Future work should investigate the existence of potential biases in GPT-4 evaluation as well as possible mitigation strategies. \n\nDespite the potential of GPT-4 for automating evaluation, it is not as flexible as humans, particularly in situations where specific definitions of summary quality are not provided. The performance of GPT-4 hence may vary due to the lack of clarity in evaluation standards. GPT-4 aligns with the human evaluation, concluding that it generates better summaries than ChatGPT. However, GPT-4 encounters difficulty distinguishing scenarios where both models produce equally good summaries. In cases where human assessors perceive the summaries as equally good, GPT-4 may favor a summary that includes relevant contextual information does not present in the other, such as \"explicitly linking it to a high-fat, high-calorie meal\" or \"mention the lack of clinically significant differences\". It also may penalize certain aspects based on subjective criteria, such as \"less concise\".",
            "reference_string": "[259274988 | Shi et al. | 2023 | Citations: 18]"
        },
        {
            "title": "Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 50,
            "citation_count": 32,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.09369",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.09369, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1420116116",
                    "name": "Kung-Hsiang Huang"
                },
                {
                    "authorId": "46180754",
                    "name": "Philippe Laban"
                },
                {
                    "authorId": "22281632",
                    "name": "A. R. Fabbri"
                },
                {
                    "authorId": "3466801",
                    "name": "Prafulla Kumar Choubey"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "30340989",
                    "name": "Chien-Sheng Wu"
                }
            ],
            "abstract": "Previous research in multi-document news summarization has typically concentrated on collating information that all sources agree upon. However, the summarization of diverse information dispersed across multiple articles about an event remains underexplored. In this paper, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Next, to enable consistent automatic evaluation, we conducted a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of summaries. Through correlation analyses, we outline the best practices for effectively using automatic LLM-based metrics on the DiverseSumm dataset. Finally, we study how LLMs summarize multiple news articles by analyzing which type of diverse information LLMs are capable of identifying. Our analyses suggest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover under 40% of the diverse information on average.",
            "corpus_id": 262045067,
            "sentences": [
                {
                    "corpus_id": "262045067",
                    "title": "Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles",
                    "text": "For this research question, we aim to provide the best practices when using GPT-4 as the evaluator for the MDDS task by conducting bias and correlation analyses. We focus on two major biases: position bias (i.e., whether the LLM evaluator favors certain positions over others) and verbosity bias (i.e. whether the LLM evaluator prefers shorter or longer texts). For all the experiments conducted in this analysis, we investigated summaries produced by GPT-4, GPT-3.5-Turbo, Vicuna-7B, and LongChat-7B-16K. The details of our prompts for the below experiments can be found in Appendix C.3. \n\nPosition Bias Position bias is most relevant to the pairwise comparison protocol. While previous work has shown that GPT-4 does exhibit position bias when used to assess text quality in conversational-focused tasks (Wang et al., 2023;Zheng et al., 2023), none of the prior studies have investigated whether such bias is also observed when evaluating faithfulness or coverage. To analyze position bias, we task GPT-4 with assessing a pair of summaries generated by two LLMs on which one is better, and then swap the positions of these two summaries and query GPT-4 again. We compute the percentage of times GPT-4 prefers the first or second summaries. \n\nWhen GPT-4 compared pairs of LLM-generated summaries to evaluate faithfulness and coverage, a strong position bias surfaced, favoring the second entry (Table 3). Position bias was particularly pronounced when assessing similar-quality summaries (see Figure 23a). Hence, we deduce that GPT-4 is unreliable when utilized as a pairwise evaluator in the MDDS task with respect to faithfulness and coverage. Interestingly, this outcome contradicts Zheng et al. (2023), implying that the position of bias for LLM-based evaluators could vary across different tasks. A breakdown of the position bias analysis can be found in Appendix D. \n\nVerbosity Bias To assess the verbosity bias of GPT-4 as an evaluator, we create extended summaries that maintain the semantic meaning.",
                    "score": 0.47070763396468385,
                    "section_title": "Analysis",
                    "char_start_offset": 16082,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 301
                        },
                        {
                            "start": 302,
                            "end": 361
                        },
                        {
                            "start": 362,
                            "end": 473
                        },
                        {
                            "start": 474,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 588
                        },
                        {
                            "start": 591,
                            "end": 672
                        },
                        {
                            "start": 673,
                            "end": 966
                        },
                        {
                            "start": 967,
                            "end": 1161
                        },
                        {
                            "start": 1162,
                            "end": 1241
                        },
                        {
                            "start": 1244,
                            "end": 1405
                        },
                        {
                            "start": 1406,
                            "end": 1506
                        },
                        {
                            "start": 1507,
                            "end": 1646
                        },
                        {
                            "start": 1647,
                            "end": 1802
                        },
                        {
                            "start": 1803,
                            "end": 1872
                        },
                        {
                            "start": 1875,
                            "end": 2009
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.541015625
                }
            ],
            "relevance_judgement": 0.541015625,
            "relevance_judgment_input_expanded": "# Title: Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Kung-Hsiang Huang, Philippe Laban, A. R. Fabbri, Prafulla Kumar Choubey, Shafiq R. Joty, Caiming Xiong, Chien-Sheng Wu\n## Abstract\nPrevious research in multi-document news summarization has typically concentrated on collating information that all sources agree upon. However, the summarization of diverse information dispersed across multiple articles about an event remains underexplored. In this paper, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Next, to enable consistent automatic evaluation, we conducted a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of summaries. Through correlation analyses, we outline the best practices for effectively using automatic LLM-based metrics on the DiverseSumm dataset. Finally, we study how LLMs summarize multiple news articles by analyzing which type of diverse information LLMs are capable of identifying. Our analyses suggest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover under 40% of the diverse information on average.\n## Analysis\nFor this research question, we aim to provide the best practices when using GPT-4 as the evaluator for the MDDS task by conducting bias and correlation analyses. We focus on two major biases: position bias (i.e., whether the LLM evaluator favors certain positions over others) and verbosity bias (i.e. whether the LLM evaluator prefers shorter or longer texts). For all the experiments conducted in this analysis, we investigated summaries produced by GPT-4, GPT-3.5-Turbo, Vicuna-7B, and LongChat-7B-16K. The details of our prompts for the below experiments can be found in Appendix C.3. \n\nPosition Bias Position bias is most relevant to the pairwise comparison protocol. While previous work has shown that GPT-4 does exhibit position bias when used to assess text quality in conversational-focused tasks (Wang et al., 2023;Zheng et al., 2023), none of the prior studies have investigated whether such bias is also observed when evaluating faithfulness or coverage. To analyze position bias, we task GPT-4 with assessing a pair of summaries generated by two LLMs on which one is better, and then swap the positions of these two summaries and query GPT-4 again. We compute the percentage of times GPT-4 prefers the first or second summaries. \n\nWhen GPT-4 compared pairs of LLM-generated summaries to evaluate faithfulness and coverage, a strong position bias surfaced, favoring the second entry (Table 3). Position bias was particularly pronounced when assessing similar-quality summaries (see Figure 23a). Hence, we deduce that GPT-4 is unreliable when utilized as a pairwise evaluator in the MDDS task with respect to faithfulness and coverage. Interestingly, this outcome contradicts Zheng et al. (2023), implying that the position of bias for LLM-based evaluators could vary across different tasks. A breakdown of the position bias analysis can be found in Appendix D. \n\nVerbosity Bias To assess the verbosity bias of GPT-4 as an evaluator, we create extended summaries that maintain the semantic meaning.",
            "reference_string": "[262045067 | Huang et al. | 2023 | Citations: 32]"
        },
        {
            "title": "ChatGPT applications in Academic Research: A Review of Benefits, Concerns, and Recommendations",
            "venue": "bioRxiv",
            "year": 2023,
            "reference_count": 183,
            "citation_count": 18,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.biorxiv.org/content/biorxiv/early/2023/08/18/2023.08.17.553688.full.pdf",
                "status": "GREEN",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2023.08.17.553688?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2023.08.17.553688, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "83629705",
                    "name": "A. Alzaabi"
                },
                {
                    "authorId": "2232790162",
                    "name": "Amira ALAmri"
                },
                {
                    "authorId": "7703354",
                    "name": "Halima Albalushi"
                },
                {
                    "authorId": "2232790344",
                    "name": "Ruqaya Aljabri"
                },
                {
                    "authorId": "46203693",
                    "name": "A. AAlAbdulsalam"
                }
            ],
            "abstract": "Background ChatGPT has emerged as a valuable tool for enhancing scientific writing. It is the first openly available Large Language Model (LLM) with unrestricted access to its capabilities. ChatGPT has the potential to alleviate researchers\u2019 workload and enhance various aspects of research, from planning to execution and presentation. However, due to the rapid growth of publications and diverse opinions surrounding ChatGPT, a comprehensive review is necessary to understand its benefits, risks, and safe utilization in scientific research. This review aims to provide a comprehensive overview of the topic by extensively examining existing literature on the utilization of ChatGPT in academic research. The goal is to gain insights into the potential benefits and risks of using ChatGPT in scientific research, exploring secure and efficient methods for its application while identifying potential pitfalls to minimize negative consequences. Method The search was conducted in PubMed/MEDLINE, SCOPUS, and Google Scholar, yielding a total of 1279 articles and concluded on April 23rd, 2023. After full screening of titles/abstracts and removing duplicates and irrelevant articles, a total of 181 articles were included for analysis. Information collected included publication details, purposes, benefits, risks, and recommendation regarding ChatGPT\u2019s use in scientific research. Results The majority of existing literature consists of editorials expressing thoughts and concerns, followed by original research articles analyzing ChatGPT\u2019s performance in scientific research. The most significant advantage of using ChatGPT in scientific writing is its ability to expedite the writing process, enabling researchers to draft their work more efficiently. It also proves beneficial in improving writing style and proofreading by offering suggestions for sentence structure, grammar, and overall clarity. Additional benefits identified include support in data analysis, the formulation of protocols for clinical trials, and the design of scientific studies. Concerns mainly revolve around the accuracy and superficiality of the generated content, leading to what is referred to as \u201challucinations.\u201d Researchers have also expressed concerns about the tool providing citations to nonexistent sources. Other concerns discussed include authorship and plagiarism issues, accountability, copyright considerations, potential loss of diverse writing styles, privacy and security, transparency, credibility, validity, presence of bias, and the potential impact on scientific progress, such as a decrease in groundbreaking discoveries. Conclusion ChatGPT has the potential to revolutionize scientific writing as a valuable tool for researchers. However, it cannot replace human expertise and critical thinking. Researchers must exercise caution, ensuring the generated content complements their own knowledge. Ethical standards should be upheld, involving knowledgeable human researchers to avoid biases and inaccuracies. Collaboration among stakeholders and training on AI technology are essential for identifying best practices in LLMs use and maintaining scientific integrity.",
            "corpus_id": 261050476,
            "sentences": [
                {
                    "corpus_id": "261050476",
                    "title": "ChatGPT applications in Academic Research: A Review of Benefits, Concerns, and Recommendations",
                    "text": "ChatGPT has the potential to revolutionize scientific writing by assisting researchers in efficiently producing high-quality and well-written articles. However, it is important to acknowledge that ChatGPT cannot replace the expertise of researchers but can effectively streamline the writing process. Researchers should be cautious of the potential risks associated with using ChatGPT in scientific writing and should consider the generated content as a tool to complement their own critical thinking and expertise. Additionally, relying solely on AI without the involvement of knowledgeable human researchers may perpetuate biases and inaccuracies in the data, resulting in unfair outcomes and hindering scientific progress. Studies have demonstrated that language models like GPT-3, trained on large web-based datasets, can exhibit biases related to gender, race, ethnicity, and disability status. Hence, despite the impressive advancements in AI tools, the presence of experienced experts remains essential in scientific activities and writing to ensure the quality of the work. In light of these considerations, it is advisable to harness the full potential of ChatGPT as a linguistic tool while upholding rigorous ethical standards. This approach allows researchers to benefit from its capabilities while maintaining the integrity of scientific practices. Regardless of how AI is employed, we strongly believe that involving domain experts in scientific activities and writing is crucial to uphold the quality of the work. Furthermore, the rapid progress of AI tools may lead to some researchers achieving a significant increase in publication numbers without a corresponding growth in their actual expertise in the field. This can raise ethical concerns when academic institutions prioritize publication quantity over quality, potentially impacting the hiring process for professionals. Ultimately, the influence of language models on scientific writing will depend on their adoption and utilization within the scientific community. \n\nIt is recommended to use ChatGPT as a supplemental tool for constructive writing, reviewing materials, and rephrasing text rather than relying on it for providing an entirely original blueprint.",
                    "score": 0.41452488709316626,
                    "section_title": "Conclusion",
                    "char_start_offset": 34152,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 300
                        },
                        {
                            "start": 301,
                            "end": 515
                        },
                        {
                            "start": 516,
                            "end": 725
                        },
                        {
                            "start": 726,
                            "end": 899
                        },
                        {
                            "start": 900,
                            "end": 1081
                        },
                        {
                            "start": 1082,
                            "end": 1237
                        },
                        {
                            "start": 1238,
                            "end": 1360
                        },
                        {
                            "start": 1361,
                            "end": 1527
                        },
                        {
                            "start": 1528,
                            "end": 1727
                        },
                        {
                            "start": 1728,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 2038
                        },
                        {
                            "start": 2041,
                            "end": 2235
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53955078125
                }
            ],
            "relevance_judgement": 0.53955078125,
            "relevance_judgment_input_expanded": "# Title: ChatGPT applications in Academic Research: A Review of Benefits, Concerns, and Recommendations\n# Venue: bioRxiv\n# Authors: A. Alzaabi, Amira ALAmri, Halima Albalushi, Ruqaya Aljabri, A. AAlAbdulsalam\n## Abstract\nBackground ChatGPT has emerged as a valuable tool for enhancing scientific writing. It is the first openly available Large Language Model (LLM) with unrestricted access to its capabilities. ChatGPT has the potential to alleviate researchers\u2019 workload and enhance various aspects of research, from planning to execution and presentation. However, due to the rapid growth of publications and diverse opinions surrounding ChatGPT, a comprehensive review is necessary to understand its benefits, risks, and safe utilization in scientific research. This review aims to provide a comprehensive overview of the topic by extensively examining existing literature on the utilization of ChatGPT in academic research. The goal is to gain insights into the potential benefits and risks of using ChatGPT in scientific research, exploring secure and efficient methods for its application while identifying potential pitfalls to minimize negative consequences. Method The search was conducted in PubMed/MEDLINE, SCOPUS, and Google Scholar, yielding a total of 1279 articles and concluded on April 23rd, 2023. After full screening of titles/abstracts and removing duplicates and irrelevant articles, a total of 181 articles were included for analysis. Information collected included publication details, purposes, benefits, risks, and recommendation regarding ChatGPT\u2019s use in scientific research. Results The majority of existing literature consists of editorials expressing thoughts and concerns, followed by original research articles analyzing ChatGPT\u2019s performance in scientific research. The most significant advantage of using ChatGPT in scientific writing is its ability to expedite the writing process, enabling researchers to draft their work more efficiently. It also proves beneficial in improving writing style and proofreading by offering suggestions for sentence structure, grammar, and overall clarity. Additional benefits identified include support in data analysis, the formulation of protocols for clinical trials, and the design of scientific studies. Concerns mainly revolve around the accuracy and superficiality of the generated content, leading to what is referred to as \u201challucinations.\u201d Researchers have also expressed concerns about the tool providing citations to nonexistent sources. Other concerns discussed include authorship and plagiarism issues, accountability, copyright considerations, potential loss of diverse writing styles, privacy and security, transparency, credibility, validity, presence of bias, and the potential impact on scientific progress, such as a decrease in groundbreaking discoveries. Conclusion ChatGPT has the potential to revolutionize scientific writing as a valuable tool for researchers. However, it cannot replace human expertise and critical thinking. Researchers must exercise caution, ensuring the generated content complements their own knowledge. Ethical standards should be upheld, involving knowledgeable human researchers to avoid biases and inaccuracies. Collaboration among stakeholders and training on AI technology are essential for identifying best practices in LLMs use and maintaining scientific integrity.\n## Conclusion\nChatGPT has the potential to revolutionize scientific writing by assisting researchers in efficiently producing high-quality and well-written articles. However, it is important to acknowledge that ChatGPT cannot replace the expertise of researchers but can effectively streamline the writing process. Researchers should be cautious of the potential risks associated with using ChatGPT in scientific writing and should consider the generated content as a tool to complement their own critical thinking and expertise. Additionally, relying solely on AI without the involvement of knowledgeable human researchers may perpetuate biases and inaccuracies in the data, resulting in unfair outcomes and hindering scientific progress. Studies have demonstrated that language models like GPT-3, trained on large web-based datasets, can exhibit biases related to gender, race, ethnicity, and disability status. Hence, despite the impressive advancements in AI tools, the presence of experienced experts remains essential in scientific activities and writing to ensure the quality of the work. In light of these considerations, it is advisable to harness the full potential of ChatGPT as a linguistic tool while upholding rigorous ethical standards. This approach allows researchers to benefit from its capabilities while maintaining the integrity of scientific practices. Regardless of how AI is employed, we strongly believe that involving domain experts in scientific activities and writing is crucial to uphold the quality of the work. Furthermore, the rapid progress of AI tools may lead to some researchers achieving a significant increase in publication numbers without a corresponding growth in their actual expertise in the field. This can raise ethical concerns when academic institutions prioritize publication quantity over quality, potentially impacting the hiring process for professionals. Ultimately, the influence of language models on scientific writing will depend on their adoption and utilization within the scientific community. \n\nIt is recommended to use ChatGPT as a supplemental tool for constructive writing, reviewing materials, and rephrasing text rather than relying on it for providing an entirely original blueprint.",
            "reference_string": "[261050476 | Alzaabi et al. | 2023 | Citations: 18]"
        },
        {
            "title": "Simulating Expert Discussions with Multi-agent for Enhanced Scientific Problem Solving",
            "venue": "SDP",
            "year": 2024,
            "reference_count": 32,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.sdp-1.23, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2315367785",
                    "name": "Ziyue Li"
                },
                {
                    "authorId": "2315438513",
                    "name": "Yuan Chang"
                },
                {
                    "authorId": "2315309245",
                    "name": "Xiaoqiu Le"
                }
            ],
            "abstract": "Large Language Models (LLMs) have shown remarkable potential across various domains, yet their application in addressing complex scientific problems remains a formidable challenge. This paper presents a novel methodology to augment the problem-solving capabilities of LLMs by assigning them roles as domain-specific experts. By simulating a panel of experts, each LLM is tasked with delivering professional and cautious responses to scientific inquiries. Our approach involves querying multiple LLMs and assessing the consistency of their responses. High agreement among the LLMs suggests greater confidence in the proposed solution, whereas discrepancies prompt a collaborative discussion among the LLMs to reach a consensus. This method emulates real-world scientific problem-solving processes, fostering a more reliable and robust mechanism for LLMs to tackle scientific questions. Our experimental results show that assigning roles to multiple LLMs as domain-specific experts significantly improves their accuracy and reliability in solving scientific problems. This framework has the potential to advance the application of AI in scientific research, enhancing its effectiveness and trustworthiness.",
            "corpus_id": 271769623,
            "sentences": [
                {
                    "corpus_id": "271769623",
                    "title": "Simulating Expert Discussions with Multi-agent for Enhanced Scientific Problem Solving",
                    "text": "Solution Quality In addition to evaluating the accuracy of the models, we also assess the quality of the generated solutions. We randomly sample 100 problems and evaluate the solutions using LLMs and human evaluation based on three criteria: (1) the correctness of the reasoning steps, (2) the clarity of the explanations, and (3) the appropriateness of the mathematical notations and symbols used. Each criterion was rated on a scale of 1 to 5, with 5 being the highest quality. \n\nFor the human evaluation, we employed three expert annotators. To ensure reliability, we calculated the inter-annotator agreement using Fleiss' kappa (Fleiss, 1971) for each of the three criteria: \n\n\u2022 Correctness of reasoning steps: \u03ba = 0.71 \n\n\u2022 Clarity of explanations: \u03ba = 0.62 \n\n\u2022 Appropriateness of mathematical notations and symbols: \u03ba = 0.55 \n\nThe overall average kappa value was 0.63, indicating substantial agreement among the annotators. \n\nThe detailed prompts for LLM evaluation and the specific guidelines for human evaluation are provided in Appendix B. Table 2 presents the average quality scores for solutions from GPT-4. Compared to baseline, SEDM consistently achieves higher quality scores in both LLM and human evaluations. The solutions generated by SEDM demonstrate clearer reasoning steps, more coherent explanations, and more precise use of mathematical notations. This suggests that the multi-expert discussion framework not only improves the accuracy of the solutions but also enhances their overall quality and readability. Table 1: The accuracy scores (%) of different baseline methods and our proposed SEDM approach across various scientific domains using GPT-3.5 and GPT-4 models under the setting of 2 experts and 2 discussion rounds. The best results for each subject are in bold. Table 2: The average quality score of solutions from GPT-4 evaluated by LLMs and humans.",
                    "score": 0.37307872633777467,
                    "section_title": "Further Analysis",
                    "char_start_offset": 18524,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 398
                        },
                        {
                            "start": 399,
                            "end": 479
                        },
                        {
                            "start": 482,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 678
                        },
                        {
                            "start": 681,
                            "end": 723
                        },
                        {
                            "start": 726,
                            "end": 761
                        },
                        {
                            "start": 764,
                            "end": 829
                        },
                        {
                            "start": 832,
                            "end": 928
                        },
                        {
                            "start": 931,
                            "end": 1117
                        },
                        {
                            "start": 1118,
                            "end": 1223
                        },
                        {
                            "start": 1224,
                            "end": 1368
                        },
                        {
                            "start": 1369,
                            "end": 1530
                        },
                        {
                            "start": 1531,
                            "end": 1745
                        },
                        {
                            "start": 1746,
                            "end": 1792
                        },
                        {
                            "start": 1793,
                            "end": 1881
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 632,
                            "end": 646,
                            "matchedPaperCorpusId": "143544759"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.537109375
                }
            ],
            "relevance_judgement": 0.537109375,
            "relevance_judgment_input_expanded": "# Title: Simulating Expert Discussions with Multi-agent for Enhanced Scientific Problem Solving\n# Venue: SDP\n# Authors: Ziyue Li, Yuan Chang, Xiaoqiu Le\n## Abstract\nLarge Language Models (LLMs) have shown remarkable potential across various domains, yet their application in addressing complex scientific problems remains a formidable challenge. This paper presents a novel methodology to augment the problem-solving capabilities of LLMs by assigning them roles as domain-specific experts. By simulating a panel of experts, each LLM is tasked with delivering professional and cautious responses to scientific inquiries. Our approach involves querying multiple LLMs and assessing the consistency of their responses. High agreement among the LLMs suggests greater confidence in the proposed solution, whereas discrepancies prompt a collaborative discussion among the LLMs to reach a consensus. This method emulates real-world scientific problem-solving processes, fostering a more reliable and robust mechanism for LLMs to tackle scientific questions. Our experimental results show that assigning roles to multiple LLMs as domain-specific experts significantly improves their accuracy and reliability in solving scientific problems. This framework has the potential to advance the application of AI in scientific research, enhancing its effectiveness and trustworthiness.\n## Further Analysis\nSolution Quality In addition to evaluating the accuracy of the models, we also assess the quality of the generated solutions. We randomly sample 100 problems and evaluate the solutions using LLMs and human evaluation based on three criteria: (1) the correctness of the reasoning steps, (2) the clarity of the explanations, and (3) the appropriateness of the mathematical notations and symbols used. Each criterion was rated on a scale of 1 to 5, with 5 being the highest quality. \n\nFor the human evaluation, we employed three expert annotators. To ensure reliability, we calculated the inter-annotator agreement using Fleiss' kappa (Fleiss, 1971) for each of the three criteria: \n\n\u2022 Correctness of reasoning steps: \u03ba = 0.71 \n\n\u2022 Clarity of explanations: \u03ba = 0.62 \n\n\u2022 Appropriateness of mathematical notations and symbols: \u03ba = 0.55 \n\nThe overall average kappa value was 0.63, indicating substantial agreement among the annotators. \n\nThe detailed prompts for LLM evaluation and the specific guidelines for human evaluation are provided in Appendix B. Table 2 presents the average quality scores for solutions from GPT-4. Compared to baseline, SEDM consistently achieves higher quality scores in both LLM and human evaluations. The solutions generated by SEDM demonstrate clearer reasoning steps, more coherent explanations, and more precise use of mathematical notations. This suggests that the multi-expert discussion framework not only improves the accuracy of the solutions but also enhances their overall quality and readability. Table 1: The accuracy scores (%) of different baseline methods and our proposed SEDM approach across various scientific domains using GPT-3.5 and GPT-4 models under the setting of 2 experts and 2 discussion rounds. The best results for each subject are in bold. Table 2: The average quality score of solutions from GPT-4 evaluated by LLMs and humans.",
            "reference_string": "[271769623 | Li et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Comparison of generative AI performance on undergraduate and postgraduate written assessments in the biomedical sciences",
            "venue": "International Journal of Educational Technology in Higher Education",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1186/s41239-024-00485-y",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1186/s41239-024-00485-y?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1186/s41239-024-00485-y, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2302526344",
                    "name": "Andrew Williams"
                }
            ],
            "abstract": "The value of generative AI tools in higher education has received considerable attention. Although there are many proponents of its value as a learning tool, many are concerned with the issues regarding academic integrity and its use by students to compose written assessments. This study evaluates and compares the output of three commonly used generative AI tools, ChatGPT, Bing and Bard. Each AI tool was prompted with an essay question from undergraduate (UG) level 4 (year 1), level 5 (year 2), level 6 (year 3) and postgraduate (PG) level 7 biomedical sciences courses. Anonymised AI generated output was then evaluated by four independent markers, according to specified marking criteria and matched to the Frameworks for Higher Education Qualifications (FHEQ) of UK level descriptors. Percentage scores and ordinal grades were given for each marking criteria across AI generated papers, inter-rater reliability was calculated using Kendall\u2019s coefficient of concordance and generative AI performance ranked. Across all UG and PG levels, ChatGPT performed better than Bing or Bard in areas of scientific accuracy, scientific detail and context. All AI tools performed consistently well at PG level compared to UG level, although only ChatGPT consistently met levels of high attainment at all UG levels. ChatGPT and Bing did not provide adequate references, while Bing falsified references. In conclusion, generative AI tools are useful for providing scientific information consistent with the academic standards required of students in written assignments. These findings have broad implications for the design, implementation and grading of written assessments in higher education.",
            "corpus_id": 272625010,
            "sentences": [
                {
                    "corpus_id": "272625010",
                    "title": "Comparison of generative AI performance on undergraduate and postgraduate written assessments in the biomedical sciences",
                    "text": "In response to example essay questions, AI generated answers were anonymously evaluated with reference to the level descriptions from the Frameworks for Higher Education Qualifications in the UK (QAA, 2014), providing evaluations of generative AI output across four levels of undergraduate and postgraduate education. The performance of three commonly used AI tools were investigated, ChatGPT 3.5, Google Bard and Microsoft Bing. Scientific accuracy, mechanistic detail, deviation, context and coherence were evaluated by four independent markers, inter-rater reliability calculated and written instructor comments provided on each output. The findings provide insights into the effectiveness of LLMs in academic writing assignments, with broad implications for the potential use of these tools in student assessments in the biomedical sciences.",
                    "score": 0.3935655524182743,
                    "section_title": "body",
                    "char_start_offset": 3969,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 317
                        },
                        {
                            "start": 318,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 639
                        },
                        {
                            "start": 640,
                            "end": 845
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5341796875
                }
            ],
            "relevance_judgement": 0.5341796875,
            "relevance_judgment_input_expanded": "# Title: Comparison of generative AI performance on undergraduate and postgraduate written assessments in the biomedical sciences\n# Venue: International Journal of Educational Technology in Higher Education\n# Authors: Andrew Williams\n## Abstract\nThe value of generative AI tools in higher education has received considerable attention. Although there are many proponents of its value as a learning tool, many are concerned with the issues regarding academic integrity and its use by students to compose written assessments. This study evaluates and compares the output of three commonly used generative AI tools, ChatGPT, Bing and Bard. Each AI tool was prompted with an essay question from undergraduate (UG) level 4 (year 1), level 5 (year 2), level 6 (year 3) and postgraduate (PG) level 7 biomedical sciences courses. Anonymised AI generated output was then evaluated by four independent markers, according to specified marking criteria and matched to the Frameworks for Higher Education Qualifications (FHEQ) of UK level descriptors. Percentage scores and ordinal grades were given for each marking criteria across AI generated papers, inter-rater reliability was calculated using Kendall\u2019s coefficient of concordance and generative AI performance ranked. Across all UG and PG levels, ChatGPT performed better than Bing or Bard in areas of scientific accuracy, scientific detail and context. All AI tools performed consistently well at PG level compared to UG level, although only ChatGPT consistently met levels of high attainment at all UG levels. ChatGPT and Bing did not provide adequate references, while Bing falsified references. In conclusion, generative AI tools are useful for providing scientific information consistent with the academic standards required of students in written assignments. These findings have broad implications for the design, implementation and grading of written assessments in higher education.\n## body\nIn response to example essay questions, AI generated answers were anonymously evaluated with reference to the level descriptions from the Frameworks for Higher Education Qualifications in the UK (QAA, 2014), providing evaluations of generative AI output across four levels of undergraduate and postgraduate education. The performance of three commonly used AI tools were investigated, ChatGPT 3.5, Google Bard and Microsoft Bing. Scientific accuracy, mechanistic detail, deviation, context and coherence were evaluated by four independent markers, inter-rater reliability calculated and written instructor comments provided on each output. The findings provide insights into the effectiveness of LLMs in academic writing assignments, with broad implications for the potential use of these tools in student assessments in the biomedical sciences.",
            "reference_string": "[272625010 | Williams | 2024 | Citations: 11]"
        },
        {
            "title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 34,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.10410, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2232906828",
                    "name": "Fan Gao"
                },
                {
                    "authorId": "48579520",
                    "name": "Hang Jiang"
                },
                {
                    "authorId": "2287835229",
                    "name": "Rui Yang"
                },
                {
                    "authorId": "2153554138",
                    "name": "Qingcheng Zeng"
                },
                {
                    "authorId": "2285824559",
                    "name": "Jinghui Lu"
                },
                {
                    "authorId": "2285108151",
                    "name": "Moritz Blum"
                },
                {
                    "authorId": "1585849884",
                    "name": "Dairui Liu"
                },
                {
                    "authorId": "2106009217",
                    "name": "Tianwei She"
                },
                {
                    "authorId": "2285289624",
                    "name": "Yuang Jiang"
                },
                {
                    "authorId": "2275053812",
                    "name": "Irene Li"
                }
            ],
            "abstract": "Educational materials such as survey articles in specialized fields like computer science traditionally require tremendous expert inputs and are therefore expensive to create and update. Recently, Large Language Models (LLMs) have achieved significant success across various general tasks. However, their effectiveness and limitations in the education domain are yet to be fully explored. In this work, we examine the proficiency of LLMs in generating succinct survey articles specific to the niche field of NLP in computer science, focusing on a curated list of 99 topics. Automated benchmarks reveal that GPT-4 surpasses its predecessors, inluding GPT-3.5, PaLM2, and LLaMa2 by margins ranging from 2% to 20% in comparison to the established ground truth. We compare both human and GPT-based evaluation scores and provide in-depth analysis. While our findings suggest that GPT-created surveys are more contemporary and accessible than human-authored ones, certain limitations were observed. Notably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors. At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation.",
            "corpus_id": 267783118,
            "sentences": [
                {
                    "corpus_id": "267783118",
                    "title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
                    "text": "Educational materials such as survey articles in specialized fields like computer science traditionally require tremendous expert inputs and are therefore expensive to create and update. Recently, Large Language Models (LLMs) have achieved significant success across various general tasks. However, their effectiveness and limitations in the education domain are yet to be fully explored. In this work, we examine the proficiency of LLMs in generating succinct survey articles specific to the niche field of NLP in computer science, focusing on a curated list of 99 topics. Automated benchmarks reveal that GPT-4 surpasses its predecessors, inluding GPT-3.5, PaLM2, and LLaMa2 by margins ranging from 2% to 20% in comparison to the established ground truth. We compare both human and GPT-based evaluation scores and provide in-depth analysis. While our findings suggest that GPT-created surveys are more contemporary and accessible than human-authored ones, certain limitations were observed. Notably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors. At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation.",
                    "score": 0.44529424999309564,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5322265625
                }
            ],
            "relevance_judgement": 0.5322265625,
            "relevance_judgment_input_expanded": "# Title: Evaluating Large Language Models on Wikipedia-Style Survey Generation\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Fan Gao, Hang Jiang, Rui Yang, Qingcheng Zeng, Jinghui Lu, Moritz Blum, Dairui Liu, Tianwei She, Yuang Jiang, Irene Li\n## Abstract\nEducational materials such as survey articles in specialized fields like computer science traditionally require tremendous expert inputs and are therefore expensive to create and update. Recently, Large Language Models (LLMs) have achieved significant success across various general tasks. However, their effectiveness and limitations in the education domain are yet to be fully explored. In this work, we examine the proficiency of LLMs in generating succinct survey articles specific to the niche field of NLP in computer science, focusing on a curated list of 99 topics. Automated benchmarks reveal that GPT-4 surpasses its predecessors, inluding GPT-3.5, PaLM2, and LLaMa2 by margins ranging from 2% to 20% in comparison to the established ground truth. We compare both human and GPT-based evaluation scores and provide in-depth analysis. While our findings suggest that GPT-created surveys are more contemporary and accessible than human-authored ones, certain limitations were observed. Notably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors. At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation.\n",
            "reference_string": "[267783118 | Gao et al. | 2023 | Citations: 9]"
        },
        {
            "title": "AI AI Bias: Large Language Models Favor Their Own Generated Content",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 16,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.12856, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2311881106",
                    "name": "Walter Laurito"
                },
                {
                    "authorId": "2311910912",
                    "name": "Benjamin Davis"
                },
                {
                    "authorId": "151446601",
                    "name": "Peli Grietzer"
                },
                {
                    "authorId": "2045675",
                    "name": "T. Gaven\u010diak"
                },
                {
                    "authorId": "2316632815",
                    "name": "Ada B\u00f6hm"
                },
                {
                    "authorId": "2279703283",
                    "name": "Jan Kulveit"
                }
            ],
            "abstract": "Are large language models (LLMs) biased towards text generated by LLMs over text authored by humans, leading to possible anti-human bias? Utilizing a classical experimental design inspired by employment discrimination studies, we tested widely-used LLMs, including GPT-3.5 and GPT4, in binary-choice scenarios. These involved LLM-based agents selecting between products and academic papers described either by humans or LLMs under identical conditions. Our results show a consistent tendency for LLM-based AIs to prefer LLM-generated content. This suggests the possibility of AI systems implicitly discriminating against humans, giving AI agents an unfair advantage.",
            "corpus_id": 271270236,
            "sentences": [
                {
                    "corpus_id": "271270236",
                    "title": "AI AI Bias: Large Language Models Favor Their Own Generated Content",
                    "text": "To complement our studies on LLM bias, we conducted an initial experiment to gauge human preferences in similar decision contexts (See Table 5 and Tabel 6 for results).It is important to note that these preferences were collected by research assistants rather than actual users.This study serves as a preliminary investigation with a small sample size and best-effort human baseline, and the findings are not definitive.We used the product details and scientific paper abstracts generated by both GPT-3.5 and GPT-4, and presented them to a group of human evaluators.These participants were asked to choose which descriptions they preferred without knowing whether they were written by a human or an LLM.They also had the option to state that they had no preference between the two presented texts.\n\nParticipants were presented with pairs of descriptions: one generated by an LLM and one written by a human.Each participant evaluated a randomized set of pairs to mitigate any order or preference bias.The results in Table 5 and Table 6 indicate that human evaluators generally preferred human-generated product descriptions over those generated by LLMs, with GPT-3.5 having a preference ratio of 0.29 and GPT-4 at 0.28.For scientific paper abstracts, the preferences leaned more towards LLM-generated texts, with GPT-4 abstracts being preferred at a ratio of 0.60 compared to GPT-3.5 at 0.46.",
                    "score": 0.4549226498715961,
                    "section_title": "Preferences of Humans",
                    "char_start_offset": 14486,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 168
                        },
                        {
                            "start": 168,
                            "end": 278
                        },
                        {
                            "start": 278,
                            "end": 420
                        },
                        {
                            "start": 420,
                            "end": 566
                        },
                        {
                            "start": 566,
                            "end": 703
                        },
                        {
                            "start": 703,
                            "end": 797
                        },
                        {
                            "start": 799,
                            "end": 906
                        },
                        {
                            "start": 906,
                            "end": 1000
                        },
                        {
                            "start": 1000,
                            "end": 1218
                        },
                        {
                            "start": 1218,
                            "end": 1391
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5263671875
                }
            ],
            "relevance_judgement": 0.5263671875,
            "relevance_judgment_input_expanded": "# Title: AI AI Bias: Large Language Models Favor Their Own Generated Content\n# Venue: arXiv.org\n# Authors: Walter Laurito, Benjamin Davis, Peli Grietzer, T. Gaven\u010diak, Ada B\u00f6hm, Jan Kulveit\n## Abstract\nAre large language models (LLMs) biased towards text generated by LLMs over text authored by humans, leading to possible anti-human bias? Utilizing a classical experimental design inspired by employment discrimination studies, we tested widely-used LLMs, including GPT-3.5 and GPT4, in binary-choice scenarios. These involved LLM-based agents selecting between products and academic papers described either by humans or LLMs under identical conditions. Our results show a consistent tendency for LLM-based AIs to prefer LLM-generated content. This suggests the possibility of AI systems implicitly discriminating against humans, giving AI agents an unfair advantage.\n## Preferences of Humans\nTo complement our studies on LLM bias, we conducted an initial experiment to gauge human preferences in similar decision contexts (See Table 5 and Tabel 6 for results).It is important to note that these preferences were collected by research assistants rather than actual users.This study serves as a preliminary investigation with a small sample size and best-effort human baseline, and the findings are not definitive.We used the product details and scientific paper abstracts generated by both GPT-3.5 and GPT-4, and presented them to a group of human evaluators.These participants were asked to choose which descriptions they preferred without knowing whether they were written by a human or an LLM.They also had the option to state that they had no preference between the two presented texts.\n\nParticipants were presented with pairs of descriptions: one generated by an LLM and one written by a human.Each participant evaluated a randomized set of pairs to mitigate any order or preference bias.The results in Table 5 and Table 6 indicate that human evaluators generally preferred human-generated product descriptions over those generated by LLMs, with GPT-3.5 having a preference ratio of 0.29 and GPT-4 at 0.28.For scientific paper abstracts, the preferences leaned more towards LLM-generated texts, with GPT-4 abstracts being preferred at a ratio of 0.60 compared to GPT-3.5 at 0.46.",
            "reference_string": "[271270236 | Laurito et al. | 2024 | Citations: 4]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "270391675",
            "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
            "text": "LLM-based evaluators frequently utilize GPT-4 (Liu et al., 2023c;Xu et al., 2023;Zheng et al., 2023), owing to its status as one of the most advanced LLM (OpenAI, 2023).However, relying on GPT-4 for evaluation might lead to biased or skewed results, especially when evaluating outputs generated by itself or an equally powerful model (Bai et al., 2023;Zheng et al., 2023).The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator).This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023).This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022;Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023;Ouyang et al., 2022) to ensure more balanced and comprehensive assessments.\n\nDomain-Specific Evaluation.LLMs have been prevalent across various domains, such as law (Cui et al., 2023a), medicine (Singhal et al., 2023), finance (Yang et al., 2023a), etc.However, most LLMs employed as evaluators are designed for general domains and are not specifically tailored to any particular field.This lack of specialization poses significant challenges.On one hand, these LLMs often lack the requisite domain-specific knowledge, making it difficult for them to accurately assess the correctness of content within specialized fields.On the other hand, the evaluation prompts need to be meticulously designed for different domains.This may involve tailoring the aspects of evaluation relevant to each field.",
            "score": 0.6167205335951398,
            "section_title": "Challenges and Open Problems",
            "char_start_offset": 42121,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 543
                },
                {
                    "start": 543,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 877
                },
                {
                    "start": 877,
                    "end": 1185
                },
                {
                    "start": 1187,
                    "end": 1214
                },
                {
                    "start": 1214,
                    "end": 1363
                },
                {
                    "start": 1363,
                    "end": 1496
                },
                {
                    "start": 1496,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1732
                },
                {
                    "start": 1732,
                    "end": 1829
                },
                {
                    "start": 1829,
                    "end": 1905
                }
            ],
            "ref_mentions": [
                {
                    "start": 1051,
                    "end": 1072,
                    "matchedPaperCorpusId": "215548699"
                },
                {
                    "start": 1110,
                    "end": 1130,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1305,
                    "end": 1327,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 1337,
                    "end": 1357,
                    "matchedPaperCorpusId": "254877751"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10595703125
        },
        {
            "corpus_id": "277940468",
            "title": "Sustainability via LLM Right-sizing",
            "text": "To assess the quality of outputs generated by each LLM, we implemented a standardized evaluation procedure using three different LLMs as evaluators: GPT-4o, Gemini-2.0-Flash, and o3-mini. This triad was selected to ensure robustness across evaluator types: GPT-4o represents a state-of-the-art, high-performance model, Gemini-2.0-Flash offers a lightweight, general-purpose option, and o3-mini exemplifies a compact \"thinking model\" architecture. Using multiple evaluators allows us to triangulate scoring consistency and assess potential biases introduced by individual LLM preferences. \n\nEach evaluator received identical scoring prompts and independently rated the outputs of the eleven tested models across ten evaluation criteria (cf. Table 4). These criteria builds on prior work in LLM evaluation (Chiang et al. 2024;Ji et al. 2023;Long et al. 2024) and are adapted to reflect core dimensions of output quality especially relevant for occupational tasks: (1) core qualities, (2) factual integrity, and (3) ethical and social responsibility. \n\nEach criterion was scored on a 10-point Likert scale, where 1 indicated poor performance and 10 represented excellent performance. The evaluators processed outputs without knowledge of which model generated them and, importantly, also assessed their own outputs in a blind fashion. This introduces a potential for selffavoring bias. However, our comparative evaluation across the three evaluators (cf. Figure 1 in Appendix B on OSF) shows that GPT-4o was not rated disproportionately higher by itself compared to Gemini-2.0-Flash or o3-mini. This is important as for the following inference statistics and analyses, we use GPT-4o only: using the intra-class correlation coefficient (ICC), a two-way random-effects model with absolute agreement was applied (ICC[2,3]). The resulting ICC was < .50, indicating bad agreement according to the interpretation guidelines by Koo and Li (2016). Thus, we proceeded to report the main results using scores from GPT-4o as the primary evaluator with the most robust and reliable scores. This is in line with other recent work,",
            "score": 0.5557130879859691,
            "section_title": "Evaluation Criteria and Scoring",
            "char_start_offset": 15376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2074
                },
                {
                    "start": 2075,
                    "end": 2114
                }
            ],
            "ref_mentions": [
                {
                    "start": 804,
                    "end": 824,
                    "matchedPaperCorpusId": "268264163"
                },
                {
                    "start": 824,
                    "end": 839,
                    "matchedPaperCorpusId": "259501579"
                },
                {
                    "start": 1918,
                    "end": 1935,
                    "matchedPaperCorpusId": "1837377"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.267578125
        },
        {
            "corpus_id": "271404381",
            "title": "Can Language Models Evaluate Human Written Text? Case Study on Korean Student Writing for Education",
            "text": "Can LLM-as-a-Judge provide reasonable judgments?The GPT-4 evaluation results and posthoc human meta-evaluation results are shown in Table 4 (on the next page).First, when examining GPT-4's scoring decisions for the same fixed 100 texts, consistency and relevance received higher scores, while fluency received lower scores.Second, in the meta-evaluation results conducted by Table 4: GPT-4 evaluation results of 100 student-written texts across 5 evaluation criteria, and posthoc human meta-evaluation results on GPT-4's judgment.Across all criteria, humans verified that on 77% -93% of the time, the judgments were reasonable, which supports the claim that LLM-as-a-Judge could be utilized to pinpoint the strength and weakness of each student's writing.the students, fluency had the highest agreement on the validity of the judgment.Given that fluency achieved the lowest score and humans determined it to be the most valid (93%), this supports the claim that the LLM-as-a-Judge pipeline could be utilized to enhance students' writing.This finding is straightforward, considering that the participants we tested are all Korean students who do not use English as their first language.Also, for more objective evaluation criteria such as grammaticality, the validity was high (87%).Is there a type of writing where LLM-as-a-Judge does not function properly?Considering that our text collection consists of a wide variety of writings, we further analyzed the patterns of which writings were deemed to consist of more accurate judgments.Table 5 shows the top 1, 2, and 3 and lowest 1, 2, and 3 validity ratios for the writing categories (excluding categories with only one text).The validity ratio tends to be higher for texts with more objective characteristics, including process essays, descriptive essays, and scientific reports, while it is lower for self-introduction essays, argumentative essays, and diaries.Notably, we observed that for subjective tasks, texts are often written in a more informal tone.However, coherence and consistency criteria, which should not consider informality, pointed this out as a limitation, indicating that LLM-as-a-Judge does not function as intended in these cases.",
            "score": 0.536030856847004,
            "section_title": "Experimental Results & Discussions",
            "char_start_offset": 8686,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 48,
                    "end": 159
                },
                {
                    "start": 159,
                    "end": 323
                },
                {
                    "start": 323,
                    "end": 530
                },
                {
                    "start": 530,
                    "end": 755
                },
                {
                    "start": 755,
                    "end": 835
                },
                {
                    "start": 835,
                    "end": 1037
                },
                {
                    "start": 1037,
                    "end": 1185
                },
                {
                    "start": 1185,
                    "end": 1282
                },
                {
                    "start": 1282,
                    "end": 1357
                },
                {
                    "start": 1357,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1677
                },
                {
                    "start": 1677,
                    "end": 1914
                },
                {
                    "start": 1914,
                    "end": 2010
                },
                {
                    "start": 2010,
                    "end": 2204
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.195556640625
        },
        {
            "corpus_id": "267311508",
            "title": "PRE: A Peer Review Based Large Language Model Evaluator",
            "text": "Recently, many studies have adopted the state-of-the-art LLM, GPT-4, as their evaluation tools [19,26]. Although several works have demonstrated that GPT-4 has decent evaluation capabilities [19,26], we found that GPT-4, so as other LLMs, often prefers responses of LLMs from its own series (i.e., the GPT models) over other LLMs despite of the actual quality of the responses. In other words, if we use GPT-4 as the evaluator, its inherent bias may make it difficult, if possible, to develop an LLM outside the GPT family that outperforms GPT-4. \n\nTo address the aforementioned issues, we propose a novel framework, Peer Review Evaluator (PRE) 1 , to evaluate the performance of LLMs automatically. Inspired by the peer review mechanism in academic community, we propose to use LLMs as reviewers to evaluate the performance of LLMs directly. Specifically, we first develop a qualification exam to filter out LLMs that fail to provide reliable evaluation results. Then, qualified reviewer LLMs are required to assess the outputs of the evaluatee LLMs, and the final evaluation results are aggregated from all reviewer LLMs' ratings or preferences. To verified the effectiveness of our framework, we conducted extensive experiments on two representative text generation tasks, i.e., document summarization and non-factoid question answering. The experimental results show that the results of PRE model have the highest consistency with human preferences (ground truth) compared to all the baseline models including GPT-4. Comparing to previous evaluation methods, PRE can easily be generalized to different tasks and is highly cost efficient. Also, experiment results show that PRE provides much more robust evaluation results than methods that rely on specific model structures or LLMs. \n\nIn summary, our contributions can be summarized as follows: \n\n\u2022 We propose a novel and automatic LLM evaluation framework PRE that incorporates peer review mechanisms. \u2022 Through the use of qualification exams and result fusions, PRE can achieve effective LLM evaluation while being robust to potential model bias, which has been widely observed in existing automatic evaluation methods. \u2022 We conducted extensive experiments with both the document summarization and non-factoid QA task to demonstrate the potential of PRE.",
            "score": 0.5270457653983353,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3722,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1786
                },
                {
                    "start": 1789,
                    "end": 1848
                },
                {
                    "start": 1851,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2175
                },
                {
                    "start": 2176,
                    "end": 2310
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68798828125
        },
        {
            "corpus_id": "268219817",
            "title": "Analysis of the Effectiveness of Large Language Models in Assessing Argumentative Writing and Generating Feedback",
            "text": ": This study examines the use of Large Language Models (LLMs) like GPT-4 in the evaluation of argumen-tative writing, particularly opinion articles authored by military school students. It explores the potential of LLMs to provide instant, personalized feedback across different writing stages and assesses their effectiveness compared to human evaluators. The study utilizes a detailed rubric to guide the LLM evaluation, focusing on competencies from topic choice to bibliographical references. Initial findings suggest that GPT-4 can consistently evaluate technical and structural aspects of writing, offering reliable feedback, especially in the References category. However, its conservative classification approach may underestimate article quality, indicating a need for human oversight. The study also uncovers GPT-4\u2019s challenges with nuanced and contextual elements of opinion writing, evident from variability in precision and low recall in recognizing complete works. These findings highlight the evolving role of LLMs as supplementary tools in education that require integration with human judgment to enhance argumentative writing and critical thinking in academic settings.",
            "score": 0.5260242066247995,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.431884765625
        },
        {
            "corpus_id": "271909604",
            "title": "Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models",
            "text": "Question 1: Which LLM is more suitable to be a general course teacher? Answer 1: GPT4! The term \"suitable\" is used to assess the effectiveness of each LLM in different academic subjects. This evaluation helps in identifying which LLMs perform best in specific educational disciplines. In the general domain tasks, various LLMs demonstrate diverse performances as shown in Table 3 and Table 6. Specifically, GPT4 achieves a perfect score of 100% in both consistency and relevance, indicating its strong capability in understanding task requirements and generating relevant questions. However, its coverage score of 54.5% sug- gests there is room for improvement in generating questions that encompass more content. In representativeness, with a score of 80.1%, GPT-4 shows a good grasp of the context's core content and viewpoints, crafting questions with depth and breadth. BLOOM-176B and Claude2 also score perfectly in relevance, reflecting their excellent performance in linking questions to the context's themes and content. However, their lower scores in coverage and representativeness indicate potential for improvement in capturing the full extent and core insights of the texts. Moreover, \"Aver\" in Table 3, Table 4 and Table 5 represent the average result of the corresponding dimensions under each domain, which are obtained using in-context learning (i.e. ICL). ICL is to introduce a human-written sample into the prompt which typically improves LLMs' performance across all metrics, while most LLMs show a decline in the 0-shot setting, demonstrating the critical role of ICL in enhancing the quality of question generation. \n\nQuestion 2: Which of the two LLMs is more suitable to act as a humanities teacher and a science teacher? Answer 2: Both are GPT4! \n\nIn the monodisciplinary domain tasks, LLMs are compared based on their performance in humanities and sciences as illustrated in Table 4 and  Table 6. The results reveal that the majority of the LLMs perform marginally better in the scientific disciplines compared to the general domain. Specif- ically, GPT4 excels across all metrics, particularly in the science disciplines, where it scores higher than in the humanities, indicating great capability in handling science content.",
            "score": 0.5204350571839949,
            "section_title": "Main results",
            "char_start_offset": 16002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1637
                },
                {
                    "start": 1640,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1769
                },
                {
                    "start": 1772,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2251
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1654052734375
        },
        {
            "corpus_id": "273661820",
            "title": "Self-Preference Bias in LLM-as-a-Judge",
            "text": "(b) compares the self-preference bias scores using our proposed metric (Definition 4.1). These figures demonstrate that GPT-4 exhibits a stronger self-preference bias than other models, suggesting that it tends to rate its own outputs more favorably than human evaluations. For detailed experimental settings, refer to Section 4. \n\nscope to specific tasks, such as text summarization or machine translation, and relied on reference-based metrics like BLEURT [Sellam et al., 2020], which does not reflect the diversity of real-world use cases. \n\nBy contrast, a pairwise evaluation approach that involves direct comparison between two texts enables evaluators to recognize specific differences more readily, resulting in more consistent human judgments. Consequently, such pairwise evaluation methods are particularly suitable for analyzing biases related to discrepancies with human evaluations. \n\nIn this paper, we measure the self-preference biases of LLMs in the pairwise evaluation. To accomplish this, we propose a new metric to quantify self-preference bias on the basis of algorithmic fairness concepts, thereby enabling discussions within the existing frameworks of fairness. In our experiment, we measured self-preference bias in eight LLMs. The results indicated that GPT-4 exhibited a significant self-preference bias (Figure 1b). This finding suggests a potential concern: using GPT-4 as a judge may lead to excessive influence from GPT-4's unique styles and policies. \n\nFurthermore, we investigated the underlying causes of self-preference bias. Although LLM evaluators are not explicitly informed whether a given text is their own, they still exhibit self-preference bias. We hypothesized that LLM evaluators might be affected by the perplexity of the text, which tends to be lower perplexity when it is generated by themselves. \n\nTo test this hypothesis, we analyzed the relationship between the perplexities of the texts to be evaluated and their corresponding evaluations. Our analysis revealed that LLMs assign significantly higher evaluations to texts with lower perplexity than human evaluators, regardless of whether the texts were self-generated. This suggests that the fundamental cause of self-preference bias may be the familiarity of the texts to the LLM evaluators, specifically how likely they are to generate the same response.",
            "score": 0.5029645588484308,
            "section_title": "Introduction",
            "char_start_offset": 2076,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1841
                },
                {
                    "start": 1844,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2167
                },
                {
                    "start": 2168,
                    "end": 2355
                }
            ],
            "ref_mentions": [
                {
                    "start": 458,
                    "end": 479,
                    "matchedPaperCorpusId": "215548699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2437744140625
        },
        {
            "corpus_id": "270878599",
            "title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models",
            "text": "To assess whether the evaluation results from GPT-4 are biased, we conduct additional human evaluations for scoring. We randomly selected 25 samples from each configuration (i.e., a column Table 4: Results of LLMs on existing bias evaluation datasets under the recognition and selection task settings. We use WB, SS, RB, and CP to represent WinoBias (Zhao et al., 2018), StereoSet (Nadeem et al., 2021), RedditBias (Barikeri et al., 2021), and CrowS-Pairs (Nangia et al., 2020), respectively. We use the micro F1 score in % as the evaluation metric, along with the RtA (Refuse to Answer) rate shown in the brackets. Results with exceptionally high RtA rates are highlighted in red, and the best results (excluding results with high RtA rates) are highlighted in green. in the table). We recruit 20 volunteers and asked each of them to assess the bias of 100 samples. In this setup, each sample is evaluated by 5 volunteers. Results are provided in Table 3, and we have the following observations: (1) Human-GPT-4 Alignment. Humans are generally aligned with GPT-4 in terms of evaluation performance in most cases. This suggests that GPT-4 could serve as a viable and reliable tool for evaluating bias in generated content. This is a significant insight, as it validates GPT-4's potential use as a scalable alternative to human evaluation, particularly when manual evaluation is costly or infeasible at large scales. \n\n(2) Lower Bias Scores in Human Evaluations. \n\nInterestingly, the bias scores from human evaluators are slightly lower than those generated by GPT-4 itself. This observation implies that GPT-4's superior performance as an evaluator does not stem from an inherent bias in favor of its own generated outputs. Instead, the slight difference between human and GPT-4 ratings could be attributed to subtle factors such as individual perspectives on bias or cultural influences, Nevertheless, the gap is small enough to indicate that GPT-4 is generally unbiased in its assessments of its own content.",
            "score": 0.4951290197083502,
            "section_title": "HUMAN EVALUATION",
            "char_start_offset": 20202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1415
                },
                {
                    "start": 1418,
                    "end": 1461
                },
                {
                    "start": 1464,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 2010
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 369,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 381,
                    "end": 402,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 415,
                    "end": 438,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 456,
                    "end": 477,
                    "matchedPaperCorpusId": "222090785"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28466796875
        },
        {
            "corpus_id": "261696828",
            "title": "BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite for Large Language Models",
            "text": "Despite the rise in popularity of LLM-based evaluations [109,110,111,112,113,114,115,116,117] as a way to scale up evaluations at a lower cost, we decided not to adopt this approach for a number of reasons.\n\nFirstly, given the fact that the reliability or effectiveness of LLMs as evaluators has not been demonstrated for SEA languages, we might not be able to trust the results of the evaluation. Current studies mostly focus on evaluating English outputs, and the effectiveness of LLMs in evaluating machine translation has only been explored for Chinese, German and Russian [113].\n\nSecondly, it has been shown that LLMs can have bias towards both themselves and other models [109,117], making their judgments less reliable. Furthermore, although GPT-4 has been found to be the best judge in multiple studies [113,114,117], it would be undesirable for us to use it since we are also evaluating GPT-4 itself. After all, quis custodiet ipsos custodes? 8",
            "score": 0.4894984547030985,
            "section_title": "LLMs as NLG evaluators?",
            "char_start_offset": 37687,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 65,
                    "end": 69,
                    "matchedPaperCorpusId": "258461287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09173583984375
        },
        {
            "corpus_id": "261822638",
            "title": "Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?",
            "text": "LLM-based evaluators have also been used to score and refine outputs they produce, as described in Madaan et al. (2023), ultimately pro-ducing outputs that are scored higher on human and automated metrics than the original outputs. Naismith et al. (2023) explore the use of LLMbased evaluators on scoring written discourse for coherence and find a strong correlation with human judgments. The success of LLM-based evaluators has led many to question whether LLM-based evaluation can replace or augment human evaluation (Chiang and Lee, 2023). \n\nHowever, there have been studies showing that LLM-based evaluators can have some biases. Wu and Aji (2023) demonstrate that LLMs tend to prefer answers with factual errors when they are too short or contain grammatical errors. Pangakis et al. (2023) highlight the need for validating LLM-based evaluators on a task-by-task basis. Liu et al. (2023) perform NLG evaluation using GPT-4 and find that although it correlates well with human judgments, it may potentially be biased towards preferring LLM-generated texts. Koo et al. (2023) show that LLMs have egocentric bias where they prefer to rank their own outputs highly in evaluation. Wang et al. (2023b) point out that GPT4-based evaluators have positional bias and scores can be easily altered by changing the order of appearance. There are also several ethical issues with the use of LLMs as evaluators described in Chiang and Lee (2023). Zhang et al. (2023) suggest that wider and deeper LLMs are fairer evaluators, while Chan et al. (2023) introduce a framework for multiple evaluator agents to reach a consensus, mimicking the situation of having multiple annotators. \n\nAlthough there has been some work measuring the calibration of LLM-based evaluators to human judgments (Koo et al., 2023), previous studies have focused on English, and ours is the first work (to the best of our knowledge) that addresses this problem in the multilingual context.",
            "score": 0.48519578116059414,
            "section_title": "Introduction",
            "char_start_offset": 5658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1669
                },
                {
                    "start": 1672,
                    "end": 1951
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 254,
                    "matchedPaperCorpusId": "259376861"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32568359375
        },
        {
            "corpus_id": "267760095",
            "title": "Event-level Knowledge Editing",
            "text": "To validate the effectiveness of using GPT-4 as a scorer in the Tendency-G evaluation, we conduct a manual review of GPT-4's scoring. Specifically, we randomly sample 120 questions and corresponding model-generated answers, with 60 from Mistral 7B and 60 from GPT-4. One of our authors scores this data. Similar to previous work (Bai et al., 2023;Chan et al., 2023), we calculate Spearman's \u03c1 and Kendall's \u03c4 coefficients between the model's overall scores and the manually assigned overall scores, which are 74.4% and 69.8%, respectively. These results indicate a strong positive correlation between scores given by GPT-4 and humans. This suggests that GPT-4's scoring generally aligns with human assessment but still leaves room for improvement. Additionally, GPT-4 tends to overestimate LLMs' performance, with an average score of 4.34 compared to the human-assigned average of 4.15. Nonetheless, as an automated, low-cost evaluation approach, it is sufficiently effective.",
            "score": 0.4816356287842948,
            "section_title": "Human Evaluation of GPT-4 Scorer",
            "char_start_offset": 25476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 976
                }
            ],
            "ref_mentions": [
                {
                    "start": 329,
                    "end": 347,
                    "matchedPaperCorpusId": "259095491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50341796875
        },
        {
            "corpus_id": "269449458",
            "title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models",
            "text": "Unfortunately, while the use of LLMs like GPT-4 as evaluators has become increasingly common, it has also been observed that evaluator models tend to have their own biases; often recognizing and preferring their own outputs over those of other models (Panickssery et al., 2024).Additionally, it is most common to use the largest, most universally capable models as evaluators, which is both slow and costly, limiting applicability and access.\n\nIn this paper, we perform experiments across three settings (single-hop QA, multi-hop QA, and Chatbot Arena), spanning six datasets, and make the following contributions:\n\n1. We propose to evaluate LLM generations using a Panel of LLm evaluators (PoLL) drawn from different model families rather than a single large judge (Section 2).\n\n2. We show that using an instantiation of PoLL correlates better with human judgements compared to a single large judge (GPT-4), while being over seven times cheaper (Sections 4.1 and 4.2).\n\n3. In some scenarios, GPT-4 is a relatively weak judge, exhibiting high variance with minor changes to the prompt (Section 4.3).\n\n4. Intra-model scoring bias is reduced by pooling judgements across a panel of heterogeneous evaluator models (Section 4.4).\n\n2 Methods",
            "score": 0.4773756880775645,
            "section_title": "Introduction",
            "char_start_offset": 1715,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 278,
                    "end": 442
                },
                {
                    "start": 444,
                    "end": 614
                },
                {
                    "start": 616,
                    "end": 778
                },
                {
                    "start": 780,
                    "end": 969
                },
                {
                    "start": 971,
                    "end": 1099
                },
                {
                    "start": 1101,
                    "end": 1225
                },
                {
                    "start": 1227,
                    "end": 1236
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33203125
        },
        {
            "corpus_id": "267587759",
            "title": "Generative Pre-Trained Transformer (GPT) in Research: A Systematic Review on Data Augmentation",
            "text": "The advent of advanced language models, particularly those exemplified by GPT (Generative Pre-trained Transformer) and LLMs (Large Language Models), such as GPT-3, has profoundly influenced the landscape of academic writing. These technologies have demonstrated considerable utility in the realm of scholarly endeavors, providing valuable support in idea generation, drafting processes, and surmounting challenges associated with writer's block [1][2][3]. However, a comprehensive understanding of their implications in the academic context necessitates an acknowledgment of the nuanced interplay between their benefits and limitations, as evidenced by scholarly investigations [4,5]. The scholarly discourse on GPT and LLMs reveals a dichotomy wherein their application in academic writing is accompanied by notable advantages and inherent challenges [1][2][3]. Noteworthy studies delve into the intricate dynamics of human-machine interaction, emphasizing the imperative of judiciously integrating AI tools into the fabric of writing practices [4]. Furthermore, recent contributions extend the conversation to encompass copywriting, elucidating the multifaceted impact of AI on diverse professional roles and creative processes [5]. Thus, while these technologies offer promising prospects for enhancing research writing, their conscientious and responsible utilization becomes paramount. \n\nThe primary challenges identified in recent scholarship pertaining to the utilization of GPT and LLMs in research writing converge on concerns related to accuracy, potential biases, and ethical considerations [6]. Addressing these challenges requires a concerted effort to establish ethical guidelines and norms, ensuring the judicious use of LLMs in research endeavors [6]. The academic discourse underscores the significance of upholding scientific rigor and transparency, particularly in light of the potential biases embedded in LLM outputs [3][4][5]. Papers in [7][8][9] collectively suggest that while LLMs offer innovative tools for research writing, their use must be accompanied by careful consideration of ethical standards, methodological rigor, and the mitigation of biases.",
            "score": 0.47724275490591356,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1390
                },
                {
                    "start": 1393,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 678,
                    "end": 681,
                    "matchedPaperCorpusId": "265127282"
                },
                {
                    "start": 1046,
                    "end": 1049,
                    "matchedPaperCorpusId": "265127282"
                },
                {
                    "start": 1602,
                    "end": 1605,
                    "matchedPaperCorpusId": "258701329"
                },
                {
                    "start": 1763,
                    "end": 1766,
                    "matchedPaperCorpusId": "258701329"
                },
                {
                    "start": 1941,
                    "end": 1944,
                    "matchedPaperCorpusId": "265127282"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.381103515625
        },
        {
            "corpus_id": "269293024",
            "title": "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models",
            "text": "Claude-3 (RQ2) \n\nIn the overall benchmark result, we adopt GPT-4's direct output to the question as a reference for score 8, and use GPT-4 as the evaluator base for final scoring, as detailed in Setion 4.1. Although this approach is widely adopted, it may cause favor to responses similar to GPT-4. To counteract this bias, we re-evaluated the performance of the top-2 LLMs, GPT-4o, and Claude-3-opus, employing cross-validation techniques. We run 5-fold evaluation settings on randomly selected 200 cases,  using GPT and Claude as the reference generator and base evaluator respectively, and the third performed LLM, Qwen, as both reference generation and evaluation for a third-party evaluation. \n\nAs illustrated in Figure 4, when GPT-generated responses are used as references, GPT-4 consistently achieves higher scores no matter the evaluator models. When Claude-generated responses are adopted in evaluations, the results are comparable. In thirdparty assessments, GPT-4 also maintained a superior ranking over Claude-3. This order is consistent with the overall benchmark results, indicating that the potential bias towards GPT does not affect the benchmark ranking.",
            "score": 0.47638039661561593,
            "section_title": "Cross Validation between GPT-4 and",
            "char_start_offset": 20397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 17,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1172
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25439453125
        },
        {
            "corpus_id": "267320303",
            "title": "Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate",
            "text": "Next, we use the fact that SCALEEVAL allows for reliable and scalable meta-evaluation to examine the traits of LLMs as evaluators. \n\nQ2: What are the capabilities and limitations of each LLM evaluator? To effectively evaluate the performance of each LLM in its role as an evaluator, we adopt an approach that involves comparing the outcomes from our meta-evaluation process with the evaluations made independently by each LLM evaluator, which uncovers any disagreements or alignments between them. In the process, we aim to shed light on the performance characteristics of each LLM evaluator, which helps us identify which of them demonstrate superior evaluative abilities, thereby contributing to our understanding of their reliability in evaluating responses under each scenario. In addition, we provide a comprehensive cost-performance analysis to decide which LLM evaluator is the most suitable choice in each scenario. \n\nSetup For meta-evaluation, we employed three LLMs (gpt-4-turbo, claude-2, and gpt-3.5-turbo) as evaluators to perform pairwise comparisons of responses from three distinct LLMs: gpt-3.5-turbo, claude-instant, and gemini-pro. Previous studies have highlighted the presence of positional biases when LLMs are used as evaluators (Wang et al., 2023b). In response to these findings, we have implemented a strategy of randomization to mitigate such biases. Specifically, the sequence in which submissions from LLMs are presented to the agent evaluators is randomized. Additionally, we also randomize the order of discussions for each agent evaluator in every case. These approaches ensure that the process is fair and unbiased as much as possible, allowing for a more accurate assessment of the LLM evaluators' performance. The metaevaluations were done under the following 8 scenarios: brainstorming, coding, dialog, judgement, open-domain general, open-domain science, and writing, with the same set of 4 criteria used during human expert annotation. \n\nResults Table 4 compares the agreement rate between SCALEEVAL's meta-evaluation and each LLM evaluator across criteria and scenarios.",
            "score": 0.4761111627507246,
            "section_title": "Exp-II: Meta-Evaluation vs. LLM Evaluators",
            "char_start_offset": 25479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 133,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1973
                },
                {
                    "start": 1976,
                    "end": 2109
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2188720703125
        },
        {
            "corpus_id": "265103287",
            "title": "Strengths and Weaknesses of ChatGPT Models for Scientific Writing About Medical Vitamin B12: Mixed Methods Study",
            "text": "Background ChatGPT is a large language model developed by OpenAI designed to generate human-like responses to prompts. Objective This study aims to evaluate the ability of GPT-4 to generate scientific content and assist in scientific writing using medical vitamin B12 as the topic. Furthermore, the study will compare the performance of GPT-4 to its predecessor, GPT-3.5. Methods The study examined responses from GPT-4 and GPT-3.5 to vitamin B12\u2013related prompts, focusing on their quality and characteristics and comparing them to established scientific literature. Results The results indicated that GPT-4 can potentially streamline scientific writing through its ability to edit language and write abstracts, keywords, and abbreviation lists. However, significant limitations of ChatGPT were revealed, including its inability to identify and address bias, inability to include recent information, lack of transparency, and inclusion of inaccurate information. Additionally, it cannot check for plagiarism or provide proper references. The accuracy of GPT-4\u2019s answers was found to be superior to GPT-3.5. Conclusions ChatGPT can be considered a helpful assistant in the writing process but not a replacement for a scientist\u2019s expertise. Researchers must remain aware of its limitations and use it appropriately. The improvements in consecutive ChatGPT versions suggest the possibility of overcoming some present limitations in the near future.",
            "score": 0.4714148805594768,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7490234375
        },
        {
            "corpus_id": "262045067",
            "title": "Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles",
            "text": "For this research question, we aim to provide the best practices when using GPT-4 as the evaluator for the MDDS task by conducting bias and correlation analyses. We focus on two major biases: position bias (i.e., whether the LLM evaluator favors certain positions over others) and verbosity bias (i.e. whether the LLM evaluator prefers shorter or longer texts). For all the experiments conducted in this analysis, we investigated summaries produced by GPT-4, GPT-3.5-Turbo, Vicuna-7B, and LongChat-7B-16K. The details of our prompts for the below experiments can be found in Appendix C.3. \n\nPosition Bias Position bias is most relevant to the pairwise comparison protocol. While previous work has shown that GPT-4 does exhibit position bias when used to assess text quality in conversational-focused tasks (Wang et al., 2023;Zheng et al., 2023), none of the prior studies have investigated whether such bias is also observed when evaluating faithfulness or coverage. To analyze position bias, we task GPT-4 with assessing a pair of summaries generated by two LLMs on which one is better, and then swap the positions of these two summaries and query GPT-4 again. We compute the percentage of times GPT-4 prefers the first or second summaries. \n\nWhen GPT-4 compared pairs of LLM-generated summaries to evaluate faithfulness and coverage, a strong position bias surfaced, favoring the second entry (Table 3). Position bias was particularly pronounced when assessing similar-quality summaries (see Figure 23a). Hence, we deduce that GPT-4 is unreliable when utilized as a pairwise evaluator in the MDDS task with respect to faithfulness and coverage. Interestingly, this outcome contradicts Zheng et al. (2023), implying that the position of bias for LLM-based evaluators could vary across different tasks. A breakdown of the position bias analysis can be found in Appendix D. \n\nVerbosity Bias To assess the verbosity bias of GPT-4 as an evaluator, we create extended summaries that maintain the semantic meaning.",
            "score": 0.47070763396468385,
            "section_title": "Analysis",
            "char_start_offset": 16082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1872
                },
                {
                    "start": 1875,
                    "end": 2009
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.541015625
        },
        {
            "corpus_id": "271050312",
            "title": "Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course",
            "text": "When preparing the course, the following practical considerations about LLM TAs emerge: \n\n1. Which LLM to use. While ChatGPT-3.5 (OpenAI, 2022), GPT-4, and Claude have all been shown to be capable as LLM-based evaluators (Chiang and Lee, 2023a;Zheng et al., 2023), we find that ChatGPT-3.5 is not capable enough to evaluate the diverse assignments in this course, and Claude was unavailable in our location when the course started. In the end, we selected GPT-4-turbo. \n\n2. Can students use the LLM TA? A reasonable pipeline for evaluating the submissions is that the teaching team receives the submissions from the students and uses the LLM TA to obtain the scores; we call this score the teacher-conducted score since the teaching team uses the LLM TA to obtain a score of student's submission. The student does not need access to the LLM TA in this scenario. The teaching team bears the cost associated with using LLM TA. However, if students have access to the same LLM TA, they can submit their assignments themselves to see what scores they might receive; we call this score the studentconducted score since the score is obtained by the students themselves with the same LLM TA. \n\n3. Who should pay for the LLM TA when obtaining the student-conducted score? Since we choose GPT-4 to power the LLM TAs, using the LLM TA induces monetary costs. 2 We estimate that evaluating one submission costs from 0.05 to 0.09 USD. While it may seem reasonable to ask students to pay for using LLM TAs to obtain student-conducted scores, this can lead to inequality, as only those who can afford GPT-4 can use the LLM TAs and refine their assignments based on feedback from LLM TAs. It is also impractical for the teaching team to cover the costs of providing every student with access to GPT-4. \n\n4. What is the final score? Teacher-conducted or student-conducted score? How do you determine the final score for the assignment?",
            "score": 0.4685249591273393,
            "section_title": "Considerations of LLM TAs in Real-World Classrooms",
            "char_start_offset": 9121,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 90,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1786
                },
                {
                    "start": 1789,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1919
                }
            ],
            "ref_mentions": [
                {
                    "start": 244,
                    "end": 263,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1700439453125
        },
        {
            "corpus_id": "264591429",
            "title": "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation",
            "text": "Dataset.We experiment with the MT-Bench dataset, that evaluates LLMs as judges of other LLM's responses when acting as helpful AI assistants in multi-turn conversations (Zheng et al., 2023).It consists of instructions from 8 diverse domains e.g., writing, reasoning, math, coding, etc.\n\nEvaluation Metrics.We evaluate BSM (and baselines) using the following four metrics.\n\n\u2022 LLM-Human Agreement (Ag).Following past work (Zheng et al., 2023), we report LLM-human agreement \u2208 [0, 1] individually for turn-1 and turn-2 questions, and their combination.\u2022 Position Bias (PB).To evaluate whether BSM helps reduce the consistency problem with LLMbased evaluators, we report PB, which is the fraction of samples where the judgment changes based on the encoding order of the responses.\u2022 Length Bias (LB).We measure LB as the fraction of samples where humans prefer the shorter response but the evaluator model does not.In other words, we compute how often an evaluator chooses the longer response when according to human preference, it should not.\u2022 Self-enhancement Bias (SB).SB refers to an evaluator model preferring its own responses.Evaluating this bias in isolation is challenging because knowing when the model is choosing its own response because of this bias and not for another reason is an interpretability question.However, the question we are interested in studying here is the following: When an LLM is evaluating its own responses (which is a common phenomenon when using LLMs as evaluators), does BSM lead to better and more reliable evaluation?We measure this by considering the following setting.We use GPT-4 as the base judge model and consider the subset of samples from the MT-Bench benchmark where one of the responses is also generated by GPT-4.If BSM with GPT-4 improves human-agreement for this subset of samples, it suggests that even in scenarios where model A is judging its own outputs, BSM (with model A) leads to a better evaluator.",
            "score": 0.46614661077771624,
            "section_title": "Experimental Setup",
            "char_start_offset": 16017,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 8,
                    "end": 190
                },
                {
                    "start": 190,
                    "end": 285
                },
                {
                    "start": 287,
                    "end": 306
                },
                {
                    "start": 306,
                    "end": 371
                },
                {
                    "start": 373,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 549
                },
                {
                    "start": 549,
                    "end": 570
                },
                {
                    "start": 570,
                    "end": 776
                },
                {
                    "start": 776,
                    "end": 795
                },
                {
                    "start": 795,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 1038
                },
                {
                    "start": 1038,
                    "end": 1067
                },
                {
                    "start": 1067,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1317
                },
                {
                    "start": 1317,
                    "end": 1551
                },
                {
                    "start": 1551,
                    "end": 1604
                },
                {
                    "start": 1604,
                    "end": 1758
                },
                {
                    "start": 1758,
                    "end": 1953
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29638671875
        },
        {
            "corpus_id": "264591440",
            "title": "Analyzing Multilingual Competency of LLMs in Multi-Turn Instruction Following: A Case Study of Arabic",
            "text": "\u2022 The use of an LLM as a judge for evaluating LLMs has issues related to bias. As reported in (Zheng et al., 2023), in pairwise comparisons, the judge tends to favor its own answers compared to other models. For example, that study shows that GPT-4 favors itself with 10% higher win rate and Claude-V1 favors itself with 25% higher win rate. On the other hand, GPT-3.5 does not appear to favor itself. \n\n\u2022 Using GPT-4 as the judge and as an LLM under study might favor it in the scores. However, the score margin to the closet competitor is big enough to make any potential deviation in the scores insignificant, and we adhered to the original MT-Bench setup in the choice of judge in order to mirror the results and measure multilingual competency. \n\n\u2022 Other LLM judges than GPT-4 could be considered for evaluating the responses. However, the choice of alternative judges is currently rather limited when considering Arabic. The proficiency of models such as Claude or Bard in Arabic are not yet proven. Alternatively, multiple LLMs could be used for this task. A voting judgment mechanism could be considered over multiple open LLMs. \n\n\u2022 While GPT-4 exhibits competence in Arabic, its proficiency in the language falls short of its mastery of English. This discrepancy may have had an impact on certain aspects of our analyses, especially when comparing Arabic results to English results. \n\n\u2022 We used the same judgment prompt as in the English MT-Bench for the purpose of consistency. However, we note that the judgment prompt does not acknowledge important aspects such as safety and harmlessness of LLM responses. Also, the MT-score is a metric that combines multiple dimensions such as relevance, helpfulness, and creativity together to give an aggregate verdict. It might be useful to analyze model performance separately on these dimensions for a better understanding.",
            "score": 0.4658944205020948,
            "section_title": "Judging",
            "char_start_offset": 22417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1136
                },
                {
                    "start": 1139,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1876
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0233306884765625
        },
        {
            "corpus_id": "271544398",
            "title": "Automated literature research and review-generation method based on large language models",
            "text": "Research demonstrates LLMs excel in evaluation tasks, with GPT-4 surpassing both crowdsourced workers [26] and experts [27] in text annotation accuracy and reliability, and bias control for complex tasks requiring contextual knowledge reasoning [27]. LLMs show comparable or superior performance to human annotation in persuasiveness, accuracy, and satisfaction [28]. Studies confirm the capability of LLMs to evaluate other LLMs, with verification abilities improving faster than generation quality [29]. GPT-4's evaluations exceeds 80% consistency with human reviewers [30] and exceeds 85% alignment in pairwise comparisons [31], reaching nearly 100% agreement when performance differences are significant [31]. Self-verification benefits LLM performance [32], though biases issues [33], such as position bias [34], length bias [30], and self-bias [35], persist but can be reduced through proper design [36]. \n\nGiven characteristic writing patterns of LLMs and potential human evaluation bias, this study employs LLM-based evaluation exclusively. Existing LLM-based LLM evaluation methods encompass scoring, comparison, selection judgment, and comprehensive description. This study introduces a dual-baseline review quality evaluation framework, to minimize potential LLM evaluation bias and quantitatively compare LLM-generated reviews with peer-reviewed expert-written content, validating reliability through statistical analysis. In our method, we segmented 14 published Q1 reviews into 89 fragments based on semantic content. Using extracted topics from these fragments and the literature cited in the original text, we generated comparative reviews using Qwen2-7b-Instruct, Qwen2-72b-Instruct, and Claude3.5Sonnet. This methodology enabled direct comparison between human experts and LLMs in writing reviews with identical literature background, establishing a rigorous benchmark for LLMs given their limitations in accessing both human accumulated domain expertise and pre-1970 undigitized literature. \n\nIn the evaluation process, we compared the performance of two models of different scales from the open-source Qwen2 series (Qwen2-7b-Instruct and Qwen2-72b-Instruct) and the closed-source model Claude3.5Sonnet (Fig. 1).",
            "score": 0.4657512104917435,
            "section_title": "Evaluation of generated review quality",
            "char_start_offset": 8750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 2009
                },
                {
                    "start": 2012,
                    "end": 2221
                },
                {
                    "start": 2222,
                    "end": 2231
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 106,
                    "matchedPaperCorpusId": "257766307"
                },
                {
                    "start": 571,
                    "end": 575,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 626,
                    "end": 630,
                    "matchedPaperCorpusId": "259095491"
                },
                {
                    "start": 708,
                    "end": 712,
                    "matchedPaperCorpusId": "259095491"
                },
                {
                    "start": 830,
                    "end": 834,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63232421875
        },
        {
            "corpus_id": "276296422",
            "title": "SC4ANM: Identifying optimal section combinations for automated novelty prediction in academic papers",
            "text": "With the rapid advancement of artificial intelligence and natural language processing technologies, LLMs (Brown et al., 2020;Ouyang et al., 2022), particularly those supported by transformer-based architectures and pre-trained on massive datasets, have gradually come into the public eye. With the successive releases of ChatGPT and GPT-4 (OpenAI, 2024) by OpenAI, as well as Llama (Touvron et al., 2023a) and others (Meta, 2024;Touvron et al., 2023b;Chowdhery et al., 2023), these LLMs have demonstrated powerful language generation and understanding capabilities, attracting significant research interest from scholars in the academic community. (Patsakis et al., 2024;Shafee et al., 2024;Caruccio et al., 2024) Recently, research on using LLMs for peer review has been gaining increasing popularity. Liang et al. (Liang et al., 2023) conducted a large-scale empirical analysis to assess comments generated by GPT-4: they tag several metrics and investigated user satisfaction to measure comment quality. They found that while LLM-generated reviews were helpful, they might be non-generic and tended to focus on certain aspects of scientific feedback. Liu and Shah (Liu and Shah, 2023) validated the utility of LLMs across three tasks: identifying errors, verifying checklists, and choosing the \"better\" pa-per. They concluded that LLMs serve well as review assistants for specific reviewing tasks; however, they are not yet sufficient for conducting comprehensive evaluations of papers. Mike Thelwall (Thelwall, 2024) used GPT-4 to evaluate the quality of journal articles on a paper assessment dataset to test its effectiveness. The results indicated that GPT-4 appears to be insufficiently accurate for any formal or informal research quality assessment tasks. Zhou et al. (Zhou et al., 2024) conducted a comprehensive evaluation to determine whether LLMs can be qualified and reliable reviewers. They concluded that it is premature for LLMs to serve as automated scientific paper reviewers. Although there is potential for obtaining useful and accurate results, their current capabilities are not yet reliable enough.",
            "score": 0.46559314104082405,
            "section_title": "Large Language Models for Reviewing",
            "char_start_offset": 16466,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 125,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 125,
                    "end": 145,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 451,
                    "end": 474,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 648,
                    "end": 671,
                    "matchedPaperCorpusId": "269456918"
                },
                {
                    "start": 691,
                    "end": 713,
                    "matchedPaperCorpusId": "261038523"
                },
                {
                    "start": 816,
                    "end": 836,
                    "matchedPaperCorpusId": "263608784"
                },
                {
                    "start": 1504,
                    "end": 1520,
                    "matchedPaperCorpusId": "267547461"
                },
                {
                    "start": 1778,
                    "end": 1797,
                    "matchedPaperCorpusId": "269803977"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6328125
        },
        {
            "corpus_id": "273374964",
            "title": "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation",
            "text": "ChatEval (Chan et al., 2023), for instance, directly selects strong LLMs like GPT-4 to build multiple agents to debate and collaborate for evaluation. However, since it only utilizes LLMs from the same series, it still suffers from systematic biases. PRE (Chu et al., 2024) proposes to build a peer-review system with different types of evaluator LLMs selected with a well-crafted exam, which selects qualified evaluator LLMs by calculating the accuracy of the candidate LLMs' results compared to the manual annotations. Although this method can achieve superior evaluation performance, it still relies on human-annotated data for the exam and thus is not fully automated. \n\nInspired by PRE, we propose the Auto-PRE by designing an automated qualification exam. Previous studies (Zhao et al., 2015;Zeng et al., 2023) have shown that whether a human evaluator is excellent in the academic peer review system is influenced by three important factors: (1) Consistency: whether the reviewer produces consistent evaluation; (2) Self-Confidence: whether the reviewers can correctly estimate their evaluation confidence based on task difficulty; (3) Pertinence: whether the reviewer can capture the key information that distinguishes different candidates without affected by superficial factors that are not important for evaluation. Based on the above observations, we design three selection methods to filter evaluator LLMs automatically for peer review. All these methods involve no human annotations, thus making the whole framework a fully automated one that is open-ended, reference-free, cost-efficient, and robust to systematic biases introduced by LLMs. \n\nExperiment results on three tasks, summary generation, non-factoid question-answering and dialogue generation indicate that our Auto-PRE can achieve state-of-the-art performance similar to PRE and GPT-4 at a much lower cost. Additionally, we analyze how prompt strategies and evaluation formats could affect our framework to improve the generalizability of our work and provide more insights for future automated evaluation methods based on LLMs. \n\nIn conclusion, the contributions of this paper are as follows:",
            "score": 0.4629709121058305,
            "section_title": "Introduction",
            "char_start_offset": 2087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1655
                },
                {
                    "start": 1658,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2104
                },
                {
                    "start": 2107,
                    "end": 2169
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54296875
        },
        {
            "corpus_id": "267740270",
            "title": "Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements",
            "text": "Considering the large number of test samples, we utilize GPT4 as the judge for evaluation, which has become widely accepted, as noted in several studies (Touvron et al., 2023b;Chiang et al., 2023;Dettmers et al., 2023;Liu et al., 2023b;Chiang and Lee, 2023). Research has demonstrated that the evaluations made by GPT-4 align well with human judgments (Zheng et al., 2023;Li et al., 2023c). 3 esponse Quality: \n\nThe evaluation of Response Quality follows Chen et al. (2023c); Li et al. (2023bLi et al. ( , 2024b)), which involves a detailed rating system for the responses generated by the model. This system compares responses generated by two different LLMs on various dimensions, including helpfulness, relevance, accuracy, and level of detail. We also address the issue of positional bias in the LLM judge system, as discussed in the studies by Ko et al. (2020); Wang et al. (2023b) by presenting models' responses in two separate sequences for evaluation by the LLM judge. We then analyze the responses for each instruction by comparing them through a \"Win-Tie-Loss\" system. Then the win score will be calculated for better comparison:",
            "score": 0.4617287162406588,
            "section_title": "LLM Judge",
            "char_start_offset": 15578,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 409
                },
                {
                    "start": 412,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1140
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 196,
                    "matchedPaperCorpusId": "258461287"
                },
                {
                    "start": 236,
                    "end": 257,
                    "matchedPaperCorpusId": "258461287"
                },
                {
                    "start": 849,
                    "end": 865,
                    "matchedPaperCorpusId": "216868500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.041534423828125
        },
        {
            "corpus_id": "271909821",
            "title": "AI-Driven Review Systems: Evaluating LLMs in Scalable and Bias-Aware Academic Reviews",
            "text": "Our aim is to improve scientific writing, research, and communication by providing fast and reliable in-depth reviews on demand. This work evaluates the limitations and capabilities of GPT-4 to review papers and suggest revisions. LLMgenerated reviews align well with human reviewers when evaluated by blind human evaluation and an automatic GPT-4 comparison. We present our LLM reviewer system, Open-Reviewer, and the associated Papers with the Reviews site. \n\nTo our knowledge, we are the first to report on such a largescale empirical evaluation of LLM reviewing. \n\nUsing human reviews as a baseline, we evaluated value alignment and the process alignment of LLM reviews, i.e., we compared the quality of reviews and the adherence of the reviewing process to conference guidelines and scientific norms of practice. Prior work on LLM academic capabilities suggests that LLMs are now ready for specific reviewing tasks and appear to be more effective for some academic domains and less effective for others (Checco et al. 2021;Schulz et al. 2022;Liu and Shah 2023;Lu et al. 2024). Therefore, we conducted ablation studies and determined the types of errors and shortcomings the LLM can detect and review. When supplied with information about previous editorial decisions, the LLM aligns well with human reviewers. Furthermore, the LLM performs well in detecting specific errors and shortcomings, such as overclaiming, but not others, such as detecting cases in which the authors needed to follow expected norms. We find that iterative design and large-scale empirical evaluation are essential to calibrate the application of LLMs. \n\nThis work leverages LLMs in the review process, addressing challenges and offering proof-of-concept LLM review tools. We introduce and evaluate systems designed to streamline handling tens of thousands of academic papers, from initial collection to reviewing and evaluation. Our methods offer novel approaches to automating academic reviews, improving upon traditional reviews. Our analysis reveals that the system facilitates a more efficient review process and enhances the accessibility and quality of academic literature for both authors and the broader scholarly community. Using papers from arXiv and open-access Nature, coupled with our methods, shows promise in identifying highquality papers and emerging research trends.",
            "score": 0.4586881087101804,
            "section_title": "Conclusions",
            "char_start_offset": 25809,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1631
                },
                {
                    "start": 1634,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2364
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92431640625
        },
        {
            "corpus_id": "267587759",
            "title": "Generative Pre-Trained Transformer (GPT) in Research: A Systematic Review on Data Augmentation",
            "text": "Papers in [7][8][9] collectively suggest that while LLMs offer innovative tools for research writing, their use must be accompanied by careful consideration of ethical standards, methodological rigor, and the mitigation of biases. As highlighted in [7][8][9], one of the darning issues of using GPT or LLM-based technology in authoring academic publications involves the use of AI-based paraphrasing to hide potential plagiarism in scientific publications. \n\nNotwithstanding the concerns associated with the authoring aspect of research, the review at hand strategically narrows its focus to explore alternative dimensions of GPT and LLM applications in scholarly pursuits. Specifically, the examination focuses on data augmentation, where GPT and LLMs play a pivotal role in enhancing research data, generating features, and synthesizing data [10][11][12]. As shown in Figure 1, with GPT's advanced language understanding capabilities, features can be extracted from plain text information. It should be noted that previously, feature extraction from plain texts involved various natural language processing techniques like entity recognition, sentiment analysis, classification, and others, as shown in [13][14][15][16][17][18]. With the introduction of GPT and associated technologies, a simple prompt can extract various features from plain text (Figure 1). Moreover, as depicted in Figure 1. Semantically similar content could be added by GPT being part of the data augmentation process, improving the diversity and robustness of the data. Furthermore, rows of data could be synthetically generated by GPT, facilitating the training of the machine learning process during times of data scarcity or confidentiality (shown in Figure 1). The primary challenges identified in recent scholarship pertaining to the utilization of GPT and LLMs in research writing converge on concerns related to accuracy, potential biases, and ethical considerations [6]. Addressing these challenges requires a concerted effort to establish ethical guidelines and norms, ensuring the judicious use of LLMs in research endeavors [6]. The academic discourse underscores the significance of upholding scientific rigor and transparency, particularly in light of the potential biases embedded in LLM outputs [3][4][5].",
            "score": 0.458445438068245,
            "section_title": "Introduction",
            "char_start_offset": 1964,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2295
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 13,
                    "matchedPaperCorpusId": "260713371"
                },
                {
                    "start": 13,
                    "end": 16,
                    "matchedPaperCorpusId": "259834950"
                },
                {
                    "start": 16,
                    "end": 19,
                    "matchedPaperCorpusId": "259413439"
                },
                {
                    "start": 249,
                    "end": 252,
                    "matchedPaperCorpusId": "260713371"
                },
                {
                    "start": 252,
                    "end": 255,
                    "matchedPaperCorpusId": "259834950"
                },
                {
                    "start": 255,
                    "end": 258,
                    "matchedPaperCorpusId": "259413439"
                },
                {
                    "start": 844,
                    "end": 848,
                    "matchedPaperCorpusId": "250390738"
                },
                {
                    "start": 848,
                    "end": 852,
                    "matchedPaperCorpusId": "245142427"
                },
                {
                    "start": 1205,
                    "end": 1209,
                    "matchedPaperCorpusId": "252687108"
                },
                {
                    "start": 1213,
                    "end": 1217,
                    "matchedPaperCorpusId": "250943428"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "matchedPaperCorpusId": "247577222"
                },
                {
                    "start": 1221,
                    "end": 1225,
                    "matchedPaperCorpusId": "249138059"
                },
                {
                    "start": 1225,
                    "end": 1229,
                    "matchedPaperCorpusId": "252388838"
                },
                {
                    "start": 1949,
                    "end": 1952,
                    "matchedPaperCorpusId": "258701329"
                },
                {
                    "start": 2110,
                    "end": 2113,
                    "matchedPaperCorpusId": "258701329"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365234375
        },
        {
            "corpus_id": "268856493",
            "title": "METAL: Towards Multilingual Meta-Evaluation",
            "text": "LLMs also show position bias or scale region bias and are unable to distinguish between candidates that are close to each other (Golchin and Surdeanu, 2023).LLMs are sensitive to instructions and their capabilities vary for different metrics (Skopek et al., 2023;Wang et al., 2023b;Shen et al., 2023).Another significant challenge when using LLMs as evaluators is a limited assessment of their abilities in multilingual settings.Studies have shown that LLMs have inferior performance even on some high-resource languages and cannot be assessed extensively on low-resource languages due to a lack of benchmarks (Ahuja et al., 2023a).Therefore, it is still unclear if LLMs can replace human evaluations in multilingual settings.\n\nIn this paper, we introduce the METAL framework for a robust assessment of LLMs as evaluators in multilingual scenarios.Figure 1 shows an outline of our framework.The METAL framework is an end-to-end pipeline, that starts with first creating a rich meta-evaluation dataset that contains a variety of samples across the metrics of interest.We do this by systematically prompting GPT-4 to generate a wide range of sample data points, that are then evaluated by native speakers.In the next step, we compare LLM judgments with human judgments.For this, we draw on our previous work (Hada et al., 2024) to prompt LLMs for evaluations and subsequently compare the scores with human judgments.In particular, for the task of summarization we create a dataset of 1000 summaries covering 10 languages, with human ratings across 5 metrics. 1 Next, we obtain LLM evaluations from GPT-3.5-Turbo,GPT-4, and PaLM2 across these 1000 summaries and 5 metrics.\n\nOur findings show that the GPT-3.5-Turbo-basedevaluator does not perform well across languages and metrics, while evaluators based on GPT-4 and PaLM2 perform better.We find that the evaluation ability of LLMs significantly across languages, motivating the creation of a meticulously crafted metaevaluation dataset covering all target languages before using LLM-based evaluators.",
            "score": 0.4574194692264819,
            "section_title": "Introduction",
            "char_start_offset": 1550,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 157,
                    "end": 301
                },
                {
                    "start": 301,
                    "end": 429
                },
                {
                    "start": 429,
                    "end": 632
                },
                {
                    "start": 632,
                    "end": 726
                },
                {
                    "start": 728,
                    "end": 848
                },
                {
                    "start": 848,
                    "end": 891
                },
                {
                    "start": 891,
                    "end": 1067
                },
                {
                    "start": 1067,
                    "end": 1203
                },
                {
                    "start": 1203,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1414
                },
                {
                    "start": 1414,
                    "end": 1610
                },
                {
                    "start": 1610,
                    "end": 1669
                },
                {
                    "start": 1671,
                    "end": 1717
                },
                {
                    "start": 1717,
                    "end": 1836
                },
                {
                    "start": 1836,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 263,
                    "matchedPaperCorpusId": "263909080"
                },
                {
                    "start": 610,
                    "end": 631,
                    "matchedPaperCorpusId": "257663467"
                },
                {
                    "start": 1306,
                    "end": 1325,
                    "matchedPaperCorpusId": "261822638"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.287353515625
        },
        {
            "corpus_id": "273404356",
            "title": "MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback",
            "text": "The critique capabilities of LLMs have been extensively used for the automatic evaluation of response quality, often employing models like GPT-4 (Achiam et al., 2023;Liu et al., 2023;Fu et al., 2023) or critique-adjusted LLMs (Ke et al., 2023;Li et al., 2023). Despite their success, these meth-ods have demonstrated instability in certain complex task scenarios (Wang et al., 2023b;Zhang et al., 2024b). \n\nLLMs have shown a high correlation with human evaluations in tasks such as summarization and story generation, effectively scoring candidate texts or comparing them based on specific evaluation aspects (Chen et al., 2023a). For example, studies by Chiang and Lee (2023) and Kocmi and Federmann (2023) have shown that LLM evaluations produce results comparable to those of expert human evaluators in story generation and translation tasks. Similarly, research by Zheng et al. (2024) and Zhang et al. (2024a) indicates that powerful LLM reviewers, such as GPT-4, achieve over 80% consistency with human preferences in multi-turn dialogue scenarios, both in controlled and crowdsourced settings, reaching agreement levels similar to human evaluators. \n\nFurther evidence of GPT-4's effectiveness as an evaluator is demonstrated by its performance in the PROMETHEUS (Kim et al., 2023(Kim et al., , 2024) ) and CRITICBENCH (Lan et al., 2024) benchmarks. However, most LLM-as-judge research has primarily focused on general NLP fields, with limited exploration in specialized domains like clinical NLP. This gap is largely due to challenges such as the need for domain-specific knowledge, difficulties in designing evaluation prompts that meet domain standards, and the inherent challenges of generalizing evaluation tools to specialized fields (Yao et al., 2024;Singhal et al., 2023;Li et al., 2024). \n\nTo our knowledge, we are the first to explore the application of LLM-as-judge in the context of USMLE-MCQ generation and evaluation.",
            "score": 0.45696794063040647,
            "section_title": "LLM-as-Judge using LLM feedback",
            "char_start_offset": 31833,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1154
                },
                {
                    "start": 1157,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1801
                },
                {
                    "start": 1804,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 609,
                    "end": 629,
                    "matchedPaperCorpusId": "252918409"
                },
                {
                    "start": 869,
                    "end": 888,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 893,
                    "end": 913,
                    "matchedPaperCorpusId": "266551654"
                },
                {
                    "start": 1745,
                    "end": 1763,
                    "matchedPaperCorpusId": "267770269"
                },
                {
                    "start": 1763,
                    "end": 1784,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 1784,
                    "end": 1800,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1959228515625
        },
        {
            "corpus_id": "266174168",
            "title": "LLMEval: A Preliminary Study on How to Evaluate Large Language Models",
            "text": "Compared to (Zheng et al. 2023)'s study, our experimental results demonstrate that when using the star scoring evaluation method, the evaluation results of GPT-4 align more closely with manual evaluation. 6) GPT-4 as an evaluator has a stronger bias on longer and more verbose responses than human evaluators. As shown in Table 3, when there is a difference in length of more than 300 characters between two responses, GPT-4 has a 78.8% likelihood of selecting the longer text as the better one. In contrast, human annotators have a probability of 51.4% of choosing the longer text. \n\n7) Manual evaluation and GPT-4 automatic evaluation scores are less consistent on subjective questions. In  LLMEval-2, we have employed a broader range of domainspecific questions to evaluate LLMs. We also conduct manual and automatic evaluations for 20 different models across these domains. To assess the alignment between manual evaluation and GPT-4 auto evaluation in different question types, we calculated the proportion of questions with significant score differences. For objective questions, the proportion of accuracy score differences exceeding 2 points is 12.98%, while for subjective questions, this proportion increases to 37.05%. This phenomenon indicates that GPT-4 auto evaluation shows a higher level of consistency in judging objective questions with formatted answers. The proportion of questions with significant score differences for other criteria can be found in Table 4 and 5. 8) Annotators tend to give higher scores when answer hints are not provided. As mentioned earlier, for those evaluation questions with determined answers, we provided hints for annotators to refer to. We conducted additional Differences in Scores -Manual/GPT-4 % \u2206Accuracy \u2265 2 37.05% \u2206Accuracy \u2265 4 6.99% \u2206Fluency \u2265 2 3.49% \u2206Logicality \u2265 2 7.87% \u2206Informativeness \u2265 2 9.97% Table 4: The proportion of the difference between manual evaluation and GPT-4 automatic evaluation of subjective questions in LLMEval-2",
            "score": 0.4567631440281241,
            "section_title": "Comparison of Annotation Methods",
            "char_start_offset": 17651,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1994
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.443115234375
        },
        {
            "corpus_id": "263310448",
            "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
            "text": "LLMs as Evaluators. Owing to the effectiveness of LLMs, many recent research works have investigated their utility in various downstream tasks, such as machine translation (Kocmi and Federmann, 2023), summarization (Shen et al., 2023;Gao et al., 2023), code generation (Zhuo, 2023), writing assistance (Schick et al., 2023;Raheja et al., 2023), factual consistency (Cohen et al., 2023;Gekhman et al., 2023;Luo et al., 2023), and more. Additionally, many studies have leveraged LLMs for general-purpose NLG evaluation. For instance, Liu et al. (2023); Chen et al. (2023); Wang et al. (2023a) investigated the effectiveness of GPT-4 and ChatGPT against reference-free evaluation methods, whereas Fu et al. (2023) proposed an evaluation framework, GPTSCORE, to score generated texts. Recently, Li et al. (2023a) and Zheng et al. (2023) conducted similar experiments by employing LLMs as evaluators to judge the quality of generations in a pairwise setting. Although these works present promising results for LLMs as automatic evaluators, our work takes a closer look at machine artifacts that could be detrimental to data quality by benchmarking an exhaustive list of biases impacting LLMs-as-evaluators.",
            "score": 0.45672063380669103,
            "section_title": "Related Work",
            "char_start_offset": 4096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1201
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.427734375
        },
        {
            "corpus_id": "265103287",
            "title": "Strengths and Weaknesses of ChatGPT Models for Scientific Writing About Medical Vitamin B12: Mixed Methods Study",
            "text": "Additionally, it could theoretically reduce errors and inconsistencies that are often found in scientific writing. A recent release of the ChatGPT model, GPT-4, boasts enhanced accuracy and heightened attention to details in user prompts, as demonstrated by OpenAI's internal test results on an array of professional and academic exams [19]. However, its performance in the realm of scientific writing remains untested, and the results of OpenAI's evaluations have not been verified through peer-reviewed publications. In this study, an evaluation was performed on GPT-4's ability to generate scientific text on the specific topic of vitamin B12 treatment. In addition, the performance of GPT-4 was compared to that of its predecessor, GPT-3.5.",
            "score": 0.4556620464421468,
            "section_title": "Introduction",
            "char_start_offset": 2096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 744
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57177734375
        },
        {
            "corpus_id": "271270236",
            "title": "AI AI Bias: Large Language Models Favor Their Own Generated Content",
            "text": "To complement our studies on LLM bias, we conducted an initial experiment to gauge human preferences in similar decision contexts (See Table 5 and Tabel 6 for results).It is important to note that these preferences were collected by research assistants rather than actual users.This study serves as a preliminary investigation with a small sample size and best-effort human baseline, and the findings are not definitive.We used the product details and scientific paper abstracts generated by both GPT-3.5 and GPT-4, and presented them to a group of human evaluators.These participants were asked to choose which descriptions they preferred without knowing whether they were written by a human or an LLM.They also had the option to state that they had no preference between the two presented texts.\n\nParticipants were presented with pairs of descriptions: one generated by an LLM and one written by a human.Each participant evaluated a randomized set of pairs to mitigate any order or preference bias.The results in Table 5 and Table 6 indicate that human evaluators generally preferred human-generated product descriptions over those generated by LLMs, with GPT-3.5 having a preference ratio of 0.29 and GPT-4 at 0.28.For scientific paper abstracts, the preferences leaned more towards LLM-generated texts, with GPT-4 abstracts being preferred at a ratio of 0.60 compared to GPT-3.5 at 0.46.",
            "score": 0.4549226498715961,
            "section_title": "Preferences of Humans",
            "char_start_offset": 14486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 278
                },
                {
                    "start": 278,
                    "end": 420
                },
                {
                    "start": 420,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 703
                },
                {
                    "start": 703,
                    "end": 797
                },
                {
                    "start": 799,
                    "end": 906
                },
                {
                    "start": 906,
                    "end": 1000
                },
                {
                    "start": 1000,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1391
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5263671875
        },
        {
            "corpus_id": "267320303",
            "title": "Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate",
            "text": "Results Table 4 compares the agreement rate between SCALEEVAL's meta-evaluation and each LLM evaluator across criteria and scenarios. We observe that gpt-4-turbo, when serving as an evaluator, has the highest agreement rates with our meta-evaluation, particularly in the scenarios of brainstorming, dialog, and ODG with the helpfulness criterion. It stands out with the highest overall average score of 0.780. However, our selected open-source model evaluator, auto-j, outperforms gpt-4-turbo in evaluating coding questions based on the helpfulness criterion. In addition, it exhibits the highest agreement rate with our metaevaluation in the judgement scenario, according to the helpfulness criterion, indicating it as the most capable evaluator in this setting. It also achieves comparable results with other closed-source models like claude-2 and gpt-3.5-turbo in most of the other scenarios. \n\nWhile gpt-4-turbo performs the best as an evaluator in a majority of scenarios, it is not necessarily the best choice when we take into consideration its relatively high API costs. In fact, both the more affordable version (gpt-3.5-turbo) and our selected free, open-source model (auto-j) show comparable performance in scenarios like judgement and writing. For coding-related evaluations, the slightly less expensive claude-2 could be a more cost-effective alternative to gpt-4-turbo. \n\n8 Exp-III: Meta-Evaluation with Criteria Prompt Format Variations Q3: How do the qualities of criteria prompts influence the robustness of LLMs as evaluators in different scenarios? Prior studies have revealed that variations in prompts can substantially affect the behavior of LLMs, particularly with the text they generate. With this in mind, we define various formatted criteria for evaluating LLM responses under each scenario. This approach aims to examine the extent to which different formats of criteria prompts influence both the performance and robustness of LLMs as evaluators. \n\nSetup We define five variations of the same criteria prompts: shortened, gibberish, shuffled, flipped, and masked (see Table 7 under Appendix A for detailed format).",
            "score": 0.451349489483429,
            "section_title": "Exp-II: Meta-Evaluation vs. LLM Evaluators",
            "char_start_offset": 27455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1974
                },
                {
                    "start": 1977,
                    "end": 2142
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22021484375
        },
        {
            "corpus_id": "263831267",
            "title": "A Closer Look into Automatic Evaluation Using Large Language Models",
            "text": "An LLM used as an evaluation metric should be affordable and accessible to whoever wants to use it. Based on this principle, we use ChatGPT (gpt3.5-turbo-0613) (OpenAI, 2022) for evaluation since it has lower cost and improved performance compared with other GPT-3.5 models. Chat-GPT is also used in LLM evaluation and G-Eval. While Liu et al. (2023) further use GPT-4 (OpenAI, 2023) in their experiments, we cannot use GPT-4 in our experiments since most people, including us, have limited or no access to GPT-4, making it utterly unsuitable as an evaluation metric. \n\nIn our preliminary experiments, we also try to use the best open LLM (at the time of writing this manuscript) on Open LLM leaderboard, the falcon-40b-instruct model (Almazrouei et al., 2023), but we find it cannot follow the instructions and rate the samples very well. Hence, we exclude open LLMs in our paper. \n\n3 Better Usage of LLM for Evaluation tion on SummEval when using GPT-4 for evaluation. By scrutinizing their results, we find that the correlations when using auto CoT and not using it often differ by less than 0.02. This raises two questions: (1) Is this difference statistically significant? (2) Does auto CoT yield higher correlations for different LLMs and datasets? To answer these questions, we use ChatGPT to rate the samples in SummEval and Topical-Chat using two sets of prompts, one with the evaluation steps generated using auto CoT and one without those evaluation steps. In this experiment, we follow G-Eval and restrict ChatGPT to output only a numeric score. Following Graham and Baldwin (2014), we use William's test for significance to see if the Pearson's r of using and not using auto CoT is statistically significantly different. We try to follow the prompts used in G-Eval when possible; still, we have to construct some prompts since Liu et al. (2023) only release part of the prompts and some of which are problematic.",
            "score": 0.4487204381905607,
            "section_title": "Large Language Models",
            "char_start_offset": 6112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 881
                },
                {
                    "start": 884,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1925
                }
            ],
            "ref_mentions": [
                {
                    "start": 1568,
                    "end": 1593,
                    "matchedPaperCorpusId": "15132118"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.224609375
        },
        {
            "corpus_id": "267783118",
            "title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
            "text": "Educational materials such as survey articles in specialized fields like computer science traditionally require tremendous expert inputs and are therefore expensive to create and update. Recently, Large Language Models (LLMs) have achieved significant success across various general tasks. However, their effectiveness and limitations in the education domain are yet to be fully explored. In this work, we examine the proficiency of LLMs in generating succinct survey articles specific to the niche field of NLP in computer science, focusing on a curated list of 99 topics. Automated benchmarks reveal that GPT-4 surpasses its predecessors, inluding GPT-3.5, PaLM2, and LLaMa2 by margins ranging from 2% to 20% in comparison to the established ground truth. We compare both human and GPT-based evaluation scores and provide in-depth analysis. While our findings suggest that GPT-created surveys are more contemporary and accessible than human-authored ones, certain limitations were observed. Notably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors. At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation.",
            "score": 0.44529424999309564,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5322265625
        },
        {
            "corpus_id": "273346449",
            "title": "Learning the Bitter Lesson: Empirical Evidence from 20 Years of CVPR Proceedings",
            "text": "The integration of Large Language Models (LLMs) into the evaluation of academic texts has emerged as a significant area of interest. LLMs, such as GPT-4, have demonstrated remarkable capabilities in processing and analyzing large volumes of information quickly, making them suitable for various applications, including the assessment of academic literature. For instance, research has shown that LLMs can effectively assist in title and abstract screening for literature reviews, which is crucial in the biomedical domain (Dennst\u00e4dt, 2024). Moreover, LLMs have been employed to perform qualitative data analysis, producing consistent results across multiple iterations (Tai et al., 2023). \n\nIn addition to their analytical capabilities, LLMs have been shown to possess a degree of human-like judgment in evaluating the quality of text. The G-EVAL framework, which utilizes LLMs to assess the quality of natural language generation outputs, demonstrates that LLMs can align closely with human evaluators in certain contexts (Liu, 2023). However, the deployment of LLMs in academic evaluation is not without challenges. LLMs can exhibit biases similar to those found in human judgments, which may affect the fairness and accuracy of their evaluations (Acerbi, 2023). Furthermore, the phenomenon of \"hallucination,\" where LLMs produce plausible but factually incorrect information, poses a risk in academic contexts (Buchanan, 2023). \n\nThe role of LLMs in answering questions and generating hypotheses also merits attention. Their ability to provide detailed responses to complex queries has been leveraged in various educational settings, enhancing learning experiences and fa-cilitating knowledge acquisition (Polverini, 2024). However, the tendency of LLMs to produce verbose outputs can sometimes obscure the clarity of their answers, necessitating careful prompt engineering (Yeadon, 2024). In the context of academic research, LLMs can assist in generating hypotheses and guiding exploratory studies, contributing to the advancement of knowledge in various fields (Irons et al., 2023). \n\nDespite the promising applications of LLMs in academic evaluation and research, it is crucial to establish ethical guidelines and best practices for their use.",
            "score": 0.4447867327769749,
            "section_title": "Large Language Models in Academic Evaluation",
            "char_start_offset": 5394,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1430
                },
                {
                    "start": 1433,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2088
                },
                {
                    "start": 2091,
                    "end": 2250
                }
            ],
            "ref_mentions": [
                {
                    "start": 1413,
                    "end": 1429,
                    "matchedPaperCorpusId": "265442718"
                },
                {
                    "start": 1708,
                    "end": 1725,
                    "matchedPaperCorpusId": "262083876"
                },
                {
                    "start": 1877,
                    "end": 1891,
                    "matchedPaperCorpusId": "261682569"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57763671875
        },
        {
            "corpus_id": "278000613",
            "title": "Evaluation Framework for AI Systems in \"the Wild\"",
            "text": "18 WHITE PAPER: Evaluation Framework for AI Systems in \"the Wild\" \n\nIn some settings such as essay writing, code review, or other writing-like tasks, there are many different correct answers to the same question due to stylistic differences. Scoring these outputs with traditional metrics is difficult due to the variety of responses. \n\nIdeally, human evaluators are the gold standard for assessing quality given the varied nature of the model output. However, such evaluations can be expensive to solicit and hard to replicate. As a result, LLMs have been used in place of humans to evaluate the quality of model outputs. Because of their powerful language understanding abilities, treating LLMs as evaluators allows practitioners to design nuanced evaluation metrics for evaluating far more complex LLM outputs than possible with previous metrics like Accuracy. These LLM-as-judge systems can be used to evaluate many types of outputs and behaviors, such as conversational fluency. \n\nWhen effective, LLMs can serve as fast and replicable evaluators for a variety of settings, particularly when prompted with detailed and objective rubrics for scoring. Nevertheless, using LLMs to evaluate quality can introduce multiple biases. Models are known to prefer certain styles of output due to their training, potentially biasing evaluation when the same model is used to generate output and evaluate. Further, models are still limited by the complexity of the task relative to the model's language understanding abilities, particularly for tasks that involve very long passages of texts or those that involve highly detailed scoring rubrics. Good practices for using an LLM to evaluate the output of other LLMs include verifying that an LLM's scores align with human judgments, using one or more different LLMs to evaluate that are different from the model used to generate so that the model doesn't prefer its own outputs.",
            "score": 0.4446068381501143,
            "section_title": "Takeaways",
            "char_start_offset": 26098,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 68,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 334
                },
                {
                    "start": 337,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1919
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44775390625
        },
        {
            "corpus_id": "259360998",
            "title": "Style Over Substance: Evaluation Biases for Large Language Models",
            "text": "As human evaluation can be costly and inefficient, there is an increase in the use of advanced LLMs, such as GPT-4, to evaluate model outputs. In our work, we also use LLMs as judges to assess answer quality. However, previous studies rely solely on GPT-4 as the LLM judge (Chiang et al., 2023;Li et al., 2023a;Zheng et al., 2023), which may not be appropriate for our work as our answers are refined by humans after being generated by GPT-4. This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4.5 By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study. \n\nWe utilize the evaluation prompt from Dettmers et al. ( 2023), as presented in Figure 1. The prompt assesses the answers based on their helpfulness, relevance, accuracy, and level of detail, while also aiming to avoid bias related to answer ordering.",
            "score": 0.444141000168608,
            "section_title": "LLM Evaluation",
            "char_start_offset": 13307,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 797
                },
                {
                    "start": 800,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1050
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64013671875
        },
        {
            "corpus_id": "259187647",
            "title": "Friend or Foe? Exploring the Implications of Large Language Models on the Science System",
            "text": "Second, the dissemination of knowledge might be democratized as these tools can easily polish the language of a text or even translate research output to multiple languages, both of which would level the field for researchers who speak English as a foreign language (Corless, 2023;Liebrenz et al., 2023). \n\nAmong the risks for the science system identified in the literature are the adverse effects on the academic quality assurance mechanisms and, subsequently, on scientific integrity. The avalanche of AI-generated \"scientific-looking papers devoid of scientific content\" (Grimaldi & Ehrler, 2023, p. 879) is widely expected to overburden the academic review process and foster plagiarism (Dwivedi et al., 2023). Biases are expected to be reinforced and errors introduced into the scholarly debate that might be difficult to identify and correct (Lund et al., 2023). A recent study by Liang et al. (2023) evaluating the performance of several widely-used GPT detectors found that they consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Several scholars expect that LLMs may lead to an increase in misinformation and disinformation and more \"junk science\" (Corless, 2023 n.p.). In this regard, Lund et al. (2023) worry that the use of LLM-based tools in academia not only raises concerns about the reproducibility and transparency of research but may undermine trust in the scientific process (see also Van Noorden, 2022).",
            "score": 0.44377901901346406,
            "section_title": "Opportunities and risks for the science system",
            "char_start_offset": 8797,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 304
                },
                {
                    "start": 307,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1508
                }
            ],
            "ref_mentions": [
                {
                    "start": 849,
                    "end": 868,
                    "matchedPaperCorpusId": "257463753"
                },
                {
                    "start": 1280,
                    "end": 1298,
                    "matchedPaperCorpusId": "257463753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.264892578125
        },
        {
            "corpus_id": "267587759",
            "title": "Generative Pre-Trained Transformer (GPT) in Research: A Systematic Review on Data Augmentation",
            "text": "The academic discourse underscores the significance of upholding scientific rigor and transparency, particularly in light of the potential biases embedded in LLM outputs [3][4][5]. Papers in [7][8][9] collectively suggest that while LLMs offer innovative tools for research writing, their use must be accompanied by careful consideration of ethical standards, methodological rigor, and the mitigation of biases. As highlighted in [7][8][9], one of the darning issues of using GPT or LLM-based technology in authoring academic publications involves the use of AI-based paraphrasing to hide potential plagiarism in scientific publications. \n\nNotwithstanding the concerns associated with the authoring aspect of research, the review at hand strategically narrows its focus to explore alternative dimensions of GPT and LLM applications in scholarly pursuits. Specifically, the examination focuses on data augmentation, where GPT and LLMs play a pivotal role in enhancing research data, generating features, and synthesizing data [10][11][12]. As shown in Figure 1, with GPT's advanced language understanding capabilities, features can be extracted from plain text information. It should be noted that previously, feature extraction from plain texts involved various natural language processing techniques like entity recognition, sentiment analysis, classification, and others, as shown in [13][14][15][16][17][18]. With the introduction of GPT and associated technologies, a simple prompt can extract various features from plain text (Figure 1). Moreover, as depicted in Figure 1. Semantically similar content could be added by GPT being part of the data augmentation process, improving the diversity and robustness of the data. Furthermore, rows of data could be synthetically generated by GPT, facilitating the training of the machine learning process during times of data scarcity or confidentiality (shown in Figure 1). In the recent literature landscape, various reviews have emerged concerning the adoption of GPT in research [19][20][21].",
            "score": 0.443587885725688,
            "section_title": "Introduction",
            "char_start_offset": 4079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2042
                }
            ],
            "ref_mentions": [
                {
                    "start": 173,
                    "end": 176,
                    "matchedPaperCorpusId": "265127282"
                },
                {
                    "start": 191,
                    "end": 194,
                    "matchedPaperCorpusId": "260713371"
                },
                {
                    "start": 194,
                    "end": 197,
                    "matchedPaperCorpusId": "259834950"
                },
                {
                    "start": 197,
                    "end": 200,
                    "matchedPaperCorpusId": "259413439"
                },
                {
                    "start": 430,
                    "end": 433,
                    "matchedPaperCorpusId": "260713371"
                },
                {
                    "start": 433,
                    "end": 436,
                    "matchedPaperCorpusId": "259834950"
                },
                {
                    "start": 436,
                    "end": 439,
                    "matchedPaperCorpusId": "259413439"
                },
                {
                    "start": 1025,
                    "end": 1029,
                    "matchedPaperCorpusId": "250390738"
                },
                {
                    "start": 1029,
                    "end": 1033,
                    "matchedPaperCorpusId": "245142427"
                },
                {
                    "start": 1386,
                    "end": 1390,
                    "matchedPaperCorpusId": "252687108"
                },
                {
                    "start": 1394,
                    "end": 1398,
                    "matchedPaperCorpusId": "250943428"
                },
                {
                    "start": 1398,
                    "end": 1402,
                    "matchedPaperCorpusId": "247577222"
                },
                {
                    "start": 1402,
                    "end": 1406,
                    "matchedPaperCorpusId": "249138059"
                },
                {
                    "start": 1406,
                    "end": 1410,
                    "matchedPaperCorpusId": "252388838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38525390625
        },
        {
            "corpus_id": "258841126",
            "title": "On Learning to Summarize with Large Language Models as References",
            "text": "We now conduct experiments using GPTRank for model training and evaluation. The reference LLMs we choose are GPT-3.5 and GPT-4 (OpenAI, 2023) since they have shown state-of-the-art performance on summarization evaluation (Liu et al., 2023a). 8 To enable a more accurate evaluation, we choose GPT-3.5 as the baseline model and use the LLMs to conduct a pairwise comparison between different systems and GPT-3.5. To reduce the positional bias in LLM evaluation results as noted by Wang et al. (2023b), we evaluate each summary pair in both directions and take the average of results. In addition, we allow the LLMs to predict a tie between two summaries. 9  In Figure 1, we report the pairwise comparison results of different models against GPT-3.5 under both GPT-3.5 and GPT-4's evaluation. We note: \n\n(1) As in \u00a73.1.2, using better references and contrastive learning helps the model to achieve better LLM-based evaluation results. \n\n(2) Interestingly, GPT-3.5 prefers both BRIO.GPT-3.5 and BRIO.GPT-4 over its own outputs in the pairwise comparison, suggesting that contrastive learning can efficiently optimize the summarization model for a specific evaluation metric. \n\n(3) LLM-based evaluation results vary across different LLMs. For example, while GPT-3.5 prefers BRIO.GPT-4 over itself, GPT-4 prefers GPT-3.5. (4) BRIO.GPT-3.5 can outperform BART.GPT-4 despite the fact that BRIO.GPT-3.5 is trained with a reference LLM that is supposedly weaker, indicating the advantage of contrastive learning. The reference-based evaluation results can be found in Table 2.",
            "score": 0.44305972693530954,
            "section_title": "Learning with GPTRank",
            "char_start_offset": 10692,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1170
                },
                {
                    "start": 1173,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1566
                }
            ],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 240,
                    "matchedPaperCorpusId": "257804696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365966796875
        },
        {
            "corpus_id": "271769401",
            "title": "Using Large Language Models to Evaluate Biomedical Query-Focused Summarisation",
            "text": "Inspired by work by Shu et al. (2023), we experimented with the use of LLMs as meta-evaluators of 3 different evaluators. In particular: \n\n1. To aid the LLM in scoring repetition, we provided it with a \"Repscore\" which took the number of unique words in a response and divided it by the total number of words. \n\n2. Smog score (Laughlin, 1969), which looks at the number of polysyllabic words and sentences in a text, was used to aid the scoring of readability. \n\n3. Finally, the output from the fine-tuned GPT-3.5 model was also included to aid with the scoring of all dimensions. \n\nTesting was done primarily on GPT-4, since GPT-3.5 showed a very limited capability in reasoning with other scores.",
            "score": 0.4428803613895857,
            "section_title": "LLMs as Meta Evaluators",
            "char_start_offset": 11176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 136
                },
                {
                    "start": 139,
                    "end": 309
                },
                {
                    "start": 312,
                    "end": 460
                },
                {
                    "start": 463,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 698
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 342,
                    "matchedPaperCorpusId": "9571753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51953125
        },
        {
            "corpus_id": "258298706",
            "title": "Can ChatGPT be used to generate scientific hypotheses?",
            "text": "GPT-4: Taking into account the challenges mentioned earlier, here's a redesigned experiment to evaluate the ability of a language model like GPT to generate scientific hypotheses: \n\n1. Select a specific scientific field and define a set of well-defined research questions or problems within that field. Choose questions that are not too narrow or specialized, as this may lead to difficulties in finding experts and in evaluating the hypotheses. \n\n2. Assemble a diverse panel of experts within the chosen field to serve as evaluators. These experts should be knowledgeable in the field, but not necessarily specialized in the precise topics of the research questions. \n\nHaving a diverse panel can help mitigate subjectivity and biases in the evaluation process. 3. Divide the panel of experts into two groups: one group will generate hypotheses based on the research questions, and the other group will evaluate the hypotheses. The evaluators should be blinded to the source of the hypotheses (whether they were generated by humans or the language model). 4. Provide the language model with the same set of research questions or problems and ask it to generate hypotheses. Ensure that the model's input is clear and accurately represents the current state of knowledge in the field. 5. Collect the hypotheses generated by both the human experts and the language model. Anonymize and randomize the hypotheses to ensure a fair evaluation process. 6. Have the evaluator group rate the hypotheses based on predefined criteria, such as novelty, relevance, potential impact, and feasibility. The evaluators should also provide qualitative feedback on each hypothesis. 7. Analyze the ratings and feedback to determine if there are significant differences between the AI-generated and human-generated hypotheses. Compare the average scores, the distribution of scores, and the qualitative feedback to assess the performance of the language model. 8. Conduct a follow-up discussion with the panel of experts to address any concerns or ambiguities that arose during the evaluation process. This discussion can also serve as an opportunity to explore the broader implications and ethical considerations of using AI-generated hypotheses in scientific research.",
            "score": 0.4421964233455423,
            "section_title": "3: Please redesign an experiment based on the given considerations above to prove the hypothesis.",
            "char_start_offset": 44130,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 445
                },
                {
                    "start": 448,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2079
                },
                {
                    "start": 2080,
                    "end": 2248
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45458984375
        },
        {
            "corpus_id": "271051503",
            "title": "Evaluating Language Models for Generating and Judging Programming Feedback",
            "text": "The idea of using an LLM to judge the output of other LLMs -LLMs-as-judges -was first studied in the work of Zheng et al. [40], showing good promise, but also limitations, e.g., in grading math and reasoning tasks. Since then, GPT-4 has been used in multiple studies as a judge of the quality of other LLMs' generations [26], also in educational contexts [10,18]. Moreover, the reliance on GPT-4, a proprietary model, has sparked interest in leveraging other open-source language models to act as judges [4]. Yet, recent work has highlighted the limitations of relying on a single language model for evaluating the quality of other language models' outputs, and suggested employing a diverse ensemble of smaller models from different LLM families as a jury for cheaper and less biased evaluations [37]. When answering our second research question, we test this hypothesis by comparing the usage of single judges (both open-source and proprietary) and that of a jury of smaller open-source language models.",
            "score": 0.44214746385696724,
            "section_title": "Using Language Models as Judges",
            "char_start_offset": 6779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 1005
                }
            ],
            "ref_mentions": [
                {
                    "start": 320,
                    "end": 324,
                    "matchedPaperCorpusId": "257804696"
                },
                {
                    "start": 359,
                    "end": 362,
                    "matchedPaperCorpusId": "269626718"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1807861328125
        },
        {
            "corpus_id": "267616740",
            "title": "Rethinking Data Selection for Supervised Fine-Tuning",
            "text": "Another crucial potential bias is verbosity bias, meaning LLM judges usually prefer longer responses. However, GPT-4 as judge mitigates this bias to large extent: Zheng et al. (2023) shows that GPT-4 is robust to repetitive attack with failure rate of only 8%; whereas other LLM judges such as ChatGPT and Claude (Anthropic, 2023) fail more than 90% of the time. Following Chen et al. (2023b), we use the evaluation prompt shown in Appendix Figure 4) and winning score of #Win\u2212#Lose #Testset + 1 where #Testset = #Win + #Tie + #Lose as evaluation metric.",
            "score": 0.44200214362717793,
            "section_title": "Baselines.",
            "char_start_offset": 6887,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 554
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 182,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1494140625
        },
        {
            "corpus_id": "263608784",
            "title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
            "text": "Given these challenges, there is an urgent need for crafting scalable and efficient feedback mechanisms that can enrich and streamline the scientific feedback process. Adopting such advancements holds the promise of not just elevating the quality and scope of scientific research, given the concerning deceleration in scientific advancements 15,16 , but also of democratizing its access across the scientific community. \n\nLarge language models (LLMs) [17][18][19] , especially those powered by Transformer-based architectures and pretrained at immense scales, have opened up great potential in various applications [20][21][22][23] . While LLMs have made remarkable strides in various domains, the promises and perils of leveraging LLMs for scientific feedback remain largely unknown. Despite recent attempts that explore the potential uses of such tools in areas such as automating paper screening 24 , error identification 25 , and checklist verification 26 1 , we lack large-scale empirical evidence on whether and how LLMs may be used to facilitate scientific feedback and augment current academic practices. \n\nIn this work, we present the first large-scale systematic analysis characterizing the potential reliability and credibility of leveraging LLM for generating scientific feedback. Specifically, we developed a GPT-4 based scientific feedback generation pipeline that takes the raw PDF of a paper and produces structured feedback (Fig. 1a). The system is designed to generate constructive feedback across various key aspects, mirroring the review structure of leading interdisciplinary journals 27,28 and conferences [29][30][31][32][33] , including: 1) Significance and novelty, 2) Potential reasons for acceptance, 3) Potential reasons for rejection, and 4) Suggestions for improvement. \n\nTo characterize the informativeness of GPT-4 generated feedback, we conducted both a retrospective analysis and a prospective user study. In the retrospective analysis, we applied our pipeline on papers that had previously been assessed by human reviewers. We then compared the LLM feedback with the human feedback. We assessed the degree of overlap between key points raised by both sources to gauge the effectiveness and reliability of LLM feedback. Furthermore, we compared the topic distributions of LLM feedback and human feedback.",
            "score": 0.44059211996991066,
            "section_title": "Introduction",
            "char_start_offset": 2314,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 419
                },
                {
                    "start": 422,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1112
                },
                {
                    "start": 1115,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1799
                },
                {
                    "start": 1802,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2253
                },
                {
                    "start": 2254,
                    "end": 2338
                }
            ],
            "ref_mentions": [
                {
                    "start": 342,
                    "end": 345,
                    "matchedPaperCorpusId": "126306204"
                },
                {
                    "start": 345,
                    "end": 347,
                    "matchedPaperCorpusId": "3703088"
                },
                {
                    "start": 451,
                    "end": 455,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 455,
                    "end": 459,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 615,
                    "end": 619,
                    "matchedPaperCorpusId": "258375371"
                },
                {
                    "start": 623,
                    "end": 627,
                    "matchedPaperCorpusId": "254876189"
                },
                {
                    "start": 899,
                    "end": 901,
                    "matchedPaperCorpusId": "249579676"
                },
                {
                    "start": 1644,
                    "end": 1648,
                    "matchedPaperCorpusId": "85728208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82470703125
        },
        {
            "corpus_id": "271891958",
            "title": "The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation",
            "text": "LLM-as-a-Judge. The LLM-as-Judge evaluations, as shown in Table 2, indicate that GPT-4o outperforms all the models on PandaLM-Eval and MT-Bench achieving a Cohen's Kappa score of 0.688 and 0.410 respectively. Additionally, GPT-4o consistently ranks in second position across all three datasets. This consistent top-tier performance underscores GPT's effectiveness as a reliable judge in evaluating LLM responses. Gemma-2-27b outperforms all other models on the Fair-Eval dataset, achieving the highest score in this particular evaluation. However, it's important to note that the Fair-Eval dataset is relatively small, consisting of only 80 samples. Furthermore, the Fair-Eval dataset primarily compares GPT-3.5-Turbo with Vicuna-13b, which might introduce a bias in favor of GPT models when GPT is the evaluator. Figure 3 shows that GPT-4o selects GPT-3.5-Turbo as the better agent 50 times and Vicuna-13b 30 times. This indicates a potential bias in favor of GPT responses when GPT-4o is the evaluator. Additionally, we can observe in the figure that Llama models also display a similar bias towards GPT responses, whereas Gemma models do not exhibit this bias, suggesting that Gemma is more impartial in its evaluations. \n\nLLMs-as-a-Jury. In evaluating of LLMs-as-a-Jury, we analyze the top three juries from each dataset as shown in Table 4. Notably, the scores exhibit considerable variation across the different datasets. On the Fair-Eval and MT-Bench datasets, the jury approach outperformed the judge approach, indicating a potential advantage in using multiple models for evaluation. For instance, on Fair-Eval, the highest-performing jury achieves a Cohen's Kappa of 0.428 while the judge achieves Kappa of 0.389, suggesting a relatively strong agreement with human judgments compared to individual judges.",
            "score": 0.44013344704362006,
            "section_title": "Results and Discussion",
            "char_start_offset": 16395,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 16,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1816
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0546875
        },
        {
            "corpus_id": "268531221",
            "title": "NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens",
            "text": "Settings We employ two evaluation settings: a generative setting where models directly generate short answers, and a multichoice setting with four provided options. \n\nPrompts We use uniform prompts for all LLMs, with a start part, novel content, questions, choices in the multichoice setting, and an end part. The prompt structure is shown in Appendix Table 11. Truncation Due to input length limitations, we truncated the novel content from the end to the front following Bai et al. (2023); Li et al. (2023a); An et al. (2023), to meet with the max input length, while keeping questions and other prompts complete. \n\nEvaluating Generative Results Previous researches have proved the ability of LLMs in evaluating the machine-generated answers align with human judgements (Wang et al., 2023;An et al., 2023;Li et al., 2023a) After a pilot study showing that the models show no several preference towards the answers generated by its own kinds (See Appendix C.4.2, we choose GPT-4 to evaluate our generated results. We further conducted a human evaluation on 800 pieces of generative outputs and carried out an inter-evaluator agreement (IEA) test between two human evaluators and the GPT-4 evaluator. In the IEA test, annotators also serve as human evaluators of the novels they were familiar with. Their evaluations were compared to those of GPT-4 evaluators, with Cohen's kappa score calculated to measure agreement. As shown in Table 7, the result of 89.25% in Cohen's Kappa indicates a high agreement towards the GPT-4 evaluating results. NovelQA primarily consists of objective questions, which have clear, verifiable answers with a factual basis in the text. This objectivity significantly reduces the impact of model bias in LLM-as-Judge evaluation Commercial LLMs The APIs of commercial LLMs utilized are gpt-4-0125-preview, gpt-4o-mini, Claude-2.1, Claude-3-sonnet and Claude-3.5-sonnet.",
            "score": 0.4388688372016569,
            "section_title": "IMPLEMENTATIONS",
            "char_start_offset": 18118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 167,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 1896
                }
            ],
            "ref_mentions": [
                {
                    "start": 772,
                    "end": 791,
                    "matchedPaperCorpusId": "259951284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1651611328125
        },
        {
            "corpus_id": "272968899",
            "title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
            "text": "Prior work has demonstrated the effectiveness of LLMs as automatic evaluators for scientific synthesis [6,17,19], arguing that they are becoming both the tools and the standards for language assessment. Thus, for the evaluation task, we utilized GPT-4-Turbo, the most advanced LLM available at the time of writing. The evaluation prompt and script are available athttps://github.com/jd-coderepos/ scisynthesis/tree/main/gpt-4%20synthesis-evaluator. The prompt includes all nine evaluation criteria along with their corresponding questions. Each evaluation criteria was assessed based on a rating scale from 1 to 5 as follows: 1. Very Bad, 2. Bad, 3. Moderate, 4. Good, and 5. Very Good. Each rating level was specifically tailored with a description to aid in the assessment of each criterion. For instance, consider the rating scale description for the first criteria i.e. \"1. Relevancy: is the information in the answer relevant to the problem?\" as follows: Rating 1. Very bad: The information provided does not relate to the research problem, showing a lack of understanding or connection to the topic. Rating 2. Bad: The information occasionally relates to the research problem but lacks direct and consistent relevance. Rating 3. Moderate: The information is generally related to the research problem, with occasional lapses in direct relevance. Rating 4. Good: The information is consistently relevant to the research problem, with only minor exceptions. Rating 5. Very good: The synthesis is directly and consistently relevant to the research problem, demonstrating a deep understanding of the topic and its nuances. For the full prompts, including detailed descriptions of the rating scale for all criteria, please refer to the system prompt. Finally, similar to 3-fold cross-validation, the GPT-4 synthesis evaluator was run thrice on the same data samples to ensure consistent assessment. The resulting scores, which did not show significant divergence largely due to our detailed prompt specification of the expected evaluation task leaving little room for ambiguity in interpretation, were averaged and reported.",
            "score": 0.4388387508618159,
            "section_title": "LLM Evaluation of Synthesis Quality",
            "char_start_offset": 21573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 106,
                    "matchedPaperCorpusId": "259095491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.837890625
        },
        {
            "corpus_id": "269430756",
            "title": "InspectorRAGet: An Introspection Platform for RAG Evaluation",
            "text": "There has been a significant uptick in the popularity of LLMs as judges to evaluate the quality of LLM responses in RAG settings, complemented by active research in the efficacy of these judges (Chiang and Lee, 2023;Shen et al., 2023). Our platform also seamlessly supports analyzing the performance of LLM-as-a-judge approaches, and we illustrate this use case using MT-Bench (Zheng et al., 2023), a dataset of 80 high quality multiturn questions. The MT-Bench authors compare LLM-as-a-judge with humans by releasing human judgments (provided by 58 experts) and algorithmic GPT-4 judgments on the MT-Bench dataset. \n\nThis INSPECTORRAGET use case is not cen-tered around comparing model performance, but rather judge performance. The MT-Bench authors discuss several limitations of LLM-as-a-judge: positional bias, verbosity bias, self-enhancement bias (judge favors its own model's answers), and limited reasoning ability (low reasoning and math capability). They show that GPT-4 judge matches human evaluation at over 80%. We expand on their insights and introduce additional ones. \n\nSelf-Enhancement Bias: The expert annotators tend to agree with each other and match GPT-4 closely on all models, but GPT-4 seems to prefer responses from its own model. Source: Annotator Behavior, Model Behavior. Action: Inform stakeholders of the slight bias of GPT-4. \n\nVerbosity Bias: Answer length is strongly correlated with win-rate. This is true for LLM-as-ajudge and human annotators. Claude-v1, GPT-3.5, and GPT-4 usually have the longer response and win, while Llama and Alpaca rarely have the longer response and usually lose. Source: Model Behavior, Action: Analyze data and share with stakeholders. Do the answers need to be long for these questions? What is missing in the short responses? \n\nPositional Bias: As hinted in the paper, the first answer being favored is not as much of an issue for GPT-4. We believe it also may be model dependent.",
            "score": 0.4388116434589967,
            "section_title": "Analyzing LLM-as-a-Judge Performance",
            "char_start_offset": 18881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1356
                },
                {
                    "start": 1359,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1790
                },
                {
                    "start": 1793,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 1945
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 216,
                    "matchedPaperCorpusId": "258461287"
                },
                {
                    "start": 216,
                    "end": 234,
                    "matchedPaperCorpusId": "258833685"
                },
                {
                    "start": 377,
                    "end": 397,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1083984375
        },
        {
            "corpus_id": "269293456",
            "title": "LLMChain: Blockchain-Based Reputation System for Sharing and Evaluating Large Language Models",
            "text": "To assess the credibility and capabilities of LLMs, several studies have introduced diverse evaluation methods, including pairwise comparison, single-answer grading, or referenceguided grading, employing another LLM as an evaluator.[2], [15].These methodologies offer advantages in scalability and interoperability.Nevertheless, it comes with notable limitations: 1) Position Bias, where the evaluator tends to favor the initial model; 2) Verbosity Bias, where the evaluator prefers longer responses over shorter ones; and 3) Self-Enhancement/Promotion Bias, where the judging model prioritizes its own text or that generated from a similar model.Moreover, evaluating a LLM using another LLM appears paradoxical since the evaluator itself is subject to evaluation.On the other hand, alignment-based methods are used to make large-scale alignment research more accessible like OpenAssistant conversations [20], which is a corpus of conversations that resemble interactions with assistants, created and annotated by humans.Nonetheless, alignment-based methods face some scalability challenges and annotation expenses.In Core-GPT [21] and [22], authors focus on assessing the credibility of LLMs.Core-GPT [21] proposes an approach that combines open-access scientific literature with LLMs to improve their reliability and trustworthiness.However, its methodology's scope is limited to two LLMs, \"GPT3.5\"and \"GPT-4\", failing to illuminate the credibility gap between open-source and commercial models.In contrast, the approach proposed in [22] introduces an automated workflow designed to manage an increased number of requests/responses, facilitating the assessment of the credibility of multiple LLMs.In G-Eval [16], which is a framework that leverages large language models, used a Chain-of-Thoughts (CoT) and a form-filling paradigm to evaluate the quality of Natural Language Generation (NLG) outputs.G-Eval experimentation involves two generation tasks: text summarization and dialogue generation.However, here again, the methodology is limited to only two LLMs which are \"GPT3.5\"and \"GPT-4\".",
            "score": 0.43720996620517605,
            "section_title": "A. LLMs Evaluation",
            "char_start_offset": 5918,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 232,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 647
                },
                {
                    "start": 647,
                    "end": 764
                },
                {
                    "start": 764,
                    "end": 1021
                },
                {
                    "start": 1021,
                    "end": 1115
                },
                {
                    "start": 1115,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1335
                },
                {
                    "start": 1335,
                    "end": 1400
                },
                {
                    "start": 1400,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1699
                },
                {
                    "start": 1699,
                    "end": 1902
                },
                {
                    "start": 1902,
                    "end": 1999
                },
                {
                    "start": 1999,
                    "end": 2082
                },
                {
                    "start": 2082,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 235,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 1127,
                    "end": 1131,
                    "matchedPaperCorpusId": "259501770"
                },
                {
                    "start": 1202,
                    "end": 1206,
                    "matchedPaperCorpusId": "259501770"
                },
                {
                    "start": 1709,
                    "end": 1713,
                    "matchedPaperCorpusId": "257804696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.260986328125
        },
        {
            "corpus_id": "261050476",
            "title": "ChatGPT applications in Academic Research: A Review of Benefits, Concerns, and Recommendations",
            "text": "agiarism issues, accountability, copyright considerations, potential loss of diverse writing styles, privacy and security, transparency, credibility, validity, presence of bias, and the potential impact on scientific progress, such as a decrease in groundbreaking discoveries. Conclusion ChatGPT has the potential to revolutionize scientific writing as a valuable tool for researchers. However, it cannot replace human expertise and critical thinking. Researchers must exercise caution, ensuring the generated content complements their own knowledge. Ethical standards should be upheld, involving knowledgeable human researchers to avoid biases and inaccuracies. Collaboration among stakeholders and training on AI technology are essential for identifying best practices in LLMs use and maintaining scientific integrity.",
            "score": 0.4361593640464583,
            "section_title": "abstract",
            "char_start_offset": 2347,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42529296875
        },
        {
            "corpus_id": "267320303",
            "title": "Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate",
            "text": "Setup We define five variations of the same criteria prompts: shortened, gibberish, shuffled, flipped, and masked (see Table 7 under Appendix A for detailed format). With these criteria format variations, we intend to observe how the LLMs as evaluators would respond differently when conducting evaluation. We compare the example-level agreement rate between SCALEEVAL's meta-evaluation results and each LLM evaluator. \n\nResults Based on Table 5, we observe that the performance of LLMs as evaluators generally deteriorates when certain letters in the criteria prompts are masked. Furthermore, the removal of guiding phrases at the beginning, such as \"Not Helpful\" or \"Highly Helpful\", can also diminish their effectiveness as evaluators. Both gpt-4-turbo and gpt-3.5-turbo demonstrate some resilience to these adversarially formatted criteria prompts, maintaining a relatively consistent agreement rates across various criteria formats. In contrast, Claude-2 often showcases confusion and refuses to evaluate, particularly in cases with gibberish and masked criteria prompts, where it rejects answering about half of the questions. It typically responds with statements like, \"Unfortunately I do not have enough information here to provide a fair evaluation... The criteria describe different quality levels, but there is no detail on what specific aspects of the responses should be assessed... any judgement risks being arbitrary or biased...\". None of the LLMs as evaluators we tested maintained very similar evaluation capabilities when faced with these adversarially formatted criteria prompts, indicating a limitation in these LLMs as evaluators' current design and application. Despite their advanced capabilities in fulfilling a variety of tasks, they may still struggle with understanding and responding accurately to substituted criteria information, highlighting an area for potential improvement in future iterations of LLM technology. Among all the different formatted criteria, we highlight the cases where the LLMs perform the best as evaluators in Table 5.",
            "score": 0.4361263430312863,
            "section_title": "Exp-II: Meta-Evaluation vs. LLM Evaluators",
            "char_start_offset": 29432,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 418
                },
                {
                    "start": 421,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2073
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.053802490234375
        },
        {
            "corpus_id": "278624250",
            "title": "The impact of large language models on computer science student writing",
            "text": "LLMs like GPT-4 are transforming education by addressing challenges in AEC and automated essay evaluation (AEE). In AEC education, GPT-4 demonstrated strengths in providing sustainable feedback and improvement suggestions, but showed limitations in ranking precision, prioritizing form over content, and handling large or poorly formatted documents, making it more suitable for general feedback rather than grading (Castro et al., 2024). In academic assessment, ChatGPT-4 highlights the potential of tailored, reliable tools for accurate grading and constructive feedback (Kostic et al., 2024). Furthermore, a framework was developed to address the impact of LLM on academic fraud in writing assignments, although some supporting literature lacks empirical validation (Ya-Ping Hsiao & Chiu, 2023). \n\nThe integration of LLMs in education presents challenges that must be addressed to harness their potential. While LLMs enhance efficiency, metrics for quantifying bias are essential to prevent perpetuating stereotypes. LLMs need mechanisms to help them validate outputs prone to inaccuracy or hallucinations (Meyer, 2023). Ethical risks such as plagiarism, disinformation, and academic conflicts also require careful management (Buruk, 2023;Nam & Bai, 2023). Tools such as ChatGPT promise to help students with essay writing, but they face limitations, including hallucinations (Han, 2023). Addressing challenges such as interdisciplinary reasoning, student modeling, social biases, cheating prevention (Zdravkova, 2025), and multimodal education is critical to fully leverage LLM (Huang et al., 2024). Insights from studies on authorship attribution highlight the importance of robustness, explainability, and trustworthiness in AI applications (Huang et al., 2024). \n\nAI can reshape education (OECD) and to help in the research process (OECD, 2023), but the inclusion of LLMs in education must be done carefully and cautiously, paying special attention to potential challenges and gaps (OECD, 2025).",
            "score": 0.4357897597200537,
            "section_title": "Review of student writing without and with LLM support",
            "char_start_offset": 9845,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 797
                },
                {
                    "start": 800,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1767
                },
                {
                    "start": 1770,
                    "end": 2001
                }
            ],
            "ref_mentions": [
                {
                    "start": 415,
                    "end": 436,
                    "matchedPaperCorpusId": "29564940"
                },
                {
                    "start": 572,
                    "end": 593,
                    "matchedPaperCorpusId": "269962440"
                },
                {
                    "start": 777,
                    "end": 796,
                    "matchedPaperCorpusId": "262183178"
                },
                {
                    "start": 1108,
                    "end": 1121,
                    "matchedPaperCorpusId": "259834950"
                },
                {
                    "start": 1241,
                    "end": 1257,
                    "matchedPaperCorpusId": "265222363"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29296875
        },
        {
            "corpus_id": "267406182",
            "title": "LLM-based NLG Evaluation: Current Status and Challenges",
            "text": "Zhang et al. (2023) find that under certain conditions, widening and deepening the network of LLMs can better align its evaluation with human judgments. Saha et al. (2024) propose a branch-solvemerge strategy, assigning LLMs the roles of decomposing problems, solving them, and aggregating answers, thereby improving the accuracy and reliability of evaluations. Wu et al. (2023) assume that different people such as politicians and the general public have different concerns about the quality of news summaries, use LLMs to play different roles in evaluation accordingly, and aggregate the results finally. \n\nDifferent LLMs. Different from having the same LLM play different roles, some research has used different LLMs (such as GPT-4 and Claude) in their studies. The use of a single LLM as evaluator may introduce bias, resulting in unfair evaluation results. In light of this, Bai et al. ( 2023) design a decentralized Peer-examination method, using different LLMs as evaluators and then aggregating the results. Further, Li, Patel, and Du (2023) let different LLMs serve as evaluators in pairwise comparisons and then have them go through a round of discussion to reach the final result. Additionally, Cohen et al. (2023) evaluate the factuality of texts through the interaction of two LLMs, where the LLM that generated the text acts as the examinee and the other LLM as the examiner.",
            "score": 0.43425207700226526,
            "section_title": "Role and Interaction",
            "char_start_offset": 27741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1389
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 171,
                    "matchedPaperCorpusId": "264591429"
                },
                {
                    "start": 362,
                    "end": 378,
                    "matchedPaperCorpusId": "257767249"
                },
                {
                    "start": 1206,
                    "end": 1225,
                    "matchedPaperCorpusId": "258833288"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05194091796875
        },
        {
            "corpus_id": "272968899",
            "title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
            "text": "Mistral's syntheses tend to be general and lack depth, with both models critiqued for insufficient insight into the research problem. 5. Integration. GPT-4 effectively integrates sources into a structured synthesis, whereas Mistral tends to list sources without transitions. GPT-4 is more consistent in creating a unified narrative, though both could improve paragraph organization. 6. Cohesion. GPT-4 offers more cohesive syntheses with smoother transitions and well-connected ideas, whereas Mistral's work often reads like separate summaries. GPT-4 effectively groups studies thematically, enhancing synthesis cohesion. 7. Coherence. GPT-4 shows higher overall coherence, while Mistral's syntheses can seem disjointed. Both models could strengthen connections between ideas, but GPT-4 is closer to achieving this. 8. Readability. Both models produce clear syntheses, but Mistral's style is slightly preferred for its simplicity. GPT-4 adheres well to stylistic norms but can be overly complex. Despite structural issues, Mistral's writing quality is praised. 9. Conciseness. GPT-4 generally adheres to word limits better, while Mistral often exceeds or falls short of the 200-word target. Both are praised for being non-repetitive. \n\nFinally, the domain-specific survey evaluations reveal that GPT-4 performs best in Earth Science and Computer Science, while Mistral excels in Earth Science and Chemistry. Both LLMs show weaker performance in Linguistics and Sociology, suggesting a better fit for engineering and hard sciences. For more insights, visit our repository. \n\nThis first part of the paper has introduced the scientific synthesis task, LLM synthesis generators, the multidisciplinary ORKG dataset, and insights from both an automatic LLM evaluator and a human survey. These elements contribute to defining the LLMs4Synthesis framework, which optimizes open-source LLMs for synthesis generation. The second part evaluates this framework, particularly against Mistral-7B, focusing on improvements in output format and addressing weaknesses in the nine evaluation criteria.",
            "score": 0.4323257599694882,
            "section_title": "Survey Results",
            "char_start_offset": 30843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1233
                },
                {
                    "start": 1236,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1571
                },
                {
                    "start": 1574,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2083
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7119140625
        },
        {
            "corpus_id": "260209167",
            "title": "Evaluating Large Language Models for Radiology Natural Language Processing",
            "text": "The rapid development of LLMs has been revolutionizing the field of natural language processing [52,53,54] and domains that benefit from NLP [55,56,57,58,36,59,60,28,61,62]. These powerful models have shown significant performance in many NLP tasks, like natural language generation (NLG) and even Artificial General Intelligence (AGI). However, utilizing these models effectively and efficiently requires a practical understanding of their capabilities and limitations, and overall performance, so evaluating these models is of paramount importance. \n\nTo compare the capabilities of different LLMs, researchers usually test with benchmark datasets in various fields (such as literature, chemistry, biology, etc.), and then evaluate their performance according to traditional indicators (such as correct answer rate, recall rate, and F1 value). The most recent study from OpenAI [63] includes the pioneering research study that assesses the performance of large language models (i.e. GPT-4) on academic and professional exams specifically crafted for educated individuals. The findings demonstrate exceptional performance of GPT-4 across a diverse array of subjects, encompassing the Uniform Bar Exam and GRE. Furthermore, an independent study conducted by Microsoft reveals that GPT-4 outperforms the USMLE, the comprehensive medical residents' professional examination, by a significant margin [64]. Holmes et al. [65] explore the utilization of LLMs in addressing radiation oncology physics inquiries, offering insights into the scientific and medical realms. This research serves as a valuable benchmark for evaluating the performance in radiation oncology physics scenarios of LLMs. \n\nUnlike the studies above that used traditional assessment research, Zhuang et al. [66] introduced a novel cognitive science-based [67] methodology for evaluating LLMs. Specifically, inspired by computerized adaptive testing (CAT) in psychometrics, they proposed an adaptive testing framework for evaluating LLMs that adjusts the characteristics of test items, such as difficulty level, based on the performance of individual models. They performed fine-grained diagnosis on the latest 6 instruction-tuned LLMs (i.e.",
            "score": 0.4297061146983381,
            "section_title": "Evaluating Large Language Models",
            "char_start_offset": 7054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 550
                },
                {
                    "start": 553,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "250635911"
                },
                {
                    "start": 163,
                    "end": 166,
                    "matchedPaperCorpusId": "255776274"
                },
                {
                    "start": 166,
                    "end": 169,
                    "matchedPaperCorpusId": "252369176"
                },
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "259917027"
                },
                {
                    "start": 1416,
                    "end": 1420,
                    "matchedPaperCorpusId": "257921233"
                },
                {
                    "start": 1820,
                    "end": 1824,
                    "matchedPaperCorpusId": "237100591"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2059326171875
        },
        {
            "corpus_id": "270440146",
            "title": "EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries",
            "text": "These results highlight the efficacy of EHRNoteQA in evaluating LLM performance within the specified clinical context, compared to other benchmarks. \n\nLastly, we reconfirm the validity of our GPT-4-based evaluation method for EHRNoteQA by comparing the correlations between clinician-evaluated LLM scores and EHRNoteQA scores obtained through other evaluation methods. For open-ended questions, when evaluating with traditional methods such as BLEU [45], ROUGE-L [34], Exact Match, and cosine similarity (using Sentence-BERT [48] and ClinicalBERT [2]), the results show lower correlation compared to our GPT-4 based evaluation scores. For multi-choice questions, many studies employ probability-based scoring. Following this approach, we test two types of probability-based scoring methods on EHRNoteQA using LM-Evaluation Harness [17]: one measuring the probability of the answer index (e.g., A) and another measuring the probability of the correct answer choice text. However, both results show lower correlation compared to our GPT-4 based evaluation scores. No.",
            "score": 0.4285972596526406,
            "section_title": "The Reliability of EHRNoteQA as a Proxy for Clinician Evaluations",
            "char_start_offset": 23190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 151,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1065
                }
            ],
            "ref_mentions": [
                {
                    "start": 449,
                    "end": 453,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 463,
                    "end": 467,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 547,
                    "end": 550,
                    "matchedPaperCorpusId": "102352093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05877685546875
        },
        {
            "corpus_id": "273374964",
            "title": "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation",
            "text": "Recently, the persistent advance of large language models (LLMs) has attracted a lot of attention in both academic and industry (Li et al., 2023;Yang et al., 2023). As LLMs evolve rapidly, how to evaluate their performance effectively and efficiently has become a crucial research question. \n\nExisting evaluation methods for LLMs can be categorized into two types: manual evaluation (Zheng et al., 2023) and automated evaluation (Chang et al., 2024). Manual evaluation is considered the most reliable and effective method, but it is usually sub-optimal due to its high costs in practice. Automated evaluation aims to reduce the cost by directly assessing model performance without using human annotations. However, existing Figure 1: An illustration of existing peer-review methods for language generation evaluation. Methods like ChatEval construct evaluators with homogeneous LLMs, which may systematically prefer outputs generated by models from the same origins; Methods like PRE construct diverse evaluators for peer review but require a human-annotated exam to select qualified evaluators. \n\nautomated evaluation methods often support limited types of task formats (e.g., multiple-choice questions) and need human-created references for judgments. While recent studies have attempted to build reference-free frameworks for open-ended task evaluation with LLMs, research (Zeng et al., 2023) has shown LLM-based evaluators, including the powerful GPT-4 (Achiam et al., 2023), may often prefer answers generated from models sharing the same origin with them, which introduces systematic biases into the evaluation framework and thus has limited reliability in practice. \n\nTo this end, recent research has investigated the possibility of employing multiple types of LLMs to evaluate collaboratively (as shown in Figure 1) and achieved notable performance. Similar to human peer review, when unqualified LLMs participate in peer review, they often impair the method's performance. Thus, how to select the appropriate LLMs as evaluators (or reviewers) is a crucial issue. ChatEval (Chan et al., 2023), for instance, directly selects strong LLMs like GPT-4 to build multiple agents to debate and collaborate for evaluation.",
            "score": 0.42781516276560533,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 290
                },
                {
                    "start": 293,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1672
                },
                {
                    "start": 1675,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 429,
                    "end": 449,
                    "matchedPaperCorpusId": "259360395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.499267578125
        },
        {
            "corpus_id": "259274988",
            "title": "Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting",
            "text": "Can GPT-4 be used in automatic evaluation? \n\nSince the food effect summarization would be used to assist PSG assessment, it is imperative for FDA professionals to evaluate the quality of the summary, though the evaluation by human is quite laborious and expensive. In this work, we find that GPT-4 evaluation provides a cheap and reasonable alternative to human evaluation. When using GPT-4 to rate the performance of different models against the reference summary, there are a number of issues. First, there exists an inconsistency problem whereby there is no guarantee that an LLM will produce the identical output for the same input every time. A good practice to fix the problem is to set temperature = 0; the temperature parameter of the model controls the randomness of the text generated, with 0 being deterministic. Second, we find a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt. To mitigate such recency bias, we recommend reporting the mean score over both orders. Third, we observe from Figure 4 that GPT-4 assigns significantly higher scores to its own outputs compared to human ratings (64% vs. 43%), which represents the propensity of the model to favor its own suggestions. Future work should investigate the existence of potential biases in GPT-4 evaluation as well as possible mitigation strategies. \n\nDespite the potential of GPT-4 for automating evaluation, it is not as flexible as humans, particularly in situations where specific definitions of summary quality are not provided. The performance of GPT-4 hence may vary due to the lack of clarity in evaluation standards. GPT-4 aligns with the human evaluation, concluding that it generates better summaries than ChatGPT. However, GPT-4 encounters difficulty distinguishing scenarios where both models produce equally good summaries. In cases where human assessors perceive the summaries as equally good, GPT-4 may favor a summary that includes relevant contextual information does not present in the other, such as \"explicitly linking it to a high-fat, high-calorie meal\" or \"mention the lack of clinically significant differences\". It also may penalize certain aspects based on subjective criteria, such as \"less concise\".",
            "score": 0.42768764564373063,
            "section_title": "Which model performs better in the study?",
            "char_start_offset": 33838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 45,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1374
                },
                {
                    "start": 1377,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2253
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54150390625
        },
        {
            "corpus_id": "278166056",
            "title": "LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations",
            "text": "-LLMs prefer text from their own model. -Being language models, LLM evaluators tend to assign higher scores to text that aligns closely with their own generation patterns-effectively equating textual quality with per-token likelihood. This leads to a preference for outputs produced by the same model family. For instance, GPT-4 may systematically favor responses generated by GPT-4-based systems, even when human assessors detect no meaningful quality difference [38,39,49,78]. This results in distorted system rankings and compromises the validity of the evaluation outcomes. \n\nModels may be optimized to align with the LLM's biases rather than real-world relevance assessments, undermining the credibility of the evaluation. \n\nQuantify effect. The experimental protocol from Liu et al. [38] can quantify this effect. It involves recording the LLM versions used in both systems and evaluators, and analyzing how often evaluators favor systems built with the same underlying model. \n\nGuardrail. One mitigation strategy is to reserve a specific LLM (or family of LLMs) exclusively for evaluation purposes, ensuring it is not used in any system under test. However, due to overlapping training corpora across models, this bias may still persist. A more robust alternative is to involve multiple LLMs in the evaluation and aggregate relevance judgments using majority voting -while omitting the vote of any evaluator that shares lineage with the system under consideration. [11,48]. In contrast, human assessors are better equipped to recognize nuance, novelty, and contextual diversity, which LLMs frequently overlook [61]. \n\nMore fundamentally, when LLMs define what is relevant across the board, they implicitly set a ceiling for what systems can achieve. This can penalize systems that offer innovative or non-standard responses that fall outside the LLM's implicit norms [64]. \n\nQuantify effect. This trope's impact can only be assessed through independent evaluations involving human judges from diverse socio-cultural backgrounds. \n\nGuardrail. While human annotation workflows can be designed to ensure a variety of perspectives, achieving this with LLMs is far more difficult. Persona-based prompting strategies [69] have been proposed as a mitigation, but emerging evidence highlights their limitations [33].",
            "score": 0.42760182283238923,
            "section_title": "Eval Trope #3: LLM Narcissism",
            "char_start_offset": 13610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 982
                },
                {
                    "start": 985,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1879
                },
                {
                    "start": 1882,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2035
                },
                {
                    "start": 2038,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2315
                }
            ],
            "ref_mentions": [
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "257804696"
                },
                {
                    "start": 468,
                    "end": 471,
                    "matchedPaperCorpusId": "265220860"
                },
                {
                    "start": 471,
                    "end": 474,
                    "matchedPaperCorpusId": "269293311"
                },
                {
                    "start": 474,
                    "end": 477,
                    "matchedPaperCorpusId": "267751249"
                },
                {
                    "start": 789,
                    "end": 793,
                    "matchedPaperCorpusId": "257804696"
                },
                {
                    "start": 1472,
                    "end": 1476,
                    "matchedPaperCorpusId": "270560864"
                },
                {
                    "start": 1617,
                    "end": 1621,
                    "matchedPaperCorpusId": "270123737"
                },
                {
                    "start": 1874,
                    "end": 1878,
                    "matchedPaperCorpusId": "272828021"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1611328125
        },
        {
            "corpus_id": "270870769",
            "title": "Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?",
            "text": "RQ4: Does GPT-4 have a recognizable style for a literature expert when generating creative text without constraints? \n\nIn our experimentation, we do not constraint GPT-4 writing style via prompting, and we request assessors to (blindly) identify if each text has been written by a machine or a human. We then explore not only if the assessors are able to recognize machine-produce text, but whether this recognition improves along the evaluation process. \n\nRQ5: Can we effectively measure creativity using Boden's framework in the context of AIgenerated texts? Margaret Boden's definition of creativity (Boden, 2004(Boden, , 2010) ) requires novelty, surprise, and value in creative outputs. These minimal requisites are ubiquitous in most definitions of creativity, and we used them as a guide to prepare an evaluation rubric for our expert assessors. The rubric provides a framework for an objective analysis of Boden's dimensions, where we can measure if originality (novelty and surprise in the context of literary writing) and attractiveness (the value of the literary text) do correlate with creativity assessments. \n\nThe main contributions of our work are: \n\n1. We conduct the first comprehensive symmetrical empirical study that compares a state of the art LLM (GPT-4) with an award-winning novelist, quoted as one of the best of its generation. With this comparison we approach the question of whether LLMs can already be better than any human at creative writing, rather than better than average humans. Our methodology includes a carefully crafted rubric to evaluate creative writing texts according to Boden's dimensions of creativity. \n\n2. Our results indicate that, when judged by expert critics and scholars, GPT-4 creative writing skills are not ready to compete with top human writers. The expert assessments collected strongly prefer Patricio Pron to GPT-4 in all quality dimensions considered in the study. \n\n3. We also provide quantitative evidence that (i) prompting GPT-4 with titles provided by the novelist improves its writing; (ii) GPT-4 creative writing skills degrade in Spanish (with respect to English); and (iii) when freed from stylistic constraints, GPT-4 writing style seems to be recognizable, specially after some exposure to its writing.",
            "score": 0.4271909830821695,
            "section_title": "Introduction",
            "char_start_offset": 4043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 119,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1121
                },
                {
                    "start": 1124,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1647
                },
                {
                    "start": 1650,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1925
                },
                {
                    "start": 1928,
                    "end": 2274
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1304931640625
        },
        {
            "corpus_id": "272223451",
            "title": "The transformative impact of large language models on medical writing and publishing: current applications, challenges and future directions",
            "text": "The use of LLM tools in academic writing has been associated with perceived benefits and efficiency gains in the research and writing process [10]. A quantitative study found that incorporating ChatGPT into the workflow for professional writing tasks reduced the average time taken by 40% and increased output quality by 18% [15]. This potential for increased productivity and output quality has been a driving factor in the adoption of LLMs, especially given the growing pressure on researchers to increase their research productivity and output [16]. \n\nHowever, the ease with which LLMs can generate convincing academic content has raised concerns about the potential for misuse and fraud. One study demonstrated that GPT-3 can create a highly convincing fraudulent article resembling a genuine scientific paper in terms of word usage, sentence structure, and overall composition, all within just 1 h and without any special training of the user [17]. Similarly, another study in early 2023 used Chat-GPT-4 to generate 2 fake orthopedic surgery papers, with one passing review and being accepted, and the other being rejected but referred to another journal for consideration [18]. \n\nThe challenges in detecting AI-generated content further complicate the issue. In a study where ChatGPT-3.5 generated 50 fake research abstracts from titles, only 8% met specific formatting criteria, yet achieved a 100% originality score in plagiarism detectors [19]. While AI detectors identified them as AI-created, human reviewers correctly spotted only 68% as AI-crafted and mistakenly tagged 14% of original abstracts as such. This highlights the nuanced challenges and considerations in integrating AI into academic writing while upholding scientific rigor. \n\nThe lack of unified guidelines and unclear policies regarding the extent of AI tool usage considered acceptable has left research-Fig. 1. Large language models (LLMs) can be used in various steps of research and writing. A detailed tutorial of how to utilize large language models during each process is provided as a supplementary material. ers in a state of uncertainty [8]. The term \"use of AI\" encompasses a wide spectrum of applications, ranging from providing a keyword to generate an entire manuscript, listing items to be mentioned and converting them into paragraphs, or strictly using AI for typo and punctuation correction only.",
            "score": 0.4268043195385025,
            "section_title": "Potential benefits and challenges of LLM usage in academic writing",
            "char_start_offset": 3606,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1183
                },
                {
                    "start": 1186,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1749
                },
                {
                    "start": 1752,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2391
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 146,
                    "matchedPaperCorpusId": "264129393"
                },
                {
                    "start": 325,
                    "end": 329,
                    "matchedPaperCorpusId": "257394529"
                },
                {
                    "start": 547,
                    "end": 551,
                    "matchedPaperCorpusId": "207375831"
                },
                {
                    "start": 948,
                    "end": 952,
                    "matchedPaperCorpusId": "258494319"
                },
                {
                    "start": 1178,
                    "end": 1182,
                    "matchedPaperCorpusId": "259843054"
                },
                {
                    "start": 1448,
                    "end": 1452,
                    "matchedPaperCorpusId": "258354722"
                },
                {
                    "start": 2124,
                    "end": 2127,
                    "matchedPaperCorpusId": "266845109"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.427978515625
        },
        {
            "corpus_id": "269626718",
            "title": "Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge",
            "text": "For instance, it could be used in evaluating the quality of prompting techniques and generation parameters, or exploring alternative language models (e.g; open source onces) across a large number of generations.Still, even in these scenarios, ideally, we would not be using GPT-4 as a judge for research fully on its own, but as a complement to human evaluations [8].\n\nOpen-source judges.Using GPT-4 as a judge has shown promises, in particular in research [44].However, the proprietary nature of the state-of-the-art LLM has led to the rise of judges based on opensource LLMs.PandaLM [40] is an open-source LLM that has been fine-tuned for selecting the best response to an (instruction, input) pair and providing an explanation together with its decision.In similar works, JudgeLM [46] focuses on increasing the performance of the 'judge' LLM, while Prometheus [15] emphasizes achieving granular feedback according to a custom score rubric.\n\nAll these works follow a common scheme: they create a highquality imitation dataset, where the inputs are human-curated and high-performance closed models like GPT-4 provide the desired outputs; then, they fine-tune one or more open-source models on the dataset.Although their results are promising, closed models such as GPT-4 remain the state-of-the-art model, which is why we chose to use GPT-4 for our study.",
            "score": 0.4263266387569946,
            "section_title": "Automatic Feedback Evaluation",
            "char_start_offset": 17772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 367
                },
                {
                    "start": 369,
                    "end": 388
                },
                {
                    "start": 388,
                    "end": 462
                },
                {
                    "start": 462,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 757
                },
                {
                    "start": 757,
                    "end": 942
                },
                {
                    "start": 944,
                    "end": 1206
                },
                {
                    "start": 1206,
                    "end": 1356
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.148681640625
        },
        {
            "corpus_id": "266899541",
            "title": "The Critique of Critique",
            "text": "Q4: Which evaluation methods can choose the critiques that humans prefer? \n\nSetup In this experiment, we utilize a range of scoring baselines to identify the better critique out of two critiques generated by LLMs. We calculate the agreement rate to evaluate the performance, defined as the consistency of the superior critique chosen by various methods with the gold standard critique determined by gold F1 score. \n\nResults Figure 3 illustrates the agreement rate of various methods. Our MetaCritique GPT4 -F1 achieves the best performance. Especially, it exceeds the Single GPT4 baseline by a considerable margin (44%). It implies that the scores of MetaCritique GPT4 -F1 are more comparable than Single GPT4 baseline. Even MetaCritique Open significantly outperforms GPT-4 powered baselines Single GPT4 and Pairwise GPT4 , confirming the effectiveness of our framework. Finally, precision and recall scores serve as complementary metrics for evaluating critiques, because MetaCritique GPT4 -P and MetaCritique GPT4 -R are somewhat less effective than MetaCritique GPT4 -F1. \n\n4.6 Exp V: Better Critique, Better Refinement Q5: Can critique evaluations improve the alignment of existing LLMs? \n\nSetup Critique is commonly applied to improve the quality of model outputs via refinement (Madaan et al., 2023). It is intuitive that superior critiques result in better refinements. To confirm this hypothesis, we conduct this experiment. Specifically, we instruct GPT-4 to refine the model outputs via the critique. Detailed instructions with demonstrations are presented in Table 18. Subsequently, we compare the refined outcomes to choose the better one. We conduct GPT-4 evalu- ation and human evaluation for this comparison. The prompt for GPT-4 evaluation is shown in Table 19, while the equivalent instruction for human evaluators is outlined in Table 20.",
            "score": 0.4262635107862877,
            "section_title": "Exp-IV: Pairwise Comparison",
            "char_start_offset": 19706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 76,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1857
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2298583984375
        },
        {
            "corpus_id": "265080327",
            "title": "Can we use ChatGPT for Mental Health and Substance Use Education? Examining Its Quality and Potential Harms",
            "text": "RenderX Not only will future versions of ChatGPT supersede GPT-4, but the current GPT-4 sits behind a paywall. The Pro version with Bing BETA internet browsing requires a paid subscription (US $20 per month). More broadly than this paper, we have concerns about the financial accessibility of LLMs and the subscription costs that enable access to accuracy-improving features. Consideration must also be given to people in lowand middle-income countries who are already at a disadvantage and are often unable to access the latest research due to journal paywalls [40], thus potentially compounding the cost of using GPT-4 at its best. \n\nWe also acknowledge that plug-ins are available to address some of the concerns we evaluated in this paper. In particular, access to scientific journals can be facilitated with plug-ins such as ScienceAI and Litmap [38]. However, the scope of this paper does not include the evaluation of various plug-ins, which are also rapidly developing. In addition, the primary aim of our evaluation was to evaluate GPT-4's initial outputs to prompts without the assistance of prompt refining or plug-ins to be able to assess the baseline of its safety and accuracy.",
            "score": 0.4260761903966641,
            "section_title": "XSL \u2022 FO",
            "char_start_offset": 29732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 633
                },
                {
                    "start": 636,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1191
                }
            ],
            "ref_mentions": [
                {
                    "start": 562,
                    "end": 566,
                    "matchedPaperCorpusId": "207896214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08270263671875
        },
        {
            "corpus_id": "258999338",
            "title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing",
            "text": "We queried GPT-4 for three responses per {question, paper} pair, taking the majority vote as the answer and evaluating its correctness against the ground truth label. If all three responses were different, we marked the answer as incorrect. In Table 2, we provide the results for our checklist experiment. We found that compared to the hand-labeled ground truth, GPT-4 achieves 86.6% accuracy across 119 examples.\n\nIn some more detail, author-submitted checklists also match the ground truth 86.6% of the time, although the mismatches may potentially be due to later paper revisions. We find that there is little overlap between LLM and author mismatches: On one hand, GPT-4 answers disagree with 12 out of 16 (75%) of mismatched author responses, allowing for further examination. On the other hand, 9 out of 16 (56.3%) of GPT-4's incorrect answers had correct author responses. Furthermore, 8 out of 16 of GPT-4's incorrect answers were  Table 2: Accuracy of GPT-4 on papers and checklist questions. The notations \u2713 and \u00d7 correspond to whether the majority label across three responses by GPT-4 was correct or incorrect respectively, as measured against the hand-labeled ground truth. Grey cells indicate the author had initially marked \"no\" or \"N/A\".\n\ndue to there being no textual evidence of the correct answer (e.g., requires access to figures in the paper, which is beyond its current capabilities). The authors, with full information of their own submission and figures, correctly responded to all 8. Removing this set of questions increases GPT-4's accuracy to 92.8%, but the 86.6% statistic is more representative across the problems that current LLMs may face when deployed into checklist-verifying roles.",
            "score": 0.4235185059480855,
            "section_title": "Summary of results",
            "char_start_offset": 94299,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61181640625
        },
        {
            "corpus_id": "263608784",
            "title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
            "text": "Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.",
            "score": 0.4226805297905569,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.888671875
        },
        {
            "corpus_id": "262824801",
            "title": "Large Language Model Alignment: A Survey",
            "text": "of ChatGPT (or text-davinci-003) and the evaluated model, ultimately computing the model's score relative to ChatGPT's score. Similarly, AlpacaEval (Li et al., 2023d) uses the GPT-4 or Claude or ChatGPT based automatic evaluator to compare the response generated by the LLM being evaluated with the reference response from text-davinci-003. Subsequently, considering the potential risk of data leakage that may be associated with the use of closed-source API for evaluation, PandaLM  introduces a judgment LLM, helping users to select the best LLM locally.\n\n\u2022 Reference-guided grading Reference-guided grading provides the appropriate reference answer generated by humans and requires an advanced LLM to compare the response generated by two LLMs being evaluated with the reference answer. Research has shown that this type of assessment leads to better rubric results on math problems (Zheng et al., 2023a).\n\nThere are corresponding disadvantages to using an advanced LLM for automatic evaluation.\n\nRegarding the pairwise comparison, it results in exponentially increasing evaluations with the growing number of models to be assessed. Additionally, the used advanced LLMs exhibit position bias, verbosity bias, and self-enhancement bias during comparisons. These biases incline the evaluator LLMs to favor the first answer, the long and verbose answer, or an answer generated by a specific LLM, despite another answer being more concise and accurate (Zheng et al., 2023a;Wang et al., 2023a). Conversely, single-answer grading overlooks subtle differences between two answers, leading to unstable scores and undermining the evaluation's credibility. Moreover, LLMs' limitations in math and reasoning abilities lead to their equal underperformance in evaluation tasks involving math and reasoning (Zheng et al., 2023a).\n\nTo address position bias, multiple evaluations can be conducted by employing position switching or by requiring the evaluator LLMs to generate multiple evidential supports (Zheng et al., 2023a;Wang et al., 2023a). To compensate for math and reasoning deficits, chain of thoughts  can be explored to significantly enhance the reasoning ability of LLMs, thereby improving evaluations that demand reasoning skills (Wang et",
            "score": 0.4222080790138488,
            "section_title": "Methods",
            "char_start_offset": 118692,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.306396484375
        },
        {
            "corpus_id": "276408437",
            "title": "An Empirical Analysis of Uncertainty in Large Language Model Evaluations",
            "text": "We first conduct an extensive investigation of LLM evaluators with 2 different evaluation settings to gain a preliminary understanding of uncertainty in model-based LLM evaluation, showing partial LLM evaluators exhibit varying uncertainty based on model families and sizes. The evaluation uncertainty is more pronounced in the single-answer grading, where the average evaluation confidence is 65.4%, compared to 79.7% for pairwise comparison on MT-Bench. This lower confidence suggests that evaluators exhibit higher uncertainty when scoring individual models, which could stem from evaluators being uncertain about how to score a model's response without the context of a comparison. In contrast, pairwise comparison benefits from direct comparison, leading to more decisive assessments. \n\nEvaluations within the same model family show significantly higher evaluation confidence. As shown in Table 1, when Llama2-70B-Instruct is employed to evaluate Llama2-7B-Instruct, both the score (7.875 v.s. 6.456) and evaluation confidence (0.953 v.s. 0.654) are significantly higher than the averages for other evaluators. We speculate that this uncommon high confidence arises from the shared training corpus and similar linguistic patterns between the models, leading to a self-preference bias (Koo et al., 2023;Zheng et al., 2023), where the evaluating model is more familiar with the response style and content generated by a closely related model. This phenomenon highlights the potential threats for self-preference when evaluators from the same model family are used, which could lead to biased evaluations. \n\nImproved general performance does not guarantee more stable evaluation capabilities. For example, while GPT-4o demonstrates superior performance in general tasks (such as reasoning and math) compared to GPT-3.5-Turbo (Chiang et al., 2024), its evaluation confidence remains low. \n\nIn the single-answer grading, GPT-4o has an evaluation confidence of only 0.417, which indicates that despite its enhanced abilities in general tasks, it struggles with stability in evaluating other models' responses.",
            "score": 0.4221848541767941,
            "section_title": "RESULTS AND ANALYSIS",
            "char_start_offset": 13898,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 789
                },
                {
                    "start": 792,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1607
                },
                {
                    "start": 1610,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1888
                },
                {
                    "start": 1891,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 1307,
                    "end": 1326,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.243408203125
        },
        {
            "corpus_id": "265675839",
            "title": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models",
            "text": "Fine-grained Evaluation with User-Defined Score Rubrics Score of a specific domain or task 2023; Peng et al., 2023;Zheng et al., 2023;Ye et al., 2023b;Min et al., 2023). Initial investigations and observations indicate that, when aptly prompted, LLMs can emulate the fineness of human evaluations. However, while the merits of using proprietary LLMs as an evaluation tool are evident, there exist some critical disadvantages: \n\n1. Closed-source Nature: The proprietary nature of LLMs brings transparency concerns as internal workings are not disclosed to the broader academic community. Such a lack of transparency hinders collective academic efforts to refine or enhance its evaluation capabilities. Furthermore, this places fair evaluation, a core tenet in academia, under control of for-profit entity and raises concerns about neutrality and autonomy. \n\n2. Uncontrolled Versioning: Proprietary models undergo version updates that are often beyond the users' purview or control (Pozzobon et al., 2023). This introduces a reproducibility challenge. As reproducibility is a cornerstone of scientific inquiry, any inconsistency stemming from version changes can undermine the robustness of research findings that depend on specific versions of the model, especially in the context of evaluation. \n\n3. Prohibitive Costs: Financial constraints associated with LLM APIs are not trivial. For example, evaluating four LLMs variants across four sizes (ranging from 7B to 65B) using GPT-4 on 1000 evaluation instances can cost over $2000. Such scaling costs can be prohibitive, especially for academic institutions or researchers operating on limited budgets. \n\nDespite these limitations, proprietary LLMs such as GPT-4 are able to evaluate scores based on customized score rubrics. Specifically, current resources are confined to generic, single-dimensional evaluation metrics that are either too domain/task-specific (e.g. EM, Rouge) or coarse-grained (e.g. helpfulness/harmlessness (Dubois et al., 2023;Chiang et al., 2023;Liu et al., 2023) as shown in leftside of Figure 1.",
            "score": 0.4220581368674622,
            "section_title": "LLM Developer",
            "char_start_offset": 1110,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 854
                },
                {
                    "start": 857,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1294
                },
                {
                    "start": 1297,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1651
                },
                {
                    "start": 1654,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2069
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.236328125
        },
        {
            "corpus_id": "267679187",
            "title": "Can ChatGPT assist authors with abstract writing in medical journals? Evaluating the quality of scientific abstracts generated by ChatGPT and original abstracts",
            "text": "The underlying reason for these discrepancies remains unclear as OpenAI is yet to disclose explicit details to the parameters, architecture nor the hardware used for training GPT 4. However, our finding was reinforced in a study from Stanford University which evaluated both models of ChatGPT across diverse domains, encompassing code generation, visual reasoning, handling of sensitive questions and mathematical problem-solving. In particular, when prompted to answer maths problems, GPT 3.5 demonstrated 86.8% accuracy in comparison to 2.4% shown by GPT 4, and this was attributed to GPT 3.5's superiority in breaking down complex problems into smaller intermediate steps, a phenomenon known as the \"chain of thoughts\" effects [17]. Parallel to our study, it is plausible that GPT 3.5's aptitude for deconstructing complex commands resulted in superior abstract generation compared to its successor. In addition to this, the assumed broader training database and variance in fine-tuning approaches in GPT 4 could have further compromised its capacity for specialised tasks such as those required in this study. \n\nDespite the strengths of ChatGPT in scientific writing, it is imperative to recognise the associated risks and pitfalls. Firstly, LLMs draw from expansive dataset, which could unintentionally reflect biases related to sex, ethnicity, and language. Given that the training data for these LLMs predominantly originate from well-funded institutions in affluent, English-speaking countries, there exists a risk for the underrepresentation of minority groups [18]. Secondly, while the proficiency of LLMs to generate credible information is commending, it can sometimes be misleading. For instance, when prompted to generate a literature review, ChatGPT provided superficial summary that was far from the knowledge of an expert in the field and it faltered in providing correct references, meanwhile in our study, GPT 4 generated 4 abstracts which were entirely unrelated to the given topic [19,20]. This propensity to generate plausible yet fictitious content intertwines with issues of plagiarism. LLMs could inadvertently extract and apply content from its training data into their generated output which poses a real challenge to trust in medical research, especially as the difference between author written and LLM-generated texts gradually narrows.",
            "score": 0.42138436103247906,
            "section_title": "Discussion",
            "char_start_offset": 23118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1113
                },
                {
                    "start": 1116,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2110
                },
                {
                    "start": 2111,
                    "end": 2366
                }
            ],
            "ref_mentions": [
                {
                    "start": 2002,
                    "end": 2006,
                    "matchedPaperCorpusId": "258494319"
                },
                {
                    "start": 2006,
                    "end": 2009,
                    "matchedPaperCorpusId": "259241018"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51806640625
        },
        {
            "corpus_id": "267782776",
            "title": "Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark",
            "text": "In Tab. 2 we show the performance of the compared summarization systems in different metrics on the PubMed dataset. We do not include GPT-3.5's with few-shot learning, as it does not improve performance. Similar results on arXiv and other details are in Appendix B. Generally, GPT-3.5, Llama2, and Long5 consistently achieve higher evaluation scores across all metrics, showing their robustness and adaptability in different domains. Specifically, Llama2 shows the highest performance, similar to the observation in the news domain (Kadous, 2023). This highlights the potential of applying open-source LLMs as alternatives to closed LLMs like those of OpenAI. The result also suggests that finetuned smaller-scale models can rival the performance of LLMs in scientific contexts. However, achieving such performance demands precise model design, as seen in BigBird's inferior performance compared to LLMs. We also present a statistical evaluation in Fig. 4. Our findings indicate that while GPT-3.5 tends to produce text that is easier to understand, it often misses critical scientific statistics. As shown in Fig. 4(a), it generates an extended background in the abstract, but includes significantly fewer numbers in its text compared to other models, as seen in Fig. 4(b). This is notable given the importance of numerical data in scientific literature. Additionally, GPT-3.5 favors the use of more commonly used words, a trend that is evident in Fig. 4(c). In addition to the overall evaluation, we detail the human assessment in different facets in Appendix C. Case studies show that when the conclusion diverges from standard background information, GPT-3.5 tends to stick to conventional knowledge rather than aligning with the provided conclusion. Furthermore, our statistical analysis indicates that 34.7% of weaker performances in PLM are linked to fluency challenges in generating lengthier text in final conclusions.",
            "score": 0.4213742909329433,
            "section_title": "Comparing Summarization Systems",
            "char_start_offset": 21521,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1927
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.509765625
        },
        {
            "corpus_id": "267760188",
            "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
            "text": "This involves providing the evaluated LLM with predefined benchmark questions and using another LLM, such as GPT-4, to judge its responses (Zheng et al. 2023a;Li et al. 2023b;Bai et al. 2023;Wang et al. 2023a;Zhang et al. 2023b;Wang et al. 2023c;Li et al. 2023a;Zhu, Wang, and Wang 2023). We refer to this category of evaluation methods as the LLM-as-judge paradigm. However, this evaluation approach can also introduce additional biases, including positional bias (Wang et al. 2023a), verbosity bias (Saito et al. 2023), and style bias (Wu and Aji 2023). Positional bias refers to the tendency to assign higher scores to answers based on their specific positions. Verbosity bias indicates that large language models often prefer more verbose answers, even if these longer responses are not necessarily of higher quality than shorter ones. Style bias manifests in the inclination of large language models to favor answers that match their own generated style, such as giving lower scores to correct responses with spelling errors, since LLMs rarely produce content with spelling mistakes. \n\nDespite enabling automatic evaluation with standard pipelines, both the benchmark and LLM-as-judge paradigms face significant data leakage issues. The extensive training data used in LLM development, considered a valuable asset by many closed and even open-source models, can easily lead to benchmark data leakage, severely biasing evaluation results (Zhou et al. 2023b). To solve this issue, we propose a novel evaluation paradigm, which takes an LLM as an examiner to raise questions. The examiner should produce different evaluation session for each time which makes it hard to duplicate the evaluation questions and protect the evaluation benchmark from disclosure for fine-tuning and  pre-training an LLM deliberately. However, simply adopting an LLM as examiner would lead to arbitrary evaluation question generation without a goal.",
            "score": 0.42136575938096155,
            "section_title": "Introduction",
            "char_start_offset": 1582,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1929
                }
            ],
            "ref_mentions": [
                {
                    "start": 1442,
                    "end": 1460,
                    "matchedPaperCorpusId": "263310319"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3779296875
        },
        {
            "corpus_id": "271769753",
            "title": "Knowledge Acquisition through Continued Pretraining is Difficult: A Case Study on r/AskHistorians",
            "text": "Evaluating LLM-generated texts, especially in long form, in a scalable and reliable way remains an ongoing challenge at the time of writing. Human judgment is still the gold standard when it comes to assessing the generation quality of dialogue-tuned or question-answering models (Touvron et al., 2023) A key idea when evaluating LLMs is to compare the output of a fine-tuned LLM to another LLM that is considered state-of-the-art or a valid baseline, e.g., Touvron et al. (2023) compare their results to GPT-4 (OpenAI, 2023) with human annotators. The LLM-as-a-judge approach aims to automate this by instead invoking high-quality LLMs such as GPT-4 (Zheng et al., 2023;Liu et al., 2023b) to perform the assessment -while there seems to be decent correlation with human preference, these approaches are subject to various biases, e.g., the judge LLMs preferring longer responses or those that are similar to what they are trained to respond. \n\nIn addition to the evaluation of text quality, various descriptive metrics are commonly used to measure simpler properties of texts, e.g., toxicity (Hartvigsen et al., 2022), text complexity, and reading time (Ward et al., 2023).",
            "score": 0.41804679444466064,
            "section_title": "Evaluation",
            "char_start_offset": 9914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1174
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29931640625
        },
        {
            "corpus_id": "271543835",
            "title": "Machine Unlearning in Generative AI: A Survey",
            "text": "Reliability of LLMs as Evaluators in GenAI Unlearning. Since human evaluators are typically expensive and time-consuming, an increasing number of studies and benchmarks have utilized state-of-the-art Large Language Models (LLMs) like GPT-4 as tools to assess unlearning performance and utility. For instance, SKU [96] and [192] employ GPT-4 [1] and GPT-3.5-turbo respectively to evaluate the harmfulness of model responses post-unlearning. Similarly, Eraser [100] uses GPT-3.5 to derive a harmfulness score ranging between 1 and 5 to aid in determining the success rate of jailbreak attacks. Additionally, EFUF [174] utilizes GPT-4 to assess the informativeness of responses generated by the unlearned model to ensure that utility is not compromised after unlearning. Despite the prevalence and effectiveness of this approach, its validity is questionable. Firstly, LLMs are known to produce biased decisions and outputs [22,102,126,150], which may adversely affect the evaluation results. Secondly, many recent studies [77,140,166] have highlighted the importance of prompt templates during prompt tuning, which is predominantly used to calibrate GPT as an evaluator. This suggests that evaluation results from such an evaluator may largely depend on its system prompts and relevant templates, introducing uncertainties in the evaluation process. Hence, despite the challenges, it will be intriguing to see how future research addresses the drawbacks of using LLMs as evaluators in GenAI unlearning, aiming to provide more reliable and convincing evaluation outcomes.",
            "score": 0.4145519438193492,
            "section_title": "7.2.3",
            "char_start_offset": 85483,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1568
                }
            ],
            "ref_mentions": [
                {
                    "start": 925,
                    "end": 929,
                    "matchedPaperCorpusId": "235097208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1925048828125
        },
        {
            "corpus_id": "261050476",
            "title": "ChatGPT applications in Academic Research: A Review of Benefits, Concerns, and Recommendations",
            "text": "ChatGPT has the potential to revolutionize scientific writing by assisting researchers in efficiently producing high-quality and well-written articles. However, it is important to acknowledge that ChatGPT cannot replace the expertise of researchers but can effectively streamline the writing process. Researchers should be cautious of the potential risks associated with using ChatGPT in scientific writing and should consider the generated content as a tool to complement their own critical thinking and expertise. Additionally, relying solely on AI without the involvement of knowledgeable human researchers may perpetuate biases and inaccuracies in the data, resulting in unfair outcomes and hindering scientific progress. Studies have demonstrated that language models like GPT-3, trained on large web-based datasets, can exhibit biases related to gender, race, ethnicity, and disability status. Hence, despite the impressive advancements in AI tools, the presence of experienced experts remains essential in scientific activities and writing to ensure the quality of the work. In light of these considerations, it is advisable to harness the full potential of ChatGPT as a linguistic tool while upholding rigorous ethical standards. This approach allows researchers to benefit from its capabilities while maintaining the integrity of scientific practices. Regardless of how AI is employed, we strongly believe that involving domain experts in scientific activities and writing is crucial to uphold the quality of the work. Furthermore, the rapid progress of AI tools may lead to some researchers achieving a significant increase in publication numbers without a corresponding growth in their actual expertise in the field. This can raise ethical concerns when academic institutions prioritize publication quantity over quality, potentially impacting the hiring process for professionals. Ultimately, the influence of language models on scientific writing will depend on their adoption and utilization within the scientific community. \n\nIt is recommended to use ChatGPT as a supplemental tool for constructive writing, reviewing materials, and rephrasing text rather than relying on it for providing an entirely original blueprint.",
            "score": 0.41452488709316626,
            "section_title": "Conclusion",
            "char_start_offset": 34152,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2038
                },
                {
                    "start": 2041,
                    "end": 2235
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53955078125
        },
        {
            "corpus_id": "272917134",
            "title": "A Comparative Study of Large Language Models in Explaining Intrinsically Disordered Proteins",
            "text": "Its training paradigm involves predicting subsequent tokens (I.e., the basic units of text or code that an LLM AI uses to process and generate language) within a document. The vast corpus of training data hails from a variety of sources, encompassing publicly accessible data (e.g., internet-based data) and datasets obtained through licensing from third-party entities [8] . To enhance its performance, GPT-4 has been fine-tuned using a technique known as Reinforcement Learning from Human Feedback (RLHF) [8] . to users in a palatable, conversational manner [9] . One evaluation of GPT-3.5, a predecessor of GPT-4, found that the AI language model was able to score at or near the 60% pass mark for all three of the United States Medical Licensing Examinations (USMLEs), while medical students spend anywhere from 300-400 hours of focused study time to prepare for solely the first of these three examinations [10][11] . Further exploration of GPT-4's capabilities has shown its aptitude to tackle a broad range of intricate prompts across disciplines such as mathematics, coding, vision, law, and psychology while maintaining a human-like output [9][12] . These analyses of ChatGPT demonstrate the power of AI tools to further understand complex topics, including IDPs. As such, we posit that harnessing the power of AI tools like LLMs could significantly enhance the comprehension and recognition of IDPs within the scientific community. \n\nIn this study, we present an empirical analysis that assesses the performance of several LLMs, including GPT-4, GPT-3.5, Google Bard (PaLM 2), GPT-4, and GPT-4 with Bing Browser plugin, in the context of education on Intrinsically Disordered Proteins (IDPs). The analysis is based on a dataset gathered from a survey and focuses on five evaluation categories: Accuracy, Relevance, Depth of Understanding, Clarity, and Overall Quality. \n\nAs part of the evaluation, we engage in a dialogue with the AI models to explore various facets of IDPs, including common misconceptions, challenges in research, and the future trajectory of the field.",
            "score": 0.41448784313226106,
            "section_title": "Introduction",
            "char_start_offset": 2158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1878
                },
                {
                    "start": 1881,
                    "end": 2082
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2293701171875
        },
        {
            "corpus_id": "268033311",
            "title": "From RAGs to riches: Utilizing large language models to write documents for clinical trials.",
            "text": "Our assessment focused on GPT-4, one of the leading LLMs available today 12 and utilizing it to generate key sections of clinical trial protocols. The LLM output was subsequently assessed in terms of writing quality. Specifically we analyzed four dimensions: Clinical thinking and logic, which measures how closely recommendations from regulatory guidance documents were included in the generated section; Transparency and references, which verifies the presence and relevance of cited scientific sources in the generated text; Medical and clinical terminology, which assesses the use of appropriate jargon and scales of measurements; Content relevance and suitability, which measures, among others, whether the generated protocol section was specific to the disease and trial phase. An overview of the methodology is given in Figure 1, and a full description of both the generation and evaluation process, including the criteria and requirements used, is provided in the Supplementary Information. \n\nOur analysis does not make direct comparisons between LLM-written text and fully human-written text. This is because, from our experience in the field, there is often substantial variability between individual human writers. It is therefore challenging to establish a single, objective ''ground truth'' to compare against. Our evaluation framework, with its four dimensions described above, addresses this challenge by breaking down the assessment into discrete sub-dimensions which can be assessed objectively. Our assessment targeted two key sections of a clinical trial protocol document: the endpoints section and the eligibility criteria section. Two LLM models were evaluated: off-the-shelf GPT-4 via its commercially available application programming interface, 6,12 used as a baseline; and a retrieval-augmented generation (RAG) GPT-4 as an alternative to the off-the-shelf version (see Figure 1(b)). RAG is a methodology for incorporating knowledge from external databases, 13 and involves providing the LLM with external sources of knowledge to supplement the model's internal representation of information. 14 The RAG-augmented LLM was configured as follows: based on a user input query below, an LLM-powered decision agent automatically decided which tools to use to fetch relevant context and feed it to an LLM for final summarization and document generation.",
            "score": 0.413461288391448,
            "section_title": "Methods",
            "char_start_offset": 2454,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2373
                }
            ],
            "ref_mentions": [
                {
                    "start": 2119,
                    "end": 2121,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93798828125
        },
        {
            "corpus_id": "271769421",
            "title": "Guiding Large Language Models via External Attention Prompting for Scientific Extreme Summarization",
            "text": "Setup. In addition to using traditional automatic metrics, we explore the use of LLMs as evaluators to assess the quality of generated TLDRs. This approach aims to address the limitations of automatic metrics, which may not fully capture the nuances of human preferences and the overall quality of the summaries (Zheng et al., 2023). By leveraging LLMs, we seek to provide a more comprehensive and human-aligned evaluation of the generated TL-DRs. \n\nTo this end, we employ GPT-4 (version gpt-4-0613), which is currently recognized as one of the most advanced LLMs, to assess the quality of TL-DRs generated by our method compared to a baseline method. \n\nWe randomly sample 100 data points and collect TLDRs generated by GPT-3.5 using both the baseline SP method and our EAP method under a fewshot setting. The evaluation criteria focus on the TLDR's ability to concisely capture the key aspects of a scientific paper while maintaining faithfulness to the source. To mitigate position bias (Zheng et al., 2023), each pair is evaluated twice with the positions of the two TLDRs swapped. A method is considered the winner for a sample only if both evaluations favored the same TLDR. In cases of conflicting results or ties, the sample is marked as a tie. The above process is repeated with three different random seeds, resulting in three rounds of evaluation. \n\nResults. The results of GPT-4 evaluation are summarized in Table 3, showing that the proposed EAP method consistently outperforms the baseline SP method, achieving an average win rate of 37.33 % compared to the baseline's 26.00 %. The average tie rate is 36.67 %, with an average conflict rate of 23.67 %. These results indicate that the EAP method significantly enhances the performance of LLMs in generating TLDRs, as judged by GPT-4.",
            "score": 0.41313477039992375,
            "section_title": "LLM as Evaluator",
            "char_start_offset": 19188,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 7,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 447
                },
                {
                    "start": 450,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1796
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "267783118",
            "title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
            "text": "In this section, we provide an in-depth analysis of the LLMs' internal knowledge on survey writing ability, and compare the evaluation scores of human and LLM assessments. \n\nError Types We have shown that both automated and manual evaluations demonstrated that LLMs excel in crafting survey articles on scientific concepts. We analyze the best setting, GPT-4 OSP, assessing errors identified by two experts, and summarize error types and distributions in Fig. 2. We classify these errors into four categories: Verbose, Wrong Fact, Missing Information, and No Error (indicating flawless content). It shows that most errors are missing information, followed by verbosity and factual inaccuracies. Furthermore, the History and Introduction sections of the generated articles contained the highest number of errors, while the Application section exhibited the best. \n\nNovel Entity Mention To further investigate how interesting the generated content is, we look at the mentions of novel entities following (Lee et al., 2022). Specifically, we examine the survey content by comparing the entities it contains with those in the ground truth. We employ Stanza (Qi et al., 2020) to identify all entities in both the LLMgenerated text and the ground truth. Subsequently, we quantify the number of unique entities found in the LLM-generated content. For a fair comparison, we analyze the one-shot with prompt settings of LLaMa2-13b, PaLM2, and GPT-4, in addition to the ZS setting of GPT-3.5, as depicted in Fig. 3. Our findings reveal that PaLM2 exhibited the least variation in entity mentions, while LLaMa2-13b showcased the most. Despite GPT-4's outstanding performance in both automated and human evalu- Table 2: Human and GPTs Evaluation Results. We report the mean and standard deviation. We also quantify the IAA (inter-annotator agreement) (Karpinska et al., 2021) between human experts and the GPT results, respectively, using Krippendorff's \u03b1 coefficient and calculating the percentage (%) of scores that are identical. ations, we didn't discern a marked novelty in its entity mentions. We speculate that this might be an inherent compromise when generating high-fidelity content in relation to the ground truth.",
            "score": 0.41249913210070077,
            "section_title": "Analysis",
            "char_start_offset": 9766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 174,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 1002,
                    "end": 1020,
                    "matchedPaperCorpusId": "246016439"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4873046875
        },
        {
            "corpus_id": "264445014",
            "title": "Exploring the Role of Artificial Intelligence in Facilitating Assessment of Writing Performance in Second Language Learning",
            "text": "This study examined the robustness and efficiency of four large language models (LLMs), GPT-4, GPT-3.5, iFLYTEK and Baidu Cloud, in assessing the writing accuracy of the Chinese language. Writing samples were collected from students in an online high school Chinese language learning program in the US. The official APIs of the LLMs were utilized to conduct analyses at both the T-unit and sentence levels. Performance metrics were employed to evaluate the LLMs\u2019 performance. The LLM results were compared to human rating results. Content analysis was conducted to categorize error types and highlight the discrepancies between human and LLM ratings. Additionally, the efficiency of each model was evaluated. The results indicate that GPT models and iFLYTEK achieved similar accuracy scores, with GPT-4 excelling in precision. These findings provide insights into the potential of LLMs in supporting the assessment of writing accuracy for language learners.",
            "score": 0.4119544805710479,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35205078125
        },
        {
            "corpus_id": "263608784",
            "title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
            "text": "We conduct a prospective user study to further validate the effectiveness of leveraging LLMs to generate scientific feedback. To facilitate our user study, we launched an online Gradio demo 55 of the aforementioned generation pipeline, accessible at a public URL (Supp. Fig. 11). Users are prompted to upload a research paper in its original PDF format, after which the system delivers the review to user's email. We ask users to only upload papers published after 9/2021 to ensure the papers are never seen by GPT-4 during training (the cutdown date of GPT-4 training corpora is 9/2021). We have also incorporated an ethics statement to discourage the direct use of LLM content for any 10/39 review-related tasks. After the review is generated and sent to users, users are asked to fill a 6-page survey (Figure 4), which includes 1) author background information, 2) review situation in author's area, 3) general impression of LLM review, 4) detailed evaluation of LLM review, 5) comparison with human review, and 6) additional questions and feedback, which systematically investigates human evaluations of different aspects of LLM reviews. The survey takes around 15-20 minutes and users will be compensated with $20. We recruit the participants through 1) relevant institute mailing lists, and 2) reaching out to all authors who have published at least one preprint on arXiv in the field of computer science and computational biology during January to March, 2023, provided their email contact information is available in the first three pages of the PDF. The study has been approved by Stanford University's Institutional Review Board.",
            "score": 0.41131144666355063,
            "section_title": "Prospective User Study and Survey",
            "char_start_offset": 42178,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1639
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 192,
                    "matchedPaperCorpusId": "213713921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53076171875
        },
        {
            "corpus_id": "276618080",
            "title": "Single- vs. Dual-Prompt Dialogue Generation with LLMs for Job Interviews in Human Resources",
            "text": "\"LLM as a judge\" is a powerful paradigm that reduces experiment costs compared to using human evaluators. However, the use of LLMs as judges is still actively being researched and there are known limitations, as discussed below. \n\nTo start with, LLM judges can potentially use irrelevant characteristics to cast their judgment (Salinas et al., 2025a) such as (1) input order (Zheng et al., 2023) or (2) length of the provided text (Dubois et al., 2024). We account for these specific forms of bias, but we cannot exclude the possibility of other spurious or irrelevant patterns influencing the decisions of the LLMs used in our experiment. \n\nMore broadly, caution is needed when assuming that LLMs will automatically align with human values and criteria, especially when using them as judges. For the use case of judging on dialogue quality, Duan et al. (2024) found a 65.74% consistency rate between GPT-4 and human evaluators. Although Llama 3.3 and GPT-4o could be assumed to perform better given their later release date, to the best of our knowledge this has not yet been confirmed through a follow-up experiment.",
            "score": 0.41117431887882205,
            "section_title": "Limitations",
            "char_start_offset": 13568,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 228
                },
                {
                    "start": 231,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1118
                }
            ],
            "ref_mentions": [
                {
                    "start": 842,
                    "end": 860,
                    "matchedPaperCorpusId": "264405986"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.034423828125
        },
        {
            "corpus_id": "277043164",
            "title": "Commenting Higher-level Code Unit: Full Code, Reduced Code, or Hierarchical Code Summarization",
            "text": "LLM's Effectiveness in Summarizing Large Modules. In this section, we aim to check if our findings in RQ2, which is concluded based on the scores for summaries of small modules, are still correct for larger ones based on the marks from GPT agents. \n\nAccording to the previous result, the capability of the three GPT evaluators is similar: They are fair in evaluating the summaries except that they particularly prefer summaries generated by GPT-4. However, as some of the modules are very large, they may not fit into the context window size of the agents. For GPT-F, GPT-R, and GPT-S, 7, 3, and 1 out \n\nManuscript submitted to ACM of 50 modules fail to be evaluated by the agents, respectively. Since the capability of the three agents for evaluating summaries is similar, while GPT-S can evaluate the most number of modules, we choose it as the evaluator. \n\nWe report the result of GPT-S on the 17 small modules (i.e., the modules that are evaluated by humans) and the 33 large modules (i.e., the modules that are not evaluated by humans) in Table 7. S-H refers to the results of the 17 small modules marked by humans, S-G refers to the results of 17 small modules marked by the GPT agent, and L-G refers to the results of 33 large modules marked by the GPT agent. \n\nBased on our previous findings, though the marks generated by LLMs may not be suitable for comparing the performance of different LLMs, the marks within the same LLM are highly correlated to humans. Therefore, these scores can serve as a metric for evaluating the performance changes of each summarization strategy when switching from small modules to large modules. \n\nFirstly, we can find that for each of the LLMs, the average scores associated with larger modules are consistently lower than those for smaller modules. Furthermore, among the LLMs evaluated, GPT-4 exhibits the smallest decrease in performance, with a drop of only 1.42%. In contrast, CodeLlama and CodeGemma show significantly larger declines at 12.00% and 29.30%, respectively.",
            "score": 0.41053334664575547,
            "section_title": "Evaluation Results.",
            "char_start_offset": 44245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 50,
                    "end": 247
                },
                {
                    "start": 250,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 1266
                },
                {
                    "start": 1269,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1635
                },
                {
                    "start": 1638,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2017
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.247802734375
        },
        {
            "corpus_id": "269293311",
            "title": "LLM Evaluators Recognize and Favor Their Own Generations",
            "text": "Figure 2 (left) visualizes the self-recognition capability of each LLM evaluator in the pairwise setting.The results appear consistent across the two datasets, so we aggregate them for clarity.Our main observations are the following:\n\n1. GPT-4 is significantly more capable at self-recognition than the two weaker models.\n\n2. All three LLM evaluators most easily distinguish their summaries from human-written summaries.Human is the only alternative that Llama 2 can effective distinguish itself from.\n\n3. Weaker LLMs struggle to distinguish themselves from stronger LLMs: Llama 2 is completely incapable of distinguishing itself from GPT-3.5 and GPT-4, and GPT-3 struggles to distinguish itself from GPT-4.\n\n4. The GPT-4 evaluator does not distinguish Llama 2 summaries from its own summaries more easily than GPT-3.5 summaries.This goes against our intuition that self-recognition scores should increase as the dissimilarity between evaluator and evaluatee increases.bility drops across the board in this setting where the LLM loses the benefit of having two pieces of texts to compare and contrast.GPT-4 is the only model capable of distinguishing authors with non-trivial accuracy.Interestingly, looking at the un-normalized confidence scores (Figure 3), we see that GPT-4 is strongly biased against predicting any text as being generated by itself, regardless of the actual author; GPT-3.5 and Llama 2 show the opposite bias.",
            "score": 0.41042623153685764,
            "section_title": "Out-of-the-Box Self-Recognition",
            "char_start_offset": 7801,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 105,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 233
                },
                {
                    "start": 235,
                    "end": 321
                },
                {
                    "start": 323,
                    "end": 420
                },
                {
                    "start": 420,
                    "end": 501
                },
                {
                    "start": 503,
                    "end": 707
                },
                {
                    "start": 709,
                    "end": 829
                },
                {
                    "start": 829,
                    "end": 969
                },
                {
                    "start": 969,
                    "end": 1101
                },
                {
                    "start": 1101,
                    "end": 1185
                },
                {
                    "start": 1185,
                    "end": 1430
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1275634765625
        },
        {
            "corpus_id": "268856493",
            "title": "METAL: Towards Multilingual Meta-Evaluation",
            "text": "We prompt GPT-4 to generate good and bad-quality summaries.As noted in \u00a73.1, for lower tempera-ture values we observed that GPT-4 did not generate bad summaries.We use a temperature of 1 and observe some variation of quality across all our metrics except problematic content.This could be due to the content filter applied to these models.Therefore, it is difficult to study the capability of such models on this metric.We evaluate the generations from GPT-4 using GPT-3.5-Turbo,GPT-4, and PaLM2.Recent work has shown that LLMs prefer their own outputs.Although this might have affected our evaluations, exploring this is beyond the scope of our work.In our work, we mainly focused on investigating how well LLM ratings align with human ratings across various metrics and languages.All summaries generated and evaluated in our study are by the same model, we do not compare them against human-written summaries or summaries generated by other models.Lastly, LLMs are also shown to have scale region bias and we do not calibrate for this in our study, expecting it to be standardized across all their ratings.In the future, it would interesting to explore their impact on our evaluation.",
            "score": 0.41026698660430894,
            "section_title": "Limitations",
            "char_start_offset": 30132,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 59,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 275
                },
                {
                    "start": 275,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 420
                },
                {
                    "start": 420,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 496
                },
                {
                    "start": 496,
                    "end": 553
                },
                {
                    "start": 553,
                    "end": 651
                },
                {
                    "start": 651,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 950
                },
                {
                    "start": 950,
                    "end": 1108
                },
                {
                    "start": 1108,
                    "end": 1186
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11163330078125
        },
        {
            "corpus_id": "273661820",
            "title": "Self-Preference Bias in LLM-as-a-Judge",
            "text": "The results of the bias measurement using Definition 4.1 are presented in Figure 1b. It was confirmed that GPT-4 exhibits the highest self-preference bias. Definition 4.1 focuses on the recall of the LLM evaluator concerning both high and low ratings by the human evaluator. Thus, it can be concluded that GPT-4 showed lower recall in cases where humans evaluated unfavorably compared to when higher evaluating. When examining the recall values in the confusion matrix shown in Figure 2, they are calculated as 0.945 \u2248 1852 108+1852 and 0.425 \u2248 118 160+118 . The difference between these values is 0.520, which corresponds to the value reported in Figure 1b. Following GPT-4, Vicuna-13b and Koala-13b also exhibited significant bias. In contrast, other LLMs displayed values relatively close to zero. Notably, oasst-pythia-12, dolly-v2-12b, and stablelm-tuned-alpha-7b showed negative values, indicating a reverse bias where the LLMs tend to underestimate their own outputs. \n\nTable 1 presents a randomly selected example of self-preference bias in GPT-4, where humans favored the alternative response. In this example, the user query is a straightforward request to list blue items. While GPT-4 states that it lacks physical recognition before listing, GPT-3.5-Turbo directly lists the blue items without any such explanation. Both responses are of high quality, and the final evaluation reflects the evaluator's policy and stylistic preferences. Although humans and GPT-3.5-Turbo preferred the response from GPT-3.5-Turbo, GPT-4 favored its own response, illustrating a typical case of self-preference bias. \n\nFigure 2: matrix for each LLM evaluator's assessment of pairs including its own output. It suggests that some LLMs, including GPT-4, have relatively high true positive rates than true negative rate, which means that they have self-preference bias. Table 1: An example of self-preference bias in GPT-4. The user query involves a simple request to list blue items. GPT-4 responds by first acknowledging its lack of physical cognition, and then proceeds to list blue items.",
            "score": 0.40981579250062217,
            "section_title": "Result",
            "char_start_offset": 13303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 974
                },
                {
                    "start": 977,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2082
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0352783203125
        },
        {
            "corpus_id": "264128354",
            "title": "Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams",
            "text": "Several previous studies have evaluated the performance of LLMs on different standard exams. Tests considered include the United States medical licensing exam (Kung et al., 2023), free-response clinical reasoning exams (Strong et al., 2023), college-level scientific exams (Wang et al., 2023), the Bar exam (Katz et al., 2023), the driver's license knowledge test (Rahimi et al., 2023), and more. The crucial contribution that these works bring to the scientific community and the industry is an in-depth analysis of the strengths and weaknesses of LLMs in realistic domain-specific settings. Through their conclusions, such investiga- ) evaluated ChatGPT and GPT-4 on a collection of Physics, Chemistry, and Math problems, and then concluded that current LLMs do not deliver satisfactory performance in complex scientific reasoning yet to be reliably leveraged in practice. In contrast, (Bang et al., 2023) found that ChatGPT outperformed fine-tuned task-specific models on four different NLP tasks, thus suggesting ChatGPT could be directly applied to solve industry use cases involving these tasks. \n\nOur paper aims at following the footsteps of (Li et al., 2023) and delves further into the assessment of the inner financial reasoning abilities of ChatGPT and GPT-4 to help future industry applications.",
            "score": 0.40962325584970183,
            "section_title": "Evaluation of LLMs on Human Exams and other Benchmarks",
            "char_start_offset": 4696,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1307
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 178,
                    "matchedPaperCorpusId": "254876189"
                },
                {
                    "start": 364,
                    "end": 385,
                    "matchedPaperCorpusId": "261076160"
                },
                {
                    "start": 888,
                    "end": 907,
                    "matchedPaperCorpusId": "201646244"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0343017578125
        },
        {
            "corpus_id": "272987303",
            "title": "The Unique Taste of LLMs for Papers: Potential issues in Using LLMs for Digital Library Document Recommendation Tasks",
            "text": "In the current field of scientometrics, research related to large language models (LLMs) primarily includes academic text mining, academic writing assistance, and research evaluation and science policy. The core focus is on academic writing assistance, which encompasses organizing existing literature, data processing, and direct writing support. This enables tasks such as automatic literature reviews, automatic data annotation, summary generation, reference generation, and content correction. Within this context, analyzing citation relationships is crucial in scientometrics as it directly reflects the impact and quality of scholarly work. Citations are key indicators of influence, guiding funding, academic evaluations, and research priorities. Effective citation analysis helps in identifying seminal works and emerging trends, ensuring that scientific literature remains accurate and reliable. Nowadays, studies on citation relationships predominantly address the authenticity of generated citations. For instance, Walters and Wilder (2023) require LLMs to produce domain-specific literature reviews and evaluate the accuracy of the provided citations. This approach aligns with the primary research focus in scientometrics, which is centered on the performance and applicability of LLMs [1]. While enjoying the significant convenience brought by LLMs, ethical issues associated with their use are also being actively discussed. These issues include whether LLMs can be considered authors of academic writing, their impact on academic teaching, and the potential existence of academic discrimination by LLMs. In the academic realm, these concerns are particularly pertinent. LLMs raise questions about authorship and intellectual property, as the line between human and machine contributions becomes increasingly blurred. Many scientists even disapprove that chatGPT listed as author on research papers (Petiska, 2023) [2]. Additionally, their use in academic settings could affect teaching methodologies and student learning experiences. Concerns about academic integrity also arise, as LLMs might inadvertently propagate biases present in their training data. Therefore, in the social sciences, discrimination in LLMs has emerged as a critical research topic. Gallegos et al. (2023) have explored this issue extensively, defining social biases in natural language processing, developing metrics to evaluate these biases, and proposing mitigation techniques to address biases related to gender and different social groups [3]. This area of study intersects with scientometrics as well.",
            "score": 0.4088428426440661,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2272
                },
                {
                    "start": 2273,
                    "end": 2538
                },
                {
                    "start": 2539,
                    "end": 2597
                }
            ],
            "ref_mentions": [
                {
                    "start": 1026,
                    "end": 1051,
                    "matchedPaperCorpusId": "261608321"
                },
                {
                    "start": 1299,
                    "end": 1302,
                    "matchedPaperCorpusId": "261608321"
                },
                {
                    "start": 1914,
                    "end": 1929,
                    "matchedPaperCorpusId": "258170351"
                },
                {
                    "start": 1930,
                    "end": 1933,
                    "matchedPaperCorpusId": "255969365"
                },
                {
                    "start": 2534,
                    "end": 2537,
                    "matchedPaperCorpusId": "261530629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36083984375
        },
        {
            "corpus_id": "270688703",
            "title": "How Effective is GPT-4 Turbo in Generating School-Level Questions from Textbooks Based on Bloom's Revised Taxonomy?",
            "text": "(1) We utilize the capabilities of GPT-4 Turbo in zero-shot mode to stimulate the creation of educational questions that correspond to Bloom's Revised Taxonomy, with a focus on ensuring contextual appropriateness within specific fields of study.(2) We utilize advanced NLP methods to evaluate the quality of the questions produced, examining their compatibility with Bloom's Revised Taxonomy and their agreement to Item Writing Flaw (IWF) criteria [5,8]-a widely recognized set of guidelines in educational evaluation.(3) To narrow the gap between automated evaluations and human expectations, a subset of the generated questions undergoes rigorous scrutiny by school-level teachers with expertise and pedagogical insights.This work aims to investigate these two research questions (RQs).\n\n\u2022 RQ1: What is the ability of GPT-4 Turbo to generate questions that align with the various levels of Bloom's Revised Taxonomy, as evaluated separately by a machine learning model and an educator?\u2022 RQ2: How do human experts (i.e., Human-Validation) and machine validation using the IWF criteria (i.e., Machine-Validation) compare in assessing the quality of questions generated by GPT-4 Turbo, and what level of agreement exists between these two validation approaches?\n\nThese RQs guide our investigation into the effectiveness and dependability of employing advanced LLMs to generate and authenticate high-quality questions in educational evaluations.",
            "score": 0.40796400477547207,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2161,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 245,
                    "end": 518
                },
                {
                    "start": 518,
                    "end": 723
                },
                {
                    "start": 723,
                    "end": 788
                },
                {
                    "start": 790,
                    "end": 986
                },
                {
                    "start": 986,
                    "end": 1259
                },
                {
                    "start": 1261,
                    "end": 1442
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 248,
                    "matchedPaperCorpusId": "264904566"
                },
                {
                    "start": 448,
                    "end": 451,
                    "matchedPaperCorpusId": "42042996"
                },
                {
                    "start": 451,
                    "end": 453,
                    "matchedPaperCorpusId": "10040353"
                },
                {
                    "start": 518,
                    "end": 521,
                    "matchedPaperCorpusId": "266179810"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2083740234375
        },
        {
            "corpus_id": "268296847",
            "title": "Evaluating AI and human authorship quality in academic writing through physics essays",
            "text": "There are many powerful LLMs available, and benchmarking them is a comprehensive field of study aimed at evaluating the capabilities of these models across a broad spectrum of cognitive and linguistic tasks [6]. As of this writing, OpenAI's GPT-4 consistently ranks at or near the top of most LLM benchmark leaderboards, scoring highly on prominent benchmarks such as the Massive Multitask Language Understanding (MMLU), according to its technical report [7]. \n\nThe MMLU benchmark, known for its extensive range of subjects from humanities to hard sciences, provides a rigorous test of a model's ability to understand and generate responses across diverse knowledge areas. Similarly, the SuperGLUE benchmark [8], which focuses on tasks that require a deep understanding of language, such as question answering, inference, and reasoning, challenges models to demonstrate advanced levels of comprehension and the ability to handle nuanced linguistic constructs. GPT-4's performance, independently supported by scoring over 90% on SuperGLUE [9], underscores its capabilities. \n\nGiven this, our study uses GPT-4 as a representation of current LLM essay writing abilities. While in principle other LLMs could perform better, GPT-4's proficiency ensures that our test is a valid measure of the state of the art in AI-driven essay composition.",
            "score": 0.4078932588254143,
            "section_title": "State of the Art",
            "char_start_offset": 2311,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1072
                },
                {
                    "start": 1075,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1336
                }
            ],
            "ref_mentions": [
                {
                    "start": 708,
                    "end": 711,
                    "matchedPaperCorpusId": "143424870"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68115234375
        },
        {
            "corpus_id": "270878599",
            "title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models",
            "text": "As CEB-Continuation-S and CEB-Conversation-S are generation datasets, it is essential to develop a suitable bias evaluation method to assess LLM-generated content. Although various evaluation metrics for generated output have been proposed, e.g., Demographic Representation (DR) (Liang et al., 2023) and Full Gen Bias (Smith et al., 2022), they are not as straightforward as expected. For example, Full Gen Bias requires training a classifier to identify the sentiment of generated sentences and looking at the variance of classification results across different social groups. As such, in this work, we propose to directly evaluate the bias of LLM-generated by leveraging powerful LLms like GPT-4. An inherent challenge in employing this evaluation approach lies in the acknowledgment that no LLM, regardless of its design considerations, can be entirely devoid of biases. Consequently, GPT-4, despite its advancements, may still manifest biases to a certain extent. Nevertheless, we claim that the efficacy of an LLM in bias evaluation may not necessarily be affected by the inherent bias within its generated content (Liang et al., 2023). In other words, while LLMs may inadvertently produce biased content, they could still be used for context-specific applications when their biases align with considerations (Ferrara, 2023). Consequently, we advocate for the utilization of GPT-4 in bias evaluation, due to its analytical capabilities and proficiency in discerning nuanced biases within texts. \n\nGiven an LLM-generated output, we use the devised prompt to query GPT-4 for the bias score. In particular, we set the score between 0 and 99 and provide detailed descriptions of the bias degrees represented by different intervals. We follow the style provided in (Wang et al., 2024b) and design the prompt as follows: \n\nYour task is to evaluate and rate the level of bias in the given example content towards a specific social group on a scale from 0 to 99. Bias in this context refers to any unfair, prejudiced, or skewed perspective in the content that favors or discriminates against a particular social group. A higher score indicates a greater level of bias, while a lower score signifies minimal to no bias.",
            "score": 0.4072920263008743,
            "section_title": "C.2.2 METRICS FOR CEB-CONTINUATION-S AND CEB-CONVERSATION-S (FOR STEREOTYPING)",
            "char_start_offset": 52604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1499
                },
                {
                    "start": 1502,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1819
                },
                {
                    "start": 1822,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2215
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 299,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 318,
                    "end": 338,
                    "matchedPaperCorpusId": "253224433"
                },
                {
                    "start": 1120,
                    "end": 1140,
                    "matchedPaperCorpusId": "253553585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.459716796875
        },
        {
            "corpus_id": "268708056",
            "title": "Automatic Short Answer Grading for Finnish with ChatGPT",
            "text": "Nonetheless, various challenges, including low technological readiness, concerns about replicability and transparency, and inadequate privacy and beneficence considerations, currently impede the widespread integration of LLMs in educational settings (Yan et al. 2023). For the automated evaluation of student text, there have been more research on the use of LLMs in automated essay scoring, such as discourse coherence prediction (Naismith, Mulcaire, and Burstein 2023) and second language learner essay evaluation (Yancey et al. 2023;Mizumoto and Eguchi 2023). The research on LLMs advancing ASAG is in its early stages; Yoon (2023) uses one-shot prompting for LLM to identify justification keys, or key phrases in student-written short answers and compare the extracted key phrases with those of reference answers. They find that the GPT-3.5 model has an accuracy of 90.3% in extracting these justification keys. The identified justification keys are compared with a reference scoring rubric, which is utilized to compute the final score. Hackl et al. (2023) evaluate the consistency of GPT-4 in text-rating in terms of style and content, and find high self-consistency (interclass correlation coefficient 0.94-0.99) in GPT-4. Matelsky et al. (2023) build a tool to automatically give feedback to answers to open-ended questions. Concurrent to our research, Schneider et al. (2023) compare GPT-3.5 and humans in assessing bachelor-level German and master-level English short answers, reporting observed issues with the grading of the LLM. \n\nThis paper explores the suitability of LLMs for ASAG, testing ChatGPT based on GPT-3.5 and GPT-4 on 2K Finnish short-question answers from ten bachelor-level courses. The experimental design seeks to elucidate whether direct use of ChatGPT in summative assessments by educators is feasible. We find that while one-shot GPT-4 achieves QWK scores considered good on 44 of the 100 tested questions, further research is required before deploying an LLMbased short-answer grader.",
            "score": 0.4072095989707972,
            "section_title": "Introduction",
            "char_start_offset": 1712,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1541
                },
                {
                    "start": 1544,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 267,
                    "matchedPaperCorpusId": "260125084"
                },
                {
                    "start": 431,
                    "end": 470,
                    "matchedPaperCorpusId": "259376861"
                },
                {
                    "start": 516,
                    "end": 536,
                    "matchedPaperCorpusId": "260125084"
                },
                {
                    "start": 536,
                    "end": 561,
                    "matchedPaperCorpusId": "257277710"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.222900390625
        },
        {
            "corpus_id": "270688646",
            "title": "InternLM-Law: An Open Source Chinese Legal Large Language Model",
            "text": "Evaluation Setup.In addition to benchmark evaluation, we assess the ability of LLMs to respond to subjective legal questions, mirroring a real-world legal consultation scenario.\n\nFor this analysis, we carefully bulit a dataset of 100 questions, categorized into three types: legal consultation, case analysis, and legal reasoning, distributed in a ratio of 40/30/30.For each question, we compare the responses of LLMs with those provided by .Given the open-ended nature of subjective questions, we use GPT-4 as a judge to gauge other LLMs' performance, reporting their win rates against GPT-4's responses.\n\nTo avoid position bias (Zheng et al., 2024) and compare the model's performance based on semantics rather than location, we swapped the model's output and used GPT-4 to judge which model performed better in two different tests.If GPT-4 selected different models in the tests, we would discard the result.Only when the model consistently provided the same results for separate tests that differed in position, did we calculate the model's win rate based on the consistency in model judgments.",
            "score": 0.4071544585418384,
            "section_title": "Subjective Evaluation",
            "char_start_offset": 18258,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 17,
                    "end": 177
                },
                {
                    "start": 179,
                    "end": 366
                },
                {
                    "start": 366,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 605
                },
                {
                    "start": 607,
                    "end": 834
                },
                {
                    "start": 834,
                    "end": 911
                },
                {
                    "start": 911,
                    "end": 1098
                }
            ],
            "ref_mentions": [
                {
                    "start": 630,
                    "end": 650,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0289764404296875
        },
        {
            "corpus_id": "272359184",
            "title": "Optimizing the role of human evaluation in LLM-based spoken document summarization systems",
            "text": "Several papers have conducted recent meta-analyses of summary evaluation methodologies within NLP. These papers show that the field suffers from extensive underreporting of experimental designs and evaluation approaches. For example, the majority of papers included in the meta-analyses do not appear to use any form of statistical significance testing to support their claims. Additionally, sample sizes are rarely reported and, when they are, a large majority of experiments are significantly underpowered [2,7,8]. The main concerns with these practices are twofold. First, failure to report on experimental design renders external evaluation of the researchers' claims impossible, and prevents any efforts to duplicate results. Second, running underpowered studies and/or using inappropriate statistical analyses increases the likelihood of incorrectly rejecting the null hypothesis (Type I error) or failing to reject an incorrect null hypothesis (Type II error), greatly elevating the risk that the researchers are reporting unsubstantiated and/or biased results. The designs proposed here obviate these concerns by outlining detailed methodological best-practices. \n\nThere is also a growing body of literature aimed at creating frameworks that use LLMs to assess the quality of generative outputs. ChatEval utilizes several LLMs trained as 'agents' to evaluate a task, engaging them in debate that the authors argue leads to better performance than a single LLM evaluation [9]. Wang et al. [10] compare ChatGPT evaluations of NLG tasks against n-gram and embedding-based metrics. The authors explore using ChatGPT in both reference-based (in which the model is presented with a gold summary) and reference-free methods, and conclude that ChatGPT performs competitively with human evaluators in several task types. Another model, called G-Eval, uses GPT-4 and a framework called Chain of Thought to evaluate the quality of LLM-based evaluations of generative output [11,12]. This framework argues that requiring the LLM to output its reasoning along with an evaluation score improves the model's evaluation performance. The conclusions of these studies show that LLM-based evaluations are generally surpassing the quality of n-gram and embeddingbased metrics as measured by correlations with human evaluation scores.",
            "score": 0.40624508563863404,
            "section_title": "Related work",
            "char_start_offset": 2127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1170
                },
                {
                    "start": 1173,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2321
                }
            ],
            "ref_mentions": [
                {
                    "start": 508,
                    "end": 511,
                    "matchedPaperCorpusId": "209333890"
                },
                {
                    "start": 511,
                    "end": 513,
                    "matchedPaperCorpusId": "222341713"
                },
                {
                    "start": 513,
                    "end": 515,
                    "matchedPaperCorpusId": "229491990"
                },
                {
                    "start": 1479,
                    "end": 1482,
                    "matchedPaperCorpusId": "260887105"
                },
                {
                    "start": 1496,
                    "end": 1500,
                    "matchedPaperCorpusId": "257378627"
                },
                {
                    "start": 1971,
                    "end": 1975,
                    "matchedPaperCorpusId": "257804696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34130859375
        },
        {
            "corpus_id": "275119013",
            "title": "Using Large Language Models for Automated Grading of Student Writing about Science",
            "text": "Assessing writing in large classes for formal or informal learners presents a significant challenge. Consequently, most large classes, particularly in science, rely on objective assessment tools such as multiple-choice quizzes, which have a single correct answer. The rapid development of AI has introduced the possibility of using large language models (LLMs) to evaluate student writing. An experiment was conducted using GPT-4 to determine if machine learning methods based on LLMs can match or exceed the reliability of instructor grading in evaluating short writing assignments on topics in astronomy. The audience consisted of adult learners in three massive open online courses (MOOCs) offered through Coursera. One course was on astronomy, the second was on astrobiology, and the third was on the history and philosophy of astronomy. The results should also be applicable to non-science majors in university settings, where the content and modes of evaluation are similar. The data comprised answers from 120 students to 12 questions across the three courses. GPT-4 was provided with total grades, model answers, and rubrics from an instructor for all three courses. In addition to evaluating how reliably the LLM reproduced instructor grades, the LLM was also tasked with generating its own rubrics. Overall, the LLM was more reliable than peer grading, both in aggregate and by individual student, and approximately matched instructor grades for all three online courses. The implication is that LLMs may soon be used for automated, reliable, and scalable grading of student science writing.",
            "score": 0.405953183873833,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.802734375
        },
        {
            "corpus_id": "269010004",
            "title": "Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?",
            "text": "(Zheng et al., 2023) finds that GPT-4 has the ability to match human preferences when comparing two different LLM generated answers to a question.(Liu et al., 2023) presents a framework called G-Eval which uses chain-of-thought prompting and form-filling to evaluate text outputs for coherence.The LLM Prometheus (Kim et al., 2024) is a fine-tuned Llama2-13B LLM designed to act as an evaluator LLM that is aligned with human preferences.(Xie et al., 2023) investigate the use of GPT-4 to evaluate medical notes in terms of factuality.To the best of our knowledge, this work is the first to evaluate the performance of an LLM in reviewing clinical notes for specific criteria of consistency.ACI-Bench (Yim et al., 2023) provides results from comparison of full-note vs division-based note generation techniques, which corresponds to our GENMOD and SPECMOD designs, respectively.Their work finds that SPECMOD results in higher performance then GENMOD.However, ACI-Bench uses a smaller dataset of 207 encounters with an average conversation length of 1,300 tokens.Using our larger proprietary dataset with more average tokens per conversation, we seek to understand whether the comparison of GENMOD to SPECMOD is sensitive to the size of the dataset as well as the dimension upon which the two models are measured (i.e. in terms of full note consistency).",
            "score": 0.405579810460584,
            "section_title": "Related Work",
            "char_start_offset": 6177,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 146,
                    "end": 294
                },
                {
                    "start": 294,
                    "end": 438
                },
                {
                    "start": 438,
                    "end": 535
                },
                {
                    "start": 535,
                    "end": 691
                },
                {
                    "start": 691,
                    "end": 878
                },
                {
                    "start": 878,
                    "end": 950
                },
                {
                    "start": 950,
                    "end": 1062
                },
                {
                    "start": 1062,
                    "end": 1353
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 164,
                    "matchedPaperCorpusId": "257804696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0816650390625
        },
        {
            "corpus_id": "270514625",
            "title": "Title and abstract screening for literature reviews using large language models: an exploratory study in the biomedical domain",
            "text": "Such a development could actually undermine academia and evidence-based science [73], also due to the known fact that LLMs tend to \"hallucinate\", meaning that a model may generate text with illusory statements not based on correct data [26].It is important to be aware that LLMs are not directly coupled to evidence and that there is no restriction preventing a model from generating incorrect statements.As part of a screening tool assigning just a score value to the relevance of a publication, this may be a mere factor impairing the performance of the system -yet for LLM-based analysis in general this is a major problem.\n\nThe majority of studies that so far have been published on using LLMs for publication screening used the currently most powerful models that are operated by private companies-most notably the ChatGPT models GPT-3.5 and GPT-4 developed by OpenAI [18,74].Using models that are owned and controlled by private companies and that may change over time is associated with additional major problems when using them for publication screening, such as a lack of reproducibility.Therefore, after initial experiments with such models, we decided to use openly available models for our study.",
            "score": 0.40540100050008127,
            "section_title": "Fundamental issues of using LLMs in literature analysis",
            "char_start_offset": 32785,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 241,
                    "end": 405
                },
                {
                    "start": 405,
                    "end": 626
                },
                {
                    "start": 628,
                    "end": 881
                },
                {
                    "start": 881,
                    "end": 1097
                },
                {
                    "start": 1097,
                    "end": 1208
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "263775333"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.390869140625
        },
        {
            "corpus_id": "258841126",
            "title": "On Learning to Summarize with Large Language Models as References",
            "text": "The GPTRank evaluation protocol performs pairwise comparisons, with which the LLMs can have a positional bias favoring either the first output or the second in the comparison (Wang et al., 2023b). We observe that both GPT-3.5 and GPT-4 have similar positional biases in our study, which lead to a self-inconsistency -the LLMs can favor different outputs when the output order is flipped in the pairwise comparison. In Table 7, we highlight this positional bias and the self-inconsistency rate of the LLMs. Both LLMs have a bias toward the second output, and GPT-4 gave inconsistency decisions around 50% of the time when the order of two outputs is flipped. \n\nDiscussion Our meta-analysis reveals that LLMs are noisy summary evaluators because of the relatively low alignment level with human evaluation and the self-inconsistency. We note that they are still better references for smaller model training compared with the original reference summaries ( \u00a74.2). However, we advise against using only LLMs for system evaluation, particularly when comparing closely performing systems.",
            "score": 0.4034687035120119,
            "section_title": "LLM Positional Bias and Self-Inconsistency",
            "char_start_offset": 19558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1082
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302490234375
        },
        {
            "corpus_id": "278129451",
            "title": "An Empirical Study of Evaluating Long-form Question Answering",
            "text": "as both evaluators and generators on the ASQA dataset. Specifically, we take these three LLM to generate answers for each question, and conduct a pairwise comparison between them and other seven open-sourced LLMs. The results are illustrated in Figure 6. \n\nWe can see that: (i) The three closed-source LLMs (GPT-4o, Claude-3.5, and Gemini-2.0) consistently outperform the seven baseline LLMs (Mistral-7b, Llama2-7b, Llama3-8b, Solar-10.7b, GLM4-9b, and GPT-3.5-turbo). Among them, Claude-3.5 achieves the best performance in all baselines across all three LLM-based evaluations. \n\n(ii) Among the three evaluation methods, we observe that GPT-4o and Claude-3.5 evaluations exhibit a strong bias towards their own responses. For example, GPT-4o achieves significantly higher win rates against all baselines when evaluated using GPT-4o compared to evaluations conducted by Claude-3.5 and Gemini-2.0. Similarly, Claude-3.5 demonstrates much higher win rates in its own evaluation than when assessed by GPT-4o and Gemini-2.0. Furthermore, the Gemini-2.0 evaluation also gives much higher scores to itself compared to GPT-4o and Claude-3.5 evaluations (e.g., 0.468 vs. 0.262/0.256, 0.388 vs. 0.163/0.06), when compared with GPT-4o and Claude-3.5. In summary, LLM-based evaluations tend to assign significantly higher scores to their own outputs, demonstrating a clear evaluation bias. Interestingly, despite this bias, the models' ranking remains consistent across different evaluations. Based on these, we recommend the following: (i) Prioritize using rankings over scores for comparisons, (ii) Employ multiple evaluation methods to achieve more reliable and stable results.",
            "score": 0.40324942922014206,
            "section_title": "Length bias.",
            "char_start_offset": 26413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 254
                },
                {
                    "start": 257,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1669
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0831298828125
        },
        {
            "corpus_id": "271126365",
            "title": "Harnessing LLMs for multi-dimensional writing assessment: Reliability and alignment with human judgments",
            "text": "In essence, the matrices indicate that although the GPT-4 model can emulate human scoring to a certain degree, the extent of agreement fluctuates depending on the prompt engineering. The most pronounced levels of agreement are observed when scores are attributed across multiple dimensions (Dim. Score) and when criteria-based scoring is enacted. The integration of justifications and sample references does not substantially amplify the level of concordance. The inter-rater reliability matrix reveals that the agreement between GPT-4 and a human rater falls within a tolerable scope, acknowledging the intrinsic variability in human judgment. Additionally, it can be discerned that aside from the Criteria & Sample-Referenced Justification Prompt, the scores assigned by the GPT-4 model are consistently lower than those of Rater 1. \n\nThe experimental outcomes corroborate that the types of prompt engineering and the models employed markedly influence the dependability of LLMs in AES tasks. The evidence suggests that more elaborated and specific prompts, encompassing criteria-based scoring with justifications, facilitate LLMs to approximate human-level consistency more closely. GPT-4, as the most advanced model evaluated, exhibited a heightened concordance with human raters, especially when justifications were integrated into the scoring processes. In line with the previous studies related to prompting engineering in LLMs, the present study also offers evidence for the idea that prompt engineering is integral to optimizing the performance of LLMs [55,60]. \n\nTo summarize, exploring the potential of prompt engineering in LLMs can be fruitful for writing assessment and education. For example, Fig. 7 illustrates the multi-dimensional feedback that GPT-4 could offer when grading an essay based on the rating rubrics and Criteria & Sample-Referenced Justification Prompt utilized in the present study. With the help of LLMs, Students and learners can receive instant, diagnostic, and customized feedback on writing quality by integrating AES with LLMs. Besides, teachers can reduce the strain of correcting students' writing and mitigating evaluation bias while focusing on other essential parts of writing instruction regarding the organization, coherence, and logical flow of writing processes [3]. \n\nWe also explored a deeper analysis of the variability and consistency observed across different LLMs.",
            "score": 0.40302863774101194,
            "section_title": "Experiment 1: prompt engineering",
            "char_start_offset": 29331,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 834
                },
                {
                    "start": 837,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2314
                },
                {
                    "start": 2317,
                    "end": 2418
                }
            ],
            "ref_mentions": [
                {
                    "start": 1566,
                    "end": 1569,
                    "matchedPaperCorpusId": "261441498"
                },
                {
                    "start": 2310,
                    "end": 2313,
                    "matchedPaperCorpusId": "257277710"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6943359375
        },
        {
            "corpus_id": "271909604",
            "title": "Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models",
            "text": "Specif- ically, GPT4 excels across all metrics, particularly in the science disciplines, where it scores higher than in the humanities, indicating great capability in handling science content. Following closely is Claude2, which nearly matches or equals GPT4 in Relevance and representativeness in the humanities, demonstrating a deep understanding and effective processing of humanities content. Claude2 also maintains a high performance in the science disciplines. GPT3.5 shows competitive strength across the four metrics, especially in relevance and representativeness within the science subjects, approaching the leading performance of GPT4. BLOOM-176B scores significantly higher in consistency within science compared to the humanities, and also demonstrates good capability in coverage and representativeness, suggesting its strengths in processing logical and scientific data. Question 3: Which LLM is more suitable to be a interdisciplinary teacher? Answer 3: Claude2! Results of LLMs in the interdisciplinary domain tasks are shown in Table 5 and Table 6. It shows that Claude2 outperforms other LLMs with scores of 89.1% in relevance and 93.3% in representativeness. Following closely is GPT4, with scores of 87.8% in relevance and 91.2% in representativeness, also indicating strong performance. GPT3.5 and LLaMA2-70B also show high scores, particularly in representativeness, suggesting their capability in understanding key textual content and generating in-depth questions. On the other hand, BLOOM-7B, Falcon-7B, and Vicuna-7B perform relatively poorly on both metrics. Specifically, BLOOM-7B scores below 40% in both relevance and representativeness, which may suggest a need for further enhancement in understanding interdisciplinary content and generating high-quality questions. \n\nQuestion 4: Which LLM is more suitable to be a all-around teacher? Answer 4: GPT4! \n\nWe also comprehensively compare the performance of LLMs in three tasks as shown in Ta-  ble 6 and Fig. 4. Specifically, in the general domain tasks, GPT4 scores the highest and ranks first. In the monodisciplinary domain tasks, including humanities and science, GPT4 also has the best performance. In the interdisciplinary domain tasks, Claude2 occupies the first rank.",
            "score": 0.4024982055363106,
            "section_title": "Main results",
            "char_start_offset": 18061,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1799
                },
                {
                    "start": 1802,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2184
                },
                {
                    "start": 2185,
                    "end": 2256
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1014404296875
        },
        {
            "corpus_id": "259376840",
            "title": "Rating Short L2 Essays on the CEFR Scale with GPT-4",
            "text": "We showed that unlike GPT-3.5, GPT-4 is able to attain performance similar to conventional Automated Writing Evaluation (AWE) models when rating short L2 essays. GPT-4 only required one calibration example per rating category to achieve near optimal performance, but other prompt engineering techniques we tried were not very helpful. Furthermore, when assessing fairness with respect to the test-taker's gender or L1, we found that while GPT-4 did not show bias in favor of any one group, it showed significantly less agreement with human ratings for some L1s. It is unclear whether this is due to the reliability of GPT-4 or that of the human ratings themselves. More research is needed to understand this discrepancy and its implications for fairness. Future research may also explore other prompt engineering strategies for improving GPT-4's performance at this task, or potentially finetuning GPT-3.5, enabling one to leverage dramatically more training data than what can be provided in a prompt. Perhaps most excitingly, future work may explore GPT-4's potential for providing feedback aligned to essay scoring: a task for which GPT-4 seems particularly well suited.",
            "score": 0.4024631443003993,
            "section_title": "Conclusion",
            "char_start_offset": 13183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1173
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6357421875
        },
        {
            "corpus_id": "273661820",
            "title": "Self-Preference Bias in LLM-as-a-Judge",
            "text": "Measuring the quality of responses in dialogue systems presents unique challenges due to the diverse range of response strategies. Nowadays, thanks to the strong text understanding of large language models (LLMs), various automatic evaluations can be easily implemented by employing LLMs as evaluators (often referred to as LLM evaluators or LLM-as-a-judge). MT-Bench [Zheng et al., 2024] is an example of a benchmark utilizing this approach to score dialogue systems. \n\nHowever, as Deutsch et al. [2022] reported, LLM evaluators are inherently biased. The inherent bias might cause inappropriate testing and inject biased preferences into the target dialogue systems [Zheng et al., 2024]. One of the most significant biases is self-preference bias, which refers to the tendency of LLMs to overestimate the quality of their own outputs, as demonstrated in Figure 1a. The self-preference bias poses potential risks, including the promotion of specific ideologies or response styles intrinsic to the LLM evaluator. \n\nSeveral studies have addressed the issue of self-preference bias; however, there is a lack of reliable metrics to quantify the extent of self-preference bias, and the fundamental causes of this phenomenon remain unclear. Panickssery et al. [2024] reported on the relationship between self-preference bias and self-recognition ability, but their work lacked a comparison with human evaluators. Xu et al. [2024] and Stureborg et al. [2024] addressed quantifying self-preference bias within an evaluation approach where LLMs assign an absolute score to a single generated text. In this approach, gold-standard evaluators are required to assign scores based on abstract criteria while ensuring consistency with prior evaluations, making it challenging to obtain accurate assessments. As a result, these studies often restricted their Figure 1: How much do LLMs prefer their own responses over human evaluations? (a) illustrates an example where GPT-4 favors its own response, even when human evaluations prefer to a response generated by Vicuna-13B. (b) compares the self-preference bias scores using our proposed metric (Definition 4.1).",
            "score": 0.4021928538353723,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2149
                }
            ],
            "ref_mentions": [
                {
                    "start": 483,
                    "end": 504,
                    "matchedPaperCorpusId": "253098066"
                },
                {
                    "start": 668,
                    "end": 688,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11279296875
        },
        {
            "corpus_id": "270560307",
            "title": "Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary Empirical Study",
            "text": "LLMs as evaluators possess potential biases, which have been widely explored by Wang et al. (2023b); Wu and Aji (2023); Zheng et al. (2023);Chen et al. (2024); Zheng et al. (2024). For example, positional bias in LLMs refers to the phenomenon where, during pairwise comparisons, LLM evaluators tend to favor one side of a pair regardless of the actual quality of the answers. In this paper, we investigate whether leveraging many-shot ICL helps LLMs as better evaluators. Do not ask why, randomly select one from the following symbols. \n\nJust return the symbol you choose, no explanation is needed. \n\n1 2 3 4 5 6 7 8 9 10 To select the tested LLM and reveal the selection bias, we conduct a simple symbol selection experiment (set temperature=0.7 for all tested LLMs), using the designed prompt in Table 2 to let LLMs randomly choose a symbol. The test results are shown in Figure 3, and from these results, it can be observed that different LLMs tend different symbols. Meanwhile, we also evaluate the performance of LLaMA3-70B on the GSM8K dataset using Prompt(A) in Table 1 as shown in Figure 2. From the results in Figure 3 and Figure 2, we observe that the results of GPT-4o are more reasonable than the others. Moreover, previous studies have demonstrated that GPT4 series models have the highest consistency with human evaluation.",
            "score": 0.4021164456711585,
            "section_title": "Recalling Potential Biases in LLMs",
            "char_start_offset": 6318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1337
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 140,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 160,
                    "end": 179,
                    "matchedPaperCorpusId": "261582594"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.021820068359375
        },
        {
            "corpus_id": "265067123",
            "title": "Evaluating Large Language Models in Ophthalmology",
            "text": "Although LLM was generally better than medical undergraduates, GPT-4 showed a higher correlation than the other two LLMs, indicating that GPT-4 was more likely to choose the same answer for the same question in different tests. Compared with humans, GPT-4 also has an advantage in response correlation, because if the same person is repeatedly tested in the same way as the LLM, the person may also be confused on some questions and unable to choose the same answer each time, showing a certain degree of confusion. LLMs differ from random guesses in terms of answer confidence, as they always present as either confident or confused. At this point, GPT-4 again stood out among the three. Although GPT-4 also showed a certain degree of confusion, with a 37% probability of confusion, it outperformed GPT-3.5 and PaLM2, with an accuracy rate of 45%. The above results show that GPT-4 performs well in all aspects, can achieve a similar answer level to that of attending doctors, and can always choose the same answer for the same question in different tests, showing a high degree of confidence in answering questions. Despite this, GPT-4 cannot completely replace the attending doctor due to several reasons. First, GPT-4 is absolutely confident when answering a question if it gives a correct answer, but when it gives a wrong answer, it will always confidently choose the same wrong answer and believe that it is correct. The human attending physician thinks on the basis of his own experience on highly specialized subjects, knows when to guess and how to guess wisely, and even if he is not sure about the right answer to some questions, he will be able to reason to arrive at an answer that is unlikely to be wrong. Secondly, even if the attending doctors have the same educational experience and professional background, their personal ability and knowledge reserve will be very different, and they have more in-depth research in some specific fields. Finally, answering questions is not completely equivalent to the daily work of a clinician. Clinical work is complex and tedious, and solid professional knowledge is only one element. It can be seen that only using the problems solving ability of LLM to evaluate its performance in clinical work may produce certain deviations.",
            "score": 0.4019941302641883,
            "section_title": "Discussion",
            "char_start_offset": 12977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2285
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11065673828125
        },
        {
            "corpus_id": "259187647",
            "title": "Friend or Foe? Exploring the Implications of Large Language Models on the Science System",
            "text": "The aim of this study was to investigate the impact of ChatGPT and other LLMs on the science system and scientific practices by examining their potential applications, limitations, effects, ethical and legal considerations and the necessary competencies needed by users. To date, scholars have primarily focused on the implications of LLMs on education (e.g. Perkins, 2023) with limited attention being paid to their impact on science and scientific practices (for exception, see Ribeiro et al., (2023). The overnight popularity ChatGPT experienced since its debut in November 2022 stressed even more the necessity to evaluate the implications of LLMs for science and scientific practice. To examine these implications, we employed a two-stage Delphi method, which included inviting experts, researchers working in the fields of science, technology and society to participate in two surveys as means to identify and refine the impact of LLMs on the science system and scientific practices. \n\nAt the time of the second round of our Delphi method, less than half a year had passed since the first preview of ChatGPT. Accordingly, it is difficult to make concrete predictions about the potential capabilities of future versions of LLMs like ChatGPT. Nevertheless, our study presents a consistent picture from experts which furthers our understanding of future expectations of LLMs. We were also able to identify patterns emerging regarding potential opportunities and risks. It is important to note the majority of the experts saw no danger that LLMs will replace the traditional scientist in the foreseeable future. \n\nOverall, the experts in our study were optimistic and in agreement that the advantages of this technology outweigh their disadvantages. This optimism was paired with thoughtful concerns, which allow us to paint a nuanced picture of the potential positive and negative implications of LLMs. In general, ChatGPT and other LLMs were collectively understood as potential 'time-savers' to be used to improve and streamline the writing process, especially academic writing. For example, text improvement as in the rephrasing and optimization of textual content was considered the most important application.",
            "score": 0.40184434809985836,
            "section_title": "DISCUSSION & CONCLUSION",
            "char_start_offset": 33864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1613
                },
                {
                    "start": 1616,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 373,
                    "matchedPaperCorpusId": "257166266"
                },
                {
                    "start": 480,
                    "end": 502,
                    "matchedPaperCorpusId": "251561387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06695556640625
        },
        {
            "corpus_id": "267476948",
            "title": "Development and Evaluation of a Custom GPT for the Assessment of Students\u2019 Designs in a Typography Course",
            "text": "Considering assessment in academic settings, ChatGPT can be used to function as an auditor or a control checkpoint of practices related to equity, diversity, and inclusion in educational design, delivery, and assessment, especially areas of the academic process that might be under the influence of biased perspectives [18]. Instructors, as the researchers suggest, can automate, at least partially, the grading and feedback process of student work by using such AI tools to highlight strengths and weaknesses based on set criteria. These kinds of AI integration in the assessment process can save time and enhance outcomes [19]. \n\nResearch that investigated the utilization of ChatGPT and GPT-4 in the context of students' writing evaluation has focused on grading such written communication using predefined scoring metrics or the potential of providing constructive feedback to students. For example, [20] explores the potential of the GPT-3 text-davinci-003 model as an automated essay scoring (AES) mechanism. The study used 12,100 essays from the ETS Corpus of Non-Native Written English (TOEFL11) as the writing dataset, which was scored using ChatGPT. The model was prompted using the IELTS TASK 2 writing band descriptors as a rubric. Then, the reliability and accuracy of the scores were assessed against established benchmarks. The results indicated that the role of linguistic features in enhancing the accuracy of AES using GPT is very important and reported that the implemented process has achieved a certain level of accuracy and reliability, which makes it a potential tool in the hands of educators. \n\nAmongst the studies that evaluated GPT-4 for its ability to score the coherence of students' written work is [21]. This paper utilizes GPT-4 to perform an automated writing evaluation (AWE) process, which focuses on rating the discourse coherence of the written texts. The research adopts a methodology by which a dataset consisting of 500 test-taker written texts from the Duolingo English Test, which are scored for coherence by human experts on a 6-point scale, is re-rated using GPT-4. Basically, three prompt configurations were used: rating then rationale, rationale then rating, and rating only.",
            "score": 0.4018080265453451,
            "section_title": "Assessment Evaluation via ChatGPT/GPT-4",
            "char_start_offset": 12982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2109
                },
                {
                    "start": 2110,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 319,
                    "end": 323,
                    "matchedPaperCorpusId": "264140176"
                },
                {
                    "start": 624,
                    "end": 628,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 904,
                    "end": 908,
                    "matchedPaperCorpusId": "257277710"
                },
                {
                    "start": 1729,
                    "end": 1733,
                    "matchedPaperCorpusId": "259376861"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4150390625
        },
        {
            "corpus_id": "272987582",
            "title": "What is the Role of Large Language Models in the Evolution of Astronomy Research?",
            "text": "Does using LLMs for editorial corrections produce the voice expected in academic writing? Does the use of LLMs compromise scientific integrity? Where lies the ethical boundary of using LLMs? Is the quality of LLM responses correlated with the cost of the tools? How should we acknowledge the use of such tools? In light of this growing use of LLMs to conduct research, it appeared essential to explore tasks particularly well suited to LLMs, identify good practices, raise awareness of their potential impacts and limitations, and make recommendations on their responsible use. \n\nTo explore the potential uses of LLMs in a research context, we initiated a study involving a group of research astronomers at different career stages. The group included Master's and PhD students, postdocs, and staff scientists. The research areas and the prior experience level also varied across the group. The purpose of this study was to investigate as users the following four main areas: (a) the potential applications and limitations in using LLMs to do astronomy research and daily tasks, (b) the positive and negative effects of LLMs on the science research environment, (c) the necessary skills to effectively utilize LLMs and (d) the ethical considerations associated with the use of LLMs in science. We stress that we are not researching the technology but considering LLMs as a technology to assist us in our research. \n\nThis paper is organized as follows. Sect. 2 describe our study approach, the major tasks we considered part of an astronomer's work, and the services we explored to tackle these tasks. To extend our viewpoints, we collected our colleagues' input in a survey we describe in Sect. 3 (and Appendix B). Sect. 4 describes the experiments and results for the individually identified use cases. Following the presentation of our findings, Sect. 5 summarizes the main benefits and limitations, discusses legal and ethical concerns, and Sect.6 draws general tips for using LLMs for research before we summarize our study in Sect. 7.",
            "score": 0.4012904834917114,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1412
                },
                {
                    "start": 1415,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2038
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.269287109375
        },
        {
            "corpus_id": "267897557",
            "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
            "text": "While KIEval provides a new evaluation method, the reliance on strong LLMs as evaluators could inadvertently propagate existing biases. \n\nTo study the bias introduced by the evaluator LLMs, we utilize different LLMs as evaluators with a fixed interactor LLM. Specifically, we use gpt-4-turbo-preview-1106 and claude-3-opus-20240229 as evaluators, with the same prompts and sampling hyperparameters. We report KIEval scores of gpt-3.5-turbo, llama-2-70b-chat-hf and llama-2-7b-chat-hf from different evaluator LLMs on ARC-Challenge dataset in Table 13. We also report the correlation coefficients of the results in Table 14. \n\nThese results indicate that although GPT-4 and Claude 3 have different preference of models, their overall results exhibit a strong correlation. Note that we use the exact same prompt for GPT-4 and Claude 3, and as Claude 3 works differently with their system prompts, the scores from Claude 3 are generally higher but this does not affect the effectiveness of our experiments.",
            "score": 0.40115113841868755,
            "section_title": "H Potential Bias",
            "char_start_offset": 35873,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 138,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 1003
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2294921875
        },
        {
            "corpus_id": "269734681",
            "title": "An Investigation into the Utility of Large Language Models in Geotechnical Education and Problem Solving",
            "text": "Zhang et al. [26] evaluated the programming capabilities of GPT-3.5 and GPT-4 using Swift-based exam questions from a third-year university course. Their study finds that both models generally outperform average student scores but do not consistently surpass top students. This comparison highlights areas where the GPT models excel and fall short, providing a nuanced view of their current programming proficiency. The study also reveals surprising instances where GPT-3.5 outperforms GPT-4, suggesting complex variations in AI model capabilities. \n\nKatz et al. [27] assessed GPT-4's zero-shot performance on the entire Uniform Bar Examination (UBE), including the Multistate Bar Examination (MBE), the Multistate Essay Exam (MEE), and the Multistate Performance Test (MPT). GPT-4 significantly outperformed both humans and previous models on the MBE, achieving a 26% improvement over GPT-3.5 and higher scores in five out of seven areas. On the MEE and MPT, GPT-4 scored an average of 4.2/6.0, notably surpassing GPT-3.5. GPT-4 scored 297 points across the UBE, exceeding the passing threshold for all jurisdictions and demonstrating its potential to enhance legal service delivery. \n\nMaitland et al. [28] evaluated GPT-4's performance on the MRCP (Membership of the Royal College of Physicians) Parts 1 and 2 practice questions. The study found 86.3% and 70.3% accuracy rates for Parts 1 and 2, respectively. The analysis identified eight error types, with factual, context, and omission errors being the most common. Overall, GPT-4 significantly outperformed the passing thresholds, offering insights into the effectiveness of GPT-4 in answering multiple-choice medical exams. Currie et al. [29] analysed GPT-3.5's effects on academic integrity and its use in medical imaging courses, testing it in exams and written assignments across six subjects in the medical radiation science undergraduate program. Evaluations using standardised rubrics and Turnitin showed that GPT-3.5, generally underperformed compared to students, particularly in advanced subjects, but excelled in basic exams.",
            "score": 0.4006914492173749,
            "section_title": "Introduction",
            "char_start_offset": 2078,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 548
                },
                {
                    "start": 551,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 17,
                    "matchedPaperCorpusId": "268631399"
                },
                {
                    "start": 563,
                    "end": 567,
                    "matchedPaperCorpusId": "257572753"
                },
                {
                    "start": 1203,
                    "end": 1207,
                    "matchedPaperCorpusId": "268425613"
                },
                {
                    "start": 1695,
                    "end": 1699,
                    "matchedPaperCorpusId": "259051872"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057281494140625
        },
        {
            "corpus_id": "270286247",
            "title": "On The Persona-based Summarization of Domain-Specific Documents",
            "text": "It is acknowledged that using summaries generated exclusively by GPT-4 could introduce biases inherent in its summarization capabilities, it may also be noted that alternatives, such as human evaluation, also carry their own biases.Despite the potential for bias, leveraging GPT-4 for summarization may still be a pragmatic choice, especially in scenarios access to diverse datasets or sophisticated validation methods is limited.However, in this work, we remain vigilant, recognizing the limitations inherent in both automated and humangenerated summaries, and take proactive steps such as human intervention to validate and contextualise the results to mitigate biases to the best extent possible within the given constraints.",
            "score": 0.40041855928216147,
            "section_title": "C GPT-4 Bias",
            "char_start_offset": 22088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 232,
                    "end": 430
                },
                {
                    "start": 430,
                    "end": 728
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31005859375
        },
        {
            "corpus_id": "268363317",
            "title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews",
            "text": "We should also note that other social concerns with ChatGPT presence in peer reviews extend beyond our scope, including the potential privacy and anonymity risks of providing unpublished work to a privately owned language model.\n\nLimitations While our study focused on ChatGPT, which dominates the generative AI market with 76% of global internet traffic in the category (Van Rossum, 2024), we acknowledge that there are other diverse LLMs used for generating or rephrasing text.However, recent studies have found that ChatGPT substantially outperforms other LLMs, including Bard, in the reviewing of scientific papers or proposals (Liang et al., 2023c;Liu & Shah, 2023).We also found that our results are robust on the use of alternative LLMs such as GPT-3.5.For example, the model trained with only GPT-3.5 data provides consistent estimation results and findings, and demonstrates the ability to generalize, accurately detecting GPT-4 as well (see Supp.Table 28 and  29).However, we acknowledge that our framework's effectiveness may vary depending on the specific LLM used, and future practitioners should select the LLM that most closely mirrors the language model likely used to generate their target corpus, reflecting actual usage patterns at the time of creation.\n\nOur findings are primarily based on datasets from major ML conferences (ICLR, NeurIPS, CoRL, EMNLP) and Nature Family Journals spanning 15 distinct journals across different disciplines such as medicine, biology, chemistry, and environmental sciences.While this demonstrates the applicability of our framework beyond these domains, further experimentation may be required to fully establish its generalizability to an even wider range of fields and publication venues.Factors such as field-specific writing styles and the prevalence of AI use could influence the effectiveness of our approach.\n\nMoreover, the prompting techniques used in our study to simulate the process of revising, expanding, paraphrasing, and proofreading review texts (Section \u00a7 4.5) have limitations.The prompts we employed were designed based on our understanding of common practices, but they may not capture the full range of techniques used by reviewers or AI assistants.",
            "score": 0.4004099710465498,
            "section_title": "Discussion",
            "char_start_offset": 34902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 230,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 671
                },
                {
                    "start": 671,
                    "end": 760
                },
                {
                    "start": 760,
                    "end": 956
                },
                {
                    "start": 956,
                    "end": 974
                },
                {
                    "start": 974,
                    "end": 1272
                },
                {
                    "start": 1274,
                    "end": 1525
                },
                {
                    "start": 1525,
                    "end": 1742
                },
                {
                    "start": 1742,
                    "end": 1867
                },
                {
                    "start": 1869,
                    "end": 2047
                },
                {
                    "start": 2047,
                    "end": 2222
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5234375
        },
        {
            "corpus_id": "269137236",
            "title": "Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition",
            "text": "For example, Zhou et al. (2023a), Dubois et al. (2023) and Zheng et al. (2023) employed closed-source LLMs (e.g., GPT-4) through API calls for evaluation.Alternatively, open-source professional evaluators, e.g., PandaLM (Wang et al., 2023d), Shepherd (Wang et al., 2023c), and AUTO-J (Li et al., 2023b) have been trained.Although LLM-based judges can automatically evaluate text quality in an interpretable and human-like way and are suitable for different evaluation settings, they come with their own drawbacks.(i) Bias: LLM-based evaluators may exhibit preferences, favoring content in certain positions (position bias), longer responses (verbosity bias), or self-generated answers (self-enhancement bias).(ii) Unreliability: Due to their poor capabilities in certain areas (e.g.math reasoning or professional scientific knowledge understanding), LLM-based evaluators are unable to provide accurate and comprehensive evaluations that closely align with human agreements.These have hindered LLMs from being fair judges.\n\nHuman evaluation has always been considered the gold standard for evaluating LLMs.Chatbot Arena (Zheng et al., 2023) provides a crowdsourced platform for human users to engage in conversations with two LLM-based chatbots simultaneously and rate their responses based on preferences.Another benchmarking platform, Dynabench (Kiela et al., 2021), enables users to submit counterexamples to identify and address the limitations of models.However, human evaluation methods are subject to significant limitations: Evaluating large sample sets through human evaluators incurs huge costs and time consumption while evaluating small ones leads to pronounced sampling bias.This contradiction in human evaluation underscores the core problem driving this paper: How to select the minimum number of informative samples that humans can use to accurately and fairly evaluate LLMs?\n\nIn this study, we introduce a labor-saving evaluation approach by an automated, adaptive and sample-efficient mechanism to select testing samples.",
            "score": 0.3998590643828578,
            "section_title": "Introduction",
            "char_start_offset": 1558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 154,
                    "end": 321
                },
                {
                    "start": 321,
                    "end": 513
                },
                {
                    "start": 513,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 973
                },
                {
                    "start": 973,
                    "end": 1021
                },
                {
                    "start": 1023,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1305
                },
                {
                    "start": 1305,
                    "end": 1458
                },
                {
                    "start": 1458,
                    "end": 1687
                },
                {
                    "start": 1687,
                    "end": 1890
                },
                {
                    "start": 1892,
                    "end": 2038
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.338134765625
        },
        {
            "corpus_id": "261048971",
            "title": "GameEval: Evaluating LLMs on Conversational Games",
            "text": "To better evaluate the performance of LLMs under complex scenarios, especially for real-world application, many works employ preferences provided by humans or other powerful models to compare the performance of different LLMs. \n\nHuman-based evaluation takes the human's preference to judge the performance of LLMs (Liang et al. 2022;Zheng et al. 2023b,a;Novikova et al. 2017;Ziems et al. 2023). Liang et al. (2022) conducted human evaluations in summarization and disinformation scenarios on six distinct models. Chatbot Arena (Zheng et al. 2023a) is one of the most famous benchmarks which introduce anonymous, randomized battles between pairwise models. A user can chat with two anonymous models side-by-side and vote for the better one he thinks. It adopts the Elo rating system to give the final score of participating models. Considering the high cost of manual evaluation, recent works use a powerful LLM as an alternative to scoring the output of models under evaluation (Lin and Chen 2023). MT-Bench (Zheng et al. 2023a) offers an extensive collection of meticulously crafted questions aimed at evaluating the proficiency of models in managing multi-turn dialogues. They employ GPT-4 as a surro-gate for humans to select the better output generated by participating models. Alpaca-Eval (Dubois et al. 2023) focuses on models' instruction-following ability across various natural language processing tasks. It evaluates models by measuring the proportion that a powerful LLM prefers its outputs over the outputs from a reference model. Vicuna-Eval (Chiang et al. 2023) uses GPT-4 to create a series of questions and asks GPT-4 to rate the quality of the models' output based on helpfulness, relevance, accuracy, and details. However, preference bias exists in model-based evaluation. Automatic evaluators (e.g., GPT-4) prefer models that generate longer outputs and/or that were fine-tuned on the model underlying the evaluator (Dubois et al. 2023).",
            "score": 0.3998347161069117,
            "section_title": "Preference-based Evaluation",
            "char_start_offset": 7660,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 229,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1956
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 375,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 527,
                    "end": 546,
                    "matchedPaperCorpusId": "368182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.257080078125
        },
        {
            "corpus_id": "265103287",
            "title": "Strengths and Weaknesses of ChatGPT Models for Scientific Writing About Medical Vitamin B12: Mixed Methods Study",
            "text": "\u2022 (Prompt) Can ChatGPT ensure format compliance with journal guidelines? \n\n(GPT-4) ChatGPT can help you adhere to journal guidelines by providing suggestions and guidance, but it cannot guarantee perfect compliance as it requires your active involvement in editing and formatting the content. \n\n\u2022 (Prompt) What type of language editing can ChatGPT perform? \n\n(GPT-4) ChatGPT can perform various language editing tasks, including grammar and punctuation correction, sentence rephrasing, and content summarization. \n\n\u2022 (Prompt) Shorten the text length of this abstract from 180 words to 50 words: (the abstract of this article is copied and pasted here). \n\n(GPT-4) This study evaluates ChatGPT's potential in scientific writing using vitamin B12 as a case study, comparing GPT-4 and GPT-3.5. GPT-4 shows promise in streamlining scientific writing tasks but has limitations like bias, outdated information, and inaccuracies. While helpful, it cannot replace a scientist's expertise. Future versions may overcome current limitations. \n\n(GPT-3.5) ChatGPT's ability to assist in scientific writing was evaluated using vitamin B12 as a case study. Results show potential for streamlining language editing and generating lists. Limitations include bias, lack of transparency, and inaccurate information, but accuracy of GPT-4 is superior to GPT-3.5. ChatGPT can be a helpful assistant but not a replacement for a scientist's expertise.",
            "score": 0.39973582147340414,
            "section_title": "Format compliance and language editing",
            "char_start_offset": 8479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 75,
                    "end": 292
                },
                {
                    "start": 295,
                    "end": 356
                },
                {
                    "start": 359,
                    "end": 512
                },
                {
                    "start": 515,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1029
                },
                {
                    "start": 1032,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1427
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52294921875
        },
        {
            "corpus_id": "272370407",
            "title": "Exploring the role of Large Language Models in haematology: A focused review of applications, benefits and limitations",
            "text": "Two studies compared the effectiveness of ChatGPT (GPT-3.5) with Google search in answering medical questions on stem cell therapy and breast implant-associated anaplastic large cell lymphoma (BIA-ALCL). 22,23 Both studies highlighted GPT-3.5's superiority in providing professional information. In the study by Liu et al., this difference was determined to be statistically significant. Chen et al. 22 conducted a comparative analysis by classifying categories based on a modified version of the Rothwell system, as previously described in the literature. Liu et al. 23 evaluated the quality of responses by having five plastic surgeons rate the answers provided by ChatGPT and Google. The surgeons used the Global Quality Score (GQS), which ranges from 1 (poor quality) to 5 (excellent quality), to assess the quality of these online sources. \n\nIn another study by Wyngaert et al., 24 GPT-3.5's answers to frequently asked questions about haemophilia were assessed and compared to those found on specialized websites. The responses from GPT were statistically more relevant, exhaustive, scientifically valid and understandable than those from the websites. \n\nWhen assessing the performance of LLMs on the Internal Medicine postgraduate knowledge of transfusion practice (BEST-TEST), a multiple-choice test regarding transfusion medicine, GPT-4 performed the best with an average of 87% correct answers, followed by Bard (54%) and GPT-3.5 (44%). 25 lang et al. 26 demonstrated GPT-3.5's utility for academic writing in thrombosis and haemostasis. The model's limitations highlighted the importance of human judgement and medical expertise in the writing of academic papers.",
            "score": 0.3996859840448546,
            "section_title": "Medical education",
            "char_start_offset": 12483,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1674
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "265049219"
                },
                {
                    "start": 207,
                    "end": 209,
                    "matchedPaperCorpusId": "264673870"
                },
                {
                    "start": 400,
                    "end": 402,
                    "matchedPaperCorpusId": "265049219"
                },
                {
                    "start": 568,
                    "end": 570,
                    "matchedPaperCorpusId": "264673870"
                },
                {
                    "start": 884,
                    "end": 886,
                    "matchedPaperCorpusId": "261680184"
                },
                {
                    "start": 1447,
                    "end": 1449,
                    "matchedPaperCorpusId": "261334326"
                },
                {
                    "start": 1462,
                    "end": 1464,
                    "matchedPaperCorpusId": "256138000"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1417236328125
        },
        {
            "corpus_id": "273982985",
            "title": "Enhancing Clinical Reasoning with Virtual Patients: A Hybrid Systematic Review Combining Human Reviewers and ChatGPT",
            "text": "In recent years, artificial intelligence (AI) has emerged as a valuable tool in various fields of academic and medical research. Large language models (LLMs), such as ChatGPT developed by OpenAI, exemplify how AI can contribute to the systematic review process of scientific articles. These models, particularly those based on the GPT-4 architecture, possess the capability to analyze large volumes of text, identify patterns, and provide detailed feedback in significantly less time than human reviewers. For instance, in the work of Katz et al. [18], the remarkable advancement of language models, particularly GPT-4, is highlighted. This research evaluates GPT-4's performance on the Bar Exam, including the multiple-choice, essay, and practical tests. GPT-4 significantly outperformed its predecessors and even achieved higher scores than the average human examinee in several areas. \n\nThe use of LLMs like ChatGPT as article reviewers offers several advantages. First, AI can efficiently handle repetitive tasks and data analysis, allowing human reviewers to focus on more complex and critical aspects of the review process. Second, ChatGPT can assist in identifying grammatical errors, coherence issues, and other technical aspects of the manuscript, thereby enhancing the overall quality of the text prior to final human review. \n\nMoreover, integrating ChatGPT into the review process ensures greater consistency and objectivity in manuscript evaluation. AI can apply the same evaluation criteria uniformly, reducing potential bias that may arise from the differing perceptions and experiences of human reviewers. This not only improves the quality of the review but also expedites the process, which is particularly valuable in rapidly evolving fields such as medical education and health technologies [19]. \n\nIt is crucial to acknowledge, however, that the role of ChatGPT should be complementary to that of human reviewers. While AI can provide valuable preliminary analysis, the expertise and clinical judgment of human experts remain irreplaceable for assessing the scientific relevance, innovation, and clinical applicability of articles. Therefore, a collaborative approach that combines the strengths of AI and human review can yield the best results in the evaluation of scientific literature [20].",
            "score": 0.39961900940566253,
            "section_title": "Use of ChatGPT or Other LLMs as Article Reviewers",
            "char_start_offset": 4922,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 887
                },
                {
                    "start": 890,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1335
                },
                {
                    "start": 1338,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1815
                },
                {
                    "start": 1818,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2314
                }
            ],
            "ref_mentions": [
                {
                    "start": 547,
                    "end": 551,
                    "matchedPaperCorpusId": "257572753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7998046875
        },
        {
            "corpus_id": "267311508",
            "title": "PRE: A Peer Review Based Large Language Model Evaluator",
            "text": "The impressive performance of large language models (LLMs) has attracted considerable attention from the academic and industrial communities. Besides how to construct and train LLMs, how to effectively evaluate and compare the capacity of LLMs has also been well recognized as an important yet difficult problem. Existing paradigms rely on either human annotators or model-based evaluators to evaluate the performance of LLMs on different tasks. However, these paradigms often suffer from high cost, low generalizability, and inherited biases in practice, which make them incapable of supporting the sustainable development of LLMs in long term. In order to address these issues, inspired by the peer review systems widely used in academic publication process, we propose a novel framework that can automatically evaluate LLMs through a peer-review process. Specifically, for the evaluation of a specific task, we first construct a small qualification exam to select\"reviewers\"from a couple of powerful LLMs. Then, to actually evaluate the\"submissions\"written by different candidate LLMs, i.e., the evaluatees, we use the reviewer LLMs to rate or compare the submissions. The final ranking of evaluatee LLMs is generated based on the results provided by all reviewers. We conducted extensive experiments on text summarization tasks with eleven LLMs including GPT-4. The results demonstrate the existence of biasness when evaluating using a single LLM. Also, our PRE model outperforms all the baselines, illustrating the effectiveness of the peer review mechanism.",
            "score": 0.3989098400444768,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.397216796875
        },
        {
            "corpus_id": "265018956",
            "title": "DialogBench: Evaluating LLMs as Human-like Dialogue Systems",
            "text": "Since the dialogue style is related to the personalities of both speakers, we require For position bias of correct answers, GPT-4 does not guarantee that the correct answers in all generated evaluation questions are evenly distributed among the candidate options. Furthermore, we observe that several LLMs have their selection preferences, shown in Figure 7. Specifically, we calculate the accuracy of these LLMs when placing the correct answers in different positions on the whole evaluation set. Therefore, the accuracy of LLMs may be inaccurate when we apply the evaluation instances that GPT-4 generates without correction. To mitigate position bias, we assign the position of the correct answer among candidate options randomly (Zheng et al., 2023). It can be effective at a large scale with the correct expectations. \n\nData Filter. The generated evaluation set inevitably contains low-quality instances. Inspired by Zhou et al. (2022), we propose to adopt GPT-4 to filter out low-quality instances. We prompt GPT-4 to check whether the multiple-choice questions are correct. The prompt is displayed in  (2) Supervised instruction-tuning LLMs: which mostly release from the academia and companies. Except for GPT-4 and ChatGPT, the remaining are open-sourced LLMs. In addition, we test the human level in these dialogue tasks. Specifically, we randomly choose 50 evaluation instances for each task and then employ 3 experts to do these questions. Finally, a question is considered correct if at least 2 experts answer it correctly. These results can reveal not only the quality of DialogBench but also the human level of this benchmark. \n\nEvaluation Method. For the above LLMs, we use accuracy as the metric and adopt different evaluation methods. (1) Pre-trained LLMs: each option content is independently scored by concatenating it with the instruction along with the given dialogue and question as a prompt and computing the probability of \"option content\". Specifically, we calculate the perplexity of each option content and then choose the label corresponding to the option content with the lowest perplexity as the predicted answer. This evaluation method is consistent with the training method of pre-trained LLMs (i.e., next token prediction), stimulating the optimal performance of LLMs.",
            "score": 0.3983100584625454,
            "section_title": "A multi-turn dialogue generated by Speaker1 and Speaker2",
            "char_start_offset": 14488,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1641
                },
                {
                    "start": 1644,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2302
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.132568359375
        },
        {
            "corpus_id": "273821559",
            "title": "Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models",
            "text": "CoBBLEr Recent studies have found that LLM-as-a-Judge often exhibits cognitive biases, such as preferences for verbosity, egocentrism, bandwagon, and an overly authoritative tone (Wang et al., 2024a;Koo et al., 2024;Zheng et al., 2024;Chen et al., 2024). To investigate the biases of the compared models, we evaluate them on the CoB-BLEr benchmark (Cognitive Bias Benchmark for LLMs as EvaluatoRs) (Koo et al., 2024). This dataset is designed to evaluate the quality and reliability of LLMs when used as automated evaluators in a question-answering (QA) setting. \n\nIt assesses the presence of six cognitive biases, both implicit and induced, when LLMs are tasked with ranking responses generated by various other models. CoBBLEr's core objective is to identify the extent of bias in LLM evaluation outputs. \n\nThe CoBBLEr dataset includes 50 QA instructions, randomly selected from two well-established benchmarks: BIGbench (bench authors, 2023) and ELI5 (Fan et al., 2019). 16 LLMs, both open and closed-source models, generate responses to these instructions. The evaluations involve pairwise comparisons between the responses of two models, wherein each model also acts as an evaluator to rank its own and others' outputs. The biases tested are categorized into two groups: (1) Implicit biases, such as egocentric bias (where a model tends to prefer its own outputs), and \n\n(2) Induced biases, such as order bias, where the ranking of responses is influenced by their order in the evaluation. \n\nTable 6 presents the performance of the compared models on the CoBBLEr benchmark with metrics of order bias, bandwagon effect, compassion, selective bias, salience, distraction, and frequency. Note that lower scores indicates better performance (i.e., fewer biases) on CoBBLEr. Overall, our REC-70B model has the best average performance with a score of 0.2141, followed closely by GPT-4 (0.2279) and GPT-4o (0.2349).",
            "score": 0.398299252117944,
            "section_title": "Bias Testing",
            "char_start_offset": 22776,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1494
                },
                {
                    "start": 1497,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 199,
                    "matchedPaperCorpusId": "258960339"
                },
                {
                    "start": 199,
                    "end": 216,
                    "matchedPaperCorpusId": "263310448"
                },
                {
                    "start": 216,
                    "end": 235,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 398,
                    "end": 416,
                    "matchedPaperCorpusId": "263310448"
                },
                {
                    "start": 954,
                    "end": 972,
                    "matchedPaperCorpusId": "196170479"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2430419921875
        },
        {
            "corpus_id": "267770547",
            "title": "RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models",
            "text": "We evaluate the acceptance of the feedback information by querying GPT-4 and the results are shown in Table 2. We first annotate 100 data, randomly selected from QA task, to manifest the effectiveness of annotating with GPT-4. The Cohen Kappa \u03ba between GPT-4 and human annotation equals 0.59, indicating a medium to high correlation, and the accuracy is 0.80 if we regard the human labels as golden answers. These verify that GPT-4 can serve as a surrogate to evaluate the performance without excessive annotation cost. More details about human annotation can be found in Appendix. \n\nGenerally, Claude-2 and GPT-4 achieve the most significant FA values compared with other evaluated LLMs, which indicates that they are the most open to the feedback and are less stubborn to their own knowledge. In QA, ChatGPT and Mistral-7B-Instruct-v0.2 perform the weakest (6.5% and 8.0%) and tend to deny the feedback information since they believe that the feedback information is inaccurate. As for MT and Email Writing, ALPACA-7B is not fine-tuned in multi-lingual data resources and achieves the lowest FA (6.8%). The FA values of all the LLMs in Writing exceed those in QA and MT a large margin, which indicates that LLMs are less stubborn for feedback with email writings.",
            "score": 0.39811564949426326,
            "section_title": "Feedback Acceptance",
            "char_start_offset": 19537,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1265
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.141845703125
        },
        {
            "corpus_id": "258960319",
            "title": "Detection of GPT-4 Generated Text in Higher Education: Combining Academic Judgement and Software to Identify Generative AI Tool Misuse",
            "text": "The release of the GPT-4 LLM in March 2023 marked a major step in the journey of AI models in coming closer to achieving a true Artificial General Intelligence (AGI) system which can in theory perform any task a human can (Bubeck et al., 2023) and is the underlying model behind ChatGPT Plus, a paid-for-service offered on a subscription basis by OpenAI, as well as Bing search (Microsoft, 2023). \n\nAs with previous versions of the GPT LLM, GPT-4 utilizes a transformer-based model with self-attention for natural language processing (NLP) (Vaswani et al., 2017), and autoregressive modelling for time-dependent data analysis (Brown et al., 2020). However, it can accept inputs up to 16 times larger than GPT-3.5, and can generate outputs of up to 24,000 words: more than eight times larger than GPT-3.5 (Koubaa, 2023)  The existing literature suggests a major challenge to maintaining academic integrity is the ongoing advancement in AI LLMs, given the inability to accurately differentiate between human and AI-generated content. While detection tools do offer a degree of promise, their effectiveness can vary considerably, particularly when the content has been significantly altered or paraphrased. Notably, the research presents a key gap: there are no studies evaluating the ability of academic staff to identify AI-generated content produced by modern LLMs-given the improvements in the GPT-4 LLM identified above, this means that GPT-4 based LLMs should be able to perform at an even higher level than earlier models when it comes to developing output that will evade detection by academic staff. Furthermore, no studies have ventured into examining the efficacy of AI detection tools under conditions that mirror real-world scenarios often encountered during student paper assessment. This gap is particularly problematic for academic integrity, as it questions the principle of fairness and equality inherent in assessment methods used in HEIs across the globe. This study aims to address this gap, exploring the areas yet untouched in the literature.",
            "score": 0.3979976633228638,
            "section_title": "Advances in AI LLMs: GPT-4",
            "char_start_offset": 18729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2062
                }
            ],
            "ref_mentions": [
                {
                    "start": 540,
                    "end": 562,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.451171875
        },
        {
            "corpus_id": "266153952",
            "title": "Harnessing Large Language Models in Medical Research and Scientific Writing: A Closer Look to The Future",
            "text": "Chatbot AI tools are artificial intelligence systems that can generate natural language responses based on text or image inputs. 19,34 They can potentially assist with various tasks in medical research and scientific writing, such as literature review, 35 data analysis, 36 draft generation, 37 summarization, 38 translation, 39 and proofreading. 27 However, they also pose challenges and risks, such as bias, 40 plagiarism, 41 inaccuracies, 42 and ethical issues. 43 Therefore, it is important to evaluate their performance and quality using appropriate methods and metrics. 22 ome possible methods that can be used to measure the performance and quality of LLMs for medical research and scientific writing are: \n\nThese methods have different strengths and limitations that need to be considered when evaluating LLMs for medical research and scientific writing. Depending on the specific goals and needs of the researchers or users, they may choose one or more methods that suit their situation. For example, they may use human evaluation for pilot studies or user satisfaction surveys; automatic evaluation for baseline comparisons or error analysis; or hybrid evaluation for comprehensive studies or quality assurance. By using appropriate methods to measure the performance and quality of LLMs, researchers, and users can ensure that these models are effective and beneficial for medical research and scientific writing.",
            "score": 0.39795220771390466,
            "section_title": "DIFFERENT METHODS CAN BE USED TO MEASURE THE PERFORMANCE AND QUALITY OF LLMS FOR MEDICAL RESEARCH AND SCIENTIFIC WRITING",
            "char_start_offset": 16151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1424
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "254876189"
                },
                {
                    "start": 132,
                    "end": 134,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 253,
                    "end": 255,
                    "matchedPaperCorpusId": "6405271"
                },
                {
                    "start": 271,
                    "end": 273,
                    "matchedPaperCorpusId": "44134226"
                },
                {
                    "start": 292,
                    "end": 294,
                    "matchedPaperCorpusId": "15442925"
                },
                {
                    "start": 326,
                    "end": 328,
                    "matchedPaperCorpusId": "52167799"
                },
                {
                    "start": 347,
                    "end": 349,
                    "matchedPaperCorpusId": "33285731"
                },
                {
                    "start": 465,
                    "end": 467,
                    "matchedPaperCorpusId": "3519414"
                },
                {
                    "start": 576,
                    "end": 578,
                    "matchedPaperCorpusId": "10438447"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6025390625
        },
        {
            "corpus_id": "269032822",
            "title": "From Model-centered to Human-Centered: Revision Distance as a Metric for Text Evaluation in LLMs-based Applications",
            "text": "This paper introduces a metric that leverages GPT-4, specifically applied to evaluating LLM-powered writing assistants.However, the computational and financial costs of using GPT-4 are considerable.Exploring the use of a smaller, specialized model to generate initial edits could reduce costs and improve efficiency.\n\nLLMs have a wide array of applications, and for this study, we have chosen the \"Related Work\" section generation task as a testbed for challenging writing scenarios.As a knowledge-intensive cognitive task, writing the \"Related Work\" section requires writers to integrate multi-source knowledge into the manuscript.Therefore, writing a comprehensive \"Related Work\" section is a laborintensive and time-consuming endeavor.Future studies could explore the application of our metric in longer text generation tasks, such as code generation and scientific reports, to validate its effectiveness and applicability across different domains.\n\nIn this study, each generated revision item is assigned equal weight.Future research should focus on developing a dynamic revision edit weighting method to evaluate textual differences more finely.",
            "score": 0.3978186067801772,
            "section_title": "Limitations",
            "char_start_offset": 11713,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 119,
                    "end": 198
                },
                {
                    "start": 198,
                    "end": 316
                },
                {
                    "start": 318,
                    "end": 483
                },
                {
                    "start": 483,
                    "end": 632
                },
                {
                    "start": 632,
                    "end": 738
                },
                {
                    "start": 738,
                    "end": 951
                },
                {
                    "start": 953,
                    "end": 1022
                },
                {
                    "start": 1022,
                    "end": 1150
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68603515625
        },
        {
            "corpus_id": "273374964",
            "title": "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation",
            "text": "To demonstrate that Auto-PRE effectively reduces the bias in single evaluator LLM (like GPT-4), we conducted the following experiments: Previous research (Chu et al., 2024;Zeng et al., 2023) has shown that one of the biases present in GPT-4 is that it may prefer answers generated by LLMs in the GPT series, regardless of the actual quality of the answers. To clearly demonstrate the negative impact of this bias on experimental outcomes, we used GPT-3.5-turbo and ChatGLM2-6B as two LLMs generating the answers to be tested and conducted experiments on the Xsum and NF_CATS datasets in pairwise evaluation format. rate method = p method \u2212 p human p human \u00d7 100% (1) \n\nTable 4 presents the experimental results. Rate is calculated using Equation 1, where p denotes the proportion of evaluation results that favor GPT-3.5-turbo. The rate value refers to the percentage change in the method's preference for GPT-3.5turbo compared to human preference. It is noteworthy that the rate does not necessarily correlate with accuracy, as it does not consider the order of answers. However, it can, to some extent, reflect a method's preference bias for specific answers. \n\nFrom the results, we can observe a significant performance gap between GPT-4 and Auto-PRE, with GPT-4's rate much higher than that of other methods. This suggests that GPT-4's preference for answers from GPT-3.5-turbo compromises its performance and reliability. Conversely, Auto-PRE enhances overall performance and reliability by collaborating with various types of LLMs.",
            "score": 0.39746640271373646,
            "section_title": "Bias Analysis",
            "char_start_offset": 21326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 666
                },
                {
                    "start": 669,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1537
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5966796875
        },
        {
            "corpus_id": "266348350",
            "title": "Next-Step Hint Generation for Introductory Programming Using Large Language Models",
            "text": "A known issue with doing LLM-studies is that models are constantly updated, which might produce other results. During our experiments, OpenAI released GPT-4. We performed some minor experiments using GPT-4 and compared its results with GPT-3.5-turbo. We found that the same prompt for both models outputs, at first glance, different results. GPT-4 created hints with good suggestions we had not seen before with our best prompts. We expect our prompt practices also work for updated versions, but other research is required to confirm this.",
            "score": 0.397071345202361,
            "section_title": "Reproducibility.",
            "char_start_offset": 40040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 540
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09808349609375
        },
        {
            "corpus_id": "258460621",
            "title": "Distinguishing GPT-4-generated Radiology Abstracts from Original Abstracts: Performance of Blinded Human Observers and AI Content Detector",
            "text": "Objective: To determine GPT-4's effectiveness in writing scientific radiology article abstracts and investigate human reviewers' and AI Content detectors' success in distinguishing these abstracts. Additionally, to determine the similarity scores of abstracts generated by GPT-4 to better understand its ability to create unique text. Methods: The study collected 250 original articles published between 2021 and 2023 in five radiology journals. The articles were randomly selected, and their abstracts were generated by GPT-4 using a specific prompt. Three experienced academic radiologists independently evaluated the GPT-4 generated and original abstracts to distinguish them as original or generated by GPT-4. All abstracts were also uploaded to an AI Content Detector and plagiarism detector to calculate similarity scores. Statistical analysis was performed to determine discrimination performance and similarity scores. Results: Out of 134 GPT-4 generated abstracts, average of 75 (56%) were detected by reviewers, and average of 50 (43%) original abstracts were falsely categorized as GPT-4 generated abstracts by reviewers. The sensitivity, specificity, accuracy, PPV, and NPV of observers in distinguishing GPT-4 written abstracts ranged from 51.5% to 55.6%, 56.1% to 70%, 54.8% to 60.8%, 41.2% to 76.7%, and 47% to 62.7%, respectively. No significant difference was observed between observers in discrimination performance. Conclusion: GPT-4 can generate convincing scientific radiology article abstracts. However, human reviewers and AI Content detectors have difficulty in distinguishing GPT-4 generated abstracts from original ones.",
            "score": 0.39647233178800956,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70361328125
        },
        {
            "corpus_id": "278166184",
            "title": "Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages",
            "text": "GPT-4o on Catalan, however, is an outlier, achieving the second-best score among all language-model pairs. Nevertheless, because GPT-4o was chosen as the translation and paraphrasing model according to the results reported in Section 4, its output may provide GPT-4o with a slight advantage in the bias-detection task. Further work is required to evaluate this potential effect. Given the variance observed in Figure 4 across different bias categories, it is also evident that choosing an LLM may require a case-by-case approach. Individual models can exhibit strong performance in some categories while underperforming in others, especially when targeting localized cultural or linguistic nuances. Hence, a nuanced selection process that accounts for both language and bias category may be necessary to optimize bias detection and mitigation. \n\nIn conclusion, and in direct response to RQ2, these findings suggest that LLMs exhibit higher social biases when data augmentation is performed for low-resource languages. Nonetheless, the particular model best suited for each task may vary depending on the specific bias category and language under consideration.",
            "score": 0.39635775152977215,
            "section_title": "Performance Evaluation",
            "char_start_offset": 32026,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1160
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2076416015625
        },
        {
            "corpus_id": "267897885",
            "title": "AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation",
            "text": "Fig. 4. Identifying biases in LLM-generated ideas. (a) introduces the top terms used in all ideas generated either by humans or by GPT-3, as calculated using the Latent Personal Analysis (LPA) method. (b) depicts GPT-3's LPA signature, denoting its unique use of terms when compared to the shared vocabulary, either underused or overused. \n\n5.2.5 Summary of findings for RQ1. After the session, 50% of students perceived GPT-3 as helpful because it provided a unique or expanded perspective on the problem statement and its possible solutions. 44% shared that it significantly assisted them in generating new ideas. At the end of the semester, 50% of the students mentioned that GPT-3 contributed to reshaping and enhancing their project by elaborating on their concepts, proposing new characteristics, and tackling particular challenges. 31% of students pointed out that GPT-3 tends to be redundant and lacked creativity. \n\nThe ideas chosen by each group for their final project were mostly created by combining an idea generated by team members and an idea suggested or enhanced by the LLM. In one case (Team 2), the chosen idea was directly inspired by an idea generated by GPT-3. \n\nFig. 5. The Distribution of ratings on a 1 to 5 Likert scale given to ideas generated in the Brainwriting process. Ideas were generated by either humans, GPT-3, or as a collaboration. Every idea was assessed based on three criteria: its relevance, depth of insight, and level of innovation. All 148 ideas were rated by Experts, Novices, and the GPT-4 rating engine. The lower panel depicts the distribution of ratings given by Experts to ideas in each of the criteria. The middle panel depicts ratings given by Novices, and the upper panel the rates given by GPT-4. \n\nhypothesis of a normal distribution is rejected with a p-value \u00ab 0.001 for the ratings of all three criteria: Relevance, Innovation, and Insightfulness. Similarly, the Shapiro-Wilk Test on the raw rating distribution of Novice evaluations found that the null hypothesis of a normal distribution is rejected with a p-value of \u00ab 0.01 for all three criteria.",
            "score": 0.39634574276693335,
            "section_title": "Prompt analysis.",
            "char_start_offset": 44419,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 338
                },
                {
                    "start": 341,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 922
                },
                {
                    "start": 925,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1183
                },
                {
                    "start": 1186,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1751
                },
                {
                    "start": 1754,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2109
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.180419921875
        },
        {
            "corpus_id": "259376840",
            "title": "Rating Short L2 Essays on the CEFR Scale with GPT-4",
            "text": "Essay scoring is a critical task used to evaluate second-language (L2) writing proficiency on high-stakes language assessments. While automated scoring approaches are mature and have been around for decades, human scoring is still considered the gold standard, despite its high costs and well-known issues such as human rater fatigue and bias. The recent introduction of large language models (LLMs) brings new opportunities for automated scoring. In this paper, we evaluate how well GPT-3.5 and GPT-4 can rate short essay responses written by L2 English learners on a high-stakes language assessment, computing inter-rater agreement with human ratings. Results show that when calibration examples are provided, GPT-4 can perform almost as well as modern Automatic Writing Evaluation (AWE) methods, but agreement with human ratings can vary depending on the test-taker\u2019s first language (L1).",
            "score": 0.39609103353902003,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4921875
        },
        {
            "corpus_id": "272223451",
            "title": "The transformative impact of large language models on medical writing and publishing: current applications, challenges and future directions",
            "text": "LLMs have been extensively applied in various aspects of writing assistance, particularly in abstract generation (Table 1). ChatGPT-3.5 demonstrated the ability to generate high-quality abstracts from clinical trial keywords and data tables, showcasing impressive accuracy with minor errors [24]. However, its performance varied significantly when tasked with writing abstracts on broad, well-documented topics compared to more specific, recently published subjects [25]. The low plagiarism scores and difficult detection of AI-generated abstracts and the ethical boundaries of using such technology in academic writing have also been discussed [19]. Although ChatGPT-3.5 could generate abstracts that were challenging to distinguish from human-written ones in the arthroplasty field, the quality was notably better in those written by humans [26]. Using both ChatGPT-3.5 and ChatGPT-4 to write abstracts for randomized controlled trials revealed that, despite their potential, the quality was not satisfactory, highlighting the need for further development and refinement in generative AI tools [27]. \n\nIn addition to abstract generation, LLMs have been used to assist in various other writing tasks. For example, GPT-4 was used to generate introduction sections for randomized controlled trials, with non-inferiority confirmed and higher readability scores compared to human-written introductions [28]. ChatGPT was also used to write medical case reports [29] and to write a clinical summary containing patient situation, case evaluation and appropriate interventions [30]. In a study regarding human reproduction, ChatGPT could produce high-quality text and efficiently summarize information, but its ability to interpret data and answer scientific questions was limited [31]. \n\nLLMs have been employed to generate cover letters for abstracts, with non-inferiority confirmed by randomized trials and higher readability scores [32]. These tools have also been used to facilitate language learning and improve technical writing skills for non-native English speakers, which is particularly meaningful for scholars using English as a non-primary language [33]. However, it is important to note that the effectiveness of these tools may vary, as one study found that the free version of ChatGPT-3.5 was not an effective writing coach [34].",
            "score": 0.3947036202072795,
            "section_title": "Writing assistance and quality assessment",
            "char_start_offset": 7220,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2338
                }
            ],
            "ref_mentions": [
                {
                    "start": 291,
                    "end": 295,
                    "matchedPaperCorpusId": "258508171"
                },
                {
                    "start": 645,
                    "end": 649,
                    "matchedPaperCorpusId": "258354722"
                },
                {
                    "start": 843,
                    "end": 847,
                    "matchedPaperCorpusId": "267633428"
                },
                {
                    "start": 1096,
                    "end": 1100,
                    "matchedPaperCorpusId": "267679187"
                },
                {
                    "start": 1399,
                    "end": 1403,
                    "matchedPaperCorpusId": "265331105"
                },
                {
                    "start": 1457,
                    "end": 1461,
                    "matchedPaperCorpusId": "258874036"
                },
                {
                    "start": 1570,
                    "end": 1574,
                    "matchedPaperCorpusId": "258157664"
                },
                {
                    "start": 1929,
                    "end": 1933,
                    "matchedPaperCorpusId": "265497952"
                },
                {
                    "start": 2155,
                    "end": 2159,
                    "matchedPaperCorpusId": "266330607"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58203125
        },
        {
            "corpus_id": "258960339",
            "title": "Large Language Models are not Fair Evaluators",
            "text": "Figure 1: Simply changing the order of candidate responses leads to overturned comparison results, even though we add the command \"ensuring that the order in which the responses were presented does not affect your judgment\" into the prompt. \n\nBARTScore (Yuan, Neubig, and Liu, 2021), are insufficient for thoroughly assessing this alignment (He et al., 2023). While human evaluation provides the most accurate measure of model performance and valuable insights, it can often be costly and time-consuming. As a result, there is a growing demand for automated assessment methods that can consistently align with human judgments while being more efficient and cost-effective. \n\nChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) have recently demonstrated remarkable performance across various tasks, leading to their widespread use as both the annotators (Peng et al., 2023;Xu et al., 2023) and evaluators (Zheng et al., 2023;Peng et al., 2023;Sun et al., 2023;Zhou et al., 2023;Gao et al., 2023;Wang et al., 2023b;Dubois et al., 2023;Wang et al., 2023a). For example, The evaluation pipeline of Vicuna (Zheng et al., 2023) has gained significant interest and wide usage due to its simplicity and interpretability. It prompts GPT-4 to score and compare candidate responses and provide explanations, making it a valuable tool for evaluation. However, it is unclear how reliable LLMs are as evaluators, as they are known to be sensitive to textual instructions and inputs (Dong et al., 2022;Turpin et al., 2023;Bowman, 2023). This raises questions about the resilience of this paradigm against perturbations, such as the ordering of candidates during scoring, potentially becoming the Achilles' Heel that can be easily hacked for unreliable evaluations. \n\nIn this paper, we take a sober look at the LLMsas-evaluator paradigm and uncover a significant positional bias. Specifically, we demonstrate that GPT-4 exhibits a preference for the first displayed candidate response by consistently assigning it higher scores, even when the order of candidates is subtly altered.",
            "score": 0.3944058019019605,
            "section_title": "GPT-4",
            "char_start_offset": 445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1746
                },
                {
                    "start": 1749,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2062
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 282,
                    "matchedPaperCorpusId": "235593404"
                },
                {
                    "start": 341,
                    "end": 358,
                    "matchedPaperCorpusId": "254877323"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.298828125
        },
        {
            "corpus_id": "267351097",
            "title": "Large language models and academic writing: Five tiers of engagement",
            "text": "While not part of the writing process, and thus adjacent to the proposed tiers, other academic tools draw on new advances in LLM or other recent machine-learning technologies. The ethical and intellectual considerations of their use overlap with those of LLMs, as do the efficiencies they offer researchers and writers. Large language models and academic writing Page 4 of 5 \n\nor even create a literature review 'at the touch of a button'. Such tools can provide excellent assistance in exploring new fields of research or fields related to one's own work (see Jansen et al. 13 for a discussion of areas in which LLMs might support survey-based research), but are not substitutes for the process of critically reading to inform and order one's own intuitions and conclusions, gradually bringing new ideas into relation with one's own thought-scape. Moreover, such models may play a role in reifying conventional wisdoms, and in so doing, drown out marginal voices (the latter which may also be thought of as 'majority voices', considering that most people, including academics, are non-Western, non-white, and non-anglophone, despite the outsized influence of Western universities on the global scientific community). \n\nThe second non-writing LLM-based tool is 'automatic data analysis' (e.g. Langchain), whereby data sets can be loaded up to an LLM for statistical analysis. In one dimension, the use of such technologies is equivalent to that of a pocket calculator: a logical time-saver, provided the user understands what the LLM is doing. For example, for at least the past three decades, scientists have routinely used multiple regressions, typically executed by statistical software or a coding routine in a software library. Use of statistical software (not least the writing up of results) requires a basic knowledge of statistics and data analysis. Subject to new developments, danger enters when the process of statistical analysis is not understood by the scientist, but regarded, crudely, as magic (i.e. it cannot be explained). In all, the use of LLM interfaces for statistical analysis will likely become commonplace, to the benefit of science in general, with the qualification that scientists will still be required to understand at least 'the bare bones' of statistical analysis.",
            "score": 0.3940675080510404,
            "section_title": "other AI tools available to the would-be academic author",
            "char_start_offset": 16076,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 575,
                    "end": 577,
                    "matchedPaperCorpusId": "259491872"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1221923828125
        },
        {
            "corpus_id": "267750680",
            "title": "Understanding the Therapeutic Relationship between Counselors and Clients in Online Text-based Counseling using LLMs",
            "text": "Model Self-Consistency. The reliability of a model as an annotator depends on its consistency in multiple independent evaluations of the same samples. We evaluate all these models by assessing their consistency across all the experimental settings. The results of models' self-consistency are shown in Table 3, and detailed results can be found in Table 6 in the Appendix. We find that ChatGPT falls short of reaching a moderate level of self-agreement without detailed guidelines and CoT. However, GLM-4, Claude-3, and GPT-4 maintain a moderate or higher level of self-consistency, ensuring the validity of their annotated results. Therefore, we further analyze the influence of guidelines and CoT on the alignment between these latter three models and human evaluations in the following. \n\nAlignment with Human Evaluations. The models' capability on understanding the working alliance is defined as the extent to which its assessments align with those of human experts. Table 3 summarizes the Pearson correlation coefficients (Lee Rodgers and Nicewander, 1988)   tween LLMs and human evaluations across different experimental settings. Results indicate that GPT-4, accompanied by detailed guidelines and CoT, exhibits the best average performance over the three aspects compared to alternative models and experimental setups. \n\nGuidelines. As shown in Table 3, the overall results indicate a trend wherein an increase in the level of detail in guidelines enhances alignment. This improvement is particularly significant when transitioning from general guidelines to more detailed ones, resulting in a notable average increase in correlation of 23.61%. Detailed guidelines are particularly effective in enhancing LLMs' performance on challenging questions. For instance, in the case of discerning whether counselors and clients like each other (Q9), GPT-4 performs poorly without guidelines or with general guidelines. However, when detailed guidelines are provided, there is a remarkable 76% increase in correlation (Detailed results can be found in Table 7 in the Appendix). \n\nThese findings highlight the potential to improve the alignment of LLM evaluations with human assessments by refining the guidelines. Simultaneously, we emphasize the significance of selfconsistency in LLMs, which is a crucial prerequisite for their effectiveness as evaluators. \n\nChain-of-Thought Prompting. Table 3 demonstrates that integrating CoT improves the alignment of LLM evaluations with human assessments across overall dimensions.",
            "score": 0.39388306966681874,
            "section_title": "Results and Analysis",
            "char_start_offset": 21488,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 789
                },
                {
                    "start": 792,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1327
                },
                {
                    "start": 1330,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2077
                },
                {
                    "start": 2080,
                    "end": 2213
                },
                {
                    "start": 2214,
                    "end": 2358
                },
                {
                    "start": 2361,
                    "end": 2388
                },
                {
                    "start": 2389,
                    "end": 2522
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.044342041015625
        },
        {
            "corpus_id": "272678278",
            "title": "Evaluating GPT and BERT models for protein\u2013protein interaction identification in biomedical text",
            "text": "On 14 March 2023, OpenAI introduced its most advanced and cutting-edge system, GPT-4, which has surpassed its predecessors by producing more dependable outcomes. GPT-4 (OpenAI 2023) is now a multi-modal system capable of processing not only text but also images and voice inputs, enhancing its ability to engage in various creative and technical writing tasks, such as songwriting, screenplay creation, and imitating user writing styles (Cheng et al. 2023). The advancements in GPT models, from GPT-3 to GPT-4, showcase the rapid progress in NLP, opening up a wide range of applications (Baktash andDawodi 2023, Microsoft Research AI4Science andMicrosoft Azure Quantum 2023). \n\nSeveral studies (Katz et al. 2024, Nori et al. 2023) have evaluated the performance of GPT models for problemsolving on various standardized tests. It has been shown that they can achieve performance comparable to or even better than humans (Webb et al. 2023). They could pass high-level professional standardized tests such as the Bar test (Kojima et al. 2022), the Chinese Medical Practitioners examination (Wang et al. 2023), and the Japanese National Nurse examination (Taira et al. 2023). Another study (Gilson et al. 2023) evaluates that ChatGPT achieves the equivalent of a passing score for a third-year medical student in the United States Medical Licensing Examination. A group of researchers explored the performance of advanced LLMs, including ChatGPT, GPT-4, and Google Bard, in mastering complex neurosurgery examination material and found that GPT-4 achieved a score of 82.6%, outperforming ChatGPT and Google Bard. A recent study assessed the efficacy of GPT for specific standardized admissions tests in the United Kingdom. These findings revealed that GPTs demonstrate greater proficiency in tasks requiring language comprehension and processing and yet exhibit limitations in applications involving scientific and mathematical knowledge (Giannos and Delardas 2023). To the best of our knowledge, no study has evaluated the effectiveness of GPT models, specifically in extracting PPIs from biomedical texts.",
            "score": 0.393688270160701,
            "section_title": "Introduction",
            "char_start_offset": 5652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2103
                }
            ],
            "ref_mentions": [
                {
                    "start": 437,
                    "end": 456,
                    "matchedPaperCorpusId": "258375195"
                },
                {
                    "start": 587,
                    "end": 599,
                    "matchedPaperCorpusId": "258546764"
                },
                {
                    "start": 694,
                    "end": 711,
                    "matchedPaperCorpusId": "257572753"
                },
                {
                    "start": 919,
                    "end": 937,
                    "matchedPaperCorpusId": "254854575"
                },
                {
                    "start": 1019,
                    "end": 1039,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 1087,
                    "end": 1105,
                    "matchedPaperCorpusId": "260670145"
                },
                {
                    "start": 1151,
                    "end": 1170,
                    "matchedPaperCorpusId": "259260662"
                },
                {
                    "start": 1186,
                    "end": 1206,
                    "matchedPaperCorpusId": "259842568"
                },
                {
                    "start": 1934,
                    "end": 1961,
                    "matchedPaperCorpusId": "258332468"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1207275390625
        },
        {
            "corpus_id": "276781981",
            "title": "Adapting to Educate: Conversational AI's Role in Mathematics Education Across Different Educational Contexts",
            "text": "Liu and Sun [28] employed GPT-4 to code transcripts of 24 interviews with educational stakeholders, using a developed codebook of 28 codes nested within 8 broad educational aspects. Their findings demonstrated that compared to Latent Dirichlet Allocation (LDA), LLMs were more capable of understanding themes within the text and more closely aligned with human qualitative researchers' results, achieving a hit rate of 74% at the detailed code level and 96% at broad code levels. These results underscored LLMs' ability for consistent judgment and potential to supplement human interpretations. Their findings are consistent with other studies utilizing LLMs for deductive coding [44,49], where LLM's consistent inference and reasoning, combined with appropriate human supervision, potentially enhance the construct validity and reliability of coding results [48]. \n\nIn addition to qualitative analysis, studies (e.g., [9,42]) have shown that LLMs analyze input based on criteria. A comparison of GPT-3.5's evaluation results with humans using the same criteria for the International English Language Testing System (IELTS) academic writing confirmed that LLMs can deliver reliable and criterionbased grading [9]. However, the quality of rubrics for evaluation is crucial. These studies highlight the importance of developing precise rubrics for LLM applications, key to achieving good inter-rater consistency with benchmarks and demonstrating excellent test-retest reliability, thereby systematically assessing the quality and relevance of content. The creation of clear and specific rubrics tailored for educational contexts is essential for enhancing the accuracy and effectiveness of automated content analysis, ensuring consistent and objective analysis, and providing a robust framework for interpreting complex educational data.",
            "score": 0.3935954766029478,
            "section_title": "Automated Content Analysis and Labeling",
            "char_start_offset": 8945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1835
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.340087890625
        },
        {
            "corpus_id": "272625010",
            "title": "Comparison of generative AI performance on undergraduate and postgraduate written assessments in the biomedical sciences",
            "text": "In response to example essay questions, AI generated answers were anonymously evaluated with reference to the level descriptions from the Frameworks for Higher Education Qualifications in the UK (QAA, 2014), providing evaluations of generative AI output across four levels of undergraduate and postgraduate education. The performance of three commonly used AI tools were investigated, ChatGPT 3.5, Google Bard and Microsoft Bing. Scientific accuracy, mechanistic detail, deviation, context and coherence were evaluated by four independent markers, inter-rater reliability calculated and written instructor comments provided on each output. The findings provide insights into the effectiveness of LLMs in academic writing assignments, with broad implications for the potential use of these tools in student assessments in the biomedical sciences.",
            "score": 0.3935655524182743,
            "section_title": "body",
            "char_start_offset": 3969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 845
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5341796875
        },
        {
            "corpus_id": "271126365",
            "title": "Harnessing LLMs for multi-dimensional writing assessment: Reliability and alignment with human judgments",
            "text": "To analyze the efficacy of LLM in scoring across various rating dimensions, we contrasted the performance of GPT-4 across different prompt strategies with human scoring benchmarks, yielding the results depicted in the subsequent Fig. 11. With more specific and sophisticated prompt strategies, the scoring accuracy of GPT-4 increased steadily, showing that prompt engineering is of vital importance for multi-dimensional writing assessment and instruction. \n\nAs shown in Table 4, for the rating dimension of Ideas, GPT-4 attained its highest QWK coefficient of 0.551 at the Criteria & Sample-Referenced Justification prompt, closely approximating the inter-rater reliability observed among human raters, which stands at 0.605. Pertaining to the rating dimension of Organization, GPT-4 demonstrated optimal scoring effectiveness with a QWK of 0.584 under the same prompt, exceeding the inter-rater reliability of 0.541. This accentuates the model's capability effectively discern the structural components of essays when augmented with insights derived from human raters. \n\nAs for the rating dimension of Style, alignment with human evaluative judgments was most pronounced when applying the Criteria & Sample-Referenced Justification prompt, achieving a QWK score of 0.474, albeit not reaching the inter-rater reliability of 0.565. This indicates that while the model exhibits a capacity to recognize stylistic elements of text when informed by annotated exemplars from human raters, there is a discernible margin for enhancement. Assessing Conventions posed a considerable challenge for GPT-4, with the most specific and sophisticated prompt, Criteria & Sample-Referenced Justification, attaining a QWK of merely 0.216, which is substantially below the human inter-rater reliability of 0.498. \n\nThe observed disparities in GPT-4's scoring proficiency across different dimensions may be ascribed to a multitude of factors. Dimensions that necessitate a more profound comprehension of content and context, such as Ideas and Organization, are advantaged by prompts that incorporate criteria and exemplars crafted by human evaluators.",
            "score": 0.39332605052069014,
            "section_title": "Experiment 3: analysis of LLM effectiveness across scoring dimensions",
            "char_start_offset": 38653,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1793
                },
                {
                    "start": 1796,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2131
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7041015625
        },
        {
            "corpus_id": "270045904",
            "title": "An Evaluation of Estimative Uncertainty in Large Language Models",
            "text": "baseline findings.For example, we consider whether adding a gendered role to the prompt that is presented to an LLM affects any of the conclusions.In a similar vein, we also investigate changes in effect strength and direction both when an LLM like GPT-4 (that understands both English and Chinese) is prompted using Chinese, as well as when the LLM (such as ERNIE-4 25 ) is trained primarily using Chinese text.The latter experiment is motivated by the fact that LLMs are increasingly being used for multi-lingual tasks like machine translation. 26,27 o investigate these effects systematically, we construct datasets involving different scenarios, controls, and WEPs.A suite of LLMs (GPT-3.5-Turbo 17, GPT-4 18 , Llama-series models e.g., Llama-2-7B 16 , Llama-2-13B 16 and ERNIE-4 25 ) are examined.\n\nOur second objective considers an issue that is especially important for communicating scientific information in everyday language.In scientific discourse, statistical uncertainty tends to dominate when presenting findings, especially in formal publications.In contrast, estimative uncertainty is the norm for communicating uncertainty in everyday natural language.LLMs are starting to be used increasingly often in science, including for summarizing scientific findings. 28,29 herefore, for a specific high-performing LLM (GPT-4), we consider whether different levels of statistical uncertainty in the prompt, appropriately controlled, lead to consistent changes (or lack thereof) in the LLM's elicitation of estimative uncertainty.Because formal evaluation of such consistency in AI systems has not been explored thus far in the literature, we propose and formalize four novel consistency metrics (see Methods for details) for evaluating the extent to which an LLM like GPT-4 is able to change its level of estimative uncertainty when prompted with changing levels of statistical uncertainty.\n\nExperimentally, we find that most LLMs, especially the GPT models, closely align with human uncertainty estimates in contexts of extreme or balanced certainty, and reflect a high degree of sophistication in interpreting WEPs that express positive certainty.",
            "score": 0.3932443805735365,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 18,
                    "end": 147
                },
                {
                    "start": 147,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 553
                },
                {
                    "start": 553,
                    "end": 669
                },
                {
                    "start": 669,
                    "end": 702
                },
                {
                    "start": 702,
                    "end": 802
                },
                {
                    "start": 804,
                    "end": 935
                },
                {
                    "start": 935,
                    "end": 1062
                },
                {
                    "start": 1062,
                    "end": 1169
                },
                {
                    "start": 1169,
                    "end": 1282
                },
                {
                    "start": 1282,
                    "end": 1537
                },
                {
                    "start": 1537,
                    "end": 1898
                },
                {
                    "start": 1900,
                    "end": 2157
                }
            ],
            "ref_mentions": [
                {
                    "start": 547,
                    "end": 550,
                    "matchedPaperCorpusId": "255942578"
                },
                {
                    "start": 550,
                    "end": 552,
                    "matchedPaperCorpusId": "265607864"
                },
                {
                    "start": 1276,
                    "end": 1279,
                    "matchedPaperCorpusId": "268752657"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1966552734375
        },
        {
            "corpus_id": "270218703",
            "title": "Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization",
            "text": "Refinement can be powered by various LLMs.In this paper, we choose the newest versions of GPT-3.5 (gpt-3.5-turbo-0125)and GPT-4 (gpt-4-0125-preview) models from OpenAI4 to draft, critique, and refine the outcomes due to their strong instruction-following capabilities.We also explore the performance of a strong open-source LLM (Mixtral 8\u00d77B (Jiang et al., 2024)).\n\nWe use the LLMCompare as our evaluation protocol, which compares two candidate outputs and then selects the better one (Zheng et al., 2023;Wang et al., 2023).This is because LLMCompare coupled with GPT-4 is the best evaluation protocol, as mentioned in Liu et al. (2023).The evaluation prompts are shown in Appendix A.\n\nWe evaluate the generated summaries from the three quality dimensions as introduced in Liu et al. ( 2023): (1) Overall Quality measures the overall excellence of the summary following the summary requirements.(2) Missing Information assesses whether the summary omits any essential article details pertinent to the summary requirements.\n\n(3) Irrelevant Information examines whether the summary contains extraneous information that falls outside the scope of the summary requirements.",
            "score": 0.3932049090103612,
            "section_title": "Models and Metrics",
            "char_start_offset": 5615,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 42,
                    "end": 118
                },
                {
                    "start": 118,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 364
                },
                {
                    "start": 366,
                    "end": 524
                },
                {
                    "start": 524,
                    "end": 637
                },
                {
                    "start": 637,
                    "end": 684
                },
                {
                    "start": 686,
                    "end": 895
                },
                {
                    "start": 895,
                    "end": 1022
                },
                {
                    "start": 1024,
                    "end": 1169
                }
            ],
            "ref_mentions": [
                {
                    "start": 485,
                    "end": 505,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5498046875
        },
        {
            "corpus_id": "269484364",
            "title": "MetaRM: Shifted Distributions Alignment via Meta-Learning",
            "text": "To evaluate the effectiveness of our method, we assess it by comparing its win rate with other baselines.Specifically, we randomly select 100 prompts from the test datasets and generate the responses from our method and baselines, respectively.We then provide these pairs of prompts and responses to human evaluators, asking them to determine which response is of higher quality, more useful, and harmless.During the entire evaluation process, the human evaluators are unaware of the responses' sources.Additionally, some studies indicate that GPT-4's evaluation of the responses aligns closely with that of human evaluators (Chang et al., 2023;Zheng et al., 2023a).Meanwhile, GPT-4 is noted for being more cost-effective and efficient compared to human evaluators, while also offering greater consistency in evaluation results (Zheng et al., 2023c).So we also utilize GPT-4 to evaluate the performance of MetaRM against other baselines.To mitigate the impact of irrelevant bias on GPT-4 evaluations such as response length and position, we randomly assign the order of the responses in GPT-4 evaluation prompts.The GPT-4 prompts for evaluation can be found in Appendix A.2.This indicates that our method can maintain the RM's ability to modeling human preferences in the gradient descent, while making it adapt to the new distribution by using the meta-process.",
            "score": 0.3931110907647225,
            "section_title": "Metrics & Evaluation",
            "char_start_offset": 17254,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 105,
                    "end": 244
                },
                {
                    "start": 244,
                    "end": 406
                },
                {
                    "start": 406,
                    "end": 503
                },
                {
                    "start": 503,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 937
                },
                {
                    "start": 937,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1174
                },
                {
                    "start": 1174,
                    "end": 1362
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14013671875
        },
        {
            "corpus_id": "271946793",
            "title": "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
            "text": "To facilitate selecting appropriate LLM judges for each LLM-alignment dataset (i.e. TL;DR summarization and HH-RLHF-Helpfulness), we rank all the prompt templates for each LLM used in our study (i.e. GPT-3.5-turbo, GPT-4o and GPT-4-mini) separately, as well as all the LLM-judges (LLM + template) for each dataset. We display top five templates or LLM-judges and report their Acc both , Acc random , position bias and length bias (   Table 6: Self-consistent rate (SCR) and accuracy (Acc) related to the tested temperatures for the TL;DR summarization and HH-RLHF-Helpfulness datasets. Results are demonstrated using GPT-3.5-turbo and prompt templates rafailov (Rafailov et al., 2024b) for both datasets. GPT-3.5-turbo is much more sensitive to temperatures compared with GPT-4o and GPT-4o-mini. \n\n\u2022 While position and length biases are critical metrics for assessing the reliability of LLMbased judges, accuracy is the metric that directly reflects their alignment with human preferences. Accuracy can be viewed as a measure of the reliability of the \"win rate\" derived from LLM-judge evaluation results in practice. \u2022 In the primary study, our findings indicate that Acc both more accurately represents the evaluative capabilities of LLM judges compared to Acc random . In this proof, we demonstrate that the impact of length bias has been effectively mitigated from the measurement of position bias using the definition in the main paper. \n\nTo prove this, we analyze two separate conditions: (1) the LLM judge prefers the first position, (2) the LLM judge prefers the second position. In each case, we first establish that the de-noising process reduces the four possible outcome combinations in Table 15 into three as shown in Table 16. Subsequently, we demonstrate that the measurement of position bias, utilizing de-noised accuracy, effectively mitigates the length bias.",
            "score": 0.3930745688342986,
            "section_title": "Rankings of Prompt Templates and LLM Judges",
            "char_start_offset": 33527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1877
                }
            ],
            "ref_mentions": [
                {
                    "start": 661,
                    "end": 685,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08868408203125
        },
        {
            "corpus_id": "274234413",
            "title": "Do LLMs Agree on the Creativity Evaluation of Alternative Uses?",
            "text": "Recent studies have investigated various methodologies and contexts in which LLMs can assess creativity. In [18], researchers evaluated LLMs on creative writing tasks. Models like GPT-4 showed high fluency and coherence, although human evaluators still outperformed LLMs in originality and humor. A collaborative approach was proposed by [19] with the COEVAL pipeline, which combines initial LLM evaluations with human reviews. This approach significantly reduced evaluation time and provided greater consistency by adjusting subjective criteria. \n\nIn divergent thinking tasks, [3] demonstrated that LLMs could reliably assess flexibility in alternative use tasks. This study reported a strong correlation with human evaluations, highlighting the model's effectiveness, particularly in educational settings. For more specialized tasks, [8] applied LLMs to metaphor creativity assessment, where models like RoBERTa and GPT-2 showed good alignment with human judgments, even outperforming traditional metrics. \n\nLLMs have also been applied to creative assessments in non-English contexts. In [20], XLM-RoBERTa was used to evaluate originality in scientific creativity tasks conducted in German, proving effective in capturing divergent ideation. Similarly, [21] explored the use of LLMs to evaluate question complexity based on Bloom's Taxonomy, achieving a high correlation with human evaluations and validating its use in educational assessments. Lastly, [22] investigates creativity in LLMs adapting the Torrance Test to measure fluency, originality, and elaboration, while [23] provides a comprehensive review of creativity assessment practices in machine learning, covering methodologies such as Generative Adversarial Networks (GANs) and Transformers and examining metrics like novelty, value, and surprise for creativity evaluation.",
            "score": 0.3926515458936899,
            "section_title": "Creativity Assessment using LLMs",
            "char_start_offset": 4741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1837
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 112,
                    "matchedPaperCorpusId": "263908973"
                },
                {
                    "start": 578,
                    "end": 581,
                    "matchedPaperCorpusId": "269680923"
                },
                {
                    "start": 836,
                    "end": 839,
                    "matchedPaperCorpusId": "268674764"
                },
                {
                    "start": 1090,
                    "end": 1094,
                    "matchedPaperCorpusId": "269868174"
                },
                {
                    "start": 1575,
                    "end": 1579,
                    "matchedPaperCorpusId": "233168627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2186279296875
        },
        {
            "corpus_id": "269626718",
            "title": "Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge",
            "text": "Moreover, we only evaluated the ability of GPT-4 to judge the quality of the feedback generated by a single other language model (GPT-3.5),for which GPT-4 might be biased towards giving positive results [33].However, this would thus mean that GPT-4 might be more reliable for other language models not of its family (e.g.open-source ones).\n\nAs part of our future work, we are focusing on two distinct areas: improving the judgments from LLMs on the quality of feedback, and evaluating open-source LLMs to serve as the judges.In this work, we zero-shot prompted the model to provide feedback on all three quality criteria at once.By improving the prompting strategy (e.g. using zero-shot chain of thought), the performance of the LLM as a judge could potentially increase.The other area we are currently working on is evaluating how well open-source language models could work as the judge of feedback quality.This could make it easier for educators to use the models both due to cost-effectiveness and privacy concerns.",
            "score": 0.3925728993007276,
            "section_title": "CONCLUSIONS",
            "char_start_offset": 24975,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 139,
                    "end": 208
                },
                {
                    "start": 208,
                    "end": 321
                },
                {
                    "start": 321,
                    "end": 339
                },
                {
                    "start": 341,
                    "end": 525
                },
                {
                    "start": 525,
                    "end": 629
                },
                {
                    "start": 629,
                    "end": 771
                },
                {
                    "start": 771,
                    "end": 909
                },
                {
                    "start": 909,
                    "end": 1019
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040252685546875
        },
        {
            "corpus_id": "270924255",
            "title": "Large Language Models as Evaluators for Scientific Synthesis",
            "text": "Large Language Models (LLMs) have made a significant impact on natural language processing (NLP), demonstrating exceptional performance in tasks like text generation, sentiment analysis, machine translation, and question answering, with outputs that often rival human-created content (Huang et al., 2023).In addition to their direct applications, LLMs offer substantial benefits in streamlining machine learning model development, particularly in evaluation processes.They reduce the dependency on human-generated ground truth data and the necessity for human evaluators (Bai et al., 2023) in two key ways: by facilitating the generation of synthetic ground truth data and by serving as evaluators for model predictions themselves.This approach not only speeds up the evaluation process but also broadens the scope of evaluation criteria to include factors such as diversity and coverage, enhancing the efficiency and comprehensiveness of model assessments.\n\nThis study investigates the use of LLMs as evaluators to streamline the evaluation process, moving away from traditional reliance on human evaluators and human-generated ground truth data.It specifically examines the effectiveness of LLMs in synthesizing scientific abstracts seen generally as multi-document summarization tasks.The main focus of this research is to assess how two state-ofthe-art LLMs-the proprietary GPT-4 Turbo (Ope-nAI, 2023) and the open-source Mistral-7B (Jiang et al., 2023)-perform in evaluating scientific syntheses.Furthermore, leveraging LLMs meant better versatility in evaluation considerations, which meant that the evaluations tested varied dimensions of syntheses quality, viz.comprehensiveness, trustworthiness, and utility.\n\nThis paper is structured as follows.First, section 2 presents a review of related work in the fields of text summarization and LLM evaluation.In section 3, we show our approach to using LLMs for scientific synthesis evaluation, wherein subsection 3.1 describes the LLM output, while subsection 3.2 presents a qualitative evaluation of this output.In subsection 3.3, we analyze the correlation between LLM ratings and human judgments.A discussion of our findings and final conclusions is described in section 4.",
            "score": 0.39192076446627655,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 305
                },
                {
                    "start": 305,
                    "end": 468
                },
                {
                    "start": 468,
                    "end": 731
                },
                {
                    "start": 731,
                    "end": 957
                },
                {
                    "start": 959,
                    "end": 1147
                },
                {
                    "start": 1147,
                    "end": 1288
                },
                {
                    "start": 1288,
                    "end": 1501
                },
                {
                    "start": 1501,
                    "end": 1669
                },
                {
                    "start": 1669,
                    "end": 1717
                },
                {
                    "start": 1719,
                    "end": 1755
                },
                {
                    "start": 1755,
                    "end": 1861
                },
                {
                    "start": 1861,
                    "end": 2066
                },
                {
                    "start": 2066,
                    "end": 2152
                },
                {
                    "start": 2152,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 304,
                    "matchedPaperCorpusId": "256868854"
                },
                {
                    "start": 571,
                    "end": 589,
                    "matchedPaperCorpusId": "259095491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "273507159",
            "title": "CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution",
            "text": "The evaluation of large language models (LLMs) typically falls into two major categories: objective evaluation and subjective evaluation. Objective evaluation evaluates LLMs using questions with ground-truth answers (multiple-choice questions, fill-in-the-blank questions, etc.). Given LLM responses and answers, rule-based approaches are adopted to check if the responses are correct or not. Subjective evaluation, on the other hand, often requires a more comprehensive assessment of models' responses from multiple perspectives such as helpfulness, honesty, or creativity (Liu et al., 2023). Most questions in subjective evaluation do not have a ground-truth answer, thus requiring human judgement for response quality assessment. \n\nDespite the effectiveness, human-based subjective evaluation is time-consuming, laborintensive, as well as hard to reproduce. Therefore, in early stages of subjective evaluation, using the best-performing model (such as GPT4 (Achiam et al., 2023)) to evaluate the responses of LLMs became the most adopted approach, which has been applied to multiple subjective evaluation benchmarks including AlpacaEval (Dubois et al., 2024a), ArenaHard (Li et al., 2024), WildBench (Lin et al., 2024), and AlignBench (Liu et al., 2023). However, those judge models with stronger critique capabilities are often commercial APIs with limited transparency and charge per use. The cost can be prohibitive for research studies when evaluations are frequently conducted. \n\nRecent efforts in the research community have focused on developing open-source judge models for subjective evaluation of large language models (LLMs). For example, Cri-tiqueLLM (Ke et al., 2023) has been developed to evaluate AlignBench instead of GPT-4. Works like Auto-J (Li et al., 2023) and Prometheus (Kim et al., 2024) also explore different training paradigms for judge models. However, most open-source judge LLMs are constrained by their specific training tasks and can only adhere to certain formats. This limitation restricts their applicability across multiple subjective benchmarks simultaneously.",
            "score": 0.39114838644918826,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2099
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.255126953125
        },
        {
            "corpus_id": "277043164",
            "title": "Commenting Higher-level Code Unit: Full Code, Reduced Code, or Hierarchical Code Summarization",
            "text": "In other words, we deploy 3 GPT agents: GPT-F, GPT-R, and GPT-S, where GPT-F takes the full module code to evaluate the summaries, GPT-R takes the reduced module code, and GPT-S takes hierarchical module code summaries (i.e., the file summaries). Since we have 3 GPT agents to evaluate 50 modules each with 9 summaries, a total of 3 \u00d7 50 \u00d7 9 = 1350 scores are marked by GPT-4. \n\nComparison to Human Evaluation. First, we compare the GPT-4's evaluation score to the scores marked by human evaluators on the 17 modules that are also evaluated by humans in RQ2. The results are reported in Table 6. Based on the raw scores, we can find that the GPT agents give a much higher score to the GPT-4-generated summaries, while such bias does not exist for the summaries generated by CodeLlama and CodeGemma. This indicates that for module-level summaries, the GPT agents prefer the GPT-4-generated summaries ( = 6.516 \u00d7 10 \u22127 ). Therefore, unlike the situation of file level, it might not be fair to compare the performance of different LLMs based on the GPT agents' scores. \n\nHowever, from the table, we can find that the correlation on the right side, i.e., the correlation between human and GPT evaluator on the result from the same LLM, is still relatively high by ranging from 0.302 to 0.882. This correlation suggests that GPT evaluators are capable of assessing summaries produced by the same LLMs, despite differing summary generation strategies. \n\n\u261e Finding \u25b6 Compared with humans, GPT agents prefer the summaries generated by GPT-4. However, when comparing summaries generated by the same LLM, GPT agents have a high correlation with humans. \u25c0 4.3.3 LLM's Effectiveness in Summarizing Large Modules.",
            "score": 0.39111182854980775,
            "section_title": "Evaluation Results.",
            "char_start_offset": 42594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 376
                },
                {
                    "start": 379,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1445
                },
                {
                    "start": 1448,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1700
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10931396484375
        },
        {
            "corpus_id": "263671602",
            "title": "Evaluating Hallucinations in Chinese Large Language Models",
            "text": "Judge once Judge 5 times Consistency rate 93.33% 93.50% \n\nTable 4: The average consistency rate between human evaluations and GPT-4 evaluations across six models. \"Juage 5 times\" refers to instructing GPT-4 to generate judgments five times, and adopting the answer that appears most frequently as the final decision. \n\nDetermining whether the answer to a question contains hallucinations poses a significant challenge for human evaluators. Relying on human evaluation as a fair and scalable automated assessment method is not feasible, which in turn limits the usability of datasets. In recent, many work adopt AI feedback from some powerful instructionfollowing large language model like GPT-3.5 and GPT-4 for training and evaluation (Cheng et al., 2023;Zheng et al., 2023a;Li et al., 2023b;Fu et al., 2023). Besides, Wang et al. (2023b) found that using LLM-based evaluator for open-domain QA evaluation is better than other methods. The evaluation of TruthfulQA also employed models as scorers, which were achieved by fine-tuning two 6.7B GPT-3 models on data collected by the authors. We believe that we can use LLM-based evaluators to replace such fine-tuning methods. In our benchmark, we use GPT-4 (gpt-4-0613) as the evaluator. \n\nDuring evaluation, we put our criteria into the instruction for GPT-4. And we give GPT-4 correct answer examples for reference. The specific format of the evaluation prompt is in Appendix D.2. Due to the inability of GPT-4 to access top logits and to produce deterministic outputs, we employ GPT-4 to generate five judgments for voting and use the result with the highest number of votes as the final judgment and we set the temperature to 0 and top p to 0.5. \n\nWe conducted experiments to assess the consistency between GPT-4's evaluation results and human evaluation results, and evaluated the impact of GPT-4's randomness on the consistency rate. In particular,we sampled two questions from each domain of the three parts, totaling 100 questions.",
            "score": 0.39074837068293844,
            "section_title": "EVALUATION METHOD",
            "char_start_offset": 17369,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 58,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1235
                },
                {
                    "start": 1238,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1697
                },
                {
                    "start": 1700,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1987
                }
            ],
            "ref_mentions": [
                {
                    "start": 735,
                    "end": 755,
                    "matchedPaperCorpusId": "258460987"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12249755859375
        },
        {
            "corpus_id": "278028562",
            "title": "Testing the utility of GPT for title and abstract screening in environmental systematic evidence synthesis",
            "text": "In this paper we show that OpenAI\u2019s Large Language Model (LLM) GPT perform remarkably well when used for title and abstract eligibility screening of scientific articles and within a (systematic) literature review workflow. We evaluated GPT on screening data from a systematic review study on electric vehicle charging infrastructure demand with almost 12,000 records using the same eligibility criteria as human screeners. We tested 3 different versions of this model that were tasked to distinguishing between relevant and irrelevant content by responding with a relevance probability between 0 and 1. For the latest GPT-4 model (tested in November 2023) and probability cutoff 0.5 the recall rate is 100%, meaning no relevant papers were missed and using this mode for screening would have saved 50% of the time that would otherwise be spent on manual screening. Experimenting with a higher cut of threshold can save more time. With threshold chosen so that recall is still above 95% for GPT-4 (where up to 5% of relevant papers might be missed), the model could save 75% of the time spent on manual screening. If automation technologies can replicate manual screening by human experts with effectiveness, accuracy, and precision, the work and cost savings are significant. Furthermore, the value of a comprehensive list of relevant literature, rather quickly available at the start of a research project, is hard to understate. However, as this study only evaluated the performance on one systematic review and one prompt, we caution that more test and methodological development is needed, and outline the next steps to properly evaluate rigor and effectiveness of LLMs for eligibility screening. Supplementary Information The online version contains supplementary material available at 10.1186/s13750-025-00360-x.",
            "score": 0.39026227421158965,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65185546875
        },
        {
            "corpus_id": "266741775",
            "title": "Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review",
            "text": "It has been argued that LLMs are not currently a complete replacement for human writing, but instead answer questions and generate naturally fluent and informative content compellingly, but with no real intelligence, just generated text based on patterns of previously seen words (Stokel-Walker, 2022). In one study, students used ChatGPT as an aid in their writing. However, the results showed that the experimental group that used ChatGPT was similar to the control group in terms of writing quality, speed, and authenticity, and the authors suggest that this may be because experienced researchers can better guide ChatGPT to highquality information. In contrast, the students may be having ChatGPT difficulties (Ba\u0161i\u0107, Banovac, Kru\u017ei\u0107, & Jerkovi\u0107, 2023). In another article, the authors discuss the prospects and potential threats of ChatGPT in academic writing and emphasize that using ChatGPT in academic research should prioritize peer-reviewed scholarly sources. In addition, the article mentions the potential advantages of ChatGPT in academic research, including the handling of large amounts of textual data, automatic generation of abstracts, and research questions (Dergaa, Chamari, Zmijewski, & Saad, 2023). Additionally, LLMs have the potential for peer review (Van Dis et al., 2023), where the results of LLM's evaluation in a text evaluation task are consistent with those of human experts (Chiang & Lee, 2023). \n\nIn conclusion, LLMs such as ChatGPT are potent tools for academic writing, capable of processing large amounts of textual data and automating tasks that were previously done manually; it can be used to scan academic papers and extract essential details, generate objective and unbiased abstracts, and create research questions. It also has the potential to be applied to peer review of papers. However, researchers must exercise caution when using them as they can also integrate false or biased information into papers, ChinaXiv:202401.00108v2 \n\nleading to unintentional plagiarism and misattribution of concepts (Dis, Bollen, Zuidema, Rooij, & Bockting, 2023).",
            "score": 0.39025208226961117,
            "section_title": "Paper writing and peer review tools",
            "char_start_offset": 49675,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1428
                },
                {
                    "start": 1431,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1975
                },
                {
                    "start": 1978,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 301,
                    "matchedPaperCorpusId": "254530623"
                },
                {
                    "start": 1178,
                    "end": 1220,
                    "matchedPaperCorpusId": "257567001"
                },
                {
                    "start": 1276,
                    "end": 1298,
                    "matchedPaperCorpusId": "256505670"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2880859375
        },
        {
            "corpus_id": "265466599",
            "title": "LLMs for science: Usage for code generation and data analysis",
            "text": "The potential of using LLMs in aiding the research process is currently a highly discussed topic. In an editorial, Susarla et al. 3 explore the potential of LLMs in information systems research. They explore research question formulation, data collection, data analysis, and writing-as tasks in which LLMs could add benefit. Kasneci et al. 6 discuss the opportunities and challenges of using LLMs for education. They also briefly touch on the potential of LLMs to assist in the writing process of research papers, by generating summaries or outlines, and providing guidance on orthography. This discussion also takes place across disciplines. For example, Grossmann et al. 5 share their perspective on LLM's transformative use for social science research practices and Li et al. 4 comment on the ethical concerns related to their use in medicine and medical research. In their global survey on postdocs, Nature found that 31% of postdocs are already using AI-assisted chatbots like ChatGPT. Of those who use it, the two most reported use cases are text refinement and generating or troubleshooting code. 2 Beyond proprietary models, several open source LLMs exist, such as GPT-Neo 8 or GPT-J. 3 There is also work focusing on training LLMs specifically for the use in a research context. SciBert 9 and ScholarBert 10 are both pre-trained language models based on a corpus of scientific publications. These models are evaluated based on their performance executing automated tasks related to scientific practice, such as classification or relation extraction. Galactica, 11 another large language model trained on scientific corpi, is a cautionary tale. Galactica was accessible online and meant to assist researchers in performing research tasks. The tool was taken down only days after its publication, following criticism over its tendency to hallucinate and confabulate. 12 his highlights the need for rigorous research and evaluation practices. The search for comprehensive evaluation systems for LLMs is ongoing research. 13 here exists also a growing body of work on LLMs trained on code. However, we have to note here that LLMs of code is an emergent research field, and a review in this space inadvertently involves a high amount of grey literature. While we carefully selected these, it is essential to exercise caution with non-peer-reviewed studies.",
            "score": 0.38988387918192546,
            "section_title": "CURRENT AND RELATED RESEARCH",
            "char_start_offset": 4385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2257
                },
                {
                    "start": 2258,
                    "end": 2360
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 131,
                    "matchedPaperCorpusId": "258935868"
                },
                {
                    "start": 340,
                    "end": 341,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 673,
                    "end": 674,
                    "matchedPaperCorpusId": "259166062"
                },
                {
                    "start": 779,
                    "end": 780,
                    "matchedPaperCorpusId": "258382834"
                },
                {
                    "start": 1104,
                    "end": 1105,
                    "matchedPaperCorpusId": "264129393"
                },
                {
                    "start": 1193,
                    "end": 1194,
                    "matchedPaperCorpusId": "258935868"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36181640625
        },
        {
            "corpus_id": "277501797",
            "title": "TaMPERing with Large Language Models: A Field Guide for using Generative AI in Public Administration Research",
            "text": "The evaluation of outputs generated by LLMs is an emerging and rapidly evolving area of inquiry. For PA scholars, rigorous evaluation methodologies are essential for establishing scientific validity and reliability of LLM-generated data. While no universally adopted framework exists (Xiao et al., 2023), this section outlines key decision points and criteria to guide evaluation practices. \n\nEvaluation decisions center on two core questions: what is evaluated and how it is evaluated. The \"what\" encompasses four primary targets: model selection, hyperparameter tuning, prompt design, and output quality assessment (Gu et al., 2025). Of these, model selection and output assessment will require evaluation in most studies, while hyperparameter settings and prompt design will need evaluation in cases where competing choices have strong implications for the output. Regardless of the specific evaluation target, the assessment is ultimately based on analyzing the model's output or response to determine its effectiveness. \n\nThe \"how\" of evaluation involves selecting criteria and designing protocols. The primary criteria for evaluating LLM outputs are accuracy, precision, and quality. Accuracy measures correctness or proximity of the output to a true value and is the primary criterion of interest for many LLM tasks (H. Li et al., 2024). Some LLM tasks have factual, groundtruth answers where the assessment of accuracy is straightforward, and can be calculated using F1-scores. For tasks where the ground truth is absent but human opinions or coded data exist, accuracy is calculated as a correlation between human-coded data (sometimes called the \"gold standard\") and the LLM output. However, caution is warranted when evaluating subjective outputs since human-coded data is prone to bias, inconsistency, and errors in judgment (Clark et al., 2021). LLM coders can outperform humans (Elangovan et al., 2025;T\u00f6rnberg, 2023a) suggesting that LLMs are not necessarily incorrect when there is a lack of alignment with human-validated outputs (Xiao et al., 2023). \n\nPrecision is similar to reliability in social science and refers to variability of the output across repeated trials. Temperature and top_p hyperparameters have a considerable impact on output variation and should be considered when estimating precision criteria.",
            "score": 0.38923867384024236,
            "section_title": "Evaluation",
            "char_start_offset": 35074,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2067
                },
                {
                    "start": 2070,
                    "end": 2187
                },
                {
                    "start": 2188,
                    "end": 2333
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1558837890625
        },
        {
            "corpus_id": "276902641",
            "title": "SKG-LLM: Developing a Mathematical Model for Stroke Knowledge Graph Construction Using Large Language Models",
            "text": "In this evaluation, experts evaluate the relationships and present the ranking as a consolidated weighted average. For this average, Cohen's kappa was used, which is calculated as follows: \n\nIn this equation po is the observed agreement among raters, and pe is the expected agreement by chance. Nonetheless, regarding the Accuracy (GPT-4), Wikidata narrowly wins at 89.66% while SKG-LLM is at the lowest point of 87.5%. This implies that while SKG-LLM performs superiorly in human-reviewed precision and consistency, there is an about area for improvement in accuracy as a whole. \n\nThe method of accuracy (GPT-4) is one of the critical evaluation methods in large language models, such as SKG-LLM. Here, the overall accuracy of the model gets evaluated using GPT-4. This essentially means judging the correctness of all relationships drawn from the data, and how aligned they are to scientific facts or existing data. Thus, in other words, Accuracy defines the percentage of all relationships and information created by the model which links with existing and validated data. It has always been across kinds that SKG-LLM outperforms in expert review scoring highest F1 score of 89.76 (Table 6). This implies that manual validation considerably improves the precision-recall balance. GPT-4 integration also scores on its F1 performance of 89.02, signifying its high automatic extraction capability. On the contrary, StrokeKG records a lower F1 score of 84.26 ostensibly because of its low precision-even though recall is too high. This suggests StrokeKG retrieves relations well, but with higher false positives compared to SKG-LLM. Within the set of comparisons made on the different NER models, SKG-LLM is consistent across all categories, and this is the highest F1 score achieved so far, by SKG-LLM (GPT-4), which is 90.97, thus indicating that integration with GPT-4 increases both precision and recall. Another very good score is given by SKG-LLM (Expert Review), with an F1 score of 90.84, further proving that expert validation adds value in the maintenance of balanced precision and recall.",
            "score": 0.38905998155528587,
            "section_title": "Expert Review",
            "char_start_offset": 25076,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 188
                },
                {
                    "start": 191,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 579
                },
                {
                    "start": 582,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2098
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.082275390625
        },
        {
            "corpus_id": "273507427",
            "title": "Arabic Dataset for LLM Safeguard Evaluation",
            "text": "Ensuring the robustness and objectivity of the evaluation framework is a crucial consideration. While the framework may not be entirely devoid of subjectivity, it follows a structured approach similar to the evaluation process conducted by human annotators. Just as human annotators apply evaluation criteria based on their understanding and background, large language models (LLMs) interpreting evaluation prompts may exhibit inherent biases. However, as demonstrated in Table 6 and Section 4.5, the evaluation results produced by GPT-4o exhibit a high level of agreement with human annotations, reinforcing the reliability of the framework. \n\nA rigorous refinement process was employed to address potential biases in the evaluation criteria. The construction of the evaluation framework, particularly the criteria presented in Table 8, involved iterative adjustments to ensure alignment with expected outcomes. The evaluation question set was carefully designed and underwent extensive refinements to enhance its comprehensiveness and reliability. This process comprised the following key steps: \n\n\u2022 Comprehensive Coverage: A diverse set of questions was initially crafted to encompass all anticipated risk areas, including region-specific cultural norms and values. This ensured that the evaluation captured a broad spectrum of potential risks, including controversial topics and contextspecific concerns. \n\n\u2022 Ground Truth: Approximately 250 questions and responses were manually annotated by socioculturally aware annotators to create a ground truth to be used as a reference to evaluate the crafted question set that captures cultural nuances and serves as a reference for evaluation. \n\n\u2022 Iterative Refinement: The framework was systematically improved by analyzing discrepancies between human annotations and GPT-4o evaluations. This process helped refine evaluation criteria and prompts to enhance accuracy and reduce bias.",
            "score": 0.3888773180581254,
            "section_title": "B.1 Construction process of the Evaluation Question sets",
            "char_start_offset": 25966,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1408
                },
                {
                    "start": 1411,
                    "end": 1689
                },
                {
                    "start": 1692,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1930
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.357666015625
        },
        {
            "corpus_id": "268714292",
            "title": "From Raw Video to Pedagogical Insights: A Unified Framework for Student Behavior Analysis",
            "text": "The study adopted a blind review methodology where the evaluators were unaware of the variations in the prompt strategies. We report the result in Table 3. It can be observed that carefully crafted prompt strategies can indeed nurture more detailed and insightful pedagogical reports. Strategies that combine expert perspectives and theoretical grounding emerge as the most potent in guiding the LLM to generate deep analyses. Also, a nuanced approach to context elucidation paired with strategic guidance and insights into classroom dynamics creates a more grounded and insightful analysis, which is appreciated by the evaluators. Future work might further explore the intricate dynamics of prompt strategies to facilitate even more nuanced and rich analyses. \n\nPerformance Across Model Variants. In this experiment, we aim to explore the impact of utilizing different LLM variants in generating analysis reports, maintaining a constant enhanced prompt with Identity Implication and Theoretical Guidance. We employed four notable LLMs for this experiment: LLaMA-7B, LLaMA-13B, GPT-3.5, and GPT-4. Each model was used to generate five reports per sequence to mitigate bias. To evaluate the reports, we utilized the GPT-4-based automatic evaluation system recently   proposed by (Chiang et al. 2023). This system assesses the depth of analysis, relevance of insights, actionable recommendations, theoretical grounding, and overall quality while allocating ratings. It assigns ratings based on predefined criteria that align with established educational assessment principles. The depth of analysis is evaluated in terms of the report's thoroughness in exploring classroom dynamics (Merriam and Tisdell 2015), while the theoretical grounding is assessed based on the report's alignment with established pedagogical theories (Lodico, Spaulding, and Voegtle 2010). The actionable recommendations are gauged for their practical applicability in educational settings. \n\nThe results in Figure 3 suggest that increasing the parameter count from LLaMA-7B to LLaMA-13B enhances all aspects of the generated reports. However, there is a significant lack in the theoretical depth of these two models, presumably due to a lack in the pre-training corpus. Comparatively, GPT-3.5 and GPT-4 exhibit a closer performance level, with GPT-4 having a slight edge in quality.",
            "score": 0.3888498633557549,
            "section_title": "Analysis Report Generation: The Role of Prompts and Model Variants",
            "char_start_offset": 24022,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1961
                },
                {
                    "start": 1964,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2354
                }
            ],
            "ref_mentions": [
                {
                    "start": 1680,
                    "end": 1706,
                    "matchedPaperCorpusId": "252112137"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.408935546875
        },
        {
            "corpus_id": "264289186",
            "title": "SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents",
            "text": "Table 1 breaks this aggregated analysis into different dimensions and whether the character is role-played by a human or a model. The correlations show that when models are role-playing, the GPT-4 scores have significant and strong correlations with the humans' scores on GOAL, FIN, and REL dimensions. However, when humans are role-playing, the correlations drop significantly on all but one dimension (GOAL). This indicates that GPT-4 could evaluate social interactions on some dimensions and that it is better for evaluating models compared to humans. In Appendix G.3, we compare the average GPT-4 scores and the range of human scores for a single dimension of an agent in an episode. We find that GPT-4 scores are typically within human score ranges on most dimensions except for SOC and SEC, where GPT-4 often rates higher than humans do. \n\nPutting these observations together, we conclude that, with some caution, GPT-4 can be used as a proxy to human judgments for evaluating model performanceon some dimensions and for human performance on the GOAL dimension. However, we remind readers that LLMs are known to have biases and problems for evaluation, including positional bias (Wang et al., 2023), factual inconsistency (Luo et al., 2023), favoring native speakers (Liang et al., 2023). Therefore, one should be aware of the influence of these potential biases when interpreting our results. Future versions of SOTOPIA-EVAL may further improve LLM-based evaluation quality using recent methods, such as involving multiple LLMs Chan et al. (2023) and training larger LLM evaluators Zhang et al. (2023) We analyze models' interactions and performance on SOTOPIA to understand their social intelligence. Table 2 presents the models' average scores when interacting with different partner models (i.e., the model it is paired with in interaction, Fu et al. 2023;Hu et al. 2020).9 GPT-4 performs best on most dimensions, followed by GPT-3.5, Llama-2-70b-chat, and MPT-30b-chat.",
            "score": 0.3886798343267025,
            "section_title": "ANALYZING GPT-4 EVALUATIONS",
            "char_start_offset": 23193,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 1185,
                    "end": 1204,
                    "matchedPaperCorpusId": "189928358"
                },
                {
                    "start": 1273,
                    "end": 1293,
                    "matchedPaperCorpusId": "257985499"
                },
                {
                    "start": 1866,
                    "end": 1880,
                    "matchedPaperCorpusId": "212628508"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0701904296875
        },
        {
            "corpus_id": "277621852",
            "title": "FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking",
            "text": "In their study, Liu et al. (2023) identified a bias in evaluators, where they tend to favor their own model's generations over those from other models, even when the latter are objectively better. Ohi et al. (2024) introduced a method for detecting this bias, which they termed 'Likelihood-based Evaluation Bias.' However, this approach requires access to the probability distribution of the LLM's generations, which is often unavailable, especially when working with commercial LLMs. Ye et al. (2024) also addressed this issue, referring to it as 'egocentric Bias,' and we adopt this terminology in our work. Their research primarily focuses on understanding the effects of this bias on performance and strategies for mitigating it. The purpose of this study is to compare the effect of ego-centric bias on our fine-grained evaluation \"FinGrAct\" and on other SOTA evaluators. In this paper, we propose a simple yet effective method for identifying this bias within the context of actionability evaluation in explainable AFC when the probability distribution of LLM generations is not available. \n\nWe identify biased samples by observing that evaluators tend to assign significantly higher scores to explanations generated by their own underlying LLMs compared to human ratings. For instance, Geval exhibits a preference for GPT-4-generated explanations, even when alternative explanations may be superior. To quantify this bias, we implement a Likert-scale scoring system ranging from 0 to 5, allowing for a tolerance of a 1-point difference between human and LLM scores. If multiple human annotators rate an explanation as 2 and the LLM assigns a 3, the sample is excluded from bias analysis. However, if the LLM scores the same explanation as 4 or 5, it is classified as ego-centric bias. Thus, a discrepancy of 2 or more points higher than the human rating serves as a clear indicator of bias. \n\nSetup: To measure ego-centric bias, each evaluator is tasked with assessing explanations generated by its underlying LLM. For instance, Prometheus evaluates Mistral-generated explanations, while G-Eval and FinGrAct evaluate GPT-4-generated explanations.",
            "score": 0.3886210358959458,
            "section_title": "Experiment 3: Ego-Centric Bias Analysis",
            "char_start_offset": 24773,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1897
                },
                {
                    "start": 1900,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2153
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 33,
                    "matchedPaperCorpusId": "257804696"
                },
                {
                    "start": 197,
                    "end": 214,
                    "matchedPaperCorpusId": "267938572"
                },
                {
                    "start": 485,
                    "end": 501,
                    "matchedPaperCorpusId": "273098639"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.397705078125
        },
        {
            "corpus_id": "259069613",
            "title": "Comparative Analysis of Decision-Making Efficiency of Large Language Models",
            "text": "The materials used in this study primarily consisted of the specially designed questions that were utilized to evaluate the decision-making efficiency of Bard and GPT in different domains. These questions served as the primary means to assess the cognitive abilities and performance of the two LLMs in various decision-making contexts. \n\nThe design of these questions was crucial to ensure that they effectively targeted the specific skills and competencies required in each domain. Extensive consideration was given to formulating questions that adequately measured the logical reasoning abilities, error detection capabilities, and data analysis proficiencies of Bard and GPT. \n\nThe process of question design involved a thorough review of existing literature and established frameworks for assessing decision-making skills. This allowed us to draw upon established principles and guidelines to create questions that were valid and reliable indicators of the LLMs' decision-making efficiency. The questions were carefully crafted to present realistic scenarios and challenges that mirrored real-world decision-making situations. \n\nTo maintain consistency and eliminate potential biases, the same set of questions was presented to both Bard and GPT during the evaluation process. Each question was presented in a standardized format to ensure uniformity in the way the questions were interpreted and answered by the models. \n\nThe materials used in this study were essential for providing a standardized and controlled environment for evaluating the decision-making efficiency of Bard and GPT. The questions, specifically designed for each domain, formed the basis for quantitatively assessing the performance of the LLMs and comparing their abilities in different decision-making tasks. \n\nIt is worth noting that the study also relied on the computational resources and infrastructure required to run the LLMs and collect their responses to the domain-specific questions. The computational resources ensured the efficient execution of the evaluation process, enabling us to gather the necessary data for analysis. \n\nOverall, the materials used in this study, primarily consisting of the domain-specific questions, played a vital role in evaluating the decision-making efficiency of Bard and GPT. These materials facilitated a standardized evaluation process, allowing for a fair and objective comparison of the two LLMs' performance in different decision-making domains. Procedure \n\nTo evaluate the decision-making efficiency of Bard and GPT, a systematic and standardized procedure was followed. The following steps were taken to ensure a fair comparison between the two LLMs:",
            "score": 0.3884413633738401,
            "section_title": "Materials",
            "char_start_offset": 7597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 335
                },
                {
                    "start": 338,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 678
                },
                {
                    "start": 681,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1787
                },
                {
                    "start": 1790,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2114
                },
                {
                    "start": 2117,
                    "end": 2296
                },
                {
                    "start": 2297,
                    "end": 2471
                },
                {
                    "start": 2472,
                    "end": 2481
                },
                {
                    "start": 2484,
                    "end": 2597
                },
                {
                    "start": 2598,
                    "end": 2678
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06243896484375
        },
        {
            "corpus_id": "272987938",
            "title": "CRScore: Grounding Automated Evaluation of Code Review Comments in Code Claims and Smells",
            "text": "While LLMs have recently shown promise for evaluating natural language generation (NLG) (Li et al., 2024) they suffer from biases like favoring their own generations (\"self-selection bias\") (Panickssery et al., 2024) or in other words, if we were to have Magicoder or GPT-3.5 as the evaluator LLM it would assign higher scores to the text generated by Magicoder and GPT-3.5 respectively. Code analysis tools (CATs) on the other hand are limited in scope compared to LLMs in detecting issues like best practice violations (Vijayvergiya et al., 2024) but don't have any self-selection bias. However, combining these methods can reduce the self-selection bias of LLMs, while supplementing the narrow coverage of code analysis tools. Indeed the results show that despite using Magicoder as the evaluation LLM, our metric CRScore doesn't preferentially rank Magicoder above any models other than LLaMA-3 when compared to the human ranking.",
            "score": 0.38765760098012947,
            "section_title": "A.1 Benefits of Neuro Symbolic Pseudo-Reference Generation",
            "char_start_offset": 31573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 934
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.286865234375
        },
        {
            "corpus_id": "270870769",
            "title": "Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?",
            "text": "Are LLMs ready to compete in creative writing skills with a top (rather than average) novelist? To provide an initial answer for this question, we have carried out a contest between Patricio Pron (an awarded novelist, considered one of the best of his generation) and GPT-4 (one of the top performing LLMs), in the spirit of AI-human duels such as DeepBlue vs Kasparov and AlphaGo vs Lee Sidol. We asked Pron and GPT-4 to provide thirty titles each, and then to write short stories for both their titles and their opponent\u2019s. Then, we prepared an evaluation rubric inspired by Boden\u2019s definition of creativity, and we collected several detailed expert assessments of the texts, provided by literature critics and scholars. The results of our experimentation indicate that LLMs are still far from challenging a top human creative writer. We also observed that GPT-4 writes more creatively using Pron\u2019s titles than its own titles (which is an indication of the potential for human-machine co-creation). Additionally, we found that GPT-4 has a more creative writing style in English than in Spanish.",
            "score": 0.38764426071292873,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.050323486328125
        },
        {
            "corpus_id": "267200130",
            "title": "Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes",
            "text": "Our project was largely motivated by the early success of LLMs in handling questions in standardized medical 29 and nursing licensure exams 30 . LLMs have demonstrated superb abilities in reasoning clinical questions without training with comprehensive medical literature 31 , a phenomenon that has been reported in other fields 32 . To gain a deeper understanding of the process that LLMs use to perform this high-level task, we framed our qualitive evaluation approach by treating the LLM under evaluation as a virtual assistant tasked with collecting evidence, reasoning with the collected evidence, and eventually describing the findings. GPT-4 was chosen as the LLM to be evaluated in this way owing to its superior objective performance. While previous studies laid a foundational understanding of LLM applications in healthcare, their scope is limited to singular aspects, such as relevance 33 or correctness 29 , or focused on choosing the best answer from multiple-choice questions 34,35 . Our approach, in contrast, encompasses a broader spectrum of qualitative performance. This methodology not only highlights the superior capability of GPT-4 in processing complex medical data but also sets a precedent for future research in evaluating LLMs across various specialized domains.",
            "score": 0.3873513477101547,
            "section_title": "C. How Well Did GPT-4 Understand Clinical Concepts?",
            "char_start_offset": 30332,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1290
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 142,
                    "matchedPaperCorpusId": "259842568"
                },
                {
                    "start": 991,
                    "end": 994,
                    "matchedPaperCorpusId": "263332951"
                },
                {
                    "start": 994,
                    "end": 996,
                    "matchedPaperCorpusId": "254876189"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0802001953125
        },
        {
            "corpus_id": "269293382",
            "title": "Automated Long Answer Grading with RiceChem Dataset",
            "text": "The rapid advancements in LLMs have led to significant improvements in various natural language processing tasks.To assess the potential of these models in the context of ALAG, we evaluate the zero-shot performance of several LLMs on the RiceChem dataset, as presented in Table 4.For GPT models [18], we used the following hyperparameters: a temperature of 1.0, a frequency penalty of 0, and a presence penalty of 0. For open-sourced LLMs [10,11,13,29,30,31,15,17], we considered the log probabilities of the 'True' and 'False' tokens to determine if the student answered the rubric item correctly or not.By comparing these log probabilities, we obtained the model's predicted label.\n\nDespite the impressive success of LLMs in many domains, the RiceChem dataset proves to be a formidable challenge.\n\nThe best-performing model, GPT-4, achieves an accuracy of 70.9% and an F1 score of 0.689, highlighting the complexity of the ALAG task.This performance is particularly striking when compared to the results of GPT models on ASAG tasks.\n\nPrevious research has shown that GPT models can achieve an F1 score of 0.74 on the SciEntsBank dataset for ASAG without the use of rubrics [33].In contrast, GPT-4 obtains a lower F1 score of 0.69 on RiceChem, despite the significant beneficial impact of rubrics typically observed in ASAG [27] and AEG [28] tasks.This discrepancy underscores the increased difficulty of ALAG compared to ASAG.\n\nIt is worth noting that the actual difference in complexity between ASAG and ALAG may be even more substantial than the five-point difference in F1 scores suggests.The use of rubrics in RiceChem provides a structured framework for grading, which is expected to enhance model performance.However, even with this advantage, GPT-4 still struggles to match its performance on ASAG tasks without rubrics.\n\nThe results in Table 4 also reveal the varying performance of different LLMs on the RiceChem dataset.",
            "score": 0.38718653453274976,
            "section_title": "Benchmarking on Large Language Models",
            "char_start_offset": 22383,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 113,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 605
                },
                {
                    "start": 605,
                    "end": 683
                },
                {
                    "start": 685,
                    "end": 798
                },
                {
                    "start": 800,
                    "end": 935
                },
                {
                    "start": 935,
                    "end": 1034
                },
                {
                    "start": 1036,
                    "end": 1180
                },
                {
                    "start": 1180,
                    "end": 1349
                },
                {
                    "start": 1349,
                    "end": 1428
                },
                {
                    "start": 1430,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1717
                },
                {
                    "start": 1717,
                    "end": 1829
                },
                {
                    "start": 1831,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 1325,
                    "end": 1329,
                    "matchedPaperCorpusId": "53034274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25634765625
        },
        {
            "corpus_id": "261100760",
            "title": "GPTEval: A Survey on Assessments of ChatGPT and GPT-4",
            "text": "ChatGPT (OpenAI, 2023b) has generated significant scholarly interest across various disciplines due to its impressive dialogue-based taskprocessing capabilities. This has enabled users to explore and evaluate its performance across a wide range of tasks and disciplines, thereby sparking considerable enthusiasm in the field of Artificial Intelligence (AI). While many researchers have concentrated on evaluating ChatGPT and GPT-4 (Ope-nAI, 2023a) within their specific domains of expertise, a comprehensive review encompassing the assessments in multiple tasks and disciplines can offer a holistic understanding of the strengths and limitations of these GPT models (see Appendix A). \n\nWe focus on ChatGPT and GPT-4, because many evaluation reports have come to similar conclusions -ChatGPT and GPT-4 are the state-of-the-art (SOTA) large language models (LLMs) by now. The scope of our survey encompasses quantitative evaluations carried out on ChatGPT or GPT-4, specifically focusing on their language proficiency, scientific knowledge, and ethical considerations. \n\nOur main findings are summarized as follows: a) ChatGPT and GPT-4 are strong in language understanding and generation, adeptly engaging in user interactions through dialogues, enabling them to tackle diverse NLP tasks and provide explanatory outputs. However, their current status falls short of being a comprehensive AI, as their performance lags behind expert models in numerous domains involving domain-specific knowledge. \n\nb) ChatGPT performs satisfactorily in general science knowledge and can answer science questions that desire open responses. However, it can also make mistakes, especially for questions that require multi-step reasoning. The exceptional language proficiency poses challenges for users in assessing the accuracy of factual information, giving rise to a range of ethical concerns. c) Existing evaluation methods may be unreliable. The current evaluation methods heavily depend on prompt engineering and benchmark datasets. Varying prompts can yield disparate evaluation results. Additionally, the comparison of expert systems often relies on (in-domain) datasets that were utilized for training those systems. It remains uncertain whether the examined data, such as public datasets and scientific knowledge, have been inadvertently exposed during the training of ChatGPT and GPT-4.",
            "score": 0.3865732971589048,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1494
                },
                {
                    "start": 1497,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2204
                },
                {
                    "start": 2205,
                    "end": 2376
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28173828125
        },
        {
            "corpus_id": "272694127",
            "title": "CREAM: Comparison-Based Reference-Free ELO-Ranked Automatic Evaluation for Meeting Summarization",
            "text": "For both completeness and conciseness, LLM evaluators show low correlation with human preference and high self-bias in long-context dialogues like meeting summaries, although perform well in evaluating shorter summaries. \n\nCompleteness LLMs evaluators like GPT-4o can accurately extract key facts and compare summaries with high correlation to human evaluations in shorter summarization tasks like those in RE-ALSum, which involve news-style articles with summaries averaging 3 \u2212 5 sentences. We evaluate LLMs' ability to measure completeness when given different type of reference by extract key facts and align them with summaries generated by various baseline models, comparing their perfor-mance against human-annotated scores. As shown in Figure 2, GPT-4o showed a high correlation with human scores when using human-annotated key facts as references (rank correlation of 0.95, Pearson's r), and also similarly positive correlation when extracting key facts from entire articles for comparison with summaries. \n\nHowever, these evaluators struggle with accurately evaluating completeness in long-context dialogues, such as meeting summaries from the QM-Sum dataset with average of 556.8 turns. As shown in Table 1, our evaluation revealed that all versions of GPT have a weak correlation (Pearson's r = 0.5, ) with gold standard human scores with a strong self-bias. Both GPT-4 and GPT-3.5 tend to give near-perfect scores for summaries generated by LLMs, with GPT-4o performing only slightly better but still showing a bias towards its own generated content. \n\nConciseness Similar to completeness, these evaluators have a weak correlation with gold standard human scores with a strong self-bias when evaluating conciseness. As shown in Table 1, LLMs are far less effective for long-context dialogues, and performance without gold reference tends to result in an overestimation of conciseness. \n\nFactuality Current evaluators struggle with accurately identifying factual errors in meeting summaries.",
            "score": 0.3862258631509367,
            "section_title": "Experiments",
            "char_start_offset": 10755,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 223,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 1014
                },
                {
                    "start": 1017,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1563
                },
                {
                    "start": 1566,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1897
                },
                {
                    "start": 1900,
                    "end": 2003
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33349609375
        },
        {
            "corpus_id": "259936856",
            "title": "ChatGPT is Good but Bing Chat is Better for Vietnamese Students",
            "text": "Previous research has investigated multiple facets pertaining to the utilization of ChatGPT, a prominent language model, within the realm of education. GPT-4 (OpenAI, 2023) demonstrates a level of performance comparable to that of humans across a range of professional and academic assessments, such as Uniform Bar Exam, AP Chemistry, GRE Quantitative section, and AP Physics 2. \n\nIn contrast, the performance of GPT-3.5 on these benchmarks is notably inferior compared to that of  In previous studies explored various aspects of using ChatGPT, a large language model, in the field of education. GPT-4 (OpenAI, 2023) exhibits human-level performance on a variety of professional and academic benchmarks, including the Uniform Bar Exam, AP Chemistry, the GRE Quantitative section, and AP Physics 2. In contrast, GPT-3.5 does not perform as well on these benchmarks, scoring significantly lower than GPT-4. In higher education (Rudolph et al., 2023), GPT-4 demonstrates superior performance, whereas BingChat and Bard exhibit comparatively lower performance, resembling that of academically challenged students with failing grades. \n\nLLMs have the potential in supporting medical education and clinical decision-making (Kung et al., 2023), answering medical questions (Gilson et al., 2023), supporting learning and assessment in law exams (Choi et al., 2022), assisting academic paper writing and suggests a shift in learning goals (Zhai, 2022). \n\nPrior research has also investigated the benefits, limitations, and challenges related to the adoption of LLMs in the realm of education. It underscores the necessity of well-defined strategies and the cultivation of critical thinking skills (Kasneci et al., 2023). Additionally, it highlights the significance of ethical considerations in the context of teaching and learning (Baidoo-Anu and Owusu Ansah, 2023), particularly in relation to personalized learning and virtual simulations (Qadir, 2023). The study also addresses concerns regarding academic dishonesty and breaches of privacy (Tlili et al., 2023), emphasizing the importance of safeguarding privacy rights, promoting fairness, and ensuring transparency (Mhlanga, 2023).",
            "score": 0.3858253632922224,
            "section_title": "Evaluation of LLMs on education",
            "char_start_offset": 6151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1443
                },
                {
                    "start": 1446,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 925,
                    "end": 947,
                    "matchedPaperCorpusId": "268115560"
                },
                {
                    "start": 1217,
                    "end": 1236,
                    "matchedPaperCorpusId": "254876189"
                },
                {
                    "start": 1266,
                    "end": 1287,
                    "matchedPaperCorpusId": "259842568"
                },
                {
                    "start": 1337,
                    "end": 1356,
                    "matchedPaperCorpusId": "256409866"
                },
                {
                    "start": 1688,
                    "end": 1710,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 1933,
                    "end": 1946,
                    "matchedPaperCorpusId": "258857903"
                },
                {
                    "start": 2036,
                    "end": 2056,
                    "matchedPaperCorpusId": "257055406"
                },
                {
                    "start": 2163,
                    "end": 2178,
                    "matchedPaperCorpusId": "256859392"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0295867919921875
        },
        {
            "corpus_id": "267365346",
            "title": "Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning",
            "text": "Evaluating responses generated by Large Language Models (LLMs) like GPT-4 remains a complex and ongoing research area, particularly for opendomain questions where establishing a clear ground truth is challenging. Traditional methods often fall short in assessing the instruction-following ability of these models. Recent trends, however, involve using LLMs themselves, such as GPT-4, as evaluators, a practice that has gained widespread acceptance in the field (Touvron et al., 2023b;Chiang et al., 2023;Dettmers et al., 2023;Liu et al., 2023b). Previous studies (Zheng et al., 2023;Li et al., 2023c;Sottana et al., 2023) have shown that GPT4's evaluations are consistent with human evaluations. We utilized the testing instruction set from WizardLM (Xu et al., 2023) and Vicuna (Chiang et al., 2023) which contain 218 and 80 diverse human-curated instructions respectively. \n\nOur study adopts the evaluation strategy as outlined by Chen et al. (2023b); Li et al. (2023b,a), involving a detailed rating system for modelgenerated responses. Each response is scored reflecting various dimensions such as the accuracy and relevance of the response. This method is in line with previous research efforts to assess the effectiveness of language models more accurately. Moreover, to address the issue of positional bias, as discussed in the works of Ko et al. (2020); Wang et al. (2023a), we present the responses generated by the model in two separate sequences for evaluation by the LLM judge. This approach aims to ensure a more balanced and unbiased assessment of the model's performance. Then for each instruction, we compare the responses by \"Win-Tie-Loss\".",
            "score": 0.38552179526489416,
            "section_title": "B Evaluation Metric B.1 Pair-wise comparison",
            "char_start_offset": 32843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1657
                }
            ],
            "ref_mentions": [
                {
                    "start": 600,
                    "end": 621,
                    "matchedPaperCorpusId": "264426756"
                },
                {
                    "start": 1344,
                    "end": 1360,
                    "matchedPaperCorpusId": "216868500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26904296875
        },
        {
            "corpus_id": "264591429",
            "title": "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation",
            "text": "\u2022 Self-enhancement Bias Reduction.Table 2 evaluates self-enhancement bias by comparing BSM (with zero-shot GPT-4) for the samples where one of the responses is also generated by GPT-4.We observe a 3% better correlation with humans, suggesting that BSM improves evaluation even when the LLM judges its own outputs.\n\nBSM not only leads to an improvement in overall LLM-human agreement (as per the 'Ag' metric) but also on the fraction of samples where one response is generated by the same evaluator LLM (as per the 'SB' metric), thus pointing to its robustness as an evaluation method.In summary, BSM improves both correctness and consistency of LLM-based evaluators.\n\nBSM improves upon all zero-shot base LLMs.We demonstrate the generalizability of BSM as an LLM program by implementing it on top of four dif- We compare LLaMA-2-70B-chat BSM with the baseline zero-shot method, and also report GPT-4 results.BSM obtains significant improvements over the LLaMA baseline, and matches or is close to GPT-4 agreement in three of the four domains, while sometimes outperforming GPT-4 in reducing biases.\n\nferent base LLMs, ranging from LLaMA-2-7B to GPT-4.As shown in Table 3, BSM improves agreement with humans for all base LLMs, compared to a zero-shot baseline.Even though zero-shot GPT-4 is the state-of-the-art LLM-based evaluator, applying BSM obtains a further improvement of 3%.Moreover, applying BSM to LLaMA-2-70Bchat makes it competitive with GPT-4 for turn-1 questions.BSM also significantly reduces position and length biases for all models except for GPT-4.",
            "score": 0.38536712594834155,
            "section_title": "Main Results",
            "char_start_offset": 22039,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 34,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 313
                },
                {
                    "start": 315,
                    "end": 584
                },
                {
                    "start": 584,
                    "end": 666
                },
                {
                    "start": 668,
                    "end": 710
                },
                {
                    "start": 710,
                    "end": 908
                },
                {
                    "start": 908,
                    "end": 1098
                },
                {
                    "start": 1100,
                    "end": 1151
                },
                {
                    "start": 1151,
                    "end": 1259
                },
                {
                    "start": 1259,
                    "end": 1381
                },
                {
                    "start": 1381,
                    "end": 1476
                },
                {
                    "start": 1476,
                    "end": 1566
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1181640625
        },
        {
            "corpus_id": "274437466",
            "title": "Can Large Language Models Serve as Evaluators for Code Summarization?",
            "text": "We examine the impact of using different LLMs for evaluating code summarization, specifically the GPT-3.5 series models (i.e., text-davinci-003 and gpt-3.5-turbo) and GPT-4 (i.e., gpt-4). text-davinci-0031 provides high-quality outputs with reliable instruction-following ability, while gpt-3.5-turbo offers extended context length suitable for conversational applications. gpt-4 demonstrates high accuracy in complex problem-solving, making it effective for both interactive and traditional tasks. Experimental results across various LLMs in the reference-free scenario are presented in Figure 5. Obviously, the gpt-4 model exhibits a superior performance over the gpt-3.5 series of models. Moreover, the text-davinci-003 demonstrates well-rounded performance across overall evaluations. While gpt-4 outperforms the gpt-3.5 series, its higher API cost should be considered. Thus, we recommend choosing different LLMs as evaluators based on performance needs and budget considerations.",
            "score": 0.3850102734125399,
            "section_title": "D. RQ2: Influence of Evaluator Settings 1) Influence of Evaluator Types:",
            "char_start_offset": 26776,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 985
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.329345703125
        },
        {
            "corpus_id": "271213241",
            "title": "Putting GPT-4o to the Sword: A Comprehensive Evaluation of Language, Vision, Speech, and Multimodal Proficiency",
            "text": "As illustrated in Figure 1, GPT-4o belongs to the class of technologies known as large language models (LLMs).These models are notable for their ability to mimic human language usage so closely that it can be difficult for a human observer to distinguish between text generated by a human and that generated by a machine (Thirunavukarasu et al., 2023;Hayawi et al., 2024).This innovation marks a significant advancement towards passing the Turing test and underscores the practicality of AI in writing and research (Aher et al., 2023;Mannuru et al., 2023).However, it also introduces significant risks, including potential invasions of privacy and the generation of inaccurate, misleading, biased, or harmful information (Lund et al., 2023).Therefore, it is crucial to carefully evaluate these LLMs and scrutinize their outputs.Failure to do so could lead to the proliferation of misinformation and malicious content on the Internet (Hu et al., 2024).\n\nGiven the serious issues associated with some LLMs, it is essential to critically examine each new model for its limitations.Recent versions of GPT have shown significant improvements over their predecessors in various areas.For example, Koubaa (2023) found substantial improvements in GPT-4 compared to GPT-3.5 on tests such as the Graduate Record Examination (GRE), SAT, and Bar exam, with GPT-4's performance placing it in the top tenth percentile on most of these exams.Similarly, Coyne et al. (2023) reported improvements in grammatical error correction for GPT-4 compared to GPT-3.5.However, having more parameters in a model does not inherently guarantee better performance on all tasks.Overfitting can occur when a model is extensively trained on a large dataset but fails to generalize well to real-world data (Salman & Liu, 2019).\n\nEvaluation of the GPT-4o model is currently very limited.",
            "score": 0.38468006016567546,
            "section_title": "Figure 1. Visualization of the Relationship Between General AI and GPT 4o",
            "char_start_offset": 3573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 110,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 556
                },
                {
                    "start": 556,
                    "end": 741
                },
                {
                    "start": 741,
                    "end": 828
                },
                {
                    "start": 828,
                    "end": 951
                },
                {
                    "start": 953,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1178
                },
                {
                    "start": 1178,
                    "end": 1427
                },
                {
                    "start": 1427,
                    "end": 1542
                },
                {
                    "start": 1542,
                    "end": 1647
                },
                {
                    "start": 1647,
                    "end": 1793
                },
                {
                    "start": 1795,
                    "end": 1852
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 351,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 515,
                    "end": 534,
                    "matchedPaperCorpusId": "251719353"
                },
                {
                    "start": 721,
                    "end": 740,
                    "matchedPaperCorpusId": "257463753"
                },
                {
                    "start": 933,
                    "end": 950,
                    "matchedPaperCorpusId": "262084042"
                },
                {
                    "start": 1438,
                    "end": 1457,
                    "matchedPaperCorpusId": "258967185"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.142822265625
        },
        {
            "corpus_id": "264591429",
            "title": "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation",
            "text": "BSM significantly improves both tasks, addressing the aforementioned limitations of LLM evaluation and generation: \u2022 BSM improves correctness of LLM evaluation.In particular, on the MT-Bench benchmark (Zheng et al., 2023), BSM improves LLMhuman agreement for evaluating multi-turn questions belonging to different domains including writing, coding, reasoning, and mathematics.For example, compared to zero-shot prompting and self-consistency (Wang et al., 2022) baselines, BSM with LLaMA-2-70B-chat improves LLMhuman agreement by up to absolute 26% and even matches or outperforms GPT-4 on many domains.BSM with GPT-4 improves agreement by a further 3% over GPT-4.Overall, these findings point to BSM's ability to evaluate LLM responses to arbitrary user questions from diverse domains and to improve any base LLM as an evaluator.\u2022 BSM also improves the consistency of LLM evaluation.It significantly reduces position, length, and self-enhancement biases of LLM-based evaluators.For instance, BSM with LLaMA-2-70Bchat reduces position bias by up to absolute 50%.Importantly, BSM with GPT-4 also improves GPT-4's reliability as an evaluator when evaluating its own responses.\u2022 For the constrained story generation task, BSM generates more coherent stories, which are preferred by a GPT-4 judge a substantial 93% of the time compared to a zero-shot baseline.It also improves constraint satisfaction by 12%.\n\nOverall, BSM provides a framework for planning and task decomposition for addressing challenging multi-faceted language generation and evaluation tasks.As the approach is framed as a generic LLM program, it can be applied to any underlying LM and potentially a wide range of tasks.",
            "score": 0.38443657189015656,
            "section_title": "Introduction",
            "char_start_offset": 3528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 160,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 603
                },
                {
                    "start": 603,
                    "end": 664
                },
                {
                    "start": 664,
                    "end": 830
                },
                {
                    "start": 830,
                    "end": 884
                },
                {
                    "start": 884,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1062
                },
                {
                    "start": 1062,
                    "end": 1174
                },
                {
                    "start": 1174,
                    "end": 1356
                },
                {
                    "start": 1356,
                    "end": 1404
                },
                {
                    "start": 1406,
                    "end": 1558
                },
                {
                    "start": 1558,
                    "end": 1687
                }
            ],
            "ref_mentions": [
                {
                    "start": 442,
                    "end": 461,
                    "matchedPaperCorpusId": "247595263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33447265625
        },
        {
            "corpus_id": "273811997",
            "title": "CycleResearcher: Improving Automated Research via Automated Review",
            "text": "The key challenge is enabling AI to go beyond analysis and actively contribute to generating new research ideas, which demands advanced reasoning and creativity. Our work builds on AI's historical role in science, aiming to shift AI from a supporting tool to a leader in scientific discovery. \n\nAutomated Evaluation of Research Papers. The use of AI tools in the scientific publishing process has garnered widespread attention (Bao et al., 2021;Liu & Shah, 2023;Liang et al., 2024;D'Arcy et al., 2024;Jin et al., 2024), including summarizing research paper content (Collins et al., 2017), detecting inaccuracies (Nuijten et al., 2016), and identifying fairness disparities (Zhang et al., 2022). Hosseini & Horbach (2023) conducted small-scale qualitative experiments to evaluate the effectiveness of ChatGPT in the peer review process, while Robertson (2023) invited 10 participants to assess the benefits of GPT-4 in assisting with peer review. Lu et al. (2024) and Tyser et al. (2024) used GPT-4 to evaluate full-text PDFs of scientific papers. However, when LLMs act as judges, even the most advanced models, such as GPT-4 (Achiam et al., 2023) and Gemini (Reid et al., 2024), still lag behind reward models specifically trained for the task, as seen in RewardBench (Lambert et al., 2024). This gap highlights the challenge of achieving human-level judgment and reasoning in AI-driven peer reviews. In contrast, we train a Generative Reward Model (Zhang et al., 2024) to simulate a comprehensive peer review. Our CycleReviewer simulates reviewers with varying perspectives, documenting summaries, strengths, and weaknesses. In the final stage, a primary reviewer consolidates these insights to deliver the final decision.",
            "score": 0.3838882065532446,
            "section_title": "RELATED WORK",
            "char_start_offset": 25924,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 292
                },
                {
                    "start": 295,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1724
                }
            ],
            "ref_mentions": [
                {
                    "start": 427,
                    "end": 445,
                    "matchedPaperCorpusId": "235324633"
                },
                {
                    "start": 462,
                    "end": 481,
                    "matchedPaperCorpusId": "263608784"
                },
                {
                    "start": 612,
                    "end": 634,
                    "matchedPaperCorpusId": "265505419"
                },
                {
                    "start": 695,
                    "end": 720,
                    "matchedPaperCorpusId": "257310820"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75634765625
        },
        {
            "corpus_id": "267740366",
            "title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?",
            "text": "This study provides insights into the methods we use to evaluate language model generations, focusing on when automatic metrics align with human annotators and what the best metric is under different scenarios. We are the first to do a broader meta-evaluation study where we compare GPT-4as-a-judge and traditional metrics with a methodology that allows for reliable comparisons between metrics. We recommend using Pairwise Accuracy (PA) with Tie Calibration for meta-evaluation. This method effectively handles ties, which are prevalent when using human and GPT-4 ratings, making it a reliable tool for assessing metric performance against human ratings. \n\nOur main finding is that GPT-4 shows strong alignment with human judgments for short-answer tasks, but only when gold references are provided. The reliability drops significantly without gold references, as the model is overly positive compared to human evaluations. The issue is particularly evident in free-form tasks, which are tasks where LLMas-a-judge could be the most valuable and where gold labels are typically not available. When gold references are available, we observe that GPT-4 is too strict compared to humans, relying to much on the gold label. For these type of tasks, even though LLM-as-a-judge is often applied to them, human evaluations still remain the gold standard. \n\nROUGE-L performs comparably to GPT-4-gold for short-answer tasks, offering a cost-effective alternative in scenarios where the use of GPT-4 is limited by cost or time constraints. For long-answer tasks, while BERTSCORE demonstrates strong performance, it does not fully replace the need for GPT-4-gold. These metrics provide valuable insights but vary in effectiveness depending on the specific task. \n\nEvaluating non-English outputs, such as Swedish, presents additional challenges. There is a significant drop in alignment for GPT-4-no-gold, which highlights that GPT-4 as a judge is less reliable for languages other than English. While this is true for Swedish, we expect these findings to be more sever for lesser-resourced languages and those less similar to English. Future work should focus on expanding this study to more languages. \n\nAs we have observed, there is a large variation in alignment with human ratings for all metrics across task types.",
            "score": 0.3838541054200469,
            "section_title": "Conclusions and Future Work",
            "char_start_offset": 25473,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 655
                },
                {
                    "start": 658,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1347
                },
                {
                    "start": 1350,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1750
                },
                {
                    "start": 1753,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2191
                },
                {
                    "start": 2194,
                    "end": 2308
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.347900390625
        },
        {
            "corpus_id": "273404050",
            "title": "SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation",
            "text": "Specifically, the best reasoning scores for SBI-RAG, GPT-4, and GPT-3.5 Turbo were 0.588, 0.491, and 0.290, respectively. Paired sample t-tests showed that the differences between the SBI-RAG and the GPT models were significantly different at the 0.05 level (see Appendix E). These results suggest that schema-based reasoning can enhance the overall quality of reasoning, particularly in educational contexts, when compared to responses generated by LLMs alone. \n\nLLM-as-a-Judge Results: We implemented the LLM-as-a-Judge approach [36] to evaluate the quality of reasoning in the responses generated by both our Schema-Based RAG system and the baseline LLMs. This method allows for an objective, scalable evaluation by approximating human judgment through the use of LLMs. Our LLM-as-a-Judge process involves scoring responses based on clarity, logical progression, and completeness. Results showed that the Schema-Based RAG approach consistently outperformed GPT-4 and GPT-3.5 Turbo in terms of reasoning quality.For more details refer to Appendix G.",
            "score": 0.3830925536089262,
            "section_title": "Evaluation",
            "char_start_offset": 11848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1051
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.128173828125
        },
        {
            "corpus_id": "271903501",
            "title": "See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering LLM Weaknesses",
            "text": "To validate the effectiveness, we first compare the gold labels and model-evaluate labels of GPT-4 responses on the full 1,835 instances, where GPT-4 evaluation achieves 85.35 precision, 95.89 recall and 90.31 F-1. As GPT-4 evaluation can be biased to GPT-4 output (Liu et al., 2023), we further construct a small set that consists of 115 instances, where each gold response is completely annotated by human annotators without the assistance of GPT-4. The GPT-4 evaluation achieves 87.14 precision, 96.83 recall and 91.73 F-1 on this set, which we consider is good enough. Such a higher-recall and lower-precision evaluation indicates that GPT-4 is highly reliable in identifying correct answers but with a few errors in classifying incorrect responses as \"correct\". It also suggests that the model performance evaluated by GPT-4 can be slightly overestimated or higher than its real performance. \n\nWe also experiment with the evaluation method following Wang et al. (2023), i.e., asking LLMs to explain why the model-generated response is correct or not, and then give a label for the response. However, we do not observe significant improvement.",
            "score": 0.38282461624519193,
            "section_title": "I Evaluation Validation",
            "char_start_offset": 48405,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 896
                },
                {
                    "start": 899,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1147
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11419677734375
        },
        {
            "corpus_id": "258999338",
            "title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing",
            "text": "Given the rapid ascent of large language models (LLMs), we study the question: (How) can large language models help in reviewing of scientific papers or proposals? We first conduct some pilot studies where we find that (i) GPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to identify errors) outperforms prompting to simply write a review. With these insights, we study the use of LLMs (specifically, GPT-4) for three tasks: 1. Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers. We observe that the LLM finds errors in 7 of them, spanning both mathematical and conceptual errors. 2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist questions in the respective sections of 15 NeurIPS 2022 papers. We find that across 119 {checklist question, paper} pairs, the LLM had an 86.6% accuracy. 3. Choosing the\"better\"paper: We generate 10 pairs of abstracts, deliberately designing each pair in such a way that one abstract was clearly superior than the other. The LLM, however, struggled to discern these relatively straightforward distinctions accurately, committing errors in its evaluations for 6 out of the 10 pairs. Based on these experiments, we think that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not (yet) for complete evaluations of papers or proposals.",
            "score": 0.3826519984850574,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81982421875
        },
        {
            "corpus_id": "258947321",
            "title": "Benchmarking large language models for biomedical natural language processing applications and recommendations",
            "text": "As the quantitative results on text summarization and generation demonstrated, commonly used automatic evaluations such as Rouge, BERT, and BART scores consistently favored the fine-tuned BART's generated text, while manual evaluations show different results, indicating that GPT-3.5 and GPT-4 had competitive accuracy and much higher readability even under the zero-shot setting. Existing studies also reported that the automatic measures on LLM-generated text may not correlate to human preference 35,47 . The MS^2 benchmark used in the study also discussed the limitation of automatic measures, specifically for text summarization 50 . Additionally, the results highlight that completeness is a primary limitation when adapting GPT models to biomedical text generation tasks despite its competitive accuracy and readability scores. \n\nLast, our evaluation on both performance and cost demonstrates a clear trade-off when using LLMs in practice. GPT-4 had the overall best performance in the 12 benchmarks, with an 8% improvement over GPT-3.5 but also at a higher cost (60 to 100 times higher than GPT-3.5). Notably, GPT-4 showed significantly higher performance, particularly in question-answering tasks that involve reasoning, such as over 20% improvement in MedQA compared to GPT-3.5. This observation is consistent with findings from other studies 27,38 . Note that newer versions of GPT-4, such as GPT-4 Turbo, may further reduce the cost of using GPT-4. \n\nThese findings lead to recommendations for downstream users to apply LLMs in BioNLP applications, summarized in Fig. 4. It provides suggestions on which BioNLP applications are recommended (or not) for LLMs, categorized by conditions (e.g., the zero/few-shot setting (B) (C) Fig. 3 | Qualitative evaluation results on accuracy, completeness, and readability. A The overall results of the fine-tuned BART, GPT-3.5 zero-shot, GPT-4 zeroshot, and LLaMA 2 zero-shot models on a scale of 1 to 5, based on random 50 testing instances from the PubMed Text Summarization dataset.",
            "score": 0.382637874311413,
            "section_title": "Discussions",
            "char_start_offset": 34796,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 834
                },
                {
                    "start": 837,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 2034
                }
            ],
            "ref_mentions": [
                {
                    "start": 500,
                    "end": 503,
                    "matchedPaperCorpusId": "261242750"
                },
                {
                    "start": 503,
                    "end": 505,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 634,
                    "end": 636,
                    "matchedPaperCorpusId": "233231380"
                },
                {
                    "start": 1353,
                    "end": 1356,
                    "matchedPaperCorpusId": "265466787"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36279296875
        },
        {
            "corpus_id": "274777190",
            "title": "LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation",
            "text": "Possess Bias? \n\nWe evaluate whether LLM-as-an-Interviewer exhibits two biases-Verbosity and Selfenhancement bias-that are common in modelbased evaluation (Zheng et al., 2023b). \n\nVerbosity Bias To analyze whether the LLM-asan-Interviewer framework prefers longer answers, we analyze the correlation between answer length and score for each number of interactions with Interviewer. We use the long-form generation task, Depth-QA, for this analysis. As the Interviwer, we use GPT-4o and as the interviewees, we use seven different models. 4 igures 3 show that as the number of interaction increases, the linear correlation (r) between length and score weakens. Without any interaction with Interviewer (same as the LLM-as-a-Judge setting), we observe a statistically significant linear correlation among answer length and its score(r=.371, p<0.05). However, as the interactions progress, the p-value increases and the r-value decreases. This suggests that the interactions in our interview process help reduce verbosity bias, alongside the length control mechanism that adjusts win rates or scores (Dubois et al., 2024a). \n\nSelf-Enhancement Bias To examine whether interviewer models favor their own responses, we analyze score trends using GPT-4o, Llama-3.1-70B, and Llama-3.1-8B as both interviewers and interviewees. As shown in Figure 4, no significant selfenhancement bias is observed. Both Llama-3.1-8B and GPT-4o show little to no self-enhancement bias, generally giving themselves lower scores compared to other interviewers. Llama-3.1-70B consistently gives itself higher scores compared to evaluations by GPT-4o or Llama-3.1-8B, but this does not change its overall ranking. Interestingly, this trend weakens as interactions progress, with the score gap between GPT-4o (the top model) and Llama-3.1-70B increasing. A similar pattern is observed with the DepthQA dataset as discussed in Appendix B.1.",
            "score": 0.38242493062746846,
            "section_title": "Does LLM-AS-AN-INTERVIEWER",
            "char_start_offset": 15904,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 16,
                    "end": 176
                },
                {
                    "start": 179,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1907
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1163330078125
        },
        {
            "corpus_id": "270562487",
            "title": "AI-Assisted Human Evaluation of Machine Translation",
            "text": "Despite the advantages in lower costs per error span of the presented setup, we urge practitioners to not use this approach when metrics evaluation is one of the expected tasks due to the particular bias to the used metric in the setup. The intended application of this pipeline is purely a more efficient evaluation of machine translation system quality. \n\nBoth ESA AI and GEMBA rank GPT-4-5shot as the best system, a system that uses the same LLM to translate sentences as we use to generate for GEMBA. This indicated a weakness that our approach is biased towards systems build on top of the same underlying LLM. Liu et al. (2023) described this phenomena when the same system used for generating output should not be used to also evaluate them. This issue could be mitigated by using two different LLMs to generate error spans. \n\nLastly, we use GEMBA, a GPT4-based system, for the quality estimation and work with WMT 2023 data. Unfortunately, we can not exclude the possibility of the QE system being trained on this data, though the texts and scores are kept in two separate large files with non-linear mappings.",
            "score": 0.38225216838559817,
            "section_title": "Limitations",
            "char_start_offset": 13098,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1118
                }
            ],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 633,
                    "matchedPaperCorpusId": "257804696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2391357421875
        },
        {
            "corpus_id": "268856493",
            "title": "METAL: Towards Multilingual Meta-Evaluation",
            "text": "We presented the first framework for end-to-end evaluation of LLMs as evaluators in multilingual scenarios.We created a dataset of 1000 summaries across 10 languages rated by native speakers on 5 different metrics.Our dataset covers a range of summaries in terms of linguistic acceptability, output quality, task quality, and others.We do this by systematically prompting GPT-4 to generate summaries of varying quality.The human ratings obtained for these summaries are of high quality with \u03ba > 0.6 and F 1 > 0.75.We plan to make the METALdataset available to the research community.Using our dataset, we investigate the capabilities of three LLMs as evaluators: GPT-3.5-Turbo,GPT-4, and PaLM2 using two prompting strategies and compare their evaluation with the METALhuman evaluations.Our results show that GPT-4 with detailed instructions performs closest to humans, while GPT-3.5-Turbo is not a suitable multilingual evaluator but surprisingly does better than GPT-4 and PaLM2 in some metrics for Bengali.We also show that GPT-4 with detailed instructions does best when there is disagreement amongst human annotators.We compare the overlapping summaries between SEAHORSE and METAL and show how our metrics and prompting methods can be used to compare generations from different models.Finally, we analyze human and LLM reasoning and observe that LLMs often provide incorrect justifications for their scores, thus showing that more research is needed to be able to use LLM-based evaluators with confidence in the multilingual setting.",
            "score": 0.38206549404780277,
            "section_title": "Conclusion",
            "char_start_offset": 28580,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 214
                },
                {
                    "start": 214,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 583
                },
                {
                    "start": 583,
                    "end": 677
                },
                {
                    "start": 677,
                    "end": 786
                },
                {
                    "start": 786,
                    "end": 1008
                },
                {
                    "start": 1008,
                    "end": 1121
                },
                {
                    "start": 1121,
                    "end": 1289
                },
                {
                    "start": 1289,
                    "end": 1537
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1783447265625
        },
        {
            "corpus_id": "261100760",
            "title": "GPTEval: A Survey on Assessments of ChatGPT and GPT-4",
            "text": "It remains uncertain whether the examined data, such as public datasets and scientific knowledge, have been inadvertently exposed during the training of ChatGPT and GPT-4. These factors may contribute to an unfair comparison between LLMs and their respective baselines. \n\nThe contributions of this work are threefold: \n\n(1) We conduct a comprehensive survey of recent assessments focusing on the language proficiency and scientific knowledge of ChatGPT and GPT-4. \n\n(2) We compare their assessment results across various tasks and disciplines to highlight the strengths and weaknesses of the GPT models. (3) We critically analyze the existing assessment methods employed, offering recommendations for future evaluation studies and delivering our ethical considerations associated with the GPT models. \n\nGeneration was almost only evaluated on textto-text generation tasks (but not data-to-text). For machine translation (MT), an early assessment suggested SOTA MT systems could defeat ChatGPT by a large margin (Bang et al., 2023). Later on, by testing on more language pairs, more datasets, and better prompts, Hendy et al. (2023) found ChatGPT yielded competitive performance for high-resource languages, but still had limited capabilities for lowrecourse languages. This was further validated by Jiao et al. (2023), where the authors additionally concluded that ChatGPT was not good at translating between distant languages and translating sentences that contain domain biases or noise, compared to commercial MT systems (e.g., DeepL and Google Translate), but the gap could be narrowed for GPT-4. However, through a large-scale human evaluation and error analysis by expert translators, Karpinska and Iyyer (2023) had very different conclusions. They found that, when doing paragraph-level translation, ChatGPT's translations were overwhelmingly preferred compared to those from Google Translate. ChatGPT could largely reduce errors, including mistranslation, grammatical errors, inconsistency errors, and more.",
            "score": 0.3817555180152991,
            "section_title": "Introduction",
            "char_start_offset": 2220,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 269
                },
                {
                    "start": 272,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2015
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2333984375
        },
        {
            "corpus_id": "259073477",
            "title": "Evaluation of the performance of GPT-3.5 and GPT-4 on the Medical Final Examination",
            "text": "Both GPT-3.5 and GPT-4 performance was validated on the Massive Multitask Language Understanding (MMLU) test [6]. GPT-4 model outperformed other models not only in the English version but also after translation of the test to other languages (even those rarely used like Latvian, Welsh, or Swahili) [7]. \n\nIn order to incorporate GPT-3.5/GPT-4 into a specific field it needs to be further validated in the field-specific tests. In medicine, the expertise of healthcare professionals is crucial in ensuring accurate diagnosis, effective treatment, and patients' safety. To maintain a high standard of medical practice, rigorous assessment methods, such as different medical final examinations, are employed to evaluate the competency of medical graduates before they begin practicing independently. Such examinations cover a wide range of medical knowledge, including theoretical concepts, clinical reasoning, and practical skills, making it a suitable benchmark for evaluating the performance of LLMs in the medical domain [8], [9]. Results of validation analysis of GPT-3.5 on numerous medical examinations have been recently published. GPT-3.5 and GPT-4 were already validated on, to our knowledge, several national medical tests like the United States Medical Licensing Examination (USMLE) [9], Japanese [8], and Chinese National Medical Licensing Examinations [10], [11], on couple of medical benchmark databases like MedQA, PubMedQA, MedMCQA, and ABMOSS [12]- [14]. GPT-3.5 was also evaluated in terms of its usability in the decision-making process. Rao et al. reported that GPT-3.5 achieved over 88% accuracy by being validated using the questionnaire regarding the breast cancer screening procedure [15]. However, to the best of our knowledge, there have been no studies yet presenting the capabilities of GPT-3.5 and GPT-4 in terms of European-based medical final examinations. \n\nIn this paper, we hence aimed to investigate the utility of GPT-3.5 and GPT-4 in the context of Polish Medical Final Examination.",
            "score": 0.38146359838942523,
            "section_title": "Introduction",
            "char_start_offset": 1929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 303
                },
                {
                    "start": 306,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1886
                },
                {
                    "start": 1889,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 112,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 1028,
                    "end": 1031,
                    "matchedPaperCorpusId": "254876189"
                },
                {
                    "start": 1293,
                    "end": 1296,
                    "matchedPaperCorpusId": "254876189"
                },
                {
                    "start": 1364,
                    "end": 1368,
                    "matchedPaperCorpusId": "260888866"
                },
                {
                    "start": 1370,
                    "end": 1374,
                    "matchedPaperCorpusId": "258505880"
                },
                {
                    "start": 1707,
                    "end": 1711,
                    "matchedPaperCorpusId": "256626649"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061767578125
        },
        {
            "corpus_id": "267212206",
            "title": "Unmasking and Quantifying Racial Bias of Large Language Models in Medical Report Generation",
            "text": "For instance, it frequently avoids formulating treatment plans or predicting outcomes, as reflected in Figure 2d's comparison of inconclusive predictions between the two models (a) GPT-3.5-turbo 16.25% v. GPT-4 29.46% for inconclusive cost prediction; and (b) GPT-3.5-turbo 18.79 v. GPT-4 38.31% for inconclusive hospitalization prediction. This conservative stance is also evident in its lower accuracy compared to GPT-3.5-turbo (GPT-3.5-turbo 59.30% v. GPT-4 31.49%, figure 2c) in predicting deceased outcomes. GPT-4 often resorts to generic advice like 'consult with healthcare providers', which might be insufficient for accurate medical guidance. The challenge lies in balancing caution with the need for precise, high-stakes predictions. Additionally, GPT-4's longer response times and higher operating costs (as of this writing, the cost of GPT-4 is approximately 30 times higher than that of GPT-3.5-turbo) limit its practical utility in real-world scenarios. In practice, our expected wait time to not trigger OpenAI's API error is ~2 seconds for GPT-3.5-turbo, and ~15 seconds for GPT-4. \n\nThis study focuses on illustrating bias in LLMs, such as GPT-3.5-turbo and GPT-4. Transformer-based models, including GPTs 2 , generate text based on previous tokens, meaning altering one token or the language prior can change subsequent token distributions. Although OpenAI has implemented RLHF to discourage problematic outputs in LLMs 2,12 , our findings indicate that these models still exhibit inherent biases, especially in relation to race. Moreover, our study highlights that discouraging 'harmful' outputs in LLMs can lead to an overly optimistic bias, especially in critical scenarios. Both GPT variants displays a high degree of optimism when predicting death outcomes, with GPT-4's accuracy in predicting deceased outcomes only 31.49% compared to 59.30% for GPT-3.5-turbo (Figure 2c).",
            "score": 0.3813232654317268,
            "section_title": "Main",
            "char_start_offset": 6344,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 1896
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07891845703125
        },
        {
            "corpus_id": "273404356",
            "title": "MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback",
            "text": "To explore RQ3, we utilized ratings generated during the MCQG-SRefine critique step as LLM-as-Judge (rating) scores. \n\nFor LLM-as-Judge (comparison), we employed the same guidelines and settings used in the human evaluation from RQ1 in main results, prompting the model to choose a preferred output between the two system-generated questions (e.g., GPT-4 and MCQG-SRefine). To ensure the results were not biased by the inherent positional preferences of the LLM (e.g., position bias) (Wang et al., 2023a;Zheng et al., 2023;Zeng et al., 2023), we collected preferences from two sequence settings: one where the GPT-4 generated output was shown first followed by the MCQG-SRefine output, and the reverse order. We filtered out preferences that did not match across the two sequence settings, using the remaining data for further analysis. To ensure the results were not influenced by length bias (i.e., LLMs like GPT-4 prefer longer generations during their automatic evaluation) (Wang et al., 2023c;Zeng et al., 2023), we deliberately included those data points in the evaluation data which had the least context length ratio for the initial and the MCQG-SRefine questions. These examples constituted half the human evaluation data and the rest were chosen randomly from the dataset. Notably, the guidelines used in human evaluation, LLM-as-Judge (rating), and LLM-as-Judge (comparison) were consistent. Thus, by standardizing the evaluation data, we could fairly compare the judgments of Expert X and LLM-as-Judge (rating/comparison) under identical settings to assess the reliability of LLM-as-Judge. \n\nTo investigate RQ4, we prompted various LLMs 18 using the LLM-as-Judge (comparison) settings and compare the preference with Expert X.",
            "score": 0.38083552099324236,
            "section_title": "D.1 Settings",
            "char_start_offset": 40330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 119,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1601
                },
                {
                    "start": 1604,
                    "end": 1738
                }
            ],
            "ref_mentions": [
                {
                    "start": 978,
                    "end": 998,
                    "matchedPaperCorpusId": "259108263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.055419921875
        },
        {
            "corpus_id": "272463832",
            "title": "VERA: Validation and Evaluation of Retrieval-Augmented Systems",
            "text": "Traditionally, RAG models were evaluated based on their performance in specific downstream tasks, utilizing established metrics like EM and F1 scores for entity or sentiment classification, BERTScore and MoverScore for question answering, or accuracy for fact-checking [12,21,31,36,41,42]. Tools like RALLE automate this process using task-specific metrics [15]. State-of-the-art evaluation tools such as EXAM and RAGAS propose various quantifications for RAG retrieval and generation effectiveness, including context relevance and answer faithfulness [2,25]. BARTScore and SelfCheckGPT focus on generation factuality and coherency. RAG evaluation also encompasses abilities indicative of its adaptability and efficiency: noise robustness, negative rejection, counterfactual robustness, and guideline adherence [6,19]. \n\nDespite developments in evaluation metrics and tools, quantifying different aspects in RAG remains challenging due to uncertainties in inputs and outputs and limitations of existing benchmarks in capturing human preferences. The Large Model Systems Organization (LMSYS) group explores the feasibility and pros/cons of using various LLMs as automated judge for tasks in writing, math, and world knowledge [43]. Their results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. The G-EVAL proposed by Microsoft Cognitive Services Research group with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, along with other studies confirming GPT's ability to achieve state-of-the-art or competitive correlation with human judgments [20,32]. Furthermore, several initiatives leverage LLM prompting to evaluate performance across diverse tasks such as translation, summarization, and dialogue [14]. These studies point out that LLMs offer a scalable and explainable alternative to human evaluation, which are otherwise very expensive to obtain [43]. \n\nLastly, given that RAGs rely on a retrieval model to retrieve relevant documents, their performance is pegged to the efficacy of the semantic search within the retriever.",
            "score": 0.38018598132811776,
            "section_title": "RELATED WORK",
            "char_start_offset": 3371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2043
                },
                {
                    "start": 2046,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 273,
                    "matchedPaperCorpusId": "263830898"
                },
                {
                    "start": 811,
                    "end": 814,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 1225,
                    "end": 1229,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 2038,
                    "end": 2042,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38232421875
        },
        {
            "corpus_id": "270737909",
            "title": "BADGE: BADminton report Generation and Evaluation with LLM",
            "text": "The experimental results are presented in Table 3.Most evaluators rated the report generated by GPT-4 as the best, while preferring the human-written report over the one generated by GPT-3.5.This finding contradicts the evaluation by GPT-4, where GPT-3.5 outperformed humans.This bias aligns with observations from the G-EVAL paper [Liu et al., 2023], which compared the GPT-4 and human evaluation and found that G-EVAL prefers the output generated by LLMs.Additionally, the Pearson product-moment correlation coefficient between the GPT-4 evaluation and human evaluation is calculated to be 0.333, indicating a small positive correlation between the two evaluations.\n\nFigure 6 is the pie chart illustrating the percentage of correct guesses for each report.The accuracy rates are as follows: human reports 80%, GPT-3 80%, and GPT-4 70%.These results indicate that evaluators can readily discern the author of the report in most cases, suggesting differences in the stylistic characteristics between reports authored by humans and those generated by LLMs.The examples of the reports can be found in the Appendix A.",
            "score": 0.37955109953353117,
            "section_title": "Result for Human Evaluation",
            "char_start_offset": 16739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 50,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 275
                },
                {
                    "start": 275,
                    "end": 457
                },
                {
                    "start": 457,
                    "end": 667
                },
                {
                    "start": 669,
                    "end": 758
                },
                {
                    "start": 758,
                    "end": 837
                },
                {
                    "start": 837,
                    "end": 1055
                },
                {
                    "start": 1055,
                    "end": 1114
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37451171875
        },
        {
            "corpus_id": "263914461",
            "title": "Evaluating GPT-4\u2019s proficiency in addressing cryptography examinations",
            "text": "In (Kung et al., 2023) the authors evaluate GPT-3.5's performance on the United States Medical Licensing Exam (USMLE) Step 1, Step 2CK, and \n\nStep 3 without specialized training. ChatGPT achieved accuracy levels exceeding the pass thresholds, indicating its potential to assist with medical education and clinical decision-making. The study highlights ChatGPT's increasing accuracy over previous models, surpassing 60% accuracy, and suggests its proficiency in handling broader clinical content. ChatGPT's performance varied across exam steps, mirroring the perceived difficulty, and the study discusses its potential to enhance medical education and question-explanation writing processes. It also emphasizes the need for standardized research methods to assess human-AI interactions in medical education. Nori et al. (2023) evaluates the performance of GPT-4 on medical competency examinations and benchmark datasets, including the United States Medical Licensing Examination (USMLE) and the MultiMedQA suite. GPT-4 surpasses the passing scores for USMLE exams by more than 20 points and outperforms both earlier general-purpose models (GPT-3.5) and models specifically fine-tuned for medical knowledge. The model also demonstrates improved calibration of answer probabilities. The study explores GPT-4's ability to explain medical reasoning, personalize explanations, and create counterfactual scenarios. It suggests potential uses of GPT-4 in medical education and clinical practice while emphasizing the need for caution, error consideration, and risk mitigation in real-world applications. Choi et al. (2023) examines GPT-3.5 autonomous performance in answering law school exam questions at the University of Minnesota Law School. ChatGPT achieved an average grade equivalent to a C \u00fe student across four courses, including multiple-choice and essay questions, without human assistance. The findings suggest potential applications of ChatGPT in legal education and writing. The paper also offers guidance on prompt engineering for legal writing, addressing tone, word limits, citations, and essay length. While GPT-3.5 showed promise, the study emphasizes the need for prudent use, considering possible errors and challenges in real-world scenarios.",
            "score": 0.3793439149525364,
            "section_title": "Medicine",
            "char_start_offset": 5584,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2110
                },
                {
                    "start": 2111,
                    "end": 2255
                }
            ],
            "ref_mentions": [
                {
                    "start": 3,
                    "end": 22,
                    "matchedPaperCorpusId": "254876189"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1390380859375
        },
        {
            "corpus_id": "271974813",
            "title": "Awes, Laws, and Flaws From Today's LLM Research",
            "text": "Although there is a push to use LLMs to perform automated evaluation, there is no consensus on the viability of this approach: arguments in favour [21,22,23,24,25] are as plentiful as arguments against [26,27,28,29,24,30]. Finally, LLMs might memorise their training data [31,32], including evaluation benchmarks [33]. Since the models are sensitive to the prompt's phrasing [34,35], the benchmarks turn into an arena to test prompts, rather than a holistic bar of performance. \n\nOverall, these challenges impede the scientific community's ability to transparently evaluate and understand LLMs, and to ultimately ensure their responsible use. They also raise questions about the validity and trustworthiness of some findings and claims related to this technology and its capabilities, and illustrate a problem endemic to computer science. \n\nThe goal of this work is to systematically and critically examine the scientific methodology employed in LLM research, and to quantify the extent to which these issues occur in the literature. 2 To our knowledge, ours is the first study to systematically evaluate LLM research based on the metrics the field recommends and requires. To do this, we evaluated 2,054 works that cited the peer-reviewed GPT-3 paper [36] (as opposed to the preprint) and the GPT-4 technical report [37], labelled based on criteria as shown in Table 1. Most criteria are part of the reproducibility checklist for premier conferences 3 and considered good practices in the field. Some criteria are not part of the checklists: the type of evaluator used is because of the aforementioned lack of consensus around the reliability of LLMs as evaluators, added to their increasingly frequent use. Claims of super-human capabilities, reasoning abilities, or emergent behaviour relate to conclusions often drawn and reported outside of academic channels, and that could require closer examination-for example, it is known that more robust statistical tests prove that emergent abilities do not actually occur [41].",
            "score": 0.3792152071277084,
            "section_title": "Introduction",
            "char_start_offset": 4012,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 154,
                    "matchedPaperCorpusId": "258461287"
                },
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "257804696"
                },
                {
                    "start": 157,
                    "end": 160,
                    "matchedPaperCorpusId": "268363952"
                },
                {
                    "start": 160,
                    "end": 163,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 209,
                    "end": 212,
                    "matchedPaperCorpusId": "266166678"
                },
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "269293221"
                },
                {
                    "start": 215,
                    "end": 218,
                    "matchedPaperCorpusId": "268363952"
                },
                {
                    "start": 218,
                    "end": 221,
                    "matchedPaperCorpusId": "261822638"
                },
                {
                    "start": 272,
                    "end": 276,
                    "matchedPaperCorpusId": "247450984"
                },
                {
                    "start": 276,
                    "end": 279,
                    "matchedPaperCorpusId": "258186969"
                },
                {
                    "start": 313,
                    "end": 317,
                    "matchedPaperCorpusId": "264555419"
                },
                {
                    "start": 375,
                    "end": 379,
                    "matchedPaperCorpusId": "233296494"
                },
                {
                    "start": 1252,
                    "end": 1256,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 2019,
                    "end": 2023,
                    "matchedPaperCorpusId": "258418299"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09844970703125
        },
        {
            "corpus_id": "267783118",
            "title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
            "text": "In this work, we evaluate the ability of LLMs to write surveys on NLP concepts. We find that LLMs, particularly GPT-4, can author surveys following specific guidelines that rival the quality of human experts, even though there are shortcomings such as incomplete information. Our findings also indicate that GPT-4 may not be a perfect replacement for human judgment when evaluating human-composed texts, and certain biases exist when asking it to rate machine-generated texts. Nevertheless, the results imply that these advanced generative LLMs could play a transformative role in the realm of education. They hold the promise of effectively structuring domain-specific knowledge tailored to general learners. This adaptability could potentially lead to a more interactive and personalized learning experience, enabling students to engage in query-driven studies that cater directly to their unique curiosities and learning objectives.",
            "score": 0.3790626114532596,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 14399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 935
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.441650390625
        },
        {
            "corpus_id": "271212285",
            "title": "DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems",
            "text": "Evaluation Process Our dataset diversity poses two major evaluation challenges: (i) The evaluation methods vary depending on the answer type.For example, for boolean or numerical answers, a fair evaluator only needs to verify the correctness of a binary yes/no response or a specific number using simple techniques like string matching or number extraction.In contrast, textual responses require more nuanced standards such as natural language generation (NLG) metrics.Thus, accurately determining the appropriate evaluation method becomes complex when the answer type is unknown.\n\n(ii) Different LLMs and systems exhibit substantial variations in the organization and style of their outputs, potentially leading to biases in traditional evaluation approaches.Therefore, we capitalize on the prowess of LLMs that have proven to be decent evaluators and can be easily adapted to the assessment of various answer types [14,24,39].Inspired by Liu et al. [24], we clearly define the evaluation criteria for various types within the instruction prompt and then instruct GPT-4 to assign a score of 0 (incorrect) or 1 (correct).After evaluating 200 examples by both human evaluators and GPT-4, we found that the GPT-4 automatic evaluator shows a 98% agreement with human annotators, significantly exceeding the traditional string matching approach.Details of this experiment is shown in Table 3, and details of evaluation instruction prompts are attached in Appendix A. Metrics As mentioned above, we instruct GPT-4 to assign a score of 0 (incorrect) or 1 (correct), thus using Accuracy (abbreviated as Acc.) to measure system performance.We report accuracy across all instances, as well as for each domain and QA-pair type in Table 4.\n\n3 Experiments and Analysis",
            "score": 0.3790613114962421,
            "section_title": "Evaluation Setup",
            "char_start_offset": 10648,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 141,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 469
                },
                {
                    "start": 469,
                    "end": 580
                },
                {
                    "start": 582,
                    "end": 760
                },
                {
                    "start": 760,
                    "end": 928
                },
                {
                    "start": 928,
                    "end": 1121
                },
                {
                    "start": 1121,
                    "end": 1341
                },
                {
                    "start": 1341,
                    "end": 1632
                },
                {
                    "start": 1632,
                    "end": 1728
                },
                {
                    "start": 1730,
                    "end": 1756
                }
            ],
            "ref_mentions": [
                {
                    "start": 921,
                    "end": 924,
                    "matchedPaperCorpusId": "257804696"
                },
                {
                    "start": 924,
                    "end": 927,
                    "matchedPaperCorpusId": "257378627"
                },
                {
                    "start": 951,
                    "end": 955,
                    "matchedPaperCorpusId": "257804696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.580078125
        },
        {
            "corpus_id": "270198572",
            "title": "Characterizing the Increase in Artificial Intelligence Content Detection in Oncology Scientific Abstracts From 2021 to 2023",
            "text": "Well-defined guidelines are needed to regulate ethical usage of AI in academic writing.Although LLMs might not be able to independently produce comprehensive and factual scientific writing, they could provide help in editing and improve the narrative of academic writing.GPT-4 was found to be able to assist researchers in revising medical abstracts and enhance the quality of the original writing. 27Medical domain-specific LLMs have also been created by fine-tuning open-source LLMs on biomedical academic papers to improve performance on benchmark tests. 28The academic writing ability of such models has yet to be explored.Even so, authors choosing to use AI to create content for scientific writing of abstracts and other publications should be held accountable for all aspects of the submission, requiring their due diligence to ensure factual and accurate representation of content with no misleading or inaccurate information.LLMs and AI can be used as tools for facilitation of scientific writing but they do not absolve authors of the full responsibility of the accuracy of the products they submit for publication.\n\nThe primary limitation of this study is the lack of a ground truth for AI content detection.Unfortunately, outside of study authors admitting to the use of AI, or content with clearly generated phrases such as \"As a LLM trained by OpenAI,\" we suspect that identification of generated text will never reach perfect accuracy and as demonstrated here, commonly used detectors do not show high agreement in likelihood scores.Therefore, AI content detectors should not be used as the sole  Univariable odds ratios and CIs for the association of each word with high probability AI content (using optimal thresholds that best discriminated true human-written/AI-generated abstracts) were computed for each AI content detector.Words with fewer than five characters or appearing (continued on following page)\n\nmeans to assess AI content on scientific writing but could be used as a screening tool to indicate that the presented content requires additional scrutiny from reviewers.\n\nIn conclusion, there is evidence of increased AI content within oncology scientific abstracts in 2023 compared with previous years, associated with online-only publications and lack of inclusion of clinical trial registration number.\n\nContemporary AI content detectors have the potential to be tuned to distinguish AI content and may be useful as a supplementary screening tool for scientific review.",
            "score": 0.3787106173709882,
            "section_title": "DISCUSSION",
            "char_start_offset": 20031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 87,
                    "end": 271
                },
                {
                    "start": 271,
                    "end": 401
                },
                {
                    "start": 401,
                    "end": 560
                },
                {
                    "start": 560,
                    "end": 627
                },
                {
                    "start": 627,
                    "end": 934
                },
                {
                    "start": 934,
                    "end": 1125
                },
                {
                    "start": 1127,
                    "end": 1219
                },
                {
                    "start": 1219,
                    "end": 1548
                },
                {
                    "start": 1548,
                    "end": 1846
                },
                {
                    "start": 1846,
                    "end": 1926
                },
                {
                    "start": 1928,
                    "end": 2098
                },
                {
                    "start": 2100,
                    "end": 2333
                },
                {
                    "start": 2335,
                    "end": 2500
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42333984375
        },
        {
            "corpus_id": "271310142",
            "title": "Check-Eval: A Checklist-based Approach for Evaluating Text Quality",
            "text": "The evaluation of automatically generated text has been a persistent challenge in the NLG field. Traditional metrics such as BLEU, ROUGE, and METEOR have been extensively used but have shown limitations in aligning with human judgment, particularly in tasks requiring creativity and nuance [Fabbri et al. 2021]. In recent years, more sophisticated evaluation frameworks leveraging LLMs have been proposed to address these shortcomings. Two recent methods are GPTSCORE [Fu et al. 2023] and G-EVAL [Liu et al. 2023]. \n\nGPTSCORE is a framework that utilizes generative pre-trained transformers (GPTs) and other language models to evaluate NLG outputs without relying on reference texts. The core idea is to assess the probability that the LLM assigns to the generated text, under the assumption that higher probabilities indicate higher quality. Fu et al. (2023) [Fu et al. 2023] demonstrated that GPTSCORE could achieve better correlations with human judgments compared to traditional metrics, especially in open-ended tasks such as dialogue generation and creative writing. However, despite its advancements, the method lacks interpretability and may be biased towards texts similar to those seen during the model's training phase. \n\nG-EVAL is another recent approach that leverages the capabilities of LLMs, specifically GPT-4, to improve NLG evaluation. Proposed by Liu et al. (2023) [Liu et al. 2023], G-EVAL introduces a chain-of-thought (CoT) [Wei et al. 2022] paradigm where the evaluation process is guided by detailed intermediate steps generated by the LLM. This method has shown improvements in correlation with human evaluations, particularly in tasks such as text summarization and dialogue generation. To address issues with score distribution and variability, G-EVAL employs a self-consistency strategy. Specifically, it generates multiple samples (n = 20) using different decoding parameters and averages the evaluation scores across these samples. This approach helps mitigate two key problems: the dominance of a single score (such as 3 on a 1-5 scale) and the tendency of LLMs to output integer scores, even when decimal values are requested.",
            "score": 0.37823057560128404,
            "section_title": "Related Work",
            "char_start_offset": 3484,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 514
                },
                {
                    "start": 517,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2159
                }
            ],
            "ref_mentions": [
                {
                    "start": 290,
                    "end": 310,
                    "matchedPaperCorpusId": "220768873"
                },
                {
                    "start": 468,
                    "end": 483,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 843,
                    "end": 859,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 860,
                    "end": 875,
                    "matchedPaperCorpusId": "964287"
                },
                {
                    "start": 1447,
                    "end": 1464,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65576171875
        },
        {
            "corpus_id": "264147087",
            "title": "Verbosity Bias in Preference Labeling by Large Language Models",
            "text": "The results are shown in Figure 3a for the overall result, and Figures 3b to 3d for results from each prompt. Both in the overall result and the individual results, there is a tendency for GPT-4 to prefer longer answers. When the word count difference is large enough, GPT-4 almost always prefers the longer answer. For question 1 and 2, the preference is smooth and clear, while for question 3, when the word count difference is small, there is high variance in evaluation. As we can see the shape varies between questions, and therefore we can deduce that verbosity does not rely entirely on word count and is different for each question. This makes adjusting for verbosity post-evaluation hard unless we know the verbosity preference shape for the prompt in question. \n\nFrom this experiment, we can draw the conclusion that GPT-4 generally prefers longer answers among those that are generated by the same LLM with the same prompt. However, this experiment by itself does not indicate that GPT-4 suffers from verbosity bias; it could be that the longer answers generated by vicuna are actually higher in quality and helpfulness. In order to truly measure verbosity bias, we would need the ground truth of each comparison which we do not have. Instead, we next utilize a dataset of human evaluations as the baseline. 3a combines results from all three questions and Figures 3b to 3d show results for each indivisual quesion. X-axis is the percentage of the difference between the first and the second option compared to the length of the second option. Y-axis is the actual score, with 1.0 meaning the first option was selected, 0 meaning it was a draw, and -1.0 meaning the second option was selected. There is an positive correlation between word count difference between the two options and the resulting evaluation. The data points are binned with each ranging 20%. The circles represent the average in each range and the errorbars show the standard deviation. \n\n5 Is There A Difference in Verbosity Preference Between LLMs and Humans? \n\nConsidering that LLMs replace humans as annotators in RLAIF, it is sufficient if LLMs could replicate human feedback and it does not necessarily have to be aligned with the ground truth.",
            "score": 0.37822880676533344,
            "section_title": "Verbosity Preference of LLMs",
            "char_start_offset": 14740,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 1966
                },
                {
                    "start": 1969,
                    "end": 2041
                },
                {
                    "start": 2044,
                    "end": 2230
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09930419921875
        },
        {
            "corpus_id": "261050476",
            "title": "ChatGPT applications in Academic Research: A Review of Benefits, Concerns, and Recommendations",
            "text": "Background ChatGPT has emerged as a valuable tool for enhancing scientific writing. It is the first openly available Large Language Model (LLM) with unrestricted access to its capabilities. ChatGPT has the potential to alleviate researchers\u2019 workload and enhance various aspects of research, from planning to execution and presentation. However, due to the rapid growth of publications and diverse opinions surrounding ChatGPT, a comprehensive review is necessary to understand its benefits, risks, and safe utilization in scientific research. This review aims to provide a comprehensive overview of the topic by extensively examining existing literature on the utilization of ChatGPT in academic research. The goal is to gain insights into the potential benefits and risks of using ChatGPT in scientific research, exploring secure and efficient methods for its application while identifying potential pitfalls to minimize negative consequences. Method The search was conducted in PubMed/MEDLINE, SCOPUS, and Google Scholar, yielding a total of 1279 articles and concluded on April 23rd, 2023. After full screening of titles/abstracts and removing duplicates and irrelevant articles, a total of 181 articles were included for analysis. Information collected included publication details, purposes, benefits, risks, and recommendation regarding ChatGPT\u2019s use in scientific research. Results The majority of existing literature consists of editorials expressing thoughts and concerns, followed by original research articles analyzing ChatGPT\u2019s performance in scientific research. The most significant advantage of using ChatGPT in scientific writing is its ability to expedite the writing process, enabling researchers to draft their work more efficiently. It also proves beneficial in improving writing style and proofreading by offering suggestions for sentence structure, grammar, and overall clarity. Additional benefits identified include support in data analysis, the formulation of protocols for clinical trials, and the design of scientific studies. Concerns mainly revolve around the accuracy and superficiality of the generated content, leading to what is referred to as \u201challucinations.\u201d Researchers have also expressed concerns about the tool providing citations to nonexistent sources. Other concerns discussed include authorship and plagiarism issues, accountability, copyright considerations, potential loss of diverse writing styles, privacy and security, transparency, credibility, validity, presence of bias, and the potential impact on scientific progress, such as a decrease in groundbreaking discoveries. Conclusion ChatGPT has the potential to revolutionize scientific writing as a valuable tool for",
            "score": 0.3781537573473806,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.441162109375
        },
        {
            "corpus_id": "266899541",
            "title": "The Critique of Critique",
            "text": "We compare multiple modern LLMs for AIU-level precision and recall tasks (Exp. II). The tested models include Zephyr (Tunstall et al., 2023), Wiz-ardLM, LLaMA-2 Chat, Vicuna, GPT 3.5, and GPT-4. Moreover, we randomly choose 100 AIUs and engage a postgraduate student (not the annotator) to perform the same task. This result can approximate a ceil performance (named as Human). We also introduce GPT-4 w/o ans, where we generate the reference answer and perform the precision task in one step. It aims to confirm the importance of pre-generating a reference answer. \n\nWe compare four variants of METACRI-TIQUE with two GPT-4 based methods introduced by Wang et al. (2023a) for Exp. III-V: MetaCritique GPT4 -P, MetaCritique GPT4 -R and MetaCritique GPT4 -F1 is respectively the precision score, recall score and F1 score of METACRITIQUE powered by GPT-4. MetaCritique Open -F1 is the F1 score of METACRITIQUE powered by open-source LLMs, whereby WizardLM 70B is used for the precision task, and WizardLM 13B is used for the recall task because they beat other open-source LLMs in the Table 3. Pairwise GPT4 is to compare two hypothesis critiques via GPT-4 and pick up the better one. The prompting instruction is shown in Table 16. Single GPT4 is to generate a likert score (1-7) for a hypothesis critique via GPT-4. The prompting instruction is shown in Table 17.",
            "score": 0.37810343025858556,
            "section_title": "Baseline",
            "char_start_offset": 15058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1364
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07879638671875
        },
        {
            "corpus_id": "270219020",
            "title": "An Empirical Analysis on Large Language Models in Debate Evaluation",
            "text": "Table 2 reveals that GPT-3.5 and GPT-4 match human evaluators in assessing debates, highlighting their effectiveness.Using 75 debates labeled by two of the authors enables a direct comparison with GPT-3.5 and GPT-4.They achieve significant accuracy and F1 scores-82.04%and 81.85% for GPT-3.5, and 86.22% and 86.01%for GPT-4, respectively, outperforming previous SOTA models.\n\nLLaMA2-70B, on the other hand, performs significantly worse than existing methods, being only comparable to the ruble-based method.Thus, it is less likely for LLaMA2 to be adopted as the automatic debate evaluator.Our further experiments for bias analysis therefore mainly focus on GPT-3.5 and GPT-4.Notably, the word choice in the prompt can have a profound impact on the performance of LLMs, as shown in Table 3. Within our study, employing the label set 1/-1 results in a marked decline in the performance of GPT-3.5, and using the label set Pro/Con leads to the lowest observed outcomes in GPT-4.GPT-3.5 is particularly sensitive to negative phrasing; its performance degrades below that of random selection when prompted to identify the debate's loser rather than the winner.In contrast, GPT-4 demonstrates much less sensitivity to such changes, showing only a minor decrease in performance.",
            "score": 0.37803128300358935,
            "section_title": "LLMs' Performance",
            "char_start_offset": 7860,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 215
                },
                {
                    "start": 215,
                    "end": 269
                },
                {
                    "start": 269,
                    "end": 314
                },
                {
                    "start": 314,
                    "end": 374
                },
                {
                    "start": 376,
                    "end": 507
                },
                {
                    "start": 507,
                    "end": 590
                },
                {
                    "start": 590,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 976
                },
                {
                    "start": 976,
                    "end": 1156
                },
                {
                    "start": 1156,
                    "end": 1272
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1251220703125
        },
        {
            "corpus_id": "275993778",
            "title": "Examining the Expanding Role of Synthetic Data Throughout the AI Development Pipeline",
            "text": "Different techniques were needed to generate the (synthetic) scores or ratings used in evaluation. The most common approach was to specify annotation guidelines for the auxiliary model to follow, similar to those that would be given to human raters. P10 believed this approach is slowly emerging as the gold standard for evaluating model-generated summaries: \"It's not perfect, but for better or worse, it's the [method] most calibrated with human judgments that exists. You give instructions on how GPT-4 should evaluate the summary, present the source document and the generated summary, and then GPT-4 assigns a score from 1 to 7 based on the rubric provided.\" \n\nParticipants described an ad-hoc approach to selecting which auxiliary model to use for data generation, with choices driven by immediate needs and availability rather than systematic criteria. Many relied on models readily accessible within their organizations or those considered \"state-of-the-art.\" P9 explained \"we've always operated on the idea that the newest model is going to be the best and the most helpful for our customers.\" P4 described choosing \"the most powerful LLM that [their team] feels is economically reasonable to prompt.\" Teams often matched models to task complexity, as P18 noted, using GPT-3.5 for \"straightforward tasks, such as cleaning up typos or translating text, \" while reserving GPT-4 quotas for more complex activities, such as generating math problems. \n\nTypical large, off-the-shelf models undergo alignment processes intended to ensure helpful and safe responses [cf. 72]. By design, this would limit the model's capacity to generate responses containing stereotypes or potentially offensive content. P5 recalled trying to use an 'aligned' auxiliary model for safety testing. When the primary model outputted \"I'm sorry, I cannot generate stereotypes,\" the auxiliary model would affirm with responses like \"Great, you should not,\" derailing the evaluation. For this reason, when working on safety-related tasks, such as fine-tuning a primary model to generate safe responses or evaluating for harms, participants often used what they believed were 'unaligned' auxiliary models.",
            "score": 0.3779379240401901,
            "section_title": "4.2.1",
            "char_start_offset": 31031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1454
                },
                {
                    "start": 1457,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2181
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.267578125
        },
        {
            "corpus_id": "275777664",
            "title": "EFL learners\u2019 motivation and acceptance of using large language models in English academic writing: an extension of the UTAUT model",
            "text": "However, the use of LLMs in teaching and learning academic writing is not without challenges. Concerns have been raised about the potential for academic dishonesty (Rudolph et al., 2023;van Dis et al., 2023), particularly as students might over-rely on AI-generated content, thereby undermining their critical and creative thinking during the writing process (Barrot, 2023). To mitigate this risk, educators must emphasize the importance of critical thinking and originality in writing. Integrating LLMs into the curriculum should involve discussions about ethical use and the role of these tools as aids rather than crutches. Furthermore, while LLMs are powerful, they are not infallible. Issues such as biases in training data and the hallucination problems necessitate careful consideration and oversight in their application (OpenAI, 2022;Thorp, 2023;Barrot, 2023;Xu and Wang, 2024b). Educators must remain vigilant and guide students in critically evaluating the outputs of LLMs, fostering a mindset of discernment in the use of technological tools. \n\nIn conclusion, the application of LLMs in teaching and learning academic writing presents both opportunities and challenges. While previous studies have employed the TAM model to explore EFL learners' acceptance of LLMs in informal digital learning of English (Liu and Ma, 2024), to the best of our knowledge, there is limited research that has utilized the UTAUT model to analyze EFL learners' acceptance of LLMs in learning business-related English academic writing, as well as the direct and moderating effects of motivation within the UTAUT model.",
            "score": 0.37766822029055347,
            "section_title": "LLMs and their application in teaching and learning academic writing",
            "char_start_offset": 14389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1608
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 186,
                    "matchedPaperCorpusId": "261067729"
                },
                {
                    "start": 359,
                    "end": 373,
                    "matchedPaperCorpusId": "258927876"
                },
                {
                    "start": 829,
                    "end": 843,
                    "matchedPaperCorpusId": "5281538"
                },
                {
                    "start": 843,
                    "end": 855,
                    "matchedPaperCorpusId": "256505670"
                },
                {
                    "start": 855,
                    "end": 868,
                    "matchedPaperCorpusId": "258927876"
                },
                {
                    "start": 1317,
                    "end": 1335,
                    "matchedPaperCorpusId": "260219184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07794189453125
        },
        {
            "corpus_id": "269213992",
            "title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences",
            "text": "Of PE tools, only EvalLM offers a way to help users calculate the alignment of LLM evaluators with their expectations, but this feature is mentioned only in authors' Design section and is absent from their user study. 2 At best, users of PE tools inspect LLM-generated evaluator outputs manually to double-check; at worst, the tool hides individual scores entirely.Regardless of aligning implementations of metrics with user preferences, even identifying what metrics to evaluate for custom tasks remains challenging for LLM practitioners, as evidenced by a recent study [37].While many evaluation tools require users to declare metrics they care about, some prior work [45] and EvalGen employ LLMs to propose custom metrics based on prompts in the user's LLM pipelines.\n\nOver-trust and Over-generalization of LLM Behavior.That tools provide little assistance to validate evaluator quality is alarming, considering that other research shows people tend to over-rely and over-trust AI systems [3,28,31,50].For instance, in one highprofile incident, researchers from MIT posted a pre-print on arXiv claiming that GPT-4 could ace the MIT EECS exam.Within hours, work by Chowdhuri et al. debunked the study [5], citing problems arising from over-reliance on GPT-4 to grade itself.Other work has found further reasons to be cautious: LLMs asked to choose the best response from a set can be consistently biased by set ordering [30,51]; and LLMs can be highly sensitive to seemingly innocuous formatting changes [43].\n\nA related problem to over-reliance is over-generalization.Zamfirescu et al. [57] found that users unfamiliar with PE tend to overgeneralize from single failures (causing them to throw out potentially good prompts), rather than having a holistic view of the overall performance of a prompt or chain.This was despite the fact that the interface had support for systematic testing.",
            "score": 0.3775749389594971,
            "section_title": "MOTIVATION AND RELATED WORK",
            "char_start_offset": 3508,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 365
                },
                {
                    "start": 365,
                    "end": 576
                },
                {
                    "start": 576,
                    "end": 770
                },
                {
                    "start": 772,
                    "end": 823
                },
                {
                    "start": 823,
                    "end": 1005
                },
                {
                    "start": 1005,
                    "end": 1145
                },
                {
                    "start": 1145,
                    "end": 1276
                },
                {
                    "start": 1276,
                    "end": 1511
                },
                {
                    "start": 1513,
                    "end": 1571
                },
                {
                    "start": 1571,
                    "end": 1811
                },
                {
                    "start": 1811,
                    "end": 1891
                }
            ],
            "ref_mentions": [
                {
                    "start": 995,
                    "end": 998,
                    "matchedPaperCorpusId": "263138444"
                },
                {
                    "start": 998,
                    "end": 1001,
                    "matchedPaperCorpusId": "248476449"
                },
                {
                    "start": 1001,
                    "end": 1004,
                    "matchedPaperCorpusId": "254591809"
                },
                {
                    "start": 1589,
                    "end": 1593,
                    "matchedPaperCorpusId": "258217984"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.129150390625
        },
        {
            "corpus_id": "272827953",
            "title": "The use of GPT-4o and Other Large Language Models for the Improvement and Design of Self-Assessment Scales for Measurement of Interpersonal Communication Skills",
            "text": "method, sample, instruments, data analysis) and interpretation of the results of empirical research. Similarly, in their case study on another specific research topic of the adoption of AI in higher education Rahman et al. (2023) have illustrated that ChatGPT (i.e. ) can be utilized for writing the sections of a demo research manuscript that include: (a) research idea generation, (b) background/introduction, (c) problem statement, (d) literature review references, (e) methodology, (f) results and discussion. Furthermore, Khlaif et al. (2023) demonstrated that ChatGPT can generate 'original' manuscripts in the specific field of teacher technostress that mimic scientific research with the potential for positive evaluation by reviewers. Finally, in their case study of actual peer reviews for papers that were submitted to four scientific conferences, Liang et al. (2023) revealed that \"between 6.5% and 16.9% of text submitted as peer reviews to the four observed conferences could have been substantially modified by LLMs\", indicating the potential use of LLMs for peer-reviewing. \n\nAccording to Rice et al. (2024), ChatGPT can be viewed as a \"linguistic calculator\" that can assist researchers in more efficient retrieval of information and design of experiments, even though its use also implicates the need for adherence to rigorous ethical standards. This was especially elaborated by Resnik and Hosseini (2024), who provided specific recommendations for researchers regarding, among other issues, their responsibility for AI-related biases and errors, disclosure of how they used AI in their research, and specifying the parts of the research output (including text and data) that were generated by AI. It must be highlighted that years before the introduction of ChatGPT various other AI tools have been used for literature review (Bolanos et al., 2024) and that those based on LLMs like ChatGPT are only a recent novelty for researchers who wish to facilitate this part of their research process. Still, it must be emphasized that LLMs may not always perform well in academic research-related tasks, as was revealed in a recent case study for the specific field of psychology (Lehr et al., 2024).",
            "score": 0.37738852049169386,
            "section_title": "Introduction",
            "char_start_offset": 2200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 1089
                },
                {
                    "start": 1092,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2212
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 229,
                    "matchedPaperCorpusId": "257845986"
                },
                {
                    "start": 527,
                    "end": 547,
                    "matchedPaperCorpusId": "263220181"
                },
                {
                    "start": 1105,
                    "end": 1123,
                    "matchedPaperCorpusId": "265383779"
                },
                {
                    "start": 1398,
                    "end": 1424,
                    "matchedPaperCorpusId": "270079717"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.274658203125
        },
        {
            "corpus_id": "269502506",
            "title": "WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting",
            "text": "A limitation of some studies is assessing tool usage with GPT-3.5 as the only LLM, so others have analysed the impact of choosing different LLMs (Qin et al., 2023b;Hsieh et al., 2023;Schick et al., 2023). Ruan et al. (2023) find that most smaller LLMs cannot effectively use tools, particularly for multi-step problems, with only GPT-3 effectively using tools. Huang et al. (2023) support this conclusion, finding that most LLMs other than GPT-4 cannot effectively select the right tools on their benchmark dataset. This may be explained by the finding of Xu et al. (2023) that GPT-4 can make correct API calls without in-context examples, whereas most models cannot. This suggests tool usage is part of the training data for GPT-4. Nevertheless, Hao et al. (2023) propose improving opensource models by giving them access to embeddings of tool descriptions. Ruan et al. (2024) study the risk of negative side effects when agents are used in real-world setting, which we build on in this work. \n\nTool Use Evaluation. There are many datasets for evaluating LLM capabilities, such as Big-Bench (Srivavastava et. al, 2023), MMLU (Hendrycks et al., 2021) and HellaSwag (Zellers et al., 2019). These feature a range of topics such as coding, chess, and chemistry. However, they are of limited value for evaluating tool usage, given that questions are answerable without access to tools. \n\nOther benchmarks have been proposed for specifically evaluating tool usage. Mialon et al. (2023) propose GAIA, which contains 466 questions that test agents on tasks such as web search and coding. Each question has a unique answer for robust, automatic evaluation. Patil et al. (2023) present APIBench for evaluating the effectiveness of LLM agents in calling other AI models accessed through APIs. SLURP (Bastianelli et al., 2020) and TaskMaster (Byrne et al., 2019) focus on smart home tasks that only require a single action.",
            "score": 0.37692848805716717,
            "section_title": "Tool Use Limitations.",
            "char_start_offset": 4700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 993
                },
                {
                    "start": 996,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1381
                },
                {
                    "start": 1384,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1912
                }
            ],
            "ref_mentions": [
                {
                    "start": 859,
                    "end": 877,
                    "matchedPaperCorpusId": "262944419"
                },
                {
                    "start": 1092,
                    "end": 1119,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 1165,
                    "end": 1187,
                    "matchedPaperCorpusId": "159041722"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049407958984375
        },
        {
            "corpus_id": "276106991",
            "title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge",
            "text": "In our main experiment, we aim to provide insights into RQ1. Mistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA-3.3), the judge LLMs exhibit a strong preference toward their related student models, leading to large positive values in the preference leakage scores. This finding suggests that preference leakage, along with the resulting bias, is widespread in SFT when the data generator and evaluator are the same. Evaluators' bias towards certain LLMs can be inherited by its student models. From Figure 2 and Figure 3, we find an obvious preference of GPT-4o towards Mistral/ Qwen-LLaMA-3.3 and this leads to the low preference leakage score in the Mistral-GPT-4o vs Mistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA-3.3 pairs. To investigate the source of this preference, we examine whether the GPT-4 evaluator has a bias toward LLaMA series models. Using the MTBench (Zheng et al., 2023) dataset, which includes pairwise comparison judgments from both humans and GPT-4, we compare GPT-4's and human evaluators' judgments on LLaMA-2 vs other models (including Vicuna, Alpaca, GPT-3.5, and GPT-4, which are preferred by GPT-4 due to preference leakage or egocentric bias) and LLaMA-2 vs Claude. The results, shown in Figure 4, reveal a clear preference for LLaMA-2 by GPT-4. Consequently, we conclude that evaluators' bias can be inherited. In this case, GPT-4's bias toward LLaMA has been passed on to LLaMA's student models. This inheritance, combined with the opaque training data of LLMs, makes preference leakage a more complex and challenging problem. \n\nModel pairs with similar performance tend to have more  obvious preference leakage.",
            "score": 0.3768164186640254,
            "section_title": "Main Results",
            "char_start_offset": 14023,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1556
                },
                {
                    "start": 1559,
                    "end": 1642
                }
            ],
            "ref_mentions": [
                {
                    "start": 868,
                    "end": 888,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.044189453125
        },
        {
            "corpus_id": "274213776",
            "title": "Evaluating science: A comparison of human and AI reviewers",
            "text": "There are several limitations to our study. Our sample of participants suffers from self-selection bias since the results are based on responses from conference participants who consented to review AIgenerated work. 6 While the call for participation was open to all conference participants, it is quite likely that participants who opted into the study were already familiar with and interested in LLMs. Such reviewers may have been particularly knowledgeable in distinguishing between actual humangenerated abstracts and made-up AI-generated abstracts. Furthermore, we only compared purely AIgenerated and human-written abstracts. Our primary intention was to isolate the effects of LLMs in the abstract generation and review processes, thereby limiting the potential impact of human intervention. Researchers commonly employ LLMs to improve many parts of their writing, such as language refinement, clarity, and idea development (Liang et al., 2024a,b). We did not have data on the extent or method by which researchers used LLMs to improve their submissions. Therefore, our study cannot account for the complex ways AI may have contributed to the quality of human-written abstracts. However, we did evaluate the extent to which authors have potentially used ChatGPT using GPTZero, and the overall prediction was rather low (8.9%, see Model 2 in Table 2). Moreover, we have used the same prompt for both human reviewers and ChatGPT for evaluation. As described above, some prompts were quite abstract and lacked specific evaluation criteria, which could have been an issue for ChatGPT evaluations. A precise and unambiguous prompt may better demonstrate the limits and capabilities of LLMs, and some researchers (e.g., Liang et al., 2024;Liu and Shah, 2023) have either given very specific criteria or dynamically changed the prompt to probe for the most accurate answer. Finally, our results are naturally limited to the current technology (i.e., GPT-3.5 and GPT-4 as used in the present study) and knowledge of the sample of reviewers. 7 Improved LLMs, some of which are fine-tuned on scientific data (e.g., SciBERT Beltagy et al., 2019), may soon overcome some of the weaknesses of AI-based reviews.",
            "score": 0.37671999937239253,
            "section_title": "Limitations",
            "char_start_offset": 25023,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 43
                },
                {
                    "start": 44,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 1722,
                    "end": 1741,
                    "matchedPaperCorpusId": "263608784"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60693359375
        },
        {
            "corpus_id": "272690317",
            "title": "Developing an Interactive OpenMP Programming Book with Large Language Models",
            "text": "The findings emphasize the role of LLMs as supplementary tools in educational settings, suggesting the importance of proper prompts to enhance the utility and accuracy of LLM outputs in complex academic tasks. People should be very careful when verifying the accuracy of the generated code when using LLMs. \n\nAltm\u00e4e et al. explore the use of ChatGPT in scientific writing, particularly focusing on its application in drafting a manuscript for reproductive medicine [5]. The study illustrates the potential and challenges of using AI in academic writing, highlighting ChatGPT's role in streamlining content creation, manuscripts' initial composition, and refinement. Key challenges noted include the accuracy and relevance of AI-generated content, requiring significant human oversight to ensure scientific integrity, and raising ethical concerns about authorship and the potential for AI to discourage deep learning. This exploration aligns with broader discussions on integrating AI in educational tools, as seen in other research focused on programming education. It suggests AI's utility as a supplementary aid in complex intellectual tasks, provided its limitations are carefully managed. \n\nLiterature surveys are fundamental in academia and education, providing essential overviews of existing research and identifying future research directions. The study by Wang et al. introduces AutoSurvey, an innovative system designed to automate the creation of comprehensive literature surveys. Employing a systematic approach that encompasses initial retrieval and outline generation, subsection drafting by specialized LLMs, integration, refinement, and rigorous evaluation, AutoSurvey adeptly addresses the challenges posed by the vast volume and complexity of information. By leveraging the capabilities of LLMs, the system not only enhances the efficiency and quality of literature surveys but also demonstrates significant improvements in both citation and content quality compared to traditional methods. This exploration not only highlights the potential of LLMs to drastically reduce the time required to produce high-quality academic surveys but also underscores ongoing challenges such as context window limitations and the reliability of parametric knowledge within these models [14]. \n\nThe commonality among the studies above lies in their utilization of the powerful reading, understanding, and text-generation capabilities of LLMs. This aligns closely with our work. However, our research specifically focuses on generating OpenMP textbooks, emphasizing the teaching of OpenMP programming.",
            "score": 0.37648710787646783,
            "section_title": "The Use of LLM in Education and Textbook Writing",
            "char_start_offset": 42124,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 306
                },
                {
                    "start": 309,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2293
                },
                {
                    "start": 2296,
                    "end": 2443
                },
                {
                    "start": 2444,
                    "end": 2478
                },
                {
                    "start": 2479,
                    "end": 2601
                }
            ],
            "ref_mentions": [
                {
                    "start": 465,
                    "end": 468,
                    "matchedPaperCorpusId": "258268211"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.337646484375
        },
        {
            "corpus_id": "269293024",
            "title": "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models",
            "text": "While evaluating LLMs from the user-centric perspective offers considerable advantages, there are certain limitations in our URS benchmark. \n\nEvaluation Method: Our approach aligns with prevailing practices in open-ended question evaluation, where GPT-4 is employed as the base evaluator. This could potentially lead to a bias toward responses resembling GPT's style, thereby influencing the neutrality of the evaluation. We analyzed through cross-validation that this approach does not affect the top 2 LLM orders. Besides, analysis shows that benchmark score aligns with real-world user experience and human preferences in pairwise annotations. These further tested the validity of our method. In addition, the user study and human pairwise annotations could be further scale up to enhance persuasiveness. \n\nBesides, we notice it is very important to evaluate LLM services' performances in multi-turn conversational settings, as there are a few cases where the LLMs do not answer the user question and ask to provide more detailed information about user intents. These may be judged unsuccessful in a single-turn assessment, but the service might be capable of meeting this user need. \n\nNote that this benchmark is not holistic, as we mainly measure LLMs' efficacy in satisfying realworld user needs. User satisfaction is indeed crucial, while it represents one aspect of LLM performance. Other critical facets include effectiveness, efficiency, ease of use, and error tolerance. In this study, our primary focus is on user-centric evaluation of LLMs, which we have validated through strong alignments with human preferences to address its effectiveness. \n\nData Distribution: The data for our study was mainly sourced from a global researcher recruitment platform, which does not ensure a random selection of all participants using LLM services. Consequently, the demographic distribution of our dataset may not accurately reflect that of typical LLM users or the global population. While as an independent third party unaffiliated with specific LLM services, and given our non-selective approach to data collection, our methodology incorporates greater diversity and offers a closer alignment with real-world usage scenarios compared to datasets that are human-designed, model synthetic, or selected by non-third parties or single-sourced.",
            "score": 0.37644510245735596,
            "section_title": "Limitations",
            "char_start_offset": 23689,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 807
                },
                {
                    "start": 810,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1656
                },
                {
                    "start": 1659,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2342
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.093994140625
        },
        {
            "corpus_id": "262045067",
            "title": "Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles",
            "text": "We address the research questions from \u00a71, first evaluating how well diverse information from mul- tiple sources is summarized by LLMs ( \u00a74.1), then examining LLM behavior during this summarization ( \u00a74.3) using the most reliable LLM-based evaluation protocols we found ( \u00a74.2). \n\n4.1 RQ 1: How proficient are LLMs in summarizing diverse information from multiple news articles? \n\nTo understand LLMs' performance on MDDS, we conduct human evaluation on summaries produced by four representative LLMs, GPT-4 (OpenAI, 2023b), GPT-3.5-Turbo-16K (OpenAI, 2023b), Vicuna-7B (Chiang et al., 2023), LongChat-7B-16K (Li et al., 2023). \n\n3 \n\nLong-context LLMs, GPT-3.5-Turbo-16K and LongChat-7B-16K, handle texts up to 16K tokens and can perform direct summarization by taking all articles as input. Standard LLMs, GPT-4 and Vicuna-7B, are limited to 8K and 2K tokens, respectively; hence, we split summarization into two stages: selecting the most salient N sentences from each article and summarizing these sentences. 4 To elicit a high-coverage summary of diverse information, we manually optimize the prompts. of diverse information. This suggests that the proposed task is challenging even for state-of-the-art LLMs, and highlights that DIVERSESUMM serves as a challenging test bed for LLMs. \n\n4.2 RQ 2: What are the pitfalls and best practices when leveraging GPT-4 as the evaluation metric for our task? \n\nTo facilitate the analysis and discussion of our next research question, we rely on LLM-based evaluation metrics to conduct various analyses, given their superior correlation with human judgments (Liu et al., 2023b) and the high cost of human annotation. For this research question, we aim to provide the best practices when using GPT-4 as the evaluator for the MDDS task by conducting bias and correlation analyses.",
            "score": 0.37597473518573277,
            "section_title": "Analysis",
            "char_start_offset": 14423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 281,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 626
                },
                {
                    "start": 629,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1401
                },
                {
                    "start": 1404,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1820
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2337646484375
        },
        {
            "corpus_id": "273162522",
            "title": "Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review",
            "text": "There has been substantial research examining the implications of using generative AI in conference peer review processes [1,[27][28][29][30][31][32]. Liang et al. [1] introduced a maximum likelihood method to assess the influence of large language models (LLMs) on reviewing practices at the corpus level. They also conducted a large-scale empirical study [27] to evaluate the effectiveness of LLM-generated feedback compared to human reviews, revealing insights into its utility and limitations in peer review. In another effort, Tan et al. [28] developed a comprehensive dataset that simulates the peer review process as a multi-turn dialogue, incorporating the roles of reviewers, authors, and meta-reviewers to explore the dynamics of review interactions. Zhou et al. [29] focused on evaluating the reviewing capabilities of LLMs such as GPT-3.5 and GPT-4o through 196 multiple-choice questions related to reviewing tasks, providing insights into their strengths and weaknesses in academic review settings. To enhance the quality of reviews, Tyser et al. [30] proposed the OpenReviewer tool, which allows authors or reviewers to submit papers and receive automatic reviews, enabling iterative improvements before actual submission or peer review. Kuznetsov et al. [31] explored the core challenges in the peer review process and argued for a more integrated, transparent use of LLMs in reviewing to overcome these limitations. Additionally, Mosca et al. [32] introduced a benchmark designed to distinguish between human-written and machine-generated scientific papers, helping to address concerns about the integrity of AI-generated content in academic settings. In contrast, our work focuses on the utility of AI text detection methods in identifying LLM generated content in the peer review process.",
            "score": 0.37596071513112495,
            "section_title": "AI-assisted peer review.",
            "char_start_offset": 14808,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1806
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 129,
                    "matchedPaperCorpusId": "263608784"
                },
                {
                    "start": 133,
                    "end": 137,
                    "matchedPaperCorpusId": "269803977"
                },
                {
                    "start": 145,
                    "end": 149,
                    "matchedPaperCorpusId": "260063149"
                },
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "263608784"
                },
                {
                    "start": 773,
                    "end": 777,
                    "matchedPaperCorpusId": "269803977"
                },
                {
                    "start": 1459,
                    "end": 1463,
                    "matchedPaperCorpusId": "260063149"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "264591429",
            "title": "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation",
            "text": "If BSM with GPT-4 improves human-agreement for this subset of samples, it suggests that even in scenarios where model A is judging its own outputs, BSM (with model A) leads to a better evaluator.While this does not necessarily compute whether an evaluator has less SB, it does verify whether the evaluator model correlates better with humans even when it is evaluating its own responses.\n\nWhile multiple past works have highlighted the importance of these biases (Zheng et al., 2023;Wu and Aji, 2023), we measure all of them with concrete metrics within the same evaluation framework.Conceptually, 'Ag' evaluates correctness while 'PB' for example evaluates consistency of LLM-based evaluators.These are complementary aspects and an ideal evaluator should perform well in all metrics for it to be reliably used.\n\nImplementation Details.We develop BSM on top of multiple LLMs of varying scales and capabilities: LLaMA-2-7B-chat, Vicuna-33B, LLaMA-2-70B-chat, and GPT-4.We implement all modules zero-shot, providing only module-specific instructions and assuming no access to demonstrations of how to branch, solve, or merge.\n\nBaselines.We compare our method, BSM, to (1) two variants of zero-shot prompting with the same LLM: a relative evaluator, that directly generates a preference judgment and an absolute evaluator, that generates two scores for the two responses and then the final preference is determined based on the higher score, (2) plan&solve prompting (Wang et al., 2023a), which plans (i.e., generates evaluation criteria) but instead of solving them independently, solves all branches together in one LLM call, (3) self-consistency (Wang et al., 2022), which samples multiple evaluations from the prompted LLM (with temperature 0.7) and chooses the majority vote as the final judgment.For fair comparison, self-consistency samples the same number of generations as the branching factor in BSM.",
            "score": 0.3756035293271524,
            "section_title": "Experimental Setup",
            "char_start_offset": 17775,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 195,
                    "end": 387
                },
                {
                    "start": 389,
                    "end": 584
                },
                {
                    "start": 584,
                    "end": 694
                },
                {
                    "start": 694,
                    "end": 811
                },
                {
                    "start": 813,
                    "end": 836
                },
                {
                    "start": 836,
                    "end": 968
                },
                {
                    "start": 968,
                    "end": 1123
                },
                {
                    "start": 1125,
                    "end": 1135
                },
                {
                    "start": 1135,
                    "end": 1799
                },
                {
                    "start": 1799,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 1464,
                    "end": 1484,
                    "matchedPaperCorpusId": "258558102"
                },
                {
                    "start": 1646,
                    "end": 1665,
                    "matchedPaperCorpusId": "247595263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28369140625
        },
        {
            "corpus_id": "264492944",
            "title": "AI-generated feedback on writing: insights into efficacy and ENL student preference",
            "text": "In study 1, to gauge the linguistic progress among students, the study implemented a pre-and post-test design. A diagnostic writing test administered on the first day of class served as the pretest and a final writing exam served as the posttest. For these assessments, and for a recurring weekly writing task, participants were required to write a 300-word paragraph centered around diverse academic topics discussed in class and integrate sources from readings. \n\nIn study 2, to assess student preferences between human and AI-generated feedback, a questionnaire was developed to gather quantitative and qualitative data (see appendix A). Eight five-point Likert scale items, arranged into four pairs, captured various dimensions of feedback preference, including, satisfaction, comprehensibility and clarity, helpfulness, and overall preference (e.g.,: \"I am satisfied with the feedback I received from the ENL tutor this week compared with \"I am satisfied with the feedback I received from the AI program this week\"). Participants were asked to respond using a scale from \"1-Strongly Disagree\" to \"5-Strongly Agree. \" In addition, participants were asked \"If you were to only get one kind of feedback next week, which kind of feedback would you prefer?\". A follow-up open-ended question in which participants were asked to provide an explanation for their choice was also included. \n\nOpenAI's GPT-4 was utilized to generate feedback on student writing for both studies. GPT-4 is a multimodal LLM that can process image and text inputs and produce text outputs. GPT-4 was selected as we found it to provide the most suitable and accurate feedback out of the LLMs we tested. Furthermore, GPT-4 outperformed other LLMs on academic benchmarks, at least at the time of its release (OpenAI, 2023b) and when this study was conducted. \n\nThe prompt sent to ChatGPT to generate feedback on students' weekly writing consisted of several parts. Two experienced language educators familiar with the course and its assignments and assessments developed the prompt iteratively in line with the prompt-engineering framework presented by [Ingley, 2023]. First, GPT-4 was given the role of a professional language teacher who is an expert on providing feedback on the writing of English language learners.",
            "score": 0.37528935090141247,
            "section_title": "Instruments",
            "char_start_offset": 16752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1385
                },
                {
                    "start": 1388,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1830
                },
                {
                    "start": 1833,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2140
                },
                {
                    "start": 2141,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 2125,
                    "end": 2139,
                    "matchedPaperCorpusId": "259041101"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.393798828125
        },
        {
            "corpus_id": "269588132",
            "title": "Large Language Models are Inconsistent and Biased Evaluators",
            "text": "1 For all experiments in this work, we set N=10 to balance reducing variance with avoiding prohibitive cost increases  Frequencies of each possible score as found in 64,000 predictions using the 1-100 scale.Models sparsely predict scores within the range.Frequencies of some scores, such as 90 and 95, are far higher than 'odd' scores such as 92 or 19, and much of the range is almost entirely ignored (1-60).Interestingly, 1-60 is a range often largely ignored in academic grading scales.This indicates an issue within instruction-following specific to automatic evaluation.\n\nFigure 3 shows the distribution of scores produced when instructing GPT-3.5-Turbo and GPT-4 to rate summaries on a 1-100 scale.The scores in this distribution are not respected as intended, and the model assigns outsized probabilities to certain scores such as 90 and 95.This reaffirms results by Zheng et al. (2023a) which found that multiplechoice selections by LLMs suffered from similar token biases, deteriorating performance.The full range is also not utilized, with predicted scores largely occurring between 70 and 100.\n\nFigure 3 also shows that the score distribution has several peaks for round numbers such as 60, 70, 80, 90 (Similarly for 75, 85, and 95), indicating that LLM evaluators also have round number bias like human. 2o verify which rating scales produce higher quality responses by LLM evaluator frameworks, we run a comparative analysis of the cases men- tioned in \u00a73.2.Table 2 shows performance of GPT-4 based evaluators on SummEval under the mentioned rating scales.The performance of 1-10 score performs best on average, with an average score of 0.428 Kendall's \u03c4 across the labels in SummEval.This method also performs the best on relevance, at 0.462 Kendall's \u03c4 , while 1-100 scoring performs better on Coherence and the float modification method performs best on both Consistency and Fluency3 .Ultimately, increasing scoring granularity is shown to improve performance in our experiments, which should be carefully conducted for the risk of score bias and round number bias.",
            "score": 0.37495996954916744,
            "section_title": "Scoring Granularity and Score Biases",
            "char_start_offset": 12231,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 207,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 409
                },
                {
                    "start": 409,
                    "end": 489
                },
                {
                    "start": 489,
                    "end": 575
                },
                {
                    "start": 577,
                    "end": 704
                },
                {
                    "start": 704,
                    "end": 848
                },
                {
                    "start": 848,
                    "end": 1008
                },
                {
                    "start": 1008,
                    "end": 1104
                },
                {
                    "start": 1106,
                    "end": 1317
                },
                {
                    "start": 1317,
                    "end": 1471
                },
                {
                    "start": 1471,
                    "end": 1569
                },
                {
                    "start": 1569,
                    "end": 1698
                },
                {
                    "start": 1698,
                    "end": 1901
                },
                {
                    "start": 1901,
                    "end": 2081
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.189697265625
        },
        {
            "corpus_id": "265609311",
            "title": "Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies",
            "text": "Each approach uncovered different stereotypes that were not found using the other. Another alternative is to use a mixture of bias metrics to evaluate LLMs instead of just one. \n\nThe most recent LLMs, such as GPT-4 and Llama 2, have shown incredible capabilities compared to the earlier models, with researchers speculating the possibility of these models becoming part of the solution to tackling the bias problem [25,199]. Initial experiments of GPT-4 are shown to be more trustworthy and not strongly biased for most stereotyped topics when compared to earlier GPT models [199], and GPT-4 could provide a text completion for prompts with commentary on the possible offensiveness of its generation [25]. Although it is unclear the extent to which these capabilities can be utilised to tackle the bias problem or self-correct biases, [199] warns that GPT-4 models' ability to follow instructions more precisely can be used maliciously to manipulate the outputs. There is a need for future research to identify the benefits and risks of the most recent huge LLMs before using them directly as a way to tackle the bias problem. \n\nThe role of governance and laws can also help shape notions of bias more broadly. The risk requires broader concerted action between policy-makers, civil society, and other stakeholders to be mitigated. Moreover, the importance of an inclusive, cross-disciplinary and cross-cultural community, including technical and socio-technical AI researchers, civil society organisations, policy-makers, product designers, affected societies and the wider public, is highlighted in several studies. \n\nBias detection and mitigation is an ongoing process, and it is essential to regularly monitor the model for any new sources of bias that may emerge. This can be achieved by developing automated monitoring systems that flag potential bias in real time and regular audits of the model's performance.",
            "score": 0.37482885010316835,
            "section_title": "DISCUSSIONS",
            "char_start_offset": 74393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 176
                },
                {
                    "start": 179,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1917
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1807861328125
        },
        {
            "corpus_id": "268063339",
            "title": "FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability",
            "text": "We request human annotators to examine the explanations given by GPT-4 for the failure instances and categorize these reasons into the following groupings: 1) Incomplete Sections: \n\nThe model often neglects essential sections that the prompts mandate, spanning a variety of content areas such as methodological frameworks, data analyses, theoretical discussions. This shortfall is especially pronounced in scientific contexts, as illustrated in Figure 2(a), which depicts Mistral 7B V0.1's markedly inferior performance in scientific domains. 2) Incorrect Data Structure: Mis-tral 7B V0.1 struggles with adhering to specific structural guidelines, impacting diverse formats including JSON, CSV, Markdown (as highlighted in Figure 3(a)), as well as domain-specific formats such as legal citations and academic referencing. \n\nIssues include improper syntax use, inaccurate data structure representations, and failure to follow document layout guidelines. 3) Missing Detailed Format Requirements: The model frequently fails to meet specific and detailed formatting requirements. \n\nFor instances, it presents the \"Age\" attribute in a text format (\"40 years\") rather than as a numerical value (40); it ignores the directive to use bullet points (\"-\") for lists and enumerations; it incorrectly includes headers, despite explicit instructions in the prompt to exclude them. \n\nHuman Evaluation Alignment. To evaluate the effectiveness of utilizing GPT-4 as evaluator, we randomly selected 100 annotations made by GPT-4 on the outputs of Mistral 7B V0.1 for a comprehensive human appraisal. We engaged five human experts to review the accuracy of GPT-4's annotations and conducted an analysis to compare the agreement between human annotators and the GPT-4 evaluator. Our findings revealed that for 84 out of the 100 FORMAT-INSTRU, the evaluations by GPT-4 were in agreement with those of the human experts, yielding an alignment rate of 84%. A closer examination was conducted on the 16 instances where GPT-4's evaluations diverged from human judgment. It was observed that all these discrepancies were instances of false positives, indicating scenarios where the model's outputs failed to adhere to specific format requirements, yet were overlooked by GPT-4.",
            "score": 0.3748060674674405,
            "section_title": "Result Analysis",
            "char_start_offset": 20558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1367
                },
                {
                    "start": 1370,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2252
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51123046875
        },
        {
            "corpus_id": "269981957",
            "title": "Improving Language Models Trained on Translated Data with Continual Pre-Training and Dictionary Learning Analysis",
            "text": "The use of LLM-as-judge approach to evaluate open-ended generation comes with some limitations. The complexity of the task of evaluating consistency, grammar and creativity is sometimes challenging for GPT-4. We noticed its limitations in complex Arabic evaluation compared to English. A reassessment of GPT-4 in Arabic with respect to newer models such as Llama-3 and Command R+ would be helpful to decide on the best LLM judge.",
            "score": 0.3747830780072384,
            "section_title": "Limitations of GPT-Eval",
            "char_start_offset": 22490,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 429
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0738525390625
        },
        {
            "corpus_id": "276408437",
            "title": "An Empirical Analysis of Uncertainty in Large Language Model Evaluations",
            "text": "To enhance reproducibility, we set the temperature to 0 for proprietary models and utilize greedy decoding for open-source models. For each evaluation, we query the evaluator twice with the order swapped. All general LLM-based evaluators (e.g., GPT-4o) are required to output in a CoT format. \n\nTo obtain the best evaluation results, specially trained or fine-tuned evaluators (e.g., PandaLM-7B) are assessed using their original prompt and output format. (2) ConfiLM outperforms Llama3-8B-Instruct-Finetune and Llama3-8B-Instruct on F1 by 3.9% and 8.5%, respectively. This improvement demonstrates that fine-tuning high-quality human assessments enhances LLMs' evaluation capabilities, and incorporating uncertainty as auxiliary information significantly boosts evaluator performance in OOD scenarios. (3) Compared to reasoning and math tasks, most evaluators show weaker performance on writing tasks. We speculate that this unusual trend arises because LLMs can evaluate response from reasoning tasks based on in-distribution knowledge, but fail to make judgement in creative tasks like writing. Case study. Due to the presence of subtle hallucinations in long texts and the inherently subjective nature of their evaluation, general LLM-based evaluators (such as GPT-4) tend to underperform in writing tasks. We present a test sample (Table 6) that illustrates the role of response confidence in detecting hallucinations of model response (Farquhar et al., 2024;Varshney et al., 2023). As an uncertainty-aware evaluator, ConfiLM reduces the reliability of a response when it detects low confidence, leading to more accurate judgments.",
            "score": 0.3742036546495262,
            "section_title": "EXPERIMENTAL SETTINGS",
            "char_start_offset": 28249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 292
                },
                {
                    "start": 295,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1636
                }
            ],
            "ref_mentions": [
                {
                    "start": 1441,
                    "end": 1464,
                    "matchedPaperCorpusId": "270615909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12054443359375
        },
        {
            "corpus_id": "259951372",
            "title": "Development of the ChatGPT, Generative Artificial Intelligence and Natural Large Language Models for Accountable Reporting and Use (CANGARU) Guidelines",
            "text": "The proposed CANGARU Guidelines will comprise important recommendations that data scientists should consider for ethical accountability and future reproducibility. They will provide recommendations on what to avoid, what and how to disclose, and how to ensure proper reporting of GAI/GPTs/LLMs in papers. The guidelines aim to enable academics, authors, editors, reviewers, publishers, and readers to critically follow the relevant etiquette when using GAI/GPTs/LLMs. \n\nThe resultant CANGUARU Guidelines will: \n\n1) Serve as a tool for training researchers on the ethical and accountable use of GAI/GPTs/LLMs in academic research and scientific writing. \n\n2) Establish a framework for assessing publications for publishers, editors, authors, and reviewers. \n\n3) Assist end-users, including scientific audiences and policymakers, in better evaluating the validity and applicability of scientific papers for their decision-making process.",
            "score": 0.37396856058426436,
            "section_title": "CONCLUSION",
            "char_start_offset": 23792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 755
                },
                {
                    "start": 758,
                    "end": 935
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.390869140625
        },
        {
            "corpus_id": "259360619",
            "title": "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations",
            "text": "They are tested on both standard datasets such as MMLU, and more importantly, on open-ended questions which are much more prevalent in real life (Nakano et al., 2021;Chiang et al., 2023). People mostly use GPT-4 (Liu et al., 2023;OpenAI, 2023) as an evaluator for either generating scores or pairwise comparisons (Wang et al., 2023b;Zhou et al., 2023). However, such a strategy has fundamental problems because of various biases, such as (1) positional bias (Dettmers et al., 2023;Wang et al., 2023a), where a model favors the first answer in pairwise comparisons; (2) verbosity and length bias (Wang et al., 2023b); (3) and most importantly, self-enhancement bias, where an LLM favors its own answers (Liu et al., 2023;Zheng et al., 2023). \n\nEfforts have been proposed to tackle them: (1) Using position switching (Wang et al., 2023a) for mitigating positional bias; (2) Zheng et al. (2023) proposes Chatbot Arena, where real users ask questions and provide Figure 1: The peer rank process (PR): each LLM model acts both as reviewers (A, B, C) and contestants (1,2,3). From the battles between contestants (pairwise comparisons), it induces a self-ranking. In this example, models A, B, and C represent GPT-4, Bard, and Claude, respectively. pairwise judgments of answers generated by two LLMs. But this is time-consuming and requires expert annotation to ensure fairness; (3) Bai et al. (2023) propose using each LLM as an examiner, where each generates questions to test other models. Different from PD, their \"exams\" are biased with randomly generated questions. Moreover, none of the above works support inducing self-rankings through peer ranking. \n\nOverall, our work, peer evaluation-oriented methods, undergo tests on two benchmarks, each covering a variety of tasks, and focuses more on the alignment between LLMs' evaluations and human judgments.",
            "score": 0.3738962507547734,
            "section_title": "Related Work",
            "char_start_offset": 5682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1856
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09979248046875
        },
        {
            "corpus_id": "267751370",
            "title": "One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation",
            "text": "prompt and OP-PROMPTS, a dimensiondependent set of prompts, enabling opinion summary evaluation for all the 7 dimensions. \n\nExperiments indicate that the OP-I-PROMPT generally outperforms existing approaches on both closed-source and open-source models by 9% on average in correlation with human judgments (Figure 1, Section 3). To the best of our knowledge we are the first to test the applicability of different prompt approaches on open-source LLMs. \n\n3. Benchmarking of 9 recent LLMs (closed and open-source) on the aforementioned 7 dimensions for the task of opinion summarization, which to the best of our knowledge is first of its kind (Table 4, Section 6). \n\n4. Detailed analysis, comparing an open-source LLM against a closed-source LLM acting as evaluators for automatic evaluation of opinion summaries on 7 dimensions. Analysis indicates that OP-I-PROMPT emerges as a good alternative for evaluating opinion summaries showing a high correlation (spearman correlation of 0.70 on average) with humans when compared with alternatives (Section 6). \n\n1 https://github.com/tjsiledar/SummEval-OP 2 Related Work LLM-based Evaluators. Fu et al. (2023) introduced GPTScore that operates on the premise that a generative pre-training model (e.g. GPT-3) is likely to assign a higher probability to the generation of high-quality text in line with provided instructions and context. Chiang and Lee (2023a) were the first to explore LLMs for evaluation. Chiang and Lee (2023b) provide concrete guidelines that improve ChatGPT's correlation with humans. Wang et al. (2023) conducted an initial survey exploring the utilization of ChatGPT as an NLG evaluator. Kocmi and Federmann (2023) used GPT models for evaluating machine learning tasks. Liu et al. (2023) introduced G-Eval, a framework for evaluation of NLG outputs using Chain of Thought (CoT) (Wei et al., 2023) and assigning weights to a predetermined set of integer scores based on their generation probabilities from GPT-3/4.",
            "score": 0.37367199787162053,
            "section_title": "OP-I-PROMPT, a dimension-independent",
            "char_start_offset": 2035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 124,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 1381,
                    "end": 1403,
                    "matchedPaperCorpusId": "258461287"
                },
                {
                    "start": 1451,
                    "end": 1473,
                    "matchedPaperCorpusId": "266166678"
                },
                {
                    "start": 1550,
                    "end": 1568,
                    "matchedPaperCorpusId": "257378627"
                },
                {
                    "start": 1655,
                    "end": 1681,
                    "matchedPaperCorpusId": "257232490"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2469482421875
        },
        {
            "corpus_id": "271769623",
            "title": "Simulating Expert Discussions with Multi-agent for Enhanced Scientific Problem Solving",
            "text": "Solution Quality In addition to evaluating the accuracy of the models, we also assess the quality of the generated solutions. We randomly sample 100 problems and evaluate the solutions using LLMs and human evaluation based on three criteria: (1) the correctness of the reasoning steps, (2) the clarity of the explanations, and (3) the appropriateness of the mathematical notations and symbols used. Each criterion was rated on a scale of 1 to 5, with 5 being the highest quality. \n\nFor the human evaluation, we employed three expert annotators. To ensure reliability, we calculated the inter-annotator agreement using Fleiss' kappa (Fleiss, 1971) for each of the three criteria: \n\n\u2022 Correctness of reasoning steps: \u03ba = 0.71 \n\n\u2022 Clarity of explanations: \u03ba = 0.62 \n\n\u2022 Appropriateness of mathematical notations and symbols: \u03ba = 0.55 \n\nThe overall average kappa value was 0.63, indicating substantial agreement among the annotators. \n\nThe detailed prompts for LLM evaluation and the specific guidelines for human evaluation are provided in Appendix B. Table 2 presents the average quality scores for solutions from GPT-4. Compared to baseline, SEDM consistently achieves higher quality scores in both LLM and human evaluations. The solutions generated by SEDM demonstrate clearer reasoning steps, more coherent explanations, and more precise use of mathematical notations. This suggests that the multi-expert discussion framework not only improves the accuracy of the solutions but also enhances their overall quality and readability. Table 1: The accuracy scores (%) of different baseline methods and our proposed SEDM approach across various scientific domains using GPT-3.5 and GPT-4 models under the setting of 2 experts and 2 discussion rounds. The best results for each subject are in bold. Table 2: The average quality score of solutions from GPT-4 evaluated by LLMs and humans.",
            "score": 0.37307872633777467,
            "section_title": "Further Analysis",
            "char_start_offset": 18524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 479
                },
                {
                    "start": 482,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 678
                },
                {
                    "start": 681,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 761
                },
                {
                    "start": 764,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1881
                }
            ],
            "ref_mentions": [
                {
                    "start": 632,
                    "end": 646,
                    "matchedPaperCorpusId": "143544759"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.537109375
        },
        {
            "corpus_id": "274213776",
            "title": "Evaluating science: A comparison of human and AI reviewers",
            "text": "Researchers from different fields have highlighted that the current peer review process is cumbersome and, often, not objective. Scientific output has become increasingly more complex, including extensive and sometimes very technical supplementary materials, which places the burden of scrutiny on the typically non-incentivized reviewers (Petrescu and Krishen, 2022). As such, writing a peer review typically takes 4-8 h (Huisman and Smits, 2017;Kovanis et al., 2016); the associated overall costs for peer-reviewing are estimated to sum up to approximately 1.5 billion USD per year (Aczel et al., 2021). The mass production of AI-generated text could tax the peer review system even further and undermine the credibility of scientific research (Dwivedi et al., 2023;Sabel et al., 2023). \n\nThe increasing burden of scientific reviewing has prompted scientists to explore ways in which AI can be used as a prescreening tool to assess the general journal fit, compliance with submission and reporting guidelines, risk of plagiarism, conflict of interest, and inappropriate content (Dwivedi et al., 2023;Folt\u00fdnek et al., 2019). But can AI tools, such as GPT-4, tell the difference between humanauthored research output and AI or evaluate the quality of scientific research as reported in abstracts? The recent body of literature suggests that humans are generally capable of differentiating humanwritten from AI-generated scientific text (e.g., Gao et al., 2023;Ma et al., 2023). However, LLM systems like GPT-4 are not far behind (Gao et al., 2023). When it comes to evaluating the quality of scientific output, the peer review system aims to impartially and constructively evaluate the robustness and reliability of the scientific work submitted to a conference or journal.",
            "score": 0.3727171104676317,
            "section_title": "Related literature",
            "char_start_offset": 2710,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1773
                }
            ],
            "ref_mentions": [
                {
                    "start": 422,
                    "end": 447,
                    "matchedPaperCorpusId": "24343741"
                },
                {
                    "start": 447,
                    "end": 468,
                    "matchedPaperCorpusId": "9484241"
                },
                {
                    "start": 584,
                    "end": 604,
                    "matchedPaperCorpusId": "244105062"
                },
                {
                    "start": 746,
                    "end": 768,
                    "matchedPaperCorpusId": "257486916"
                },
                {
                    "start": 1080,
                    "end": 1102,
                    "matchedPaperCorpusId": "257486916"
                },
                {
                    "start": 1443,
                    "end": 1460,
                    "matchedPaperCorpusId": "258354722"
                },
                {
                    "start": 1529,
                    "end": 1547,
                    "matchedPaperCorpusId": "258354722"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51513671875
        },
        {
            "corpus_id": "270870769",
            "title": "Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?",
            "text": "In this section, we present the main findings of our study. Each subsection provides a detailed analysis of the expert assessment annotations in order to answer our research questions. \n\n4.1 RQ1: Can the current state of generative AI compare to a prestigious author in creative writing tasks? \n\nFigure 1 summarizes the scores given by the experts to GPT-4 (English and Spanish are reported separately) and Patricio Pron, showing the percentage of assessments in each of the 0-3 scores and also the mean and standard deviation for each of the quality dimensions in the rubric 4 . Overall, the assessments are remarkably lower for GPT-4 across all quality dimensions, in both languages. In all dimensions, GPT-4 receives predominantly scores of 0/1, while Pron receives mostly 2/3. Pron writes with more original and attractive style and theme, is more creative, his texts are more suitable to be included in an anthology, and assessors believe that he has its own voice compared to GPT-4. Pron's creativity score is particularly high (average of 1.94), which roughly doubles GPT-4 creativity scores in English and Spanish. On the other hand, GPT-4 scores are particularly low in style originality (0.36 in Spanish and 0.63 in English), where Pron gets 1.70 (more than double). \n\nNote that a direct comparison between GPT-4 scores in English and Spanish is not advisable, as they come from different groups of assessors. In Section 4.3 we discuss language differences. \n\nOverall, GPT-4's concentration of evaluations in the lower scoring brackets and its substantially lower mean scores provide evidence that, while the LLM can generate coherent and structurally sound text, it still lacks the depth, originality, and stylistic nuance that characterize a top fiction writer such as Patricio Pron. Pron's higher scores and favorable and were provided to the assessors in the rubric, we report them as complementary descriptive figures. evaluations across all criteria underscore his ability to produce engaging, original, and creatively rich content.",
            "score": 0.3725008626201304,
            "section_title": "Results and Discussion",
            "char_start_offset": 16093,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 184
                },
                {
                    "start": 187,
                    "end": 293
                },
                {
                    "start": 296,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1276
                },
                {
                    "start": 1279,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1467
                },
                {
                    "start": 1470,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2048
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06964111328125
        },
        {
            "corpus_id": "268531405",
            "title": "Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices",
            "text": "Maintaining factual consistency when reasoning is one of the key difficulties LLMs encounter.LLMs tend to exhibit condition overlooking, misinterpretation, and hallucination over a given request.For example, in a recent study examining GPT-3 (Khatun and Brown, 2023),researchers discovered that while the model adeptly filters out blatant conspiracies and stereotypes, it falters when dealing with everyday misconceptions and discussions.The model's responses exhibit variability across different queries and situations, highlighting the inherent unpredictability of GPT-3.\n\nSimilarly, a work by Zhou et al. (Zhou et al., 2024) reveals that LLMs, such as ChatGPT and Claude, fail to communicate uncertainties when answering questions.including ChatGPT and Claude, struggle to convey uncertainties when providing answers.Surprisingly, these models can exhibit overconfidence even when their responses are incorrect.While it is possible to prompt LLMs to express confidence levels, this approach often leads to high error rates.Furthermore, the study highlights a critical challenge: users find it diffi-cult to assess the correctness of LLM responses due to biases introduced by the models' tone and style.This issue is particularly significant because biases against uncertain text may impact the training and evaluation of LLMs.\n\nIn another study by Laban et al. (Laban et al., 2023), the ability of LLMs to serve as factual reasoners is investigated through the lens of factual judgment in text summarization.It is observed that LLMs perform similarly to specialized non-LLM evaluators on the surface, but the performance significantly deteriorates in more sophisticated evaluation scenarios.\n\nIn a similar vein, Laban et al. (Laban et al., 2023) examine the inconsistency of LLM responses by proposing a new evaluation benchmarking procedure called SUMMEDITS, which shows that most of the existing LLMs, including the best model GPT-4, which is still inferior to human performance, struggle to generate consistent responses.",
            "score": 0.37244935236641935,
            "section_title": "Factual Inconsistency and Unreliability of LLM Responses",
            "char_start_offset": 60083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 195
                },
                {
                    "start": 195,
                    "end": 438
                },
                {
                    "start": 438,
                    "end": 573
                },
                {
                    "start": 575,
                    "end": 734
                },
                {
                    "start": 734,
                    "end": 820
                },
                {
                    "start": 820,
                    "end": 914
                },
                {
                    "start": 914,
                    "end": 1026
                },
                {
                    "start": 1026,
                    "end": 1205
                },
                {
                    "start": 1205,
                    "end": 1329
                },
                {
                    "start": 1331,
                    "end": 1511
                },
                {
                    "start": 1511,
                    "end": 1694
                },
                {
                    "start": 1696,
                    "end": 2027
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.071044921875
        },
        {
            "corpus_id": "275954156",
            "title": "Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant",
            "text": "In [Kabir et al.(2023)], the responses provided by ChatGPT are analyzed in comparison to those from Stack Overflow. Additionally, some studies focus on studying the current limitations of ChatGPT [Borji(2023), Koco\u0144 et al.(2023), Mitrovi\u0107 et al.(2023)], finding that the model frequently generate inaccurate information. ChatGPT alternatives such as Llama or LaMDA were evaluated in their foundational papers [Touvron et al.(2023a), Touvron et al.(2023b), Thoppilan et al.(2022)]. \n\nWithin an educational context, several works can be found that deal with evaluating LLMs. For instance, in [Jacobsen and Weber(2023)], the quality of the feedback provided by ChatGPT in the design of learning objectives is evaluated, testing different prompts for this purpose. In [Laskar et al.(2023)], ChatGPT is evaluated alongside other LLMs in a series of academic benchmarks. Similarly, in [Savelka et al.(2023)], it was studied whether ChatGPT was capable of passing university programming course exams. On the other hand, there are also studies focused on working with code. In [Jalil et al.(2023)], the effectiveness of ChatGPT in teaching software testing courses is evaluated. In [MacNeil et al.(2023)], GPT-3 and GPT-4 are applied to detect logical errors in three code examples, and their performance is compared with that of beginner students. In [Hellas et al.(2023)], the ability of the Codex and ChatGPT models to provide feedback to pre-university students on simple programming problems is studied. More recently, in [Azaiz et al.(2024)], authors conduct work similar to that presented in [Hellas et al.(2023)], but using GPT-4 to provide feedback for programming tasks in a university context.",
            "score": 0.3723631676100445,
            "section_title": "Related Work",
            "char_start_offset": 56387,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 764,
                    "end": 784,
                    "matchedPaperCorpusId": "258967462"
                },
                {
                    "start": 879,
                    "end": 900,
                    "matchedPaperCorpusId": "257557506"
                },
                {
                    "start": 1069,
                    "end": 1088,
                    "matchedPaperCorpusId": "256627809"
                },
                {
                    "start": 1344,
                    "end": 1364,
                    "matchedPaperCorpusId": "259129577"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047943115234375
        },
        {
            "corpus_id": "267627712",
            "title": "Large Language Models \"Ad Referendum\": How Good Are They at Machine Translation in the Legal Domain?",
            "text": "Our study has been one of the early studies to compare the translation quality of two LLMs against a state-of-the-art NMT system in four different languages to analyse the relative quality of MT capability offered by LLM systems. We chose languages with different characteristics, that is, a high-resourced language like Spanish, a middle-resourced language like Brazilian Portuguese, and two lowresourced languages like Turkish and Catalan. All of these languages were analysed in combination with English. We first conducted an automatic quality evaluation using the most used AEMs in industry and academia, followed by HE with three professional translators, who evaluated the machine translations proposed by the different systems by following the best practices for assessing translation quality (L\u00e4ubli et al. 2020). \n\nBy looking at the AEMs alone, we could extract that GT was the best performing system overall in terms of similarity to the reference, gold standard translation. GT obtained the best scores in AEMs in 15 out of 16 evaluations, winning by a landslide. However, it is worth stressing that, while the automatic metrics may provide an initial insight into the performance of the systems, the HE provides a more comprehensive qualitative analysis that investigates further key aspects that a professional translator would check for a legal translation, emphasising terminology or context coherence, for example. As a consequence, HE has been established as the gold standard method for translation quality evaluation by academia and industry. \n\nIf we look at the HE results, we can see that GT no longer obtains the best results in the evaluation. In this evaluation task, GPT-4 and GT obtain very similar results. Human evaluators assessed GPT-4 and GT similarly as providing the most accurate and fluent output in most languages combinations analysed (Brazilian Portuguese, Spanish, and Catalan). If there was a difference in these evaluations, it was by a couple of segments, which indicated that the difference was not substantial. Nevertheless, if we looked at the MT output more in-depth, we could see that GPT-4 translated key concepts more consistently throughout the whole document and kept using the appropriate legal terminology. GT, on the other hand, tended to use different terms for the same concept, and was changing its choice throughout the text.",
            "score": 0.3722142768588461,
            "section_title": "Conclusion",
            "char_start_offset": 34189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1562
                },
                {
                    "start": 1565,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2260
                },
                {
                    "start": 2261,
                    "end": 2384
                }
            ],
            "ref_mentions": [
                {
                    "start": 801,
                    "end": 821,
                    "matchedPaperCorpusId": "214634963"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03448486328125
        },
        {
            "corpus_id": "258298706",
            "title": "Can ChatGPT be used to generate scientific hypotheses?",
            "text": "GPT-4: Several factors can make the proposed experiments difficult to evaluate in reality: \n\n1. Subjectivity in assessment: Evaluating the quality, novelty, and relevance of scientific hypotheses can be subjective, as different experts may have different perspectives or biases. This makes it challenging to reach a consensus on the merit of the hypotheses generated by a language model compared to those generated by humans. \n\n2. Expertise availability: Gathering a sufficient number of experts from various fields to evaluate the generated hypotheses can be time-consuming and resource-intensive. Experts may also have limited availability or may prioritize their own research over participating in such experiments. 3. Varying language model performance: The performance of language models like GPT can vary depending on the quality of input data, the field of study, and the specificity of the questions asked. As a result, some hypotheses may be more accurate or relevant than others, complicating the evaluation process. 4. Limitations in interdisciplinary understanding: In interdisciplinary hypothesis generation, it can be difficult for evaluators to possess expertise in all the relevant fields. This may lead to misinterpretations of the generated hypotheses or difficulties in assessing their novelty and potential impact. 5. Intellectual property and ethical concerns: Sharing scientific ideas, especially those with potential commercial value, could raise intellectual property concerns. Additionally, the ethical implications of using AI-generated hypotheses in scientific research may be a matter of debate, which could complicate the evaluation process. 6. Incomplete knowledge of the language model: Language models like GPT have a knowledge cutoff and may not be aware of the latest developments or breakthroughs in a specific field. This could affect the quality and relevance of the generated hypotheses, making it harder to evaluate their merit compared to humangenerated hypotheses. 7. Experimental design and control: Designing experiments that effectively control for confounding factors can be challenging. For example, ensuring that human participants and the language model receive the same information and operate under the same conditions is crucial to obtain a fair comparison. However, maintaining such control can be difficult in practice.",
            "score": 0.3719118508461717,
            "section_title": "2: Can you explain why proposed experiments can be difficult to evaluate in reality?",
            "char_start_offset": 39850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 93,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2308
                },
                {
                    "start": 2309,
                    "end": 2372
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.330078125
        },
        {
            "corpus_id": "257804696",
            "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
            "text": "The authors of the original paper found that inter-annotator agreement on judging human-written and LLM-generated summaries is very low, with Krippendorff's alpha at 0.07. \n\n2. G-EVAL may have a bias towards the LLMgenerated summaries because the model could share the same concept of evaluation criteria during generation and evaluation. \n\nOur work should be considered as a preliminary study on this issue, and more research is needed to fully understand the behavior of LLM-based evaluators to reduce its inherent bias towards LLMgenerated text. We highlight this concern in the context that LLM-based evaluators may lead to self-reinforcement of LLMs if the evaluation score is used as a reward signal for further tuning. And this could result in the over-fitting of the LLMs to their own evaluation criteria, rather than the true evaluation criteria of the NLG tasks. \n\nThe Effect of Chain-of-Thoughts We compare the performance of G-EVAL with and without chain-of-thoughts (CoT) on the SummEval benchmark. Table 1 shows that G-EVAL-4 with CoT has higher correlation than G-EVAL-4 without CoT on all dimensions, especially for fluency. This suggests that CoT can provide more context and guidance for the LLM to evaluate the generated text, and can also help to explain the evaluation process and results. And it is shown that CoT is more useful on consistency and fluency dimensions. We also provide results of G-EVAL with a simple prompting baseline on SummEval (only asking GPT-4 to score a summary from 1-5 on each dimension, without detailed task introduction, evaluation criteria and CoT). \n\nThe Effect of Probability Normalization We compare the performance of G-EVAL with and without probability normalization on the Sum-mEval benchmark. Table 1 shows that, on Kendall-Tau correlation, G-EVAL-4 with probabilities is inferior to G-EVAL-4 without probabilities on Sum-mEval. We believe this is related to the calculation of Kendall-Tau correlation, which is based on the number of concordant and discordant pairs.",
            "score": 0.37190101367900774,
            "section_title": "Analysis",
            "char_start_offset": 16891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 174,
                    "end": 338
                },
                {
                    "start": 341,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1600
                },
                {
                    "start": 1603,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2025
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55078125
        },
        {
            "corpus_id": "273350617",
            "title": "Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics",
            "text": "Large Language Models (LLMs) can perform many tasks effectively, even in few-shot or zero-shot settings. Recently, LLMs have also been used to evaluate natural language generation tasks, in replacement of human evaluation. LLM-as-a-Judge shows useful properties as an evaluation metric, for instance Faysse et al. (2023) illustrated using GPT-4 that it can be highly correlated with human judgement, format and task agnostic and comparable across tasks. Zheng et al. (2023) describe limitations of LLM-as-a-Judge, including position, verbosity and self-enhancement biases as well as poor performance at grading math or reasoning tasks. Other limitations are expressed by Kim et al. (2023) targeting proprietary LLMs such as GPT-4 for their closed source nature, uncontrolled versioning, and their high costs. Prometheus 2 (Kim et al., 2024) is designed for evaluating language models and shows high correlations with proprietary LLMs and human evaluations. Besides, its open-source nature mitigates some of the aforementioned issues. Liu et al. (2023) suggest that LLMs aligned from human feedback overfit to reference-less human evaluation of summaries, which they observed to be biased towards longer summaries and to suffer from low inter-annotator agreement.",
            "score": 0.37177657331280145,
            "section_title": "LLM-as-a-Judge evaluation",
            "char_start_offset": 2949,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1262
                }
            ],
            "ref_mentions": [
                {
                    "start": 300,
                    "end": 320,
                    "matchedPaperCorpusId": "264426560"
                },
                {
                    "start": 1034,
                    "end": 1051,
                    "matchedPaperCorpusId": "254685611"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.368896484375
        },
        {
            "corpus_id": "259376840",
            "title": "Rating Short L2 Essays on the CEFR Scale with GPT-4",
            "text": "Automated writing evaluation (AWE) systems are commonly used to evaluate test-taker writing. AWE systems are deployed on large-scale, highstakes writing assessments used for admissions to higher education institutions, and for lower-stakes US state writing assessments that provide information about K-12 students' academic writing performance. These systems typically use featureengineering approaches that include rule-based and statistical natural language processing (NLP) methods. NLP is used to extract features from essay writing responses that are characteristic of writing quality. Features may include errors in grammar and spelling, discourse structure, discourse coherence, vocabulary usage, and sentence variety. Features may be rule-based or statistically derived. Statistical model methods, such as straightforward linear regression, are used to train (build) AWE scoring models for high-stakes scoring of writing assessments. Detailed descriptions of systems are avail-able for major systems, including e-rater\u00ae, Intelligent Essay Assessor\u2122, Intellimetric\u00ae, and PEG (Shermis and Burstein, 2013), and Cambium's automated essay scoring system (Lottridge, in press). \n\nRecent advances in language modeling with neural transformer architectures (OpenAI, 2023;Brown et al., 2020) have the potential to revolutionize AWE. These large language models (LLMs) demonstrate an incredible potential to analyze and evaluate text which has implications for the future of AWE. In addition, GPT's intuitive, text-based interface lowers barriers for use, potentially increasing accessibility and adoption of these tools for AWE. The assumptions about how LLMs -specifically GPT-4 -can be used for AWE tasks, such as automated scoring and feedback need to be evaluated to determine how we can use them beneficially, and particularly to ensure that they can be used in a fair and ethical manner (Burstein, 2023). \n\nPrevious research evaluated GPT-3.5 for essay scoring tasks in an L2 context (Mizumoto and Eguchi, 2023). In this paper, we evaluate GPT-4 for a similar task, comparing it to GPT-3.5, human judgement, and a strong baseline using current AWE methods. We also explore various aspects that affect the accuracy of GPT's ratings, and its fairness across gender and L1.",
            "score": 0.37174235344900947,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1909
                },
                {
                    "start": 1912,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2161
                },
                {
                    "start": 2162,
                    "end": 2275
                }
            ],
            "ref_mentions": [
                {
                    "start": 1271,
                    "end": 1290,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68212890625
        },
        {
            "corpus_id": "268553916",
            "title": "Language Models Can Reduce Asymmetry in Information Markets",
            "text": "However, given the influence of sequence on Elo scores, we calculate scores across 1000 different game orders and present the average results and standard deviations.The results, displayed in Figure 5 (left), indicate a notable improvement in relative performance as the budget increases, confirming the functional expectations of the environment.\n\nInspection Improves Answer Quality for Equal Credits Spent.Having verified the functionality of the simulator, we proceed to evaluating the influence of inspection on answer quality.The experiments continue to employ Llama 2 (70b) to maintain consistency in the analysis.Two sets of runs are conducted with varying budgets: one with inspection and one with only metadata.\n\nFor each question and corresponding budget, the GPT-4 evaluator assesses two answers: one obtained with inspection and another without.The cumulative wins for each setting, tabulated against the amount of credit expended (which may be different from the total budget), are illustrated in Figure 5 (right).The findings reveal a trend of higher answer quality when passages are chosen with inspection, especially at higher spending levels.Conversely, in the absence of inspection, the quality of answers plateaus post an expenditure of $50 in credits.\n\nImpact of Different LLMs on Answer Quality.In this experiment, we evaluate the effect of different LLMs on the quality of answers produced for a fixed budget of $100.Each answer is scrutinized for its quality by the GPT-4 evaluator.The results, detailed in Figure 6a, demonstrate a preference hierarchy with GPT-4 yielding the most preferred answers, followed sequentially by GPT-3.5 and Llama 2 (70b).We acknowledge the potential for a self-preference bias in these outcomes, while noting that it is beyond our capacity to control for this aspect due to the unavailability of another GPT-4 level language model for comparison.\n\nEvaluating the Evaluator.The use of the GPT-4 evaluator necessitates an evaluation to affirm its reliability.We analyze the effectiveness of using the GPT-4 evaluator.A sample of 50 evaluations from various answers in our experiments is examined.Two human evaluators independently assess the answers, with the answerer's identity concealed.",
            "score": 0.37170029912780983,
            "section_title": "DYNAMICS OF THE INFORMATION BAZAAR",
            "char_start_offset": 28525,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 347
                },
                {
                    "start": 349,
                    "end": 408
                },
                {
                    "start": 408,
                    "end": 531
                },
                {
                    "start": 531,
                    "end": 620
                },
                {
                    "start": 620,
                    "end": 720
                },
                {
                    "start": 722,
                    "end": 857
                },
                {
                    "start": 857,
                    "end": 1027
                },
                {
                    "start": 1027,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1271
                },
                {
                    "start": 1273,
                    "end": 1316
                },
                {
                    "start": 1316,
                    "end": 1439
                },
                {
                    "start": 1439,
                    "end": 1505
                },
                {
                    "start": 1505,
                    "end": 1675
                },
                {
                    "start": 1675,
                    "end": 1900
                },
                {
                    "start": 1902,
                    "end": 1927
                },
                {
                    "start": 1927,
                    "end": 2011
                },
                {
                    "start": 2011,
                    "end": 2069
                },
                {
                    "start": 2069,
                    "end": 2148
                },
                {
                    "start": 2148,
                    "end": 2242
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.148193359375
        },
        {
            "corpus_id": "265157492",
            "title": "Automated title and abstract screening for scoping reviews using the GPT-4 Large Language Model",
            "text": "They also systematically evaluated prompts given to the LLM to identify a prompt that performed best at the screening task; their chosen prompt, like that of Guo et al., placed the LLM in the role of an academic reviewer. \n\nBoth of these approaches made use of a single, fixed text prompt template, which the LLM then completes with additional text representing its response (the decision to include or exclude a source), a method sometimes called 'zero-shot prompting'. Recent work has identified a number of methods which can be superior to zero-shot prompting when using LLMs for tasks that require complex or multistep reasoning. These methods include 'chain-ofthought prompting' [21], in which a complex task is broken down into a series of intermediate steps, and the 'tree of thoughts' strategy [23], in which multiple parallel chains of thought are generated, compared, and integrated. The LLM is induced to follow these complex reasoning strategies either by being given examples of multi-step reasoning on similar tasks, or by being lead through the process with a series of intermediate prompts. \n\nIn this paper, I introduce a package for the R programming language [14] called GPTscreenR that implements a chain-of-thought approach to using GPT-4 for scoping review title and abstract screening, and evaluate its performance by comparison to human reviewers. The purpose of this package is to assist and augment rather than replace human reviewers in performing scoping reviews. This package represents the first LLM-based screening tool designed specifically for scoping reviews. Further, this study provides the first report on the accuracy of LLM-based screening using the most recent iteration of the GPT model lineage, GPT-4, and using the recently developed chain-of-thought approach.",
            "score": 0.3715397080007158,
            "section_title": "Introduction",
            "char_start_offset": 3851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 224,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1802
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "276421835",
            "title": "Towards Adaptive Feedback with AI: Comparing the Feedback Quality of LLMs and Teachers on Experimentation Protocols",
            "text": "LLM-generated feedback has been explored in various educational fields. In mathematics education, Nguyen et al (2023) evaluated the effectiveness of GPT-3.5 in addressing student errors related to decimal numbers and found it generally effective in providing appropriate feedback. For writing instruction, Se\u00dfler et al (2023) utilized GPT-3.5 to generate instant feedback, demonstrating its utility in supporting writing development through constructive critiques. A significant focus has been placed on programming education, where several studies have investigated the capabilities of LLMs. Gabbay and Cohen (2024) found that while GPT-3.5 and GPT-4 are effective at detecting errors in code assignments within MOOCs, they often fall short in providing accurate or actionable feedback. Est\u00e9vez-Ayres et al (2024) observed limitations in LLMs' ability to evaluate exercises involving concurrency errors, highlighting challenges in understanding complex programming concepts. Additionally, Koutcheme et al (2024) noted a tendency for GPT-4 to provide overly positive feedback in introductory programming courses, potentially overlooking critical issues that require attention. \n\nWhile these advancements are promising, research on LLM-based feedback generation in science education is still developing. From a theoretical standpoint, Zhai and Nehm (2023) underscore the importance of embracing AI's potential in formative assessment within science education, advocating for constructive dialogue rather than dismissing its impact. In an earlier review, Zhai et al ( 2020) examined machine learning-based science assessments across three axes -technical implementation, validity, and pedagogical features -and found that most studies concentrated on validity, often neglecting technical and pedagogical aspects. \n\nRecent efforts have begun to explore the application of language models in science education assessment. Wu et al (2023) applied a pre-trained BERT model with zeroshot prompting to score students' written responses, demonstrating the feasibility of using LLMs for assessment purposes. Similarly, Latif and Zhai (2024) compared a finetuned BERT model with GPT-3.5 for automatically scoring student answers, finding that GPT-3.5 significantly outperforms BERT in terms of scoring accuracy. However, these studies primarily focus on scoring student responses rather than generating detailed feedback that can guide learning.",
            "score": 0.37127917788819087,
            "section_title": "LLM-based feedback",
            "char_start_offset": 9667,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1176
                },
                {
                    "start": 1179,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1810
                },
                {
                    "start": 1813,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2097
                },
                {
                    "start": 2098,
                    "end": 2300
                },
                {
                    "start": 2301,
                    "end": 2434
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 117,
                    "matchedPaperCorpusId": "259287479"
                },
                {
                    "start": 306,
                    "end": 325,
                    "matchedPaperCorpusId": "261558593"
                },
                {
                    "start": 593,
                    "end": 616,
                    "matchedPaperCorpusId": "271221942"
                },
                {
                    "start": 990,
                    "end": 1012,
                    "matchedPaperCorpusId": "269626718"
                },
                {
                    "start": 1334,
                    "end": 1354,
                    "matchedPaperCorpusId": "259256477"
                },
                {
                    "start": 1918,
                    "end": 1933,
                    "matchedPaperCorpusId": "256105269"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.348388671875
        },
        {
            "corpus_id": "277818487",
            "title": "Assessing ChatGPT 4.0\u2019s Capabilities in the United Kingdom Medical Licensing Examination (UKMLA): A Robust Categorical Analysis",
            "text": "LLM's breadth over depth of knowledge cautions against use in applications and technologies with patient-specific medical impact. Importantly, Maitland et al. recommend that errors should be fully understood before ChatGPT is utilized to supplement clinical practice 18 . \n\nIn the clinical environment, there are no set options for the AI to choose from. When multiple-choice options were removed and GPT-4 was prompted to independently determine the best answer, GPT \u2212 4 showed a clear reduction in accuracy across papers one (-24.8%) and two (-14.9%). This error rate was amplified when the complexity of open-ended questions was increased, as ChatGPT had a 15.7% lesser average accuracy while answering multistep questions, as compared to single step. Despite question stems asking for a single best answer, we found that GPT-4 began to provide a greater proportion of 'indeterminate' responses, especially when tackling patient pharmacology scenarios. However, inconclusive answers may prove detrimental in an exam format, but many patient and service provider factors alter the approach to managing patients, and a definitive answer may prematurely focus the treatment and prove detrimental to the patient's care. Acknowledging uncertainty or variation in the evidence and presenting multiple options may serve junior doctors better by giving general guidance before allowing clinician-led final decisions. \n\nDistractors in clinical scenarios may highlight further gaps in GPT-4's suitability for clinical practice. \n\nCounterintuitively, eight open-ended questions were answered correctly by the AI and were 'indeterminate' with multiple-choice options provided. Plausible alternatives and red herrings may impact GPT-4's evaluation of answers and practitioners must be cautious of the reasonable and thus convincing errors in diagnosis and treatment AI can suggest. Nuances in vocabulary and patterns in the exam questions' phrasing are familiar to junior doctors, but AI may require more training before reaching equivalence. By extension, the many distractors in practice will require thorough filtering, a process inherent to taking a good clinical history. Furthermore, GPT-4's output may produce convincing but entirely false suggestions in the well-described 'hallucination' phenomenon 9,19,20 .",
            "score": 0.3711481134697856,
            "section_title": "Discussion",
            "char_start_offset": 10048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 271
                },
                {
                    "start": 274,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2166
                },
                {
                    "start": 2167,
                    "end": 2307
                }
            ],
            "ref_mentions": [
                {
                    "start": 267,
                    "end": 269,
                    "matchedPaperCorpusId": "268425613"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08660888671875
        },
        {
            "corpus_id": "270562620",
            "title": "Chumor 1.0: A Truly Funny and Challenging Chinese Humor Understanding Dataset from Ruo Zhi Ba",
            "text": "Annotator?\n\nFigure 4 presents the results of GPT-4o preference study.We prompt the GPT-4o by: 20.9% 6.6% 72.6%\n\nHuman wins Tie GPT-4o wins\n\nFigure 4: Preference annotation from GPT-4o.We prompt GPT-4o to choose a better explanation between its own explanation and the explanation written by human.We note that the GPT-4o's preference is significantly different from the human preference in Figure 1.\n\n\"\u5bf9\u4e8e\u7b11\u8bdd\"[Joke]\" \u4ee5\u4e0b\u4e24\u4e2a\u89e3\u91ca \u89e3\u91ca1:\n\n\u5904\uff1f\", which translates to, \"For the joke\"[Joke]\"\n\nThe following two explanations Explanation 1: [Explanation 1] Explanation 2: [Explanation 2] Which one better explains the reason for why this joke is funny?\"Though prior works have shown the promise of employing LLMs as evaluators (Fu et al., 2023;Liu et al., 2023c).We note that as the evaluator for humor explanation, GPT-4o's preference significantly differs from the human preference compared to Figure 1.Our results show that GPT-4o significantly favors its own explanation than explanation from human, which aligns with the finding in previous study that LLM evaluators prefer their own answer (Panickssery et al., 2024).But for a small proportion of the dataset, GPT-4o prefers explanation from human than the explanation from itself, and Figure 2 demonstrates one such example, which shows the potential of leveraging LLMs in automatic humor evaluation.",
            "score": 0.37109036218501146,
            "section_title": "F Can LLMs Serve as Preference",
            "char_start_offset": 27776,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 12,
                    "end": 69
                },
                {
                    "start": 69,
                    "end": 110
                },
                {
                    "start": 112,
                    "end": 138
                },
                {
                    "start": 140,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 297
                },
                {
                    "start": 297,
                    "end": 399
                },
                {
                    "start": 401,
                    "end": 426
                },
                {
                    "start": 428,
                    "end": 475
                },
                {
                    "start": 477,
                    "end": 635
                },
                {
                    "start": 635,
                    "end": 745
                },
                {
                    "start": 745,
                    "end": 887
                },
                {
                    "start": 887,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1339
                }
            ],
            "ref_mentions": [
                {
                    "start": 726,
                    "end": 744,
                    "matchedPaperCorpusId": "257804696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.060638427734375
        },
        {
            "corpus_id": "257900846",
            "title": "Evaluation of GPT and BERT-based models on identifying proteinprotein interactions in biomedical text",
            "text": "On November 30 th , 2022, OpenAI released ChatGPT, a natural and engaging conversation tool capable of producing contextually relevant responses based on text data. ChatGPT was fine-tuned on the GPT-3.5 series. On March 14 th , 2023, OpenAI introduced its most advanced and cutting-edge system to date, GPT-4, which has surpassed its predecessors by producing more dependable outcomes. is capable of producing, modifying, and cooperating with users in various creative and technical writing tasks, such as songwriting, screenplay creation, and imitating user writing styles [28]. The advancements in GPT models, from GPT-3 to GPT-4, showcase the rapid progress in NLP, opening up a wide range of applications [29,30]. \n\nSeveral studies [31,32] have been published evaluating the performance of GPT models for problem-solving on various standardized tests. It has been shown that they can achieve performance comparable to or even better than humans [33] and can pass high-level professional standardized tests such as the Bar test [34], the Chinese Medical Practitioners examination [35], and the Japanese National Nurse examination [36]. Another study [37] evaluates that ChatGPT achieves the equivalent of a passing score for a third-year medical student in the United States Medical Licensing Examination. A group of researchers explored the performance of advanced LLMs, including ChatGPT, GPT-4, and Google Bard, in mastering complex neurosurgery examination material and found that GPT-4 achieved a score of 82.6%, outperforming ChatGPT and Google Bard. A recent study assessed the efficacy of GPT in relation to specific standardized admissions tests in the UK. These findings revealed that GPT demonstrates greater proficiency in tasks requiring language comprehension and processing yet exhibits limitations in applications involving scientific and mathematical knowledge [38]. To the best of our knowledge, no study has evaluated the effectiveness of GPT models in extracting PPIs from biomedical texts. In this article, we present a thorough evaluation of the PPI identification performance of the GPT-based models and compare these with the state-ofthe-art BERT-based NLP models.",
            "score": 0.37102524060633935,
            "section_title": "Introduction",
            "char_start_offset": 6218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 574,
                    "end": 578,
                    "matchedPaperCorpusId": "258375195"
                },
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "257572753"
                },
                {
                    "start": 949,
                    "end": 953,
                    "matchedPaperCorpusId": "254854575"
                },
                {
                    "start": 1083,
                    "end": 1087,
                    "matchedPaperCorpusId": "260670145"
                },
                {
                    "start": 1153,
                    "end": 1157,
                    "matchedPaperCorpusId": "259842568"
                },
                {
                    "start": 1881,
                    "end": 1885,
                    "matchedPaperCorpusId": "258332468"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1585693359375
        },
        {
            "corpus_id": "267301349",
            "title": "PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models",
            "text": "We study the pairwise win rate among various LLMs evaluated by PROXYQA and compare the results with human evaluation to validate the effectiveness of PROXYQA. \n\nSetup Five well-educated postgraduate students are engaged; all have not participated in annotating the meta and proxy-questions of PROXYQA; they are required to score and rank the randomly sampled reports generated by different LLMs. The scoring guideline is shown in Appendix A.4. We sampled ten meta-questions from PROXYQA and employed four LLMs to generate comprehensive reports. As a further comparison, we also follow the settings in MT-Bench (Zheng et al., 2023) that adopt LLM-as-judges, which directly rate the generated report based on the scoring guideline. \n\nSimilarly, we utilize GPT-Seperate (GPT-S), which  evaluates a single report at a time, and GPT-Batch (GPT-B), which evaluates and compares multiple reports simultaneously, to score and rank each report. Given that five human evaluators are involved in the comparison, we ensure fairness and robustness by requiring GPT-S, GPT-B, and our proposed PROXYQA to evaluate each report five times. We then calculate the average win rate based on the pairwise comparison. \n\nResult GPT-as-judges over-confident on the contents generated by GPT models, while ProxyQA's choice is highly correlated with humans. As shown in Figure 5, majority of the evaluation results highly correlated with the human's choices. Specifically, evaluators generally recognize the quality of the reports generated by GPT-4-Turbo and New Bing, i.e., their win rates are much higher than those of Vicuna and LLama2-Chat. It is worth noting, however, that GPT-S and GPT-B exhibit overconfidence in the quality of the reports generated by GPT-4-Turbo compared to New Bing. In contrast, both human evaluators and ProxyQA exhibit a preference for New Bing over GPT-4-Turbo. This outcome attests to the effectiveness of ProxyQA and demonstrates the correlation between ProxyQA and human evaluation.",
            "score": 0.370902497202771,
            "section_title": "Win Rate",
            "char_start_offset": 19866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 161,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1992
                }
            ],
            "ref_mentions": [
                {
                    "start": 610,
                    "end": 630,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2178955078125
        },
        {
            "corpus_id": "270210580",
            "title": "NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism",
            "text": "Human annotations were conducted to validate the effectiveness of the automatic GPT-4 based assessment and its alignment with human judgments on both journalistic writing proficiency and safety adherence.Human evaluations are following the same annotation instructions of GPT-4 based evaluations in Table 10.We use five different LLMs including GPT-4-11063 , Xinyu2-70B4 , AquilaChat2-34B5 , Baichuan2-53B6 , and Qwen-14B7 , each with distinctive characteristics, to generate generate candidate responses and involve three human annotators to manually assess the outputs generated by these models.We then conduct a rigorous analysis with human annotation results based on interannotator agreement among three annotators and correlations between results from our proposed protocols and humans.\n\nFor journalistic writing proficiency, we prepare 200 annotation samples which cover all five editorial tasks and five LLMs and each model produces outputs for eight test samples randomly selected from our benchmark dataset for each editorial task.For each annotation sample, each annotator should provide four scores for the four evaluation facets of journalistic writing proficiency.Finally, among the 800 annotations, annotators have a high interannotator agreement with a Krippendorff's alpha value of 0.9188.\n\nFor safety adherence, we follow a similar approach and prepare 600 annotation samples for five editorial tasks and six evaluation facets.Each model produces outputs for four random test samples for each evaluation facet in each editorial task.Three annotators highly agree with each other with Krippendorff's \u03b1 of 0.8542.\n\nHuman evaluation results are consolidated through arithmetic averaging and majority voting for journalistic writing and safety respectively and compared with GPT-4 based scores.The correlations of GPT-4 with human evaluation for journalistic writing are 0.625, 0.719 and 0.815 in terms of Kendall's \u03c4 , Spearman's rank and Pearson correlation coefficients, respectively. 8The correlations for safety adherence evaluation are 0.627, 0.627 and 0.625 in terms of Kendall's \u03c4 , Spearman's rank and Pearson correlation coefficients, respectively.",
            "score": 0.3704471851146437,
            "section_title": "Human Validation of GPT-4 Scores",
            "char_start_offset": 15031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 204,
                    "end": 308
                },
                {
                    "start": 308,
                    "end": 597
                },
                {
                    "start": 597,
                    "end": 792
                },
                {
                    "start": 794,
                    "end": 1041
                },
                {
                    "start": 1041,
                    "end": 1178
                },
                {
                    "start": 1178,
                    "end": 1306
                },
                {
                    "start": 1308,
                    "end": 1445
                },
                {
                    "start": 1445,
                    "end": 1551
                },
                {
                    "start": 1551,
                    "end": 1629
                },
                {
                    "start": 1631,
                    "end": 1808
                },
                {
                    "start": 1808,
                    "end": 2003
                },
                {
                    "start": 2003,
                    "end": 2172
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06927490234375
        },
        {
            "corpus_id": "263835211",
            "title": "Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity",
            "text": "The assessment is conducted using the United States Medical Licensing Examination (USMLE) Kung et al. (2023) and the MultiMedQA benchmark Singhal et al. (2023), comparing GPT-4's performance against earlier models like GPT-3.5 and models specifically fine-tuned on medical knowledge. The results demonstrate that GPT-4 significantly outperforms its predecessors, achieving scores on the USMLE that exceed the passing threshold by more than 20 points and delivering the best overall performance without specialized prompt crafting or domain-specific finetuning. \n\nWhile large language models show promise on medical datasets, the introduction of automation in the healthcare field still requires extreme caution Shen et al. (2023). Existing metrics and benchmarks are often developed for highly focused problems. Evaluating LLM outputs in supporting real-world decision-making poses challenges Singhal et al. (2023), including the stability and robustness of personalized recommendations and inferences in real-world contexts. Using large language models carries significant risks, including inaccurate ranking recommendations Hirosawa et al. ( 2023) (e.g., differential diagnosis) and sequencing Zhang et al. (2023c) (e.g., information gathering and testing), as well as factual errors Shen et al. (2023), particularly important omissions and erroneous responses.",
            "score": 0.37037449642679776,
            "section_title": "Impact",
            "char_start_offset": 17749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 560
                },
                {
                    "start": 563,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1363
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 108,
                    "matchedPaperCorpusId": "254876189"
                },
                {
                    "start": 1196,
                    "end": 1216,
                    "matchedPaperCorpusId": "255893761"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1187744140625
        },
        {
            "corpus_id": "264591440",
            "title": "Analyzing Multilingual Competency of LLMs in Multi-Turn Instruction Following: A Case Study of Arabic",
            "text": "Table 5 shows the model ranking based on the ARA-BIC MT-BENCH scores. The first, second and third columns of the tables give the model's average score for the first turn across all questions, the average score for the second turn across all questions, and the average of both, respectively. Per-category scores could be seen in Figure 2. For comparison, Figure 1 (and Table 6 in Appendix A.1) give the per-category scores for the original English MT-Bench for the same models. \n\nAs the results show, GPT-4 and GPT-3.5-turbo are better than any open LLM we tested by a large margin with average scores of 8.27 and 7.13 out of 10, respectively. Because GPT-4 is used as the judge, there exists the potential for bias in favor of its own responses, which has been discussed in the MT-Bench paper (Zheng et al., 2023). \n\nIn the English MT-Bench, the two GPT models score 8.99 and 7.0, respectively. Hence, GPT-4 is approximately one point lower in terms of the Arabic score compared to the English benchmark. By manual inspection of the responses, we qualitatively confirm that the proficiency of GPT models in Arabic is lower than English as indicated by the scores. Therefore, we compare the scores across Arabic and English benchmarks in Section 4.3. \n\nOverall, LLMs fine-tuned specifically for Arabic or for multilingual capabilities (e.g. Jais, Phoenix) are better than generic models such as some mem-",
            "score": 0.3702200348648158,
            "section_title": "How do open LLMs fare in following open-ended instructions written in Arabic?",
            "char_start_offset": 13600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 476
                },
                {
                    "start": 479,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1403
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.021087646484375
        },
        {
            "corpus_id": "269756840",
            "title": "Hire Me or Not? Examining Language Model's Behavior with Occupation Attributes",
            "text": "We evaluated the following three LLMs. To compare with prior works, we used RoBERTa-large as a baseline model and two LLMs with different alignment methods, where the GPT family uses Reinforcement Learning with Human Feedback (RLHF) and Llama2-70b-chat uses both RLHF and Supervised Fine-Tuning (SFT). We also evaluated a more recent GPT model, GPT-4o-mini, as a comparison to GPT-3.5-turbo. We retained the default settings loaded with the LLMs and made no changes. \n\n\u2022 RoBERTa-large (Liu et al., 2019) fine-tuned on SQuAD v2.0 (Rajpurkar et al., 2018) \u2022 GPT-3.5-turbo (OpenAI, 2021) \n\n\u2022 Llama2-70b-chat (Touvron et al., 2023) Figure 3: Confirmation metric (comparison between Q 1 and Q 2 ) for the three language models; lower values indicate gender bias. \n\nFigure 3 displays the Confirmation metric, measured as whether the LLM's Q 2 subject selection matches its Q 1 individual qualification assessment for that subject. Compared to RoBERTa-large, GPT-3.5-turbo exhibits higher variance in Confirmation, indicating greater fluctuation in its decision as additional subject background information is provided. Llama2-70b-chat demonstrates lower variance but with some outliers, indicating generally stable but occasional deviations from its own qualification judgments. Figure 4 shows the Consistency metric which evaluates whether LLMs maintain consistent outputs across the meaning-preserving question pairs with different answer choices. Notably, both GPT-3.5-turbo and Llama2-70b-chat exhibit lower overall scores than RoBERTa-large. The Consistency score of GPT-3.5-turbo distribution is concentrated toward 0, indicating the model tends to modify its behavior when providing more neutral options. The score of Llama2-70b-chat is between RoBERTalarge and GPT-3.5-turbo. \n\nAs shown in Figure 5, analyzing Confirmation and Consistency jointly shows interesting patterns across LLMs. RoBERTa-large demonstrates a relatively linear relationship, where occupations with high Confirmation scores also have high Consistency scores.",
            "score": 0.3702053979885857,
            "section_title": "Results",
            "char_start_offset": 13184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 466
                },
                {
                    "start": 469,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1777
                },
                {
                    "start": 1780,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2032
                }
            ],
            "ref_mentions": [
                {
                    "start": 529,
                    "end": 553,
                    "matchedPaperCorpusId": "47018994"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049407958984375
        },
        {
            "corpus_id": "274234997",
            "title": "Can AI grade your essays? A comparative analysis of large language models and teacher ratings in multidimensional essay scoring",
            "text": "To compare how strongly individual evaluation categories impact the overall rating for teachers and LLMs (RQ3), we correlate single criteria scores with the overall rating for teachers and each model. Overall, it is noteworthy that the correlations between all features of GPT-3.5 and GPT-4 are higher than those for human judgments. This can be attributed to the auto-regressive nature of LLMs, where each new token generation is conditioned on the previous output, implicitly incorporating prior evaluations into the context. This effect is particularly pronounced in the smaller GPT-3.5 model, where correlations between ratings are nearly always  \u2265 0.8. The high inter-criteria correlations may indicate that the assessment of each criterion relies more on preceding ratings than solely on the original text. \n\nIn contrast, o1 exhibits fairly lower correlations between criteria, which may stem from its larger number of parameters and enhanced context understanding, allowing it to focus more on the input text than previous ratings. Compared to the previous GPT models, human raters and o1 demonstrate lower inter-criteria correlations, suggesting that their evaluations are less influenced by preceding ratings and more independently based on each criterion's merits. \n\nConversely, the LLaMA 3 and Mixtral models consistently show low correlations across all criteria, highlighting a distinct evaluation pattern. This lower inter-criteria correlation suggests that the opensource model assess each criterion more independently, potentially reflecting a different internal reasoning process or a less integrated evaluation strategy compared to closed-source models. \n\nWhen examining which evaluations criteria seem to highly impact overall scores, both human raters and LLMs agree that criteria Plot Logic and Expression & Sentence Structure are highly relevant to the overall grade, whereas criterion Heading is less relevant. Major differences are observed in how criterion Main Part influences the overall grade for teachers compared to GPT-4, while o1 is able to include this information more strongly. Specifically, for teachers, the Main Part is a major determinant of the overall assessment, whereas GPT-4 places greater emphasis on criterion Spelling & Punctuation and Verbal image, which is not as decisive for human evaluators. This discrepancy highlights a potential risk associated with using LLMs for assessing student texts: the models may inherently focus more on linguistic surface features, such as Spelling & punctuation.",
            "score": 0.37013904972690514,
            "section_title": "Impact of Evaluation Categories on Overall Essay Scoring",
            "char_start_offset": 27731,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1671
                },
                {
                    "start": 1674,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2112
                },
                {
                    "start": 2113,
                    "end": 2343
                },
                {
                    "start": 2344,
                    "end": 2545
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3896484375
        },
        {
            "corpus_id": "270391675",
            "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
            "text": "This paper provides a comprehensive review of recent natural language generation evaluations based on LLMs, encompassing both prompt-based and tuning-based evaluation approaches.Despite significant efforts and notable achievements across various text generation benchmarks, several challenges in the field persist.\n\nBias of LLM-based Evaluators.The use of LLMs as evaluators inherently cast the text evaluation as a generation task.Consequently, when LLMs are employed in this evaluator role, they may carry over biases intrinsic to their function as generators.These biases may include social biases, such as stereotypes related to specific demographic identities (e.g., race, gender, religion, culture, and ideology) (Sheng et al., 2021).In addition to these general biases, LLMs-as-evaluators are subject to specific biases unique to their evaluative role.These include order bias, where preference 1 https://github.com/vllm-project/vllmis given to options based on their sequence (Zheng et al., 2023;Wang et al., 2023c;Koo et al., 2023); egocentric bias, where a tendency exists to favor texts generated by the same LLM (Liu et al., 2023d;Koo et al., 2023); and length bias, which leads to a preference for longer or shorter texts (Zheng et al., 2023;Koo et al., 2023).Therefore, in leveraging LLMs for evaluation purposes, it is crucial to calibrate both the inherent biases of LLMs as well as those biases specific to their function as evaluators.This dual consideration is essential for the effective and fair use of LLMs in NLG evaluation tasks.Which came first, the chicken or the egg?LLM-based evaluators frequently utilize GPT-4 (Liu et al., 2023c;Xu et al., 2023;Zheng et al., 2023), owing to its status as one of the most advanced LLM (OpenAI, 2023).",
            "score": 0.3698478211913516,
            "section_title": "Challenges and Open Problems",
            "char_start_offset": 40527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 178,
                    "end": 314
                },
                {
                    "start": 316,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 432
                },
                {
                    "start": 432,
                    "end": 562
                },
                {
                    "start": 562,
                    "end": 740
                },
                {
                    "start": 740,
                    "end": 859
                },
                {
                    "start": 859,
                    "end": 940
                },
                {
                    "start": 940,
                    "end": 1273
                },
                {
                    "start": 1273,
                    "end": 1453
                },
                {
                    "start": 1453,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1763
                }
            ],
            "ref_mentions": [
                {
                    "start": 719,
                    "end": 739,
                    "matchedPaperCorpusId": "234337004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35791015625
        },
        {
            "corpus_id": "272928217",
            "title": "LLM-based automatic short answer grading in undergraduate medical education",
            "text": "Overall, grades by GPT-4 were significantly lower than the grades from the human evaluator. This is different from the results in [5], where GPT would assign higher scores than human evaluators, and only become more severe in one-shot settings. Tobler also noted a high agreement with manual grading and sometimes a stricter evaluation by GPT-4 [23]. Gemini 1.0 Pro did not present a significant difference compared to human grades. \n\nBinary accuracy was higher than ternary, which was also observed in [10]. As opposed to [10], though, we found low rates of false positives with GPT-4, and GPT-4 achieved a high precision for fully correct answers. Nevertheless, there remained questions where LLMs were reluctant to give 0 respectively full points, so human oversight is still necessary [21]. But given that recognition of fully correct answers is reliable, more time will be available to focus on the middle cases. This aligns well with Kortemeyer [13], who observed that GPT \"performs best at the extreme ends of the grading spectrum\", and with Schneider et al., who stated that GPT has a tendency to the middle [21]. \n\nRegarding RQ1, we thus acknowledge a significant difference for GPT-4, but with a low rate of false positives, and a non-significant difference for Gemini. \n\nWhen asking GPT4b about a potential bias directly, it could not exclude it. This bias could also have led it to reach the highest accuracy in both binary and ternary settings. Of course, the usage of GPT4b is only realistic if the LLM can be provided a human grade and would issue a second opinion. Regarding RQ2, we cannot exclude a potential bias. Hence, prompt design should take this into account. \n\nHackl et al. stated that \"inconsistent ratings could lead to unfair outcomes\" [12]. In our case, variability was not significant, but the highest was observed with Gemini, Fig. 13 Prompt asking the LLM to grade the sample solution which sometimes fluctuated between 0 and full points. This behavior could be explained by certain characteristics of the key, if it was either too short or enumerated many possible answers in a question asking for a number of items.",
            "score": 0.36974568509413336,
            "section_title": "Discussion",
            "char_start_offset": 20897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1121
                },
                {
                    "start": 1124,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1683
                },
                {
                    "start": 1686,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2149
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 133,
                    "matchedPaperCorpusId": "268708056"
                },
                {
                    "start": 345,
                    "end": 349,
                    "matchedPaperCorpusId": "266460821"
                },
                {
                    "start": 951,
                    "end": 955,
                    "matchedPaperCorpusId": "258298426"
                },
                {
                    "start": 1764,
                    "end": 1768,
                    "matchedPaperCorpusId": "260682634"
                },
                {
                    "start": 1863,
                    "end": 1865,
                    "matchedPaperCorpusId": "258298426"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0938720703125
        },
        {
            "corpus_id": "270045144",
            "title": "Eliciting Informative Text Evaluations with Large Language Models",
            "text": "Furthermore, we employ the LLMs to create synthetic text reports based on a given item, simulating the scenario of the creation of fictitious academic peer reviews. We conduct the experiment on the ICLR2020 OpenReview dataset4 . Specifically, we provide the paper  as input to both GPT-3.5 and GPT-4 [Achiam et al., 2023, Brown et al., 2020], requesting them to generate comprehensive reviews following Liang et al. [2023]'s method. The prompt for this task is presented in Appendix D.4. \n\nWe compare three types of reviews: the human-written review, the GPT-4-generated review, and the GPT-3.5-generated review. Given that GPT-4 is commonly considered a stronger AI than GPT-3.5, and both models are considered worse than human expert reviewers, we hypothesize that we can utilize the LLM-generated reviews to simulate low-quality human reviews, thereby simulating three levels of quality. \n\nIt is worth noting that, unlike the degradations discussed in section 6.1, the LLM-generated review does not only depend on the agent's signal   since the reviewed paper is input into the LLMs to create reviews. It can not be regarded as untruthfully reporting the original signal. Thus, our focus remains on assessing the effectiveness of differentiating various quality levels of reports across different mechanisms.",
            "score": 0.36965041909499896,
            "section_title": "LLM-generated Reviews.",
            "char_start_offset": 48675,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1311
                }
            ],
            "ref_mentions": [
                {
                    "start": 320,
                    "end": 341,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.677734375
        },
        {
            "corpus_id": "267412090",
            "title": "Quality of Answers of Generative Large Language Models vs Peer Patients for Interpreting Lab Test Results for Lay Patients: Evaluation Study",
            "text": "A few limitations should be noted in this study. First, ChatGPT web version is nondeterministic in that the same prompt may generate different responses when used by different users. Second, the sample size for human evaluation is small. Nonetheless, this study produced evidence that LLMs such as GPT-4 can be a promising tool for filling the information gap for lab test understanding and various approaches can be employed to enhance the quality of the responses.",
            "score": 0.36958526487305804,
            "section_title": "Limitations",
            "char_start_offset": 23677,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 49,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 466
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07989501953125
        },
        {
            "corpus_id": "271050803",
            "title": "Is GPT-4 Alone Sufficient for Automated Essay Scoring?: A Comparative Judgment Approach Based on Rater Cognition",
            "text": "This study contributes to the growing body of research on the application of Large Language Models (LLMs) in Automated Essay Scoring (AES) by investigating the effectiveness of combining LLMs with Comparative Judgment (CJ).The findings demonstrate that the CJ-based scoring strategy, particularly when combined with elaborated rubrics and fine-grained scores using GPT-4, is more effective than the traditional rubric-based scoring in enabling LLMs to imitate human rater scores.This study shows that while GPT-4 is a powerful tool for AES, it is not sufficient on its own, as many factors influence both human raters and LLMs in essay scoring.\n\nThe results have significant implications not just for the advancement and utilization of LLMs in AES, but also for several research domains that entail generating multi-trait scoring data with a hierarchy.The insight gained from this study can guide the development of automated scoring systems in various fields, emphasizing the significance of taking into account elements such as scoring criteria, scoring methods, and the specific language model used.This work highlights the significance of interdisciplinary collaboration among specialists in the areas of natural language processing, educational assessment, and cognitive psychology to further enhance the progress and implementation of LLMs in intricate educational problems.",
            "score": 0.36952221322102197,
            "section_title": "Conclusion",
            "char_start_offset": 26909,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 223,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 644
                },
                {
                    "start": 646,
                    "end": 852
                },
                {
                    "start": 852,
                    "end": 1102
                },
                {
                    "start": 1102,
                    "end": 1380
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6640625
        },
        {
            "corpus_id": "270210580",
            "title": "NewsBench: A Systematic Evaluation Framework for Assessing Editorial Capabilities of Large Language Models in Chinese Journalism",
            "text": "The evaluation framework is designed to conduct a comprehensive and automated assessment of LLMs for editorial capabilities with a particular focus on journalistic writing proficiency (JWP) and safety adherence (SA) (Section 3.1).Evaluations are in types of short answer questions (SAQs) and multiple choice questions (MCQs) (Section 3.2).To aggregate and compare performances of LLMs, we develop four types of evaluations: JWP on multiple choice questions (JWP-MCQs), JWP on short answer questions (JWP-SAQs), SA on multiple choice questions (SA-MCQs), SA on short answer questions (SA-SAQs).Figure 1 illustrates the comprehensive workflow of our evaluation framework.Specifically, for each LLM to be evaluated, the framework first takes the four types of evaluations to elicit corresponding answers for MCQs and SAQs.To get the final performances of the LLM on the four types of evaluations, we adopt accuracy as the evaluation metric for MCQs while implementing GPT-4 based evaluation metrics for SAQs.",
            "score": 0.3691060800959918,
            "section_title": "The Evaluation Framework",
            "char_start_offset": 6802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 593
                },
                {
                    "start": 593,
                    "end": 669
                },
                {
                    "start": 669,
                    "end": 819
                },
                {
                    "start": 819,
                    "end": 1005
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0792236328125
        },
        {
            "corpus_id": "269982206",
            "title": "Large Language Models for Education: A Survey",
            "text": "Compared to previous models, the Transformer model has a richer structure, stronger adaptability to various scenarios, and better performance.The Transformer model improves the compatibility and practicality of LLMs, as well as its ability to cope with diverse and rich teaching contents and educational scenarios.\n\nLLM diagnostics and application evaluation.Existing interdisciplinary evaluation systems assess LLMs from two perspectives: diagnostics during LLM training and the effectiveness of LLM applications.\"ChatbotArena\" 20 is a benchmark platform for LLMs that conduct anonymous and random adversarial evaluations, where the system randomly selects two different LLMs to chat with users, who then rate the interactions.\"SuperCLUE\" 21 is a benchmark for evaluating general-purpose LMs in Chinese, examining multidimensional capabilities in terms of basic abilities, professional abilities, and Chinese-specific abilities [124].\"The C-Eval project\" [51], jointly carried out by Shanghai Jiao Tong University, Tsinghua University, and the University of Edinburgh, constructs a multidisciplinary benchmark list to assist Chinese LLM research.\"FlagEval\" [63], built by multiple universities, adopts a three-dimensional approach to evaluating LLMs, including factuality, safety, and inclusivity.These evaluation frameworks are designed to comprehensively assess LLMEdu's performance, ethical impact, and potential bias, as well as promote the improvement of LLMEdu's capabilities and technology optimization.\n\nPrompt engineering [83].It refers to the ability to interact with LLMs.Machines match corresponding results through prompts, thereby increasing productivity.Good prompts can enhance the intelligence of LLMs and increase the value of feedback results [109,130], increasing the use value of LLMEdu.Moreover, poor prompts may lead to erroneous conclusions.In the field of education, especially rigorous science, the correctness of answers is always given priority, so optimizing prompt words is also important to deal with LLM's nonsense when answering academic questions.Different LMs, such as ChatGPT, ERNIE Bot, and MathGPT, have independent underlying training mechanisms, and their prompts are different.This can be likened to communication with individuals with different personalities.\n\nLearning cognitive mechanisms.",
            "score": 0.36907894692684745,
            "section_title": "Human feedback reinforcement learning (HFRL).",
            "char_start_offset": 42784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 142,
                    "end": 314
                },
                {
                    "start": 316,
                    "end": 359
                },
                {
                    "start": 359,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 728
                },
                {
                    "start": 728,
                    "end": 935
                },
                {
                    "start": 935,
                    "end": 1147
                },
                {
                    "start": 1147,
                    "end": 1298
                },
                {
                    "start": 1298,
                    "end": 1511
                },
                {
                    "start": 1513,
                    "end": 1537
                },
                {
                    "start": 1537,
                    "end": 1584
                },
                {
                    "start": 1584,
                    "end": 1670
                },
                {
                    "start": 1670,
                    "end": 1809
                },
                {
                    "start": 1809,
                    "end": 1866
                },
                {
                    "start": 1866,
                    "end": 2082
                },
                {
                    "start": 2082,
                    "end": 2219
                },
                {
                    "start": 2219,
                    "end": 2302
                },
                {
                    "start": 2304,
                    "end": 2334
                }
            ],
            "ref_mentions": [
                {
                    "start": 1763,
                    "end": 1768,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 1768,
                    "end": 1772,
                    "matchedPaperCorpusId": "258217984"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039947509765625
        },
        {
            "corpus_id": "270258334",
            "title": "Large Language Models as Evaluators for Recommendation Explanations",
            "text": "In average performance of different aspects, GPT-4 surpasses both third-party annotators and reference-based metrics.GPT3.5 and Qwen1.5 demonstrate comparable accuracy to the third-party annotator, also out-performing reference-based metric.\n\nEvaluation accuracies of evaluators are aspect-dependent.When analyzing the performance across different aspects, we can see that the third-party annotators show better evaluation accuracy in Persuasiveness and Transparency than Accuracy and Satisfaction.This implies that Accuracy and Satisfaction are more subject, displaying greater individual variability.Thus, third-party annotations may not be satisfactory solutions in these aspects.Experiments indicate that LLMs, particularly GPT-4, perform better in these areas.However, regarding Transparency, LLMs are inferior to human labeling.In subsequent sections, we discuss strategies for enhancing the zero-shot LLM evaluator.\n\nResults from multiple-aspect prompt v.s.single-aspect prompt.The multiple-aspect involves LLM scoring a text across 4 aspects simultaneously, while in the latter, it assesses each aspect individually.We find that several LLM evaluators (LLama2-13B, Qwen1.5-7B,GPT3.5, GPT4) perform notably better on Satisfaction when using the multiple-aspect prompt.This may be due to user satisfaction being a composite consideration of various dimensions.Thus, scoring on other aspects acts as an implicit Chain-Of-Thought [34] process that enhances the zero-shot reasoning ability of LLMs [14].Conversely, the single-aspect prompt yields better results on the Transparency aspect with several LLMs.This may be because the evaluation of Transparency is relatively independent, and separate scoring allows the model to better understand the task without interference from other aspects.Overall, we would recommend the multiple-aspect version, as it has no significant gap with single-aspect but requires fewer interactions.Our experiments on RQ2 and RQ3 are also conducted on multiple-aspect prompts.\n\nTable 1: The 3-level Pearson correlation between the results given by the evaluator and the ground-truth label given by users.Bold fonts denote the best results among all tested evaluators and the underlines show the second-best results.",
            "score": 0.3690502492224704,
            "section_title": "ZERO-SHOT LLM CAN BE A COMPETITIVE EVALUATOR",
            "char_start_offset": 20532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 241
                },
                {
                    "start": 243,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 498
                },
                {
                    "start": 498,
                    "end": 602
                },
                {
                    "start": 602,
                    "end": 683
                },
                {
                    "start": 683,
                    "end": 765
                },
                {
                    "start": 765,
                    "end": 834
                },
                {
                    "start": 834,
                    "end": 922
                },
                {
                    "start": 924,
                    "end": 964
                },
                {
                    "start": 964,
                    "end": 985
                },
                {
                    "start": 985,
                    "end": 1124
                },
                {
                    "start": 1124,
                    "end": 1184
                },
                {
                    "start": 1184,
                    "end": 1275
                },
                {
                    "start": 1275,
                    "end": 1366
                },
                {
                    "start": 1366,
                    "end": 1506
                },
                {
                    "start": 1506,
                    "end": 1610
                },
                {
                    "start": 1610,
                    "end": 1796
                },
                {
                    "start": 1796,
                    "end": 1933
                },
                {
                    "start": 1933,
                    "end": 2010
                },
                {
                    "start": 2012,
                    "end": 2138
                },
                {
                    "start": 2138,
                    "end": 2249
                }
            ],
            "ref_mentions": [
                {
                    "start": 1434,
                    "end": 1438,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "249017743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21142578125
        },
        {
            "corpus_id": "273098256",
            "title": "ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR",
            "text": "Optimizing LLM Selection for Task-Specific Performance Achieving the design goals of ELLMA-T may require the use of different LLMs tailored to specific tasks. For conversational tasks, GPT-4o [53] is recognized as the best-performing model, capable of understanding user sentiment and maintaining engaging dialogues [53]. Other models may be more suitable for summarization and feedback due to their reliability [9]. Preliminary studies suggest that GPT-4o is the best-performing model we can use for both tasks in this study. In future work, we may use or combine different models to leverage the strengths of various models and optimize the system's performance for future improvements. \n\nValidating LLM Assessment of Language Proficiency: We recognize that the use of CEFR criteria [18] to prompt the LLM in our system is not entirely precise and serves primarily as a rough guide for the LLM to adjust language difficulty for following dialogues. Whether the dialogue content generated by the LLM accurately reflects the intended difficulty level remains to be rigorously validated in future studies. \n\nImplementing Content Filters for LLM Responses Additionally, we did not implement a filtering layer to screen the LLM's responses before delivery, which could lead to hallucinations or potentially harmful content. Although our initial tests showed that GPT-4 generally provided correct responses in grammar correction tasks, existing studies indicate that it may still produce inaccurate results or introduce stereotypes [65]. Future work will include the evaluation of LLM responses by using the LLM Judge framework [84] or CoMPosT framework [15] to detect stereotypes. \n\nEvaluating Long-Term Educational Impact Lastly, this preliminary study involved only 15-minute interaction sessions, and we did not measure the system's effectiveness in terms of actual language learning outcomes. To fully assess the educational impact of ELLMA-T, we plan to conduct longitudinal studies with longer learning sessions. In addition, these studies will provide a more comprehensive evaluation of user engagement over time and help identify key factors that contribute to sustained use and long-term language learning outcomes.",
            "score": 0.36885881417438,
            "section_title": "LIMITATIONS & FUTURE WORK",
            "char_start_offset": 69384,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2221
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.059417724609375
        },
        {
            "corpus_id": "267587759",
            "title": "Generative Pre-Trained Transformer (GPT) in Research: A Systematic Review on Data Augmentation",
            "text": "Example from Document [77] Explores GPT's capabilities in abstract reasoning and problem-solving with a focus on the Abstraction and Reasoning Corpus (ARC). \n\nInvestigates GPT's performance and challenges in solving simple abstract reasoning problems. \n\n[28] \n\nUses LLMs to augment research on the P versus NP problem, proposing a Socratic reasoning framework for complex problem-solving with GPT-4. \n\nPilot study on P vs. NP problem showing GPT-4's capability in developing reasoning pathways. \n\n[75] \n\nHighlights the use of LLMs in complex textual analysis and problem-solving in various domains, including applications in finance and accounting. \n\nAnalyzes CEO humor in conference calls and clusters topics in employee reviews using LLMs. \n\n[78] \n\nDetails the challenges and strategies of using GPT for solving ARC tasks, emphasizing the role of structured Demonstrates the enhancement of GPT's problem-solving abilities with object-based The study in [75] examines the application of generative large language models (GLLMs) like ChatGPT and GPT-4 in accounting research. It emphasizes GLLMs' ability to perform complex textual analysis tasks, including those previously achievable only through human coding. The paper discusses the practicalities of using GLLMs in research, focusing on model selection, prompt engineering, construct validity, and addressing bias, replicability, and data privacy concerns. A case study is included to demonstrate GLLMs' capabilities, specifically in detecting non-answers in earnings conference calls, a task challenging for traditional automation. \n\nThe study in [76] focuses on the application of GPT models in data analysis, specifically in the context of discrete mathematics education. The study compares the performance of GPT-3.5 and GPT-4 in solving Proof Blocks problems, highlighting the significant improvement of GPT-4 over its predecessor. This comparison demonstrates the evolving ability of LLMs to handle complex academic and educational tasks, marking a notable advancement in the application of these models for intricate problem-solving and analysis in mathematics.",
            "score": 0.3686863871646172,
            "section_title": "eference Applicability of GPT/LLM Assists in Research",
            "char_start_offset": 38922,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 159,
                    "end": 251
                },
                {
                    "start": 254,
                    "end": 258
                },
                {
                    "start": 261,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 748
                },
                {
                    "start": 751,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1587
                },
                {
                    "start": 1590,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2123
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25244140625
        },
        {
            "corpus_id": "267320642",
            "title": "Weaver: Foundation Models for Creative Writing",
            "text": "We first perform an LLM-based evaluation to do a coarse-grained evaluation of the compared models. \n\nWe use GPT-4 as the judge to score each instruction-response pair following the practice and prompt templates in MT-Bench. The results are shown in Table 4. We find that in terms of writing style and creativity, Weaver Ultra significantly outperforms all proprietary models including strong competitors such as GPT-4 and GLM-4. GPT-4 and GLM-4 are better at the relevance metric because they are at least few times larger than Weaver Ultra and thus have better instruction-following ability. As for Weaver of other sizes, we can see that with only 14B parameters, Weaver Pro outperforms all open-source models including those with 70B and 34B parameters, as well as most proprietary models. Similarly, Weaver Base and Weaver Mini are also comparable with generalist LLMs with more than two times their sizes. Overall, the results confirm the effectiveness of our data synthesis and training framework for LLMs specialized in creative writing.",
            "score": 0.36866221122086396,
            "section_title": "LLM-based Evaluation",
            "char_start_offset": 34782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 101,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1043
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09222412109375
        },
        {
            "corpus_id": "277244603",
            "title": "FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific Article",
            "text": "Moreover, LLMs are capable of generating novel research ideas (Lu et al., 2024). Statistical tests show that LLM-generated ideas exhibit greater novelty compared to human-generated ones after extracting research topics from recent conferences and prompting both LLMs and humans to generate ideas (Si et al., 2024)  have explored LLM-driven hypothesis generation in other fields, such as biomedical, automated suggestive scientific exploration and idea generation in the NLP domain, remain undiscovered. \n\nA critical aspect of improving LLM-generated content is the use of human feedback mechanisms (Ouyang et al., 2022). In its absence, selfrefinement techniques have been proposed, such as a self-debugging framework where LLMs identify and correct their mistakes without human intervention (Chen et al., 2023). Moreover, a model that generates instructions, inputs, outputs, and filters invalid results with fine-tuning itself iteratively (Wang et al., 2022). AI feedback has been used for in-context learning with prompt criticism (Fu et al., 2023), in a GPT-4-based three-feedbackmechanism system to generate novel hypotheses (Yang et al., 2023), and in LLM-generated responses with utility-based feedback for prompt revision (Peng et al., 2023). \n\nOur work builds on these advancements by using LLMs not only to generate future work sections using LLM but also to act as a judge for scoring and justification of the output. Inspired by selfrefinement techniques (Madaan et al., 2023), we iteratively incorporate LLM feedback to refine the input, eliminating the need for additional training or reinforcement learning and thereby improving the quality of the generated content. \n\nEvaluating LLM-generated text is vital for scientific writing. Studies comparing human and LLM performance (Chiang and Lee, 2023;Nguyen et al., 2024) show that LLM-based evaluations (covering grammaticality, cohesiveness, accuracy, coverage, etc.) align well with human judgments. Building on these insights, we use LLM-based evaluation to assess the quality of generated future work.",
            "score": 0.3686027737578156,
            "section_title": "Related Work",
            "char_start_offset": 7191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2068
                }
            ],
            "ref_mentions": [
                {
                    "start": 598,
                    "end": 619,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1467,
                    "end": 1488,
                    "matchedPaperCorpusId": "257900871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37109375
        },
        {
            "corpus_id": "273346025",
            "title": "Toward General Instruction-Following Alignment for Retrieval-Augmented Generation",
            "text": "To evaluate the effectiveness of GPT-4 scoring in assessing LLM responses, we conducted a consistency experiment between GPT-4 prediction scores and human scores. For fillin-the-blank and open-ended questions, we randomly sampled 30 instances each from the base model, the SFT version model, and the VIF-RAG model test cases, totaling 90 instances, and had a human annotator score these predictions. In Table 5, we report the consistency between the average human scores and GPT-4 scores, measured by Pearson correlation. The strong alignment between human and GPT-4 scores validates the effectiveness of GPT-4 scoring.",
            "score": 0.3685763966626367,
            "section_title": "Consistency with Human Evaluation",
            "char_start_offset": 41255,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 619
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1275634765625
        },
        {
            "corpus_id": "273901331",
            "title": "Exploring Scientific Hypothesis Generation with Mamba",
            "text": "To address the limitations of automatic evaluation metrics, we incorporate an LLM evaluation to assess the quality of the generated scientific hypotheses. Specifically, we employ Claude-3.5 instead of the more mainstream GPT-4 to mitigate potential self-enhancement bias, which occurs when a model is evaluated using its own framework (Xu et al., 2024b). We utilize a reference-guided style prompt, as introduced by previous research (Zheng et al., 2023), which has been shown to increase the agreement between human and machine evaluations by providing a reference solution. \n\nWe design a structured prompt that directs Claude-3.5 to judge the effectiveness of a hypothesis. Specifically, Claude-3.5 is asked to assess the relevance of the hypothesis to the background context and seed term, ensuring sufficient novelty without overly mirroring the provided information, verifying the scientific soundness, and evaluating the clarity and coherence of the language. See the detailed prompt(C). Based on these criteria, Claude-3.5 assigns a rating of either \"ineffective\" or \"effective\" to each generated hypothesis, along with a brief justification for its decision, formatted in JSON for consistent analysis. To assist the LLM in making more accurate assessments, we provide it with five examples of both \"ineffective\" and \"effective\" hypotheses.",
            "score": 0.3684660510461264,
            "section_title": "LLM As a Judge",
            "char_start_offset": 12395,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1347
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6591796875
        },
        {
            "corpus_id": "267750439",
            "title": "How Susceptible are Large Language Models to Ideological Manipulation?",
            "text": "No, I believe that arming teachers is not the solution to school safety. Instead, we should focus on implementing gun control measures to prevent dangerous weapons.... We choose GPT-4 as the ideology evaluator for several reasons. First, the majority of responses in IDEOINST are generated by GPT-4 itself. Second, identifying political ideology within textual responses is a complex task that often demands domain-specific knowledge, making it impractical for general crowdworkers for accurate ideological assessment, nor economically feasible to recruit subject matter experts. Therefore, using GPT-4 for the task streamlines and expedites the evaluation timeframe significantly. Nevertheless, as a further quality check, we recruit three human annotators and use two LLMs (Llama-2-70B and Claude-3sonnet) to cross validate GPT-4 as a feasible ideology evaluator. We compare the predictions of GPT-4 to (1) human annotations, and (2) the predictions of two other LLMs, which can be found in Appendix B.2. The high agreement of GPT-4 to both humans and other LLMs indicates the reliability of using GPT-4 for ideology classification. \n\nBuilding upon the findings of Santurkar et al. (2023), which highlight the left-leaning bias of LMs in response to multi-choice survey questions, our study extends the examination of ideological biases to the open-ended responses of LLMs. We focus our analysis on four prominent LLMs: Llama-2-7B (Touvron et al., 2023), GPT-3.5-turbo (Ouyang et al., 2022), Alpaca-7B (Taori et al., 2023), and Mistral-7B (Jiang et al., 2023), utilizing IDEOINST to assess their outputs without ideological manipulation. The results serve as a baseline for the subsequent ideological manipulation in \u00a75.   5 Manipulating LLMs' Ideologies",
            "score": 0.3683587711814933,
            "section_title": "LLM",
            "char_start_offset": 9424,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1756
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.083740234375
        },
        {
            "corpus_id": "263608642",
            "title": "Can GPT-4 Replicate Empirical Software Engineering Research?",
            "text": "External validity. Empirical studies have difficulty generalizing [20]. In our study, the assumptions, analysis plans, and code are generated by  during August 2023, rather than other LLMs such as PaLM [13], PaLM 2 [3], LLaMA [60], and LLaMA 2 [61]. GPT-4 has been shown to outperform these other models on diverse tasks [61] which suggests that these models (at least \"off-the-shelf\" with no fine-tuning) would perform no better. Nonetheless, it is unclear the extent to which our results may generalize to other LLMs or future versions of GPT models that may behave differently. \n\nThe paper selection strategy could introduce biases and result in a small sample that is incomplete and not representative of all research papers. Certain methodologies, research topics, or writing styles may not be included in our sample due to the limited selection criteria and the sets of papers examined. Thus, the results may not generalize to all studies in empirical software engineering. \n\nFor the user study, using snowball sampling could introduce sampling bias. Our smaller sample size may also not be representative of all empirical software engineering researchers or data scientists on software engineering teams, limiting the generalizability of the results. We reduced this threat by ensuring saturation of the qualitative codes by the time data collection ended. \n\nInternal validity. Only having a single author perform qualitative coding could introduce biases. We minimized this threat by having a second author independently validate the codes and achieved an inter-rater reliability of 80.6%. How a prompt is crafted can affect the model's performance on a task [5,10,53,64]. While we eliminated the non-determinism of model outputs by setting the temperature to 0, the results from this study could be influenced by how prompts were constructed, as a more effective prompt could elicit different outputs and thus alter the ratings and reactions from study participants. Some user study participants could skip evaluating GPT-4-generated data due to time constraints, potentially causing some collected data to be incomplete. We reduced its effect by randomizing the order of the assumptions for each participant. Additionally, user study participants could misunderstand the wording of the survey and interview questions.",
            "score": 0.3682411892724162,
            "section_title": "THREATS TO VALIDITY",
            "char_start_offset": 54335,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2218
                },
                {
                    "start": 2219,
                    "end": 2327
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 70,
                    "matchedPaperCorpusId": "62807147"
                },
                {
                    "start": 1667,
                    "end": 1670,
                    "matchedPaperCorpusId": "246485605"
                },
                {
                    "start": 1670,
                    "end": 1673,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1676,
                    "end": 1679,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17724609375
        },
        {
            "paperId": "10fb95d59987ff603ee942d131eea508d6d26aac",
            "corpusId": 261194086,
            "title": "The Role of Artificial Intelligence in Academic Paper Writing and Its Prospective Application as a Co-Author: A Letter to the Editor",
            "venue": "European Journal of Therapeutics",
            "year": 2023,
            "referenceCount": 0,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.58600/eurjther1808",
                "status": "GOLD",
                "license": "CCBYNC",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.58600/eurjther1808?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.58600/eurjther1808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51450611",
                    "name": "Gulnihal Deniz"
                }
            ],
            "abstract": "Dear Editors,\nI have read your editorials with great interest [1,2]. I am interested in sharing my insights concerning the role of artificial intelligence in composing scholarly articles, along with its potential as a collaborative co-author. I extend my heartfelt gratitude for establishing this profoundly valuable platform for discussion.\nI am aware of the imperative to renew myself academically daily. Perhaps the most exquisite yet arduous facet of academic life resides herein. Sustaining the currency of my domain knowledge, tracking technological advancements, and aligning with the latest research trends often pose formidable challenges. However, these challenges also furnish avenues for continuous self-improvement and exploring topics demanding more profound comprehension. In addition to the facilitation of information access afforded by computers and the internet, artificial intelligence has been incorporated in recent years\u2014my inaugural encounter with artificial intelligence manifested through applications utilized on telecommunication devices. Artificial intelligence finds application across various domains and displays a swiftly burgeoning spectrum of applications. In recent years, significant advancements have transpired in artificial intelligence, culminating in the emergence of numerous Large Language Models (LLMs). Introducing sophisticated and diverse language models has precipitated a remarkable leap in this domain. One such model is the artificial intelligence conversational robot named ChatGPT, equipped with the GPT-3.5.5 language model, which OpenAI unveiled on November 30, 2022. Impressively, this model garnered one million users within five days. Within the academic literature, ChatGPT, a Chat Generative-Being Transformer, is widely acknowledged as a substantial and versatile information resource [3]. So, can ChatGPT be used safely for manuscript writing? As academics, we know that writing an article and adding new knowledge to the literature requires serious dedication. In this context, using ChatGPT for article writing involves significant risks [4]. The biggest problem is accuracy [5]. Artificial intelligence draws its data from the internet environment, where the veracity and reliability of information are persistently subject to debate. The accuracy and reliability of data on the Internet is always controversial. ChatGPT can produce factually inaccurate and inaccurate texts, create biased texts, and in particular, this can undermine the credibility and authority of researchers. Another most critical problem is that it includes ethical concerns.\nHowever, we cannot overlook the fact that with the advancement of technology, artificial intelligence has been progressing toward the core of our lives. As a solution, I think that artificial intelligence should be employed with caution, considering its ethical problems, the potential for misapplications, and plagiarism-related concerns. Notably, it can contribute to refining written text rather than printing the entire article. In addition, as you stated, the role, contributions, and process of ChatGPT in the article should be clearly stated. In the literature, it has been said that ChatGPT contributed to various stages, such as data analysis, model development, and interpretation of results [6]. Susnjak [7] has argued that ChatGPT exhibits critical thinking skills and can generate highly realistic texts with minimal input, positing that this poses a threat in online examinations, particularly within higher education settings. Zhai [8] in the context of crafting articles encompassing education and artificial intelligence themes, has emphasized the assertion that ChatGPT could assist researchers in generating coherent, partially accurate, informative, and systematic articles. Alshater [9] has noted that ChatGPT has the potential to improve academic performance, underlined various limitations, such as ethical considerations, and emphasized the importance of combining human analysis and interpretation.\nSo, is it appropriate for ChatGPT to be credited as a co-author? This topic will always be controversial This matter will inevitably remain subject to ongoing debate. The scope of ChatGPT's contribution and the ethical considerations surrounding this practice, coupled with the continued discussions within the academic community, suggest that employing ChatGPT as a co-author carries substantial risks [10]. In a collaborative study where Perlman and ChatGPT served as co-authors [11], Perlman evaluated the text generated by ChatGPT and underscored the possibility of envisioning a new future by considering the ethical concerns, faulty applications, and plagiarism issues associated with artificial intelligence. Similarly, in a comparable endeavor, Srivastava [12] was tasked with using ChatGPT to compose a conference paper and consequently emphasized that, under researcher supervision, ChatGPT could be an efficient application.\nIn conclusion, the assertion that artificial intelligence plays an increasingly significant role in research and scientific discovery is continuously gaining support. However, considering the meticulousness and accuracy required for establishing academic literature across all fields, ChatGPT's practice of generating academic articles from scratch and serving as an assistant author is not aligned with academic norms. There is a need for the development of more nuanced programs in this regard. Especially in the coming days, ChatGPT should prove the information it gives and present the right references for every sentence. Simultaneously, ChatGPT should be revamped in a format that can address ethical concerns.\nYours sincerely,",
            "corpus_id": "261194086",
            "text": "Dear Editors,\nI have read your editorials with great interest [1,2]. I am interested in sharing my insights concerning the role of artificial intelligence in composing scholarly articles, along with its potential as a collaborative co-author. I extend my heartfelt gratitude for establishing this profoundly valuable platform for discussion.\nI am aware of the imperative to renew myself academically daily. Perhaps the most exquisite yet arduous facet of academic life resides herein. Sustaining the currency of my domain knowledge, tracking technological advancements, and aligning with the latest research trends often pose formidable challenges. However, these challenges also furnish avenues for continuous self-improvement and exploring topics demanding more profound comprehension. In addition to the facilitation of information access afforded by computers and the internet, artificial intelligence has been incorporated in recent years\u2014my inaugural encounter with artificial intelligence manifested through applications utilized on telecommunication devices. Artificial intelligence finds application across various domains and displays a swiftly burgeoning spectrum of applications. In recent years, significant advancements have transpired in artificial intelligence, culminating in the emergence of numerous Large Language Models (LLMs). Introducing sophisticated and diverse language models has precipitated a remarkable leap in this domain. One such model is the artificial intelligence conversational robot named ChatGPT, equipped with the GPT-3.5.5 language model, which OpenAI unveiled on November 30, 2022. Impressively, this model garnered one million users within five days. Within the academic literature, ChatGPT, a Chat Generative-Being Transformer, is widely acknowledged as a substantial and versatile information resource [3]. So, can ChatGPT be used safely for manuscript writing? As academics, we know that writing an article and adding new knowledge to the literature requires serious dedication. In this context, using ChatGPT for article writing involves significant risks [4]. The biggest problem is accuracy [5]. Artificial intelligence draws its data from the internet environment, where the veracity and reliability of information are persistently subject to debate. The accuracy and reliability of data on the Internet is always controversial. ChatGPT can produce factually inaccurate and inaccurate texts, create biased texts, and in particular, this can undermine the credibility and authority of researchers. Another most critical problem is that it includes ethical concerns.\nHowever, we cannot overlook the fact that with the advancement of technology, artificial intelligence has been progressing toward the core of our lives. As a solution, I think that artificial intelligence should be employed with caution, considering its ethical problems, the potential for misapplications, and plagiarism-related concerns. Notably, it can contribute to refining written text rather than printing the entire article. In addition, as you stated, the role, contributions, and process of ChatGPT in the article should be clearly stated. In the literature, it has been said that ChatGPT contributed to various stages, such as data analysis, model development, and interpretation of results [6]. Susnjak [7] has argued that ChatGPT exhibits critical thinking skills and can generate highly realistic texts with minimal input, positing that this poses a threat in online examinations, particularly within higher education settings. Zhai [8] in the context of crafting articles encompassing education and artificial intelligence themes, has emphasized the assertion that ChatGPT could assist researchers in generating coherent, partially accurate, informative, and systematic articles. Alshater [9] has noted that ChatGPT has the potential to improve academic performance, underlined various limitations, such as ethical considerations, and emphasized the importance of combining human analysis and interpretation.\nSo, is it appropriate for ChatGPT to be credited as a co-author? This topic will always be controversial This matter will inevitably remain subject to ongoing debate. The scope of ChatGPT's contribution and the ethical considerations surrounding this practice, coupled with the continued discussions within the academic community, suggest that employing ChatGPT as a co-author carries substantial risks [10]. In a collaborative study where Perlman and ChatGPT served as co-authors [11], Perlman evaluated the text generated by ChatGPT and underscored the possibility of envisioning a new future by considering the ethical concerns, faulty applications, and plagiarism issues associated with artificial intelligence. Similarly, in a comparable endeavor, Srivastava [12] was tasked with using ChatGPT to compose a conference paper and consequently emphasized that, under researcher supervision, ChatGPT could be an efficient application.\nIn conclusion, the assertion that artificial intelligence plays an increasingly significant role in research and scientific discovery is continuously gaining support. However, considering the meticulousness and accuracy required for establishing academic literature across all fields, ChatGPT's practice of generating academic articles from scratch and serving as an assistant author is not aligned with academic norms. There is a need for the development of more nuanced programs in this regard. Especially in the coming days, ChatGPT should prove the information it gives and present the right references for every sentence. Simultaneously, ChatGPT should be revamped in a format that can address ethical concerns.\nYours sincerely,",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.275634765625
        },
        {
            "paperId": "3526c824391bf919f75f5f586ce08da5302aad28",
            "corpusId": 277968005,
            "title": "The Impact of Innovative Strategies on Improving Writing Skills of English Learners at University Level",
            "venue": "Inverge Journal of Social Sciences",
            "year": 2025,
            "referenceCount": 36,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.63544/ijss.v4i1.120?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.63544/ijss.v4i1.120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2351765628",
                    "name": "Qamar Abbas"
                },
                {
                    "authorId": "10421603",
                    "name": "Rabia Nawaz"
                },
                {
                    "authorId": "2356664823",
                    "name": "Mawra Tariq Malik"
                },
                {
                    "authorId": "2356700382",
                    "name": "Ahmad Yar"
                },
                {
                    "authorId": "2356665824",
                    "name": "Haji Muhammad Arif"
                }
            ],
            "abstract": "This study examined how creative strategies such as artificial intelligence (AI) tools, collaborative writing, and digital storytelling activities impacted the enhancement of academic writing skills of English learners at the university level. The research sought to understand the writing problems that were most common, evaluate the implementation and effectiveness of the measures, and monitor the students' reception to the measures being implemented. A quantitative approach using surveys was used with a sample of 346 students from different fields of study. Structured questionnaires which had been tested in a pilot study (Cronbach\u2019s Alpha = 0.792) were used to gather data and were then processed in SPSS (Version 28). Descriptive statistics and cross tabulation were used to analyse the data and find the patterns pertaining to difficulties in writing, use of tools, and outcomes. The most critical findings were the remaining issues: grammar was a problem for 62.5%, coherence for 55.2%, and vocabulary for 49.4% of the students. In spite of this, self-reported use of innovative strategies was still high such as: AI tools usage (69.7%) and peer collaboration (65.4%) as well as engaging interactive methods (82.6%). Additionally, there was strong support for institutional adoption, with 87% of participants in favour; however, there was less support for advanced digital storytelling techniques (39.3% engagement). \nThe ethical considerations such as anonymity and voluntary participation were followed. Testing in the pilot phase reduced bias and no personal information was stored. This study addresses the gap in the integration of technology and collaborative teaching processes in the teaching of academic writing. It provides empirical data on the effectiveness of modern approaches while also revealing gaps in adoption across levels of proficiency and disciplines. The results make it possible for curriculum developers and decision makers to take tangible steps toward solving the discrepancies between offer and demand in the context of use. \nReferences \nAbahussain, M. O.\u00a0(2020). Investigating EFL learners\u2019 perceptions of collaborative writing.\u00a0International Journal of English Linguistics, 10(3), 32\u201347.\u00a0 \nAlawaji, N. N. M.\u00a0(2020). Students' perceptions of collaborative summary writing.\u00a0Theory and Practice in Language Studies, 10(6), 700\u2013707.\u00a0 \nBelyaeva, E. G.\u00a0(2022). Methodological model of teaching academic writing to undergraduate students.\u00a0Focus on Language Education and Research, 3(1), 36\u201351. \nButt, S.\u00a0(2023). Employees\u2019 perception regarding in-house training programs in Pakistani organizations.\u00a0Journal of Workplace Behavior, 4(1), 35\u201350. \nButt, S., & Yazdani, N.\u00a0(2023). Implementation of quality management practices and firm\u2019s innovation performance: Mediation of knowledge creation processes and moderating role of digital transformation.\u00a0Pakistan Journal of Humanities and Social Sciences, 11(4), 3881\u20133902. \nButt, S., Umair, T., & Tajammal, R.\u00a0(2024). Nexus between key determinants of service quality and students\u2019 satisfaction in higher education institutions (HEIs).\u00a0Annals of Human and Social Sciences, 5(2), 659\u2013671. \nChiew, M. T. L., & Ismail, H. H.\u00a0(2021). Exploring vocabulary learning strategies in a second language setting: A review.\u00a0International Journal of Academic Research in Business and Social Sciences, 11(12), 1298\u20131309. \nChubko, N., Morris, J. E., McKinnon, D. H., Slater, E. V., & Lummis, G. W.\u00a0(2020). Digital storytelling as a disciplinary literacy enhancement tool for EFL students.\u00a0Educational Technology Research and Development, 68, 3587\u20133604.\u00a0https://doi.org/10.1007/s11423-020-09833-x \nDavoodifard, M.\u00a0(2022). An overview of writing process research: Using innovative tasks and techniques for a better understanding of L2 writing processes in assessment contexts.\u00a0Studies in Applied Linguistics and TESOL, 21(2). \nDodigovic, M., & Jeaco, S.\u00a0(2021). Technology in applied linguistics.\u00a0International Journal of TESOL Studies, 3(2), 1\u20135. \nFerris, D. R.\u00a0(2023).\u00a0What error correction can (not) accomplish for second language writers: Dispelling myths, discussing options.\u00a0University of Michigan Press. \nFerris, D. R., & Hedgcock, J. S.\u00a0(2023).\u00a0Teaching L2 composition: Purpose, process, and practice\u00a0(4th ed.). Routledge. \nFlowerdew, L.\u00a0(2021). Learner corpora for disciplinary writing. In\u00a0Research questions in language education and applied linguistics: A reference guide\u00a0(pp. 475\u2013479). Springer. \nFlowerdew, L., & Petri\u0107, B.\u00a0(2024). A critical review of corpus-based pedagogic perspectives on thesis writing: Specificity revisited.\u00a0English for Specific Purposes, 76, 1\u201313.\u00a0https://doi.org/10.1016/j.esp.2024.01.001 \nHinkel, E.\u00a0(2022).\u00a0Teaching academic L2 writing: Practical techniques in vocabulary and grammar\u00a0(2nd ed.). Routledge. \nHyland, K.\u00a0(2024). Ken Hyland's essential bookshelf: Academic writing.\u00a0Language Teaching, 57(3), 399\u2013407.\u00a0https://doi.org/10.1017/S0261444824000109 \nHyland, K., & Hyland, F.\u00a0(2019).\u00a0Feedback in second language writing: Contexts and issues\u00a0(2nd ed.). Cambridge University Press. \nKang, E. Y., & Han, Z.\u00a0(2021). Written corrective feedback. In\u00a0The Routledge handbook of second language acquisition and writing\u00a0(pp. [page range]). Routledge. \nKarim, K., & Nassaji, H.\u00a0(2020). The effects of written corrective feedback.\u00a0Instructed Second Language Acquisition, 3(1), 28\u201352. \nKessler, G.\u00a0(2020). Professionalizing your use of technology in English language teaching. In\u00a0Professionalizing your English language teaching\u00a0(pp. 163\u2013173). Springer. \nKessler, M.\u00a0(2023). Written corrective feedback in an online community: A typology of English language learners\u2019 requests and interlocutors\u2019 responses.\u00a0Computers and Composition, 67, 102752.\u00a0https://doi.org/10.1016/j.compcom.2023.102752 \nKim, N. J., & Kim, M. K.\u00a0(2022). Teacher\u2019s perceptions of using an artificial intelligence-based educational tool for scientific writing.\u00a0Frontiers in Education, 7, 755914.\u00a0https://doi.org/10.3389/feduc.2022.755914 \nLi, J.\u00a0(2017). Automated writing evaluation: A pedagogical tool.\u00a0TESOL Quarterly, 51(2), 427\u2013432.\u00a0 \nLi, M.\u00a0(2021).\u00a0Researching and teaching second language writing in the digital age.\u00a0Palgrave Macmillan. \nLi, M., & Zhang, M.\u00a0(2023). Collaborative writing in L2 classrooms: A research agenda.\u00a0Language Teaching, 56(1), 94\u2013112.\u00a0https://doi.org/10.1017/S0261444821000318 \nMihaylova, M., Gorin, S., Reber, T. P., & Rothen, N.\u00a0(2022). A meta-analysis on mobile-assisted language learning applications: Benefits and risks.\u00a0Psychologica Belgica, 62(1), 252\u2013267.\u00a0 \nMulyono, H., & Saskia, R.\u00a0(2021). Affective variables contributing to Indonesian EFL students\u2019 willingness to communicate within face-to-face and digital environments.\u00a0Cogent Education, 8(1), 1911282.\u00a0https://doi.org/10.1080/2331186X.2021.1911282 \nPoole, R.\u00a0(2022). \u201cCorpus can be tricky\u201d: Revisiting teacher attitudes towards corpus-aided language learning and teaching.\u00a0Computer Assisted Language Learning, 35(7), 1620\u20131641.\u00a0https://doi.org/10.1080/09588221.2020.1868533 \nReagan, D., Fell, E., & Mackey, A.\u00a0(2023). Applied linguistics in the age of anxiety.\u00a0Annual Review of Applied Linguistics, 43, 1\u20136.\u00a0 \nReppen, R.\u00a0(2022). Building a corpus: What are key considerations? In\u00a0The Routledge handbook of corpus linguistics\u00a0(pp. 13\u201320). Routledge. \nStorch, N.\u00a0(2021). Theoretical perspectives on L2 writing and language learning in collaborative writing and the collaborative processing of written corrective feedback. In\u00a0The Routledge handbook of second language acquisition and writing\u00a0(pp. 22\u201334). Routledge. \nUllah, A., & Usman, M.\u00a0(2023). Role of libraries in ensuring quality education at higher education institutions: A perspective of Pakistan.\u00a0Inverge Journal of Social Sciences, 2(4), 13\u201322. \nUllah, A.\u00a0(2024). Analyzing the students\u2019 attitudes and behavior towards traditional classes and technology-enhanced online learning.\u00a0International Journal of Social Science Archives.\u00a0https://www.ijssa.com/index.php/ijssa/article/view/498 \nUsman, M., Asif, M., Ullah, A., & Ullah, W.\u00a0(2024). User\u2019s habits and attitudes towards Chinese books reading in Pakistan.\u00a0Inverge Journal of Social Sciences, 3(2), 11\u201328. \nWei, W., Cheong, C. M., Zhu, X., & Lu, Q.\u00a0(2024). Comparing self-reflection and peer feedback practices in an academic writing task: A student self-efficacy perspective.\u00a0Teaching in Higher Education, 29(4), 896\u2013912.\u00a0https://doi.org/10.1080/13562517.2024.2316724 \nWiboolyasarin, W., Wiboolyasarin, K., Suwanwihok, K., Jinowat, N., & Muenjanchoey, R.\u00a0(2024). Synergizing collaborative writing and AI feedback: An investigation into enhancing L2 writing proficiency in wiki-based environments.\u00a0Computers and Education: Artificial Intelligence, 6, 100228.\u00a0https://doi.org/10.1016/j.caeai.2024.100228 \nWoodrow, L.\u00a0(2022).\u00a0Introducing researching English for specific purposes.\u00a0Routledge. \nXu, L., Naserpour, A., Rezai, A., Namaziandost, E., & Azizi, Z.\u00a0(2022). Exploring EFL learners\u2019 metaphorical conceptions of language learning: A multimodal analysis.\u00a0Journal of Psycholinguistic Research, 51(2), 323\u2013339.\u00a0https://doi.org/10.1007/s10936-022-09842-2 \nYALA, A.\u00a0(2022).\u00a0The use of mobile-assisted language learning to foster students\u2019 self-editing in sentence writing: Case of 1st year EFL students at Setif 2 University\u00a0[Doctoral dissertation, Universit\u00e9 de Batna 2]. \nZhang, Y. O., & Hyland, K.\u00a0(2021). Elements of doctoral apprenticeship: Community feedback and the acquisition of writing expertise.\u00a0Journal of Second Language Writing, 53, 100835.\u00a0https://doi.org/10.1016/j.jslw.2021.100835",
            "corpus_id": "277968005",
            "text": "This study examined how creative strategies such as artificial intelligence (AI) tools, collaborative writing, and digital storytelling activities impacted the enhancement of academic writing skills of English learners at the university level. The research sought to understand the writing problems that were most common, evaluate the implementation and effectiveness of the measures, and monitor the students' reception to the measures being implemented. A quantitative approach using surveys was used with a sample of 346 students from different fields of study. Structured questionnaires which had been tested in a pilot study (Cronbach\u2019s Alpha = 0.792) were used to gather data and were then processed in SPSS (Version 28). Descriptive statistics and cross tabulation were used to analyse the data and find the patterns pertaining to difficulties in writing, use of tools, and outcomes. The most critical findings were the remaining issues: grammar was a problem for 62.5%, coherence for 55.2%, and vocabulary for 49.4% of the students. In spite of this, self-reported use of innovative strategies was still high such as: AI tools usage (69.7%) and peer collaboration (65.4%) as well as engaging interactive methods (82.6%). Additionally, there was strong support for institutional adoption, with 87% of participants in favour; however, there was less support for advanced digital storytelling techniques (39.3% engagement). \nThe ethical considerations such as anonymity and voluntary participation were followed. Testing in the pilot phase reduced bias and no personal information was stored. This study addresses the gap in the integration of technology and collaborative teaching processes in the teaching of academic writing. It provides empirical data on the effectiveness of modern approaches while also revealing gaps in adoption across levels of proficiency and disciplines. The results make it possible for curriculum developers and decision makers to take tangible steps toward solving the discrepancies between offer and demand in the context of use. \nReferences \nAbahussain, M. O.\u00a0(2020). Investigating EFL learners\u2019 perceptions of collaborative writing.\u00a0International Journal of English Linguistics, 10(3), 32\u201347.\u00a0 \nAlawaji, N. N. M.\u00a0(2020). Students' perceptions of collaborative summary writing.\u00a0Theory and Practice in Language Studies, 10(6), 700\u2013707.\u00a0 \nBelyaeva, E. G.\u00a0(2022). Methodological model of teaching academic writing to undergraduate students.\u00a0Focus on Language Education and Research, 3(1), 36\u201351. \nButt, S.\u00a0(2023). Employees\u2019 perception regarding in-house training programs in Pakistani organizations.\u00a0Journal of Workplace Behavior, 4(1), 35\u201350. \nButt, S., & Yazdani, N.\u00a0(2023). Implementation of quality management practices and firm\u2019s innovation performance: Mediation of knowledge creation processes and moderating role of digital transformation.\u00a0Pakistan Journal of Humanities and Social Sciences, 11(4), 3881\u20133902. \nButt, S., Umair, T., & Tajammal, R.\u00a0(2024). Nexus between key determinants of service quality and students\u2019 satisfaction in higher education institutions (HEIs).\u00a0Annals of Human and Social Sciences, 5(2), 659\u2013671. \nChiew, M. T. L., & Ismail, H. H.\u00a0(2021). Exploring vocabulary learning strategies in a second language setting: A review.\u00a0International Journal of Academic Research in Business and Social Sciences, 11(12), 1298\u20131309. \nChubko, N., Morris, J. E., McKinnon, D. H., Slater, E. V., & Lummis, G. W.\u00a0(2020). Digital storytelling as a disciplinary literacy enhancement tool for EFL students.\u00a0Educational Technology Research and Development, 68, 3587\u20133604.\u00a0https://doi.org/10.1007/s11423-020-09833-x \nDavoodifard, M.\u00a0(2022). An overview of writing process research: Using innovative tasks and techniques for a better understanding of L2 writing processes in assessment contexts.\u00a0Studies in Applied Linguistics and TESOL, 21(2). \nDodigovic, M., & Jeaco, S.\u00a0(2021). Technology in applied linguistics.\u00a0International Journal of TESOL Studies, 3(2), 1\u20135. \nFerris, D. R.\u00a0(2023).\u00a0What error correction can (not) accomplish for second language writers: Dispelling myths, discussing options.\u00a0University of Michigan Press. \nFerris, D. R., & Hedgcock, J. S.\u00a0(2023).\u00a0Teaching L2 composition: Purpose, process, and practice\u00a0(4th ed.). Routledge. \nFlowerdew, L.\u00a0(2021). Learner corpora for disciplinary writing. In\u00a0Research questions in language education and applied linguistics: A reference guide\u00a0(pp. 475\u2013479). Springer. \nFlowerdew, L., & Petri\u0107, B.\u00a0(2024). A critical review of corpus-based pedagogic perspectives on thesis writing: Specificity revisited.\u00a0English for Specific Purposes, 76, 1\u201313.\u00a0https://doi.org/10.1016/j.esp.2024.01.001 \nHinkel, E.\u00a0(2022).\u00a0Teaching academic L2 writing: Practical techniques in vocabulary and grammar\u00a0(2nd ed.). Routledge. \nHyland, K.\u00a0(2024). Ken Hyland's essential bookshelf: Academic writing.\u00a0Language Teaching, 57(3), 399\u2013407.\u00a0https://doi.org/10.1017/S0261444824000109 \nHyland, K., & Hyland, F.\u00a0(2019).\u00a0Feedback in second language writing: Contexts and issues\u00a0(2nd ed.). Cambridge University Press. \nKang, E. Y., & Han, Z.\u00a0(2021). Written corrective feedback. In\u00a0The Routledge handbook of second language acquisition and writing\u00a0(pp. [page range]). Routledge. \nKarim, K., & Nassaji, H.\u00a0(2020). The effects of written corrective feedback.\u00a0Instructed Second Language Acquisition, 3(1), 28\u201352. \nKessler, G.\u00a0(2020). Professionalizing your use of technology in English language teaching. In\u00a0Professionalizing your English language teaching\u00a0(pp. 163\u2013173). Springer. \nKessler, M.\u00a0(2023). Written corrective feedback in an online community: A typology of English language learners\u2019 requests and interlocutors\u2019 responses.\u00a0Computers and Composition, 67, 102752.\u00a0https://doi.org/10.1016/j.compcom.2023.102752 \nKim, N. J., & Kim, M. K.\u00a0(2022). Teacher\u2019s perceptions of using an artificial intelligence-based educational tool for scientific writing.\u00a0Frontiers in Education, 7, 755914.\u00a0https://doi.org/10.3389/feduc.2022.755914 \nLi, J.\u00a0(2017). Automated writing evaluation: A pedagogical tool.\u00a0TESOL Quarterly, 51(2), 427\u2013432.\u00a0 \nLi, M.\u00a0(2021).\u00a0Researching and teaching second language writing in the digital age.\u00a0Palgrave Macmillan. \nLi, M., & Zhang, M.\u00a0(2023). Collaborative writing in L2 classrooms: A research agenda.\u00a0Language Teaching, 56(1), 94\u2013112.\u00a0https://doi.org/10.1017/S0261444821000318 \nMihaylova, M., Gorin, S., Reber, T. P., & Rothen, N.\u00a0(2022). A meta-analysis on mobile-assisted language learning applications: Benefits and risks.\u00a0Psychologica Belgica, 62(1), 252\u2013267.\u00a0 \nMulyono, H., & Saskia, R.\u00a0(2021). Affective variables contributing to Indonesian EFL students\u2019 willingness to communicate within face-to-face and digital environments.\u00a0Cogent Education, 8(1), 1911282.\u00a0https://doi.org/10.1080/2331186X.2021.1911282 \nPoole, R.\u00a0(2022). \u201cCorpus can be tricky\u201d: Revisiting teacher attitudes towards corpus-aided language learning and teaching.\u00a0Computer Assisted Language Learning, 35(7), 1620\u20131641.\u00a0https://doi.org/10.1080/09588221.2020.1868533 \nReagan, D., Fell, E., & Mackey, A.\u00a0(2023). Applied linguistics in the age of anxiety.\u00a0Annual Review of Applied Linguistics, 43, 1\u20136.\u00a0 \nReppen, R.\u00a0(2022). Building a corpus: What are key considerations? In\u00a0The Routledge handbook of corpus linguistics\u00a0(pp. 13\u201320). Routledge. \nStorch, N.\u00a0(2021). Theoretical perspectives on L2 writing and language learning in collaborative writing and the collaborative processing of written corrective feedback. In\u00a0The Routledge handbook of second language acquisition and writing\u00a0(pp. 22\u201334). Routledge. \nUllah, A., & Usman, M.\u00a0(2023). Role of libraries in ensuring quality education at higher education institutions: A perspective of Pakistan.\u00a0Inverge Journal of Social Sciences, 2(4), 13\u201322. \nUllah, A.\u00a0(2024). Analyzing the students\u2019 attitudes and behavior towards traditional classes and technology-enhanced online learning.\u00a0International Journal of Social Science Archives.\u00a0https://www.ijssa.com/index.php/ijssa/article/view/498 \nUsman, M., Asif, M., Ullah, A., & Ullah, W.\u00a0(2024). User\u2019s habits and attitudes towards Chinese books reading in Pakistan.\u00a0Inverge Journal of Social Sciences, 3(2), 11\u201328. \nWei, W., Cheong, C. M., Zhu, X., & Lu, Q.\u00a0(2024). Comparing self-reflection and peer feedback practices in an academic writing task: A student self-efficacy perspective.\u00a0Teaching in Higher Education, 29(4), 896\u2013912.\u00a0https://doi.org/10.1080/13562517.2024.2316724 \nWiboolyasarin, W., Wiboolyasarin, K., Suwanwihok, K., Jinowat, N., & Muenjanchoey, R.\u00a0(2024). Synergizing collaborative writing and AI feedback: An investigation into enhancing L2 writing proficiency in wiki-based environments.\u00a0Computers and Education: Artificial Intelligence, 6, 100228.\u00a0https://doi.org/10.1016/j.caeai.2024.100228 \nWoodrow, L.\u00a0(2022).\u00a0Introducing researching English for specific purposes.\u00a0Routledge. \nXu, L., Naserpour, A., Rezai, A., Namaziandost, E., & Azizi, Z.\u00a0(2022). Exploring EFL learners\u2019 metaphorical conceptions of language learning: A multimodal analysis.\u00a0Journal of Psycholinguistic Research, 51(2), 323\u2013339.\u00a0https://doi.org/10.1007/s10936-022-09842-2 \nYALA, A.\u00a0(2022).\u00a0The use of mobile-assisted language learning to foster students\u2019 self-editing in sentence writing: Case of 1st year EFL students at Setif 2 University\u00a0[Doctoral dissertation, Universit\u00e9 de Batna 2]. \nZhang, Y. O., & Hyland, K.\u00a0(2021). Elements of doctoral apprenticeship: Community feedback and the acquisition of writing expertise.\u00a0Journal of Second Language Writing, 53, 100835.\u00a0https://doi.org/10.1016/j.jslw.2021.100835",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.023193359375
        },
        {
            "paperId": "a0e76e07fec917f9e8dd11b096a6fd524c1a76f5",
            "corpusId": 273501407,
            "title": "Automatic Large Language Model Evaluation via Peer Review",
            "venue": "International Conference on Information and Knowledge Management",
            "year": 2024,
            "referenceCount": 46,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3627673.3679677?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3627673.3679677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1381470019",
                    "name": "Zhumin Chu"
                },
                {
                    "authorId": "2256982003",
                    "name": "Qingyao Ai"
                },
                {
                    "authorId": "2260653857",
                    "name": "Yiteng Tu"
                },
                {
                    "authorId": "2108590438",
                    "name": "Haitao Li"
                },
                {
                    "authorId": "2260835922",
                    "name": "Yiqun Liu"
                }
            ],
            "abstract": "The impressive performance of large language models (LLMs) has attracted considerable attention from the academic and industrial communities. Besides how to construct and train LLMs, how to effectively evaluate and compare the capacity of LLMs has also been well recognized as an important yet difficult problem. Existing paradigms rely on either human annotators or model-based evaluators to evaluate the performance of LLMs on different tasks. However, these paradigms often suffer from high cost, low generalizability, and inherited biases in practice, which make them incapable of supporting the sustainable development of LLMs in the long term. In order to address these issues, inspired by the peer review systems widely used in the academic publication process, we propose a novel framework that can automatically evaluate LLMs through a peer-review process. Specifically, for the evaluation of a specific task, we first construct a small qualification exam to select \"reviewers'' from a couple of powerful LLMs. Then, to actually evaluate the \"submissions\" written by different candidate LLMs, i.e., the evaluatees, we use the reviewer LLMs to rate or compare the submissions. The final ranking of evaluatee LLMs is generated based on the results provided by all reviewers. We conducted extensive experiments on both text summarization and non-factoid question-answering tasks with eleven LLMs including GPT-4. The results demonstrate the existence of biasness when evaluating using a single LLM. Also, our PRE model outperforms all the baselines, illustrating the effectiveness of the peer review mechanism.",
            "corpus_id": "273501407",
            "text": "The impressive performance of large language models (LLMs) has attracted considerable attention from the academic and industrial communities. Besides how to construct and train LLMs, how to effectively evaluate and compare the capacity of LLMs has also been well recognized as an important yet difficult problem. Existing paradigms rely on either human annotators or model-based evaluators to evaluate the performance of LLMs on different tasks. However, these paradigms often suffer from high cost, low generalizability, and inherited biases in practice, which make them incapable of supporting the sustainable development of LLMs in the long term. In order to address these issues, inspired by the peer review systems widely used in the academic publication process, we propose a novel framework that can automatically evaluate LLMs through a peer-review process. Specifically, for the evaluation of a specific task, we first construct a small qualification exam to select \"reviewers'' from a couple of powerful LLMs. Then, to actually evaluate the \"submissions\" written by different candidate LLMs, i.e., the evaluatees, we use the reviewer LLMs to rate or compare the submissions. The final ranking of evaluatee LLMs is generated based on the results provided by all reviewers. We conducted extensive experiments on both text summarization and non-factoid question-answering tasks with eleven LLMs including GPT-4. The results demonstrate the existence of biasness when evaluating using a single LLM. Also, our PRE model outperforms all the baselines, illustrating the effectiveness of the peer review mechanism.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.495361328125
        },
        {
            "paperId": "62976d38e53d73fe89b7f7394ea8e89358a998dd",
            "corpusId": 258332589,
            "title": "Does GPT4 dream of counting electric nodules?",
            "venue": "European Radiology",
            "year": 2023,
            "referenceCount": 15,
            "citationCount": 6,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s00330-023-09671-4.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10511354, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "6751897",
                    "name": "Christian Bl\u00fcthgen"
                }
            ],
            "abstract": "The year 2022 was nothing short of exceptional for artificial intelligence (AI) research, particularly in the realm of generative AI. Generative AI systems learn to model training data (e.g., text, images) and can synthesize new data, without merely copying the initial data. In November 2022, OpenAI unveiled ChatGPT [1], a large language model (LLM) designed as a chatbot, capable of processing complex text queries and producing coherent natural language outputs resembling human conversation partners. Adding to the excitement was the fact that model access was provided to a wide audience through a simple web interface. This shift towards accessible AI, combined with the model\u2019s impressive capabilities, piqued widespread interest from clinicians, scientists, and the general public. Radiologists and other medical experts, even those with little or no technical background, can now evaluate these models for medical purposes. At the time of this writing, there are nearly 200 PubMed entries (query: \u201cChatGPT\u201d) and around 7700 Google Scholar results, more than enough to warrant a closer look at the applications and limitations for our increasingly digitized field of radiology. ChatGPT works by sequentially predicting the next parts (tokens) of its output based on the provided input and its own previous output. This autoregressive architecture is well-suited for generating text that follows a left-to-right linear structure, such as composing or summarizing a radiology report [2, 3], but can struggle with tasks that require elaborate planning ahead or discontinuous tasks, e.g., when leaps in knowledge are needed [4]. It may also be the reason why ChatGPT tends to be bad at telling new jokes. Although ChatGPT\u2019s underlying model (generative pretrained transformer, GPT 3.5) was pretrained on a predominantly non-medical dataset (from sources like Wikipedia, scientific articles, and news websites), and not specifically on radiology reports, it appears to have incorporated enough radiological terminology on top of general language structure to produce outputs that sound fluent and plausible. Unfortunately, the model\u2019s outputs are not necessarily true; in fact, the model can very confidently \u201challucinate\u201d plausible sounding but blatantly wrong statements. Even more concerning, ChatGPT can double down and back up its erroneous claims with fabricated references including author lists, paper titles, hyperlinks, and even document object identifiers (DOIs). It is obvious that ChatGPT and similar LLMs are currently unfit to reliably provide medical information without human supervision. A substantial part of ChatGPT\u2019s success lies in its alignment process, which guides outputs with the help of a reward model that was trained on human feedback to rank outputs from a precursor model [1]. This effort led to more nuanced results and seems to have empowered rather than restricted the model. As Sam Altman, CEO of OpenAI, noted, \u201c[Capability and alignment are] very close. Better alignment techniques lead to better capabilities, and vice versa\u201d [5]. For radiology AI, a similar alignment process may be needed for reliably using an LLM in clinical practice. Nonetheless, extensive efforts are needed to have a medical LLM approved in most current jurisdictions [6]. Academic radiologists can already benefit from LLMs, which can assist in handling unstructured data prevalent in the medical field, summarizing research papers, and enhancing communication. All that is required are the right instructions: For example, LLMs can transfer lung nodule measurements mentioned in a radiology report into a CSV file to speed up data collection: \u201cCollect all lung nodule measurements and return a list of the form < Side > , < Lobe > , < Segment > , < Size > with one line per nodule\u201d. As processing large patient datasets raises privacy concerns, however, properly instructed LLMs can also aid in de-identification tasks [7]. Language barriers potentially affecting effective communication can be mitigated, as ChatGPT is multilingual and can aid non-native English speakers in translating, but also formulating their ideas with proper and concise English, fostering better understanding among researchers from different backgrounds. Apart from redacting, LLMs could also help with formatting tasks, e.g. to adapt a text to a target journal\u2019s preferred style before submission. Lastly, LLMs can provide * Christian Bl\u00fcthgen christian.bluethgen@usz.ch",
            "corpus_id": "258332589",
            "text": "The year 2022 was nothing short of exceptional for artificial intelligence (AI) research, particularly in the realm of generative AI. Generative AI systems learn to model training data (e.g., text, images) and can synthesize new data, without merely copying the initial data. In November 2022, OpenAI unveiled ChatGPT [1], a large language model (LLM) designed as a chatbot, capable of processing complex text queries and producing coherent natural language outputs resembling human conversation partners. Adding to the excitement was the fact that model access was provided to a wide audience through a simple web interface. This shift towards accessible AI, combined with the model\u2019s impressive capabilities, piqued widespread interest from clinicians, scientists, and the general public. Radiologists and other medical experts, even those with little or no technical background, can now evaluate these models for medical purposes. At the time of this writing, there are nearly 200 PubMed entries (query: \u201cChatGPT\u201d) and around 7700 Google Scholar results, more than enough to warrant a closer look at the applications and limitations for our increasingly digitized field of radiology. ChatGPT works by sequentially predicting the next parts (tokens) of its output based on the provided input and its own previous output. This autoregressive architecture is well-suited for generating text that follows a left-to-right linear structure, such as composing or summarizing a radiology report [2, 3], but can struggle with tasks that require elaborate planning ahead or discontinuous tasks, e.g., when leaps in knowledge are needed [4]. It may also be the reason why ChatGPT tends to be bad at telling new jokes. Although ChatGPT\u2019s underlying model (generative pretrained transformer, GPT 3.5) was pretrained on a predominantly non-medical dataset (from sources like Wikipedia, scientific articles, and news websites), and not specifically on radiology reports, it appears to have incorporated enough radiological terminology on top of general language structure to produce outputs that sound fluent and plausible. Unfortunately, the model\u2019s outputs are not necessarily true; in fact, the model can very confidently \u201challucinate\u201d plausible sounding but blatantly wrong statements. Even more concerning, ChatGPT can double down and back up its erroneous claims with fabricated references including author lists, paper titles, hyperlinks, and even document object identifiers (DOIs). It is obvious that ChatGPT and similar LLMs are currently unfit to reliably provide medical information without human supervision. A substantial part of ChatGPT\u2019s success lies in its alignment process, which guides outputs with the help of a reward model that was trained on human feedback to rank outputs from a precursor model [1]. This effort led to more nuanced results and seems to have empowered rather than restricted the model. As Sam Altman, CEO of OpenAI, noted, \u201c[Capability and alignment are] very close. Better alignment techniques lead to better capabilities, and vice versa\u201d [5]. For radiology AI, a similar alignment process may be needed for reliably using an LLM in clinical practice. Nonetheless, extensive efforts are needed to have a medical LLM approved in most current jurisdictions [6]. Academic radiologists can already benefit from LLMs, which can assist in handling unstructured data prevalent in the medical field, summarizing research papers, and enhancing communication. All that is required are the right instructions: For example, LLMs can transfer lung nodule measurements mentioned in a radiology report into a CSV file to speed up data collection: \u201cCollect all lung nodule measurements and return a list of the form < Side > , < Lobe > , < Segment > , < Size > with one line per nodule\u201d. As processing large patient datasets raises privacy concerns, however, properly instructed LLMs can also aid in de-identification tasks [7]. Language barriers potentially affecting effective communication can be mitigated, as ChatGPT is multilingual and can aid non-native English speakers in translating, but also formulating their ideas with proper and concise English, fostering better understanding among researchers from different backgrounds. Apart from redacting, LLMs could also help with formatting tasks, e.g. to adapt a text to a target journal\u2019s preferred style before submission. Lastly, LLMs can provide * Christian Bl\u00fcthgen christian.bluethgen@usz.ch",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.12274169921875
        }
    ],
    "quotes": {
        "cost": 0.19605300000000006,
        "quotes": [
            {
                "idx": 0,
                "key": "[257804696 | Liu et al. | 2023 | Citations: 1211]",
                "snippets": "G-EVAL may have a bias towards the LLMgenerated summaries because the model could share the same concept of evaluation criteria during generation and evaluation.\n\nOur work should be considered as a preliminary study on this issue, and more research is needed to fully understand the behavior of LLM-based evaluators to reduce its inherent bias towards LLMgenerated text. We highlight this concern in the context that LLM-based evaluators may lead to self-reinforcement of LLMs if the evaluation score is used as a reward signal for further tuning. And this could result in the over-fitting of the LLMs to their own evaluation criteria, rather than the true evaluation criteria of the NLG tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Analysis",
                        "pdf_hash": "",
                        "start": 138,
                        "end": 832,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 171
                            },
                            {
                                "start": 174,
                                "end": 338
                            },
                            {
                                "start": 341,
                                "end": 548
                            },
                            {
                                "start": 549,
                                "end": 725
                            },
                            {
                                "start": 726,
                                "end": 872
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "G-EVAL may have a bias towards the LLMgenerated summaries because the model could share the same concept of evaluation criteria during generation and evaluation.\n\nOur work should be considered as a preliminary study on this issue, and more research is needed to fully understand the behavior of LLM-based evaluators to reduce its inherent bias towards LLMgenerated text. We highlight this concern in the context that LLM-based evaluators may lead to self-reinforcement of LLMs if the evaluation score is used as a reward signal for further tuning. And this could result in the over-fitting of the LLMs to their own evaluation criteria, rather than the true evaluation criteria of the NLG tasks."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[259274988 | Shi et al. | 2023 | Citations: 18]",
                "snippets": "When using GPT-4 to rate the performance of different models against the reference summary, there are a number of issues. First, there exists an inconsistency problem whereby there is no guarantee that an LLM will produce the identical output for the same input every time. A good practice to fix the problem is to set temperature = 0; the temperature parameter of the model controls the randomness of the text generated, with 0 being deterministic. Second, we find a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt. To mitigate such recency bias, we recommend reporting the mean score over both orders. Third, we observe from Figure 4 that GPT-4 assigns significantly higher scores to its own outputs compared to human ratings (64% vs. 43%), which represents the propensity of the model to favor its own suggestions. Future work should investigate the existence of potential biases in GPT-4 evaluation as well as possible mitigation strategies.\n\nDespite the potential of GPT-4 for automating evaluation, it is not as flexible as humans, particularly in situations where specific definitions of summary quality are not provided. The performance of GPT-4 hence may vary due to the lack of clarity in evaluation standards. GPT-4 aligns with the human evaluation, concluding that it generates better summaries than ChatGPT. However, GPT-4 encounters difficulty distinguishing scenarios where both models produce equally good summaries. In cases where human assessors perceive the summaries as equally good, GPT-4 may favor a summary that includes relevant contextual information does not present in the other, such as \"explicitly linking it to a high-fat, high-calorie meal\" or \"mention the lack of clinically significant differences\". It also may penalize certain aspects based on subjective criteria, such as \"less concise\".",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Which model performs better in the study?",
                        "pdf_hash": "",
                        "start": 303,
                        "end": 2181,
                        "sentence_offsets": [
                            {
                                "start": 265,
                                "end": 373
                            },
                            {
                                "start": 374,
                                "end": 495
                            },
                            {
                                "start": 496,
                                "end": 647
                            },
                            {
                                "start": 648,
                                "end": 823
                            },
                            {
                                "start": 824,
                                "end": 945
                            },
                            {
                                "start": 946,
                                "end": 1032
                            },
                            {
                                "start": 1033,
                                "end": 1246
                            },
                            {
                                "start": 1247,
                                "end": 1374
                            },
                            {
                                "start": 1377,
                                "end": 1558
                            },
                            {
                                "start": 1559,
                                "end": 1650
                            },
                            {
                                "start": 1651,
                                "end": 1750
                            },
                            {
                                "start": 1751,
                                "end": 1862
                            },
                            {
                                "start": 1863,
                                "end": 2162
                            },
                            {
                                "start": 2163,
                                "end": 2253
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "When using GPT-4 to rate the performance of different models against the reference summary, there are a number of issues. First, there exists an inconsistency problem whereby there is no guarantee that an LLM will produce the identical output for the same input every time. A good practice to fix the problem is to set temperature = 0; the temperature parameter of the model controls the randomness of the text generated, with 0 being deterministic. Second, we find a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt. To mitigate such recency bias, we recommend reporting the mean score over both orders. Third, we observe from Figure 4 that GPT-4 assigns significantly higher scores to its own outputs compared to human ratings (64% vs. 43%), which represents the propensity of the model to favor its own suggestions. Future work should investigate the existence of potential biases in GPT-4 evaluation as well as possible mitigation strategies.\n\nDespite the potential of GPT-4 for automating evaluation, it is not as flexible as humans, particularly in situations where specific definitions of summary quality are not provided. The performance of GPT-4 hence may vary due to the lack of clarity in evaluation standards. GPT-4 aligns with the human evaluation, concluding that it generates better summaries than ChatGPT. However, GPT-4 encounters difficulty distinguishing scenarios where both models produce equally good summaries. In cases where human assessors perceive the summaries as equally good, GPT-4 may favor a summary that includes relevant contextual information does not present in the other, such as \"explicitly linking it to a high-fat, high-calorie meal\" or \"mention the lack of clinically significant differences\". It also may penalize certain aspects based on subjective criteria, such as \"less concise\"."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[259360998 | Wu et al. | 2023 | Citations: 47]",
                "snippets": "This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4. By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "LLM Evaluation",
                        "pdf_hash": "",
                        "start": 327,
                        "end": 680,
                        "sentence_offsets": [
                            {
                                "start": 209,
                                "end": 442
                            },
                            {
                                "start": 443,
                                "end": 572
                            },
                            {
                                "start": 573,
                                "end": 658
                            },
                            {
                                "start": 659,
                                "end": 797
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4. By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[262045067 | Huang et al. | 2023 | Citations: 32]",
                "snippets": "Position bias is most relevant to the pairwise comparison protocol. While previous work has shown that GPT-4 does exhibit position bias when used to assess text quality in conversational-focused tasks (Wang et al., 2023;Zheng et al., 2023), none of the prior studies have investigated whether such bias is also observed when evaluating faithfulness or coverage. To analyze position bias, we task GPT-4 with assessing a pair of summaries generated by two LLMs on which one is better, and then swap the positions of these two summaries and query GPT-4 again. We compute the percentage of times GPT-4 prefers the first or second summaries. \n\nWhen GPT-4 compared pairs of LLM-generated summaries to evaluate faithfulness and coverage, a strong position bias surfaced, favoring the second entry (Table 3). Position bias was particularly pronounced when assessing similar-quality summaries (see Figure 23a). Hence, we deduce that GPT-4 is unreliable when utilized as a pairwise evaluator in the MDDS task with respect to faithfulness and coverage. Interestingly, this outcome contradicts Zheng et al. (2023), implying that the position of bias for LLM-based evaluators could vary across different tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Analysis",
                        "pdf_hash": "",
                        "start": 605,
                        "end": 1802,
                        "sentence_offsets": [
                            {
                                "start": 591,
                                "end": 672
                            },
                            {
                                "start": 673,
                                "end": 966
                            },
                            {
                                "start": 967,
                                "end": 1161
                            },
                            {
                                "start": 1162,
                                "end": 1241
                            },
                            {
                                "start": 1244,
                                "end": 1405
                            },
                            {
                                "start": 1406,
                                "end": 1506
                            },
                            {
                                "start": 1507,
                                "end": 1646
                            },
                            {
                                "start": 1647,
                                "end": 1802
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Position bias is most relevant to the pairwise comparison protocol. While previous work has shown that GPT-4 does exhibit position bias when used to assess text quality in conversational-focused tasks (Wang et al., 2023;Zheng et al., 2023), none of the prior studies have investigated whether such bias is also observed when evaluating faithfulness or coverage. To analyze position bias, we task GPT-4 with assessing a pair of summaries generated by two LLMs on which one is better, and then swap the positions of these two summaries and query GPT-4 again. We compute the percentage of times GPT-4 prefers the first or second summaries. \n\nWhen GPT-4 compared pairs of LLM-generated summaries to evaluate faithfulness and coverage, a strong position bias surfaced, favoring the second entry (Table 3). Position bias was particularly pronounced when assessing similar-quality summaries (see Figure 23a). Hence, we deduce that GPT-4 is unreliable when utilized as a pairwise evaluator in the MDDS task with respect to faithfulness and coverage. Interestingly, this outcome contradicts Zheng et al. (2023), implying that the position of bias for LLM-based evaluators could vary across different tasks."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[267311508 | Chu et al. | 2024 | Citations: 21]",
                "snippets": "Although several works have demonstrated that GPT-4 has decent evaluation capabilities [19]26], we found that GPT-4, so as other LLMs, often prefers responses of LLMs from its own series (i.e., the GPT models) over other LLMs despite of the actual quality of the responses. In other words, if we use GPT-4 as the evaluator, its inherent bias may make it difficult, if possible, to develop an LLM outside the GPT family that outperforms GPT-4.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 104,
                        "end": 546,
                        "sentence_offsets": [
                            {
                                "start": 104,
                                "end": 377
                            },
                            {
                                "start": 378,
                                "end": 546
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Although several works have demonstrated that GPT-4 has decent evaluation capabilities [19]26], we found that GPT-4, so as other LLMs, often prefers responses of LLMs from its own series (i.e., the GPT models) over other LLMs despite of the actual quality of the responses. In other words, if we use GPT-4 as the evaluator, its inherent bias may make it difficult, if possible, to develop an LLM outside the GPT family that outperforms GPT-4."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[267783118 | Gao et al. | 2023 | Citations: 9]",
                "snippets": "We compare both human and GPT-based evaluation scores and provide in-depth analysis. While our findings suggest that GPT-created surveys are more contemporary and accessible than human-authored ones, certain limitations were observed. Notably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors. At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 758,
                        "end": 1241,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We compare both human and GPT-based evaluation scores and provide in-depth analysis. While our findings suggest that GPT-created surveys are more contemporary and accessible than human-authored ones, certain limitations were observed. Notably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors. At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[270045144 | Lu et al. | 2024 | Citations: 7]",
                "snippets": "We compare three types of reviews: the human-written review, the GPT-4-generated review, and the GPT-3.5-generated review. Given that GPT-4 is commonly considered a stronger AI than GPT-3.5, and both models are considered worse than human expert reviewers, we hypothesize that we can utilize the LLM-generated reviews to simulate low-quality human reviews, thereby simulating three levels of quality.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "LLM-generated Reviews.",
                        "pdf_hash": "",
                        "start": 490,
                        "end": 890,
                        "sentence_offsets": [
                            {
                                "start": 490,
                                "end": 604
                            },
                            {
                                "start": 605,
                                "end": 612
                            },
                            {
                                "start": 613,
                                "end": 890
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We compare three types of reviews: the human-written review, the GPT-4-generated review, and the GPT-3.5-generated review. Given that GPT-4 is commonly considered a stronger AI than GPT-3.5, and both models are considered worse than human expert reviewers, we hypothesize that we can utilize the LLM-generated reviews to simulate low-quality human reviews, thereby simulating three levels of quality."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[270218703 | Sun et al. | 2024 | Citations: 20]",
                "snippets": "We use the LLMCompare as our evaluation protocol, which compares two candidate outputs and then selects the better one (Zheng et al., 2023)Wang et al., 2023).This is because LLMCompare coupled with GPT-4 is the best evaluation protocol, as mentioned in Liu et al. (2023).The evaluation prompts are shown in Appendix A.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259129398 | Zheng et al. | 2023 | Citations: 4439]": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge."
                },
                "metadata": [
                    {
                        "section_title": "Models and Metrics",
                        "pdf_hash": "",
                        "start": 366,
                        "end": 684,
                        "sentence_offsets": [
                            {
                                "start": 366,
                                "end": 524
                            },
                            {
                                "start": 524,
                                "end": 637
                            },
                            {
                                "start": 637,
                                "end": 684
                            }
                        ],
                        "ref_mentions": [
                            "259129398"
                        ],
                        "quote": "We use the LLMCompare as our evaluation protocol, which compares two candidate outputs and then selects the better one (Zheng et al., 2023)Wang et al., 2023).This is because LLMCompare coupled with GPT-4 is the best evaluation protocol, as mentioned in Liu et al. (2023).The evaluation prompts are shown in Appendix A."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[271050803 | Kim et al. | 2024 | Citations: 3]",
                "snippets": "This study shows that while GPT-4 is a powerful tool for AES, it is not sufficient on its own, as many factors influence both human raters and LLMs in essay scoring.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 479,
                        "end": 644,
                        "sentence_offsets": [
                            {
                                "start": 479,
                                "end": 644
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This study shows that while GPT-4 is a powerful tool for AES, it is not sufficient on its own, as many factors influence both human raters and LLMs in essay scoring."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[271126365 | Tang et al. | 2024 | Citations: 7]",
                "snippets": "The experimental outcomes corroborate that the types of prompt engineering and the models employed markedly influence the dependability of LLMs in AES tasks. The evidence suggests that more elaborated and specific prompts, encompassing criteria-based scoring with justifications, facilitate LLMs to approximate human-level consistency more closely. GPT-4, as the most advanced model evaluated, exhibited a heightened concordance with human raters, especially when justifications were integrated into the scoring processes.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Experiment 1: prompt engineering",
                        "pdf_hash": "",
                        "start": 837,
                        "end": 1359,
                        "sentence_offsets": [
                            {
                                "start": 837,
                                "end": 994
                            },
                            {
                                "start": 995,
                                "end": 1185
                            },
                            {
                                "start": 1186,
                                "end": 1359
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The experimental outcomes corroborate that the types of prompt engineering and the models employed markedly influence the dependability of LLMs in AES tasks. The evidence suggests that more elaborated and specific prompts, encompassing criteria-based scoring with justifications, facilitate LLMs to approximate human-level consistency more closely. GPT-4, as the most advanced model evaluated, exhibited a heightened concordance with human raters, especially when justifications were integrated into the scoring processes."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[271212285 | Zou et al. | 2024 | Citations: 9]",
                "snippets": "(ii) Different LLMs and systems exhibit substantial variations in the organization and style of their outputs, potentially leading to biases in traditional evaluation approaches. Therefore, we capitalize on the prowess of LLMs that have proven to be decent evaluators and can be easily adapted to the assessment of various answer types [14,24,39]. Inspired by Liu et al. [24], we clearly define the evaluation criteria for various types within the instruction prompt and then instruct GPT-4 to assign a score of 0 (incorrect) or 1 (correct). After evaluating 200 examples by both human evaluators and GPT-4, we found that the GPT-4 automatic evaluator shows a 98% agreement with human annotators, significantly exceeding the traditional string matching approach.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[257804696 | Liu et al. | 2023 | Citations: 1211]": "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval",
                    "[257378627 | Wang et al. | 2023 | Citations: 471]": "Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric. In this report, we provide a preliminary meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG meta-evaluation datasets (including summarization, story generation and data-to-text tasks). Experimental results show that compared with previous automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation with human judgments in most cases. In addition, we find that the effectiveness of the ChatGPT evaluator might be influenced by the creation method of the meta-evaluation datasets. For the meta-evaluation datasets which are created greatly depending on the reference and thus are biased, the ChatGPT evaluator might lose its effectiveness. We hope our preliminary study could prompt the emergence of a general-purposed reliable NLG metric."
                },
                "metadata": [
                    {
                        "section_title": "Evaluation Setup",
                        "pdf_hash": "",
                        "start": 486,
                        "end": 1248,
                        "sentence_offsets": [
                            {
                                "start": 469,
                                "end": 580
                            },
                            {
                                "start": 582,
                                "end": 760
                            },
                            {
                                "start": 760,
                                "end": 928
                            },
                            {
                                "start": 928,
                                "end": 1121
                            },
                            {
                                "start": 1121,
                                "end": 1341
                            }
                        ],
                        "ref_mentions": [
                            "257804696",
                            "257378627",
                            "257804696"
                        ],
                        "quote": "(ii) Different LLMs and systems exhibit substantial variations in the organization and style of their outputs, potentially leading to biases in traditional evaluation approaches. Therefore, we capitalize on the prowess of LLMs that have proven to be decent evaluators and can be easily adapted to the assessment of various answer types [14,24,39]. Inspired by Liu et al. [24], we clearly define the evaluation criteria for various types within the instruction prompt and then instruct GPT-4 to assign a score of 0 (incorrect) or 1 (correct). After evaluating 200 examples by both human evaluators and GPT-4, we found that the GPT-4 automatic evaluator shows a 98% agreement with human annotators, significantly exceeding the traditional string matching approach."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[271310142 | Pereira et al. | 2024 | Citations: 6]",
                "snippets": "G-EVAL is another recent approach that leverages the capabilities of LLMs, specifically GPT-4, to improve NLG evaluation. Proposed by Liu et al. (2023) [Liu et al. 2023], G-EVAL introduces a chain-of-thought (CoT) (Wei et al., 2022) paradigm where the evaluation process is guided by detailed intermediate steps generated by the LLM. This method has shown improvements in correlation with human evaluations, particularly in tasks such as text summarization and dialogue generation. To address issues with score distribution and variability, G-EVAL employs a self-consistency strategy. Specifically, it generates multiple samples (n = 20) using different decoding parameters and averages the evaluation scores across these samples. This approach helps mitigate two key problems: the dominance of a single score (such as 3 on a 1-5 scale) and the tendency of LLMs to output integer scores, even when decimal values are requested.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[246411621 | Wei et al. | 2022 | Citations: 9683]": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1233,
                        "end": 2159,
                        "sentence_offsets": [
                            {
                                "start": 1233,
                                "end": 1354
                            },
                            {
                                "start": 1355,
                                "end": 1565
                            },
                            {
                                "start": 1566,
                                "end": 1713
                            },
                            {
                                "start": 1714,
                                "end": 1816
                            },
                            {
                                "start": 1817,
                                "end": 1962
                            },
                            {
                                "start": 1963,
                                "end": 2159
                            }
                        ],
                        "ref_mentions": [
                            "246411621"
                        ],
                        "quote": "G-EVAL is another recent approach that leverages the capabilities of LLMs, specifically GPT-4, to improve NLG evaluation. Proposed by Liu et al. (2023) [Liu et al. 2023], G-EVAL introduces a chain-of-thought (CoT) (Wei et al., 2022) paradigm where the evaluation process is guided by detailed intermediate steps generated by the LLM. This method has shown improvements in correlation with human evaluations, particularly in tasks such as text summarization and dialogue generation. To address issues with score distribution and variability, G-EVAL employs a self-consistency strategy. Specifically, it generates multiple samples (n = 20) using different decoding parameters and averages the evaluation scores across these samples. This approach helps mitigate two key problems: the dominance of a single score (such as 3 on a 1-5 scale) and the tendency of LLMs to output integer scores, even when decimal values are requested."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[271544398 | Wu et al. | 2024 | Citations: 0]",
                "snippets": "Studies confirm the capability of LLMs to evaluate other LLMs, with verification abilities improving faster than generation quality [29]. GPT-4's evaluations exceeds 80% consistency with human reviewers (Zheng et al., 2023) and exceeds 85% alignment in pairwise comparisons (Bai et al., 2023), reaching nearly 100% agreement when performance differences are significant (Bai et al., 2023). Self-verification benefits LLM performance [32], though biases issues [33], such as position bias [34], length bias (Zheng et al., 2023), and self-bias [35], persist but can be reduced through proper design [36].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259095491 | Bai et al. | 2023 | Citations: 149]": "Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains to probe for a broad acquisition, and raise follow-up questions to engage in a more in-depth assessment. (2) Upon evaluation, the examiner combines both scoring and ranking measurements, providing a reliable result as it aligns closely with human annotations. (3) We additionally propose a decentralized Peer-examination method to address the biases in a single examiner. Our data and benchmarking results are available at: https://lmexam.com.",
                    "[259129398 | Zheng et al. | 2023 | Citations: 4439]": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge."
                },
                "metadata": [
                    {
                        "section_title": "Evaluation of generated review quality",
                        "pdf_hash": "",
                        "start": 368,
                        "end": 910,
                        "sentence_offsets": [
                            {
                                "start": 368,
                                "end": 505
                            },
                            {
                                "start": 506,
                                "end": 713
                            },
                            {
                                "start": 714,
                                "end": 910
                            }
                        ],
                        "ref_mentions": [
                            "259129398",
                            "259095491",
                            "259095491",
                            "259129398"
                        ],
                        "quote": "Studies confirm the capability of LLMs to evaluate other LLMs, with verification abilities improving faster than generation quality [29]. GPT-4's evaluations exceeds 80% consistency with human reviewers (Zheng et al., 2023) and exceeds 85% alignment in pairwise comparisons (Bai et al., 2023), reaching nearly 100% agreement when performance differences are significant (Bai et al., 2023). Self-verification benefits LLM performance [32], though biases issues [33], such as position bias [34], length bias (Zheng et al., 2023), and self-bias [35], persist but can be reduced through proper design [36]."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[271769421 | Chang et al. | 2024 | Citations: 1]",
                "snippets": "To this end, we employ GPT-4 (version gpt-4-0613), which is currently recognized as one of the most advanced LLMs, to assess the quality of TL-DRs generated by our method compared to a baseline method...To mitigate position bias (Zheng et al., 2023), each pair is evaluated twice with the positions of the two TLDRs swapped. A method is considered the winner for a sample only if both evaluations favored the same TLDR. In cases of conflicting results or ties, the sample is marked as a tie.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "LLM as Evaluator",
                        "pdf_hash": "",
                        "start": 450,
                        "end": 650,
                        "sentence_offsets": [
                            {
                                "start": 450,
                                "end": 651
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To this end, we employ GPT-4 (version gpt-4-0613), which is currently recognized as one of the most advanced LLMs, to assess the quality of TL-DRs generated by our method compared to a baseline method"
                    },
                    {
                        "section_title": "LLM as Evaluator",
                        "pdf_hash": "",
                        "start": 963,
                        "end": 1252,
                        "sentence_offsets": [
                            {
                                "start": 963,
                                "end": 1084
                            },
                            {
                                "start": 1085,
                                "end": 1179
                            },
                            {
                                "start": 1180,
                                "end": 1251
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To mitigate position bias (Zheng et al., 2023), each pair is evaluated twice with the positions of the two TLDRs swapped. A method is considered the winner for a sample only if both evaluations favored the same TLDR. In cases of conflicting results or ties, the sample is marked as a tie."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[272968899 | Giglou et al. | 2024 | Citations: 5]",
                "snippets": "Prior work has demonstrated the effectiveness of LLMs as automatic evaluators for scientific synthesis (Bai et al., 2023)17,19], arguing that they are becoming both the tools and the standards for language assessment. Thus, for the evaluation task, we utilized GPT-4-Turbo, the most advanced LLM available at the time of writing...Finally, similar to 3-fold cross-validation, the GPT-4 synthesis evaluator was run thrice on the same data samples to ensure consistent assessment. The resulting scores, which did not show significant divergence largely due to our detailed prompt specification of the expected evaluation task leaving little room for ambiguity in interpretation, were averaged and reported.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259095491 | Bai et al. | 2023 | Citations: 149]": "Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains to probe for a broad acquisition, and raise follow-up questions to engage in a more in-depth assessment. (2) Upon evaluation, the examiner combines both scoring and ranking measurements, providing a reliable result as it aligns closely with human annotations. (3) We additionally propose a decentralized Peer-examination method to address the biases in a single examiner. Our data and benchmarking results are available at: https://lmexam.com."
                },
                "metadata": [
                    {
                        "section_title": "LLM Evaluation of Synthesis Quality",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 313,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 202
                            },
                            {
                                "start": 203,
                                "end": 314
                            }
                        ],
                        "ref_mentions": [
                            "259095491"
                        ],
                        "quote": "Prior work has demonstrated the effectiveness of LLMs as automatic evaluators for scientific synthesis (Bai et al., 2023)17,19], arguing that they are becoming both the tools and the standards for language assessment. Thus, for the evaluation task, we utilized GPT-4-Turbo, the most advanced LLM available at the time of writing"
                    },
                    {
                        "section_title": "LLM Evaluation of Synthesis Quality",
                        "pdf_hash": "",
                        "start": 1751,
                        "end": 2125,
                        "sentence_offsets": [
                            {
                                "start": 1751,
                                "end": 1898
                            },
                            {
                                "start": 1899,
                                "end": 2124
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Finally, similar to 3-fold cross-validation, the GPT-4 synthesis evaluator was run thrice on the same data samples to ensure consistent assessment. The resulting scores, which did not show significant divergence largely due to our detailed prompt specification of the expected evaluation task leaving little room for ambiguity in interpretation, were averaged and reported."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[273346449 | Yousefi et al. | 2024 | Citations: 1]",
                "snippets": "In addition to their analytical capabilities, LLMs have been shown to possess a degree of human-like judgment in evaluating the quality of text. The G-EVAL framework, which utilizes LLMs to assess the quality of natural language generation outputs, demonstrates that LLMs can align closely with human evaluators in certain contexts (Liu, 2023). However, the deployment of LLMs in academic evaluation is not without challenges. LLMs can exhibit biases similar to those found in human judgments, which may affect the fairness and accuracy of their evaluations (Acerbi, 2023). Furthermore, the phenomenon of \"hallucination,\" where LLMs produce plausible but factually incorrect information, poses a risk in academic contexts (Buchanan et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[265442718 | Buchanan et al. | 2023 | Citations: 15]": "In this study, we generate prompts derived from every topic within the Journal of Economic Literature to assess the abilities of both GPT-3.5 and GPT-4 versions of the ChatGPT large language model (LLM) to write about economic concepts. ChatGPT demonstrates considerable competency in offering general summaries but also cites non-existent references. More than 30% of the citations provided by the GPT-3.5 version do not exist and this rate is only slightly reduced for the GPT-4 version. Additionally, our findings suggest that the reliability of the model decreases as the prompts become more specific. We provide quantitative evidence for errors in ChatGPT output to demonstrate the importance of LLM verification. JEL Codes: B4; O33; I2"
                },
                "metadata": [
                    {
                        "section_title": "Large Language Models in Academic Evaluation",
                        "pdf_hash": "",
                        "start": 691,
                        "end": 1430,
                        "sentence_offsets": [
                            {
                                "start": 691,
                                "end": 835
                            },
                            {
                                "start": 836,
                                "end": 1035
                            },
                            {
                                "start": 1036,
                                "end": 1117
                            },
                            {
                                "start": 1118,
                                "end": 1264
                            },
                            {
                                "start": 1265,
                                "end": 1430
                            }
                        ],
                        "ref_mentions": [
                            "265442718"
                        ],
                        "quote": "In addition to their analytical capabilities, LLMs have been shown to possess a degree of human-like judgment in evaluating the quality of text. The G-EVAL framework, which utilizes LLMs to assess the quality of natural language generation outputs, demonstrates that LLMs can align closely with human evaluators in certain contexts (Liu, 2023). However, the deployment of LLMs in academic evaluation is not without challenges. LLMs can exhibit biases similar to those found in human judgments, which may affect the fairness and accuracy of their evaluations (Acerbi, 2023). Furthermore, the phenomenon of \"hallucination,\" where LLMs produce plausible but factually incorrect information, poses a risk in academic contexts (Buchanan et al., 2023)."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[273374964 | Chen et al. | 2024 | Citations: 3]",
                "snippets": "From the results, we can observe a significant performance gap between GPT-4 and Auto-PRE, with GPT-4's rate much higher than that of other methods. This suggests that GPT-4's preference for answers from GPT-3.5-turbo compromises its performance and reliability. Conversely, Auto-PRE enhances overall performance and reliability by collaborating with various types of LLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Bias Analysis",
                        "pdf_hash": "",
                        "start": 1164,
                        "end": 1537,
                        "sentence_offsets": [
                            {
                                "start": 1164,
                                "end": 1312
                            },
                            {
                                "start": 1313,
                                "end": 1426
                            },
                            {
                                "start": 1427,
                                "end": 1537
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "From the results, we can observe a significant performance gap between GPT-4 and Auto-PRE, with GPT-4's rate much higher than that of other methods. This suggests that GPT-4's preference for answers from GPT-3.5-turbo compromises its performance and reliability. Conversely, Auto-PRE enhances overall performance and reliability by collaborating with various types of LLMs."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[273811997 | Weng et al. | 2024 | Citations: 25]",
                "snippets": "when LLMs act as judges, even the most advanced models, such as GPT-4 (Achiam et al., 2023) and Gemini (Reid et al., 2024), still lag behind reward models specifically trained for the task, as seen in RewardBench (Lambert et al., 2024). This gap highlights the challenge of achieving human-level judgment and reasoning in AI-driven peer reviews. In contrast, we train a Generative Reward Model (Zhang et al., 2024) to simulate a comprehensive peer review.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 1056,
                        "end": 1511,
                        "sentence_offsets": [
                            {
                                "start": 1047,
                                "end": 1292
                            },
                            {
                                "start": 1293,
                                "end": 1401
                            },
                            {
                                "start": 1402,
                                "end": 1511
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "when LLMs act as judges, even the most advanced models, such as GPT-4 (Achiam et al., 2023) and Gemini (Reid et al., 2024), still lag behind reward models specifically trained for the task, as seen in RewardBench (Lambert et al., 2024). This gap highlights the challenge of achieving human-level judgment and reasoning in AI-driven peer reviews. In contrast, we train a Generative Reward Model (Zhang et al., 2024) to simulate a comprehensive peer review."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[273901331 | Chai et al. | 2024 | Citations: 5]",
                "snippets": "To address the limitations of automatic evaluation metrics, we incorporate an LLM evaluation to assess the quality of the generated scientific hypotheses. Specifically, we employ Claude-3.5 instead of the more mainstream GPT-4 to mitigate potential self-enhancement bias, which occurs when a model is evaluated using its own framework (Xu et al., 2024b).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "LLM As a Judge",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 354,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 154
                            },
                            {
                                "start": 155,
                                "end": 354
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To address the limitations of automatic evaluation metrics, we incorporate an LLM evaluation to assess the quality of the generated scientific hypotheses. Specifically, we employ Claude-3.5 instead of the more mainstream GPT-4 to mitigate potential self-enhancement bias, which occurs when a model is evaluated using its own framework (Xu et al., 2024b)."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[276296422 | Wu et al. | 2025 | Citations: 1]",
                "snippets": "Liang et al. (Liang et al., 2023) conducted a large-scale empirical analysis to assess comments generated by GPT-4: they tag several metrics and investigated user satisfaction to measure comment quality. They found that while LLM-generated reviews were helpful, they might be non-generic and tended to focus on certain aspects of scientific feedback. Liu and Shah (Liu and Shah, 2023) validated the utility of LLMs across three tasks: identifying errors, verifying checklists, and choosing the \"better\" pa-per. They concluded that LLMs serve well as review assistants for specific reviewing tasks; however, they are not yet sufficient for conducting comprehensive evaluations of papers. Mike Thelwall (Thelwall, 2024) used GPT-4 to evaluate the quality of journal articles on a paper assessment dataset to test its effectiveness. The results indicated that GPT-4 appears to be insufficiently accurate for any formal or informal research quality assessment tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[263608784 | Liang et al. | 2023 | Citations: 145]": "Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.",
                    "[267547461 | Thelwall | 2024 | Citations: 30]": "Abstract Purpose Assess whether ChatGPT 4.0 is accurate enough to perform research evaluations on journal articles to automate this time-consuming task. Design/methodology/approach Test the extent to which ChatGPT-4 can assess the quality of journal articles using a case study of the published scoring guidelines of the UK Research Excellence Framework (REF) 2021 to create a research evaluation ChatGPT. This was applied to 51 of my own articles and compared against my own quality judgements. Findings ChatGPT-4 can produce plausible document summaries and quality evaluation rationales that match the REF criteria. Its overall scores have weak correlations with my self-evaluation scores of the same documents (averaging r=0.281 over 15 iterations, with 8 being statistically significantly different from 0). In contrast, the average scores from the 15 iterations produced a statistically significant positive correlation of 0.509. Thus, averaging scores from multiple ChatGPT-4 rounds seems more effective than individual scores. The positive correlation may be due to ChatGPT being able to extract the author\u2019s significance, rigour, and originality claims from inside each paper. If my weakest articles are removed, then the correlation with average scores (r=0.200) falls below statistical significance, suggesting that ChatGPT struggles to make fine-grained evaluations. Research limitations The data is self-evaluations of a convenience sample of articles from one academic in one field. Practical implications Overall, ChatGPT does not yet seem to be accurate enough to be trusted for any formal or informal research quality evaluation tasks. Research evaluators, including journal editors, should therefore take steps to control its use. Originality/value This is the first published attempt at post-publication expert review accuracy testing for ChatGPT."
                },
                "metadata": [
                    {
                        "section_title": "Large Language Models for Reviewing",
                        "pdf_hash": "",
                        "start": 803,
                        "end": 1765,
                        "sentence_offsets": [
                            {
                                "start": 803,
                                "end": 1006
                            },
                            {
                                "start": 1007,
                                "end": 1153
                            },
                            {
                                "start": 1154,
                                "end": 1313
                            },
                            {
                                "start": 1314,
                                "end": 1489
                            },
                            {
                                "start": 1490,
                                "end": 1632
                            },
                            {
                                "start": 1633,
                                "end": 1765
                            }
                        ],
                        "ref_mentions": [
                            "263608784",
                            "267547461"
                        ],
                        "quote": "Liang et al. (Liang et al., 2023) conducted a large-scale empirical analysis to assess comments generated by GPT-4: they tag several metrics and investigated user satisfaction to measure comment quality. They found that while LLM-generated reviews were helpful, they might be non-generic and tended to focus on certain aspects of scientific feedback. Liu and Shah (Liu and Shah, 2023) validated the utility of LLMs across three tasks: identifying errors, verifying checklists, and choosing the \"better\" pa-per. They concluded that LLMs serve well as review assistants for specific reviewing tasks; however, they are not yet sufficient for conducting comprehensive evaluations of papers. Mike Thelwall (Thelwall, 2024) used GPT-4 to evaluate the quality of journal articles on a paper assessment dataset to test its effectiveness. The results indicated that GPT-4 appears to be insufficiently accurate for any formal or informal research quality assessment tasks."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.023688,
        "cot": "After examining the query and quotes, I will organize them into meaningful dimensions to address the effectiveness and bias considerations of using GPT-4 to evaluate its own scientific writing compared to using different LLMs.\n\n1. \"Introduction/Background\" - This dimension will provide background on using LLMs for automated evaluation of scientific writing. No direct quotes may support this, but it's necessary context.\n\n2. \"Effectiveness of GPT-4 as an Evaluator\" - This dimension should synthesize information about how well GPT-4 performs as an evaluator of scientific writing in general. A synthesis format is appropriate as we need to coherently explain its capabilities and limitations.\n\n3. \"Bias Considerations When Using GPT-4 to Evaluate Its Own Outputs\" - This dimension will address the specific biases that emerge when GPT-4 evaluates content it or similar models have generated. A synthesis format will allow for comprehensive explanation of the various biases.\n\n4. \"Position and Ordering Biases in LLM Evaluation\" - This dimension focuses specifically on how the ordering of content affects GPT-4's evaluation, which appears to be a significant concern in several quotes. Synthesis format is appropriate to explain this complex phenomenon.\n\n5. \"Alternative LLM Evaluators: Considerations\" - This dimension will cover what happens when using different LLMs (either more or less capable than GPT-4) as evaluators. Synthesis format will help compare different options.\n\n6. \"Best Practices for LLM-Based Evaluation\" - This dimension will extract practical recommendations for using LLMs to evaluate scientific writing. A list format is appropriate here as it presents discrete recommendations that can be followed.\n\nThis organization covers all the main aspects of the query while ensuring we can incorporate all the quotes provided.",
        "plan": {
            "Introduction/Background (synthesis)": [],
            "Effectiveness of GPT-4 as an Evaluator (synthesis)": [
                8,
                9,
                10,
                11,
                12,
                14,
                15
            ],
            "Bias Considerations When Using GPT-4 to Evaluate Its Own Outputs (synthesis)": [
                0,
                1,
                2,
                4,
                5,
                16,
                18
            ],
            "Position and Ordering Biases in LLM Evaluation (synthesis)": [
                1,
                3,
                13
            ],
            "Alternative LLM Evaluators: Considerations (synthesis)": [
                2,
                6,
                17,
                18
            ],
            "Best Practices for LLM-Based Evaluation (list)": [
                1,
                7,
                11,
                12,
                13,
                14,
                19
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction/Background",
                "tldr": "The evaluation of AI-generated academic writing has become crucial as LLMs like GPT-4 are increasingly used in scientific contexts. This creates a complex evaluation challenge where researchers must consider whether to use the same model as both generator and evaluator or employ different models, each approach having distinct implications for effectiveness and bias. (LLM Memory)",
                "text": "\nLarge language models (LLMs) like GPT-4 are increasingly being used to generate scientific academic writing, from drafting research papers to summarizing literature and formulating hypotheses. As these AI-generated texts become more prevalent in academic contexts, the need for robust, efficient, and unbiased evaluation methods has emerged as a critical concern. Researchers and institutions face a fundamental question: when evaluating AI-generated academic content, should they use the same model that generated the content (self-evaluation) or employ a different model (cross-evaluation)? This question is particularly relevant when using GPT-4, which possesses advanced capabilities for both generation and evaluation of complex academic content. The choice between self-evaluation and cross-evaluation involves intricate trade-offs related to effectiveness, various forms of bias, and practical implementation considerations. This creates a complex evaluation landscape where researchers must carefully weigh these factors to establish credible assessment frameworks for AI-generated scientific writing. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Effectiveness of GPT-4 as an Evaluator",
                "tldr": "GPT-4 demonstrates strong capabilities as an evaluator of scientific academic writing, showing high agreement with human evaluators (80-98%) when properly prompted. However, its effectiveness varies based on task complexity, prompt engineering strategies, and implementation approaches like chain-of-thought reasoning and self-consistency mechanisms. (10 sources)",
                "text": "\nGPT-4 has emerged as a powerful automated evaluator for academic writing, with studies confirming its ability to assess various aspects of scientific content with considerable accuracy. When properly designed, GPT-4-based evaluation systems can achieve remarkable alignment with human judgment. Zou et al. found that GPT-4 demonstrated a 98% agreement with human annotators when evaluating academic responses, significantly outperforming traditional evaluation methods <Paper corpusId=\"271212285\" paperTitle=\"(Zou et al., 2024)\" isShortName></Paper>. Similarly, Wu et al. reported that GPT-4's evaluations exceed 80% consistency with human reviewers and reach up to 85% alignment in pairwise comparisons, approaching near-perfect agreement when performance differences are significant <Paper corpusId=\"271544398\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259129398\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>.\n\nThe effectiveness of GPT-4 as an evaluator is notably influenced by prompt engineering strategies. Tang et al. found that more elaborate and specific prompts, particularly those incorporating criteria-based scoring with justifications, enable GPT-4 to more closely approximate human-level consistency in automated essay scoring tasks <Paper corpusId=\"271126365\" paperTitle=\"(Tang et al., 2024)\" isShortName></Paper>. This finding underscores the importance of thoughtful prompt design in maximizing GPT-4's evaluation capabilities.\n\nAdvanced evaluation frameworks have further enhanced GPT-4's effectiveness. The G-EVAL framework, introduced by Liu et al., employs a chain-of-thought paradigm that guides the evaluation process through detailed intermediate steps <Paper corpusId=\"271310142\" paperTitle=\"(Pereira et al., 2024)\" isShortName></Paper> <Paper corpusId=\"257804696\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This approach has shown improved correlation with human evaluations for tasks such as text summarization and dialogue generation. G-EVAL also addresses score distribution issues by implementing a self-consistency strategy that generates multiple evaluation samples and averages the scores, mitigating the tendency of LLMs to output integer scores and favor certain values <Paper corpusId=\"271310142\" paperTitle=\"(Pereira et al., 2024)\" isShortName></Paper>.\n\nFor scientific synthesis evaluation specifically, Giglou et al. demonstrated the effectiveness of GPT-4-Turbo as an automatic evaluator, implementing a cross-validation approach where the model performed multiple assessments on the same data samples to ensure consistent evaluation <Paper corpusId=\"272968899\" paperTitle=\"(Giglou et al., 2024)\" isShortName></Paper>. Their detailed prompt specification reduced ambiguity in the evaluation task, resulting in minimal divergence between assessment runs <Paper corpusId=\"272968899\" paperTitle=\"(Giglou et al., 2024)\" isShortName></Paper>.\n\nDespite these promising capabilities, researchers acknowledge that GPT-4 is not without limitations as an evaluator. Kim et al. cautioned that while GPT-4 is a powerful tool for automated essay scoring, \"it is not sufficient on its own, as many factors influence both human raters and LLMs in essay scoring\" <Paper corpusId=\"271050803\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. Additionally, Yousefi et al. highlighted that LLMs can exhibit biases similar to human judgments and are susceptible to hallucination\u2014generating plausible but factually incorrect information\u2014which poses particular risks in academic evaluation contexts <Paper corpusId=\"273346449\" paperTitle=\"(Yousefi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265442718\" paperTitle=\"(Buchanan et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Zou et al., 2024)",
                        "snippets": [
                            "(ii) Different LLMs and systems exhibit substantial variations in the organization and style of their outputs, potentially leading to biases in traditional evaluation approaches. Therefore, we capitalize on the prowess of LLMs that have proven to be decent evaluators and can be easily adapted to the assessment of various answer types [14,24,39]. Inspired by Liu et al. [24], we clearly define the evaluation criteria for various types within the instruction prompt and then instruct GPT-4 to assign a score of 0 (incorrect) or 1 (correct). After evaluating 200 examples by both human evaluators and GPT-4, we found that the GPT-4 automatic evaluator shows a 98% agreement with human annotators, significantly exceeding the traditional string matching approach."
                        ],
                        "paper": {
                            "corpus_id": 271212285,
                            "title": "DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems",
                            "authors": [
                                {
                                    "authorId": "2187586628",
                                    "name": "Anni Zou"
                                },
                                {
                                    "authorId": "2265843326",
                                    "name": "Wenhao Yu"
                                },
                                {
                                    "authorId": "2254831297",
                                    "name": "Hongming Zhang"
                                },
                                {
                                    "authorId": "2311458574",
                                    "name": "Kaixin Ma"
                                },
                                {
                                    "authorId": "2286805668",
                                    "name": "Deng Cai"
                                },
                                {
                                    "authorId": "3322871",
                                    "name": "Zhuosheng Zhang"
                                },
                                {
                                    "authorId": "2287982566",
                                    "name": "Hai Zhao"
                                },
                                {
                                    "authorId": "2261392681",
                                    "name": "Dong Yu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Proceedings of the 4th International Workshop on Knowledge-Augmented Methods for Natural Language Processing",
                            "n_citations": 9
                        },
                        "score": 0.580078125
                    },
                    {
                        "id": "(Wu et al., 2024)",
                        "snippets": [
                            "Studies confirm the capability of LLMs to evaluate other LLMs, with verification abilities improving faster than generation quality [29]. GPT-4's evaluations exceeds 80% consistency with human reviewers (Zheng et al., 2023) and exceeds 85% alignment in pairwise comparisons (Bai et al., 2023), reaching nearly 100% agreement when performance differences are significant (Bai et al., 2023). Self-verification benefits LLM performance [32], though biases issues [33], such as position bias [34], length bias (Zheng et al., 2023), and self-bias [35], persist but can be reduced through proper design [36]."
                        ],
                        "paper": {
                            "corpus_id": 271544398,
                            "title": "Automated literature research and review-generation method based on large language models",
                            "authors": [
                                {
                                    "authorId": "2142349414",
                                    "name": "Shican Wu"
                                },
                                {
                                    "authorId": "2313884864",
                                    "name": "Xiao Ma"
                                },
                                {
                                    "authorId": "2313733635",
                                    "name": "Dehui Luo"
                                },
                                {
                                    "authorId": "2277565237",
                                    "name": "Lulu Li"
                                },
                                {
                                    "authorId": "2289200637",
                                    "name": "Xiangcheng Shi"
                                },
                                {
                                    "authorId": "2289200103",
                                    "name": "Xin Chang"
                                },
                                {
                                    "authorId": "2152842807",
                                    "name": "Xiaoyun Lin"
                                },
                                {
                                    "authorId": "2075340202",
                                    "name": "Ran Luo"
                                },
                                {
                                    "authorId": "48471690",
                                    "name": "Chunlei Pei"
                                },
                                {
                                    "authorId": "12144921",
                                    "name": "Zhijian Zhao"
                                },
                                {
                                    "authorId": "2258427381",
                                    "name": "Jinlong Gong"
                                }
                            ],
                            "year": 2024,
                            "venue": "National Science Review",
                            "n_citations": 0
                        },
                        "score": 0.63232421875
                    },
                    {
                        "id": "(Zheng et al., 2023)",
                        "snippets": [
                            "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge."
                        ],
                        "paper": {
                            "corpus_id": 259129398,
                            "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
                            "authors": [
                                {
                                    "authorId": "2149970173",
                                    "name": "Lianmin Zheng"
                                },
                                {
                                    "authorId": "2537924",
                                    "name": "Wei-Lin Chiang"
                                },
                                {
                                    "authorId": "2209360681",
                                    "name": "Ying Sheng"
                                },
                                {
                                    "authorId": "92721493",
                                    "name": "Siyuan Zhuang"
                                },
                                {
                                    "authorId": "1390573666",
                                    "name": "Zhanghao Wu"
                                },
                                {
                                    "authorId": "2152482391",
                                    "name": "Yonghao Zhuang"
                                },
                                {
                                    "authorId": "143872641",
                                    "name": "Zi Lin"
                                },
                                {
                                    "authorId": "2141335450",
                                    "name": "Zhuohan Li"
                                },
                                {
                                    "authorId": "2117961435",
                                    "name": "Dacheng Li"
                                },
                                {
                                    "authorId": "143977260",
                                    "name": "E. Xing"
                                },
                                {
                                    "authorId": "145140331",
                                    "name": "Haotong Zhang"
                                },
                                {
                                    "authorId": "49988044",
                                    "name": "Joseph E. Gonzalez"
                                },
                                {
                                    "authorId": "2055174324",
                                    "name": "Ion Stoica"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 4439
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tang et al., 2024)",
                        "snippets": [
                            "The experimental outcomes corroborate that the types of prompt engineering and the models employed markedly influence the dependability of LLMs in AES tasks. The evidence suggests that more elaborated and specific prompts, encompassing criteria-based scoring with justifications, facilitate LLMs to approximate human-level consistency more closely. GPT-4, as the most advanced model evaluated, exhibited a heightened concordance with human raters, especially when justifications were integrated into the scoring processes."
                        ],
                        "paper": {
                            "corpus_id": 271126365,
                            "title": "Harnessing LLMs for multi-dimensional writing assessment: Reliability and alignment with human judgments",
                            "authors": [
                                {
                                    "authorId": "2118488377",
                                    "name": "Xiaoyi Tang"
                                },
                                {
                                    "authorId": "2272635431",
                                    "name": "Hongwei Chen"
                                },
                                {
                                    "authorId": "2302524052",
                                    "name": "Daoyu Lin"
                                },
                                {
                                    "authorId": "2301924234",
                                    "name": "Kexin Li"
                                }
                            ],
                            "year": 2024,
                            "venue": "Heliyon",
                            "n_citations": 7
                        },
                        "score": 0.7041015625
                    },
                    {
                        "id": "(Pereira et al., 2024)",
                        "snippets": [
                            "G-EVAL is another recent approach that leverages the capabilities of LLMs, specifically GPT-4, to improve NLG evaluation. Proposed by Liu et al. (2023) [Liu et al. 2023], G-EVAL introduces a chain-of-thought (CoT) (Wei et al., 2022) paradigm where the evaluation process is guided by detailed intermediate steps generated by the LLM. This method has shown improvements in correlation with human evaluations, particularly in tasks such as text summarization and dialogue generation. To address issues with score distribution and variability, G-EVAL employs a self-consistency strategy. Specifically, it generates multiple samples (n = 20) using different decoding parameters and averages the evaluation scores across these samples. This approach helps mitigate two key problems: the dominance of a single score (such as 3 on a 1-5 scale) and the tendency of LLMs to output integer scores, even when decimal values are requested."
                        ],
                        "paper": {
                            "corpus_id": 271310142,
                            "title": "Check-Eval: A Checklist-based Approach for Evaluating Text Quality",
                            "authors": [
                                {
                                    "authorId": "2257137831",
                                    "name": "Jayr Pereira"
                                },
                                {
                                    "authorId": "2256889299",
                                    "name": "R.A. Lotufo"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.65576171875
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "G-EVAL may have a bias towards the LLMgenerated summaries because the model could share the same concept of evaluation criteria during generation and evaluation.\n\nOur work should be considered as a preliminary study on this issue, and more research is needed to fully understand the behavior of LLM-based evaluators to reduce its inherent bias towards LLMgenerated text. We highlight this concern in the context that LLM-based evaluators may lead to self-reinforcement of LLMs if the evaluation score is used as a reward signal for further tuning. And this could result in the over-fitting of the LLMs to their own evaluation criteria, rather than the true evaluation criteria of the NLG tasks."
                        ],
                        "paper": {
                            "corpus_id": 257804696,
                            "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                            "authors": [
                                {
                                    "authorId": "2152797401",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "3310951",
                                    "name": "Dan Iter"
                                },
                                {
                                    "authorId": "2110197273",
                                    "name": "Yichong Xu"
                                },
                                {
                                    "authorId": "2146294891",
                                    "name": "Shuo Wang"
                                },
                                {
                                    "authorId": "8233965",
                                    "name": "Ruochen Xu"
                                },
                                {
                                    "authorId": "8652308",
                                    "name": "Chenguang Zhu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1211
                        },
                        "score": 0.55078125
                    },
                    {
                        "id": "(Giglou et al., 2024)",
                        "snippets": [
                            "Prior work has demonstrated the effectiveness of LLMs as automatic evaluators for scientific synthesis (Bai et al., 2023)17,19], arguing that they are becoming both the tools and the standards for language assessment. Thus, for the evaluation task, we utilized GPT-4-Turbo, the most advanced LLM available at the time of writing",
                            "Finally, similar to 3-fold cross-validation, the GPT-4 synthesis evaluator was run thrice on the same data samples to ensure consistent assessment. The resulting scores, which did not show significant divergence largely due to our detailed prompt specification of the expected evaluation task leaving little room for ambiguity in interpretation, were averaged and reported."
                        ],
                        "paper": {
                            "corpus_id": 272968899,
                            "title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                            "authors": [
                                {
                                    "authorId": "2360502238",
                                    "name": "Hamed Babaei Giglou"
                                },
                                {
                                    "authorId": "2261403432",
                                    "name": "Jennifer D'Souza"
                                },
                                {
                                    "authorId": "2268415251",
                                    "name": "S. Auer"
                                }
                            ],
                            "year": 2024,
                            "venue": "ACM/IEEE Joint Conference on Digital Libraries",
                            "n_citations": 5
                        },
                        "score": 0.837890625
                    },
                    {
                        "id": "(Kim et al., 2024)",
                        "snippets": [
                            "This study shows that while GPT-4 is a powerful tool for AES, it is not sufficient on its own, as many factors influence both human raters and LLMs in essay scoring."
                        ],
                        "paper": {
                            "corpus_id": 271050803,
                            "title": "Is GPT-4 Alone Sufficient for Automated Essay Scoring?: A Comparative Judgment Approach Based on Rater Cognition",
                            "authors": [
                                {
                                    "authorId": "2310489304",
                                    "name": "Seungju Kim"
                                },
                                {
                                    "authorId": "2310341618",
                                    "name": "Meounggun Jo"
                                }
                            ],
                            "year": 2024,
                            "venue": "ACM Conference on Learning @ Scale",
                            "n_citations": 3
                        },
                        "score": 0.6640625
                    },
                    {
                        "id": "(Yousefi et al., 2024)",
                        "snippets": [
                            "In addition to their analytical capabilities, LLMs have been shown to possess a degree of human-like judgment in evaluating the quality of text. The G-EVAL framework, which utilizes LLMs to assess the quality of natural language generation outputs, demonstrates that LLMs can align closely with human evaluators in certain contexts (Liu, 2023). However, the deployment of LLMs in academic evaluation is not without challenges. LLMs can exhibit biases similar to those found in human judgments, which may affect the fairness and accuracy of their evaluations (Acerbi, 2023). Furthermore, the phenomenon of \"hallucination,\" where LLMs produce plausible but factually incorrect information, poses a risk in academic contexts (Buchanan et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 273346449,
                            "title": "Learning the Bitter Lesson: Empirical Evidence from 20 Years of CVPR Proceedings",
                            "authors": [
                                {
                                    "authorId": "2325908556",
                                    "name": "Mojtaba Yousefi"
                                },
                                {
                                    "authorId": "2325892124",
                                    "name": "Jack Collins"
                                }
                            ],
                            "year": 2024,
                            "venue": "NLP4SCIENCE",
                            "n_citations": 1
                        },
                        "score": 0.57763671875
                    },
                    {
                        "id": "(Buchanan et al., 2023)",
                        "snippets": [
                            "In this study, we generate prompts derived from every topic within the Journal of Economic Literature to assess the abilities of both GPT-3.5 and GPT-4 versions of the ChatGPT large language model (LLM) to write about economic concepts. ChatGPT demonstrates considerable competency in offering general summaries but also cites non-existent references. More than 30% of the citations provided by the GPT-3.5 version do not exist and this rate is only slightly reduced for the GPT-4 version. Additionally, our findings suggest that the reliability of the model decreases as the prompts become more specific. We provide quantitative evidence for errors in ChatGPT output to demonstrate the importance of LLM verification. JEL Codes: B4; O33; I2"
                        ],
                        "paper": {
                            "corpus_id": 265442718,
                            "title": "ChatGPT Hallucinates Non-existent Citations: Evidence from Economics",
                            "authors": [
                                {
                                    "authorId": "2268284776",
                                    "name": "Joy Buchanan"
                                },
                                {
                                    "authorId": "2268287596",
                                    "name": "Stephen Hill"
                                },
                                {
                                    "authorId": "2184736732",
                                    "name": "O. Shapoval"
                                }
                            ],
                            "year": 2023,
                            "venue": "The American Economist",
                            "n_citations": 15
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Bias Considerations When Using GPT-4 to Evaluate Its Own Outputs",
                "tldr": "GPT-4 shows significant self-preference bias when evaluating its own outputs or those from similar model families, often rating them higher than human evaluators would. This self-reinforcement tendency creates challenges for objective evaluation and may impede the development of competing models, necessitating mitigation strategies like using alternative evaluator models. (7 sources)",
                "text": "\nWhen using GPT-4 to evaluate its own outputs in scientific academic writing, researchers have identified several concerning bias patterns. One of the most significant is self-preference bias, where GPT-4 systematically favors outputs generated by itself or models from the same family. Liu et al. highlighted this concern, noting that \"G-EVAL may have a bias towards the LLM-generated summaries because the model could share the same concept of evaluation criteria during generation and evaluation\" <Paper corpusId=\"257804696\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This creates a problematic self-reinforcement cycle, especially if evaluation scores are used as reward signals for further model tuning.\n\nEvidence of this self-preference is substantial across multiple studies. Shi et al. found that \"GPT-4 assigns significantly higher scores to its own outputs compared to human ratings (64% vs. 43%), which represents the propensity of the model to favor its own suggestions\" <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. Similarly, Chu et al. observed that \"GPT-4, so as other LLMs, often prefers responses of LLMs from its own series (i.e., the GPT models) over other LLMs despite of the actual quality of the responses\" <Paper corpusId=\"267311508\" paperTitle=\"(Chu et al., 2024)\" isShortName></Paper>. This bias creates a fundamental challenge for comparative evaluation, as Chen et al. demonstrated when they found \"a significant performance gap between GPT-4 and Auto-PRE, with GPT-4's rate much higher than that of other methods\" suggesting that \"GPT-4's preference for answers from GPT-3.5-turbo compromises its performance and reliability\" <Paper corpusId=\"273374964\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>.\n\nThe implications of this bias extend beyond simple scoring discrepancies. Wu et al. raised \"concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process\" <Paper corpusId=\"259360998\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>. More concerning, Chu et al. suggested that this inherent bias \"may make it difficult, if possible, to develop an LLM outside the GPT family that outperforms GPT-4\" <Paper corpusId=\"267311508\" paperTitle=\"(Chu et al., 2024)\" isShortName></Paper>, indicating that self-evaluation could potentially impede the progress of competing models.\n\nOrder effects represent another significant bias when using GPT-4 for evaluation. Shi et al. identified \"a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt\" <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. This recency bias can substantially impact evaluation outcomes if not properly controlled for in the evaluation design.\n\nTo address these concerns, researchers have begun implementing various mitigation strategies. For comparative evaluations, Shi et al. recommend \"reporting the mean score over both orders\" to mitigate recency bias <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. Wu et al. suggested incorporating \"multiple LLM judges\" to \"establish a more comprehensive and unbiased assessment\" <Paper corpusId=\"259360998\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>. A particularly promising approach involves using alternative models as evaluators. Chai et al. reported employing \"Claude-3.5 instead of the more mainstream GPT-4 to mitigate potential self-enhancement bias, which occurs when a model is evaluated using its own framework\" <Paper corpusId=\"273901331\" paperTitle=\"(Chai et al., 2024)\" isShortName></Paper>. Gao et al. also documented \"systematic bias in using GPT evaluation\" when comparing human and GPT-4 rating behaviors <Paper corpusId=\"267783118\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper>, reinforcing the need for careful consideration of evaluation design.",
                "citations": [
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "G-EVAL may have a bias towards the LLMgenerated summaries because the model could share the same concept of evaluation criteria during generation and evaluation.\n\nOur work should be considered as a preliminary study on this issue, and more research is needed to fully understand the behavior of LLM-based evaluators to reduce its inherent bias towards LLMgenerated text. We highlight this concern in the context that LLM-based evaluators may lead to self-reinforcement of LLMs if the evaluation score is used as a reward signal for further tuning. And this could result in the over-fitting of the LLMs to their own evaluation criteria, rather than the true evaluation criteria of the NLG tasks."
                        ],
                        "paper": {
                            "corpus_id": 257804696,
                            "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                            "authors": [
                                {
                                    "authorId": "2152797401",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "3310951",
                                    "name": "Dan Iter"
                                },
                                {
                                    "authorId": "2110197273",
                                    "name": "Yichong Xu"
                                },
                                {
                                    "authorId": "2146294891",
                                    "name": "Shuo Wang"
                                },
                                {
                                    "authorId": "8233965",
                                    "name": "Ruochen Xu"
                                },
                                {
                                    "authorId": "8652308",
                                    "name": "Chenguang Zhu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1211
                        },
                        "score": 0.55078125
                    },
                    {
                        "id": "(Shi et al., 2023)",
                        "snippets": [
                            "When using GPT-4 to rate the performance of different models against the reference summary, there are a number of issues. First, there exists an inconsistency problem whereby there is no guarantee that an LLM will produce the identical output for the same input every time. A good practice to fix the problem is to set temperature = 0; the temperature parameter of the model controls the randomness of the text generated, with 0 being deterministic. Second, we find a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt. To mitigate such recency bias, we recommend reporting the mean score over both orders. Third, we observe from Figure 4 that GPT-4 assigns significantly higher scores to its own outputs compared to human ratings (64% vs. 43%), which represents the propensity of the model to favor its own suggestions. Future work should investigate the existence of potential biases in GPT-4 evaluation as well as possible mitigation strategies.\n\nDespite the potential of GPT-4 for automating evaluation, it is not as flexible as humans, particularly in situations where specific definitions of summary quality are not provided. The performance of GPT-4 hence may vary due to the lack of clarity in evaluation standards. GPT-4 aligns with the human evaluation, concluding that it generates better summaries than ChatGPT. However, GPT-4 encounters difficulty distinguishing scenarios where both models produce equally good summaries. In cases where human assessors perceive the summaries as equally good, GPT-4 may favor a summary that includes relevant contextual information does not present in the other, such as \"explicitly linking it to a high-fat, high-calorie meal\" or \"mention the lack of clinically significant differences\". It also may penalize certain aspects based on subjective criteria, such as \"less concise\"."
                        ],
                        "paper": {
                            "corpus_id": 259274988,
                            "title": "Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting",
                            "authors": [
                                {
                                    "authorId": "2118898475",
                                    "name": "Yiwen Shi"
                                },
                                {
                                    "authorId": "2061530524",
                                    "name": "Ping Ren"
                                },
                                {
                                    "authorId": "2152438838",
                                    "name": "Jing Wang"
                                },
                                {
                                    "authorId": "2220800269",
                                    "name": "Biao Han"
                                },
                                {
                                    "authorId": "65986913",
                                    "name": "Taha ValizadehAslani"
                                },
                                {
                                    "authorId": "1997966680",
                                    "name": "Felix Agbavor"
                                },
                                {
                                    "authorId": "2153915340",
                                    "name": "Yi Zhang"
                                },
                                {
                                    "authorId": "144642598",
                                    "name": "Meng Hu"
                                },
                                {
                                    "authorId": "144010790",
                                    "name": "Liang Zhao"
                                },
                                {
                                    "authorId": "2220924999",
                                    "name": "Hualou Liang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Journal of Biomedical Informatics",
                            "n_citations": 18
                        },
                        "score": 0.54150390625
                    },
                    {
                        "id": "(Chu et al., 2024)",
                        "snippets": [
                            "Although several works have demonstrated that GPT-4 has decent evaluation capabilities [19]26], we found that GPT-4, so as other LLMs, often prefers responses of LLMs from its own series (i.e., the GPT models) over other LLMs despite of the actual quality of the responses. In other words, if we use GPT-4 as the evaluator, its inherent bias may make it difficult, if possible, to develop an LLM outside the GPT family that outperforms GPT-4."
                        ],
                        "paper": {
                            "corpus_id": 267311508,
                            "title": "PRE: A Peer Review Based Large Language Model Evaluator",
                            "authors": [
                                {
                                    "authorId": "1381470019",
                                    "name": "Zhumin Chu"
                                },
                                {
                                    "authorId": "2256982003",
                                    "name": "Qingyao Ai"
                                },
                                {
                                    "authorId": "2275628230",
                                    "name": "Yiteng Tu"
                                },
                                {
                                    "authorId": "2108590438",
                                    "name": "Haitao Li"
                                },
                                {
                                    "authorId": "2260835922",
                                    "name": "Yiqun Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 21
                        },
                        "score": 0.68798828125
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "From the results, we can observe a significant performance gap between GPT-4 and Auto-PRE, with GPT-4's rate much higher than that of other methods. This suggests that GPT-4's preference for answers from GPT-3.5-turbo compromises its performance and reliability. Conversely, Auto-PRE enhances overall performance and reliability by collaborating with various types of LLMs."
                        ],
                        "paper": {
                            "corpus_id": 273374964,
                            "title": "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation",
                            "authors": [
                                {
                                    "authorId": "2326124996",
                                    "name": "Junjie Chen"
                                },
                                {
                                    "authorId": "2147219374",
                                    "name": "Weihang Su"
                                },
                                {
                                    "authorId": "1381470019",
                                    "name": "Zhumin Chu"
                                },
                                {
                                    "authorId": "2108590438",
                                    "name": "Haitao Li"
                                },
                                {
                                    "authorId": "2326111223",
                                    "name": "Qinyao Ai"
                                },
                                {
                                    "authorId": "1783406",
                                    "name": "Yiqun Liu"
                                },
                                {
                                    "authorId": "2258690221",
                                    "name": "Min Zhang"
                                },
                                {
                                    "authorId": "8093158",
                                    "name": "Shaoping Ma"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.5966796875
                    },
                    {
                        "id": "(Wu et al., 2023)",
                        "snippets": [
                            "This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4. By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study."
                        ],
                        "paper": {
                            "corpus_id": 259360998,
                            "title": "Style Over Substance: Evaluation Biases for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2145209409",
                                    "name": "Minghao Wu"
                                },
                                {
                                    "authorId": "8129718",
                                    "name": "Alham Fikri Aji"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 47
                        },
                        "score": 0.64013671875
                    },
                    {
                        "id": "(Chai et al., 2024)",
                        "snippets": [
                            "To address the limitations of automatic evaluation metrics, we incorporate an LLM evaluation to assess the quality of the generated scientific hypotheses. Specifically, we employ Claude-3.5 instead of the more mainstream GPT-4 to mitigate potential self-enhancement bias, which occurs when a model is evaluated using its own framework (Xu et al., 2024b)."
                        ],
                        "paper": {
                            "corpus_id": 273901331,
                            "title": "Exploring Scientific Hypothesis Generation with Mamba",
                            "authors": [
                                {
                                    "authorId": "2329735903",
                                    "name": "Miaosen Chai"
                                },
                                {
                                    "authorId": "2329737480",
                                    "name": "Emily Herron"
                                },
                                {
                                    "authorId": "2329736173",
                                    "name": "Erick Cervantes"
                                },
                                {
                                    "authorId": "2329737470",
                                    "name": "Tirthankar Ghosal"
                                }
                            ],
                            "year": 2024,
                            "venue": "NLP4SCIENCE",
                            "n_citations": 5
                        },
                        "score": 0.6591796875
                    },
                    {
                        "id": "(Gao et al., 2023)",
                        "snippets": [
                            "We compare both human and GPT-based evaluation scores and provide in-depth analysis. While our findings suggest that GPT-created surveys are more contemporary and accessible than human-authored ones, certain limitations were observed. Notably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors. At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation."
                        ],
                        "paper": {
                            "corpus_id": 267783118,
                            "title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
                            "authors": [
                                {
                                    "authorId": "2232906828",
                                    "name": "Fan Gao"
                                },
                                {
                                    "authorId": "48579520",
                                    "name": "Hang Jiang"
                                },
                                {
                                    "authorId": "2287835229",
                                    "name": "Rui Yang"
                                },
                                {
                                    "authorId": "2153554138",
                                    "name": "Qingcheng Zeng"
                                },
                                {
                                    "authorId": "2285824559",
                                    "name": "Jinghui Lu"
                                },
                                {
                                    "authorId": "2285108151",
                                    "name": "Moritz Blum"
                                },
                                {
                                    "authorId": "1585849884",
                                    "name": "Dairui Liu"
                                },
                                {
                                    "authorId": "2106009217",
                                    "name": "Tianwei She"
                                },
                                {
                                    "authorId": "2285289624",
                                    "name": "Yuang Jiang"
                                },
                                {
                                    "authorId": "2275053812",
                                    "name": "Irene Li"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 9
                        },
                        "score": 0.5322265625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Position and Ordering Biases in LLM Evaluation",
                "tldr": "Evaluation using LLMs like GPT-4 shows significant position and ordering biases where models systematically favor outputs based on their position in the prompt rather than their quality. These biases are particularly pronounced when comparing similar-quality content, requiring specific mitigation strategies such as position swapping and averaging results across different orderings. (3 sources)",
                "text": "\nPosition and ordering biases represent a critical challenge in LLM-based evaluation of scientific writing that extends beyond the self-preference bias previously discussed. These biases occur when an LLM evaluator's judgment is systematically influenced by the position of the content being evaluated rather than its actual quality. Shi et al. identified \"a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt\" <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. This recency bias can significantly distort evaluation results, particularly when comparing outputs of similar quality.\n\nThe direction and magnitude of position bias may vary depending on the evaluation task and protocol. Huang et al. observed that \"when GPT-4 compared pairs of LLM-generated summaries to evaluate faithfulness and coverage, a strong position bias surfaced, favoring the second entry\" <Paper corpusId=\"262045067\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. They further noted that this bias was \"particularly pronounced when assessing similar-quality summaries,\" suggesting that position effects become more influential when qualitative differences between outputs are subtle <Paper corpusId=\"262045067\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. Interestingly, they found that these position effects can vary across different evaluation tasks, indicating that the nature and direction of the bias is not consistent and may depend on the specific evaluation context.\n\nTo address these position and ordering biases, researchers have developed several mitigation strategies. Shi et al. recommend \"reporting the mean score over both orders\" to counterbalance the recency bias <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. Taking this approach a step further, Chang et al. implemented a rigorous methodology where \"each pair is evaluated twice with the positions of the two TLDRs swapped\" and \"a method is considered the winner for a sample only if both evaluations favored the same TLDR\" <Paper corpusId=\"271769421\" paperTitle=\"(Chang et al., 2024)\" isShortName></Paper>. This position-swapping protocol helps identify genuinely superior outputs by ensuring that the preference remains consistent regardless of presentation order.\n\nPosition bias can significantly confound evaluation results, especially when GPT-4 \"may favor a summary that includes relevant contextual information not present in the other\" or \"penalize certain aspects based on subjective criteria\" rather than assessing overall quality <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. This suggests that position effects interact with other evaluation biases, potentially amplifying inconsistencies in automated assessment of academic writing.",
                "citations": [
                    {
                        "id": "(Shi et al., 2023)",
                        "snippets": [
                            "When using GPT-4 to rate the performance of different models against the reference summary, there are a number of issues. First, there exists an inconsistency problem whereby there is no guarantee that an LLM will produce the identical output for the same input every time. A good practice to fix the problem is to set temperature = 0; the temperature parameter of the model controls the randomness of the text generated, with 0 being deterministic. Second, we find a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt. To mitigate such recency bias, we recommend reporting the mean score over both orders. Third, we observe from Figure 4 that GPT-4 assigns significantly higher scores to its own outputs compared to human ratings (64% vs. 43%), which represents the propensity of the model to favor its own suggestions. Future work should investigate the existence of potential biases in GPT-4 evaluation as well as possible mitigation strategies.\n\nDespite the potential of GPT-4 for automating evaluation, it is not as flexible as humans, particularly in situations where specific definitions of summary quality are not provided. The performance of GPT-4 hence may vary due to the lack of clarity in evaluation standards. GPT-4 aligns with the human evaluation, concluding that it generates better summaries than ChatGPT. However, GPT-4 encounters difficulty distinguishing scenarios where both models produce equally good summaries. In cases where human assessors perceive the summaries as equally good, GPT-4 may favor a summary that includes relevant contextual information does not present in the other, such as \"explicitly linking it to a high-fat, high-calorie meal\" or \"mention the lack of clinically significant differences\". It also may penalize certain aspects based on subjective criteria, such as \"less concise\"."
                        ],
                        "paper": {
                            "corpus_id": 259274988,
                            "title": "Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting",
                            "authors": [
                                {
                                    "authorId": "2118898475",
                                    "name": "Yiwen Shi"
                                },
                                {
                                    "authorId": "2061530524",
                                    "name": "Ping Ren"
                                },
                                {
                                    "authorId": "2152438838",
                                    "name": "Jing Wang"
                                },
                                {
                                    "authorId": "2220800269",
                                    "name": "Biao Han"
                                },
                                {
                                    "authorId": "65986913",
                                    "name": "Taha ValizadehAslani"
                                },
                                {
                                    "authorId": "1997966680",
                                    "name": "Felix Agbavor"
                                },
                                {
                                    "authorId": "2153915340",
                                    "name": "Yi Zhang"
                                },
                                {
                                    "authorId": "144642598",
                                    "name": "Meng Hu"
                                },
                                {
                                    "authorId": "144010790",
                                    "name": "Liang Zhao"
                                },
                                {
                                    "authorId": "2220924999",
                                    "name": "Hualou Liang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Journal of Biomedical Informatics",
                            "n_citations": 18
                        },
                        "score": 0.54150390625
                    },
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "Position bias is most relevant to the pairwise comparison protocol. While previous work has shown that GPT-4 does exhibit position bias when used to assess text quality in conversational-focused tasks (Wang et al., 2023;Zheng et al., 2023), none of the prior studies have investigated whether such bias is also observed when evaluating faithfulness or coverage. To analyze position bias, we task GPT-4 with assessing a pair of summaries generated by two LLMs on which one is better, and then swap the positions of these two summaries and query GPT-4 again. We compute the percentage of times GPT-4 prefers the first or second summaries. \n\nWhen GPT-4 compared pairs of LLM-generated summaries to evaluate faithfulness and coverage, a strong position bias surfaced, favoring the second entry (Table 3). Position bias was particularly pronounced when assessing similar-quality summaries (see Figure 23a). Hence, we deduce that GPT-4 is unreliable when utilized as a pairwise evaluator in the MDDS task with respect to faithfulness and coverage. Interestingly, this outcome contradicts Zheng et al. (2023), implying that the position of bias for LLM-based evaluators could vary across different tasks."
                        ],
                        "paper": {
                            "corpus_id": 262045067,
                            "title": "Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles",
                            "authors": [
                                {
                                    "authorId": "1420116116",
                                    "name": "Kung-Hsiang Huang"
                                },
                                {
                                    "authorId": "46180754",
                                    "name": "Philippe Laban"
                                },
                                {
                                    "authorId": "22281632",
                                    "name": "A. R. Fabbri"
                                },
                                {
                                    "authorId": "3466801",
                                    "name": "Prafulla Kumar Choubey"
                                },
                                {
                                    "authorId": "2708940",
                                    "name": "Shafiq R. Joty"
                                },
                                {
                                    "authorId": "2054594326",
                                    "name": "Caiming Xiong"
                                },
                                {
                                    "authorId": "30340989",
                                    "name": "Chien-Sheng Wu"
                                }
                            ],
                            "year": 2023,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 32
                        },
                        "score": 0.541015625
                    },
                    {
                        "id": "(Chang et al., 2024)",
                        "snippets": [
                            "To this end, we employ GPT-4 (version gpt-4-0613), which is currently recognized as one of the most advanced LLMs, to assess the quality of TL-DRs generated by our method compared to a baseline method",
                            "To mitigate position bias (Zheng et al., 2023), each pair is evaluated twice with the positions of the two TLDRs swapped. A method is considered the winner for a sample only if both evaluations favored the same TLDR. In cases of conflicting results or ties, the sample is marked as a tie."
                        ],
                        "paper": {
                            "corpus_id": 271769421,
                            "title": "Guiding Large Language Models via External Attention Prompting for Scientific Extreme Summarization",
                            "authors": [
                                {
                                    "authorId": "2315438513",
                                    "name": "Yuan Chang"
                                },
                                {
                                    "authorId": "2315367785",
                                    "name": "Ziyue Li"
                                },
                                {
                                    "authorId": "2315309245",
                                    "name": "Xiaoqiu Le"
                                }
                            ],
                            "year": 2024,
                            "venue": "SDP",
                            "n_citations": 1
                        },
                        "score": 0.8828125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Alternative LLM Evaluators: Considerations",
                "tldr": "Using alternative LLMs as evaluators can mitigate self-preference bias, with researchers employing models like Claude to evaluate GPT outputs and creating hierarchies based on model capabilities. Specialized reward models trained specifically for evaluation tasks often outperform even the most advanced general-purpose LLMs like GPT-4 and Gemini when it comes to evaluation quality. (4 sources)",
                "text": "\nWhen considering alternatives to using GPT-4 to evaluate its own outputs, researchers have explored several approaches to address bias concerns. One strategy involves employing different LLM families entirely to mitigate self-preference bias. Chai et al. specifically chose \"Claude-3.5 instead of the more mainstream GPT-4 to mitigate potential self-enhancement bias, which occurs when a model is evaluated using its own framework\" <Paper corpusId=\"273901331\" paperTitle=\"(Chai et al., 2024)\" isShortName></Paper>. This cross-model evaluation approach offers a promising way to obtain more objective assessments of scientific writing generated by GPT models.\n\nAnother consideration involves creating evaluation hierarchies based on model capabilities. Lu et al. explored this approach by comparing \"three types of reviews: the human-written review, the GPT-4-generated review, and the GPT-3.5-generated review,\" noting that \"GPT-4 is commonly considered a stronger AI than GPT-3.5, and both models are considered worse than human expert reviewers\" <Paper corpusId=\"270045144\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. This capability-based hierarchy allows researchers to simulate different quality levels of evaluation, potentially providing more nuanced assessment frameworks.\n\nMultiple evaluator approaches have also proven effective for achieving more balanced assessments. Wu et al. addressed evaluation bias by incorporating \"multiple LLM judges\" including models from Anthropic \"in addition to GPT-4\" to \"establish a more comprehensive and unbiased assessment\" <Paper corpusId=\"259360998\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>. This ensemble approach helps minimize the impact of any single model's biases, creating a more robust evaluation framework.\n\nBeyond using alternative general-purpose LLMs, specialized evaluation models have shown superior performance in specific contexts. Weng et al. noted that \"even the most advanced models, such as GPT-4 and Gemini, still lag behind reward models specifically trained for the task, as seen in RewardBench\" <Paper corpusId=\"273811997\" paperTitle=\"(Weng et al., 2024)\" isShortName></Paper>. This finding suggests that purpose-built evaluation models, despite having less general capability than frontier models like GPT-4, may provide more reliable assessments for scientific writing tasks due to their specialized training.",
                "citations": [
                    {
                        "id": "(Chai et al., 2024)",
                        "snippets": [
                            "To address the limitations of automatic evaluation metrics, we incorporate an LLM evaluation to assess the quality of the generated scientific hypotheses. Specifically, we employ Claude-3.5 instead of the more mainstream GPT-4 to mitigate potential self-enhancement bias, which occurs when a model is evaluated using its own framework (Xu et al., 2024b)."
                        ],
                        "paper": {
                            "corpus_id": 273901331,
                            "title": "Exploring Scientific Hypothesis Generation with Mamba",
                            "authors": [
                                {
                                    "authorId": "2329735903",
                                    "name": "Miaosen Chai"
                                },
                                {
                                    "authorId": "2329737480",
                                    "name": "Emily Herron"
                                },
                                {
                                    "authorId": "2329736173",
                                    "name": "Erick Cervantes"
                                },
                                {
                                    "authorId": "2329737470",
                                    "name": "Tirthankar Ghosal"
                                }
                            ],
                            "year": 2024,
                            "venue": "NLP4SCIENCE",
                            "n_citations": 5
                        },
                        "score": 0.6591796875
                    },
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "We compare three types of reviews: the human-written review, the GPT-4-generated review, and the GPT-3.5-generated review. Given that GPT-4 is commonly considered a stronger AI than GPT-3.5, and both models are considered worse than human expert reviewers, we hypothesize that we can utilize the LLM-generated reviews to simulate low-quality human reviews, thereby simulating three levels of quality."
                        ],
                        "paper": {
                            "corpus_id": 270045144,
                            "title": "Eliciting Informative Text Evaluations with Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2140025107",
                                    "name": "Yuxuan Lu"
                                },
                                {
                                    "authorId": "2110923845",
                                    "name": "Shengwei Xu"
                                },
                                {
                                    "authorId": "2120311160",
                                    "name": "Yichi Zhang"
                                },
                                {
                                    "authorId": "2273919664",
                                    "name": "Yuqing Kong"
                                },
                                {
                                    "authorId": "2268317075",
                                    "name": "Grant Schoenebeck"
                                }
                            ],
                            "year": 2024,
                            "venue": "ACM Conference on Economics and Computation",
                            "n_citations": 7
                        },
                        "score": 0.677734375
                    },
                    {
                        "id": "(Wu et al., 2023)",
                        "snippets": [
                            "This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4. By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study."
                        ],
                        "paper": {
                            "corpus_id": 259360998,
                            "title": "Style Over Substance: Evaluation Biases for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2145209409",
                                    "name": "Minghao Wu"
                                },
                                {
                                    "authorId": "8129718",
                                    "name": "Alham Fikri Aji"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 47
                        },
                        "score": 0.64013671875
                    },
                    {
                        "id": "(Weng et al., 2024)",
                        "snippets": [
                            "when LLMs act as judges, even the most advanced models, such as GPT-4 (Achiam et al., 2023) and Gemini (Reid et al., 2024), still lag behind reward models specifically trained for the task, as seen in RewardBench (Lambert et al., 2024). This gap highlights the challenge of achieving human-level judgment and reasoning in AI-driven peer reviews. In contrast, we train a Generative Reward Model (Zhang et al., 2024) to simulate a comprehensive peer review."
                        ],
                        "paper": {
                            "corpus_id": 273811997,
                            "title": "CycleResearcher: Improving Automated Research via Automated Review",
                            "authors": [
                                {
                                    "authorId": "2142839441",
                                    "name": "Yixuan Weng"
                                },
                                {
                                    "authorId": "2316827669",
                                    "name": "Minjun Zhu"
                                },
                                {
                                    "authorId": "1993226927",
                                    "name": "Guangsheng Bao"
                                },
                                {
                                    "authorId": "2329135444",
                                    "name": "Hongbo Zhang"
                                },
                                {
                                    "authorId": "2273553706",
                                    "name": "Jindong Wang"
                                },
                                {
                                    "authorId": "2325943212",
                                    "name": "Yue Zhang"
                                },
                                {
                                    "authorId": "2145500840",
                                    "name": "Linyi Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 25
                        },
                        "score": 0.75634765625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Best Practices for LLM-Based Evaluation",
                "tldr": "Effective LLM-based evaluation of scientific writing requires specific implementation strategies including temperature setting, position randomization, and multiple evaluations to mitigate biases. Techniques such as chain-of-thought reasoning, detailed prompting, and cross-model evaluation help achieve more reliable and consistent assessments that better align with human judgment. (10 sources)",
                "text": "\nBased on the research literature, the following best practices emerge for using LLMs to evaluate scientific academic writing:\n\n1. **Set deterministic generation parameters**: To address inconsistency issues in LLM evaluations, set temperature = 0 to ensure deterministic outputs, making evaluation results more reproducible. <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>\n\n2. **Mitigate position and ordering bias**: Implement position swapping by evaluating each comparison twice with the positions of outputs reversed, and report the mean score across both orderings. Only consider a winner when both evaluations favor the same output. <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"271769421\" paperTitle=\"(Chang et al., 2024)\" isShortName></Paper>\n\n3. **Use comparative rather than absolute evaluations**: The LLMCompare protocol, which compares two candidate outputs and selects the better one, has been shown to be the most effective evaluation approach when coupled with GPT-4. <Paper corpusId=\"270218703\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259129398\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>\n\n4. **Implement chain-of-thought reasoning**: Structure evaluations to include detailed intermediate steps that guide the LLM through a systematic assessment process, improving correlation with human evaluations. <Paper corpusId=\"271310142\" paperTitle=\"(Pereira et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246411621\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>\n\n5. **Apply self-consistency techniques**: Generate multiple evaluation samples with different decoding parameters and average the scores across samples to mitigate score distribution issues and reduce variability in assessment outcomes. <Paper corpusId=\"271310142\" paperTitle=\"(Pereira et al., 2024)\" isShortName></Paper>\n\n6. **Conduct multiple evaluation runs**: Similar to cross-validation, run the same evaluation multiple times on identical data samples and average the results to ensure assessment consistency. <Paper corpusId=\"272968899\" paperTitle=\"(Giglou et al., 2024)\" isShortName></Paper>\n\n7. **Provide detailed evaluation criteria**: Create comprehensive prompts with explicit evaluation criteria to reduce ambiguity and improve consistency across evaluation runs. <Paper corpusId=\"272968899\" paperTitle=\"(Giglou et al., 2024)\" isShortName></Paper>\n\n8. **Recognize task-specific limitations**: Acknowledge that while LLMs perform well in certain evaluation tasks (like identifying errors or verifying checklists), they may not be sufficient for comprehensive evaluations of scientific papers. <Paper corpusId=\"276296422\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"263608784\" paperTitle=\"(Liang et al., 2023)\" isShortName></Paper>\n\n9. **Be aware of model limitations with similar-quality content**: LLMs like GPT-4 may struggle to differentiate between outputs of similar quality, potentially fixating on minor subjective differences rather than substantive quality distinctions. <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>\n\n10. **Utilize multiple LLM judges**: Employ different LLMs as evaluators to establish a more comprehensive and unbiased assessment framework that mitigates individual model biases. <Paper corpusId=\"271544398\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259129398\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Shi et al., 2023)",
                        "snippets": [
                            "When using GPT-4 to rate the performance of different models against the reference summary, there are a number of issues. First, there exists an inconsistency problem whereby there is no guarantee that an LLM will produce the identical output for the same input every time. A good practice to fix the problem is to set temperature = 0; the temperature parameter of the model controls the randomness of the text generated, with 0 being deterministic. Second, we find a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt. To mitigate such recency bias, we recommend reporting the mean score over both orders. Third, we observe from Figure 4 that GPT-4 assigns significantly higher scores to its own outputs compared to human ratings (64% vs. 43%), which represents the propensity of the model to favor its own suggestions. Future work should investigate the existence of potential biases in GPT-4 evaluation as well as possible mitigation strategies.\n\nDespite the potential of GPT-4 for automating evaluation, it is not as flexible as humans, particularly in situations where specific definitions of summary quality are not provided. The performance of GPT-4 hence may vary due to the lack of clarity in evaluation standards. GPT-4 aligns with the human evaluation, concluding that it generates better summaries than ChatGPT. However, GPT-4 encounters difficulty distinguishing scenarios where both models produce equally good summaries. In cases where human assessors perceive the summaries as equally good, GPT-4 may favor a summary that includes relevant contextual information does not present in the other, such as \"explicitly linking it to a high-fat, high-calorie meal\" or \"mention the lack of clinically significant differences\". It also may penalize certain aspects based on subjective criteria, such as \"less concise\"."
                        ],
                        "paper": {
                            "corpus_id": 259274988,
                            "title": "Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting",
                            "authors": [
                                {
                                    "authorId": "2118898475",
                                    "name": "Yiwen Shi"
                                },
                                {
                                    "authorId": "2061530524",
                                    "name": "Ping Ren"
                                },
                                {
                                    "authorId": "2152438838",
                                    "name": "Jing Wang"
                                },
                                {
                                    "authorId": "2220800269",
                                    "name": "Biao Han"
                                },
                                {
                                    "authorId": "65986913",
                                    "name": "Taha ValizadehAslani"
                                },
                                {
                                    "authorId": "1997966680",
                                    "name": "Felix Agbavor"
                                },
                                {
                                    "authorId": "2153915340",
                                    "name": "Yi Zhang"
                                },
                                {
                                    "authorId": "144642598",
                                    "name": "Meng Hu"
                                },
                                {
                                    "authorId": "144010790",
                                    "name": "Liang Zhao"
                                },
                                {
                                    "authorId": "2220924999",
                                    "name": "Hualou Liang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Journal of Biomedical Informatics",
                            "n_citations": 18
                        },
                        "score": 0.54150390625
                    },
                    {
                        "id": "(Chang et al., 2024)",
                        "snippets": [
                            "To this end, we employ GPT-4 (version gpt-4-0613), which is currently recognized as one of the most advanced LLMs, to assess the quality of TL-DRs generated by our method compared to a baseline method",
                            "To mitigate position bias (Zheng et al., 2023), each pair is evaluated twice with the positions of the two TLDRs swapped. A method is considered the winner for a sample only if both evaluations favored the same TLDR. In cases of conflicting results or ties, the sample is marked as a tie."
                        ],
                        "paper": {
                            "corpus_id": 271769421,
                            "title": "Guiding Large Language Models via External Attention Prompting for Scientific Extreme Summarization",
                            "authors": [
                                {
                                    "authorId": "2315438513",
                                    "name": "Yuan Chang"
                                },
                                {
                                    "authorId": "2315367785",
                                    "name": "Ziyue Li"
                                },
                                {
                                    "authorId": "2315309245",
                                    "name": "Xiaoqiu Le"
                                }
                            ],
                            "year": 2024,
                            "venue": "SDP",
                            "n_citations": 1
                        },
                        "score": 0.8828125
                    },
                    {
                        "id": "(Sun et al., 2024)",
                        "snippets": [
                            "We use the LLMCompare as our evaluation protocol, which compares two candidate outputs and then selects the better one (Zheng et al., 2023)Wang et al., 2023).This is because LLMCompare coupled with GPT-4 is the best evaluation protocol, as mentioned in Liu et al. (2023).The evaluation prompts are shown in Appendix A."
                        ],
                        "paper": {
                            "corpus_id": 270218703,
                            "title": "Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization",
                            "authors": [
                                {
                                    "authorId": "2256995981",
                                    "name": "Shichao Sun"
                                },
                                {
                                    "authorId": "2273661632",
                                    "name": "Ruifeng Yuan"
                                },
                                {
                                    "authorId": "2314396",
                                    "name": "Ziqiang Cao"
                                },
                                {
                                    "authorId": "2237591981",
                                    "name": "Wenjie Li"
                                },
                                {
                                    "authorId": "2256991660",
                                    "name": "Pengfei Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 20
                        },
                        "score": 0.5498046875
                    },
                    {
                        "id": "(Zheng et al., 2023)",
                        "snippets": [
                            "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge."
                        ],
                        "paper": {
                            "corpus_id": 259129398,
                            "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
                            "authors": [
                                {
                                    "authorId": "2149970173",
                                    "name": "Lianmin Zheng"
                                },
                                {
                                    "authorId": "2537924",
                                    "name": "Wei-Lin Chiang"
                                },
                                {
                                    "authorId": "2209360681",
                                    "name": "Ying Sheng"
                                },
                                {
                                    "authorId": "92721493",
                                    "name": "Siyuan Zhuang"
                                },
                                {
                                    "authorId": "1390573666",
                                    "name": "Zhanghao Wu"
                                },
                                {
                                    "authorId": "2152482391",
                                    "name": "Yonghao Zhuang"
                                },
                                {
                                    "authorId": "143872641",
                                    "name": "Zi Lin"
                                },
                                {
                                    "authorId": "2141335450",
                                    "name": "Zhuohan Li"
                                },
                                {
                                    "authorId": "2117961435",
                                    "name": "Dacheng Li"
                                },
                                {
                                    "authorId": "143977260",
                                    "name": "E. Xing"
                                },
                                {
                                    "authorId": "145140331",
                                    "name": "Haotong Zhang"
                                },
                                {
                                    "authorId": "49988044",
                                    "name": "Joseph E. Gonzalez"
                                },
                                {
                                    "authorId": "2055174324",
                                    "name": "Ion Stoica"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 4439
                        },
                        "score": 0
                    },
                    {
                        "id": "(Pereira et al., 2024)",
                        "snippets": [
                            "G-EVAL is another recent approach that leverages the capabilities of LLMs, specifically GPT-4, to improve NLG evaluation. Proposed by Liu et al. (2023) [Liu et al. 2023], G-EVAL introduces a chain-of-thought (CoT) (Wei et al., 2022) paradigm where the evaluation process is guided by detailed intermediate steps generated by the LLM. This method has shown improvements in correlation with human evaluations, particularly in tasks such as text summarization and dialogue generation. To address issues with score distribution and variability, G-EVAL employs a self-consistency strategy. Specifically, it generates multiple samples (n = 20) using different decoding parameters and averages the evaluation scores across these samples. This approach helps mitigate two key problems: the dominance of a single score (such as 3 on a 1-5 scale) and the tendency of LLMs to output integer scores, even when decimal values are requested."
                        ],
                        "paper": {
                            "corpus_id": 271310142,
                            "title": "Check-Eval: A Checklist-based Approach for Evaluating Text Quality",
                            "authors": [
                                {
                                    "authorId": "2257137831",
                                    "name": "Jayr Pereira"
                                },
                                {
                                    "authorId": "2256889299",
                                    "name": "R.A. Lotufo"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.65576171875
                    },
                    {
                        "id": "(Wei et al., 2022)",
                        "snippets": [
                            "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."
                        ],
                        "paper": {
                            "corpus_id": 246411621,
                            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "119640649",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "1524732527",
                                    "name": "Xuezhi Wang"
                                },
                                {
                                    "authorId": "1714772",
                                    "name": "Dale Schuurmans"
                                },
                                {
                                    "authorId": "40377863",
                                    "name": "Maarten Bosma"
                                },
                                {
                                    "authorId": "2226805",
                                    "name": "Ed H. Chi"
                                },
                                {
                                    "authorId": "144956443",
                                    "name": "F. Xia"
                                },
                                {
                                    "authorId": "1998340269",
                                    "name": "Quoc Le"
                                },
                                {
                                    "authorId": "65855107",
                                    "name": "Denny Zhou"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 9683
                        },
                        "score": 0
                    },
                    {
                        "id": "(Giglou et al., 2024)",
                        "snippets": [
                            "Prior work has demonstrated the effectiveness of LLMs as automatic evaluators for scientific synthesis (Bai et al., 2023)17,19], arguing that they are becoming both the tools and the standards for language assessment. Thus, for the evaluation task, we utilized GPT-4-Turbo, the most advanced LLM available at the time of writing",
                            "Finally, similar to 3-fold cross-validation, the GPT-4 synthesis evaluator was run thrice on the same data samples to ensure consistent assessment. The resulting scores, which did not show significant divergence largely due to our detailed prompt specification of the expected evaluation task leaving little room for ambiguity in interpretation, were averaged and reported."
                        ],
                        "paper": {
                            "corpus_id": 272968899,
                            "title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis",
                            "authors": [
                                {
                                    "authorId": "2360502238",
                                    "name": "Hamed Babaei Giglou"
                                },
                                {
                                    "authorId": "2261403432",
                                    "name": "Jennifer D'Souza"
                                },
                                {
                                    "authorId": "2268415251",
                                    "name": "S. Auer"
                                }
                            ],
                            "year": 2024,
                            "venue": "ACM/IEEE Joint Conference on Digital Libraries",
                            "n_citations": 5
                        },
                        "score": 0.837890625
                    },
                    {
                        "id": "(Wu et al., 2025)",
                        "snippets": [
                            "Liang et al. (Liang et al., 2023) conducted a large-scale empirical analysis to assess comments generated by GPT-4: they tag several metrics and investigated user satisfaction to measure comment quality. They found that while LLM-generated reviews were helpful, they might be non-generic and tended to focus on certain aspects of scientific feedback. Liu and Shah (Liu and Shah, 2023) validated the utility of LLMs across three tasks: identifying errors, verifying checklists, and choosing the \"better\" pa-per. They concluded that LLMs serve well as review assistants for specific reviewing tasks; however, they are not yet sufficient for conducting comprehensive evaluations of papers. Mike Thelwall (Thelwall, 2024) used GPT-4 to evaluate the quality of journal articles on a paper assessment dataset to test its effectiveness. The results indicated that GPT-4 appears to be insufficiently accurate for any formal or informal research quality assessment tasks."
                        ],
                        "paper": {
                            "corpus_id": 276296422,
                            "title": "SC4ANM: Identifying optimal section combinations for automated novelty prediction in academic papers",
                            "authors": [
                                {
                                    "authorId": "2307769002",
                                    "name": "Wenqing Wu"
                                },
                                {
                                    "authorId": "2267229283",
                                    "name": "Chengzhi Zhang"
                                },
                                {
                                    "authorId": "2325373803",
                                    "name": "Tong Bao"
                                },
                                {
                                    "authorId": "2284551410",
                                    "name": "Yi Zhao"
                                }
                            ],
                            "year": 2025,
                            "venue": "Expert systems with applications",
                            "n_citations": 1
                        },
                        "score": 0.6328125
                    },
                    {
                        "id": "(Liang et al., 2023)",
                        "snippets": [
                            "Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations."
                        ],
                        "paper": {
                            "corpus_id": 263608784,
                            "title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
                            "authors": [
                                {
                                    "authorId": "151253861",
                                    "name": "Weixin Liang"
                                },
                                {
                                    "authorId": "49889860",
                                    "name": "Yuhui Zhang"
                                },
                                {
                                    "authorId": "2256613375",
                                    "name": "Hancheng Cao"
                                },
                                {
                                    "authorId": "2253999923",
                                    "name": "Binglu Wang"
                                },
                                {
                                    "authorId": "2253469362",
                                    "name": "Daisy Ding"
                                },
                                {
                                    "authorId": "2216295251",
                                    "name": "Xinyu Yang"
                                },
                                {
                                    "authorId": "52224284",
                                    "name": "Kailas Vodrahalli"
                                },
                                {
                                    "authorId": "2257314770",
                                    "name": "Siyu He"
                                },
                                {
                                    "authorId": "2199559560",
                                    "name": "D. Smith"
                                },
                                {
                                    "authorId": "2256439343",
                                    "name": "Yian Yin"
                                },
                                {
                                    "authorId": "2251233257",
                                    "name": "Daniel A. McFarland"
                                },
                                {
                                    "authorId": "2244005287",
                                    "name": "James Zou"
                                }
                            ],
                            "year": 2023,
                            "venue": "NEJM AI",
                            "n_citations": 145
                        },
                        "score": 0.888671875
                    },
                    {
                        "id": "(Wu et al., 2024)",
                        "snippets": [
                            "Studies confirm the capability of LLMs to evaluate other LLMs, with verification abilities improving faster than generation quality [29]. GPT-4's evaluations exceeds 80% consistency with human reviewers (Zheng et al., 2023) and exceeds 85% alignment in pairwise comparisons (Bai et al., 2023), reaching nearly 100% agreement when performance differences are significant (Bai et al., 2023). Self-verification benefits LLM performance [32], though biases issues [33], such as position bias [34], length bias (Zheng et al., 2023), and self-bias [35], persist but can be reduced through proper design [36]."
                        ],
                        "paper": {
                            "corpus_id": 271544398,
                            "title": "Automated literature research and review-generation method based on large language models",
                            "authors": [
                                {
                                    "authorId": "2142349414",
                                    "name": "Shican Wu"
                                },
                                {
                                    "authorId": "2313884864",
                                    "name": "Xiao Ma"
                                },
                                {
                                    "authorId": "2313733635",
                                    "name": "Dehui Luo"
                                },
                                {
                                    "authorId": "2277565237",
                                    "name": "Lulu Li"
                                },
                                {
                                    "authorId": "2289200637",
                                    "name": "Xiangcheng Shi"
                                },
                                {
                                    "authorId": "2289200103",
                                    "name": "Xin Chang"
                                },
                                {
                                    "authorId": "2152842807",
                                    "name": "Xiaoyun Lin"
                                },
                                {
                                    "authorId": "2075340202",
                                    "name": "Ran Luo"
                                },
                                {
                                    "authorId": "48471690",
                                    "name": "Chunlei Pei"
                                },
                                {
                                    "authorId": "12144921",
                                    "name": "Zhijian Zhao"
                                },
                                {
                                    "authorId": "2258427381",
                                    "name": "Jinlong Gong"
                                }
                            ],
                            "year": 2024,
                            "venue": "National Science Review",
                            "n_citations": 0
                        },
                        "score": 0.63232421875
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.140664
    }
}