{
    "query": "How does sequence parallelism interact and integrate with other efficiency-focused methods, such as sparse attention and memory-efficient algorithms, for extending Transformer model context length in practice? What are the synergies and trade-offs among these approaches?",
    "user_id": "lib_user",
    "task_id": "9211efef-e9b3-450d-91b1-65762c12627f",
    "timestamp": "2025-06-23T22:31:10.743581",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 25,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.417546,
    "decomposed_query": {
        "rewritten_query": "How sequence parallelism interacts and integrates with sparse attention and memory-efficient algorithms for extending Transformer model context length, including synergies and trade-offs among these approaches.",
        "keyword_query": "sequence parallelism sparse attention memory-efficient algorithms Transformer context length synergies trade-offs",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010161,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 38,
            "citation_count": 102,
            "influential_citation_count": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.acl-long.134.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.13120, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153703322",
                    "name": "Shenggui Li"
                },
                {
                    "authorId": "2144332771",
                    "name": "Fuzhao Xue"
                },
                {
                    "authorId": "2180329661",
                    "name": "Yongbin Li"
                },
                {
                    "authorId": "144259229",
                    "name": "Yang You"
                }
            ],
            "abstract": "Transformer achieves promising results on various tasks. However, self-attention suffers from quadratic memory requirements with respect to the sequence length. Existing work focuses on reducing time and space complexity from an algorithm perspective. In this work, we propose sequence parallelism, a memory-efficient parallelism to solve this issue from system perspective instead. Our approach is compatible with most existing parallelisms (e.g., data, pipeline, and tensor parallelism), which means our sequence parallelism makes 4D parallelism possible. More importantly, we no longer require a single device to hold the whole sequence. Besides, using efficient attention with linear complexity, our sequence parallelism enables us to train transformer with infinite long sequence. Specifically, we split the input sequence into multiple chunks and feed each chunk into its corresponding device (i.e., GPU). To compute the attention output, we integrated ring-style communication with self-attention calculation and proposed Ring Self-Attention (RSA). Experiments show that sequence parallelism performs well when scaling with batch size and sequence length. Compared with tensor parallelism, our approach achieved 13.7\\times and 3.0\\times maximum batch size and sequence length respectively when scaling up to 64 NVIDIA P100 GPUs. With efficient attention, sequence can handle sequence with over 114K tokens, which is over 27\\times longer than existing efficient attention works holding the whole sequence on a single device.",
            "corpus_id": 246017095,
            "sentences": [
                {
                    "corpus_id": "246017095",
                    "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
                    "text": "efficiently. To tackle this problem, we proposed Ring Self-Attention (RSA), which circulates key and value embeddings across GPUs in a ring manner. In this case, each device is just required to keep the attention embeddings corresponding to its own sub-sequence. As a result, our sequence parallelism is memory-efficient, especially for long input sequences.\n\nTo model long sequences, existing works mainly focus on sparse attention (e.g., [Zaheer et al., 2020]) with linear instead of quadratic space complexity. In this paper, we aim to solve the long sequence modeling problem from the distributed system perspective. Compared with sparse attention, we devote ourselves to designing and implementing a system instead of a deep learning algorithm to train attentionbased models with longer sequences. Existing pipeline parallelism (PP) [Huang et al., 2018] and tensor parallelism (TP) [Shoeybi et al., 2019]) are designed to cope with a larger model size instead of longer sequences, although they can still process longer sequences to some extent. However, the challenge is, these existing parallelism methods keep the whole sequence on a single device, which limits the maximum length of the input sequence. In contrast, our approach splits the whole sequence into multiple devices, making it possible to fit longer input data.\n\nIn summary, our main contributions are three folds: (1) Our system breaks the length limitation of Transformer model training. SP splits long sequences into multiple chunks and feeds them into different devices. It is memory-efficient because each device only keeps the attention embeddings corresponding to its own sub-sequences. Theoretically, with linear space complexity attention, SP can help us train the at-tention model with infinite long sequences. (2) To our best knowledge, our work first proposed to use distributed system to handle long sequence training for attention-based models. Our implementation is fully based on PyTorch and is compatible with data parallelism (DP), PP, and TP without any extra compiler or library. This makes it possible to integrate SP with DP, PP and TP into 4D parallelism, and pave the way to train large-scale models with long sequences. (3) Our system achieves 3.0\u00d7 maximum sequence length than",
                    "score": 0.5530848211482116,
                    "section_title": "Introduction",
                    "char_start_offset": 1880,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 838,
                            "end": 858,
                            "matchedPaperCorpusId": "233289729"
                        },
                        {
                            "start": 887,
                            "end": 909,
                            "matchedPaperCorpusId": "233289729"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93017578125
                },
                {
                    "corpus_id": "246017095",
                    "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
                    "text": "Transformer achieves promising results on various tasks. However, self-attention suffers from quadratic memory requirements with respect to the sequence length. Existing work focuses on reducing time and space complexity from an algorithm perspective. In this work, we propose sequence parallelism, a memory-efficient parallelism to solve this issue from system perspective instead. Our approach is compatible with most existing parallelisms (e.g., data, pipeline, and tensor parallelism), which means our sequence parallelism makes 4D parallelism possible. More importantly, we no longer require a single device to hold the whole sequence. Besides, using efficient attention with linear complexity, our sequence parallelism enables us to train transformer with infinite long sequence. Specifically, we split the input sequence into multiple chunks and feed each chunk into its corresponding device (i.e., GPU). To compute the attention output, we integrated ring-style communication with self-attention calculation and proposed Ring Self-Attention (RSA). Experiments show that sequence parallelism performs well when scaling with batch size and sequence length. Compared with tensor parallelism, our approach achieved 13.7\\times and 3.0\\times maximum batch size and sequence length respectively when scaling up to 64 NVIDIA P100 GPUs. With efficient attention, sequence can handle sequence with over 114K tokens, which is over 27\\times longer than existing efficient attention works holding the whole sequence on a single device.",
                    "score": 0.7784116088962301,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92236328125
                },
                {
                    "corpus_id": "246017095",
                    "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
                    "text": "Attention-based models (e.g., Transformer) have achieved impressive performance on various natural language processing tasks (e.g., Q&A [Qu et al., 2019], relation extraction [Xue et al., 2020b;Xue et al., 2020a]). Recently, Transformer also achieved promising results on computer vision tasks [Dosovitskiy et al., 2020;Zhang et al., 2021] and even on bioinformatics tasks [Wang et al., 2021]. These Transformer-based models learn powerful context-aware representation by applying self-attention to all pairs of tokens from the input sequence. This mechanism captures longterm dependencies at the token level for sequence modeling. However, self-attention suffers from quadratic memory requirements with respect to sequence length. Existing system requires us to hold the whole sequence in one GPU, which limits the length of input sequence. Unfortunately, the long sequence is common in real-world applications. For instance, when we train Transformer for medical image classification, each image is much larger than it is in usual (e.g., 512\u00d7512\u00d7512 vs 256\u00d7256\u00d73). Then, each medical image includes much more tokens, that is, each input sequence is much longer than usual. In this case, it is challenging to hold the whole sequence within single GPU.\n\nIn this paper, we designed and implemented sequence parallelism (SP), a novel parallelism aiming at breaking the limitation that we need to store the whole sequence in one GPU. The proposed system can train transformer-based models with longer sequences and a larger batch size. We first split the input sequence into multiple chunks along the sequence dimension and feed each sub-sequence chunk to one corresponding GPU. Each GPU thus only holds a part of the full sequence. To apply self-attention to the tokens from different chunks, the main challenge is to compute attention scores and outputs across GPUs efficiently. To tackle this problem, we proposed Ring Self-Attention (RSA), which circulates key and value embeddings across GPUs in a ring manner. In this case, each device is just required to keep the attention embeddings corresponding to its own sub-sequence. As a",
                    "score": 0.5621823497311355,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 194,
                            "end": 212,
                            "matchedPaperCorpusId": "229678726"
                        },
                        {
                            "start": 294,
                            "end": 320,
                            "matchedPaperCorpusId": "233289729"
                        },
                        {
                            "start": 320,
                            "end": 339,
                            "matchedPaperCorpusId": "232036688"
                        },
                        {
                            "start": 373,
                            "end": 392,
                            "matchedPaperCorpusId": "233289729"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.908203125
                },
                {
                    "corpus_id": "246017095",
                    "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
                    "text": "In this paper, we proposed sequence parallelism for training Transformer-based models with longer sequence. Sequence parallelism is designed to break the limitation of sequence length on a single device (i.e., GPU). We have shown that sequence parallelism can handle longer sequence and is more memory-efficient than SoTA. In particular, sequence parallelism achieves 3.0\u00d7 maximum sequence length and 13.7\u00d7 maximum batch size than tensor parallelism when scaling up to 64 GPUs. Unlike both tensor and pipeline parallelism, sequence parallelism is not limited by the smaller hyperparameters (e.g., number of attention heads, number of layers). Therefore, our sequence parallelism can be adapted as long as the sequence length is divisible by sequence parallel size. We used a language model (i.e., BERT) to evaluate our system. However, sequence parallelism can also be adapted to computer vision tasks. This work paves the way to process large images [Hou et al., 2019] by ViT [Dosovitskiy et al., 2020] as a larger image means more patches or longer sequences. In the future, we plan to integrate data, pipeline, tensor and sequence parallelism to construct 4D parallelism. This would enable us to train extremely large models with very long sequences.",
                    "score": 0.7151411153240448,
                    "section_title": "Conclusion",
                    "char_start_offset": 18012,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 951,
                            "end": 969,
                            "matchedPaperCorpusId": "233289729"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9072265625
                },
                {
                    "corpus_id": "246017095",
                    "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
                    "text": "We analyzed and compared our SP with TP in both theoretical modeling and experiments, although this is not our direct baseline. To our best knowledge, SP is the first system designed for breaking the length limitation of sequence, so there is actually no direct baseline. Therefore, as a distributed training system designed for attention-based models, we compare with a SoTA model parallelism. TP [Narayanan et al., 2021] is compatible with DP, PP. Our SP is compatible with them. We expect our system can outperform TP with and without PP. In the future, we will integrate SP with DP, PP and TP into 4D parallelism.\n\nHere, we mainly focus on memory usage and communication cost. According to the architecture of Transformer, the comparison is divided into two parts, MLP block and attention block. In this part, we consider multi-head attention instead of self-attention for a fair and accurate comparison. We assume the optimizer is Adam used in Megatron.   Table 1, for the MLP blocks, TP stores the matrices after row or column-style splitting of the whole sequence. Our SP stores the matrices without row or column-style splitting of only one single sub-sequence on each GPU. If we assume that our sequence parallelism is more memory-efficient:\n\nWe can find that, in MLP blocks, SP is more memory-efficient when BL > 32H.\n\nAs for communication, an all-reduce operation is needed in both the forward pass and backward pass in the MLP block of Megatron due to tensor splitting. As our SP does not split the linear layer weights, no additional communication is required.",
                    "score": 0.5713079399812329,
                    "section_title": "Modelling",
                    "char_start_offset": 9398,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 398,
                            "end": 422,
                            "matchedPaperCorpusId": "233289729"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.681640625
                },
                {
                    "corpus_id": "246017095",
                    "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
                    "text": "Sequence parallelism is designed for training Transformerbased models with longer input sequences so we investigated the maximum sequence length it can handle. Similarly, we still compared TP with and without PP. We fixed batch size as 64 for BERT Base and no PP was used.\n\nWe show the maximum sequence length of the BERT Base model in Figure 5. If we scale up to 64 GPUs, we can achieve around 3\u00d7 maximum sequence length on BERT Base. Another observation is splitting along the number of attention heads limits the input sequence length of tensor parallelism in Megatron, but our sequence parallelism can scale easily by  Table 3: Weak scaling results. P is the tensor or sequence parallel size. B and S are global batch size and sequence length, respectively. M and T denote max allocated memory/MB and tokens processed per second. OOM means that CUDA out of memory occurs. splitting a sequence into multiple chunks. When using the same 16 GPUs, our sequence parallelism still can achieve 1.4 times larger sequence length than tensor parallelism. The gap is expected to widen if we use 32GB GPUs instead of 16GB GPUs. Also, in Appendix, we investigate the maximum sequence length our system can handle when we use a smaller batch size. Our RSA focuses on full self-attention in this paper. According to Table 2, when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences, because three terms of memory usage include L/N . We leave it as our future work.",
                    "score": 0.61582621786598,
                    "section_title": "Maximum sequence length",
                    "char_start_offset": 15467,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5693359375
                }
            ],
            "relevance_judgement": 0.93017578125,
            "relevance_judgment_input_expanded": "# Title: Sequence Parallelism: Long Sequence Training from System Perspective\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Shenggui Li, Fuzhao Xue, Yongbin Li, Yang You\n## Abstract\nTransformer achieves promising results on various tasks. However, self-attention suffers from quadratic memory requirements with respect to the sequence length. Existing work focuses on reducing time and space complexity from an algorithm perspective. In this work, we propose sequence parallelism, a memory-efficient parallelism to solve this issue from system perspective instead. Our approach is compatible with most existing parallelisms (e.g., data, pipeline, and tensor parallelism), which means our sequence parallelism makes 4D parallelism possible. More importantly, we no longer require a single device to hold the whole sequence. Besides, using efficient attention with linear complexity, our sequence parallelism enables us to train transformer with infinite long sequence. Specifically, we split the input sequence into multiple chunks and feed each chunk into its corresponding device (i.e., GPU). To compute the attention output, we integrated ring-style communication with self-attention calculation and proposed Ring Self-Attention (RSA). Experiments show that sequence parallelism performs well when scaling with batch size and sequence length. Compared with tensor parallelism, our approach achieved 13.7\\times and 3.0\\times maximum batch size and sequence length respectively when scaling up to 64 NVIDIA P100 GPUs. With efficient attention, sequence can handle sequence with over 114K tokens, which is over 27\\times longer than existing efficient attention works holding the whole sequence on a single device.\n## Introduction\nAttention-based models (e.g., Transformer) have achieved impressive performance on various natural language processing tasks (e.g., Q&A [Qu et al., 2019], relation extraction [Xue et al., 2020b;Xue et al., 2020a]). Recently, Transformer also achieved promising results on computer vision tasks [Dosovitskiy et al., 2020;Zhang et al., 2021] and even on bioinformatics tasks [Wang et al., 2021]. These Transformer-based models learn powerful context-aware representation by applying self-attention to all pairs of tokens from the input sequence. This mechanism captures longterm dependencies at the token level for sequence modeling. However, self-attention suffers from quadratic memory requirements with respect to sequence length. Existing system requires us to hold the whole sequence in one GPU, which limits the length of input sequence. Unfortunately, the long sequence is common in real-world applications. For instance, when we train Transformer for medical image classification, each image is much larger than it is in usual (e.g., 512\u00d7512\u00d7512 vs 256\u00d7256\u00d73). Then, each medical image includes much more tokens, that is, each input sequence is much longer than usual. In this case, it is challenging to hold the whole sequence within single GPU.\n\nIn this paper, we designed and implemented sequence parallelism (SP), a novel parallelism aiming at breaking the limitation that we need to store the whole sequence in one GPU. The proposed system can train transformer-based models with longer sequences and a larger batch size. We first split the input sequence into multiple chunks along the sequence dimension and feed each sub-sequence chunk to one corresponding GPU. Each GPU thus only holds a part of the full sequence. To apply self-attention to the tokens from different chunks, the main challenge is to compute attention scores and outputs across GPUs efficiently. To tackle this problem, we proposed Ring Self-Attention (RSA), which circulates key and value embeddings across GPUs in a ring manner. In this case, each device is just required to keep the attention embeddings corresponding to its own sub-sequence. As a\n...\nefficiently. To tackle this problem, we proposed Ring Self-Attention (RSA), which circulates key and value embeddings across GPUs in a ring manner. In this case, each device is just required to keep the attention embeddings corresponding to its own sub-sequence. As a result, our sequence parallelism is memory-efficient, especially for long input sequences.\n\nTo model long sequences, existing works mainly focus on sparse attention (e.g., [Zaheer et al., 2020]) with linear instead of quadratic space complexity. In this paper, we aim to solve the long sequence modeling problem from the distributed system perspective. Compared with sparse attention, we devote ourselves to designing and implementing a system instead of a deep learning algorithm to train attentionbased models with longer sequences. Existing pipeline parallelism (PP) [Huang et al., 2018] and tensor parallelism (TP) [Shoeybi et al., 2019]) are designed to cope with a larger model size instead of longer sequences, although they can still process longer sequences to some extent. However, the challenge is, these existing parallelism methods keep the whole sequence on a single device, which limits the maximum length of the input sequence. In contrast, our approach splits the whole sequence into multiple devices, making it possible to fit longer input data.\n\nIn summary, our main contributions are three folds: (1) Our system breaks the length limitation of Transformer model training. SP splits long sequences into multiple chunks and feeds them into different devices. It is memory-efficient because each device only keeps the attention embeddings corresponding to its own sub-sequences. Theoretically, with linear space complexity attention, SP can help us train the at-tention model with infinite long sequences. (2) To our best knowledge, our work first proposed to use distributed system to handle long sequence training for attention-based models. Our implementation is fully based on PyTorch and is compatible with data parallelism (DP), PP, and TP without any extra compiler or library. This makes it possible to integrate SP with DP, PP and TP into 4D parallelism, and pave the way to train large-scale models with long sequences. (3) Our system achieves 3.0\u00d7 maximum sequence length than\n\n## Modelling\nWe analyzed and compared our SP with TP in both theoretical modeling and experiments, although this is not our direct baseline. To our best knowledge, SP is the first system designed for breaking the length limitation of sequence, so there is actually no direct baseline. Therefore, as a distributed training system designed for attention-based models, we compare with a SoTA model parallelism. TP [Narayanan et al., 2021] is compatible with DP, PP. Our SP is compatible with them. We expect our system can outperform TP with and without PP. In the future, we will integrate SP with DP, PP and TP into 4D parallelism.\n\nHere, we mainly focus on memory usage and communication cost. According to the architecture of Transformer, the comparison is divided into two parts, MLP block and attention block. In this part, we consider multi-head attention instead of self-attention for a fair and accurate comparison. We assume the optimizer is Adam used in Megatron.   Table 1, for the MLP blocks, TP stores the matrices after row or column-style splitting of the whole sequence. Our SP stores the matrices without row or column-style splitting of only one single sub-sequence on each GPU. If we assume that our sequence parallelism is more memory-efficient:\n\nWe can find that, in MLP blocks, SP is more memory-efficient when BL > 32H.\n\nAs for communication, an all-reduce operation is needed in both the forward pass and backward pass in the MLP block of Megatron due to tensor splitting. As our SP does not split the linear layer weights, no additional communication is required.\n\n## Maximum sequence length\nSequence parallelism is designed for training Transformerbased models with longer input sequences so we investigated the maximum sequence length it can handle. Similarly, we still compared TP with and without PP. We fixed batch size as 64 for BERT Base and no PP was used.\n\nWe show the maximum sequence length of the BERT Base model in Figure 5. If we scale up to 64 GPUs, we can achieve around 3\u00d7 maximum sequence length on BERT Base. Another observation is splitting along the number of attention heads limits the input sequence length of tensor parallelism in Megatron, but our sequence parallelism can scale easily by  Table 3: Weak scaling results. P is the tensor or sequence parallel size. B and S are global batch size and sequence length, respectively. M and T denote max allocated memory/MB and tokens processed per second. OOM means that CUDA out of memory occurs. splitting a sequence into multiple chunks. When using the same 16 GPUs, our sequence parallelism still can achieve 1.4 times larger sequence length than tensor parallelism. The gap is expected to widen if we use 32GB GPUs instead of 16GB GPUs. Also, in Appendix, we investigate the maximum sequence length our system can handle when we use a smaller batch size. Our RSA focuses on full self-attention in this paper. According to Table 2, when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences, because three terms of memory usage include L/N . We leave it as our future work.\n\n## Conclusion\nIn this paper, we proposed sequence parallelism for training Transformer-based models with longer sequence. Sequence parallelism is designed to break the limitation of sequence length on a single device (i.e., GPU). We have shown that sequence parallelism can handle longer sequence and is more memory-efficient than SoTA. In particular, sequence parallelism achieves 3.0\u00d7 maximum sequence length and 13.7\u00d7 maximum batch size than tensor parallelism when scaling up to 64 GPUs. Unlike both tensor and pipeline parallelism, sequence parallelism is not limited by the smaller hyperparameters (e.g., number of attention heads, number of layers). Therefore, our sequence parallelism can be adapted as long as the sequence length is divisible by sequence parallel size. We used a language model (i.e., BERT) to evaluate our system. However, sequence parallelism can also be adapted to computer vision tasks. This work paves the way to process large images [Hou et al., 2019] by ViT [Dosovitskiy et al., 2020] as a larger image means more patches or longer sequences. In the future, we plan to integrate data, pipeline, tensor and sequence parallelism to construct 4D parallelism. This would enable us to train extremely large models with very long sequences.",
            "reference_string": "[246017095 | Li et al. | 2021 | Citations: 102]"
        },
        {
            "title": "WallFacer: Harnessing Multi-dimensional Ring Parallelism for Efficient Long Sequence Model Training",
            "venue": "",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.00611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280088379",
                    "name": "Ziming Liu"
                },
                {
                    "authorId": "2309188857",
                    "name": "Shaoyu Wang"
                },
                {
                    "authorId": "1454003795",
                    "name": "Shenggan Cheng"
                },
                {
                    "authorId": "2309195825",
                    "name": "Zhongkai Zhao"
                },
                {
                    "authorId": "2314739123",
                    "name": "Kai Wang"
                },
                {
                    "authorId": "2280071587",
                    "name": "Xuanlei Zhao"
                },
                {
                    "authorId": "2261562755",
                    "name": "Jim Demmel"
                },
                {
                    "authorId": "2285076067",
                    "name": "Yang You"
                }
            ],
            "abstract": "Training Transformer models on long sequences in a distributed setting poses significant challenges in terms of efficiency and scalability. Current methods are either constrained by the number of attention heads or excessive communication overheads. To address this problem, we propose WallFacer, a multi-dimensional distributed training system for long sequences, fostering an efficient communication paradigm and providing additional tuning flexibility for communication arrangements. Specifically, WallFacer introduces an extra parallel dimension to substantially reduce communication volume and avoid bandwidth bottlenecks. Through comprehensive experiments across diverse hardware environments and on both Natural Language Processing (NLP) and Computer Vision (CV) tasks, we demonstrate that our approach significantly surpasses state-of-the-art methods that support near-infinite sequence lengths, achieving performance improvements of up to 77.12% on GPT-style models and up to 114.33% on DiT (Diffusion Transformer) models.",
            "corpus_id": 272753750,
            "sentences": [
                {
                    "corpus_id": "272753750",
                    "title": "WallFacer: Harnessing Multi-dimensional Ring Parallelism for Efficient Long Sequence Model Training",
                    "text": "Over the past decade, Transformer [41] models have made remarkable strides in diverse fields, including computer vision (CV) and natural language processing (NLP). As the technology has evolved, the ability to efficiently process long sequences with Transformer has emerged as a pivotal challenge. For instance, in text summarization, the ability to handle extensive sequences is vital, as the content to be summarized can range from lengthy chapters to entire books [19,3]. Similarly, chat-based applications, such as ChatGPT [1], require the capacity to process extensive dialogue histories to ensure conversational consistency. There are also applications in other fields like video generation [5,33] and protein structure prediction [16,7]. \n\nThe long context in the above scenarios has introduced several challenges for model training and inference: 1) Efficiency and Adaptability. The challenge of efficiency predominantly lies in handling long sequences that require quadratic computations during attention, and in addressing the large amount of communication during distributed processing. 2) Memory. Besides the major obstacle of storing the model weight and optimizer states, the activation has also exceeded the capacity of a single GPU and risen as a new memory challenge due to the extreme sequence length. 3) Scalability. Current Transformer models usually require thousands of GPUs for pre-training, even with datasets of regular lengths. For longer sequences, ensuring an acceptable scaling speedup rate with both the sequence length and the number of GPUs increasing is even more critical to reducing time and economic costs. \n\nTraditional parallelisms such as Data Parallelism [12,40,25,44], Tensor Parallelism [40,42,43], and Pipeline Parallelism [14,11,24,27] distribute the model, input batch, and the optimizer states, but can not directly address the large memory requirement of extremely long sequences as the sequence length dimension remains unchanged. To break through this obstacle, Sequence Parallelism has been introduced, splitting the input on the sequence length dimension. Mainstream Sequence Parallelism schemes can generally be classified into two categories: those based on all-to-all communication, and those based on ring peer-to-peer communication. Methods like DeepSpeed Ulysses [15], which are based on all-to-all communication, offer efficiency but require the splitting of attention heads.",
                    "score": 0.7159530658914602,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 297
                        },
                        {
                            "start": 298,
                            "end": 474
                        },
                        {
                            "start": 475,
                            "end": 630
                        },
                        {
                            "start": 631,
                            "end": 744
                        },
                        {
                            "start": 747,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1097
                        },
                        {
                            "start": 1098,
                            "end": 1108
                        },
                        {
                            "start": 1109,
                            "end": 1319
                        },
                        {
                            "start": 1320,
                            "end": 1335
                        },
                        {
                            "start": 1336,
                            "end": 1453
                        },
                        {
                            "start": 1454,
                            "end": 1642
                        },
                        {
                            "start": 1645,
                            "end": 1978
                        },
                        {
                            "start": 1979,
                            "end": 2106
                        },
                        {
                            "start": 2107,
                            "end": 2288
                        },
                        {
                            "start": 2289,
                            "end": 2433
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 467,
                            "end": 471,
                            "matchedPaperCorpusId": "250118028"
                        },
                        {
                            "start": 737,
                            "end": 741,
                            "matchedPaperCorpusId": "235959867"
                        },
                        {
                            "start": 741,
                            "end": 743,
                            "matchedPaperCorpusId": "267789157"
                        },
                        {
                            "start": 1695,
                            "end": 1699,
                            "matchedPaperCorpusId": "2315965"
                        },
                        {
                            "start": 1702,
                            "end": 1705,
                            "matchedPaperCorpusId": "199543548"
                        },
                        {
                            "start": 1705,
                            "end": 1708,
                            "matchedPaperCorpusId": "20425665"
                        },
                        {
                            "start": 1733,
                            "end": 1736,
                            "matchedPaperCorpusId": "251979875"
                        },
                        {
                            "start": 1773,
                            "end": 1776,
                            "matchedPaperCorpusId": "235898937"
                        },
                        {
                            "start": 1776,
                            "end": 1779,
                            "matchedPaperCorpusId": "261339639"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.892578125
                },
                {
                    "corpus_id": "272753750",
                    "title": "WallFacer: Harnessing Multi-dimensional Ring Parallelism for Efficient Long Sequence Model Training",
                    "text": "Attention Optimization. Traditional full attention mechanisms necessitate O(n 2 ) memory for storing the outputs of QK T , leading to significant computational and memory demands. To address these challenges within the GPU, several approaches have been devised to reduce both memory and computational requirements. Memory-efficient attention [35] introduces a straightforward algorithm that requires only O(1) memory relative to the sequence length, with an extension for self-attention that needs only O(log n) memory. Flash Attention further minimizes I/O overhead and enhances overall efficiency. Additionally, optimization methods specifically tailored for inference, such as PagedAttention [21], are also being developed to improve the efficiency of attention computations. In this work, we utilize Flash Attention within each iteration to reduce the computation overhead. \n\nLong-Sequence Training Techniques. Sequence Parallelism [23] was initially introduced to enhance the efficiency of parallel long-sequence training. Ring Attention [26] improved communication efficiency through memory-efficient methods [35], supporting near-infinite sequence lengths. Deep-Speed Ulysses [15] employs attention head splitting to achieve high efficiency, though it is constrained by the number of heads. Megatron Sequence Parallelism focuses on reducing memory costs during Tensor Parallelism, while DistFlashAttention [22] features a load-balance scheme and a novel gradient checkpoint method. Our work builds on these innovations, introducing a system that supports large-scale training with an efficient communication scheme. \n\nTechniques for Distributed Model Training. Distributed model training encompasses two primary areas: 1) Memory Management: Various techniques aim to conserve GPU memory during distributed training, such as mixed precision training [28] and the ZeRO series [37]. In this work, we implement ZeRO-2 to manage optimizer states and gradients efficiently. 2) Hybrid Parallelism: Frameworks like Megatron [30] and Colossal AI [4] integrate multiple forms of parallelism. There are various existing Parallelism techniques like Pipeline Parallelism [14,11,24,27] and Tensor Parallelism [40], which can be combined with WallFacer Parallelism to facilitate largescale training. We are also considering the integration of additional frameworks such as [6] to enhance overlapping capabilities in future implementations.",
                    "score": 0.5671767886759387,
                    "section_title": "Related Work",
                    "char_start_offset": 40056,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 23
                        },
                        {
                            "start": 24,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 314
                        },
                        {
                            "start": 315,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 599
                        },
                        {
                            "start": 600,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 877
                        },
                        {
                            "start": 880,
                            "end": 914
                        },
                        {
                            "start": 915,
                            "end": 1027
                        },
                        {
                            "start": 1028,
                            "end": 1163
                        },
                        {
                            "start": 1164,
                            "end": 1297
                        },
                        {
                            "start": 1298,
                            "end": 1488
                        },
                        {
                            "start": 1489,
                            "end": 1622
                        },
                        {
                            "start": 1625,
                            "end": 1667
                        },
                        {
                            "start": 1668,
                            "end": 1886
                        },
                        {
                            "start": 1887,
                            "end": 1974
                        },
                        {
                            "start": 1975,
                            "end": 2088
                        },
                        {
                            "start": 2089,
                            "end": 2291
                        },
                        {
                            "start": 2292,
                            "end": 2431
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 936,
                            "end": 940,
                            "matchedPaperCorpusId": "246017095"
                        },
                        {
                            "start": 2172,
                            "end": 2175,
                            "matchedPaperCorpusId": "235898937"
                        },
                        {
                            "start": 2175,
                            "end": 2178,
                            "matchedPaperCorpusId": "261339639"
                        },
                        {
                            "start": 2365,
                            "end": 2368,
                            "matchedPaperCorpusId": "269363779"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5859375
                }
            ],
            "relevance_judgement": 0.892578125,
            "relevance_judgment_input_expanded": "# Title: WallFacer: Harnessing Multi-dimensional Ring Parallelism for Efficient Long Sequence Model Training\n# Venue: \n# Authors: Ziming Liu, Shaoyu Wang, Shenggan Cheng, Zhongkai Zhao, Kai Wang, Xuanlei Zhao, Jim Demmel, Yang You\n## Abstract\nTraining Transformer models on long sequences in a distributed setting poses significant challenges in terms of efficiency and scalability. Current methods are either constrained by the number of attention heads or excessive communication overheads. To address this problem, we propose WallFacer, a multi-dimensional distributed training system for long sequences, fostering an efficient communication paradigm and providing additional tuning flexibility for communication arrangements. Specifically, WallFacer introduces an extra parallel dimension to substantially reduce communication volume and avoid bandwidth bottlenecks. Through comprehensive experiments across diverse hardware environments and on both Natural Language Processing (NLP) and Computer Vision (CV) tasks, we demonstrate that our approach significantly surpasses state-of-the-art methods that support near-infinite sequence lengths, achieving performance improvements of up to 77.12% on GPT-style models and up to 114.33% on DiT (Diffusion Transformer) models.\n## Introduction\nOver the past decade, Transformer [41] models have made remarkable strides in diverse fields, including computer vision (CV) and natural language processing (NLP). As the technology has evolved, the ability to efficiently process long sequences with Transformer has emerged as a pivotal challenge. For instance, in text summarization, the ability to handle extensive sequences is vital, as the content to be summarized can range from lengthy chapters to entire books [19,3]. Similarly, chat-based applications, such as ChatGPT [1], require the capacity to process extensive dialogue histories to ensure conversational consistency. There are also applications in other fields like video generation [5,33] and protein structure prediction [16,7]. \n\nThe long context in the above scenarios has introduced several challenges for model training and inference: 1) Efficiency and Adaptability. The challenge of efficiency predominantly lies in handling long sequences that require quadratic computations during attention, and in addressing the large amount of communication during distributed processing. 2) Memory. Besides the major obstacle of storing the model weight and optimizer states, the activation has also exceeded the capacity of a single GPU and risen as a new memory challenge due to the extreme sequence length. 3) Scalability. Current Transformer models usually require thousands of GPUs for pre-training, even with datasets of regular lengths. For longer sequences, ensuring an acceptable scaling speedup rate with both the sequence length and the number of GPUs increasing is even more critical to reducing time and economic costs. \n\nTraditional parallelisms such as Data Parallelism [12,40,25,44], Tensor Parallelism [40,42,43], and Pipeline Parallelism [14,11,24,27] distribute the model, input batch, and the optimizer states, but can not directly address the large memory requirement of extremely long sequences as the sequence length dimension remains unchanged. To break through this obstacle, Sequence Parallelism has been introduced, splitting the input on the sequence length dimension. Mainstream Sequence Parallelism schemes can generally be classified into two categories: those based on all-to-all communication, and those based on ring peer-to-peer communication. Methods like DeepSpeed Ulysses [15], which are based on all-to-all communication, offer efficiency but require the splitting of attention heads.\n\n## Related Work\nAttention Optimization. Traditional full attention mechanisms necessitate O(n 2 ) memory for storing the outputs of QK T , leading to significant computational and memory demands. To address these challenges within the GPU, several approaches have been devised to reduce both memory and computational requirements. Memory-efficient attention [35] introduces a straightforward algorithm that requires only O(1) memory relative to the sequence length, with an extension for self-attention that needs only O(log n) memory. Flash Attention further minimizes I/O overhead and enhances overall efficiency. Additionally, optimization methods specifically tailored for inference, such as PagedAttention [21], are also being developed to improve the efficiency of attention computations. In this work, we utilize Flash Attention within each iteration to reduce the computation overhead. \n\nLong-Sequence Training Techniques. Sequence Parallelism [23] was initially introduced to enhance the efficiency of parallel long-sequence training. Ring Attention [26] improved communication efficiency through memory-efficient methods [35], supporting near-infinite sequence lengths. Deep-Speed Ulysses [15] employs attention head splitting to achieve high efficiency, though it is constrained by the number of heads. Megatron Sequence Parallelism focuses on reducing memory costs during Tensor Parallelism, while DistFlashAttention [22] features a load-balance scheme and a novel gradient checkpoint method. Our work builds on these innovations, introducing a system that supports large-scale training with an efficient communication scheme. \n\nTechniques for Distributed Model Training. Distributed model training encompasses two primary areas: 1) Memory Management: Various techniques aim to conserve GPU memory during distributed training, such as mixed precision training [28] and the ZeRO series [37]. In this work, we implement ZeRO-2 to manage optimizer states and gradients efficiently. 2) Hybrid Parallelism: Frameworks like Megatron [30] and Colossal AI [4] integrate multiple forms of parallelism. There are various existing Parallelism techniques like Pipeline Parallelism [14,11,24,27] and Tensor Parallelism [40], which can be combined with WallFacer Parallelism to facilitate largescale training. We are also considering the integration of additional frameworks such as [6] to enhance overlapping capabilities in future implementations.",
            "reference_string": "[272753750 | Liu et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Ultra-Long Sequence Distributed Transformer",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 32,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.02382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2265563075",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "152418718",
                    "name": "Isaac Lyngaas"
                },
                {
                    "authorId": "3395528",
                    "name": "A. Tsaris"
                },
                {
                    "authorId": "2265565597",
                    "name": "Peng Chen"
                },
                {
                    "authorId": "2143124",
                    "name": "Sajal Dash"
                },
                {
                    "authorId": "1505802839",
                    "name": "Mayanka Chandra Shekar"
                },
                {
                    "authorId": "2196345134",
                    "name": "Tao Luo"
                },
                {
                    "authorId": "2265537059",
                    "name": "Hong-Jun Yoon"
                },
                {
                    "authorId": "1700692",
                    "name": "M. Wahib"
                },
                {
                    "authorId": "2265588161",
                    "name": "J. Gounley"
                }
            ],
            "abstract": "Transformer models trained on long sequences often achieve higher accuracy than short sequences. Unfortunately, conventional transformers struggle with long sequence training due to the overwhelming computation and memory requirements. Existing methods for long sequence training offer limited speedup and memory reduction, and may compromise accuracy. This paper presents a novel and efficient distributed training method, the Long Short-Sequence Transformer (LSS Transformer), for training transformer with long sequences. It distributes a long sequence into segments among GPUs, with each GPU computing a partial self-attention for its segment. Then, it uses a fused communication and a novel double gradient averaging technique to avoid the need to aggregate partial self-attention and minimize communication overhead. We evaluated the performance between LSS Transformer and the state-of-the-art Nvidia sequence parallelism on a Wikipedia enwik8 dataset. Results show that our proposed method lead to 5.6x faster and 10.2x more memory-efficient implementation compared to state-of-the-art sequence parallelism on 144 Nvidia V100 GPUs. Moreover, our algorithm scales to an extreme sequence length of 50,112 at 3,456 GPUs, achieving 161% super-linear parallel efficiency and a throughput of 32 petaflops.",
            "corpus_id": 265034082,
            "sentences": [
                {
                    "corpus_id": "265034082",
                    "title": "Ultra-Long Sequence Distributed Transformer",
                    "text": "The LSS Transformer's sequence parallelism has three limitations. First, it still requires 2 global inter-GPU communications per attention layer, which degrades parallel efficiency at many GPUs. Second, while sequence parallelism tackles the long sequence issue, it does not address computation challenge for training large dataset. Three, sequence parallelism is only one source of parallelism. To scale to a large supercomputer for training, the LSS Transformer needs more sources of parallelism to achieve better scalability. To address these issues, this section introduces a method to integrate the LSS Transformer's sequence parallelism with data parallelism. With the integration, the parallel algorithm can (1) achieve better scalability; (2) simultaneously tackle long sequence and large dataset challenges; and (3) constrain the self-attention communications among local communicative groups for reduced overhead. \n\nDespite that sequence and data parallelisms are mostly orthogonal, one technical challenge to overcome is that both parallelisms require model parameter synchronization, but among GPUs in different communicative groups and communicate in different ways. Sequence parallelism requires model parameter synchronization among sequence parallel GPUs, but excludes positional embedding parameters from synchronization given that positional embeddings are distributed in sequence dimension. Data parallelism requires model parameter synchronization among data parallel GPUs, but must include positional embeddings given that data parallel GPUs have the same copy of the positional embedding parameters, but train them with different data batches. \n\nTo address this issue, we use an innovative double gradient averaging technique to avoid synchronization conflicts for positional embeddings. Fig. 3. illustrates an example of how the integrated sequence and data parallelism uses double gradient averaging. In this example, GPUs 1 and 2 process a sequence x 1 together using sequence parallelism, with the first segment x 1 1 assigned to GPU 1 and the second segment x 1 2 assigned to GPU 2. The positional embedding parameters are distributed in the same way with the first half P E 1 assigned to GPU 1 and the second half P E 2 assigned to GPU 2. Similarly, GPUs 3 and 4 handle a difference sequence x 2 using sequence parallelism.",
                    "score": 0.5632034889714013,
                    "section_title": "INTEGRATED SEQUENCE & DATA PARALLELISM",
                    "char_start_offset": 21806,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 65
                        },
                        {
                            "start": 66,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 395
                        },
                        {
                            "start": 396,
                            "end": 528
                        },
                        {
                            "start": 529,
                            "end": 665
                        },
                        {
                            "start": 666,
                            "end": 923
                        },
                        {
                            "start": 926,
                            "end": 1179
                        },
                        {
                            "start": 1180,
                            "end": 1409
                        },
                        {
                            "start": 1410,
                            "end": 1665
                        },
                        {
                            "start": 1668,
                            "end": 1809
                        },
                        {
                            "start": 1810,
                            "end": 1924
                        },
                        {
                            "start": 1925,
                            "end": 2351
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88427734375
                },
                {
                    "corpus_id": "265034082",
                    "title": "Ultra-Long Sequence Distributed Transformer",
                    "text": "Transformer models trained on long sequences often achieve higher accuracy than short sequences. Unfortunately, conventional transformers struggle with long sequence training due to the overwhelming computation and memory requirements. Existing methods for long sequence training offer limited speedup and memory reduction, and may compromise accuracy. This paper presents a novel and efficient distributed training method, the Long Short-Sequence Transformer (LSS Transformer), for training transformer with long sequences. It distributes a long sequence into segments among GPUs, with each GPU computing a partial self-attention for its segment. Then, it uses a fused communication and a novel double gradient averaging technique to avoid the need to aggregate partial self-attention and minimize communication overhead. We evaluated the performance between LSS Transformer and the state-of-the-art Nvidia sequence parallelism on a Wikipedia enwik8 dataset. Results show that our proposed method lead to 5.6x faster and 10.2x more memory-efficient implementation compared to state-of-the-art sequence parallelism on 144 Nvidia V100 GPUs. Moreover, our algorithm scales to an extreme sequence length of 50,112 at 3,456 GPUs, achieving 161% super-linear parallel efficiency and a throughput of 32 petaflops.",
                    "score": 0.5841051777787966,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72509765625
                },
                {
                    "corpus_id": "265034082",
                    "title": "Ultra-Long Sequence Distributed Transformer",
                    "text": "To address this issue, there are three research approaches: hierarchical training, attention approximation, and distributed sequence parallelism. Hierarchical training involves training multiple transformers at different levels of abstraction (Si & Roberts, 2021;Chen et al., 2022;2021b;Yu et al., 2023). The transformer at the lowest abstraction level trains on the shortest sequence segments. Then, the transformer at the next higher level uses the previous level outputs as additional input to train on longer segments. The process then repeats until reaching the highest abstraction level. However, training multiple transformers at different abstraction levels significantly increases training time and memory footprint. In addition, hyper-parameter tuning is required for the optimal model architecture at each abstraction level. \n\nThe approximation approach, in contrast, aims to reduce the computations and memory usage by approximating the self-  (Kitaev et al., 2020;Roy et al., 2021) (Child et al., 2019;Beltagy et al., 2020) Yes Serial O(Z) O(Z 3 ) N/A N/A N/A Low sparsity",
                    "score": 0.5906958859984591,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 2098,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 304
                        },
                        {
                            "start": 305,
                            "end": 394
                        },
                        {
                            "start": 395,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 593
                        },
                        {
                            "start": 594,
                            "end": 725
                        },
                        {
                            "start": 726,
                            "end": 835
                        },
                        {
                            "start": 838,
                            "end": 1085
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 263,
                            "end": 281,
                            "matchedPaperCorpusId": "249395419"
                        },
                        {
                            "start": 281,
                            "end": 287,
                            "matchedPaperCorpusId": "232404237"
                        },
                        {
                            "start": 956,
                            "end": 977,
                            "matchedPaperCorpusId": "209315300"
                        },
                        {
                            "start": 977,
                            "end": 993,
                            "matchedPaperCorpusId": "212718077"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.388671875
                },
                {
                    "corpus_id": "265034082",
                    "title": "Ultra-Long Sequence Distributed Transformer",
                    "text": "The transformer is a powerful neural network architecture widely used in natural language and image processing (Vaswani et al., 2017). Its versatility is evidenced by its wide range of applications, including machine translation (Wang et al., 2019), chatbots (Caldarini et al., 2022), speech recognition (Dong et al., 2018), image captioning (Yu et al., 2019), image segmentation (Valanarasu et al., 2021;Strudel et al., 2021), and classification (Chen et al., 2021b). The transformer achieves its impressive performance by recognizing that different input sequence tokens have varying levels of importance to the final output prediction. The transformer captures the relationship between each pair of input tokens using a process called \"self-attention\". This allows the transformer to generate highly accurate outputs by focusing on the most relevant tokens in an input sequence while also paying attention to the overall context. This approach has proven to be highly effective and makes transformer a leading technology in artificial intelligence. \n\nWith long sequence training, transformer attends to many more input tokens than a transformer trained with short sequences. Therefore, long sequence training often captures more contextual information and leads to markedly 1, Oak Ridge National Laboratory, US. 2, National Institute of Advanced Industrial Science and Technology, Japan. 3, Agency for Science, Technology and Research, Singapore. Corresponding email: wangx2@ornl.gov higher prediction accuracy for many tasks with long-range dependencies, such as DNA sequence analysis (Zaheer et al., 2020), long document summary (Beltagy et al., 2020) and image segmentation (Strudel et al., 2021). Unfortunately, transformer's memory footprint increases quadratically and computations increase cubically with longer sequence lengths (Beltagy et al., 2020;Dao et al., 2022). Therefore, the transformer's sequence length is typically truncated to no more than a couple thousand tokens due to runtime and memory constraints, despite longer sequences leading to higher accuracy. \n\nTo address this issue, there are three research approaches: hierarchical training, attention approximation, and distributed sequence parallelism.",
                    "score": 0.6594307980095785,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 135,
                            "end": 468
                        },
                        {
                            "start": 469,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 755
                        },
                        {
                            "start": 756,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 1051
                        },
                        {
                            "start": 1054,
                            "end": 1177
                        },
                        {
                            "start": 1178,
                            "end": 1390
                        },
                        {
                            "start": 1391,
                            "end": 1449
                        },
                        {
                            "start": 1450,
                            "end": 1486
                        },
                        {
                            "start": 1487,
                            "end": 1703
                        },
                        {
                            "start": 1704,
                            "end": 1879
                        },
                        {
                            "start": 1880,
                            "end": 2080
                        },
                        {
                            "start": 2083,
                            "end": 2228
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 111,
                            "end": 133,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 229,
                            "end": 248,
                            "matchedPaperCorpusId": "174799399"
                        },
                        {
                            "start": 304,
                            "end": 323,
                            "matchedPaperCorpusId": "52287921"
                        },
                        {
                            "start": 342,
                            "end": 359,
                            "matchedPaperCorpusId": "159041705"
                        },
                        {
                            "start": 380,
                            "end": 405,
                            "matchedPaperCorpusId": "231986084"
                        },
                        {
                            "start": 405,
                            "end": 426,
                            "matchedPaperCorpusId": "234470051"
                        },
                        {
                            "start": 447,
                            "end": 467,
                            "matchedPaperCorpusId": "232404237"
                        },
                        {
                            "start": 1589,
                            "end": 1610,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 1680,
                            "end": 1702,
                            "matchedPaperCorpusId": "234470051"
                        },
                        {
                            "start": 1861,
                            "end": 1878,
                            "matchedPaperCorpusId": "249151871"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.349609375
                }
            ],
            "relevance_judgement": 0.88427734375,
            "relevance_judgment_input_expanded": "# Title: Ultra-Long Sequence Distributed Transformer\n# Venue: arXiv.org\n# Authors: Xiao Wang, Isaac Lyngaas, A. Tsaris, Peng Chen, Sajal Dash, Mayanka Chandra Shekar, Tao Luo, Hong-Jun Yoon, M. Wahib, J. Gounley\n## Abstract\nTransformer models trained on long sequences often achieve higher accuracy than short sequences. Unfortunately, conventional transformers struggle with long sequence training due to the overwhelming computation and memory requirements. Existing methods for long sequence training offer limited speedup and memory reduction, and may compromise accuracy. This paper presents a novel and efficient distributed training method, the Long Short-Sequence Transformer (LSS Transformer), for training transformer with long sequences. It distributes a long sequence into segments among GPUs, with each GPU computing a partial self-attention for its segment. Then, it uses a fused communication and a novel double gradient averaging technique to avoid the need to aggregate partial self-attention and minimize communication overhead. We evaluated the performance between LSS Transformer and the state-of-the-art Nvidia sequence parallelism on a Wikipedia enwik8 dataset. Results show that our proposed method lead to 5.6x faster and 10.2x more memory-efficient implementation compared to state-of-the-art sequence parallelism on 144 Nvidia V100 GPUs. Moreover, our algorithm scales to an extreme sequence length of 50,112 at 3,456 GPUs, achieving 161% super-linear parallel efficiency and a throughput of 32 petaflops.\n## INTRODUCTION\nThe transformer is a powerful neural network architecture widely used in natural language and image processing (Vaswani et al., 2017). Its versatility is evidenced by its wide range of applications, including machine translation (Wang et al., 2019), chatbots (Caldarini et al., 2022), speech recognition (Dong et al., 2018), image captioning (Yu et al., 2019), image segmentation (Valanarasu et al., 2021;Strudel et al., 2021), and classification (Chen et al., 2021b). The transformer achieves its impressive performance by recognizing that different input sequence tokens have varying levels of importance to the final output prediction. The transformer captures the relationship between each pair of input tokens using a process called \"self-attention\". This allows the transformer to generate highly accurate outputs by focusing on the most relevant tokens in an input sequence while also paying attention to the overall context. This approach has proven to be highly effective and makes transformer a leading technology in artificial intelligence. \n\nWith long sequence training, transformer attends to many more input tokens than a transformer trained with short sequences. Therefore, long sequence training often captures more contextual information and leads to markedly 1, Oak Ridge National Laboratory, US. 2, National Institute of Advanced Industrial Science and Technology, Japan. 3, Agency for Science, Technology and Research, Singapore. Corresponding email: wangx2@ornl.gov higher prediction accuracy for many tasks with long-range dependencies, such as DNA sequence analysis (Zaheer et al., 2020), long document summary (Beltagy et al., 2020) and image segmentation (Strudel et al., 2021). Unfortunately, transformer's memory footprint increases quadratically and computations increase cubically with longer sequence lengths (Beltagy et al., 2020;Dao et al., 2022). Therefore, the transformer's sequence length is typically truncated to no more than a couple thousand tokens due to runtime and memory constraints, despite longer sequences leading to higher accuracy. \n\nTo address this issue, there are three research approaches: hierarchical training, attention approximation, and distributed sequence parallelism.\n...\nTo address this issue, there are three research approaches: hierarchical training, attention approximation, and distributed sequence parallelism. Hierarchical training involves training multiple transformers at different levels of abstraction (Si & Roberts, 2021;Chen et al., 2022;2021b;Yu et al., 2023). The transformer at the lowest abstraction level trains on the shortest sequence segments. Then, the transformer at the next higher level uses the previous level outputs as additional input to train on longer segments. The process then repeats until reaching the highest abstraction level. However, training multiple transformers at different abstraction levels significantly increases training time and memory footprint. In addition, hyper-parameter tuning is required for the optimal model architecture at each abstraction level. \n\nThe approximation approach, in contrast, aims to reduce the computations and memory usage by approximating the self-  (Kitaev et al., 2020;Roy et al., 2021) (Child et al., 2019;Beltagy et al., 2020) Yes Serial O(Z) O(Z 3 ) N/A N/A N/A Low sparsity\n\n## INTEGRATED SEQUENCE & DATA PARALLELISM\nThe LSS Transformer's sequence parallelism has three limitations. First, it still requires 2 global inter-GPU communications per attention layer, which degrades parallel efficiency at many GPUs. Second, while sequence parallelism tackles the long sequence issue, it does not address computation challenge for training large dataset. Three, sequence parallelism is only one source of parallelism. To scale to a large supercomputer for training, the LSS Transformer needs more sources of parallelism to achieve better scalability. To address these issues, this section introduces a method to integrate the LSS Transformer's sequence parallelism with data parallelism. With the integration, the parallel algorithm can (1) achieve better scalability; (2) simultaneously tackle long sequence and large dataset challenges; and (3) constrain the self-attention communications among local communicative groups for reduced overhead. \n\nDespite that sequence and data parallelisms are mostly orthogonal, one technical challenge to overcome is that both parallelisms require model parameter synchronization, but among GPUs in different communicative groups and communicate in different ways. Sequence parallelism requires model parameter synchronization among sequence parallel GPUs, but excludes positional embedding parameters from synchronization given that positional embeddings are distributed in sequence dimension. Data parallelism requires model parameter synchronization among data parallel GPUs, but must include positional embeddings given that data parallel GPUs have the same copy of the positional embedding parameters, but train them with different data batches. \n\nTo address this issue, we use an innovative double gradient averaging technique to avoid synchronization conflicts for positional embeddings. Fig. 3. illustrates an example of how the integrated sequence and data parallelism uses double gradient averaging. In this example, GPUs 1 and 2 process a sequence x 1 together using sequence parallelism, with the first segment x 1 1 assigned to GPU 1 and the second segment x 1 2 assigned to GPU 2. The positional embedding parameters are distributed in the same way with the first half P E 1 assigned to GPU 1 and the second half P E 2 assigned to GPU 2. Similarly, GPUs 3 and 4 handle a difference sequence x 2 using sequence parallelism.",
            "reference_string": "[265034082 | Wang et al. | 2023 | Citations: 4]"
        },
        {
            "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 31,
            "citation_count": 120,
            "influential_citation_count": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.14509",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.14509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1683824",
                    "name": "S. A. Jacobs"
                },
                {
                    "authorId": "2118817779",
                    "name": "Masahiro Tanaka"
                },
                {
                    "authorId": "2248958850",
                    "name": "Chengming Zhang"
                },
                {
                    "authorId": "67016465",
                    "name": "Minjia Zhang"
                },
                {
                    "authorId": "2145202945",
                    "name": "L. Song"
                },
                {
                    "authorId": "32817044",
                    "name": "Samyam Rajbhandari"
                },
                {
                    "authorId": "2145020341",
                    "name": "Yuxiong He"
                }
            ],
            "abstract": "Computation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input data along the sequence dimension and employs an efficient all-to-all collective communication for attention computation. Theoretical communication analysis shows that whereas other methods incur communication overhead as sequence length increases, DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally. Furthermore, experimental evaluations show that DeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than the existing method SOTA baseline.",
            "corpus_id": 262826014,
            "sentences": [
                {
                    "corpus_id": "262826014",
                    "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
                    "text": "Computation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input data along the sequence dimension and employs an efficient all-to-all collective communication for attention computation. Theoretical communication analysis shows that whereas other methods incur communication overhead as sequence length increases, DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally. Furthermore, experimental evaluations show that DeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than the existing method SOTA baseline.",
                    "score": 0.6040364653326177,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87060546875
                },
                {
                    "corpus_id": "262826014",
                    "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
                    "text": "Two challenges with existing parallelism approach come to the fore. First, existing parallelism approach such as data, tensor and pipeline parallelism cannot address the scaling along sequence dimension. Second, existing sequence parallelism approaches are not effective because of memory-communication inefficiencies. Furthermore, existing approaches have limited usability requiring intrusive and error prone code refactoring. \n\nIn this paper, we introduce DeepSpeed-Ulysses (or Ulysses, a very long novel), a simple, portable, and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence lengths. DeepSpeed-Ulysses partitions individual samples along the sequence dimension among participating GPUs. Then right before the attention computation, it employs all-to-all communication collective on the partitioned queries, keys and values such that each GPU receives the full sequence but only for a non-overlapping subset of the attention heads. This allows the participating GPUs to compute attention for different attention heads in parallel. Finally, DeepSpeed-Ulysses employs another all-to-all to gather the results along the attention heads while re-partitioning along the sequence dimension. \n\nIn this work, we put forward the following contributions of DeepSpeed-Ulysses to advance state of the art in long sequence parallelism: \n\n\u2022 DeepSpeed-Ulysses trains Transformer models 4x larger sequence lengths than existing systems, while enabling training with sequences with over a million tokens. \n\n\u2022 Communication reduction of over 10x compared to existing systems, resulting in throughput improvements of up to 2.5x, and sustained throughput of over 175 TFlops/GPU (over 54% of hardware peak). \n\n\u2022 Fully general and implementation agnostic attention: DeepSpeed sequence parallelism (Ulysses) supports dense as well as sparse attention, and it works with efficient attention implementations such as FlashAttention v2 [Dao, 2023]. \n\n\u2022 Support for massive model training: DeepSpeed sequence parallelism works together with ZeRO-3 to not only support large sequence lengths but also massive model sizes. \n\n\u2022 Easy-to-use and portable, requiring minimal code changes to the existing training frameworks.",
                    "score": 0.6090932191491494,
                    "section_title": "Introduction",
                    "char_start_offset": 2188,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 67
                        },
                        {
                            "start": 68,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 318
                        },
                        {
                            "start": 319,
                            "end": 428
                        },
                        {
                            "start": 431,
                            "end": 649
                        },
                        {
                            "start": 650,
                            "end": 752
                        },
                        {
                            "start": 753,
                            "end": 996
                        },
                        {
                            "start": 997,
                            "end": 1095
                        },
                        {
                            "start": 1096,
                            "end": 1249
                        },
                        {
                            "start": 1252,
                            "end": 1387
                        },
                        {
                            "start": 1390,
                            "end": 1552
                        },
                        {
                            "start": 1555,
                            "end": 1751
                        },
                        {
                            "start": 1754,
                            "end": 1986
                        },
                        {
                            "start": 1989,
                            "end": 2157
                        },
                        {
                            "start": 2160,
                            "end": 2255
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83935546875
                },
                {
                    "corpus_id": "262826014",
                    "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
                    "text": "Shown in Figure 1 is a sketch of building blocks of a typical multihead attention Transformer architecture [Vaswani et al., 2017]. It consists of input sequences which are projected into queries (Q),keys (K) and values (V) embeddings. \n\nQKV are typically a 3D tensor of size N, b, d where N is sequence length, b is micro batch size and d is hidden dimension. \n\nThe QKV tensors are fed to the attention block, a central component of Transformer model. Outputs of attentions are inputs to the multilayer perceptron (MLP) or position-wise feed-forward block of Transformer architecture. \n\nThe attention block followed by MLP block are replicated multiple times to form an encoder, a decoder or an encoderdecoder Transformer network. It is worth to note that our proposed approach is orthogonal to both data parallelism and ZeRO. Our proposed approach can be used with both methods. Also, by leveraging sequence parallelism to keep global batch size at reasonable size on large systems, we effectively ameliorate the impact of large batch size on model convergence. Sequence parallelism serves two purposes in this regard. First, sequence parallelism can accelerate time to solution for same (already explored) long sequence length; in other words, sequence parallelism reduces the iteration time proportional to additional compute resources. Second, sequence parallelism enables longer sequence training or continual pretraining where training context length gradually increase over time [Xiong et al., 2023]. Consider a real world scenario of large scale training on 1024 GPUs. The initial exploratory or pretraining set up of a (proxy) LLM has a sequence length of 8192 (8K), a micro batch size of 1 (thus, 8 million token global size) per GPU. A simple change to improve the quality of the pretrained model requires a change of sequence length from 8K to 32K, which would result in approximately 32 million global batch size. However, increasing the global batch size is not an option due to the negative impact on model quality. Therefore, sequence parallelism comes in handy as a system optimization technique with no requirement for laborious hyperparameter search.",
                    "score": 0.6952242035281685,
                    "section_title": "Transformer Architecture",
                    "char_start_offset": 5182,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 130
                        },
                        {
                            "start": 131,
                            "end": 234
                        },
                        {
                            "start": 237,
                            "end": 359
                        },
                        {
                            "start": 362,
                            "end": 451
                        },
                        {
                            "start": 452,
                            "end": 584
                        },
                        {
                            "start": 587,
                            "end": 730
                        },
                        {
                            "start": 731,
                            "end": 826
                        },
                        {
                            "start": 827,
                            "end": 879
                        },
                        {
                            "start": 880,
                            "end": 1062
                        },
                        {
                            "start": 1063,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1507
                        },
                        {
                            "start": 1508,
                            "end": 1576
                        },
                        {
                            "start": 1577,
                            "end": 1744
                        },
                        {
                            "start": 1745,
                            "end": 1926
                        },
                        {
                            "start": 1927,
                            "end": 2030
                        },
                        {
                            "start": 2031,
                            "end": 2169
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.779296875
                },
                {
                    "corpus_id": "262826014",
                    "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
                    "text": "Similarly, we evaluate DeepSpeed sequence parallelism on 7 billion and 30 billion parameter sparse attention models and benchmark against Megatron-LM sequence parallelism. Results of our evaluation are shown in Figures 6 and 7. \n\nWe observe similar trends with sparse attention as dense attention experiments. We observe more than 2x throughput performance of DeepSpeed sequence parallelism compared to Megatron-LM. For memory saving, DeepSpeed sequence parallelism leveraging ZeRO-3 scales to 4x longer sequence lengths than Megatron-LM. \n\nDeepSpeed sequence parallelism outperforms Megatron-LM for sequence length that can be run with both. In fact, the current DeepSpeed throughput is bottlenecked by the local sparse attention implementation, and as a result DeepSpeed throughput decreases as the sequence length increases. We expect this gap in performance between DeepSpeed and Megatron-LM to increase further for larger sequence lengths as we improve the performance of the local sparse attention implementation in future. Furthermore, we conduct parallel scaling studies of DeepSpeed-Ulysses along two axes. First, we fix sequence length at 131,072 tokens and increase GPU count from 64 to 256. Second, we increase the GPU count proportionally to the increase in sequence length. The results of these experiments are shown in Tables 2 and 3 respectively. For both evaluations, we used GPT-7B dense model at global batch size of 8. The tables show iteration time in microseconds as well as the achieved throughput measured in per GPU TFLOPs. Table 2 can be interpreted as strong scaling and shows that execution time decreases almost linearly as we increase the GPU count. Table 3 on the other hand, is a form of weak scaling (not in the traditional sense) with caveat that attention computation, a function of sequence length, is quadratic in complexity. In other words, as we increase sequence length, the work increases quadratically.",
                    "score": 0.5585385263552101,
                    "section_title": "Sparse Attention Evaluation",
                    "char_start_offset": 17038,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 171
                        },
                        {
                            "start": 172,
                            "end": 227
                        },
                        {
                            "start": 230,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 415
                        },
                        {
                            "start": 416,
                            "end": 538
                        },
                        {
                            "start": 541,
                            "end": 642
                        },
                        {
                            "start": 643,
                            "end": 827
                        },
                        {
                            "start": 828,
                            "end": 1029
                        },
                        {
                            "start": 1030,
                            "end": 1115
                        },
                        {
                            "start": 1116,
                            "end": 1202
                        },
                        {
                            "start": 1203,
                            "end": 1287
                        },
                        {
                            "start": 1288,
                            "end": 1362
                        },
                        {
                            "start": 1363,
                            "end": 1438
                        },
                        {
                            "start": 1439,
                            "end": 1548
                        },
                        {
                            "start": 1549,
                            "end": 1679
                        },
                        {
                            "start": 1680,
                            "end": 1862
                        },
                        {
                            "start": 1863,
                            "end": 1944
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70556640625
                }
            ],
            "relevance_judgement": 0.87060546875,
            "relevance_judgment_input_expanded": "# Title: DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models\n# Venue: arXiv.org\n# Authors: S. A. Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, L. Song, Samyam Rajbhandari, Yuxiong He\n## Abstract\nComputation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input data along the sequence dimension and employs an efficient all-to-all collective communication for attention computation. Theoretical communication analysis shows that whereas other methods incur communication overhead as sequence length increases, DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally. Furthermore, experimental evaluations show that DeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than the existing method SOTA baseline.\n## Introduction\nTwo challenges with existing parallelism approach come to the fore. First, existing parallelism approach such as data, tensor and pipeline parallelism cannot address the scaling along sequence dimension. Second, existing sequence parallelism approaches are not effective because of memory-communication inefficiencies. Furthermore, existing approaches have limited usability requiring intrusive and error prone code refactoring. \n\nIn this paper, we introduce DeepSpeed-Ulysses (or Ulysses, a very long novel), a simple, portable, and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence lengths. DeepSpeed-Ulysses partitions individual samples along the sequence dimension among participating GPUs. Then right before the attention computation, it employs all-to-all communication collective on the partitioned queries, keys and values such that each GPU receives the full sequence but only for a non-overlapping subset of the attention heads. This allows the participating GPUs to compute attention for different attention heads in parallel. Finally, DeepSpeed-Ulysses employs another all-to-all to gather the results along the attention heads while re-partitioning along the sequence dimension. \n\nIn this work, we put forward the following contributions of DeepSpeed-Ulysses to advance state of the art in long sequence parallelism: \n\n\u2022 DeepSpeed-Ulysses trains Transformer models 4x larger sequence lengths than existing systems, while enabling training with sequences with over a million tokens. \n\n\u2022 Communication reduction of over 10x compared to existing systems, resulting in throughput improvements of up to 2.5x, and sustained throughput of over 175 TFlops/GPU (over 54% of hardware peak). \n\n\u2022 Fully general and implementation agnostic attention: DeepSpeed sequence parallelism (Ulysses) supports dense as well as sparse attention, and it works with efficient attention implementations such as FlashAttention v2 [Dao, 2023]. \n\n\u2022 Support for massive model training: DeepSpeed sequence parallelism works together with ZeRO-3 to not only support large sequence lengths but also massive model sizes. \n\n\u2022 Easy-to-use and portable, requiring minimal code changes to the existing training frameworks.\n\n## Transformer Architecture\nShown in Figure 1 is a sketch of building blocks of a typical multihead attention Transformer architecture [Vaswani et al., 2017]. It consists of input sequences which are projected into queries (Q),keys (K) and values (V) embeddings. \n\nQKV are typically a 3D tensor of size N, b, d where N is sequence length, b is micro batch size and d is hidden dimension. \n\nThe QKV tensors are fed to the attention block, a central component of Transformer model. Outputs of attentions are inputs to the multilayer perceptron (MLP) or position-wise feed-forward block of Transformer architecture. \n\nThe attention block followed by MLP block are replicated multiple times to form an encoder, a decoder or an encoderdecoder Transformer network. It is worth to note that our proposed approach is orthogonal to both data parallelism and ZeRO. Our proposed approach can be used with both methods. Also, by leveraging sequence parallelism to keep global batch size at reasonable size on large systems, we effectively ameliorate the impact of large batch size on model convergence. Sequence parallelism serves two purposes in this regard. First, sequence parallelism can accelerate time to solution for same (already explored) long sequence length; in other words, sequence parallelism reduces the iteration time proportional to additional compute resources. Second, sequence parallelism enables longer sequence training or continual pretraining where training context length gradually increase over time [Xiong et al., 2023]. Consider a real world scenario of large scale training on 1024 GPUs. The initial exploratory or pretraining set up of a (proxy) LLM has a sequence length of 8192 (8K), a micro batch size of 1 (thus, 8 million token global size) per GPU. A simple change to improve the quality of the pretrained model requires a change of sequence length from 8K to 32K, which would result in approximately 32 million global batch size. However, increasing the global batch size is not an option due to the negative impact on model quality. Therefore, sequence parallelism comes in handy as a system optimization technique with no requirement for laborious hyperparameter search.\n\n## Sparse Attention Evaluation\nSimilarly, we evaluate DeepSpeed sequence parallelism on 7 billion and 30 billion parameter sparse attention models and benchmark against Megatron-LM sequence parallelism. Results of our evaluation are shown in Figures 6 and 7. \n\nWe observe similar trends with sparse attention as dense attention experiments. We observe more than 2x throughput performance of DeepSpeed sequence parallelism compared to Megatron-LM. For memory saving, DeepSpeed sequence parallelism leveraging ZeRO-3 scales to 4x longer sequence lengths than Megatron-LM. \n\nDeepSpeed sequence parallelism outperforms Megatron-LM for sequence length that can be run with both. In fact, the current DeepSpeed throughput is bottlenecked by the local sparse attention implementation, and as a result DeepSpeed throughput decreases as the sequence length increases. We expect this gap in performance between DeepSpeed and Megatron-LM to increase further for larger sequence lengths as we improve the performance of the local sparse attention implementation in future. Furthermore, we conduct parallel scaling studies of DeepSpeed-Ulysses along two axes. First, we fix sequence length at 131,072 tokens and increase GPU count from 64 to 256. Second, we increase the GPU count proportionally to the increase in sequence length. The results of these experiments are shown in Tables 2 and 3 respectively. For both evaluations, we used GPT-7B dense model at global batch size of 8. The tables show iteration time in microseconds as well as the achieved throughput measured in per GPU TFLOPs. Table 2 can be interpreted as strong scaling and shows that execution time decreases almost linearly as we increase the GPU count. Table 3 on the other hand, is a form of weak scaling (not in the traditional sense) with caveat that attention computation, a function of sequence length, is quadratic in complexity. In other words, as we increase sequence length, the work increases quadratically.",
            "reference_string": "[262826014 | Jacobs et al. | 2023 | Citations: 120]"
        },
        {
            "title": "Linear Attention Sequence Parallelism",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 65,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.02882, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2225238340",
                    "name": "Weigao Sun"
                },
                {
                    "authorId": "2171650015",
                    "name": "Zhen Qin"
                },
                {
                    "authorId": "2179703418",
                    "name": "Dong Li"
                },
                {
                    "authorId": "2116517206",
                    "name": "Xuyang Shen"
                },
                {
                    "authorId": "2281747139",
                    "name": "Yu Qiao"
                },
                {
                    "authorId": "2266275708",
                    "name": "Yiran Zhong"
                }
            ],
            "abstract": "Sequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$ longer than existing SP methods. Code is available at: https://github.com/OpenNLPLab/LASP.",
            "corpus_id": 268875936,
            "sentences": [
                {
                    "corpus_id": "268875936",
                    "title": "Linear Attention Sequence Parallelism",
                    "text": "Sequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$ longer than existing SP methods. Code is available at: https://github.com/OpenNLPLab/LASP.",
                    "score": 0.6972127476534055,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86767578125
                },
                {
                    "corpus_id": "268875936",
                    "title": "Linear Attention Sequence Parallelism",
                    "text": "Linear sequence modeling methods (Katharopoulos et al., 2020;Choromanski et al., 2022;Sun et al., 2025a) including linear attention (Qin et al., 2024d), state space models (Dao & Gu, 2024) and linear RNN (Qin et al., 2024e), are becoming increasingly popular due to their faster training and inference speed and comparable modeling performance to vanilla Softmax attention-based transformer models (Vaswani et al., 2017;Zeng et al., 2022;Touvron et al., 2023a;b;Team, 2023). The hybrid architecture, which interleaves Softmax attention and linear attention Transformer layers, has proven to be an effective balance between their respective strengths. This approach has been successfully implemented in large-scale commercial models such as Minimax-01 (Li et al., 2025) and Tencent Hunyuan Turbo-S (Tencent, 2025), as well as in smaller-scale hybrid models like Samba (Ren et al., 2024), Jamba (Lieber et al., 2024). \n\nAs the size of large language models (LLMs) increases and sequence lengths extend, the capacity limitations of single GPU's memory become a significant challenge, constraining the maximum sequence length manageable by a large model. To address this, Sequence Parallelism (SP) techniques (Li et al., 2022;Korthikanti et al., 2022) are employed, which partition a long sequence into multiple sub-sequences to be processed on separate devices. However, current implementations of SP methods do not fully exploit the right-product advantages of linear-complexity attention mechanisms Qin et al. (2024b). This results in less than optimal parallelism efficiency and reduced usability on linear sequence modeling methods. \n\nIn this paper, we present Linear Attention Sequence Parallelism (LASP) approach for efficient SP on models with linear sequence modeling. Our approach takes linear attention (Katharopoulos et al., 2020) as an instance to design a sophisticated point-to-point (P2P) ring-style communication mechanism during both forward and backward among devices within a node or across multiple nodes.",
                    "score": 0.6016893976547616,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 474
                        },
                        {
                            "start": 475,
                            "end": 650
                        },
                        {
                            "start": 651,
                            "end": 915
                        },
                        {
                            "start": 918,
                            "end": 1150
                        },
                        {
                            "start": 1151,
                            "end": 1358
                        },
                        {
                            "start": 1359,
                            "end": 1517
                        },
                        {
                            "start": 1518,
                            "end": 1633
                        },
                        {
                            "start": 1636,
                            "end": 1773
                        },
                        {
                            "start": 1774,
                            "end": 2022
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 33,
                            "end": 61,
                            "matchedPaperCorpusId": "220250819"
                        },
                        {
                            "start": 86,
                            "end": 104,
                            "matchedPaperCorpusId": "267312207"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71875
                },
                {
                    "corpus_id": "268875936",
                    "title": "Linear Attention Sequence Parallelism",
                    "text": "Memory-Efficient Attention. Rabe & Staats (2021) first employs the online Softmax technique to efficiently compute numerically stable attention scores sequentially, resulting in a linear memory for attention, yet still needs quadratic time complexity. While FlashAttention (Dao et al., 2022;Dao, 2023) employs tiling to minimize the number of memory reads/writes between GPU's high bandwidth memory (HBM) and on-chip SRAM to reduce time and memory in the training process, PagedAttention (Kwon et al., 2023) optimizes the utilization of the KV cache memory by reducing waste and enabling adaptable sharing among batched requests during inference. Ring Attention (Liu et al., 2023) reduces memory requirements for Transformer models when handling long sequences by distributing sequences across multiple devices and overlapping the communication of key-value blocks with blockwise attention computation. \n\nSequence Parallelism. SP as a widely used method to train long sequences has been integrated into many large model training frameworks, including Megatron-LM, DeepSpeed, and Colossal-AI. Megatron-LM (Shoeybi et al., 2019) implements SP along with model (tensor) parallelism (MP) to perform large matrix multiplications on GPUs. However, MP partitions the attention heads, which limits the maximum parallelism degree to be less than the number of attention heads. DeepSpeed-Ulysses (Jacobs et al., 2023) uses an all-to-all communication primitive to reduce communication volume, but also partitions attention heads and faces similar issues as Megatron-LM.",
                    "score": 0.6385612949265947,
                    "section_title": "Related Work",
                    "char_start_offset": 6137,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 27
                        },
                        {
                            "start": 28,
                            "end": 251
                        },
                        {
                            "start": 252,
                            "end": 646
                        },
                        {
                            "start": 647,
                            "end": 902
                        },
                        {
                            "start": 905,
                            "end": 926
                        },
                        {
                            "start": 927,
                            "end": 1091
                        },
                        {
                            "start": 1092,
                            "end": 1232
                        },
                        {
                            "start": 1233,
                            "end": 1367
                        },
                        {
                            "start": 1368,
                            "end": 1559
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69189453125
                }
            ],
            "relevance_judgement": 0.86767578125,
            "relevance_judgment_input_expanded": "# Title: Linear Attention Sequence Parallelism\n# Venue: arXiv.org\n# Authors: Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong\n## Abstract\nSequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$ longer than existing SP methods. Code is available at: https://github.com/OpenNLPLab/LASP.\n## Introduction\nLinear sequence modeling methods (Katharopoulos et al., 2020;Choromanski et al., 2022;Sun et al., 2025a) including linear attention (Qin et al., 2024d), state space models (Dao & Gu, 2024) and linear RNN (Qin et al., 2024e), are becoming increasingly popular due to their faster training and inference speed and comparable modeling performance to vanilla Softmax attention-based transformer models (Vaswani et al., 2017;Zeng et al., 2022;Touvron et al., 2023a;b;Team, 2023). The hybrid architecture, which interleaves Softmax attention and linear attention Transformer layers, has proven to be an effective balance between their respective strengths. This approach has been successfully implemented in large-scale commercial models such as Minimax-01 (Li et al., 2025) and Tencent Hunyuan Turbo-S (Tencent, 2025), as well as in smaller-scale hybrid models like Samba (Ren et al., 2024), Jamba (Lieber et al., 2024). \n\nAs the size of large language models (LLMs) increases and sequence lengths extend, the capacity limitations of single GPU's memory become a significant challenge, constraining the maximum sequence length manageable by a large model. To address this, Sequence Parallelism (SP) techniques (Li et al., 2022;Korthikanti et al., 2022) are employed, which partition a long sequence into multiple sub-sequences to be processed on separate devices. However, current implementations of SP methods do not fully exploit the right-product advantages of linear-complexity attention mechanisms Qin et al. (2024b). This results in less than optimal parallelism efficiency and reduced usability on linear sequence modeling methods. \n\nIn this paper, we present Linear Attention Sequence Parallelism (LASP) approach for efficient SP on models with linear sequence modeling. Our approach takes linear attention (Katharopoulos et al., 2020) as an instance to design a sophisticated point-to-point (P2P) ring-style communication mechanism during both forward and backward among devices within a node or across multiple nodes.\n\n## Related Work\nMemory-Efficient Attention. Rabe & Staats (2021) first employs the online Softmax technique to efficiently compute numerically stable attention scores sequentially, resulting in a linear memory for attention, yet still needs quadratic time complexity. While FlashAttention (Dao et al., 2022;Dao, 2023) employs tiling to minimize the number of memory reads/writes between GPU's high bandwidth memory (HBM) and on-chip SRAM to reduce time and memory in the training process, PagedAttention (Kwon et al., 2023) optimizes the utilization of the KV cache memory by reducing waste and enabling adaptable sharing among batched requests during inference. Ring Attention (Liu et al., 2023) reduces memory requirements for Transformer models when handling long sequences by distributing sequences across multiple devices and overlapping the communication of key-value blocks with blockwise attention computation. \n\nSequence Parallelism. SP as a widely used method to train long sequences has been integrated into many large model training frameworks, including Megatron-LM, DeepSpeed, and Colossal-AI. Megatron-LM (Shoeybi et al., 2019) implements SP along with model (tensor) parallelism (MP) to perform large matrix multiplications on GPUs. However, MP partitions the attention heads, which limits the maximum parallelism degree to be less than the number of attention heads. DeepSpeed-Ulysses (Jacobs et al., 2023) uses an all-to-all communication primitive to reduce communication volume, but also partitions attention heads and faces similar issues as Megatron-LM.",
            "reference_string": "[268875936 | Sun et al. | 2024 | Citations: 2]"
        },
        {
            "title": "USP: A Unified Sequence Parallelism Approach for Long Context Generative AI",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 24,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.07719, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301458823",
                    "name": "Jiarui Fang"
                },
                {
                    "authorId": "2301166902",
                    "name": "Shangchun Zhao"
                }
            ],
            "abstract": "Sequence parallelism (SP), which divides the sequence dimension of input tensors across multiple computational devices, is becoming key to unlocking the long-context capabilities of generative AI models. This paper investigates the state-of-the-art SP approaches, i.e. DeepSpeed-Ulysses and Ring-Attention, and proposes a unified SP approach, which is more robust to transformer model architectures and network hardware topology. This paper compares the communication and memory cost of SP and existing parallelism, including data/tensor/zero/pipeline parallelism, and discusses the best practices for designing hybrid 4D parallelism involving SP. We achieved 47% MFU on two 8xA800 nodes using SP for the LLAMA3-8B model training using sequence length 208K. Our code is publicly available at https://github.com/feifeibear/long-context-attention.",
            "corpus_id": 269757781,
            "sentences": [
                {
                    "corpus_id": "269757781",
                    "title": "USP: A Unified Sequence Parallelism Approach for Long Context Generative AI",
                    "text": "The field of artificial intelligence is witnessing a trend as the context length in generative AI models grows ever longer.Claude has pioneered this trend in large language models (LLMs) by extending the sequence length to 100K tokens.Following closely in its footsteps, OpenAI's GPT-4 has expanded the context length to 128K tokens.The advent of multi-modality models is propelling this trend forward, with Gemini 1.5 Pro boasting a context length of a staggering 10 million tokens, and OpenAI's Sora, a Diffusion Model, accommodating at least 1 million visual tokens.These breakthroughs underscore the imperative for generative AI techniques to adeptly handle a larger context length.\n\nSequence Parallelism (SP), a technique that partitions input sequences, has emerged as a promising approach for the training or inference of longer sequences.Following an initial exploration period of two years, by the latter of 2023, two landmark works, DeepSpeed-Ulysses [1] and Ring-Attention [2], marked the maturation of the SP technique.DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally, while Ring-Attention hides P2P communication costs introduced by SP through overlapping computation and communication.However, challenges remain, such as the SP parallel degree of DeepSpeed-Ulysses is limited to less than the number of attention heads, and the computational efficiency of Ring-Attention degrading due to the subdivision of matrix multiplications.These limitations currently hinder the broader adoption of Sequence Parallelism in distributed Transformer computation.\n\nIn this paper, we delve deeper into the realm of SP.We begin by highlighting that Ulysses and Ring are not mutually exclusive approaches; they can be combined through a hybrid parallel strategy to mitigate their drawbacks.Then, we discussed the relationship between SP and data/tensor/zero/expert/pipeline parallelism.The most complex among these is the relationship between SP and tensor parallelism.Since tensor parallelism also has its specific sequence parallel optimizations to reduce activation memory cost [3].For each parallelism approach, whether SP should replace it or is there some issue to using SP with it together, remains an open question.",
                    "score": 0.637862553788059,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 123
                        },
                        {
                            "start": 123,
                            "end": 235
                        },
                        {
                            "start": 235,
                            "end": 333
                        },
                        {
                            "start": 333,
                            "end": 569
                        },
                        {
                            "start": 569,
                            "end": 686
                        },
                        {
                            "start": 688,
                            "end": 846
                        },
                        {
                            "start": 846,
                            "end": 1031
                        },
                        {
                            "start": 1031,
                            "end": 1278
                        },
                        {
                            "start": 1278,
                            "end": 1523
                        },
                        {
                            "start": 1523,
                            "end": 1642
                        },
                        {
                            "start": 1644,
                            "end": 1696
                        },
                        {
                            "start": 1696,
                            "end": 1866
                        },
                        {
                            "start": 1866,
                            "end": 1962
                        },
                        {
                            "start": 1962,
                            "end": 2045
                        },
                        {
                            "start": 2045,
                            "end": 2161
                        },
                        {
                            "start": 2161,
                            "end": 2299
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 2157,
                            "end": 2160,
                            "matchedPaperCorpusId": "248693351"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8671875
                },
                {
                    "corpus_id": "269757781",
                    "title": "USP: A Unified Sequence Parallelism Approach for Long Context Generative AI",
                    "text": "Sequence parallelism (SP), which divides the sequence dimension of input tensors across multiple computational devices, is becoming key to unlocking the long-context capabilities of generative AI models. This paper investigates the state-of-the-art SP approaches, i.e. DeepSpeed-Ulysses and Ring-Attention, and proposes a unified SP approach, which is more robust to transformer model architectures and network hardware topology. This paper compares the communication and memory cost of SP and existing parallelism, including data/tensor/zero/pipeline parallelism, and discusses the best practices for designing hybrid 4D parallelism involving SP. We achieved 47% MFU on two 8xA800 nodes using SP for the LLAMA3-8B model training using sequence length 208K. Our code is publicly available at https://github.com/feifeibear/long-context-attention.",
                    "score": 0.6709898285133628,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83203125
                }
            ],
            "relevance_judgement": 0.8671875,
            "relevance_judgment_input_expanded": "# Title: USP: A Unified Sequence Parallelism Approach for Long Context Generative AI\n# Venue: arXiv.org\n# Authors: Jiarui Fang, Shangchun Zhao\n## Abstract\nSequence parallelism (SP), which divides the sequence dimension of input tensors across multiple computational devices, is becoming key to unlocking the long-context capabilities of generative AI models. This paper investigates the state-of-the-art SP approaches, i.e. DeepSpeed-Ulysses and Ring-Attention, and proposes a unified SP approach, which is more robust to transformer model architectures and network hardware topology. This paper compares the communication and memory cost of SP and existing parallelism, including data/tensor/zero/pipeline parallelism, and discusses the best practices for designing hybrid 4D parallelism involving SP. We achieved 47% MFU on two 8xA800 nodes using SP for the LLAMA3-8B model training using sequence length 208K. Our code is publicly available at https://github.com/feifeibear/long-context-attention.\n## Introduction\nThe field of artificial intelligence is witnessing a trend as the context length in generative AI models grows ever longer.Claude has pioneered this trend in large language models (LLMs) by extending the sequence length to 100K tokens.Following closely in its footsteps, OpenAI's GPT-4 has expanded the context length to 128K tokens.The advent of multi-modality models is propelling this trend forward, with Gemini 1.5 Pro boasting a context length of a staggering 10 million tokens, and OpenAI's Sora, a Diffusion Model, accommodating at least 1 million visual tokens.These breakthroughs underscore the imperative for generative AI techniques to adeptly handle a larger context length.\n\nSequence Parallelism (SP), a technique that partitions input sequences, has emerged as a promising approach for the training or inference of longer sequences.Following an initial exploration period of two years, by the latter of 2023, two landmark works, DeepSpeed-Ulysses [1] and Ring-Attention [2], marked the maturation of the SP technique.DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally, while Ring-Attention hides P2P communication costs introduced by SP through overlapping computation and communication.However, challenges remain, such as the SP parallel degree of DeepSpeed-Ulysses is limited to less than the number of attention heads, and the computational efficiency of Ring-Attention degrading due to the subdivision of matrix multiplications.These limitations currently hinder the broader adoption of Sequence Parallelism in distributed Transformer computation.\n\nIn this paper, we delve deeper into the realm of SP.We begin by highlighting that Ulysses and Ring are not mutually exclusive approaches; they can be combined through a hybrid parallel strategy to mitigate their drawbacks.Then, we discussed the relationship between SP and data/tensor/zero/expert/pipeline parallelism.The most complex among these is the relationship between SP and tensor parallelism.Since tensor parallelism also has its specific sequence parallel optimizations to reduce activation memory cost [3].For each parallelism approach, whether SP should replace it or is there some issue to using SP with it together, remains an open question.",
            "reference_string": "[269757781 | Fang et al. | 2024 | Citations: 24]"
        },
        {
            "title": "Achieving Peak Performance for Large Language Models: A Systematic Review",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 98,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3424945",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.04833, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1470666105",
                    "name": "Z. R. K. Rostam"
                },
                {
                    "authorId": "3208184",
                    "name": "S. Sz\u00e9n\u00e1si"
                },
                {
                    "authorId": "9717627",
                    "name": "G\u00e1bor Kert\u00e9sz"
                }
            ],
            "abstract": "In recent years, large language models (LLMs) have achieved remarkable success in natural language processing (NLP). LLMs require an extreme amount of parameters to attain high performance. As models grow into the trillion-parameter range, computational and memory costs increase significantly. This makes it difficult for many researchers to access the resources needed to train or apply these models. Optimizing LLM performance involves two main approaches: fine-tuning pre-trained models for specific tasks to achieve state-of-the-art performance, and reducing costs or improving training time while maintaining similar performance. This paper presents a systematic literature review (SLR) following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement. We reviewed 65 publications out of 983 from 2017 to December 2023, retrieved from 5 databases. The study presents methods to optimize and accelerate LLMs while achieving cutting-edge results without sacrificing accuracy. We begin with an overview of the development of language modeling, followed by a detailed explanation of commonly used frameworks and libraries, and a taxonomy for improving and speeding up LLMs based on three classes: LLM training, LLM inference, and system serving. We then delve into recent optimization and acceleration strategies such as training optimization, hardware optimization, scalability and reliability, accompanied by the taxonomy and categorization of these strategies. Finally, we provide an in-depth comparison of each class and strategy, with two case studies on optimizing model training and enhancing inference efficiency. These case studies showcase practical approaches to address LLM resource limitations while maintaining performance.",
            "corpus_id": 271083368,
            "sentences": [
                {
                    "corpus_id": "271083368",
                    "title": "Achieving Peak Performance for Large Language Models: A Systematic Review",
                    "text": "Sequence parallelism [15], [89], is a novel approach proposed to efficiently train Transformers with longer sequences on GPUs. It addresses the quadratic memory requirements of self-attention in Transformer models. Unlike traditional methods, it does not require a single device to handle the entire sequence. By splitting sequences into chunks and distributing them across devices, it achieves effective training with infinitely long sequences. It introduces Ring Self-Attention to enhance the process, demonstrating superior performance in batch size and sequence length compared to tensor parallelism, handling sequences over 27\u00d7 longer than existing methods.",
                    "score": 0.78715895046751,
                    "section_title": "5) Sequence Parallelism",
                    "char_start_offset": 92308,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 126
                        },
                        {
                            "start": 127,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 662
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 21,
                            "end": 25,
                            "matchedPaperCorpusId": "240070340"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85498046875
                }
            ],
            "relevance_judgement": 0.85498046875,
            "relevance_judgment_input_expanded": "# Title: Achieving Peak Performance for Large Language Models: A Systematic Review\n# Venue: IEEE Access\n# Authors: Z. R. K. Rostam, S. Sz\u00e9n\u00e1si, G\u00e1bor Kert\u00e9sz\n## Abstract\nIn recent years, large language models (LLMs) have achieved remarkable success in natural language processing (NLP). LLMs require an extreme amount of parameters to attain high performance. As models grow into the trillion-parameter range, computational and memory costs increase significantly. This makes it difficult for many researchers to access the resources needed to train or apply these models. Optimizing LLM performance involves two main approaches: fine-tuning pre-trained models for specific tasks to achieve state-of-the-art performance, and reducing costs or improving training time while maintaining similar performance. This paper presents a systematic literature review (SLR) following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement. We reviewed 65 publications out of 983 from 2017 to December 2023, retrieved from 5 databases. The study presents methods to optimize and accelerate LLMs while achieving cutting-edge results without sacrificing accuracy. We begin with an overview of the development of language modeling, followed by a detailed explanation of commonly used frameworks and libraries, and a taxonomy for improving and speeding up LLMs based on three classes: LLM training, LLM inference, and system serving. We then delve into recent optimization and acceleration strategies such as training optimization, hardware optimization, scalability and reliability, accompanied by the taxonomy and categorization of these strategies. Finally, we provide an in-depth comparison of each class and strategy, with two case studies on optimizing model training and enhancing inference efficiency. These case studies showcase practical approaches to address LLM resource limitations while maintaining performance.\n## 5) Sequence Parallelism\nSequence parallelism [15], [89], is a novel approach proposed to efficiently train Transformers with longer sequences on GPUs. It addresses the quadratic memory requirements of self-attention in Transformer models. Unlike traditional methods, it does not require a single device to handle the entire sequence. By splitting sequences into chunks and distributing them across devices, it achieves effective training with infinitely long sequences. It introduces Ring Self-Attention to enhance the process, demonstrating superior performance in batch size and sequence length compared to tensor parallelism, handling sequences over 27\u00d7 longer than existing methods.",
            "reference_string": "[271083368 | Rostam et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Sequence Length Scaling in Vision Transformers for Scientific Images on Frontier",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.15780, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3395528",
                    "name": "A. Tsaris"
                },
                {
                    "authorId": "2303676556",
                    "name": "Chengming Zhang"
                },
                {
                    "authorId": "2265563075",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2244593486",
                    "name": "Junqi Yin"
                },
                {
                    "authorId": "2257073765",
                    "name": "Siyan Liu"
                },
                {
                    "authorId": "9259766",
                    "name": "M. Ashfaq"
                },
                {
                    "authorId": "2180823649",
                    "name": "Ming Fan"
                },
                {
                    "authorId": "2298006038",
                    "name": "Jong-Youl Choi"
                },
                {
                    "authorId": "1700692",
                    "name": "M. Wahib"
                },
                {
                    "authorId": "2258305751",
                    "name": "Dan Lu"
                },
                {
                    "authorId": "2138151793",
                    "name": "Prasanna Balaprakash"
                },
                {
                    "authorId": "2244140922",
                    "name": "Feiyi Wang"
                }
            ],
            "abstract": "Vision Transformers (ViTs) are pivotal for foundational models in scientific imagery, including Earth science applications, due to their capability to process large sequence lengths. While transformers for text has inspired scaling sequence lengths in ViTs, yet adapting these for ViTs introduces unique challenges. We develop distributed sequence parallelism for ViTs, enabling them to handle up to 1M tokens. Our approach, leveraging DeepSpeed-Ulysses and Long-Sequence-Segmentation with model sharding, is the first to apply sequence parallelism in ViT training, achieving a 94% batch scaling efficiency on 2,048 AMD-MI250X GPUs. Evaluating sequence parallelism in ViTs, particularly in models up to 10B parameters, highlighted substantial bottlenecks. We countered these with hybrid sequence, pipeline, tensor parallelism, and flash attention strategies, to scale beyond single GPU memory limits. Our method significantly enhances climate modeling accuracy by 20% in temperature predictions, marking the first training of a transformer model on a full-attention matrix over 188K sequence length.",
            "corpus_id": 270062939,
            "sentences": [
                {
                    "corpus_id": "270062939",
                    "title": "Sequence Length Scaling in Vision Transformers for Scientific Images on Frontier",
                    "text": "Sequence parallelism, which distributes the self-attention computation in transformer models across the sequence dimension, has been an active area of research, with many developed methods employing various approaches.DeepSpeed-Ulysses [15]introduces a suite of system optimizations specifically designed for the efficient training of extreme long sequence transformer models.DeepSpeed-Ulysses partitions the input data along the sequence dimension, which allows the system to handle much longer sequences.For attention computation, it employs an all-to-all collective communication to ensure that each GPU receives a complete sequence, but only for a non-overlapping subset of the attention heads, allowing for parallel computation of attention across GPUs.LSS [16] divides a long sequence into segments distributed among GPUs, with each GPU computing a partial selfattention for its segment.It also introduces a fused communication strategy to minimize the communication overhead.Dynamic Sequence Parallelism (DSP) [22] enables efficient sequence parallelism for multi-dimensional transformers by dynamically switching the parallelism dimension according to the current computation stage.Ring attention [23], [24] is another variant that organizes tokens in a ring structure, allowing each token to attend to a fixed number of adjacent tokens in the sequence.This method offers a compromise between dense and sparse attention, balancing computational efficiency with the ability to capture local and semi-distant relationships.However, ring attention may still fall short in tasks where long-range dependencies are crucial.",
                    "score": 0.7658026210688643,
                    "section_title": "E. Sequence Parallel (SP)",
                    "char_start_offset": 12371,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 218
                        },
                        {
                            "start": 218,
                            "end": 376
                        },
                        {
                            "start": 376,
                            "end": 506
                        },
                        {
                            "start": 506,
                            "end": 758
                        },
                        {
                            "start": 758,
                            "end": 893
                        },
                        {
                            "start": 893,
                            "end": 982
                        },
                        {
                            "start": 982,
                            "end": 1190
                        },
                        {
                            "start": 1190,
                            "end": 1361
                        },
                        {
                            "start": 1361,
                            "end": 1529
                        },
                        {
                            "start": 1529,
                            "end": 1625
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8359375
                }
            ],
            "relevance_judgement": 0.8359375,
            "relevance_judgment_input_expanded": "# Title: Sequence Length Scaling in Vision Transformers for Scientific Images on Frontier\n# Venue: arXiv.org\n# Authors: A. Tsaris, Chengming Zhang, Xiao Wang, Junqi Yin, Siyan Liu, M. Ashfaq, Ming Fan, Jong-Youl Choi, M. Wahib, Dan Lu, Prasanna Balaprakash, Feiyi Wang\n## Abstract\nVision Transformers (ViTs) are pivotal for foundational models in scientific imagery, including Earth science applications, due to their capability to process large sequence lengths. While transformers for text has inspired scaling sequence lengths in ViTs, yet adapting these for ViTs introduces unique challenges. We develop distributed sequence parallelism for ViTs, enabling them to handle up to 1M tokens. Our approach, leveraging DeepSpeed-Ulysses and Long-Sequence-Segmentation with model sharding, is the first to apply sequence parallelism in ViT training, achieving a 94% batch scaling efficiency on 2,048 AMD-MI250X GPUs. Evaluating sequence parallelism in ViTs, particularly in models up to 10B parameters, highlighted substantial bottlenecks. We countered these with hybrid sequence, pipeline, tensor parallelism, and flash attention strategies, to scale beyond single GPU memory limits. Our method significantly enhances climate modeling accuracy by 20% in temperature predictions, marking the first training of a transformer model on a full-attention matrix over 188K sequence length.\n## E. Sequence Parallel (SP)\nSequence parallelism, which distributes the self-attention computation in transformer models across the sequence dimension, has been an active area of research, with many developed methods employing various approaches.DeepSpeed-Ulysses [15]introduces a suite of system optimizations specifically designed for the efficient training of extreme long sequence transformer models.DeepSpeed-Ulysses partitions the input data along the sequence dimension, which allows the system to handle much longer sequences.For attention computation, it employs an all-to-all collective communication to ensure that each GPU receives a complete sequence, but only for a non-overlapping subset of the attention heads, allowing for parallel computation of attention across GPUs.LSS [16] divides a long sequence into segments distributed among GPUs, with each GPU computing a partial selfattention for its segment.It also introduces a fused communication strategy to minimize the communication overhead.Dynamic Sequence Parallelism (DSP) [22] enables efficient sequence parallelism for multi-dimensional transformers by dynamically switching the parallelism dimension according to the current computation stage.Ring attention [23], [24] is another variant that organizes tokens in a ring structure, allowing each token to attend to a fixed number of adjacent tokens in the sequence.This method offers a compromise between dense and sparse attention, balancing computational efficiency with the ability to capture local and semi-distant relationships.However, ring attention may still fall short in tasks where long-range dependencies are crucial.",
            "reference_string": "[270062939 | Tsaris et al. | 2024 | Citations: 1]"
        },
        {
            "title": "CQS-Attention: Scaling Up the Standard Attention Computation for Infinitely Long Sequences",
            "venue": "IEEE Access",
            "year": 2025,
            "reference_count": 40,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2025.3544550",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2025.3544550?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2025.3544550, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2248690560",
                    "name": "Yiming Bian"
                },
                {
                    "authorId": "35088844",
                    "name": "Arun Somani"
                }
            ],
            "abstract": "Transformer models suffer from unaffordable high memory consumption when the sequence is long and standard self-attention is utilized. We developed a sequence parallelism scheme called CQS-Attention that can break the limit of sequence length. A long sequence is divided into multiple overlapping subsequences. The attention of each subsequence is independently computed and gathered as the final exact attention of the original long sequence. CQS-Attention is a fork-join parallel model comprising three components: Scheduler, Workers, and Tiler. The Scheduler equally partitions computation responsibility in a completely mutually exclusive manner and ensures the local subsequence length is minimum. Each worker independently computes the standard attention of the assigned subsequence and transfers local results to the Tiler, which produces the final attention. CQS-Attention makes attention computation embarrassingly parallel. Hence, it enjoys great performance regarding single-device memory and computation time consumption, mathematical stability and scalability. More importantly, it is fully compatible with all state-of-the-art attention optimizations. Our code and supplementary information (SI) are available at https://github.com/CQS-Attention/CQS_Attention.",
            "corpus_id": 276585307,
            "sentences": [
                {
                    "corpus_id": "276585307",
                    "title": "CQS-Attention: Scaling Up the Standard Attention Computation for Infinitely Long Sequences",
                    "text": "Transformer models suffer from unaffordable high memory consumption when the sequence is long and standard self-attention is utilized. We developed a sequence parallelism scheme called CQS-Attention that can break the limit of sequence length. A long sequence is divided into multiple overlapping subsequences. The attention of each subsequence is independently computed and gathered as the final exact attention of the original long sequence. CQS-Attention is a fork-join parallel model comprising three components: Scheduler, Workers, and Tiler. The Scheduler equally partitions computation responsibility in a completely mutually exclusive manner and ensures the local subsequence length is minimum. Each worker independently computes the standard attention of the assigned subsequence and transfers local results to the Tiler, which produces the final attention. CQS-Attention makes attention computation embarrassingly parallel. Hence, it enjoys great performance regarding single-device memory and computation time consumption, mathematical stability and scalability. More importantly, it is fully compatible with all state-of-the-art attention optimizations. Our code and supplementary information (SI) are available at https://github.com/CQS-Attention/CQS_Attention.",
                    "score": 0.5758763397795322,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83056640625
                }
            ],
            "relevance_judgement": 0.83056640625,
            "relevance_judgment_input_expanded": "# Title: CQS-Attention: Scaling Up the Standard Attention Computation for Infinitely Long Sequences\n# Venue: IEEE Access\n# Authors: Yiming Bian, Arun Somani\n## Abstract\nTransformer models suffer from unaffordable high memory consumption when the sequence is long and standard self-attention is utilized. We developed a sequence parallelism scheme called CQS-Attention that can break the limit of sequence length. A long sequence is divided into multiple overlapping subsequences. The attention of each subsequence is independently computed and gathered as the final exact attention of the original long sequence. CQS-Attention is a fork-join parallel model comprising three components: Scheduler, Workers, and Tiler. The Scheduler equally partitions computation responsibility in a completely mutually exclusive manner and ensures the local subsequence length is minimum. Each worker independently computes the standard attention of the assigned subsequence and transfers local results to the Tiler, which produces the final attention. CQS-Attention makes attention computation embarrassingly parallel. Hence, it enjoys great performance regarding single-device memory and computation time consumption, mathematical stability and scalability. More importantly, it is fully compatible with all state-of-the-art attention optimizations. Our code and supplementary information (SI) are available at https://github.com/CQS-Attention/CQS_Attention.\n",
            "reference_string": "[276585307 | Bian et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 23,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.01659, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2276535724",
                    "name": "Nathaniel Tomczak"
                },
                {
                    "authorId": "2873546",
                    "name": "S. Kuppannagari"
                }
            ],
            "abstract": "Transformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the input's context length - the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve\"true sparsity\"are lacking. In this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).",
            "corpus_id": 276106883,
            "sentences": [
                {
                    "corpus_id": "276106883",
                    "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
                    "text": "As the key objective of this work is to increase context length by exploiting sparsity, we discuss relevant works that focus on either achieving efficient sequence parallelism or on achieving efficient implementation of sparse masks. Sequence Parallelism: DeepSpeed Ulysses [16] achieves sequence parallelism for the original L 2 complexity attention by distributing the partitions of the K, Q, V matrices along the sequence dimensions across computing nodes. Megatron [17] performs sequence parallelism that is tightly coupled with their tensor parallelism [16]. Ring attention achieves sequence parallelism for block sparse attention masks [14]. LongNet achieves sequence parallelism using a dilated attention mask and requires all-gather of K, Q matrices [7]. On a single node, these techniques still rely on dense matrix operations for attention calculations. Our technique, that exploits sparsity, is orthogonal to these methods and has the potential to significantly scale the sequence length that can be achieved using these techniques for a fixed number of nodes. Efficient Sparse Attention Implementations: While works such as BigBird [6], Longformer [8], and Reformer [11] proposed sparse attention masks, their implementations in libraries such as PyTorch [9] and xFormers [10] still rely on dense operations. Specifically, they employ a variant of the Scaled Dot Product (SDP) attention [5] where they first perform a dense matrix multiplication of Q and K that can incorporate block sparsity for low-resolution masking, set the excess terms corresponding to the zero entries in the attention mask to \u2212\u221e, perform a row-wise softmax (which results in \u2212\u221e getting converted to 0) and finally a sparse-dense matrix multiplication is performed between the resultant matrix (sparse) and the V matrix (dense). \n\nAs a result, FlashAttention -a highly optimized version of SDP -has remained the most efficient attention implementation despite the fact that it performs the full L 2 computations [18]- [20]. Recent works [21], [22] have enabled support for sparsity by partitioning the attention mask and the K, Q, V matrices into blocks and only computing the blocks that have at least one non-zero element using the FlashAttention algorithm.",
                    "score": 0.6412553458871415,
                    "section_title": "III. RELATED WORKS",
                    "char_start_offset": 10382,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 233
                        },
                        {
                            "start": 234,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 563
                        },
                        {
                            "start": 564,
                            "end": 647
                        },
                        {
                            "start": 648,
                            "end": 762
                        },
                        {
                            "start": 763,
                            "end": 863
                        },
                        {
                            "start": 864,
                            "end": 1071
                        },
                        {
                            "start": 1072,
                            "end": 1320
                        },
                        {
                            "start": 1321,
                            "end": 1814
                        },
                        {
                            "start": 1817,
                            "end": 2009
                        },
                        {
                            "start": 2010,
                            "end": 2245
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1144,
                            "end": 1147,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 1399,
                            "end": 1402,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81494140625
                },
                {
                    "corpus_id": "276106883",
                    "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
                    "text": "Transformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the input's context length - the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve\"true sparsity\"are lacking. In this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).",
                    "score": 0.6782086569150801,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.403564453125
                }
            ],
            "relevance_judgement": 0.81494140625,
            "relevance_judgment_input_expanded": "# Title: Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques\n# Venue: arXiv.org\n# Authors: Nathaniel Tomczak, S. Kuppannagari\n## Abstract\nTransformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the input's context length - the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve\"true sparsity\"are lacking. In this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).\n## III. RELATED WORKS\nAs the key objective of this work is to increase context length by exploiting sparsity, we discuss relevant works that focus on either achieving efficient sequence parallelism or on achieving efficient implementation of sparse masks. Sequence Parallelism: DeepSpeed Ulysses [16] achieves sequence parallelism for the original L 2 complexity attention by distributing the partitions of the K, Q, V matrices along the sequence dimensions across computing nodes. Megatron [17] performs sequence parallelism that is tightly coupled with their tensor parallelism [16]. Ring attention achieves sequence parallelism for block sparse attention masks [14]. LongNet achieves sequence parallelism using a dilated attention mask and requires all-gather of K, Q matrices [7]. On a single node, these techniques still rely on dense matrix operations for attention calculations. Our technique, that exploits sparsity, is orthogonal to these methods and has the potential to significantly scale the sequence length that can be achieved using these techniques for a fixed number of nodes. Efficient Sparse Attention Implementations: While works such as BigBird [6], Longformer [8], and Reformer [11] proposed sparse attention masks, their implementations in libraries such as PyTorch [9] and xFormers [10] still rely on dense operations. Specifically, they employ a variant of the Scaled Dot Product (SDP) attention [5] where they first perform a dense matrix multiplication of Q and K that can incorporate block sparsity for low-resolution masking, set the excess terms corresponding to the zero entries in the attention mask to \u2212\u221e, perform a row-wise softmax (which results in \u2212\u221e getting converted to 0) and finally a sparse-dense matrix multiplication is performed between the resultant matrix (sparse) and the V matrix (dense). \n\nAs a result, FlashAttention -a highly optimized version of SDP -has remained the most efficient attention implementation despite the fact that it performs the full L 2 computations [18]- [20]. Recent works [21], [22] have enabled support for sparsity by partitioning the attention mask and the K, Q, V matrices into blocks and only computing the blocks that have at least one non-zero element using the FlashAttention algorithm.",
            "reference_string": "[276106883 | Tomczak et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.16978, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2266686678",
                    "name": "Jinghan Yao"
                },
                {
                    "authorId": "2297768912",
                    "name": "Sam Ade Jacobs"
                },
                {
                    "authorId": "2226706029",
                    "name": "Masahiro Tanaka"
                },
                {
                    "authorId": "2537545",
                    "name": "Olatunji Ruwase"
                },
                {
                    "authorId": "1685408",
                    "name": "A. Shafi"
                },
                {
                    "authorId": "1802958",
                    "name": "H. Subramoni"
                },
                {
                    "authorId": "2222521323",
                    "name": "Dhabaleswar K. Panda"
                }
            ],
            "abstract": "Large Language Models (LLMs) with long context capabilities are integral to complex tasks in natural language processing and computational biology, such as text generation and protein sequence analysis. However, training LLMs directly on extremely long contexts demands considerable GPU resources and increased memory, leading to higher costs and greater complexity. Alternative approaches that introduce long context capabilities via downstream finetuning or adaptations impose significant design limitations. In this paper, we propose Fully Pipelined Distributed Transformer (FPDT) for efficiently training long-context LLMs with extreme hardware efficiency. For GPT and Llama models, we achieve a 16x increase in sequence length that can be trained on the same hardware compared to current state-of-the-art solutions. With our dedicated sequence chunk pipeline design, we can now train 8B LLM with 2 million sequence length on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed FPDT is agnostic to existing training techniques and is proven to work efficiently across different LLM models.",
            "corpus_id": 272310078,
            "sentences": [
                {
                    "corpus_id": "272310078",
                    "title": "Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer",
                    "text": "Recent advancements in Transformer architectures have significantly enhanced their capability to process long sequences, which is crucial for tasks that require extensive contextual understanding. This section reviews pivotal contributions in this domain, each addressing the inherent memory limitations of standard Transformer models, while also pointing out some of their practical challenges. \n\nMegatron-SP (Korthikanti et al., 2023) adopts a sequence parallelism technique which is tightly integrated with its tensor parallelism. In this approach, sequences are partitioned along the sequence dimension, and all-gather and reduce-scatter collectives are employed to aggregate the QKV (query, key, value) projections for attention computation. The communication complexity analysis indicates that, in contrast to our approach, the communication volume in Megatron-SP's sequence parallelism increases linearly with the sequence length regardless of the number of compute devices. \n\nThe Blockwise Parallel Transformer (BPT) (Liu & Abbeel, 2024) employs a blockwise computation strategy for both self-attention and feedforward layers, optimizing memory usage and allowing the processing of sequences much longer than traditional Transformers. However, despite its efficiency, BPT requires careful tuning of block sizes and memory management to avoid diminishing returns on performance when scaling to extremely long sequences. \n\nRing Attention (Liu et al., 2023) enhances Transformer's scalability by distributing long sequences across multiple devices. This innovative approach overlaps the communication of key-value pairs with the computation of blockwise attention, effectively increasing the feasible sequence length proportionally to the number of available devices. However, reliance on device count for scaling and multi-step communications introduces potential issues in environments with sub-optimal hardware regards network interconnects, where performance can be unpredictably affected by network latency and bandwidth constraints. \n\nDeepSpeed Ulysses (Jacobs et al., 2023) tackles the challenges of sequence parallelism by partitioning input data along the sequence dimension and utilizing an efficient allto-all collective communication strategy for attention computations. Although this method maintains a constant communication volume regardless of the increase in sequence lengths and device counts, achieving significant speedups and scalability, it may still encounter practical hurdles in deployment related to large-scale clusters and the optimization of communication patterns across diverse computing environments.",
                    "score": 0.7464886684845268,
                    "section_title": "Long context training",
                    "char_start_offset": 6723,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 196
                        },
                        {
                            "start": 197,
                            "end": 395
                        },
                        {
                            "start": 398,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 746
                        },
                        {
                            "start": 747,
                            "end": 981
                        },
                        {
                            "start": 984,
                            "end": 1242
                        },
                        {
                            "start": 1243,
                            "end": 1426
                        },
                        {
                            "start": 1429,
                            "end": 1553
                        },
                        {
                            "start": 1554,
                            "end": 1772
                        },
                        {
                            "start": 1773,
                            "end": 2043
                        },
                        {
                            "start": 2046,
                            "end": 2287
                        },
                        {
                            "start": 2288,
                            "end": 2637
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 410,
                            "end": 435,
                            "matchedPaperCorpusId": "248693351"
                        },
                        {
                            "start": 1025,
                            "end": 1045,
                            "matchedPaperCorpusId": "266351737"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81103515625
                }
            ],
            "relevance_judgement": 0.81103515625,
            "relevance_judgment_input_expanded": "# Title: Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer\n# Venue: arXiv.org\n# Authors: Jinghan Yao, Sam Ade Jacobs, Masahiro Tanaka, Olatunji Ruwase, A. Shafi, H. Subramoni, Dhabaleswar K. Panda\n## Abstract\nLarge Language Models (LLMs) with long context capabilities are integral to complex tasks in natural language processing and computational biology, such as text generation and protein sequence analysis. However, training LLMs directly on extremely long contexts demands considerable GPU resources and increased memory, leading to higher costs and greater complexity. Alternative approaches that introduce long context capabilities via downstream finetuning or adaptations impose significant design limitations. In this paper, we propose Fully Pipelined Distributed Transformer (FPDT) for efficiently training long-context LLMs with extreme hardware efficiency. For GPT and Llama models, we achieve a 16x increase in sequence length that can be trained on the same hardware compared to current state-of-the-art solutions. With our dedicated sequence chunk pipeline design, we can now train 8B LLM with 2 million sequence length on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed FPDT is agnostic to existing training techniques and is proven to work efficiently across different LLM models.\n## Long context training\nRecent advancements in Transformer architectures have significantly enhanced their capability to process long sequences, which is crucial for tasks that require extensive contextual understanding. This section reviews pivotal contributions in this domain, each addressing the inherent memory limitations of standard Transformer models, while also pointing out some of their practical challenges. \n\nMegatron-SP (Korthikanti et al., 2023) adopts a sequence parallelism technique which is tightly integrated with its tensor parallelism. In this approach, sequences are partitioned along the sequence dimension, and all-gather and reduce-scatter collectives are employed to aggregate the QKV (query, key, value) projections for attention computation. The communication complexity analysis indicates that, in contrast to our approach, the communication volume in Megatron-SP's sequence parallelism increases linearly with the sequence length regardless of the number of compute devices. \n\nThe Blockwise Parallel Transformer (BPT) (Liu & Abbeel, 2024) employs a blockwise computation strategy for both self-attention and feedforward layers, optimizing memory usage and allowing the processing of sequences much longer than traditional Transformers. However, despite its efficiency, BPT requires careful tuning of block sizes and memory management to avoid diminishing returns on performance when scaling to extremely long sequences. \n\nRing Attention (Liu et al., 2023) enhances Transformer's scalability by distributing long sequences across multiple devices. This innovative approach overlaps the communication of key-value pairs with the computation of blockwise attention, effectively increasing the feasible sequence length proportionally to the number of available devices. However, reliance on device count for scaling and multi-step communications introduces potential issues in environments with sub-optimal hardware regards network interconnects, where performance can be unpredictably affected by network latency and bandwidth constraints. \n\nDeepSpeed Ulysses (Jacobs et al., 2023) tackles the challenges of sequence parallelism by partitioning input data along the sequence dimension and utilizing an efficient allto-all collective communication strategy for attention computations. Although this method maintains a constant communication volume regardless of the increase in sequence lengths and device counts, achieving significant speedups and scalability, it may still encounter practical hurdles in deployment related to large-scale clusters and the optimization of communication patterns across diverse computing environments.",
            "reference_string": "[272310078 | Yao et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training",
            "venue": "International Conference on Parallel Processing",
            "year": 2021,
            "reference_count": 45,
            "citation_count": 148,
            "influential_citation_count": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2110.14883",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2110.14883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153703322",
                    "name": "Shenggui Li"
                },
                {
                    "authorId": "1996240097",
                    "name": "Zhengda Bian"
                },
                {
                    "authorId": "2110176595",
                    "name": "Hongxin Liu"
                },
                {
                    "authorId": "49292014",
                    "name": "Boxiang Wang"
                },
                {
                    "authorId": "2146286212",
                    "name": "Haichen Huang"
                },
                {
                    "authorId": "2180329661",
                    "name": "Yongbin Li"
                },
                {
                    "authorId": "2118397214",
                    "name": "Chuan-Qing Wang"
                },
                {
                    "authorId": "2211340476",
                    "name": "Fan Cui"
                },
                {
                    "authorId": "144259229",
                    "name": "Yang You"
                }
            ],
            "abstract": "The success of Transformer models has pushed the deep learning model scale to billions of parameters, but the memory limitation of a single GPU has led to an urgent need for training on multi-GPU clusters. However, the best practice for choosing the optimal parallel strategy is still lacking, as it requires domain expertise in both deep learning and parallel computing. The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism and is integrated with heterogeneous training and zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.",
            "corpus_id": 240070340,
            "sentences": [
                {
                    "corpus_id": "240070340",
                    "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training",
                    "text": "Tensor parallelism mainly tackles the memory bottleneck brought by model data. However, the non-model data can be the bottleneck in applications such as AlphaFold and document-level text understanding. This is because these applications rely on long-sequence data. As the self-attention module in the Transformer layer is of quadratic complexity with respect to the sequence length, longsequence data will increase the memory usage consumed by the intermediate activation, limiting the training capability of the devices. \n\nSequence parallelism [21] is proposed to enable long-sequence modeling by breaking the memory wall brought by the large sequence dimension. In sequence parallelism, the model is replicated across devices just like data parallelism. The input data is split along the sequence dimension and each device only keeps a sub-sequence. The self-attention module is replaced with the Ring Self-Attention module such that the partial query, key, and value embeddings are exchanged among devices to complete the self-attention calculation.",
                    "score": 0.5642473185802919,
                    "section_title": "Sequence Parallelism",
                    "char_start_offset": 11663,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 78
                        },
                        {
                            "start": 79,
                            "end": 201
                        },
                        {
                            "start": 202,
                            "end": 264
                        },
                        {
                            "start": 265,
                            "end": 521
                        },
                        {
                            "start": 524,
                            "end": 663
                        },
                        {
                            "start": 664,
                            "end": 755
                        },
                        {
                            "start": 756,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 1052
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80712890625
                }
            ],
            "relevance_judgement": 0.80712890625,
            "relevance_judgment_input_expanded": "# Title: Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training\n# Venue: International Conference on Parallel Processing\n# Authors: Shenggui Li, Zhengda Bian, Hongxin Liu, Boxiang Wang, Haichen Huang, Yongbin Li, Chuan-Qing Wang, Fan Cui, Yang You\n## Abstract\nThe success of Transformer models has pushed the deep learning model scale to billions of parameters, but the memory limitation of a single GPU has led to an urgent need for training on multi-GPU clusters. However, the best practice for choosing the optimal parallel strategy is still lacking, as it requires domain expertise in both deep learning and parallel computing. The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism and is integrated with heterogeneous training and zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.\n## Sequence Parallelism\nTensor parallelism mainly tackles the memory bottleneck brought by model data. However, the non-model data can be the bottleneck in applications such as AlphaFold and document-level text understanding. This is because these applications rely on long-sequence data. As the self-attention module in the Transformer layer is of quadratic complexity with respect to the sequence length, longsequence data will increase the memory usage consumed by the intermediate activation, limiting the training capability of the devices. \n\nSequence parallelism [21] is proposed to enable long-sequence modeling by breaking the memory wall brought by the large sequence dimension. In sequence parallelism, the model is replicated across devices just like data parallelism. The input data is split along the sequence dimension and each device only keeps a sub-sequence. The self-attention module is replaced with the Ring Self-Attention module such that the partial query, key, and value embeddings are exchanged among devices to complete the self-attention calculation.",
            "reference_string": "[240070340 | Li et al. | 2021 | Citations: 148]"
        },
        {
            "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.07563, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2225238340",
                    "name": "Weigao Sun"
                },
                {
                    "authorId": "2344833625",
                    "name": "Disen Lan"
                },
                {
                    "authorId": "2266275708",
                    "name": "Yiran Zhong"
                },
                {
                    "authorId": "2265753258",
                    "name": "Xiaoye Qu"
                },
                {
                    "authorId": "2344895705",
                    "name": "Yu Cheng"
                }
            ],
            "abstract": "Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE.",
            "corpus_id": 276259019,
            "sentences": [
                {
                    "corpus_id": "276259019",
                    "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid",
                    "text": "Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE.",
                    "score": 0.6814620220617496,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.767578125
                }
            ],
            "relevance_judgement": 0.767578125,
            "relevance_judgment_input_expanded": "# Title: LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid\n# Venue: arXiv.org\n# Authors: Weigao Sun, Disen Lan, Yiran Zhong, Xiaoye Qu, Yu Cheng\n## Abstract\nLinear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE.\n",
            "reference_string": "[276259019 | Sun et al. | 2025 | Citations: 4]"
        },
        {
            "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 78,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.05447, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2346291295",
                    "name": "Weigao Sun"
                },
                {
                    "authorId": "2344833625",
                    "name": "Disen Lan"
                },
                {
                    "authorId": "2349575238",
                    "name": "Tong Zhu"
                },
                {
                    "authorId": "2265753258",
                    "name": "Xiaoye Qu"
                },
                {
                    "authorId": "2344895705",
                    "name": "Yu Cheng"
                }
            ],
            "abstract": "Linear Sequence Modeling (LSM) like linear attention, state space models and linear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant architectural improvements. In this paper, we introduce Linear-MoE, a production-level system for modeling and training large-scale models that integrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules for linear-complexity sequence modeling and MoE layers for sparsely activation, aiming to offer high performance with efficient training. The Linear-MoE system comprises: 1) Modeling subsystem, which provides a unified framework supporting all instances of LSM. and 2) Training subsystem, which facilitates efficient training by incorporating various advanced parallelism technologies, particularly Sequence Parallelism designed for Linear-MoE models. Additionally, we explore hybrid models that combine Linear-MoE layers with standard Transformer-MoE layers with its Sequence Parallelism to further enhance model flexibility and performance. Evaluations on two model series, A0.3B-2B and A1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining competitive performance on various benchmarks, showcasing its potential as a next-generation foundational model architecture. Code: https://github.com/OpenSparseLLMs/Linear-MoE.",
            "corpus_id": 276885221,
            "sentences": [
                {
                    "corpus_id": "276885221",
                    "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
                    "text": "The existing methods, LASP (Sun et al., 2024a) and its improved version LASP-2 (Sun et al., 2025), are designed specifically to leverage the right-productfirst property of linear attention techniques for efficient sequence parallelism (SP). LASP employs a point-to-point ring-style communication pattern, facilitating the exchange of incremental memory states across devices. This communication pattern is particularly effective for managing dependencies while minimizing the data transferred between devices, enhancing the scalability of SP. LASP-2 further refines this approach by replacing the ring-style communication with an all-gather collective communication operation, streamlining the entire communication process. This modification not only simplifies the communication structure but also improves the parallelism of computation and communication. \n\nIn this work, we extend the capabilities of LASP series to the Linear-MoE system, allowing for the efficient SP training on LSM modules, particularly when dealing with extremely long sequences across large-scale distributed clusters. This extension significantly enhances the scalability and efficiency of training large-scale Linear-MoE models with long-context sequences on extensive compute re- \n\nFigure 2: Sequence Parallelism Approach on Hybrid Linear-MoE models. We exemplify the parallelism on the hybrid layers of LSM and standard attention with both TP and SP (both have a dimension of 2). The communication operations colored in yellow and green are for TP and SP, respectively. AG/RS: all-gather in forward and reducescatter in backward, RS/AG: reduce-scatter in forward and all-gather in backward, AG/No: all-gather in forward and no-op in backward, No/AG: no-op in forward and all-gather in backward. Note that the SP communication operations for linear attention operate on the memory state M s \u2208 R d\u00d7d , while for standard attention, they operate on states K s , V s \u2208 R C\u00d7d . \n\nsources. A detailed breakdown of the SP algorithm on Linear-MoE, with and without masking, is provided in Appendix A.3.",
                    "score": 0.5589986310671853,
                    "section_title": "Sequence Parallelism on Linear-MoE",
                    "char_start_offset": 10488,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 240
                        },
                        {
                            "start": 241,
                            "end": 375
                        },
                        {
                            "start": 376,
                            "end": 542
                        },
                        {
                            "start": 543,
                            "end": 723
                        },
                        {
                            "start": 724,
                            "end": 857
                        },
                        {
                            "start": 860,
                            "end": 1093
                        },
                        {
                            "start": 1094,
                            "end": 1257
                        },
                        {
                            "start": 1260,
                            "end": 1328
                        },
                        {
                            "start": 1329,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1548
                        },
                        {
                            "start": 1549,
                            "end": 1773
                        },
                        {
                            "start": 1774,
                            "end": 1951
                        },
                        {
                            "start": 1954,
                            "end": 1962
                        },
                        {
                            "start": 1963,
                            "end": 2073
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 27,
                            "end": 46,
                            "matchedPaperCorpusId": "276259019"
                        },
                        {
                            "start": 79,
                            "end": 97,
                            "matchedPaperCorpusId": "276259019"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75830078125
                }
            ],
            "relevance_judgement": 0.75830078125,
            "relevance_judgment_input_expanded": "# Title: Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts\n# Venue: arXiv.org\n# Authors: Weigao Sun, Disen Lan, Tong Zhu, Xiaoye Qu, Yu Cheng\n## Abstract\nLinear Sequence Modeling (LSM) like linear attention, state space models and linear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant architectural improvements. In this paper, we introduce Linear-MoE, a production-level system for modeling and training large-scale models that integrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules for linear-complexity sequence modeling and MoE layers for sparsely activation, aiming to offer high performance with efficient training. The Linear-MoE system comprises: 1) Modeling subsystem, which provides a unified framework supporting all instances of LSM. and 2) Training subsystem, which facilitates efficient training by incorporating various advanced parallelism technologies, particularly Sequence Parallelism designed for Linear-MoE models. Additionally, we explore hybrid models that combine Linear-MoE layers with standard Transformer-MoE layers with its Sequence Parallelism to further enhance model flexibility and performance. Evaluations on two model series, A0.3B-2B and A1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining competitive performance on various benchmarks, showcasing its potential as a next-generation foundational model architecture. Code: https://github.com/OpenSparseLLMs/Linear-MoE.\n## Sequence Parallelism on Linear-MoE\nThe existing methods, LASP (Sun et al., 2024a) and its improved version LASP-2 (Sun et al., 2025), are designed specifically to leverage the right-productfirst property of linear attention techniques for efficient sequence parallelism (SP). LASP employs a point-to-point ring-style communication pattern, facilitating the exchange of incremental memory states across devices. This communication pattern is particularly effective for managing dependencies while minimizing the data transferred between devices, enhancing the scalability of SP. LASP-2 further refines this approach by replacing the ring-style communication with an all-gather collective communication operation, streamlining the entire communication process. This modification not only simplifies the communication structure but also improves the parallelism of computation and communication. \n\nIn this work, we extend the capabilities of LASP series to the Linear-MoE system, allowing for the efficient SP training on LSM modules, particularly when dealing with extremely long sequences across large-scale distributed clusters. This extension significantly enhances the scalability and efficiency of training large-scale Linear-MoE models with long-context sequences on extensive compute re- \n\nFigure 2: Sequence Parallelism Approach on Hybrid Linear-MoE models. We exemplify the parallelism on the hybrid layers of LSM and standard attention with both TP and SP (both have a dimension of 2). The communication operations colored in yellow and green are for TP and SP, respectively. AG/RS: all-gather in forward and reducescatter in backward, RS/AG: reduce-scatter in forward and all-gather in backward, AG/No: all-gather in forward and no-op in backward, No/AG: no-op in forward and all-gather in backward. Note that the SP communication operations for linear attention operate on the memory state M s \u2208 R d\u00d7d , while for standard attention, they operate on states K s , V s \u2208 R C\u00d7d . \n\nsources. A detailed breakdown of the SP algorithm on Linear-MoE, with and without masking, is provided in Appendix A.3.",
            "reference_string": "[276885221 | Sun et al. | 2025 | Citations: 4]"
        },
        {
            "title": "Blockwise Parallel Transformer for Large Context Models",
            "venue": "",
            "year": 2023,
            "reference_count": 61,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.19370, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2143856672",
                    "name": "Hao Liu"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                }
            ],
            "abstract": "Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.",
            "corpus_id": 258987968,
            "sentences": [
                {
                    "corpus_id": "258987968",
                    "title": "Blockwise Parallel Transformer for Large Context Models",
                    "text": "Transformers have garnered significant attention in the field of natural language processing (NLP) and have become the basis for numerous state-of-the-art models. Several works have explored memoryefficient techniques to address the memory limitations of Transformers and enable their application to longer input sequences. One line of research focuses on various approximation techniques or compressing along the sequence dimension [see e.g. 24,12,14,4,42,54,36,25]. Other works explored replacing attention [19,20,41,23,3,57,40,53]. Another line of work explores partitioning the large hidden dimension of the feedforward network into parts and retrieving only one part per token [30,48,17,26,58,60]. Additionally, extending the context by attending over states from previous sequences has been explored [13,44], as well as combining local and global contexts [21,11]. For a comprehensive review of these techniques, we recommend referring to the surveys by Tay et al. \n\n[51], Narang et al. [38], Tay et al. [50]. Several studies explored sharding large model on distributed devices tensor, data, or sequence parallelism [49,16,55,27,59,31,46]. Ours shares similarities with the sequence parallelism [27] where sequences are distributed across devices, in contrast, ours implements blockwise computation on sequences for each device. This creates an orthogonal relationship between our method and sequence parallelism, allowing for straightforward combination. In addition, our methodology is compatible with both tensor and data parallelism. Another direction involves computing exact self-attention in a blockwise manner using the tiling technique [37]. This approach has led to the development of memory efficient attention mechanisms [14,42]. In line with these advancements, our work falls into this category. We propose computing both the feedforward network and self-attention in a blockwise manner, resulting in a significant reduction in memory requirements.",
                    "score": 0.7560676711761268,
                    "section_title": "Related Work",
                    "char_start_offset": 20945,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 323
                        },
                        {
                            "start": 324,
                            "end": 442
                        },
                        {
                            "start": 443,
                            "end": 467
                        },
                        {
                            "start": 468,
                            "end": 534
                        },
                        {
                            "start": 535,
                            "end": 702
                        },
                        {
                            "start": 703,
                            "end": 870
                        },
                        {
                            "start": 871,
                            "end": 970
                        },
                        {
                            "start": 973,
                            "end": 1015
                        },
                        {
                            "start": 1016,
                            "end": 1146
                        },
                        {
                            "start": 1147,
                            "end": 1335
                        },
                        {
                            "start": 1336,
                            "end": 1462
                        },
                        {
                            "start": 1463,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1657
                        },
                        {
                            "start": 1658,
                            "end": 1748
                        },
                        {
                            "start": 1749,
                            "end": 1816
                        },
                        {
                            "start": 1817,
                            "end": 1969
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 443,
                            "end": 446,
                            "matchedPaperCorpusId": "232110866"
                        },
                        {
                            "start": 449,
                            "end": 452,
                            "matchedPaperCorpusId": "249151871"
                        },
                        {
                            "start": 509,
                            "end": 513,
                            "matchedPaperCorpusId": "221150566"
                        },
                        {
                            "start": 519,
                            "end": 522,
                            "matchedPaperCorpusId": "247011581"
                        },
                        {
                            "start": 689,
                            "end": 692,
                            "matchedPaperCorpusId": "231573431"
                        },
                        {
                            "start": 1142,
                            "end": 1145,
                            "matchedPaperCorpusId": "221191193"
                        },
                        {
                            "start": 1740,
                            "end": 1744,
                            "matchedPaperCorpusId": "249151871"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74169921875
                },
                {
                    "corpus_id": "258987968",
                    "title": "Blockwise Parallel Transformer for Large Context Models",
                    "text": "Section 3.1 provides a memory cost breakdown. \n\nDespite the resulting reduced memory requirements of the self-attention block in Transformer models, a significant challenge still arises from the feedforward network. This network contains a large number of parameters and produces high-dimensional intermediate vectors, resulting in substantial memory requirements. This issue is becomes the key memory challenge once employing memoryefficient attention mechanisms. Consequently, training Transformers on longer context lengths and scaling up Transformer models become significantly hindered due to the overwhelming memory demands imposed by the feedforward network. \n\nTo address this challenge, we make an important observation: when self-attention is computed in a blockwise manner to reduce memory requirements, it becomes feasible to merge the computation of the feedforward network. This eliminates the need to wait for the self-attention computation to finish before performing the feedforward step on the entire sequence. By computing the feedforward network on a block-by-block basis, we effectively reduce the memory cost associated with the feedforward network. This process involves the utilization of two nested loops over the input sequence blocks. In the outer loop, we iterate over each block and compute the query. In the inner loop, we iterate over each block to calculate the key and value. These key-value pairs, along with the query, are then used to compute the blockwise attention specific to the corresponding input block. This blockwise attention is subsequently used to calculate the output of the feedforward network, followed by a residual connection. This approach enables us to process longer input sequences while maintaining lower memory budget. Since our approach performs blockwise parallel computation and fuses the feedforward and self-attention computations, we name our method the Blockwise Parallel Transformer (BPT). \n\nWe evaluate the effectiveness of our approach on several benchmarks, including language modeling and reinforcement learning. Our experiments show that BPT can reduce the memory requirements of Transformers, enabling us to train 32 times longer sequence than vanilla attention [52] based GPT models and up to 4 times longer sequence than prior state-of-the-arts FlashAttention [14] and Memory Efficient Attention [42]. Furthermore, we demonstrate the application of BPT on the task of traning Transformer based RL agent.",
                    "score": 0.6446266327132574,
                    "section_title": "Introduction",
                    "char_start_offset": 2176,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 45
                        },
                        {
                            "start": 48,
                            "end": 215
                        },
                        {
                            "start": 216,
                            "end": 364
                        },
                        {
                            "start": 365,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 665
                        },
                        {
                            "start": 668,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1027
                        },
                        {
                            "start": 1028,
                            "end": 1170
                        },
                        {
                            "start": 1171,
                            "end": 1260
                        },
                        {
                            "start": 1261,
                            "end": 1329
                        },
                        {
                            "start": 1330,
                            "end": 1407
                        },
                        {
                            "start": 1408,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1677
                        },
                        {
                            "start": 1678,
                            "end": 1775
                        },
                        {
                            "start": 1776,
                            "end": 1954
                        },
                        {
                            "start": 1957,
                            "end": 2081
                        },
                        {
                            "start": 2082,
                            "end": 2374
                        },
                        {
                            "start": 2375,
                            "end": 2476
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 2333,
                            "end": 2337,
                            "matchedPaperCorpusId": "249151871"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.494384765625
                },
                {
                    "corpus_id": "258987968",
                    "title": "Blockwise Parallel Transformer for Large Context Models",
                    "text": "Transformers [52] have become the backbone of many state-of-the-art natural language processing models [15,43,5,35]. They have demonstrated impressive performance across a wide range of AI problems, including language modeling, machine translation, image captioning, and protein folding [39,47,32,43,5,45,9]. Transformers achieve this success through their architecture design that uses self-attention and position-wise feedforward mechanisms. These components facilitate the efficient capture of long-range dependencies between input tokens, enabling scalability in terms of context length and model size through highly parallel computations. \n\nHowever, the memory requirements of Transformers limit their ability to handle long sequences, which is necessary for many AI problems, such as high-resolution images, podcasts, code, or books and especially those that involve multiple long sequences or long-term dependencies [10,7,39,7,34,29,47,32,1]. The quadratic self-attention and the large feed forward network of Transformers require a large amount of memory, which makes it challenging to scale to longer input sequences. This limitation has led to various techniques proposed to reduce the memory requirements of Transformers, including sparse-approximation, low-rank approximation, and low precision approximation [see e.g. 51,24,22,11,25,36,54]. \n\nOne distinct line of research does not rely on approximation but instead focuses on computing exact self-attention with linear memory complexity. This approach leverages the observation that the softmax matrix in self-attention can be computed without materializing the full matrix [37]. This technique has led to the development of FlashAttention [14] and Memory Efficient Attention [42]. Both methods propose a blockwise computation of the self-attention softmax, demonstrating reduced memory requirements. (A), (B), and (C) show evaluation using one, eight A100, and 64 TPUv4, respectively, with a single sequence. Our method enables training sequences 32 times longer than vanilla attention-based Transformer [52], and 2 to 4 times longer than FlashAttention [14] and Memory Efficient Attention [42]. Section 3.1 provides a memory cost breakdown.",
                    "score": 0.6282041020594631,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 443
                        },
                        {
                            "start": 444,
                            "end": 643
                        },
                        {
                            "start": 646,
                            "end": 1126
                        },
                        {
                            "start": 1127,
                            "end": 1330
                        },
                        {
                            "start": 1331,
                            "end": 1353
                        },
                        {
                            "start": 1356,
                            "end": 1501
                        },
                        {
                            "start": 1502,
                            "end": 1643
                        },
                        {
                            "start": 1644,
                            "end": 1745
                        },
                        {
                            "start": 1746,
                            "end": 1864
                        },
                        {
                            "start": 1865,
                            "end": 1973
                        },
                        {
                            "start": 1974,
                            "end": 2160
                        },
                        {
                            "start": 2161,
                            "end": 2206
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 107,
                            "end": 110,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 110,
                            "end": 112,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 291,
                            "end": 294,
                            "matchedPaperCorpusId": "237260635"
                        },
                        {
                            "start": 294,
                            "end": 297,
                            "matchedPaperCorpusId": "246527904"
                        },
                        {
                            "start": 297,
                            "end": 300,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 300,
                            "end": 302,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 302,
                            "end": 305,
                            "matchedPaperCorpusId": "231939146"
                        },
                        {
                            "start": 940,
                            "end": 943,
                            "matchedPaperCorpusId": "237260635"
                        },
                        {
                            "start": 943,
                            "end": 946,
                            "matchedPaperCorpusId": "246527904"
                        },
                        {
                            "start": 946,
                            "end": 948,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 1331,
                            "end": 1334,
                            "matchedPaperCorpusId": "221702858"
                        },
                        {
                            "start": 1334,
                            "end": 1337,
                            "matchedPaperCorpusId": "232110866"
                        },
                        {
                            "start": 1704,
                            "end": 1708,
                            "matchedPaperCorpusId": "249151871"
                        },
                        {
                            "start": 2119,
                            "end": 2123,
                            "matchedPaperCorpusId": "249151871"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.3779296875
                }
            ],
            "relevance_judgement": 0.74169921875,
            "relevance_judgment_input_expanded": "# Title: Blockwise Parallel Transformer for Large Context Models\n# Venue: \n# Authors: Hao Liu, P. Abbeel\n## Abstract\nTransformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n## Introduction\nTransformers [52] have become the backbone of many state-of-the-art natural language processing models [15,43,5,35]. They have demonstrated impressive performance across a wide range of AI problems, including language modeling, machine translation, image captioning, and protein folding [39,47,32,43,5,45,9]. Transformers achieve this success through their architecture design that uses self-attention and position-wise feedforward mechanisms. These components facilitate the efficient capture of long-range dependencies between input tokens, enabling scalability in terms of context length and model size through highly parallel computations. \n\nHowever, the memory requirements of Transformers limit their ability to handle long sequences, which is necessary for many AI problems, such as high-resolution images, podcasts, code, or books and especially those that involve multiple long sequences or long-term dependencies [10,7,39,7,34,29,47,32,1]. The quadratic self-attention and the large feed forward network of Transformers require a large amount of memory, which makes it challenging to scale to longer input sequences. This limitation has led to various techniques proposed to reduce the memory requirements of Transformers, including sparse-approximation, low-rank approximation, and low precision approximation [see e.g. 51,24,22,11,25,36,54]. \n\nOne distinct line of research does not rely on approximation but instead focuses on computing exact self-attention with linear memory complexity. This approach leverages the observation that the softmax matrix in self-attention can be computed without materializing the full matrix [37]. This technique has led to the development of FlashAttention [14] and Memory Efficient Attention [42]. Both methods propose a blockwise computation of the self-attention softmax, demonstrating reduced memory requirements. (A), (B), and (C) show evaluation using one, eight A100, and 64 TPUv4, respectively, with a single sequence. Our method enables training sequences 32 times longer than vanilla attention-based Transformer [52], and 2 to 4 times longer than FlashAttention [14] and Memory Efficient Attention [42]. Section 3.1 provides a memory cost breakdown.\n...\nSection 3.1 provides a memory cost breakdown. \n\nDespite the resulting reduced memory requirements of the self-attention block in Transformer models, a significant challenge still arises from the feedforward network. This network contains a large number of parameters and produces high-dimensional intermediate vectors, resulting in substantial memory requirements. This issue is becomes the key memory challenge once employing memoryefficient attention mechanisms. Consequently, training Transformers on longer context lengths and scaling up Transformer models become significantly hindered due to the overwhelming memory demands imposed by the feedforward network. \n\nTo address this challenge, we make an important observation: when self-attention is computed in a blockwise manner to reduce memory requirements, it becomes feasible to merge the computation of the feedforward network. This eliminates the need to wait for the self-attention computation to finish before performing the feedforward step on the entire sequence. By computing the feedforward network on a block-by-block basis, we effectively reduce the memory cost associated with the feedforward network. This process involves the utilization of two nested loops over the input sequence blocks. In the outer loop, we iterate over each block and compute the query. In the inner loop, we iterate over each block to calculate the key and value. These key-value pairs, along with the query, are then used to compute the blockwise attention specific to the corresponding input block. This blockwise attention is subsequently used to calculate the output of the feedforward network, followed by a residual connection. This approach enables us to process longer input sequences while maintaining lower memory budget. Since our approach performs blockwise parallel computation and fuses the feedforward and self-attention computations, we name our method the Blockwise Parallel Transformer (BPT). \n\nWe evaluate the effectiveness of our approach on several benchmarks, including language modeling and reinforcement learning. Our experiments show that BPT can reduce the memory requirements of Transformers, enabling us to train 32 times longer sequence than vanilla attention [52] based GPT models and up to 4 times longer sequence than prior state-of-the-arts FlashAttention [14] and Memory Efficient Attention [42]. Furthermore, we demonstrate the application of BPT on the task of traning Transformer based RL agent.\n\n## Related Work\nTransformers have garnered significant attention in the field of natural language processing (NLP) and have become the basis for numerous state-of-the-art models. Several works have explored memoryefficient techniques to address the memory limitations of Transformers and enable their application to longer input sequences. One line of research focuses on various approximation techniques or compressing along the sequence dimension [see e.g. 24,12,14,4,42,54,36,25]. Other works explored replacing attention [19,20,41,23,3,57,40,53]. Another line of work explores partitioning the large hidden dimension of the feedforward network into parts and retrieving only one part per token [30,48,17,26,58,60]. Additionally, extending the context by attending over states from previous sequences has been explored [13,44], as well as combining local and global contexts [21,11]. For a comprehensive review of these techniques, we recommend referring to the surveys by Tay et al. \n\n[51], Narang et al. [38], Tay et al. [50]. Several studies explored sharding large model on distributed devices tensor, data, or sequence parallelism [49,16,55,27,59,31,46]. Ours shares similarities with the sequence parallelism [27] where sequences are distributed across devices, in contrast, ours implements blockwise computation on sequences for each device. This creates an orthogonal relationship between our method and sequence parallelism, allowing for straightforward combination. In addition, our methodology is compatible with both tensor and data parallelism. Another direction involves computing exact self-attention in a blockwise manner using the tiling technique [37]. This approach has led to the development of memory efficient attention mechanisms [14,42]. In line with these advancements, our work falls into this category. We propose computing both the feedforward network and self-attention in a blockwise manner, resulting in a significant reduction in memory requirements.",
            "reference_string": "[258987968 | Liu et al. | 2023 | Citations: 11]"
        },
        {
            "title": "InternEvo: Efficient Long-sequence Large Language Model Training via Hybrid Parallelism and Redundant Sharding",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 65,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.09149, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2189935212",
                    "name": "Qiaoling Chen"
                },
                {
                    "authorId": "1393328013",
                    "name": "Diandian Gu"
                },
                {
                    "authorId": "2263696698",
                    "name": "Guoteng Wang"
                },
                {
                    "authorId": "2293554318",
                    "name": "Xun Chen"
                },
                {
                    "authorId": "2279899091",
                    "name": "Yingtong Xiong"
                },
                {
                    "authorId": "2279784429",
                    "name": "Ting Huang"
                },
                {
                    "authorId": "2150570711",
                    "name": "Qi Hu"
                },
                {
                    "authorId": "2279869979",
                    "name": "Xin Jin"
                },
                {
                    "authorId": "2114783855",
                    "name": "Yonggang Wen"
                },
                {
                    "authorId": "2146333441",
                    "name": "Tianwei Zhang"
                },
                {
                    "authorId": "2075416290",
                    "name": "Peng Sun"
                }
            ],
            "abstract": "Large language models (LLMs) with long sequences begin to power more and more fundamentally new applications we use every day. Existing methods for long-sequence LLM training are neither efficient nor compatible with commonly-used training algorithms such as FlashAttention. We design InternEvo to address these issues. InternEvo decouples all of the sharding dimensions into a new hierarchical space, and systematically analyzes the memory and communication cost of LLM training. Then, it generates an effective hybrid parallelism strategy. We design a new selective overlap mechanism to mitigate the communication overhead introduced by the hybrid parallelism. We also implement memory management techniques to reduce GPU memory fragmentation. Evaluation results show that InternEvo generates parallelization strategies that match or outperform existing methods in model FLOPs utilization.",
            "corpus_id": 267027729,
            "sentences": [
                {
                    "corpus_id": "267027729",
                    "title": "InternEvo: Efficient Long-sequence Large Language Model Training via Hybrid Parallelism and Redundant Sharding",
                    "text": "Processing long sequences is crucial for supporting important applications such as dealing with longer histories in chat applications. To this end, sequence parallelism (SP) has emerged as a technique aimed at alleviating activation memory footprints during the training of Transformers. In SP, the input tensor of each Transformer layer is divided along the sequence dimension, allowing for parallel computation across multiple GPUs. This segmentation, in conjunction with activation recomputation, results in a substantial reduction in activation memory requirements by a factor of s sp . In this paper, we classify existing SP approaches into 3 stages, which correspond to slicing the sequence into Norm and Dropout modules, Linear modules, and MHA module. When enabled cumulatively: \n\nSP-1: Norm and Dropout modules. As shown in Figure 2 (b), Megatron-LM capitalizes on TP to parallelize the linear layers and MHA, which are the most time-consuming components during training. Simultaneously, it employs SP on Norm and Dropout modules, effectively reducing the activation memory of these layers by a factor of s sp , where s sp = s t p . To maintain consistency in computational results, it integrates necessary communications, including all-gather and reduce-scatter to transfer the activation in forward and backward passes. When the activation size increases with the sequence length, this way of communicating the activation will incur a high overhead. \n\nSP-2: Add Linear modules. DeepSpeed Ulysses utilizes sequence parallelism on Linear, Norm, and Dropout layers, as shown in Figure 2 (c). An all-to-all communi- cation is applied to the output of Linear qkv with a shape of B \u00d7 3 \u00d7 S/s sp \u00d7 H. This enables each GPU to receive the complete sequence of Q, K, and V (each with a shape of B \u00d7 S \u00d7 H/s sp ) for a subset of attention heads. Subsequently, FlashAttention is employed for highly efficient MHA computation with a constraint that the number of attention heads D should not exceed s sp . Another all-to-all communication is then used to gather the MHA layer results and the results are re-partitioned along the sequence dimension.",
                    "score": 0.6490428437307956,
                    "section_title": "Long-sequence Training",
                    "char_start_offset": 9473,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 135,
                            "end": 287
                        },
                        {
                            "start": 288,
                            "end": 434
                        },
                        {
                            "start": 435,
                            "end": 590
                        },
                        {
                            "start": 591,
                            "end": 759
                        },
                        {
                            "start": 760,
                            "end": 786
                        },
                        {
                            "start": 789,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 980
                        },
                        {
                            "start": 981,
                            "end": 1141
                        },
                        {
                            "start": 1142,
                            "end": 1330
                        },
                        {
                            "start": 1331,
                            "end": 1460
                        },
                        {
                            "start": 1463,
                            "end": 1488
                        },
                        {
                            "start": 1489,
                            "end": 1599
                        },
                        {
                            "start": 1600,
                            "end": 1704
                        },
                        {
                            "start": 1705,
                            "end": 1846
                        },
                        {
                            "start": 1847,
                            "end": 2004
                        },
                        {
                            "start": 2005,
                            "end": 2147
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68310546875
                }
            ],
            "relevance_judgement": 0.68310546875,
            "relevance_judgment_input_expanded": "# Title: InternEvo: Efficient Long-sequence Large Language Model Training via Hybrid Parallelism and Redundant Sharding\n# Venue: arXiv.org\n# Authors: Qiaoling Chen, Diandian Gu, Guoteng Wang, Xun Chen, Yingtong Xiong, Ting Huang, Qi Hu, Xin Jin, Yonggang Wen, Tianwei Zhang, Peng Sun\n## Abstract\nLarge language models (LLMs) with long sequences begin to power more and more fundamentally new applications we use every day. Existing methods for long-sequence LLM training are neither efficient nor compatible with commonly-used training algorithms such as FlashAttention. We design InternEvo to address these issues. InternEvo decouples all of the sharding dimensions into a new hierarchical space, and systematically analyzes the memory and communication cost of LLM training. Then, it generates an effective hybrid parallelism strategy. We design a new selective overlap mechanism to mitigate the communication overhead introduced by the hybrid parallelism. We also implement memory management techniques to reduce GPU memory fragmentation. Evaluation results show that InternEvo generates parallelization strategies that match or outperform existing methods in model FLOPs utilization.\n## Long-sequence Training\nProcessing long sequences is crucial for supporting important applications such as dealing with longer histories in chat applications. To this end, sequence parallelism (SP) has emerged as a technique aimed at alleviating activation memory footprints during the training of Transformers. In SP, the input tensor of each Transformer layer is divided along the sequence dimension, allowing for parallel computation across multiple GPUs. This segmentation, in conjunction with activation recomputation, results in a substantial reduction in activation memory requirements by a factor of s sp . In this paper, we classify existing SP approaches into 3 stages, which correspond to slicing the sequence into Norm and Dropout modules, Linear modules, and MHA module. When enabled cumulatively: \n\nSP-1: Norm and Dropout modules. As shown in Figure 2 (b), Megatron-LM capitalizes on TP to parallelize the linear layers and MHA, which are the most time-consuming components during training. Simultaneously, it employs SP on Norm and Dropout modules, effectively reducing the activation memory of these layers by a factor of s sp , where s sp = s t p . To maintain consistency in computational results, it integrates necessary communications, including all-gather and reduce-scatter to transfer the activation in forward and backward passes. When the activation size increases with the sequence length, this way of communicating the activation will incur a high overhead. \n\nSP-2: Add Linear modules. DeepSpeed Ulysses utilizes sequence parallelism on Linear, Norm, and Dropout layers, as shown in Figure 2 (c). An all-to-all communi- cation is applied to the output of Linear qkv with a shape of B \u00d7 3 \u00d7 S/s sp \u00d7 H. This enables each GPU to receive the complete sequence of Q, K, and V (each with a shape of B \u00d7 S \u00d7 H/s sp ) for a subset of attention heads. Subsequently, FlashAttention is employed for highly efficient MHA computation with a constraint that the number of attention heads D should not exceed s sp . Another all-to-all communication is then used to gather the MHA layer results and the results are re-partitioned along the sequence dimension.",
            "reference_string": "[267027729 | Chen et al. | 2024 | Citations: 8]"
        },
        {
            "title": "The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 106,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.11299, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2302327872",
                    "name": "Pai Zeng"
                },
                {
                    "authorId": "2302322712",
                    "name": "Zhenyu Ning"
                },
                {
                    "authorId": "2302350607",
                    "name": "Jieru Zhao"
                },
                {
                    "authorId": "1492129615",
                    "name": "Weihao Cui"
                },
                {
                    "authorId": "2239403838",
                    "name": "Mengwei Xu"
                },
                {
                    "authorId": "2286749413",
                    "name": "Liwei Guo"
                },
                {
                    "authorId": "2280208638",
                    "name": "XuSheng Chen"
                },
                {
                    "authorId": "2280137293",
                    "name": "Yizhou Shan"
                }
            ],
            "abstract": "We survey the large language model (LLM) serving area to understand the intricate dynamics between cost-efficiency and accuracy, which is magnified by the growing need for longer contextual understanding when deploying models at a massive scale. Our findings reveal that works in this space optimize along three distinct but conflicting goals: improving serving context length (C), improving serving accuracy (A), and improving serving performance (P). Drawing inspiration from the CAP theorem in databases, we propose a CAP principle for LLM serving, which suggests that any optimization can improve at most two of these three goals simultaneously. Our survey categorizes existing works within this framework. We find the definition and continuity of user-perceived measurement metrics are crucial in determining whether a goal has been met, akin to prior CAP databases in the wild. We recognize the CAP principle for LLM serving as a guiding principle, rather than a formal theorem, to inform designers of the inherent and dynamic trade-offs in serving models. As serving accuracy and performance have been extensively studied, this survey focuses on works that extend serving context length and address the resulting challenges.",
            "corpus_id": 269922123,
            "sentences": [
                {
                    "corpus_id": "269922123",
                    "title": "The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving",
                    "text": "We discuss works that explore the Sequence Parallelism (SP) dimension in a distributed fashion.\n\nHere, a long-context inference request is segmented into sub-sequences and distributed across nodes for parallel processing.While traditional distributed strategies like tensor parallelism (TP) or pipeline parallelism (PP) can also enhance inference performance, we omit them in this survey because, they are not specifically designed for long-context handling and generally serve as orthogonal or complementary to SP optimizations.\n\nOur analysis unfolds in two steps.First, we investigate methods to accelerate a single long-context request using SP.Second, we investigate methods to accelerate a cluster serving long-context requests.\n\nAccelerate a Single Request.\n\n\u2022 Figure 3 shows the relation among this line of research work.This line of research can be traced back to the online normalizer work [88], a mathematically equivalent method for block-wise softmax calculation that avoids materializing the full attention matrix\n\nOnline Normalizer [88] Memory Efficient Attention [89] Flash Attention [48] Blockwise Parallel Transformer [90] Ring Attention [91] Burst Attention [92] Striped Attention [93] Dist Attention [94] Figure 3: Works using sequence parallelism.Gray boxes are not tailored for long-context serving.\n\nsoftmax(QK T ).This method is a foundation for memory-efficient attention [89] and their CUDA implementations [48,96].\n\n\u2022 SP was first introduced by Li et al. [97] and has been widely used in distributed LLM training frameworks such as Megatron [98] and Deepspeed [99].In the context of LLM serving systems, new challenges emerge: (1) LLM serving is usually latency-sensitive and thus requires much smaller batch sizes than LLM training; (2) LLM serving has an autoregressive decode phase, where the sequence length is only one, but it requires large memory for KV cache storage; (3) LLM serving usually relies on large fused kernels for improving performance.While the feed-forward network (FFN) computations for each token in a sequence are linearly independent, the computations for attention are not.Consequently, substantial data exchange is involved when computing distributed attention using SP, thereby opening significant space for performance optimization.",
                    "score": 0.6365913015733615,
                    "section_title": "Distributed Acceleration",
                    "char_start_offset": 30981,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 95
                        },
                        {
                            "start": 97,
                            "end": 221
                        },
                        {
                            "start": 221,
                            "end": 529
                        },
                        {
                            "start": 531,
                            "end": 565
                        },
                        {
                            "start": 565,
                            "end": 648
                        },
                        {
                            "start": 648,
                            "end": 733
                        },
                        {
                            "start": 735,
                            "end": 763
                        },
                        {
                            "start": 765,
                            "end": 828
                        },
                        {
                            "start": 828,
                            "end": 1026
                        },
                        {
                            "start": 1028,
                            "end": 1267
                        },
                        {
                            "start": 1267,
                            "end": 1320
                        },
                        {
                            "start": 1322,
                            "end": 1337
                        },
                        {
                            "start": 1337,
                            "end": 1440
                        },
                        {
                            "start": 1442,
                            "end": 1591
                        },
                        {
                            "start": 1591,
                            "end": 1982
                        },
                        {
                            "start": 1982,
                            "end": 2126
                        },
                        {
                            "start": 2126,
                            "end": 2288
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1099,
                            "end": 1103,
                            "matchedPaperCorpusId": "249151871"
                        },
                        {
                            "start": 1135,
                            "end": 1139,
                            "matchedPaperCorpusId": "266351737"
                        },
                        {
                            "start": 1432,
                            "end": 1436,
                            "matchedPaperCorpusId": "249151871"
                        },
                        {
                            "start": 1567,
                            "end": 1571,
                            "matchedPaperCorpusId": "248693351"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6708984375
                }
            ],
            "relevance_judgement": 0.6708984375,
            "relevance_judgment_input_expanded": "# Title: The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving\n# Venue: arXiv.org\n# Authors: Pai Zeng, Zhenyu Ning, Jieru Zhao, Weihao Cui, Mengwei Xu, Liwei Guo, XuSheng Chen, Yizhou Shan\n## Abstract\nWe survey the large language model (LLM) serving area to understand the intricate dynamics between cost-efficiency and accuracy, which is magnified by the growing need for longer contextual understanding when deploying models at a massive scale. Our findings reveal that works in this space optimize along three distinct but conflicting goals: improving serving context length (C), improving serving accuracy (A), and improving serving performance (P). Drawing inspiration from the CAP theorem in databases, we propose a CAP principle for LLM serving, which suggests that any optimization can improve at most two of these three goals simultaneously. Our survey categorizes existing works within this framework. We find the definition and continuity of user-perceived measurement metrics are crucial in determining whether a goal has been met, akin to prior CAP databases in the wild. We recognize the CAP principle for LLM serving as a guiding principle, rather than a formal theorem, to inform designers of the inherent and dynamic trade-offs in serving models. As serving accuracy and performance have been extensively studied, this survey focuses on works that extend serving context length and address the resulting challenges.\n## Distributed Acceleration\nWe discuss works that explore the Sequence Parallelism (SP) dimension in a distributed fashion.\n\nHere, a long-context inference request is segmented into sub-sequences and distributed across nodes for parallel processing.While traditional distributed strategies like tensor parallelism (TP) or pipeline parallelism (PP) can also enhance inference performance, we omit them in this survey because, they are not specifically designed for long-context handling and generally serve as orthogonal or complementary to SP optimizations.\n\nOur analysis unfolds in two steps.First, we investigate methods to accelerate a single long-context request using SP.Second, we investigate methods to accelerate a cluster serving long-context requests.\n\nAccelerate a Single Request.\n\n\u2022 Figure 3 shows the relation among this line of research work.This line of research can be traced back to the online normalizer work [88], a mathematically equivalent method for block-wise softmax calculation that avoids materializing the full attention matrix\n\nOnline Normalizer [88] Memory Efficient Attention [89] Flash Attention [48] Blockwise Parallel Transformer [90] Ring Attention [91] Burst Attention [92] Striped Attention [93] Dist Attention [94] Figure 3: Works using sequence parallelism.Gray boxes are not tailored for long-context serving.\n\nsoftmax(QK T ).This method is a foundation for memory-efficient attention [89] and their CUDA implementations [48,96].\n\n\u2022 SP was first introduced by Li et al. [97] and has been widely used in distributed LLM training frameworks such as Megatron [98] and Deepspeed [99].In the context of LLM serving systems, new challenges emerge: (1) LLM serving is usually latency-sensitive and thus requires much smaller batch sizes than LLM training; (2) LLM serving has an autoregressive decode phase, where the sequence length is only one, but it requires large memory for KV cache storage; (3) LLM serving usually relies on large fused kernels for improving performance.While the feed-forward network (FFN) computations for each token in a sequence are linearly independent, the computations for attention are not.Consequently, substantial data exchange is involved when computing distributed attention using SP, thereby opening significant space for performance optimization.",
            "reference_string": "[269922123 | Zeng et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Aquarius: A Family of Industry-Level Video Generation Models for Marketing Scenarios",
            "venue": "",
            "year": 2025,
            "reference_count": 38,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.10584, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2362191870",
                    "name": "Huafeng Shi"
                },
                {
                    "authorId": "2362279072",
                    "name": "Jianzhong Liang"
                },
                {
                    "authorId": "2332574955",
                    "name": "Rongchang Xie"
                },
                {
                    "authorId": "2357119154",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2362091271",
                    "name": "Cheng Chen"
                },
                {
                    "authorId": "2356750113",
                    "name": "Chang Liu"
                }
            ],
            "abstract": "This report introduces Aquarius, a family of industry-level video generation models for marketing scenarios designed for thousands-xPU clusters and models with hundreds of billions of parameters. Leveraging efficient engineering architecture and algorithmic innovation, Aquarius demonstrates exceptional performance in high-fidelity, multi-aspect-ratio, and long-duration video synthesis. By disclosing the framework's design details, we aim to demystify industrial-scale video generation systems and catalyze advancements in the generative video community. The Aquarius framework consists of five components: Distributed Graph and Video Data Processing Pipeline: Manages tens of thousands of CPUs and thousands of xPUs via automated task distribution, enabling efficient video data processing. Additionally, we are about to open-source the entire data processing framework named\"Aquarius-Datapipe\". Model Architectures for Different Scales: Include a Single-DiT architecture for 2B models and a Multimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios, multi-resolution, and multi-duration video generation. High-Performance infrastructure designed for video generation model training: Incorporating hybrid parallelism and fine-grained memory optimization strategies, this infrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference Acceleration: Utilizes diffusion cache and attention optimization to achieve a 2.35x inference speedup. Multiple marketing-scenarios applications: Including image-to-video, text-to-video (avatar), video inpainting and video personalization, among others. More downstream applications and multi-dimensional evaluation metrics will be added in the upcoming version updates.",
            "corpus_id": 278714643,
            "sentences": [
                {
                    "corpus_id": "278714643",
                    "title": "Aquarius: A Family of Industry-Level Video Generation Models for Marketing Scenarios",
                    "text": "Innermost TP-SP. Among 8 ranks within a node, we adopt Tensor Parallelism(TP) (Shoeybi et al., 2019) and Sequence Parallelism(SP) (Korthikanti et al., 2023) to partition the model states and activations. In each Transformer block, we employ a dedicated MLP (a Linear layer followed by a SiLU) to incorporate conditioning embedding for Adaptive Layer Normalization (AdaLN), aiming to enhance the model's capacity to capture conditional information. However, this design incurs substantial memory overhead: these dedicated Linear collectively increase the model's total parameter count by over 3B and introduce redundant model states exceeding 20GB per rank. To address this issue, we partition these Linear layers in a column parallel fashion with their computation and parameters parallelized across devices. Specifically, during the forward pass, all-gather reconstructs the complete Linear output, while reduce-scatter synchronizes gradients across the sequence-parallel group before the Linear in the backward pass. \n\nMid-Optional CP. While TP-SP supports training with moderately long contexts, Context Parallelism (CP) (Jacobs et al., 2023) is optionally enabled only for ultra-long sequences (exceeds 200k tokens). \n\nFor non-attention computations, each layer processes 1/CP of the input sequence. Before and after attention computations, all-to-all communication is applied to transpose the sequence, enabling each rank to process attention with full context length but only 1/CP of the attention heads. Since CP incurs lower communication overhead than TP-SP, we configure TP-SP as inner parallelism and CP as outer parallelism. \n\nOutermost ZeRO-DP. We employ DP as the outermost parallelization to enhance throughput. Since the exponential-moving-average (EMA) parameters and moments in AdamW contribute substantially to memory footprint, we adopt ZeRO (Rajbhandari et al., 2020) optimization to partition redundant optimizer states across data-parallel groups. Compared to optimizer states, memory consumption for model parameters and gradients proves relatively minor.",
                    "score": 0.6238436413990509,
                    "section_title": "Parallel Strategy",
                    "char_start_offset": 21999,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 16
                        },
                        {
                            "start": 17,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 447
                        },
                        {
                            "start": 448,
                            "end": 656
                        },
                        {
                            "start": 657,
                            "end": 808
                        },
                        {
                            "start": 809,
                            "end": 1018
                        },
                        {
                            "start": 1021,
                            "end": 1037
                        },
                        {
                            "start": 1038,
                            "end": 1220
                        },
                        {
                            "start": 1223,
                            "end": 1303
                        },
                        {
                            "start": 1304,
                            "end": 1510
                        },
                        {
                            "start": 1511,
                            "end": 1636
                        },
                        {
                            "start": 1639,
                            "end": 1657
                        },
                        {
                            "start": 1658,
                            "end": 1726
                        },
                        {
                            "start": 1727,
                            "end": 1970
                        },
                        {
                            "start": 1971,
                            "end": 2079
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 130,
                            "end": 156,
                            "matchedPaperCorpusId": "248693351"
                        },
                        {
                            "start": 1862,
                            "end": 1888,
                            "matchedPaperCorpusId": "269617042"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65576171875
                }
            ],
            "relevance_judgement": 0.65576171875,
            "relevance_judgment_input_expanded": "# Title: Aquarius: A Family of Industry-Level Video Generation Models for Marketing Scenarios\n# Venue: \n# Authors: Huafeng Shi, Jianzhong Liang, Rongchang Xie, Xian Wu, Cheng Chen, Chang Liu\n## Abstract\nThis report introduces Aquarius, a family of industry-level video generation models for marketing scenarios designed for thousands-xPU clusters and models with hundreds of billions of parameters. Leveraging efficient engineering architecture and algorithmic innovation, Aquarius demonstrates exceptional performance in high-fidelity, multi-aspect-ratio, and long-duration video synthesis. By disclosing the framework's design details, we aim to demystify industrial-scale video generation systems and catalyze advancements in the generative video community. The Aquarius framework consists of five components: Distributed Graph and Video Data Processing Pipeline: Manages tens of thousands of CPUs and thousands of xPUs via automated task distribution, enabling efficient video data processing. Additionally, we are about to open-source the entire data processing framework named\"Aquarius-Datapipe\". Model Architectures for Different Scales: Include a Single-DiT architecture for 2B models and a Multimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios, multi-resolution, and multi-duration video generation. High-Performance infrastructure designed for video generation model training: Incorporating hybrid parallelism and fine-grained memory optimization strategies, this infrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference Acceleration: Utilizes diffusion cache and attention optimization to achieve a 2.35x inference speedup. Multiple marketing-scenarios applications: Including image-to-video, text-to-video (avatar), video inpainting and video personalization, among others. More downstream applications and multi-dimensional evaluation metrics will be added in the upcoming version updates.\n## Parallel Strategy\nInnermost TP-SP. Among 8 ranks within a node, we adopt Tensor Parallelism(TP) (Shoeybi et al., 2019) and Sequence Parallelism(SP) (Korthikanti et al., 2023) to partition the model states and activations. In each Transformer block, we employ a dedicated MLP (a Linear layer followed by a SiLU) to incorporate conditioning embedding for Adaptive Layer Normalization (AdaLN), aiming to enhance the model's capacity to capture conditional information. However, this design incurs substantial memory overhead: these dedicated Linear collectively increase the model's total parameter count by over 3B and introduce redundant model states exceeding 20GB per rank. To address this issue, we partition these Linear layers in a column parallel fashion with their computation and parameters parallelized across devices. Specifically, during the forward pass, all-gather reconstructs the complete Linear output, while reduce-scatter synchronizes gradients across the sequence-parallel group before the Linear in the backward pass. \n\nMid-Optional CP. While TP-SP supports training with moderately long contexts, Context Parallelism (CP) (Jacobs et al., 2023) is optionally enabled only for ultra-long sequences (exceeds 200k tokens). \n\nFor non-attention computations, each layer processes 1/CP of the input sequence. Before and after attention computations, all-to-all communication is applied to transpose the sequence, enabling each rank to process attention with full context length but only 1/CP of the attention heads. Since CP incurs lower communication overhead than TP-SP, we configure TP-SP as inner parallelism and CP as outer parallelism. \n\nOutermost ZeRO-DP. We employ DP as the outermost parallelization to enhance throughput. Since the exponential-moving-average (EMA) parameters and moments in AdamW contribute substantially to memory footprint, we adopt ZeRO (Rajbhandari et al., 2020) optimization to partition redundant optimizer states across data-parallel groups. Compared to optimizer states, memory consumption for model parameters and gradients proves relatively minor.",
            "reference_string": "[278714643 | Shi et al. | 2025 | Citations: 0]"
        },
        {
            "title": "PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference",
            "venue": "",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 6,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.14430, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303233521",
                    "name": "Jiarui Fang"
                },
                {
                    "authorId": "2329908310",
                    "name": "Jinzhe Pan"
                },
                {
                    "authorId": "2302815939",
                    "name": "Jiannan Wang"
                },
                {
                    "authorId": "2303334242",
                    "name": "Aoyu Li"
                },
                {
                    "authorId": "2328615626",
                    "name": "Xibo Sun"
                }
            ],
            "abstract": "This paper presents PipeFusion, an innovative parallel methodology to tackle the high latency issues associated with generating high-resolution images using diffusion transformers (DiTs) models. PipeFusion partitions images into patches and the model layers across multiple GPUs. It employs a patch-level pipeline parallel strategy to orchestrate communication and computation efficiently. By capitalizing on the high similarity between inputs from successive diffusion steps, PipeFusion reuses one-step stale feature maps to provide context for the current pipeline step. This approach notably reduces communication costs compared to existing DiTs inference parallelism, including tensor parallel, sequence parallel and DistriFusion. PipeFusion also exhibits superior memory efficiency, because it can distribute model parameters across multiple devices, making it more suitable for DiTs with large parameter sizes, such as Flux.1. Experimental results demonstrate that PipeFusion achieves state-of-the-art performance on 8xL40 PCIe GPUs for Pixart, Stable-Diffusion 3 and Flux.1 models.Our Source code is available at https://github.com/xdit-project/xDiT.",
            "corpus_id": 273707418,
            "sentences": [
                {
                    "corpus_id": "273707418",
                    "title": "PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference",
                    "text": "PAB (Zhao et al., 2024) exploits the U-shaped attention pattern to mitigate temporal redundancy through a pyramid-style broadcasting approach. Finally, DiTFastAttn (Yuan et al., 2024) identifies three types of redundancies, such as spatial redundancy, temporal redundancy, and conditional redundancy, and presents an attention compression method to speed up generation. \n\nParallel Inference for Diffusion Model: Given the similar transformer architecture, tensor parallelism (TP) (Shoeybi et al., 2019) and sequence parallelism (SP) (Jacobs et al., 2023;Liu et al., 2023;Li et al., 2023), which are commonly employed for efficient inference in LLMs, can be adapted for DiTs. TP partitions model parameters across multiple devices, enabling parallel computation and reducing memory requirements on individual devices. However, it necessitates an AllReduce operation for the outputs of both the Attention and Feedforward Network modules, resulting in communication overhead proportional to sequence length. For DiTs with exceptionally long sequences, this communication overhead becomes substantial. In contrast, SP partitions the input image across multiple devices, utilizing methods such as Ulysses or Ring to communicate the Attention input and output tensors. SP demonstrates su-perior communication efficiency compared to TP, but each device stores the entire model parameters, which can be memory intensive. DistriFusion (Li et al., 2024) leverages Input Temporal Redundancy to design an asynchronous sequence parallel method that utilizes stable activations. It is specifically designed for with a U-Net-based models.",
                    "score": 0.562983059215619,
                    "section_title": "VAE",
                    "char_start_offset": 9727,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 369
                        },
                        {
                            "start": 372,
                            "end": 674
                        },
                        {
                            "start": 675,
                            "end": 816
                        },
                        {
                            "start": 817,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1097
                        },
                        {
                            "start": 1098,
                            "end": 1262
                        },
                        {
                            "start": 1263,
                            "end": 1412
                        },
                        {
                            "start": 1413,
                            "end": 1564
                        },
                        {
                            "start": 1565,
                            "end": 1623
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1426,
                            "end": 1443,
                            "matchedPaperCorpusId": "268091166"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64892578125
                }
            ],
            "relevance_judgement": 0.64892578125,
            "relevance_judgment_input_expanded": "# Title: PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference\n# Venue: \n# Authors: Jiarui Fang, Jinzhe Pan, Jiannan Wang, Aoyu Li, Xibo Sun\n## Abstract\nThis paper presents PipeFusion, an innovative parallel methodology to tackle the high latency issues associated with generating high-resolution images using diffusion transformers (DiTs) models. PipeFusion partitions images into patches and the model layers across multiple GPUs. It employs a patch-level pipeline parallel strategy to orchestrate communication and computation efficiently. By capitalizing on the high similarity between inputs from successive diffusion steps, PipeFusion reuses one-step stale feature maps to provide context for the current pipeline step. This approach notably reduces communication costs compared to existing DiTs inference parallelism, including tensor parallel, sequence parallel and DistriFusion. PipeFusion also exhibits superior memory efficiency, because it can distribute model parameters across multiple devices, making it more suitable for DiTs with large parameter sizes, such as Flux.1. Experimental results demonstrate that PipeFusion achieves state-of-the-art performance on 8xL40 PCIe GPUs for Pixart, Stable-Diffusion 3 and Flux.1 models.Our Source code is available at https://github.com/xdit-project/xDiT.\n## VAE\nPAB (Zhao et al., 2024) exploits the U-shaped attention pattern to mitigate temporal redundancy through a pyramid-style broadcasting approach. Finally, DiTFastAttn (Yuan et al., 2024) identifies three types of redundancies, such as spatial redundancy, temporal redundancy, and conditional redundancy, and presents an attention compression method to speed up generation. \n\nParallel Inference for Diffusion Model: Given the similar transformer architecture, tensor parallelism (TP) (Shoeybi et al., 2019) and sequence parallelism (SP) (Jacobs et al., 2023;Liu et al., 2023;Li et al., 2023), which are commonly employed for efficient inference in LLMs, can be adapted for DiTs. TP partitions model parameters across multiple devices, enabling parallel computation and reducing memory requirements on individual devices. However, it necessitates an AllReduce operation for the outputs of both the Attention and Feedforward Network modules, resulting in communication overhead proportional to sequence length. For DiTs with exceptionally long sequences, this communication overhead becomes substantial. In contrast, SP partitions the input image across multiple devices, utilizing methods such as Ulysses or Ring to communicate the Attention input and output tensors. SP demonstrates su-perior communication efficiency compared to TP, but each device stores the entire model parameters, which can be memory intensive. DistriFusion (Li et al., 2024) leverages Input Temporal Redundancy to design an asynchronous sequence parallel method that utilizes stable activations. It is specifically designed for with a U-Net-based models.",
            "reference_string": "[273707418 | Fang et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 44,
            "citation_count": 258,
            "influential_citation_count": 28,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.01889",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.01889, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256317240",
                    "name": "Hao Liu"
                },
                {
                    "authorId": "2253469012",
                    "name": "Matei Zaharia"
                },
                {
                    "authorId": "2253464956",
                    "name": "Pieter Abbeel"
                }
            ],
            "abstract": "Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.",
            "corpus_id": 263608461,
            "sentences": [
                {
                    "corpus_id": "263608461",
                    "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context",
                    "text": "Transformers have garnered significant attention in the field of AI and have become the backbone for numerous state-of-the-art models. Several works have explored memory-efficient techniques to address the memory limitations of Transformers and enable their application to a wider range of problems. Computing exact self-attention in a blockwise manner using the tiling technique [24] has led to the development of memory efficient attention mechanisms [30] and its efficient CUDA implementation [9], and blockwise parallel transformer [23] that proposes computing both feedforward and self-attention block-by-block, resulting in a significant reduction in memory requirements. \n\nIn line with these advancements, our work falls into the category of memory efficient computation for Transformers. Other works have investigated the approximation of attention mechanisms, yet these efforts have often yielded sub-optimal results or encountered challenges during scaling up. For an in-depth review of these techniques, we recommend referring to the surveys [26,35]. Another avenue of research explores various parallelism methods, including data parallelism [10], tensor parallelism [34], pipeline parallelism [27,15,28], sequence parallelism [21,18,17], and FSDP [11,31]. The activations of self-attention take a substantial amount of memory for large context models. \n\nTensor parallelism can only reduce parts of activations memory and sequence parallelism introduces a significant communication overhead that cannot be fully overlapped with computation. Prior work has studied sharding along sequence and attention heads, and gathering sequences via an optimized all-to-all topology, achieving reduced communication [17]. However, this method is restricted by the number of attention heads and requires gathering the full sequence on each device. In comparison, our approach fully overlaps communication with blockwise computation, enhancing its scalability. Prior work extends sequence parallelism for computing self-attention using a ring topology [21], which reduces the communication cost compared to standard sequence parallelism. However, overlapping communication with computation remains challenging due to the constraints of arithmetic intensity. The communication overheads render this approach infeasible for training and inference in largecontext scenarios.",
                    "score": 0.8060153468254,
                    "section_title": "Related Work",
                    "char_start_offset": 21820,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 135,
                            "end": 299
                        },
                        {
                            "start": 300,
                            "end": 677
                        },
                        {
                            "start": 680,
                            "end": 795
                        },
                        {
                            "start": 796,
                            "end": 970
                        },
                        {
                            "start": 971,
                            "end": 1061
                        },
                        {
                            "start": 1062,
                            "end": 1268
                        },
                        {
                            "start": 1269,
                            "end": 1364
                        },
                        {
                            "start": 1367,
                            "end": 1552
                        },
                        {
                            "start": 1553,
                            "end": 1720
                        },
                        {
                            "start": 1721,
                            "end": 1845
                        },
                        {
                            "start": 1846,
                            "end": 1957
                        },
                        {
                            "start": 1958,
                            "end": 2134
                        },
                        {
                            "start": 2135,
                            "end": 2254
                        },
                        {
                            "start": 2255,
                            "end": 2368
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 496,
                            "end": 499,
                            "matchedPaperCorpusId": "249151871"
                        },
                        {
                            "start": 536,
                            "end": 540,
                            "matchedPaperCorpusId": "258987968"
                        },
                        {
                            "start": 1154,
                            "end": 1158,
                            "matchedPaperCorpusId": "372467"
                        },
                        {
                            "start": 1206,
                            "end": 1210,
                            "matchedPaperCorpusId": "202488191"
                        },
                        {
                            "start": 1210,
                            "end": 1213,
                            "matchedPaperCorpusId": "53670168"
                        },
                        {
                            "start": 1213,
                            "end": 1216,
                            "matchedPaperCorpusId": "219720945"
                        },
                        {
                            "start": 1239,
                            "end": 1243,
                            "matchedPaperCorpusId": "246017095"
                        },
                        {
                            "start": 1264,
                            "end": 1267,
                            "matchedPaperCorpusId": "269617042"
                        },
                        {
                            "start": 2049,
                            "end": 2053,
                            "matchedPaperCorpusId": "246017095"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.646484375
                }
            ],
            "relevance_judgement": 0.646484375,
            "relevance_judgment_input_expanded": "# Title: Ring Attention with Blockwise Transformers for Near-Infinite Context\n# Venue: International Conference on Learning Representations\n# Authors: Hao Liu, Matei Zaharia, Pieter Abbeel\n## Abstract\nTransformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.\n## Related Work\nTransformers have garnered significant attention in the field of AI and have become the backbone for numerous state-of-the-art models. Several works have explored memory-efficient techniques to address the memory limitations of Transformers and enable their application to a wider range of problems. Computing exact self-attention in a blockwise manner using the tiling technique [24] has led to the development of memory efficient attention mechanisms [30] and its efficient CUDA implementation [9], and blockwise parallel transformer [23] that proposes computing both feedforward and self-attention block-by-block, resulting in a significant reduction in memory requirements. \n\nIn line with these advancements, our work falls into the category of memory efficient computation for Transformers. Other works have investigated the approximation of attention mechanisms, yet these efforts have often yielded sub-optimal results or encountered challenges during scaling up. For an in-depth review of these techniques, we recommend referring to the surveys [26,35]. Another avenue of research explores various parallelism methods, including data parallelism [10], tensor parallelism [34], pipeline parallelism [27,15,28], sequence parallelism [21,18,17], and FSDP [11,31]. The activations of self-attention take a substantial amount of memory for large context models. \n\nTensor parallelism can only reduce parts of activations memory and sequence parallelism introduces a significant communication overhead that cannot be fully overlapped with computation. Prior work has studied sharding along sequence and attention heads, and gathering sequences via an optimized all-to-all topology, achieving reduced communication [17]. However, this method is restricted by the number of attention heads and requires gathering the full sequence on each device. In comparison, our approach fully overlaps communication with blockwise computation, enhancing its scalability. Prior work extends sequence parallelism for computing self-attention using a ring topology [21], which reduces the communication cost compared to standard sequence parallelism. However, overlapping communication with computation remains challenging due to the constraints of arithmetic intensity. The communication overheads render this approach infeasible for training and inference in largecontext scenarios.",
            "reference_string": "[263608461 | Liu et al. | 2023 | Citations: 258]"
        },
        {
            "title": "DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training",
            "venue": "",
            "year": 2023,
            "reference_count": 25,
            "citation_count": 22,
            "influential_citation_count": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.03294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2117961435",
                    "name": "Dacheng Li"
                },
                {
                    "authorId": "2254264970",
                    "name": "Rulin Shao"
                },
                {
                    "authorId": "2254221083",
                    "name": "Anze Xie"
                },
                {
                    "authorId": "2243336934",
                    "name": "Eric P. Xing"
                },
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                },
                {
                    "authorId": "2055174324",
                    "name": "Ion Stoica"
                },
                {
                    "authorId": "2254681613",
                    "name": "Joseph Gonzalez"
                },
                {
                    "authorId": "2257340589",
                    "name": "Hao Zhang"
                }
            ],
            "abstract": "FlashAttention (Dao, 2023) effectively reduces the quadratic peak memory usage to linear in training transformer-based large language models (LLMs) on a single GPU. In this paper, we introduce DISTFLASHATTN, a distributed memory-efficient attention mechanism optimized for long-context LLMs training. We propose three key techniques: token-level workload balancing, overlapping key-value communication, and a rematerialization-aware gradient checkpointing algorithm. We evaluate DISTFLASHATTN on Llama-7B and variants with sequence lengths from 32K to 512K. DISTFLASHATTN achieves 8x longer sequences, 4.45 - 5.64x speedup compared to Ring Self-Attention, 2 - 8x longer sequences, 1.24 - 2.01x speedup compared to Megatron-LM with FlashAttention. It achieves 1.67x and 1.26 - 1.88x speedup compared to recent Ring Attention and DeepSpeed-Ulysses. Code is available at https://github.com/RulinShao/LightSeq.",
            "corpus_id": 263671659,
            "sentences": [
                {
                    "corpus_id": "263671659",
                    "title": "DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training",
                    "text": "In this work, we introduce DISTFLASHATTN, a distributed memory-efficient attention prototype for long-context transformer training based on sequence parallelism. DISTFLASHATTN presents novel system optimizations including load balancing for causal language modelings, overlapped communication with computation in the distributed attention computation, and a re-materialization-aware checkpointing strategy. Experiments evaluate multiple families of transformer models and on different cluster types, and over four strong distributed system baselines. In particular, DISTFLASHATTN has demonstrated up to 2.01\u00d7 speedup and scales up to 8x longer sequences, compared to the popular system, Megatron-LM with FlashAttention.",
                    "score": 0.6007884574508708,
                    "section_title": "Conclusion",
                    "char_start_offset": 25673,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 406
                        },
                        {
                            "start": 407,
                            "end": 550
                        },
                        {
                            "start": 551,
                            "end": 719
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64111328125
                },
                {
                    "corpus_id": "263671659",
                    "title": "DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training",
                    "text": "Memory-efficient attention. Dao et al. (2022) and Lefaudeux et al. (2022) propose to use an online normalizer (Milakov & Gimelshein, 2018) to compute the attention in a blockwise and memory-efficient way. It reduces peak memory usage by not materializing large intermediate states, e.g. the attention softmax matrix. In addition, research on sparse attention computes only a sparse subset of the attention score, which also reduces the memory footprints yet may lead to inferior performance (Beltagy et al., 2020;Sun et al., 2022;Zaheer et al., 2020). In this work, we limit our scope to exact attention. \n\nSequence parallelism and ring attention Ring Self-Attention (Li et al., 2021) is among the first to parallelize Transformers in the sequence dimension. However, its distributed attention design is not optimized for causal language modeling and incompatible with memory-efficient attention, which are crucial for long-context LLM training. Ring Attention (Liu et al., 2023) proposes to compute distributed attention in a memory-efficient blockwise pattern. However, it is also not optimized for causal language modeling, leading to 2\u00d7 extra computation. DISTFLASHATTN optimizes for both memory-efficient attention and causal language modeling. More recently, DeepSpeed Ulysses (Jacobs et al., 2023) proposes a hybrid parallelism strategy. It computes distributed attention in the tensor model parallelism to address these two problems and utilizes sequence parallelism elsewhere (Shoeybi et al., 2019). We provide head-to-head comparison in Table 4. \n\nModel Parallelism and FSDP Tensor Model parallelism (Korthikanti et al., 2023) partitions model parameters and also distributes the activation in parallel LLM training. Pipeline model parallelism (Huang et al., 2019) also partitions the activations. However, it applies high memory pressure to the first pipeline stage. We show in \u00a7 4.2 that this leads to a less effective support for long sequences. Thus, we focus on comparing with tensor model parallelism and only consider pipeline parallelism when the number of heads is insufficient for tensor parallelism.",
                    "score": 0.7853056516422943,
                    "section_title": "Related work",
                    "char_start_offset": 5333,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 27
                        },
                        {
                            "start": 28,
                            "end": 204
                        },
                        {
                            "start": 205,
                            "end": 316
                        },
                        {
                            "start": 317,
                            "end": 551
                        },
                        {
                            "start": 552,
                            "end": 604
                        },
                        {
                            "start": 607,
                            "end": 758
                        },
                        {
                            "start": 759,
                            "end": 945
                        },
                        {
                            "start": 946,
                            "end": 1062
                        },
                        {
                            "start": 1063,
                            "end": 1159
                        },
                        {
                            "start": 1160,
                            "end": 1249
                        },
                        {
                            "start": 1250,
                            "end": 1344
                        },
                        {
                            "start": 1345,
                            "end": 1508
                        },
                        {
                            "start": 1509,
                            "end": 1555
                        },
                        {
                            "start": 1558,
                            "end": 1726
                        },
                        {
                            "start": 1727,
                            "end": 1807
                        },
                        {
                            "start": 1808,
                            "end": 1877
                        },
                        {
                            "start": 1878,
                            "end": 1958
                        },
                        {
                            "start": 1959,
                            "end": 2120
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 28,
                            "end": 45,
                            "matchedPaperCorpusId": "249151871"
                        },
                        {
                            "start": 530,
                            "end": 550,
                            "matchedPaperCorpusId": "220831004"
                        },
                        {
                            "start": 1610,
                            "end": 1636,
                            "matchedPaperCorpusId": "248693351"
                        },
                        {
                            "start": 1754,
                            "end": 1773,
                            "matchedPaperCorpusId": "53670168"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5078125
                }
            ],
            "relevance_judgement": 0.64111328125,
            "relevance_judgment_input_expanded": "# Title: DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training\n# Venue: \n# Authors: Dacheng Li, Rulin Shao, Anze Xie, Eric P. Xing, Xuezhe Ma, Ion Stoica, Joseph Gonzalez, Hao Zhang\n## Abstract\nFlashAttention (Dao, 2023) effectively reduces the quadratic peak memory usage to linear in training transformer-based large language models (LLMs) on a single GPU. In this paper, we introduce DISTFLASHATTN, a distributed memory-efficient attention mechanism optimized for long-context LLMs training. We propose three key techniques: token-level workload balancing, overlapping key-value communication, and a rematerialization-aware gradient checkpointing algorithm. We evaluate DISTFLASHATTN on Llama-7B and variants with sequence lengths from 32K to 512K. DISTFLASHATTN achieves 8x longer sequences, 4.45 - 5.64x speedup compared to Ring Self-Attention, 2 - 8x longer sequences, 1.24 - 2.01x speedup compared to Megatron-LM with FlashAttention. It achieves 1.67x and 1.26 - 1.88x speedup compared to recent Ring Attention and DeepSpeed-Ulysses. Code is available at https://github.com/RulinShao/LightSeq.\n## Related work\nMemory-efficient attention. Dao et al. (2022) and Lefaudeux et al. (2022) propose to use an online normalizer (Milakov & Gimelshein, 2018) to compute the attention in a blockwise and memory-efficient way. It reduces peak memory usage by not materializing large intermediate states, e.g. the attention softmax matrix. In addition, research on sparse attention computes only a sparse subset of the attention score, which also reduces the memory footprints yet may lead to inferior performance (Beltagy et al., 2020;Sun et al., 2022;Zaheer et al., 2020). In this work, we limit our scope to exact attention. \n\nSequence parallelism and ring attention Ring Self-Attention (Li et al., 2021) is among the first to parallelize Transformers in the sequence dimension. However, its distributed attention design is not optimized for causal language modeling and incompatible with memory-efficient attention, which are crucial for long-context LLM training. Ring Attention (Liu et al., 2023) proposes to compute distributed attention in a memory-efficient blockwise pattern. However, it is also not optimized for causal language modeling, leading to 2\u00d7 extra computation. DISTFLASHATTN optimizes for both memory-efficient attention and causal language modeling. More recently, DeepSpeed Ulysses (Jacobs et al., 2023) proposes a hybrid parallelism strategy. It computes distributed attention in the tensor model parallelism to address these two problems and utilizes sequence parallelism elsewhere (Shoeybi et al., 2019). We provide head-to-head comparison in Table 4. \n\nModel Parallelism and FSDP Tensor Model parallelism (Korthikanti et al., 2023) partitions model parameters and also distributes the activation in parallel LLM training. Pipeline model parallelism (Huang et al., 2019) also partitions the activations. However, it applies high memory pressure to the first pipeline stage. We show in \u00a7 4.2 that this leads to a less effective support for long sequences. Thus, we focus on comparing with tensor model parallelism and only consider pipeline parallelism when the number of heads is insufficient for tensor parallelism.\n\n## Conclusion\nIn this work, we introduce DISTFLASHATTN, a distributed memory-efficient attention prototype for long-context transformer training based on sequence parallelism. DISTFLASHATTN presents novel system optimizations including load balancing for causal language modelings, overlapped communication with computation in the distributed attention computation, and a re-materialization-aware checkpointing strategy. Experiments evaluate multiple families of transformer models and on different cluster types, and over four strong distributed system baselines. In particular, DISTFLASHATTN has demonstrated up to 2.01\u00d7 speedup and scales up to 8x longer sequences, compared to the popular system, Megatron-LM with FlashAttention.",
            "reference_string": "[263671659 | Li et al. | 2023 | Citations: 22]"
        },
        {
            "title": "Scaling Laws of RoPE-based Extrapolation",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 58,
            "citation_count": 89,
            "influential_citation_count": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.05209",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05209, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257094943",
                    "name": "Xiaoran Liu"
                },
                {
                    "authorId": "146948229",
                    "name": "Hang Yan"
                },
                {
                    "authorId": "2257086624",
                    "name": "Shuo Zhang"
                },
                {
                    "authorId": "2064164220",
                    "name": "Chen An"
                },
                {
                    "authorId": "1767521",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2258618409",
                    "name": "Dahua Lin"
                }
            ],
            "abstract": "The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose \\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \\textbf{\\textit{critical dimension for extrapolation}}. Besides these observations and analyses, we achieve extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.",
            "corpus_id": 263828829,
            "sentences": [
                {
                    "corpus_id": "263828829",
                    "title": "Scaling Laws of RoPE-based Extrapolation",
                    "text": "The second challenge is parallelism for sequence length, that is, Sequence Parallelism (SP) (Li et al., 2023c). Since the memory and calculation of a single device are limited, ColossalAI-SP (Li et al., 2023c) first proposes the segmentation and parallelism for the sequence dimension in addition to tensor parallelism for the hidden dimension and pipeline parallelism for model depth. On this basis, Ring Attention (Liu et al., 2023) uses blockwise self-attention to split long sequences into different devices and overlap the communication of key-value blocks. Besides, LightSeq (Li et al., 2023b) further improves the efficiency of long sequence modeling through load balancing for causal language modelings and a re-materialization-aware checkpointing strategy. Although the above sequence parallelism can achieve infinitely long sequence modeling, they ignore the compatibility with the existing efficient self-attention mechanisms such as FlashAttention (Dao et al., 2022;Dao, 2023) to achieve input with almost infinity. On the contrary, Megatron-SP (Korthikanti et al., 2022) only uses sequence parallelism during Dropout and Layernorm operations, thereby reducing activation redundancy. In addition, DeepSpeed-Ulysses (Jacobs et al., 2023) uses an alternative all-to-all collective communication gathering weight for attention computation when segmenting the sequence, avoiding communication overhead that originally increases with length. \n\nRecently, there have been a great deal of efforts devoted to solving the collapse of the performance beyond the training length and expanding the context length of RoPE-based LLMs to 16K, 32K, or even 100K (Chen et al., 2023;Su, 2023c;bloc97, 2023b). Considering that LLMs, such as LLaMA2 (Touvron et al., 2023b), have already acquired sufficient knowledge in the pre-training stage and demonstrated excellence in short-context tasks, the emerging extrapolation improvements have primarily focused on the fine-tuning and the testing phase. Initially, the context window is a well-discussed strategy for any LLM without further training.",
                    "score": 0.5758763397795322,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 18552,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 111
                        },
                        {
                            "start": 112,
                            "end": 385
                        },
                        {
                            "start": 386,
                            "end": 562
                        },
                        {
                            "start": 563,
                            "end": 765
                        },
                        {
                            "start": 766,
                            "end": 1027
                        },
                        {
                            "start": 1028,
                            "end": 1195
                        },
                        {
                            "start": 1196,
                            "end": 1448
                        },
                        {
                            "start": 1451,
                            "end": 1701
                        },
                        {
                            "start": 1702,
                            "end": 1990
                        },
                        {
                            "start": 1991,
                            "end": 2087
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 92,
                            "end": 110,
                            "matchedPaperCorpusId": "246017095"
                        },
                        {
                            "start": 191,
                            "end": 209,
                            "matchedPaperCorpusId": "246017095"
                        },
                        {
                            "start": 416,
                            "end": 434,
                            "matchedPaperCorpusId": "265551519"
                        },
                        {
                            "start": 960,
                            "end": 978,
                            "matchedPaperCorpusId": "249151871"
                        },
                        {
                            "start": 1657,
                            "end": 1676,
                            "matchedPaperCorpusId": "265551519"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56689453125
                }
            ],
            "relevance_judgement": 0.56689453125,
            "relevance_judgment_input_expanded": "# Title: Scaling Laws of RoPE-based Extrapolation\n# Venue: International Conference on Learning Representations\n# Authors: Xiaoran Liu, Hang Yan, Shuo Zhang, Chen An, Xipeng Qiu, Dahua Lin\n## Abstract\nThe extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose \\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \\textbf{\\textit{critical dimension for extrapolation}}. Besides these observations and analyses, we achieve extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.\n## RELATED WORK\nThe second challenge is parallelism for sequence length, that is, Sequence Parallelism (SP) (Li et al., 2023c). Since the memory and calculation of a single device are limited, ColossalAI-SP (Li et al., 2023c) first proposes the segmentation and parallelism for the sequence dimension in addition to tensor parallelism for the hidden dimension and pipeline parallelism for model depth. On this basis, Ring Attention (Liu et al., 2023) uses blockwise self-attention to split long sequences into different devices and overlap the communication of key-value blocks. Besides, LightSeq (Li et al., 2023b) further improves the efficiency of long sequence modeling through load balancing for causal language modelings and a re-materialization-aware checkpointing strategy. Although the above sequence parallelism can achieve infinitely long sequence modeling, they ignore the compatibility with the existing efficient self-attention mechanisms such as FlashAttention (Dao et al., 2022;Dao, 2023) to achieve input with almost infinity. On the contrary, Megatron-SP (Korthikanti et al., 2022) only uses sequence parallelism during Dropout and Layernorm operations, thereby reducing activation redundancy. In addition, DeepSpeed-Ulysses (Jacobs et al., 2023) uses an alternative all-to-all collective communication gathering weight for attention computation when segmenting the sequence, avoiding communication overhead that originally increases with length. \n\nRecently, there have been a great deal of efforts devoted to solving the collapse of the performance beyond the training length and expanding the context length of RoPE-based LLMs to 16K, 32K, or even 100K (Chen et al., 2023;Su, 2023c;bloc97, 2023b). Considering that LLMs, such as LLaMA2 (Touvron et al., 2023b), have already acquired sufficient knowledge in the pre-training stage and demonstrated excellence in short-context tasks, the emerging extrapolation improvements have primarily focused on the fine-tuning and the testing phase. Initially, the context window is a well-discussed strategy for any LLM without further training.",
            "reference_string": "[263828829 | Liu et al. | 2023 | Citations: 89]"
        },
        {
            "title": "Foundation Models for Time Series: A Survey",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 133,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.04011, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2354177739",
                    "name": "Siva Rama Krishna Kottapalli"
                },
                {
                    "authorId": "2354177337",
                    "name": "Karthik Hubli"
                },
                {
                    "authorId": "2354178763",
                    "name": "Sandeep Chandrashekhara"
                },
                {
                    "authorId": "2354142186",
                    "name": "Garima Jain"
                },
                {
                    "authorId": "2354177341",
                    "name": "Sunayana Hubli"
                },
                {
                    "authorId": "2354179172",
                    "name": "Gayathri Botla"
                },
                {
                    "authorId": "2354179043",
                    "name": "Ramesh Doddaiah"
                }
            ],
            "abstract": "Transformer-based foundation models have emerged as a dominant paradigm in time series analysis, offering unprecedented capabilities in tasks such as forecasting, anomaly detection, classification, trend analysis and many more time series analytical tasks. This survey provides a comprehensive overview of the current state of the art pre-trained foundation models, introducing a novel taxonomy to categorize them across several dimensions. Specifically, we classify models by their architecture design, distinguishing between those leveraging patch-based representations and those operating directly on raw sequences. The taxonomy further includes whether the models provide probabilistic or deterministic predictions, and whether they are designed to work with univariate time series or can handle multivariate time series out of the box. Additionally, the taxonomy encompasses model scale and complexity, highlighting differences between lightweight architectures and large-scale foundation models. A unique aspect of this survey is its categorization by the type of objective function employed during training phase. By synthesizing these perspectives, this survey serves as a resource for researchers and practitioners, providing insights into current trends and identifying promising directions for future research in transformer-based time series modeling.",
            "corpus_id": 277621915,
            "sentences": [
                {
                    "corpus_id": "277621915",
                    "title": "Foundation Models for Time Series: A Survey",
                    "text": "Advantages Over Traditional Architectures. One of the most transformative aspects of the Transformer model is its ability to leverage parallelism, making it significantly more scalable compared to traditional sequential architectures [69]. This scalability stems from two critical factors: \n\n\u2022 Non-Sequential Processing: Unlike RNNs, which process inputs one time step at a time, Transformers operate on the entire sequence simultaneously [50]. This non-sequential processing eliminates the dependency on previous computations, allowing for efficient utilization of modern hardware accelerators like GPUs and TPUs. \u2022 Efficient Handling of Long Sequences: Time series datasets often involve long sequences, posing challenges for RNNs [40] and LSTMs [45] due to their inherent limitations in memory and computation. Transformers, on the other hand, process all time steps in parallel, ensuring consistent computational efficiency regardless of sequence length [69]. This  advantage is especially relevant in applications like highfrequency financial trading, where datasets consist of millions of observations. \n\nHowever, the scalability of Transformers is not without challenges. The self-attention mechanism requires O(n2) memory and computation for a sequence of length n, which can become prohibitive for extremely long time series [70]. To address this, recent innovations such as sparse attention and linear Transformers have been proposed, reducing the complexity to O(n) [71], [72] in certain cases. These advancements extend the applicability of Transformers to resource-constrained environments.",
                    "score": 0.5798295144776457,
                    "section_title": "Scalability and Parallelism",
                    "char_start_offset": 21613,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 42
                        },
                        {
                            "start": 43,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 289
                        },
                        {
                            "start": 292,
                            "end": 444
                        },
                        {
                            "start": 445,
                            "end": 614
                        },
                        {
                            "start": 615,
                            "end": 813
                        },
                        {
                            "start": 814,
                            "end": 963
                        },
                        {
                            "start": 964,
                            "end": 1108
                        },
                        {
                            "start": 1111,
                            "end": 1178
                        },
                        {
                            "start": 1179,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1505
                        },
                        {
                            "start": 1506,
                            "end": 1603
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 439,
                            "end": 443,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 733,
                            "end": 737,
                            "matchedPaperCorpusId": "205001834"
                        },
                        {
                            "start": 748,
                            "end": 752,
                            "matchedPaperCorpusId": "1915014"
                        },
                        {
                            "start": 1334,
                            "end": 1338,
                            "matchedPaperCorpusId": "235623791"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.552734375
                }
            ],
            "relevance_judgement": 0.552734375,
            "relevance_judgment_input_expanded": "# Title: Foundation Models for Time Series: A Survey\n# Venue: arXiv.org\n# Authors: Siva Rama Krishna Kottapalli, Karthik Hubli, Sandeep Chandrashekhara, Garima Jain, Sunayana Hubli, Gayathri Botla, Ramesh Doddaiah\n## Abstract\nTransformer-based foundation models have emerged as a dominant paradigm in time series analysis, offering unprecedented capabilities in tasks such as forecasting, anomaly detection, classification, trend analysis and many more time series analytical tasks. This survey provides a comprehensive overview of the current state of the art pre-trained foundation models, introducing a novel taxonomy to categorize them across several dimensions. Specifically, we classify models by their architecture design, distinguishing between those leveraging patch-based representations and those operating directly on raw sequences. The taxonomy further includes whether the models provide probabilistic or deterministic predictions, and whether they are designed to work with univariate time series or can handle multivariate time series out of the box. Additionally, the taxonomy encompasses model scale and complexity, highlighting differences between lightweight architectures and large-scale foundation models. A unique aspect of this survey is its categorization by the type of objective function employed during training phase. By synthesizing these perspectives, this survey serves as a resource for researchers and practitioners, providing insights into current trends and identifying promising directions for future research in transformer-based time series modeling.\n## Scalability and Parallelism\nAdvantages Over Traditional Architectures. One of the most transformative aspects of the Transformer model is its ability to leverage parallelism, making it significantly more scalable compared to traditional sequential architectures [69]. This scalability stems from two critical factors: \n\n\u2022 Non-Sequential Processing: Unlike RNNs, which process inputs one time step at a time, Transformers operate on the entire sequence simultaneously [50]. This non-sequential processing eliminates the dependency on previous computations, allowing for efficient utilization of modern hardware accelerators like GPUs and TPUs. \u2022 Efficient Handling of Long Sequences: Time series datasets often involve long sequences, posing challenges for RNNs [40] and LSTMs [45] due to their inherent limitations in memory and computation. Transformers, on the other hand, process all time steps in parallel, ensuring consistent computational efficiency regardless of sequence length [69]. This  advantage is especially relevant in applications like highfrequency financial trading, where datasets consist of millions of observations. \n\nHowever, the scalability of Transformers is not without challenges. The self-attention mechanism requires O(n2) memory and computation for a sequence of length n, which can become prohibitive for extremely long time series [70]. To address this, recent innovations such as sparse attention and linear Transformers have been proposed, reducing the complexity to O(n) [71], [72] in certain cases. These advancements extend the applicability of Transformers to resource-constrained environments.",
            "reference_string": "[277621915 | Kottapalli et al. | 2025 | Citations: 0]"
        },
        {
            "title": "DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 8,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.10266, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280071587",
                    "name": "Xuanlei Zhao"
                },
                {
                    "authorId": "1454003795",
                    "name": "Shenggan Cheng"
                },
                {
                    "authorId": "2109654065",
                    "name": "Zangwei Zheng"
                },
                {
                    "authorId": "2291936683",
                    "name": "Zheming Yang"
                },
                {
                    "authorId": "2280088379",
                    "name": "Ziming Liu"
                },
                {
                    "authorId": "2285076067",
                    "name": "Yang You"
                }
            ],
            "abstract": "Scaling multi-dimensional transformers to long sequences is indispensable across various domains. However, the challenges of large memory requirements and slow speeds of such sequences necessitate sequence parallelism. All existing approaches fall under the category of embedded sequence parallelism, which are limited to shard along a single sequence dimension, thereby introducing significant communication overhead. However, the nature of multi-dimensional transformers involves independent calculations across multiple sequence dimensions. To this end, we propose Dynamic Sequence Parallelism (DSP) as a novel abstraction of sequence parallelism. DSP dynamically switches the parallel dimension among all sequences according to the computation stage with efficient resharding strategy. DSP offers significant reductions in communication costs, adaptability across modules, and ease of implementation with minimal constraints. Experimental evaluations demonstrate DSP's superiority over state-of-the-art embedded sequence parallelism methods by remarkable throughput improvements ranging from 32.2% to 10x, with less than 25% communication volume.",
            "corpus_id": 268510227,
            "sentences": [
                {
                    "corpus_id": "268510227",
                    "title": "DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers",
                    "text": "Efficiently scaling multi-dimensional transformers to accommodate long sequences is necessary across diverse domains, including video generation (Singer et al., 2022;Blattmann et al., 2023;Ma et al., 2024), image generation (Ramesh et al., 2021;Rombach et al., 2022;Liu et al., 2024), protein structure prediction (Jumper et al., 2021), spatial-temporal information processing (Cong et al., 2021), and beyond. The long length of sequences introduces substantial activation memory costs and notable slowdown for speed, underscoring the need for employing parallelism. \n\nApart from data parallel and pipeline parallel (Huang et al., 2019) which cannot reduce memory cost and inference time, Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). \n\nsequence parallel is the only option. Current sequence parallelism, such as Megatron-LM (Shoeybi et al., 2019), Ring-Attention (Li et al., 2021;Liu et al., 2023a), Megatron-SP (Korthikanti et al., 2022), and DeepSpeed-Ulysses (Jacobs et al., 2023) are all embedded sequence parallelism methods. As shown in Figure 1, these embedded methods shard along a single sequence dimension, which are tailored to the specific pattern and introduce extra communication and complex code modification. \n\nHowever, multi-dimensional transformers calculate independently across multiple sequence dimensions. For instance, for video generation models like OpenSora (Zangwei Zheng, 2024) and Latte (Ma et al., 2024), Spatial-Temporal Attention (Yan et al., 2021) is adopted which separates attention computations to independent temporal and spatial computation. Therefore, there exists a potential space for a new sequence parallelism paradigm. \n\nTo adapt to the flexible patterns of multi-dimensional transformers, we introduce Dynamic Sequence Parallelism (DSP) as a novel abstraction of sequence parallelism, featured by its elegant design, high effectiveness, and excellent compatibility.",
                    "score": 0.704240112108693,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 409
                        },
                        {
                            "start": 410,
                            "end": 566
                        },
                        {
                            "start": 569,
                            "end": 794
                        },
                        {
                            "start": 795,
                            "end": 827
                        },
                        {
                            "start": 830,
                            "end": 867
                        },
                        {
                            "start": 868,
                            "end": 1124
                        },
                        {
                            "start": 1125,
                            "end": 1318
                        },
                        {
                            "start": 1321,
                            "end": 1421
                        },
                        {
                            "start": 1422,
                            "end": 1673
                        },
                        {
                            "start": 1674,
                            "end": 1756
                        },
                        {
                            "start": 1759,
                            "end": 2004
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 245,
                            "end": 266,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 266,
                            "end": 283,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 314,
                            "end": 335,
                            "matchedPaperCorpusId": "248693351"
                        },
                        {
                            "start": 616,
                            "end": 636,
                            "matchedPaperCorpusId": "53670168"
                        },
                        {
                            "start": 1556,
                            "end": 1573,
                            "matchedPaperCorpusId": "232428140"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53564453125
                }
            ],
            "relevance_judgement": 0.53564453125,
            "relevance_judgment_input_expanded": "# Title: DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers\n# Venue: arXiv.org\n# Authors: Xuanlei Zhao, Shenggan Cheng, Zangwei Zheng, Zheming Yang, Ziming Liu, Yang You\n## Abstract\nScaling multi-dimensional transformers to long sequences is indispensable across various domains. However, the challenges of large memory requirements and slow speeds of such sequences necessitate sequence parallelism. All existing approaches fall under the category of embedded sequence parallelism, which are limited to shard along a single sequence dimension, thereby introducing significant communication overhead. However, the nature of multi-dimensional transformers involves independent calculations across multiple sequence dimensions. To this end, we propose Dynamic Sequence Parallelism (DSP) as a novel abstraction of sequence parallelism. DSP dynamically switches the parallel dimension among all sequences according to the computation stage with efficient resharding strategy. DSP offers significant reductions in communication costs, adaptability across modules, and ease of implementation with minimal constraints. Experimental evaluations demonstrate DSP's superiority over state-of-the-art embedded sequence parallelism methods by remarkable throughput improvements ranging from 32.2% to 10x, with less than 25% communication volume.\n## Introduction\nEfficiently scaling multi-dimensional transformers to accommodate long sequences is necessary across diverse domains, including video generation (Singer et al., 2022;Blattmann et al., 2023;Ma et al., 2024), image generation (Ramesh et al., 2021;Rombach et al., 2022;Liu et al., 2024), protein structure prediction (Jumper et al., 2021), spatial-temporal information processing (Cong et al., 2021), and beyond. The long length of sequences introduces substantial activation memory costs and notable slowdown for speed, underscoring the need for employing parallelism. \n\nApart from data parallel and pipeline parallel (Huang et al., 2019) which cannot reduce memory cost and inference time, Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). \n\nsequence parallel is the only option. Current sequence parallelism, such as Megatron-LM (Shoeybi et al., 2019), Ring-Attention (Li et al., 2021;Liu et al., 2023a), Megatron-SP (Korthikanti et al., 2022), and DeepSpeed-Ulysses (Jacobs et al., 2023) are all embedded sequence parallelism methods. As shown in Figure 1, these embedded methods shard along a single sequence dimension, which are tailored to the specific pattern and introduce extra communication and complex code modification. \n\nHowever, multi-dimensional transformers calculate independently across multiple sequence dimensions. For instance, for video generation models like OpenSora (Zangwei Zheng, 2024) and Latte (Ma et al., 2024), Spatial-Temporal Attention (Yan et al., 2021) is adopted which separates attention computations to independent temporal and spatial computation. Therefore, there exists a potential space for a new sequence parallelism paradigm. \n\nTo adapt to the flexible patterns of multi-dimensional transformers, we introduce Dynamic Sequence Parallelism (DSP) as a novel abstraction of sequence parallelism, featured by its elegant design, high effectiveness, and excellent compatibility.",
            "reference_string": "[268510227 | Zhao et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Accelerating Large Language Model Training with 4D Parallelism and Memory Consumption Estimator",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 18,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.06465, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2298889885",
                    "name": "Kazuki Fujii"
                },
                {
                    "authorId": "2330230064",
                    "name": "Kohei Watanabe"
                },
                {
                    "authorId": "2294362068",
                    "name": "Rio Yokota"
                }
            ],
            "abstract": "In large language model (LLM) training, several parallelization strategies, including Tensor Parallelism (TP), Pipeline Parallelism (PP), Data Parallelism (DP), as well as Sequence Parallelism (SP) and Context Parallelism (CP), are employed to distribute model parameters, activations, and optimizer states across devices. Identifying the optimal parallelization configuration for each environment while avoiding GPU memory overflow remains a challenging task. In this study, we provide precise formulas to estimate the memory consumed by parameters, gradients, optimizer states, and activations for 4D parallel training (DP, TP, PP, CP) in the Llama architecture. We conducted 454 experiments on A100 and H100 GPUs, incorporating often neglected factors such as temporary buffers and memory fragmentation into our analysis. Results indicate that when the estimated memory usage is below 80\\% of the available GPU memory, the training never encounters out-of-memory errors. This simple yet effective formula allows us to identify parallelization configurations that could lead to memory overflow in advance, significantly reducing the configuration search space. Additionally, through a comprehensive exploration of optimal configurations in 4D parallelism, our analysis of the 454 experimental results provides empirical insights into optimal 4D parallelism configurations.",
            "corpus_id": 273963185,
            "sentences": [
                {
                    "corpus_id": "273963185",
                    "title": "Accelerating Large Language Model Training with 4D Parallelism and Memory Consumption Estimator",
                    "text": "Unlike Sequence Parallelism (Korthikanti et al., 2023), which only parallelizes the activations of Dropout and LayerNorm, Context Parallelism enables partitioning of the model's network inputs and all activations along the sequence dimension. \n\nLlama 2 (Touvron et al., 2023) had a sequence length of 4,096 tokens, but Llama 3 (Dubey et al., 2024) increased this to 8,192 tokens, and Llama 3.1 further extended it to 131,072 tokens. As efficient training that supports long contexts is increasingly demanded, context parallelismwhich allows partitioning along the sequence dimensionis extremely useful for reducing the activations per GPU. \n\nIn components other than self-attention, there are no intertoken operations; thus, introducing context parallelism does not alter the operation. However, in self-attention layers, inter-token operations occur, necessitating the gathering of the full sequence, which requires additional all-gather communications between GPUs in the forward pass. During backpropagation, reduce-scatter is applied to the activation gradients, and each GPU stores only its sequence chunk to reduce the activation memory footprint. \n\nTo date, there is no comprehensive performance evaluation of 4D parallelism (DP, TP, PP, CP) utilizing context parallelism, and knowledge for applying it to actual LLM training is lacking.",
                    "score": 0.5869973283977934,
                    "section_title": "Parallelism",
                    "char_start_offset": 6612,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 242
                        },
                        {
                            "start": 245,
                            "end": 432
                        },
                        {
                            "start": 433,
                            "end": 639
                        },
                        {
                            "start": 642,
                            "end": 786
                        },
                        {
                            "start": 787,
                            "end": 987
                        },
                        {
                            "start": 988,
                            "end": 1153
                        },
                        {
                            "start": 1156,
                            "end": 1344
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 28,
                            "end": 54,
                            "matchedPaperCorpusId": "248693351"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5078125
                }
            ],
            "relevance_judgement": 0.5078125,
            "relevance_judgment_input_expanded": "# Title: Accelerating Large Language Model Training with 4D Parallelism and Memory Consumption Estimator\n# Venue: arXiv.org\n# Authors: Kazuki Fujii, Kohei Watanabe, Rio Yokota\n## Abstract\nIn large language model (LLM) training, several parallelization strategies, including Tensor Parallelism (TP), Pipeline Parallelism (PP), Data Parallelism (DP), as well as Sequence Parallelism (SP) and Context Parallelism (CP), are employed to distribute model parameters, activations, and optimizer states across devices. Identifying the optimal parallelization configuration for each environment while avoiding GPU memory overflow remains a challenging task. In this study, we provide precise formulas to estimate the memory consumed by parameters, gradients, optimizer states, and activations for 4D parallel training (DP, TP, PP, CP) in the Llama architecture. We conducted 454 experiments on A100 and H100 GPUs, incorporating often neglected factors such as temporary buffers and memory fragmentation into our analysis. Results indicate that when the estimated memory usage is below 80\\% of the available GPU memory, the training never encounters out-of-memory errors. This simple yet effective formula allows us to identify parallelization configurations that could lead to memory overflow in advance, significantly reducing the configuration search space. Additionally, through a comprehensive exploration of optimal configurations in 4D parallelism, our analysis of the 454 experimental results provides empirical insights into optimal 4D parallelism configurations.\n## Parallelism\nUnlike Sequence Parallelism (Korthikanti et al., 2023), which only parallelizes the activations of Dropout and LayerNorm, Context Parallelism enables partitioning of the model's network inputs and all activations along the sequence dimension. \n\nLlama 2 (Touvron et al., 2023) had a sequence length of 4,096 tokens, but Llama 3 (Dubey et al., 2024) increased this to 8,192 tokens, and Llama 3.1 further extended it to 131,072 tokens. As efficient training that supports long contexts is increasingly demanded, context parallelismwhich allows partitioning along the sequence dimensionis extremely useful for reducing the activations per GPU. \n\nIn components other than self-attention, there are no intertoken operations; thus, introducing context parallelism does not alter the operation. However, in self-attention layers, inter-token operations occur, necessitating the gathering of the full sequence, which requires additional all-gather communications between GPUs in the forward pass. During backpropagation, reduce-scatter is applied to the activation gradients, and each GPU stores only its sequence chunk to reduce the activation memory footprint. \n\nTo date, there is no comprehensive performance evaluation of 4D parallelism (DP, TP, PP, CP) utilizing context parallelism, and knowledge for applying it to actual LLM training is lacking.",
            "reference_string": "[273963185 | Fujii et al. | 2024 | Citations: 2]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "263608461",
            "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context",
            "text": "Transformers have garnered significant attention in the field of AI and have become the backbone for numerous state-of-the-art models. Several works have explored memory-efficient techniques to address the memory limitations of Transformers and enable their application to a wider range of problems. Computing exact self-attention in a blockwise manner using the tiling technique [24] has led to the development of memory efficient attention mechanisms [30] and its efficient CUDA implementation [9], and blockwise parallel transformer [23] that proposes computing both feedforward and self-attention block-by-block, resulting in a significant reduction in memory requirements. \n\nIn line with these advancements, our work falls into the category of memory efficient computation for Transformers. Other works have investigated the approximation of attention mechanisms, yet these efforts have often yielded sub-optimal results or encountered challenges during scaling up. For an in-depth review of these techniques, we recommend referring to the surveys [26,35]. Another avenue of research explores various parallelism methods, including data parallelism [10], tensor parallelism [34], pipeline parallelism [27,15,28], sequence parallelism [21,18,17], and FSDP [11,31]. The activations of self-attention take a substantial amount of memory for large context models. \n\nTensor parallelism can only reduce parts of activations memory and sequence parallelism introduces a significant communication overhead that cannot be fully overlapped with computation. Prior work has studied sharding along sequence and attention heads, and gathering sequences via an optimized all-to-all topology, achieving reduced communication [17]. However, this method is restricted by the number of attention heads and requires gathering the full sequence on each device. In comparison, our approach fully overlaps communication with blockwise computation, enhancing its scalability. Prior work extends sequence parallelism for computing self-attention using a ring topology [21], which reduces the communication cost compared to standard sequence parallelism. However, overlapping communication with computation remains challenging due to the constraints of arithmetic intensity. The communication overheads render this approach infeasible for training and inference in largecontext scenarios.",
            "score": 0.8060153468254,
            "section_title": "Related Work",
            "char_start_offset": 21820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 677
                },
                {
                    "start": 680,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1364
                },
                {
                    "start": 1367,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2254
                },
                {
                    "start": 2255,
                    "end": 2368
                }
            ],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 499,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "258987968"
                },
                {
                    "start": 1154,
                    "end": 1158,
                    "matchedPaperCorpusId": "372467"
                },
                {
                    "start": 1206,
                    "end": 1210,
                    "matchedPaperCorpusId": "202488191"
                },
                {
                    "start": 1210,
                    "end": 1213,
                    "matchedPaperCorpusId": "53670168"
                },
                {
                    "start": 1213,
                    "end": 1216,
                    "matchedPaperCorpusId": "219720945"
                },
                {
                    "start": 1239,
                    "end": 1243,
                    "matchedPaperCorpusId": "246017095"
                },
                {
                    "start": 1264,
                    "end": 1267,
                    "matchedPaperCorpusId": "269617042"
                },
                {
                    "start": 2049,
                    "end": 2053,
                    "matchedPaperCorpusId": "246017095"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.646484375
        },
        {
            "corpus_id": "271083368",
            "title": "Achieving Peak Performance for Large Language Models: A Systematic Review",
            "text": "Sequence parallelism [15], [89], is a novel approach proposed to efficiently train Transformers with longer sequences on GPUs. It addresses the quadratic memory requirements of self-attention in Transformer models. Unlike traditional methods, it does not require a single device to handle the entire sequence. By splitting sequences into chunks and distributing them across devices, it achieves effective training with infinitely long sequences. It introduces Ring Self-Attention to enhance the process, demonstrating superior performance in batch size and sequence length compared to tensor parallelism, handling sequences over 27\u00d7 longer than existing methods.",
            "score": 0.78715895046751,
            "section_title": "5) Sequence Parallelism",
            "char_start_offset": 92308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 662
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 25,
                    "matchedPaperCorpusId": "240070340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85498046875
        },
        {
            "corpus_id": "263671659",
            "title": "DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training",
            "text": "Memory-efficient attention. Dao et al. (2022) and Lefaudeux et al. (2022) propose to use an online normalizer (Milakov & Gimelshein, 2018) to compute the attention in a blockwise and memory-efficient way. It reduces peak memory usage by not materializing large intermediate states, e.g. the attention softmax matrix. In addition, research on sparse attention computes only a sparse subset of the attention score, which also reduces the memory footprints yet may lead to inferior performance (Beltagy et al., 2020;Sun et al., 2022;Zaheer et al., 2020). In this work, we limit our scope to exact attention. \n\nSequence parallelism and ring attention Ring Self-Attention (Li et al., 2021) is among the first to parallelize Transformers in the sequence dimension. However, its distributed attention design is not optimized for causal language modeling and incompatible with memory-efficient attention, which are crucial for long-context LLM training. Ring Attention (Liu et al., 2023) proposes to compute distributed attention in a memory-efficient blockwise pattern. However, it is also not optimized for causal language modeling, leading to 2\u00d7 extra computation. DISTFLASHATTN optimizes for both memory-efficient attention and causal language modeling. More recently, DeepSpeed Ulysses (Jacobs et al., 2023) proposes a hybrid parallelism strategy. It computes distributed attention in the tensor model parallelism to address these two problems and utilizes sequence parallelism elsewhere (Shoeybi et al., 2019). We provide head-to-head comparison in Table 4. \n\nModel Parallelism and FSDP Tensor Model parallelism (Korthikanti et al., 2023) partitions model parameters and also distributes the activation in parallel LLM training. Pipeline model parallelism (Huang et al., 2019) also partitions the activations. However, it applies high memory pressure to the first pipeline stage. We show in \u00a7 4.2 that this leads to a less effective support for long sequences. Thus, we focus on comparing with tensor model parallelism and only consider pipeline parallelism when the number of heads is insufficient for tensor parallelism.",
            "score": 0.7853056516422943,
            "section_title": "Related work",
            "char_start_offset": 5333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1555
                },
                {
                    "start": 1558,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2120
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 45,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 530,
                    "end": 550,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1610,
                    "end": 1636,
                    "matchedPaperCorpusId": "248693351"
                },
                {
                    "start": 1754,
                    "end": 1773,
                    "matchedPaperCorpusId": "53670168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5078125
        },
        {
            "corpus_id": "246017095",
            "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
            "text": "Transformer achieves promising results on various tasks. However, self-attention suffers from quadratic memory requirements with respect to the sequence length. Existing work focuses on reducing time and space complexity from an algorithm perspective. In this work, we propose sequence parallelism, a memory-efficient parallelism to solve this issue from system perspective instead. Our approach is compatible with most existing parallelisms (e.g., data, pipeline, and tensor parallelism), which means our sequence parallelism makes 4D parallelism possible. More importantly, we no longer require a single device to hold the whole sequence. Besides, using efficient attention with linear complexity, our sequence parallelism enables us to train transformer with infinite long sequence. Specifically, we split the input sequence into multiple chunks and feed each chunk into its corresponding device (i.e., GPU). To compute the attention output, we integrated ring-style communication with self-attention calculation and proposed Ring Self-Attention (RSA). Experiments show that sequence parallelism performs well when scaling with batch size and sequence length. Compared with tensor parallelism, our approach achieved 13.7\\times and 3.0\\times maximum batch size and sequence length respectively when scaling up to 64 NVIDIA P100 GPUs. With efficient attention, sequence can handle sequence with over 114K tokens, which is over 27\\times longer than existing efficient attention works holding the whole sequence on a single device.",
            "score": 0.7784116088962301,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92236328125
        },
        {
            "corpus_id": "276106883",
            "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
            "text": "Transformers have emerged as one of the most successful AI/ML techniques for modeling sequence data. They have drastically impacted a variety of scientific and engineering applications such as language modeling [1], molecular design [2], cancer pathology classification [3], genomic sequence modeling [4], and others. These models derive their power from an operation called the attention mechanism. Given a sequence of tokens of length L, the attention mechanism extracts pairwise similarities between the tokens [5]. \n\nHowever, the time and memory complexity of the attention mechanism is quadratic in L as it captures interactions between each pair of the sequence. This has severely limited the context length, i.e., the length of a sequence over which a transformer model can capture these interactions (also know as sequence length in the literature). For applications such as genomics, at least 4-5 orders of magnitude of increase in context length is needed [4]. \n\nA popular approach for reducing the computational complexity is to introduce sparsity in the attention mechanism [6]- [8]. In this approach, an L \u00d7 L 0-1 attention mask is used to capture the interactions between pairs. The mask is made sparse by reducing the number of interacting pairs (making the corresponding entries 0) in a structured manner. Local, dilated windowed, block, and random are popular patterns that are used to introduce sparsity while ensuring that critical interactions are preserved to maintain the accuracy of the models. [7] has demonstrated that, even with an exponentially decreasing number of connections with respect to the distance between pairs, the test perplexity scores remain comparable to dense models. This demonstrates that utilizing sparsity to achieve ultra-long sequence modeling is a feasible approach. \n\nHowever, existing implementations of sparse attention mechanisms are not \"hardware-aware\". They perform densedense matrix multiplication followed by invalidation of entries based on the attention mask [9], [10]. Optimizations have focused on partitioning the matrices into blocks, applying matrix reordering techniques to obtain denser blocks, and performing block matrix multiplication only on the blocks that have nonzero entries in the mask (see Section III for details). However, these techniques end up performing excess computations as the block may contain several 0 mask elements.",
            "score": 0.7744861406706961,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1816
                },
                {
                    "start": 1819,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2293
                },
                {
                    "start": 2294,
                    "end": 2407
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 236,
                    "matchedPaperCorpusId": "268423149"
                },
                {
                    "start": 270,
                    "end": 273,
                    "matchedPaperCorpusId": "268040015"
                },
                {
                    "start": 301,
                    "end": 304,
                    "matchedPaperCorpusId": "259274952"
                },
                {
                    "start": 514,
                    "end": 517,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 966,
                    "end": 969,
                    "matchedPaperCorpusId": "259274952"
                },
                {
                    "start": 1086,
                    "end": 1089,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.271484375
        },
        {
            "corpus_id": "270062939",
            "title": "Sequence Length Scaling in Vision Transformers for Scientific Images on Frontier",
            "text": "Sequence parallelism, which distributes the self-attention computation in transformer models across the sequence dimension, has been an active area of research, with many developed methods employing various approaches.DeepSpeed-Ulysses [15]introduces a suite of system optimizations specifically designed for the efficient training of extreme long sequence transformer models.DeepSpeed-Ulysses partitions the input data along the sequence dimension, which allows the system to handle much longer sequences.For attention computation, it employs an all-to-all collective communication to ensure that each GPU receives a complete sequence, but only for a non-overlapping subset of the attention heads, allowing for parallel computation of attention across GPUs.LSS [16] divides a long sequence into segments distributed among GPUs, with each GPU computing a partial selfattention for its segment.It also introduces a fused communication strategy to minimize the communication overhead.Dynamic Sequence Parallelism (DSP) [22] enables efficient sequence parallelism for multi-dimensional transformers by dynamically switching the parallelism dimension according to the current computation stage.Ring attention [23], [24] is another variant that organizes tokens in a ring structure, allowing each token to attend to a fixed number of adjacent tokens in the sequence.This method offers a compromise between dense and sparse attention, balancing computational efficiency with the ability to capture local and semi-distant relationships.However, ring attention may still fall short in tasks where long-range dependencies are crucial.",
            "score": 0.7658026210688643,
            "section_title": "E. Sequence Parallel (SP)",
            "char_start_offset": 12371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 218,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 506
                },
                {
                    "start": 506,
                    "end": 758
                },
                {
                    "start": 758,
                    "end": 893
                },
                {
                    "start": 893,
                    "end": 982
                },
                {
                    "start": 982,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1361
                },
                {
                    "start": 1361,
                    "end": 1529
                },
                {
                    "start": 1529,
                    "end": 1625
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8359375
        },
        {
            "corpus_id": "258987968",
            "title": "Blockwise Parallel Transformer for Large Context Models",
            "text": "Transformers have garnered significant attention in the field of natural language processing (NLP) and have become the basis for numerous state-of-the-art models. Several works have explored memoryefficient techniques to address the memory limitations of Transformers and enable their application to longer input sequences. One line of research focuses on various approximation techniques or compressing along the sequence dimension [see e.g. 24,12,14,4,42,54,36,25]. Other works explored replacing attention [19,20,41,23,3,57,40,53]. Another line of work explores partitioning the large hidden dimension of the feedforward network into parts and retrieving only one part per token [30,48,17,26,58,60]. Additionally, extending the context by attending over states from previous sequences has been explored [13,44], as well as combining local and global contexts [21,11]. For a comprehensive review of these techniques, we recommend referring to the surveys by Tay et al. \n\n[51], Narang et al. [38], Tay et al. [50]. Several studies explored sharding large model on distributed devices tensor, data, or sequence parallelism [49,16,55,27,59,31,46]. Ours shares similarities with the sequence parallelism [27] where sequences are distributed across devices, in contrast, ours implements blockwise computation on sequences for each device. This creates an orthogonal relationship between our method and sequence parallelism, allowing for straightforward combination. In addition, our methodology is compatible with both tensor and data parallelism. Another direction involves computing exact self-attention in a blockwise manner using the tiling technique [37]. This approach has led to the development of memory efficient attention mechanisms [14,42]. In line with these advancements, our work falls into this category. We propose computing both the feedforward network and self-attention in a blockwise manner, resulting in a significant reduction in memory requirements.",
            "score": 0.7560676711761268,
            "section_title": "Related Work",
            "char_start_offset": 20945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1969
                }
            ],
            "ref_mentions": [
                {
                    "start": 443,
                    "end": 446,
                    "matchedPaperCorpusId": "232110866"
                },
                {
                    "start": 449,
                    "end": 452,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 509,
                    "end": 513,
                    "matchedPaperCorpusId": "221150566"
                },
                {
                    "start": 519,
                    "end": 522,
                    "matchedPaperCorpusId": "247011581"
                },
                {
                    "start": 689,
                    "end": 692,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 1142,
                    "end": 1145,
                    "matchedPaperCorpusId": "221191193"
                },
                {
                    "start": 1740,
                    "end": 1744,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74169921875
        },
        {
            "corpus_id": "272310078",
            "title": "Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer",
            "text": "Recent advancements in Transformer architectures have significantly enhanced their capability to process long sequences, which is crucial for tasks that require extensive contextual understanding. This section reviews pivotal contributions in this domain, each addressing the inherent memory limitations of standard Transformer models, while also pointing out some of their practical challenges. \n\nMegatron-SP (Korthikanti et al., 2023) adopts a sequence parallelism technique which is tightly integrated with its tensor parallelism. In this approach, sequences are partitioned along the sequence dimension, and all-gather and reduce-scatter collectives are employed to aggregate the QKV (query, key, value) projections for attention computation. The communication complexity analysis indicates that, in contrast to our approach, the communication volume in Megatron-SP's sequence parallelism increases linearly with the sequence length regardless of the number of compute devices. \n\nThe Blockwise Parallel Transformer (BPT) (Liu & Abbeel, 2024) employs a blockwise computation strategy for both self-attention and feedforward layers, optimizing memory usage and allowing the processing of sequences much longer than traditional Transformers. However, despite its efficiency, BPT requires careful tuning of block sizes and memory management to avoid diminishing returns on performance when scaling to extremely long sequences. \n\nRing Attention (Liu et al., 2023) enhances Transformer's scalability by distributing long sequences across multiple devices. This innovative approach overlaps the communication of key-value pairs with the computation of blockwise attention, effectively increasing the feasible sequence length proportionally to the number of available devices. However, reliance on device count for scaling and multi-step communications introduces potential issues in environments with sub-optimal hardware regards network interconnects, where performance can be unpredictably affected by network latency and bandwidth constraints. \n\nDeepSpeed Ulysses (Jacobs et al., 2023) tackles the challenges of sequence parallelism by partitioning input data along the sequence dimension and utilizing an efficient allto-all collective communication strategy for attention computations. Although this method maintains a constant communication volume regardless of the increase in sequence lengths and device counts, achieving significant speedups and scalability, it may still encounter practical hurdles in deployment related to large-scale clusters and the optimization of communication patterns across diverse computing environments.",
            "score": 0.7464886684845268,
            "section_title": "Long context training",
            "char_start_offset": 6723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 981
                },
                {
                    "start": 984,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1426
                },
                {
                    "start": 1429,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 2043
                },
                {
                    "start": 2046,
                    "end": 2287
                },
                {
                    "start": 2288,
                    "end": 2637
                }
            ],
            "ref_mentions": [
                {
                    "start": 410,
                    "end": 435,
                    "matchedPaperCorpusId": "248693351"
                },
                {
                    "start": 1025,
                    "end": 1045,
                    "matchedPaperCorpusId": "266351737"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81103515625
        },
        {
            "corpus_id": "276906703",
            "title": "Linear attention based spatiotemporal multi graph GCN for traffic flow prediction",
            "text": "There are two main strategies to mitigate the memory and computational complexity of transformers, which inherently scale quadratically with sequence length. The first strategy focuses on refining the Attention Mechanism 13 . A plethora of transformer derivatives, termed \"x-formers\" have been developed, employing tactics such as sparse attention [36][37][38] , full attention matrix approximation 19,39,40 , and chunked attention with gating 41 . Notably, recent innovations like FlashAttention 17 improve memory efficiency, but time complexity remains an issue, either remaining quadratic or influenced by hidden factors like chunk size. Conversely, RWKV 20 exemplifies superior space-time complexity trade-offs during inference by ingeniously casting linear attention as an RNN. \n\nThe second strategy involves supplanting the attention mechanism with alternative modules for scalability in long sequences. For instance, MLP-Mixer 42,43 advocates replacing attention with Multi-Layer Perceptrons in computer vision. Additionally, the Attention Free Transformer (AFT) 44 replaces dot-product self-attention with an efficient variant resembling multi-head attention in its feature dimension correspondence. Concurrently, RNN-based components have been adapted to extend context length, such as the Recurrent Memory Transformer 45 and Linear Recurrent Units 46 .",
            "score": 0.7411896836499137,
            "section_title": "Optimizing attention mechanism",
            "char_start_offset": 5842,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 782
                },
                {
                    "start": 785,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1362
                }
            ],
            "ref_mentions": [
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "245144820"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1265869140625
        },
        {
            "corpus_id": "248227781",
            "title": "FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks",
            "text": "Attention mechanisms, the key building block of transformer models, have enabled state-of-the-art results across a wide range of machine learning (ML) tasks-from natural language processing (NLP) [17,53,90], to object detection [6,82,111], image classification [28,100,108,110], image generation [7,21,66], and music synthesis [34,35].\n\nThis exponential growth of transformer models are expected to serve as the foundation of a new bread of machine learning models in the upcoming years. A key attribute of attention-based models is the sequence length ( ) defining the number of input elements for which a pairwise correlation scores is computed. Intuitively, increasing sequence length enables the attention-based models to better capture the context of input sentences or the relation between image segments. The demand for leveraging long-sequence (e.g. = 8 to = 69 ) attention-based models has already emerged in ML community [87], beyond natural language understanding [70] into protein folding [14] and text summarization [47] and audio generation [57]. Employing long sequences is pivotal in these algorithms because the property of input emerges from the global context. For example, two proteins may look identical if we examine identical sequence fragments, but when the entire sequence is considered, the differences in their function arise. We observe an analogous phenomenon in text summarization, where context can drastically alter the meaning of the selected text subset. In that instance, the subset represents a shorter sequence while the entire context refers to the full-length one.\n\nCompared to existing neural network accelerators [9,11,20,67,78,105], architecting accelerators for attention-based models poses different design challenges, attributed to their soaring demand for on-chip memory and compute complexities. Recent accelerators for attention-based models [30,31] have mainly relied on algorithmic optimizations, often with negative repercussion on model accuracy. Algorithmic techniques in practice include sparsification or compression [5, 12-14, 16, 17, 43, 47, 56, 66, 68, 70, 73, 80, 86, 94, 104] and/or leveraging lossy approximation [30,31,93].\n\nIn this work",
            "score": 0.7257594750978744,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 206,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 234,
                    "end": 238,
                    "matchedPaperCorpusId": "222208633"
                },
                {
                    "start": 261,
                    "end": 265,
                    "matchedPaperCorpusId": "233004577"
                },
                {
                    "start": 265,
                    "end": 269,
                    "matchedPaperCorpusId": "232417787"
                },
                {
                    "start": 273,
                    "end": 277,
                    "matchedPaperCorpusId": "235743105"
                },
                {
                    "start": 296,
                    "end": 299,
                    "matchedPaperCorpusId": "219781060"
                },
                {
                    "start": 331,
                    "end": 334,
                    "matchedPaperCorpusId": "54477714"
                },
                {
                    "start": 931,
                    "end": 935,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 1654,
                    "end": 1657,
                    "matchedPaperCorpusId": "207882941"
                },
                {
                    "start": 1660,
                    "end": 1663,
                    "matchedPaperCorpusId": "11504619"
                },
                {
                    "start": 1663,
                    "end": 1666,
                    "matchedPaperCorpusId": "211114941"
                },
                {
                    "start": 1666,
                    "end": 1669,
                    "matchedPaperCorpusId": "202547735"
                },
                {
                    "start": 1669,
                    "end": 1673,
                    "matchedPaperCorpusId": "21656909"
                },
                {
                    "start": 1894,
                    "end": 1897,
                    "matchedPaperCorpusId": "235414966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08038330078125
        },
        {
            "corpus_id": "273849947",
            "title": "LASER: Attention with Exponential Transformation",
            "text": "The attention mechanism was used in Bahdanau et al. (2015) to drastically improve machine translation performance compared to encoder-decoder recurrent neural networks (RNNs) (Cho, 2014). This was later adopted in Transformers (Vaswani et al., 2017), which introduced self-attention to improve the performance in machine translation even further. Efficient attention mechanisms have been an active area of research due to the quadratic computational complexity in sequence length of Attention, which prevents long-context language modeling. One notable contribution is Linear Attention (Katharopoulos et al., 2020), which reduces the quadratic complexity of self-attention to linear in sequence length by using kernel approximation of the softmax operation. Similarly, the Performer (Choromanski et al., 2021) develops an alternative kernel approximation using random feature maps to achieve linear complexity. \n\nThe Mamba architecture introduces state-space models (SSMs) as a replacement for traditional attention. Models like S6 (S4+selection+scan) (Gu & Dao, 2023) from Mamba and SSD from Mamba-2 (Dao & Gu, 2024) have linear computational complexity in sequence length without the use of attention. However, despite these innovations, attention-based models like Gemini 1.5 Flash (Gemini, 2024) and LLaMA 3 (Dubey et al., 2024) continue to dominate long context regime, particularly through advancements in context parallelism (Liu et al., 2023), which ensures scalability while maintaining the strengths of attention mechanisms in Transformer models. \n\nEfficient attention mechanisms have become critical in handling long sequences, especially in Transformer-based architectures. This mechanism is used for faster inference and training, particularly when scaling up to large sequence lengths. Sparse Transformers (Child et al., 2019) use fixed sparse attention patterns, enabling them to efficiently handle very long sequences by reducing the quadratic complexity of standard attention to linear or sub-quadratic in practice. Routing Transformers (Roy et al., 2021) take a different approach by introducing a mechanism that sparsifies attention through data-dependent sparsity patterns with subquadratic computational complexity in sequence length.",
            "score": 0.7177528938500317,
            "section_title": "RELATED WORK",
            "char_start_offset": 5651,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1556
                },
                {
                    "start": 1559,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2255
                }
            ],
            "ref_mentions": [
                {
                    "start": 36,
                    "end": 58,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 227,
                    "end": 249,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 586,
                    "end": 614,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 783,
                    "end": 809,
                    "matchedPaperCorpusId": "222067132"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28857421875
        },
        {
            "corpus_id": "272753750",
            "title": "WallFacer: Harnessing Multi-dimensional Ring Parallelism for Efficient Long Sequence Model Training",
            "text": "Over the past decade, Transformer [41] models have made remarkable strides in diverse fields, including computer vision (CV) and natural language processing (NLP). As the technology has evolved, the ability to efficiently process long sequences with Transformer has emerged as a pivotal challenge. For instance, in text summarization, the ability to handle extensive sequences is vital, as the content to be summarized can range from lengthy chapters to entire books [19,3]. Similarly, chat-based applications, such as ChatGPT [1], require the capacity to process extensive dialogue histories to ensure conversational consistency. There are also applications in other fields like video generation [5,33] and protein structure prediction [16,7]. \n\nThe long context in the above scenarios has introduced several challenges for model training and inference: 1) Efficiency and Adaptability. The challenge of efficiency predominantly lies in handling long sequences that require quadratic computations during attention, and in addressing the large amount of communication during distributed processing. 2) Memory. Besides the major obstacle of storing the model weight and optimizer states, the activation has also exceeded the capacity of a single GPU and risen as a new memory challenge due to the extreme sequence length. 3) Scalability. Current Transformer models usually require thousands of GPUs for pre-training, even with datasets of regular lengths. For longer sequences, ensuring an acceptable scaling speedup rate with both the sequence length and the number of GPUs increasing is even more critical to reducing time and economic costs. \n\nTraditional parallelisms such as Data Parallelism [12,40,25,44], Tensor Parallelism [40,42,43], and Pipeline Parallelism [14,11,24,27] distribute the model, input batch, and the optimizer states, but can not directly address the large memory requirement of extremely long sequences as the sequence length dimension remains unchanged. To break through this obstacle, Sequence Parallelism has been introduced, splitting the input on the sequence length dimension. Mainstream Sequence Parallelism schemes can generally be classified into two categories: those based on all-to-all communication, and those based on ring peer-to-peer communication. Methods like DeepSpeed Ulysses [15], which are based on all-to-all communication, offer efficiency but require the splitting of attention heads.",
            "score": 0.7159530658914602,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1642
                },
                {
                    "start": 1645,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2288
                },
                {
                    "start": 2289,
                    "end": 2433
                }
            ],
            "ref_mentions": [
                {
                    "start": 467,
                    "end": 471,
                    "matchedPaperCorpusId": "250118028"
                },
                {
                    "start": 737,
                    "end": 741,
                    "matchedPaperCorpusId": "235959867"
                },
                {
                    "start": 741,
                    "end": 743,
                    "matchedPaperCorpusId": "267789157"
                },
                {
                    "start": 1695,
                    "end": 1699,
                    "matchedPaperCorpusId": "2315965"
                },
                {
                    "start": 1702,
                    "end": 1705,
                    "matchedPaperCorpusId": "199543548"
                },
                {
                    "start": 1705,
                    "end": 1708,
                    "matchedPaperCorpusId": "20425665"
                },
                {
                    "start": 1733,
                    "end": 1736,
                    "matchedPaperCorpusId": "251979875"
                },
                {
                    "start": 1773,
                    "end": 1776,
                    "matchedPaperCorpusId": "235898937"
                },
                {
                    "start": 1776,
                    "end": 1779,
                    "matchedPaperCorpusId": "261339639"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "246017095",
            "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
            "text": "In this paper, we proposed sequence parallelism for training Transformer-based models with longer sequence. Sequence parallelism is designed to break the limitation of sequence length on a single device (i.e., GPU). We have shown that sequence parallelism can handle longer sequence and is more memory-efficient than SoTA. In particular, sequence parallelism achieves 3.0\u00d7 maximum sequence length and 13.7\u00d7 maximum batch size than tensor parallelism when scaling up to 64 GPUs. Unlike both tensor and pipeline parallelism, sequence parallelism is not limited by the smaller hyperparameters (e.g., number of attention heads, number of layers). Therefore, our sequence parallelism can be adapted as long as the sequence length is divisible by sequence parallel size. We used a language model (i.e., BERT) to evaluate our system. However, sequence parallelism can also be adapted to computer vision tasks. This work paves the way to process large images [Hou et al., 2019] by ViT [Dosovitskiy et al., 2020] as a larger image means more patches or longer sequences. In the future, we plan to integrate data, pipeline, tensor and sequence parallelism to construct 4D parallelism. This would enable us to train extremely large models with very long sequences.",
            "score": 0.7151411153240448,
            "section_title": "Conclusion",
            "char_start_offset": 18012,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 951,
                    "end": 969,
                    "matchedPaperCorpusId": "233289729"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9072265625
        },
        {
            "corpus_id": "232318583",
            "title": "Linear-Time Self Attention with Codeword Histogram for Efficient Recommendation",
            "text": "Considerable efforts have been made trying to scale Transformers to long sequences. Transformer-XL in [9] captures longer-term dependency by employing a segment-level recurrent mechanism, which splits the inputs into segments to perform attention. Sukhbaatar et al. [30] limited the self-attention context to the closest samples. However, these techniques do not improve the  ( 2 ) asymptotic complexity of self-attention. \n\nIn another line of work, attempts in reducing the asymptotic complexity are made. Child et al. [6] proposed to factorize the attention computation into local and strided ones. Tay et al. [33], on the other hand, improved local attention by introducing a differentiable sorting network to re-sort the buckets. Reformer [19] hashes the query-keys into buckets via hashing functions based on random projection, and attention is computed within each bucket. In a similar manner, Roy et al. [28] assign tokens to buckets through clustering. Built on top of ETC [1], Big Bird [44] considers a mixture of various sparse patterns, including sliding window attention and random attention. Clustered Attention, introduced in [36], however, groups queries into clusters and perform attention on centroids. Linformer [37] resorts to a low-rank projection on the length dimension. However, it can only operate in a bidirectional mode without casual masking. \n\nMost of the aforementioned approaches rely on sparse attention patterns, while our method performs full contextual attention over the whole sequence. Besides, Linformer and Sinkhorn Transformer assume a fixed sequence length due to the use of sorting network and projection, while our method poses no such constraint. Our method is also notably faster than the existing approaches, enjoying an asymptotic complexity of  (), while inner product can be stored in a table.",
            "score": 0.7110068838285188,
            "section_title": "Improving Efficiency of Attention",
            "char_start_offset": 7237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1369
                },
                {
                    "start": 1372,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1841
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 105,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "159041867"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 743,
                    "end": 747,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1140,
                    "end": 1144,
                    "matchedPaperCorpusId": "220424511"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061767578125
        },
        {
            "corpus_id": "268510227",
            "title": "DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers",
            "text": "Efficiently scaling multi-dimensional transformers to accommodate long sequences is necessary across diverse domains, including video generation (Singer et al., 2022;Blattmann et al., 2023;Ma et al., 2024), image generation (Ramesh et al., 2021;Rombach et al., 2022;Liu et al., 2024), protein structure prediction (Jumper et al., 2021), spatial-temporal information processing (Cong et al., 2021), and beyond. The long length of sequences introduces substantial activation memory costs and notable slowdown for speed, underscoring the need for employing parallelism. \n\nApart from data parallel and pipeline parallel (Huang et al., 2019) which cannot reduce memory cost and inference time, Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). \n\nsequence parallel is the only option. Current sequence parallelism, such as Megatron-LM (Shoeybi et al., 2019), Ring-Attention (Li et al., 2021;Liu et al., 2023a), Megatron-SP (Korthikanti et al., 2022), and DeepSpeed-Ulysses (Jacobs et al., 2023) are all embedded sequence parallelism methods. As shown in Figure 1, these embedded methods shard along a single sequence dimension, which are tailored to the specific pattern and introduce extra communication and complex code modification. \n\nHowever, multi-dimensional transformers calculate independently across multiple sequence dimensions. For instance, for video generation models like OpenSora (Zangwei Zheng, 2024) and Latte (Ma et al., 2024), Spatial-Temporal Attention (Yan et al., 2021) is adopted which separates attention computations to independent temporal and spatial computation. Therefore, there exists a potential space for a new sequence parallelism paradigm. \n\nTo adapt to the flexible patterns of multi-dimensional transformers, we introduce Dynamic Sequence Parallelism (DSP) as a novel abstraction of sequence parallelism, featured by its elegant design, high effectiveness, and excellent compatibility.",
            "score": 0.704240112108693,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1756
                },
                {
                    "start": 1759,
                    "end": 2004
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 266,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 266,
                    "end": 283,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 314,
                    "end": 335,
                    "matchedPaperCorpusId": "248693351"
                },
                {
                    "start": 616,
                    "end": 636,
                    "matchedPaperCorpusId": "53670168"
                },
                {
                    "start": 1556,
                    "end": 1573,
                    "matchedPaperCorpusId": "232428140"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53564453125
        },
        {
            "corpus_id": "268875936",
            "title": "Linear Attention Sequence Parallelism",
            "text": "Sequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$ longer than existing SP methods. Code is available at: https://github.com/OpenNLPLab/LASP.",
            "score": 0.6972127476534055,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86767578125
        },
        {
            "corpus_id": "262826014",
            "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
            "text": "Shown in Figure 1 is a sketch of building blocks of a typical multihead attention Transformer architecture [Vaswani et al., 2017]. It consists of input sequences which are projected into queries (Q),keys (K) and values (V) embeddings. \n\nQKV are typically a 3D tensor of size N, b, d where N is sequence length, b is micro batch size and d is hidden dimension. \n\nThe QKV tensors are fed to the attention block, a central component of Transformer model. Outputs of attentions are inputs to the multilayer perceptron (MLP) or position-wise feed-forward block of Transformer architecture. \n\nThe attention block followed by MLP block are replicated multiple times to form an encoder, a decoder or an encoderdecoder Transformer network. It is worth to note that our proposed approach is orthogonal to both data parallelism and ZeRO. Our proposed approach can be used with both methods. Also, by leveraging sequence parallelism to keep global batch size at reasonable size on large systems, we effectively ameliorate the impact of large batch size on model convergence. Sequence parallelism serves two purposes in this regard. First, sequence parallelism can accelerate time to solution for same (already explored) long sequence length; in other words, sequence parallelism reduces the iteration time proportional to additional compute resources. Second, sequence parallelism enables longer sequence training or continual pretraining where training context length gradually increase over time [Xiong et al., 2023]. Consider a real world scenario of large scale training on 1024 GPUs. The initial exploratory or pretraining set up of a (proxy) LLM has a sequence length of 8192 (8K), a micro batch size of 1 (thus, 8 million token global size) per GPU. A simple change to improve the quality of the pretrained model requires a change of sequence length from 8K to 32K, which would result in approximately 32 million global batch size. However, increasing the global batch size is not an option due to the negative impact on model quality. Therefore, sequence parallelism comes in handy as a system optimization technique with no requirement for laborious hyperparameter search.",
            "score": 0.6952242035281685,
            "section_title": "Transformer Architecture",
            "char_start_offset": 5182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 234
                },
                {
                    "start": 237,
                    "end": 359
                },
                {
                    "start": 362,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2169
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.779296875
        },
        {
            "corpus_id": "259858862",
            "title": "Adaptive Attention for Sparse-based Long-sequence Transformer",
            "text": "Transformer-based models (Vaswani et al., 2017) have achieved state-of-the-art performance on a wide variety of natural language processing tasks (Devlin et al., 2019;Liu et al., 2019;Yang et al., 2019). It is also gradually applied to other research fields such as speech and computer vision (Dong et al., 2018;Li et al., 2019;Zhang et al., 2020;Dosovitskiy et al., 2021;Zhu et al., 2021;Touvron et al., 2021). Although self-attention module, the core component in Transformer, can capture global contexts from the whole sequence, the time and memory complexity are both quadratic to the sequence length. Especially when facing longer sequences, Transformer becomes more difficult to process them efficiently and effectively. \n\nRecently, a wide spectrum of efficient Transformers (Child et al., 2019;Ho et al., 2019;Rae et al., 2020;Zhao et al., 2019;Kitaev et al., 2020;Tay et al., 2020;Beltagy et al., 2020;Choromanski et al., 2020;Wang et al., 2020;Zaheer et al., 2020;Roy et al., 2021;Xiong et al., 2021;Tay et al., 2021a;Ma et al., 2021;Chen, 2021;Zhu and Soricut, 2021;Liu et al., 2022) have been proposed to tackle the problem of efficiency, which can be roughly divided into the following directions: sparse attention, low-rank and kernel methods. Because sparse-based attention is intuitive and interpretable in addition to efficiency, we focus on this method in this paper. It usually utilizes some strategies or patterns to limit the number of tokens involved in the attention calculation. Different from traditional sparse Transformer (Martins and Astudillo, 2016;Correia et al., 2019;Peters et al., 2019) with different softmax and pattern-related quadratic computation, recent works mainly adopt sliding windows to achieve linear complexity.",
            "score": 0.6936743168507791,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1756
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 47,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 146,
                    "end": 167,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 184,
                    "end": 202,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 312,
                    "end": 328,
                    "matchedPaperCorpusId": "59413863"
                },
                {
                    "start": 347,
                    "end": 372,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 389,
                    "end": 410,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 817,
                    "end": 834,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 852,
                    "end": 872,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 872,
                    "end": 889,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 973,
                    "end": 990,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 990,
                    "end": 1009,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 1009,
                    "end": 1027,
                    "matchedPaperCorpusId": "218487423"
                },
                {
                    "start": 1027,
                    "end": 1043,
                    "matchedPaperCorpusId": "235313355"
                },
                {
                    "start": 1043,
                    "end": 1054,
                    "matchedPaperCorpusId": "237421288"
                },
                {
                    "start": 1054,
                    "end": 1076,
                    "matchedPaperCorpusId": "236428421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1497802734375
        },
        {
            "corpus_id": "273850602",
            "title": "$k$NN Attention Demystified: A Theoretical Exploration for Scalable Transformers",
            "text": "Transformer models have become the dominant neural architecture across language, vision, and other domains [Vas17, DBK + 20]. However, scaling them to handle larger input sequences remains a significant challenge [TDA + 20], primarily due to the quadratic complexity of computing self-attention. Overcoming this limitation is crucial for advancing neural networks. Extending context length would enable Transformers to tackle complex tasks like book summarization [KRA + 21] and time-series forecasting [WZZ + 22, ZCZX23, ZZP + 21]. Furthermore, improving attention efficiency would reduce the computational burden of training, making these models more accessible. Bridging this \"compute divide\" is vital for democratizing AI [AW20]. \n\nEfficient computation of self-attention has been a focal point of research in recent years [FCA23]. Flash Attention [DFE + 22] and related work [SY24] optimize the exact calculation of attention by minimizing wasted computation during GPU I/O operations. However, most approaches focus on approximating the attention function. Sparse Transformers improve efficiency by allowing each token to attend to only a small subset of tokens [MLAC21]. These subsets are identified through deterministic methods [CGRS19, GQL + 19, SM20, LJX + 19, QML + 19, BPC20], randomized algorithms [KKL20, HJK + 23, ZHDK23, PPJF24], or adaptive techniques [CNM19]. Additionally, self-attention is often approximated using low-rank matrices and kernel methods [WLK + 20, TBM + 21, XZC + 21, KVPF20, CLD + 20]. On the negative side, recent finegrained complexity reductions indicate that achieving a good approximation with sub-quadratic time is not feasible across all scenarios [KWH23,AS24a]. \n\nIn this work, we focus on sparse attention methods where each token vector q i \u2208 R d attends to the k tokens k j \u2208 R d with the largest inner products q T i k j [GDG + 21, WWW + 22], a paradigm we refer to as kNN Attention.",
            "score": 0.6910970174642923,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 733
                },
                {
                    "start": 736,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 827,
                    "end": 834,
                    "matchedPaperCorpusId": "232380042"
                },
                {
                    "start": 1692,
                    "end": 1699,
                    "matchedPaperCorpusId": "252198880"
                },
                {
                    "start": 1699,
                    "end": 1705,
                    "matchedPaperCorpusId": "257219595"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.191650390625
        },
        {
            "corpus_id": "253080899",
            "title": "Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences",
            "text": "Transformers (Vaswani et al. 2017) designed for sequential data have revolutionized the field of Natural Language Processing (NLP) (Liu et al. 2019;Zhu et al. 2020;Li et al. 2022), and have recently made tremendous impact in graph learning (Yang et al. 2021;Dwivedi and Bresson 2020) and computer vision (Dosovitskiy et al. 2020;Huynh 2022). The self-attention used by regular Transformer models comes with a quadratic time and memory complexity O(n 2 ) for input sequence of length n, which prevents the application of Transformers to longer sequences in practical settings with limited computational resources. Recently, many efficient Transformers that improve computational efficiency have emerged. One line of works approximates the n \u00d7 n matrix multiplications by imposing a low-rank assumption on the attention structure, while the other line of works focuses on sparsification of the attention matrix. However, the improved computation efficiency always sacrifices expressiveness due to the following challenges: Approximations of full-attention. The first line of works avoid explicitly computing n \u00d7 n matrix through various approximations such as using the dot-product through kernalization (Wang et al. 2020;Katharopoulos et al. 2020) or random projections (Peng et al. 2021;Choromanski et al. 2020). However, such approximations are usually based on strict assumptions about the underlying attention structures such as the low-rank approximation (Shen et al. 2021;Tay et al. 2021a). There is currently a lack of rigorous guarantees for these assumptions to hold for potentially full-rank and dense self-attention matrices. Therefore these methods lead to empirically inferior results in sequence modeling tasks (Tay et al. 2021b;Ren et al. 2021), compared to the sparse Transformer approach. Slow information propagation. The current state-of-theart uses the sparse Transformer approach to approximate the full self-attention (see a local pattern example in Figure 1) (Zaheer et al. 2020). However, such sparsity-based approach can be lossy or even misleading in capturing important token correlations when they are not directly connected.",
            "score": 0.6896430486157739,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 34,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 148,
                    "end": 164,
                    "matchedPaperCorpusId": "210846009"
                },
                {
                    "start": 164,
                    "end": 179,
                    "matchedPaperCorpusId": "248986847"
                },
                {
                    "start": 240,
                    "end": 258,
                    "matchedPaperCorpusId": "238227259"
                },
                {
                    "start": 329,
                    "end": 340,
                    "matchedPaperCorpusId": "26501419"
                },
                {
                    "start": 1220,
                    "end": 1246,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1459,
                    "end": 1477,
                    "matchedPaperCorpusId": "215999966"
                },
                {
                    "start": 1477,
                    "end": 1494,
                    "matchedPaperCorpusId": "218487423"
                },
                {
                    "start": 1724,
                    "end": 1742,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 1742,
                    "end": 1757,
                    "matchedPaperCorpusId": "235829099"
                },
                {
                    "start": 1981,
                    "end": 2001,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1033935546875
        },
        {
            "corpus_id": "270703226",
            "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
            "text": "Long-range Transformers Self-attention is a cornerstone of Transformer success, but its quadratic complexity concerning input length poses challenges for tasks requiring long context.Numerous efficient approaches have emerged, spanning state-space models [30,62], recurrent neural networks [45,52,49], linear attention [55,38] and low-rank approximations of self-attention [75,14,53], which replace the self-attention with novel linear blocks for long-context modeling.Nonetheless, these approaches historically underperformed compared to modern Transformer models [70] in language modeling tasks until recent efforts [29,77].Besides, a few studies combine the Transformer with block-wise recurrence [17,35,36,12] or key-value compression [60,59,18].In contrast, our approach falls under sparse attention, reducing complexity by pruning the attention matrix.This approach is motivated by observations that the attention matrix in dense models naturally becomes sparse, and the performance of language models remains robust under reasonably sparse conditions [15,27,42].\n\nSparse attention Some sparse attention utilized fixed patterns to restrict the number of tokens involved, such as sliding windows [56,51], dilated sliding windows [4,22], combination of patterns [34,13], or domain-specific patterns [31].Recent studies have aimed at achieving constant memory costs during inference through predefined heuristic cache eviction policies [81,42,27].However, these static methods often prove suboptimal in various scenarios [66,2].Alternatively, sparse patterns can be learned in a data-driven manner.For example, Reformer [39] employs locality-sensitive hashing for token clustering and do attention within a cluster, while Routing Transformers [61], Cluster-Former [74] and Clustered Attention [73] use K-Means clustering on tokens.Besides, Sparse Sinkhorn Attention [68] establishes sparsity by sorting blocks of inputs.Despite achieving sub-quadratic complexity, these methods still remain above linear complexity and face challenges when handling extremely long sequences or failing to offer constant memory cost during inference.",
            "score": 0.6890826867434406,
            "section_title": "Related Work",
            "char_start_offset": 5218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 183,
                    "end": 469
                },
                {
                    "start": 469,
                    "end": 626
                },
                {
                    "start": 626,
                    "end": 750
                },
                {
                    "start": 750,
                    "end": 858
                },
                {
                    "start": 858,
                    "end": 1069
                },
                {
                    "start": 1071,
                    "end": 1308
                },
                {
                    "start": 1308,
                    "end": 1450
                },
                {
                    "start": 1450,
                    "end": 1531
                },
                {
                    "start": 1531,
                    "end": 1601
                },
                {
                    "start": 1601,
                    "end": 1834
                },
                {
                    "start": 1834,
                    "end": 1923
                },
                {
                    "start": 1923,
                    "end": 2135
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "258832459"
                },
                {
                    "start": 323,
                    "end": 326,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 704,
                    "end": 707,
                    "matchedPaperCorpusId": "247011581"
                },
                {
                    "start": 1058,
                    "end": 1062,
                    "matchedPaperCorpusId": "184486746"
                },
                {
                    "start": 1205,
                    "end": 1208,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 1303,
                    "end": 1307,
                    "matchedPaperCorpusId": "259262301"
                },
                {
                    "start": 1746,
                    "end": 1750,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1869,
                    "end": 1873,
                    "matchedPaperCorpusId": "211505992"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.130126953125
        },
        {
            "corpus_id": "276259019",
            "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid",
            "text": "Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE.",
            "score": 0.6814620220617496,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "239885427",
            "title": "Hierarchical Transformers Are More Efficient Language Models",
            "text": "Transformer models (Vaswani et al., 2017) are capable of solving many sequence modeling tasks, including classical NLP tasks (Devlin et al., 2019), summarization (Zhang et al., 2020), language modeling (Radford et al., 2019;Brown et al., 2020), code generation (Chen et al., 2021), or even music generation (Huang et al., 2018;Dhariwal et al., 2020) and image generation (Parmar et al., 2018;Chen et al., 2020;Ramesh et al., 2021). One compelling feature of Transformers is their ability to handle long contexts given as part of the input. This is particularly visible in tasks where the output depends on parts of the context that may not be * Equal contribution. Order determined by coin toss. close-by in the generated sequence, like in summarization, where the summary may need to refer to information scattered across the context, or in largescale image generation, where pixels belonging to the same object may be far apart in the generation order. Transformers excel at such tasks thanks to self-attention, and they are used with longer and longer contexts. Transformer-XL Hourglass Figure 1: Bits-per-character vs. training cost for baseline (orange) and hierarchical Transformers (green). We observe significant perplexity improvements on enwik8 over the vanilla Transformer-XL baseline, see text for details.\n\nThe ability of Transformers to handle long contexts comes at a price: each self-attention layer, at least in its original form, has complexity quadratic in the length of the context. When a stack of n Transformer layers is used, both memory and time complexity is equal to O(L 2 n) where L is a sequence length and n number of decoder blocks. Due to this limitation, vanilla transformers are infeasible to train on tasks with very long input sequences, for instance, on high-resolution images. This issue has been studied extensively, and a number of techniques were introduced that modify attention mechanism without changing overall transformer architecture (Child et al., 2019;Roy et al., 2020;Ren et al., 2021). These sparse attention mechanisms reduce the complexity of self-attention but still force the model to operate on",
            "score": 0.6808277513229217,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.160888671875
        },
        {
            "corpus_id": "269982567",
            "title": "Dynamic Context Adaptation and Information Flow Control in Transformers: Introducing the Evaluator Adjuster Unit and Gated Residual Connections",
            "text": "Attention mechanisms, the core of transformer architectures, have seen various adaptations to enhance model performance and interpretability.[12] proposed the Reformer, which reduces memory consumption by limiting the self-attention computation to a subset of key elements.A significant advancement, known as the Sparse Transformer [5], employs sparse factorizations of the attention matrix, enabling the model to handle longer sequences efficiently without a corresponding rise in computational demands.[25] introduced Linformer, which projects the attention matrix into a lowerdimensional space, significantly reducing the computational complexity from quadratic to linear with respect to sequence length.This adaptation maintains performance while enhancing efficiency, making it suitable for longer sequences.[6] developed the Performer, which utilizes random feature maps through the Fast Attention Via positive Orthogonal Random features approach (FAVOR+) to approximate the softmax function in attention.This method allows the Performer to scale linearly in terms of memory and compute, irrespective of sequence length.",
            "score": 0.6797012284898587,
            "section_title": "Adaptations in attention mechanisms",
            "char_start_offset": 3767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 141,
                    "end": 273
                },
                {
                    "start": 273,
                    "end": 504
                },
                {
                    "start": 504,
                    "end": 707
                },
                {
                    "start": 707,
                    "end": 813
                },
                {
                    "start": 813,
                    "end": 1011
                },
                {
                    "start": 1011,
                    "end": 1126
                }
            ],
            "ref_mentions": [
                {
                    "start": 332,
                    "end": 335,
                    "matchedPaperCorpusId": "5590763"
                },
                {
                    "start": 813,
                    "end": 816,
                    "matchedPaperCorpusId": "222067132"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1087646484375
        },
        {
            "corpus_id": "276775748",
            "title": "Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer",
            "text": "Transformer [5] architecture represents a significant advancement in artificial intelligence, bringing forth unparalleled capabilities alongside the challenge of resourceintensive training and serving processes. Significant enthusiasm has been ignited for actively enhancing the efficiency of transformer architectures. A preponderance of the works concentrate on reducing memory usage and improving computational efficiency. Sparse Transformer [6] introduces sparse attention mechanisms to selectively attend to relevant tokens within a sequence. Linformer [7] and Lightning-Attention [8,9] leverages linear-complexity self-attention mechanisms, catering to large-scale data and lengthy sequences. Reformer [10] mitigates memory constraints by employing reversible layers and locality-sensitive hashing techniques. Flash Attention [11] reduces memory access costs through tiling, while its subsequent version [12] further enhances performance by optimizing memory access and computation fusion. DeepSeek-V2 [13] introduces the Multi-Head Latent Attention (MLA) mechanism, which employs low-rank joint compression to enhance training efficiency and reduce the KV cache size during inference. Tensor Product Attention (TPA) [14] dynamically constructs QKV as context-dependent decomposed tensors, enabling adaptive adjustments and facilitating seamless integration with effective Rotary Position Embedding. \n\nThe other works aims to enhance the modeling capabilities of long sequences. Transformer-XL [15] introducing segmentlevel recurrence to enhance sequence modeling beyond the scope of the original Transformer. Sinkhorn Transformer [16] fuses Sinkhorn algorithm with self-attention mechanisms to improve sequence modeling accuracy. Long-Short-Term Memory Transformer [17] combines Transformer with Long Short-Term Memory (LSTM) mechanisms and thus enhance the modeling capability for long sequences. SeerAttention [18] integrates a learnable gating mechanism into standard attention mechanism, enabling adaptive selection of salient blocks within the attention map. Although These innovations have notably improved the performance of the transformer frameworks in their respective domains, they are typically model-specific and may not be universally applicable. By comparison, our work provides a model-free method. By simply adding selection mechanism to existing model parallel transformers, we can train a model more efficiently while maintaining or even improving its performance.",
            "score": 0.6785198155689008,
            "section_title": "A. Variants of Transformer",
            "char_start_offset": 6646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1405
                },
                {
                    "start": 1408,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2267
                },
                {
                    "start": 2268,
                    "end": 2321
                },
                {
                    "start": 2322,
                    "end": 2490
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 15,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 832,
                    "end": 836,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1637,
                    "end": 1641,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 1772,
                    "end": 1776,
                    "matchedPaperCorpusId": "250628527"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23681640625
        },
        {
            "corpus_id": "276106883",
            "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
            "text": "Transformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the input's context length - the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve\"true sparsity\"are lacking. In this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).",
            "score": 0.6782086569150801,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.403564453125
        },
        {
            "corpus_id": "275906620",
            "title": "ZETA: Leveraging Z-order Curves for Efficient Top-k Attention",
            "text": "Efficient Transformer The Transformer architecture (Vaswani et al., 2017) is foundational for sequence modeling, but its quadratic complexity limits efficiency with long sequences. Various efficient variants (Tay et al., 2022;2020;Chen et al., 2021;Qin et al., 2022b;Zhang et al., 2024) have been proposed as alternatives, mainly categorized into sparse, low-rank, and linear transformers. Sparse transformers, such as BigBird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020), restrict attention to local windows or global tokens to achieve linear complexity. SparseAxial (Ho et al., 2020) further enhances this by combining sparse attention with axial mechanisms for high-dimensional inputs. Reformer (Kitaev et al., 2020) locality-sensitive hashing to handle variable-length sequences efficiently. Low-rank transformers like Linformer (Wang et al., 2020) reduce the attention matrix to a lower-dimensional space, reducing memory and computation costs. Linear transformers such as Performer (Choromanski et al., 2021) use kernel-based approximations for linear-time complexity, while Nystr\u00f6mformer (Xiong et al., 2021) leverages Nystr\u00f6m decomposition for near-linear performance. \n\nTop-k Attention (Gupta et al., 2021) falls under the category of sparse attention, reducing attention complexity by selecting only the top-k most relevant tokens at each layer, thereby focusing computational resources on the most critical interactions. Unlimiformer (Bertsch et al., 2023) enables transformers to handle arbitrarily long sequences by chunking inputs and using a retrieval mechanism to attend to relevant past contexts. Similarly, IceFormer (Mao et al., 2024) improves transformer efficiency by integrating a k-nearest-neighbor (KNN) search mechanism that focuses on the KNN results as the most relevant tokens during inference, bypassing the need to compute the full attention matrix. However, with causal masks, these approaches can not compute the outputs of a long sequences in parallel, making them less efficient for training models from scratch by not fully exploiting the parallel computing power of GPUs.",
            "score": 0.6760199625028259,
            "section_title": "RELATED WORKS",
            "char_start_offset": 4960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 73,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 208,
                    "end": 226,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 231,
                    "end": 249,
                    "matchedPaperCorpusId": "240354799"
                },
                {
                    "start": 249,
                    "end": 267,
                    "matchedPaperCorpusId": "246904340"
                },
                {
                    "start": 267,
                    "end": 286,
                    "matchedPaperCorpusId": "267523164"
                },
                {
                    "start": 713,
                    "end": 734,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1003,
                    "end": 1029,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1110,
                    "end": 1130,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 1210,
                    "end": 1230,
                    "matchedPaperCorpusId": "235422257"
                },
                {
                    "start": 1460,
                    "end": 1482,
                    "matchedPaperCorpusId": "258436892"
                },
                {
                    "start": 1650,
                    "end": 1668,
                    "matchedPaperCorpusId": "269605811"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25390625
        },
        {
            "corpus_id": "269757781",
            "title": "USP: A Unified Sequence Parallelism Approach for Long Context Generative AI",
            "text": "Sequence parallelism (SP), which divides the sequence dimension of input tensors across multiple computational devices, is becoming key to unlocking the long-context capabilities of generative AI models. This paper investigates the state-of-the-art SP approaches, i.e. DeepSpeed-Ulysses and Ring-Attention, and proposes a unified SP approach, which is more robust to transformer model architectures and network hardware topology. This paper compares the communication and memory cost of SP and existing parallelism, including data/tensor/zero/pipeline parallelism, and discusses the best practices for designing hybrid 4D parallelism involving SP. We achieved 47% MFU on two 8xA800 nodes using SP for the LLAMA3-8B model training using sequence length 208K. Our code is publicly available at https://github.com/feifeibear/long-context-attention.",
            "score": 0.6709898285133628,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83203125
        },
        {
            "corpus_id": "268820218",
            "title": "On Difficulties of Attention Factorization through Shared Memory",
            "text": "In the era of big data and natural language processing, handling long-form text is crucial.Transformers (Vaswani et al., 2017) have shown promise in some tasks, but they do not scale well with longer inputs due to their quadratic time and memory complexity inherent in their attention framework.This challenge has given rise to multiple approaches designed to handle sequences exceeding typical input lengths, including attention reformulation for efficient computing (Rabe & Staats, 2021;Dao et al., 2022), exploration of weight sharing techniques (Dehghani et al., 2018;Raffel et al., 2019), heavy use of quantization (Shen et al., 2019) or replacing the attention operation itself with a faster alternative.\n\nIn the present work, we focus on designs that alter the Transformer architecture to lower the computational demands by leveraging an external memory in the form of a set of learnable vectors.Models like Linear Unified Nested Attention (Luna; Ma et al., 2021) or Perceiver (Jaegle et al., 2021) use it to factorize an attention operation into a sequence of attentions with a linear complexity, while the Memory Augmented Transformer (Wu et al., 2022) processes long inputs chunk-by-chunk using the memory as a hidden state to carry information between chunks.While these models adopt different perspectives on long input processing, they all leverage the attention mechanism as an interface for communication between the input and memory.The latter can be used as a convenient fixed-length dense representation of sparse inputs such as texts.\n\nGiven the properties of the attention operation, we discover the phenomenon which does not allow to utilize multiple memory cells properly, which we call memory degradation.Overcoming it may significantly improve the performance of the named models, and we propose several tweaks which lead to noticeable performance gains on the considered benchmarks.",
            "score": 0.6673894578775627,
            "section_title": "INTRODUCTION & RELATED WORK",
            "char_start_offset": 30,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 91,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 710
                },
                {
                    "start": 712,
                    "end": 903
                },
                {
                    "start": 903,
                    "end": 1270
                },
                {
                    "start": 1270,
                    "end": 1449
                },
                {
                    "start": 1449,
                    "end": 1553
                },
                {
                    "start": 1555,
                    "end": 1728
                },
                {
                    "start": 1728,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 126,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1144,
                    "end": 1161,
                    "matchedPaperCorpusId": "248157404"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1282958984375
        },
        {
            "corpus_id": "263835309",
            "title": "SparseCoder: Advancing Source Code Analysis with Sparse Attention and Learned Token Pruning",
            "text": "Self-Attention: Self-attention or full attention mechanism, is widely used in Transformer-based models. Given a sequence chunk with length n in natural language, we preprocess the chunk with basic natural language preprocessors. After conducting tokenization, each token is fully connected in the multi-head self-attention layer to make everything routed to everything. The highlighted principal diagonal means the attention of every token to itself as shown in (a) in Figure 3. The units are the tokens from the input sequence. The complexity is dominated by O(n 2 ). To advance the model efficiency, our SparseCoder modifies the Transformer architecture by adopting the the following sparse attention mechanism. \n\nLocal Attention: Although the full attention mechanism is powerful as a vital attribute of the transformer-based models, these prior proposed models have a core disadvantage due to the quadratic dependency in terms of memory. Consequently, given the available computing resources, this approach could not process entire long input sequences (with over 512 tokens) at the same time. To address this limitation, there are several research works that explored the feasibility of sparse attention mechanisms to analyze longer input sequences by reducing the overall algorithm complexity. Beltagy et al. [4] propose Longformer, a modified Transformer architecture by adopting local and global attention operations that scale linearly with the sequence length, making it practical for processing long sequences. Within the framework, the local attention mechanism utilizing a sliding window scheme is the crucial component in reducing the complexity and scaling the input to long sequences. \n\nGlobal attention remedies the attention dependency loss in the long sequence for the local attention by attending the special tokens to every other token in the input sequence. The technical details of these two attention mechanisms will be illustrated in the following subsections. There are several research papers also making attempts at sparse attention similar to Longformer by combining different global (e.g., random attention generated with graph) Fig. 4 A demonstration of sliding window mechanism for local attention in Transformer, where the token length is n, the window size is w and i is the i-th token in the sequence.",
            "score": 0.6645899193384279,
            "section_title": "Attention Mechanisms of Transformers",
            "char_start_offset": 21222,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2336
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.205078125
        },
        {
            "corpus_id": "235313355",
            "title": "Luna: Linear Unified Nested Attention",
            "text": "Transformers (Vaswani et al., 2017) are surprisingly versatile models that preform well on a wide range of language and vision tasks, including machine translation (Vaswani et al., 2017;Ott et al., 2018), language understanding (Devlin et al., 2019), image recognition (Dosovitskiy et al., 2020) and bioinformatics (Madani et al., 2020). Attention (Bahdanau et al., 2015) provides the key mechanism that captures contextual information from the entire sequence by modeling pairwise interactions between the inputs at every timestep. However, a common weakness of Transformers is their quadratic time and memory complexity within the attention mechanism w.r.t the length of the input sequence, which prohibitively restricts their potential application to tasks requiring longer input sequences. \n\nA number of techniques have been recently introduced to improve the time and memory efficiency of Transformer models ('xformers') (Tay et al., 2020b(Tay et al., , 2021)). One popular technique is using sparsity to restrict the attention field range, such as local attention (Parmar et al., 2018), blockwise attention (Qiu et al., 2019), strided attention patterns (Child et al., 2019;Beltagy et al., 2020), compressed attention (Liu et al., 2018), and attention with learnable patterns (Kitaev et al., 2020;Tay et al., 2020a;Roy et al., 2021). Another emerging approach is to improve efficiency by leveraging low-rank approximations of the attention matrix. Linformer (Wang et al., 2020), for example, projects the length dimension of key and value matrices to a fixed-dimensional representation by assuming low-rank structure in the full-rank attention matrix. Recently, some kernel-based methods, such as Linear Transformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020) and Random Feature Attention (Peng et al., 2021), attempt to efficiently approximate regular (softmax) full-rank attention through kernelization.",
            "score": 0.6636481632368602,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 35,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 164,
                    "end": 186,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 186,
                    "end": 203,
                    "matchedPaperCorpusId": "44131019"
                },
                {
                    "start": 228,
                    "end": 249,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 315,
                    "end": 336,
                    "matchedPaperCorpusId": "214725226"
                },
                {
                    "start": 348,
                    "end": 371,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 944,
                    "end": 965,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 1070,
                    "end": 1091,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 1224,
                    "end": 1242,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 1303,
                    "end": 1321,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 1321,
                    "end": 1338,
                    "matchedPaperCorpusId": "212718077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09088134765625
        },
        {
            "corpus_id": "268714807",
            "title": "Proxyformer: Nystr\u00f6m-Based Linear Transformer with Trainable Proxy Tokens",
            "text": "Transformer-based models (Vaswani et al. 2017) such as BERT (Kenton and Toutanova 2019) and GPT-3 (Brown et al. 2020) have demonstrated state-of-the-art performance in various Natural Language Processing (NLP) tasks, including question answering (Rajpurkar et al. 2016), summarization (Miller 2019), and language modeling (Child et al. 2019). Recently, they have extended their influence to a wide range of applications, including image processing (Touvron et al. 2021) and generative modeling (Child et al. 2019). The most significant factor contributing to the successful performance of Transformer models is the self-attention mechanism. This key component enables transformer models to effectively understand interactions between tokens in the input sequence, model long-range dependencies, and capture contextual information from the entire sequence. However, a well-known scalability issue arises from the quadratic dependency (i.e., O(n 2 )) of self-attention operations on the input sequence length n, leading to slow and memoryintensive processing for long sequence inputs. \n\nTo tackle the scalability challenge, several Efficient Transformers (Tay et al. 2022) have been recently introduced, aiming to reduce the quadratic dependency to a subquadratic level. For example, BigBird (Zaheer et al. 2020) exploits the sparsity of attention to reduce the complexity of attention operation. Reformer (Kitaev, Kaiser, and Levskaya 2020) learns the sparse attention pattern in a datadriven fashion. Linformer (Wang et al. 2020) leverages the low-rankness of the attention map. Nystr\u00f6mformer (Xiong et al. 2021) decomposes the softmax matrix of self-attention with the Nystr\u00f6m method (Wang and Zhang 2013). For an efficient approximation, the decomposed matrix uses landmarks sampled from the input sequence. \n\nAlthough these techniques have all demonstrated successful optimization of self-attention operations, they have their respective limitations. Fig. 1 illustrates a comparative overview of the performance, inference throughput, and memory footprint of various existing efficient transformer models on the Long Range Arena (LRA) benchmark (Tay et al. 2021).",
            "score": 0.6636423974907966,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1082
                },
                {
                    "start": 1085,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1809
                },
                {
                    "start": 1812,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2166
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 117,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 246,
                    "end": 269,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 285,
                    "end": 298,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 448,
                    "end": 468,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 1153,
                    "end": 1169,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 1290,
                    "end": 1310,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1404,
                    "end": 1439,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1593,
                    "end": 1612,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 1685,
                    "end": 1706,
                    "matchedPaperCorpusId": "6204627"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1778564453125
        },
        {
            "corpus_id": "254877252",
            "title": "A Length-Extrapolatable Transformer",
            "text": "Long-sequence Transformers aim to solve two key problems. First, the computation or memory consumption is not efficient enough for long sequences. Second, there is a trade-off between performance and efficiency. One popular solution (Wang et al., 2020b;Katharopoulos et al., 2020;Choromanski et al., 2020) is linear attention, i.e., using a kernel-based or low-rank approximation to replace vanilla attention. The methods typically target efficiency while underperforming vanilla Transformers for regular length. Another strand is sparse attention Beltagy et al., 2020;Zaheer et al., 2020;Xiong et al., 2021), which usually leverages structured sparsity to reduce computation. For causal sequence modeling, the recurrent-style designs Hutchins et al., 2022;Ma et al., 2022b) are also competitive.\n\nIn comparison, we focus on length extrapolation (Press et al., 2021) for language modeling, i.e., training on short texts while evaluating long texts. The training process is kept the same as vanilla Transformers. The capability of long-sequence modeling is given for free during inference. So training efficiency (which is typically expensive for large-scale language models) is not affected compared with previous work. Moreover, the performance on regular length is perfectly retained, without trade-offs for long-sequence modeling.",
            "score": 0.6627375944507463,
            "section_title": "Long-Sequence Transformers",
            "char_start_offset": 20468,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 569,
                    "end": 589,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 735,
                    "end": 757,
                    "matchedPaperCorpusId": "247451135"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14697265625
        },
        {
            "corpus_id": "265034082",
            "title": "Ultra-Long Sequence Distributed Transformer",
            "text": "The transformer is a powerful neural network architecture widely used in natural language and image processing (Vaswani et al., 2017). Its versatility is evidenced by its wide range of applications, including machine translation (Wang et al., 2019), chatbots (Caldarini et al., 2022), speech recognition (Dong et al., 2018), image captioning (Yu et al., 2019), image segmentation (Valanarasu et al., 2021;Strudel et al., 2021), and classification (Chen et al., 2021b). The transformer achieves its impressive performance by recognizing that different input sequence tokens have varying levels of importance to the final output prediction. The transformer captures the relationship between each pair of input tokens using a process called \"self-attention\". This allows the transformer to generate highly accurate outputs by focusing on the most relevant tokens in an input sequence while also paying attention to the overall context. This approach has proven to be highly effective and makes transformer a leading technology in artificial intelligence. \n\nWith long sequence training, transformer attends to many more input tokens than a transformer trained with short sequences. Therefore, long sequence training often captures more contextual information and leads to markedly 1, Oak Ridge National Laboratory, US. 2, National Institute of Advanced Industrial Science and Technology, Japan. 3, Agency for Science, Technology and Research, Singapore. Corresponding email: wangx2@ornl.gov higher prediction accuracy for many tasks with long-range dependencies, such as DNA sequence analysis (Zaheer et al., 2020), long document summary (Beltagy et al., 2020) and image segmentation (Strudel et al., 2021). Unfortunately, transformer's memory footprint increases quadratically and computations increase cubically with longer sequence lengths (Beltagy et al., 2020;Dao et al., 2022). Therefore, the transformer's sequence length is typically truncated to no more than a couple thousand tokens due to runtime and memory constraints, despite longer sequences leading to higher accuracy. \n\nTo address this issue, there are three research approaches: hierarchical training, attention approximation, and distributed sequence parallelism.",
            "score": 0.6594307980095785,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1051
                },
                {
                    "start": 1054,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2080
                },
                {
                    "start": 2083,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 133,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 229,
                    "end": 248,
                    "matchedPaperCorpusId": "174799399"
                },
                {
                    "start": 304,
                    "end": 323,
                    "matchedPaperCorpusId": "52287921"
                },
                {
                    "start": 342,
                    "end": 359,
                    "matchedPaperCorpusId": "159041705"
                },
                {
                    "start": 380,
                    "end": 405,
                    "matchedPaperCorpusId": "231986084"
                },
                {
                    "start": 405,
                    "end": 426,
                    "matchedPaperCorpusId": "234470051"
                },
                {
                    "start": 447,
                    "end": 467,
                    "matchedPaperCorpusId": "232404237"
                },
                {
                    "start": 1589,
                    "end": 1610,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1680,
                    "end": 1702,
                    "matchedPaperCorpusId": "234470051"
                },
                {
                    "start": 1861,
                    "end": 1878,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.349609375
        },
        {
            "corpus_id": "266110855",
            "title": "LongT5-Mulla: LongT5 With Multi-Level Local Attention for a Longer Sequence",
            "text": "As another category, transformers with hierarchical or recurrent architectures [11], [12], [13], [14], [15] are based on the concept of divide-and-conquer. They follow a typical procedure that involves splitting the input sequence into multiple parts, processing them with full attention one-byone, and then summarizing the results from each part or historical memory. These efforts have achieved competitive results compared to other efficient Transformers with both efficiency and accuracy. However, they significantly alter the architecture of Transformers and cannot be easily applied to existing pre-trained Transformers without retraining the entire model from scratch. This demonstrates their relatively low compatibility. \n\nSummarizing the previous works mentioned above, it is evident that current efficient Transformers face challenges in terms of sacrificing either efficiency, accuracy, or compatibility. These challenges hinder their application in longer sequences. Therefore, exploring how to extend the length limit of efficient Transformers while maintaining a well-balance among these three factors is still necessary. \n\nTo address this research gap, in this paper, we propose a new sparse attention mechanism called multi-level local attention (Mulla attention), which has linear or log-linear attention complexity and makes efficient Transformer models have better performance on long-text NLP tasks. To achieve this, Mulla attention fuses local attention and global attention into a unified but hierarchical local attention that acts on the input sequence and multiple pooling sequences of different granularity, without introducing a large amount of extra parameters except for positional embeddings weights with negligible parameters, and can be directly applied in existing pre-trained Transformer models. To construct an efficient Transformer model with Mulla attention, we start with pretrained LongT5 [10] sequence-to-sequence (seq2seq) model by replacing the original attention modules in the Encoders with Mulla attention thanks to the compatibility of sparse attention, and thus obtain our LongT5-Mulla model.",
            "score": 0.6584063265569512,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2101,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1136
                },
                {
                    "start": 1139,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 83,
                    "matchedPaperCorpusId": "219425367"
                },
                {
                    "start": 85,
                    "end": 89,
                    "matchedPaperCorpusId": "249354709"
                },
                {
                    "start": 91,
                    "end": 95,
                    "matchedPaperCorpusId": "235294151"
                },
                {
                    "start": 97,
                    "end": 101,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 103,
                    "end": 107,
                    "matchedPaperCorpusId": "207930593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.253662109375
        },
        {
            "corpus_id": "247058768",
            "title": "FastRPB: a Scalable Relative Positional Encoding for Long Sequence Tasks",
            "text": "Transformer architecture (Vaswani et al., 2017) originally proposed for machine translation tasks has shown impressive results in a wide range of domains, including natural language processing, image recognition, audio captioning, graph analysis, and bioinformatics (Lin et al., 2021). However, in applications that require processing long sequences, the benefits of transformers are often accompanied by high consumption of computational and memory resources. The main bottleneck is the transformer's core component, the self-attention mechanism. Self-attention computes similarity scores for all pairs of tokens in the input sequence, and therefore, it has a quadratic complexity O(N 2 ) in computations and memory relative to the length of the input sequence N 2 . \n\nRecently, several approaches have been introduced to reduce the computational complexity and memory footprint of self-attention. Some works utilize the sparsity of the attention map (Beltagy et al., 2020), others express self-attention as a linear dot-product of kernel feature maps \u03c6(\u2022) (Katharopoulos et al., 2020), or utilize random feature vectors (Choromanski et al., 2020). Pro- posed approaches reduce the computational complexity to O(N )3 . One of the promising variants of a transformer is the Linear Transformer (Katharopoulos et al., 2020) since, along with linear complexity, it requires constant O(1) memory in auto-regressive language modeling. Experiments with the long sequence benchmark Long Range Arena (LRA) (Tay et al., 2020) 4 have indeed shown that the Linear Transformer is 5x times faster than the vanilla Transformer in training speed. However, the drawback of this architecture is lower performance compared to the original Transformer. \n\nOne way to reduce the performance gap between the Linear Transformer and the original one is to select a more suitable kernel function \u03c6(\u2022) in linear attention (Choromanski et al., 2020;Schlag et al., 2021). The poor performance of efficient transformers on LRA can also be attributed to the model's ability to capture positional information.",
            "score": 0.6575925031572581,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 767
                },
                {
                    "start": 770,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1733
                },
                {
                    "start": 1736,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 47,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1058,
                    "end": 1086,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1293,
                    "end": 1321,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.149658203125
        },
        {
            "corpus_id": "263310483",
            "title": "Efficient Streaming Language Models with Attention Sinks",
            "text": "Sparse Transformers. The literature on efficient Transformer models primarily focuses on reducing the computational and memory complexity of the self-attention mechanism. A relevant line of work involves sparsifying the attention matrix by restricting the field of view to fixed, predefined patterns, such as local windows or block patterns with fixed strides (Tay et al., 2022). Sparse Transformer (Child et al., 2019)  Building on ETC, BigBird (Zaheer et al., 2020a) proposes another linear complexity attention alternative, utilizing global tokens, local sliding window attentions, and random attention. However, these methods have several limitations. First, Sparse Transformer and ETC require custom GPU kernels for a specific block-sparse variant of matrix-matrix multiplication. Second, LongFormer, ETC, and BigBird all rely on a global attention pattern, which is unsuitable for autoregressive language models. Third, these methods are incompatible with pre-trained models, necessitating retraining from scratch. In contrast, our method offers ease of implementation using standard GPU kernels and is compatible with pre-trained autoregressive language models using dense attention, which are prevalent in the NLP community. This compatibility provides a significant advantage, allowing for the leveraging of existing pre-trained models without any fine-tuning. \n\nConcurrent Works. Our research coincides with the work of Han et al., who conducted a theoretical study on the length generalization failure of language models, identifying three out-of-distribution factors. Their approach, inspired by this analysis, involves employing a \"\u039b\"-shaped attention pattern and reconfiguring position encoding distances to enhance length generalization in LLMs. This approach bears a resemblance to our methodology. However, our work uncovers the \"attention sink\" phenomenon, wherein Transformer models tend to assign high attention scores to initial tokens with small semantics. This phenomenon extends beyond the scope of length generalization failure, indicating a more pervasive issue in Transformer models. We observe this \"attention sink\" behavior not only in auto-regressive language models but also in encoder Transformers such as BERT (see Section H), and Vision Transformers (ViTs) (Darcet et al., 2023), suggesting its broader prevalence in Transformer architectures.",
            "score": 0.6563122405208683,
            "section_title": "B ADDITIONAL RELATED WORKS",
            "char_start_offset": 27608,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1369
                },
                {
                    "start": 1372,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2110
                },
                {
                    "start": 2111,
                    "end": 2377
                }
            ],
            "ref_mentions": [
                {
                    "start": 360,
                    "end": 378,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 446,
                    "end": 468,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1175537109375
        },
        {
            "corpus_id": "264451707",
            "title": "CLEX: Continuous Length Extrapolation for Large Language Models",
            "text": "Hierarchical Architecture / Sparse Attention. To overcome the quadratic complexity of attention, Dai et al. (2019) proposes the Transformer-XL that handles the long sequence at segment level by Transformer, with these segments interacting through a recurrence mechanism. The Recurrent Memory Transformer (Bulatov et al., 2022) refines this mechanism by incorporating special memory tokens into the recurrence, which is capable of scaling the context length to the millions (Bulatov et al., 2023). On the other hand, Child et al. (2019); Beltagy et al. (2020) proposed using the sparse attention to circumvent the full access to the long sequences, hence reducing the complexity. The sparse attention has been adopted by Ding et al. (2023a) to scale the context length of transformers into the billions. However, these methods sacrifice the utilisation of the entire sequence during attention, resulting in an inevitable loss of some contextual information. Additionally, modifications to the model architecture make these methods challenging to apply to existing pre-trained LLMs. Conversely, our CLEX serves as a drop-in component for LLMs, can efficiently extend the capacity of models to tack the entire long sequences without explicit drops of context information. \n\nLength Extrapolation. Building on the foundation laid by ALiBi (Press et al., 2022), a series of works (Sun et al., 2023;Chi et al., 2022;2023) seek to train the Transformer-based models on a short length, while directly testing on longer counterparts. These methods substitute the position embedding with bias introduced into attention scores, thereby incorporating positional information. Notably, the bias typically gives higher profits to closer tokens. This mechanism intuitively amplifies the local context for each token at the expense of distant information. Consequently, these length-extrapolation methods encounter challenges in effectively handling long contexts in practical applications (Pal et al., 2023). However, our CLEX demonstrates remarkable effectiveness in practical tasks such as summarization, indicating the de facto extrapolation ability for applications. \n\nPosition Embedding (PE) Scaling. Recent research has sought to extend the context length of Transformers through the scaling of the extensively employed RoPE.",
            "score": 0.6549182851616722,
            "section_title": "RELATED WORK",
            "char_start_offset": 21877,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2153
                },
                {
                    "start": 2156,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2314
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 114,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 304,
                    "end": 326,
                    "matchedPaperCorpusId": "250526424"
                },
                {
                    "start": 1334,
                    "end": 1354,
                    "matchedPaperCorpusId": "237347130"
                },
                {
                    "start": 1374,
                    "end": 1392,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 1392,
                    "end": 1409,
                    "matchedPaperCorpusId": "248965309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26708984375
        },
        {
            "corpus_id": "248070506",
            "title": "Hardware-Software Co-Design of an In-Memory Transformer Network Accelerator",
            "text": "Transformer networks have high computational and space complexity because of the employed attention mechanism. Transformer network GPU implementations are bounded by the memory, particularly with longer sequences, because of the O (dn + dn 2 ) spatial complexity, where d represents the feature embedding dimension, and n is the sequence length. A transformer can also be computation-limited because of the O (dn 2 ) serialized time complexity of the MHA. The O (dn 2 ) complexity comes from each time step (sequence element) attending to every other time step (sequence element). However, an O (1) time complexity can be achieved with adequate parallelism. The following sections review four types of algorithm-based acceleration: quantization (Section 2.2.1), attention caching (Section 2.2.2), model parallelism (Section 2.2.3) and sparse attention (Section 2.2.4).",
            "score": 0.6525077377112722,
            "section_title": "Algorithm-Based Transformer Network Acceleration",
            "char_start_offset": 16931,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 868
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29296875
        },
        {
            "corpus_id": "268248144",
            "title": "TaylorShift: Shifting the Complexity of Self-Attention from Squared to Linear (and Back) using Taylor-Softmax",
            "text": "Ever since their introduction by Vaswani et al. [28], Transformers have revolutionized numerous domains of deep learning, from Natural Language Processing to Computer Vision, while also underpinning the emergence of novel applications such as Large Language Models. This success stems largely from their ability to capture intricate dependencies and token-to-token interactions. \n\nTo extend the utility of Transformers to more complex tasks, they need to be able to process long sequences. However, the computational complexity of the attention mechanism scales quadratically in the length of the input sequence O(N 2 ). Therefore, computing twice as many sequence elements requires four times the number of computations, which hinders scaling to very long context windows. This makes some practitioners turn to approaches like compressing portions of the input into single states [3,5], which reduces the amount of information available at each step. Despite this progress, exploiting long context windows to significantly improve performance and incorporate new information without retraining remains challenging. Current Transformers encounter limitations when processing long documents, high-resolution images, or a combination of data from multiple domains and modalities. Especially, considering the limited resources of smaller enterprises or individual consumers. \n\nWhile linearly scaling Transformers have been proposed, these often experience compromised accuracy [20], specialize in a particular domain, like language [34] or images [15], or only convey averaged global information across tokens, neglecting individual token-to-token interactions [1,9]. These models end up being ill-suited for handling longer sequences, leaving the standard Transformer as the preferred choice due to its large capacity and established performance [14]. \n\nIn this work, we approach this bottleneck of the Transformer by reformulating the softmax function in the attention mechanism after introducing the Taylor approximation of the exponential. While some methods alter the softmax, their goal is to split interactions of queries and keys, computing global average interactions only [1,4]. In contrast, our proposed approach, TaylorShift, preserves individual token-to-token interactions. Combining a tensor-product-based operator with the Taylor approximation of the exponential function allows us to compute full token-to-token interactions in linear time. Moreover, this approach has the added benefit of adhering to concrete error bounds when viewed as an approximation of vanilla attention [12].",
            "score": 0.6510571929143405,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1849
                },
                {
                    "start": 1852,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2185
                },
                {
                    "start": 2186,
                    "end": 2284
                },
                {
                    "start": 2285,
                    "end": 2454
                },
                {
                    "start": 2455,
                    "end": 2596
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23876953125
        },
        {
            "corpus_id": "278033756",
            "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
            "text": "Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on our experiments, we report a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) We introduce and validate novel scaling laws specifically tailored for sparse attention, providing evidence that our findings are likely to hold true beyond our range of experiments. Through these insights, we demonstrate that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications.",
            "score": 0.6507386483153771,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.291748046875
        },
        {
            "corpus_id": "259187506",
            "title": "Block-State Transformers",
            "text": "We have introduced a model that combines the attention mechanism of Transformers with the long-range memory mechanism, and parallelism afforded by State Space Models. We explored several memory state variants that make different trade-offs between redundancy and retrievability. \n\nExperiments show that our model can minimize perplexity on par with and often improves upon recent competing baselines, while achieving up to more than 10\u00d7 speedups at the layer level, provided there is hardware support to fully take advantage of parallelism. This is an appealing property for scaling up BST which makes the addition of SSMs into Transformers computationally appealing. We show that integrating SSM states into the Transformer attention provides larger benefits than simply interleaving SSM and attention layers. Finally, we show that the model generalizes to longer sequences than it was trained.",
            "score": 0.6505348338980258,
            "section_title": "Conclusion",
            "char_start_offset": 31594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 278
                },
                {
                    "start": 281,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 895
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37939453125
        },
        {
            "corpus_id": "267027729",
            "title": "InternEvo: Efficient Long-sequence Large Language Model Training via Hybrid Parallelism and Redundant Sharding",
            "text": "Processing long sequences is crucial for supporting important applications such as dealing with longer histories in chat applications. To this end, sequence parallelism (SP) has emerged as a technique aimed at alleviating activation memory footprints during the training of Transformers. In SP, the input tensor of each Transformer layer is divided along the sequence dimension, allowing for parallel computation across multiple GPUs. This segmentation, in conjunction with activation recomputation, results in a substantial reduction in activation memory requirements by a factor of s sp . In this paper, we classify existing SP approaches into 3 stages, which correspond to slicing the sequence into Norm and Dropout modules, Linear modules, and MHA module. When enabled cumulatively: \n\nSP-1: Norm and Dropout modules. As shown in Figure 2 (b), Megatron-LM capitalizes on TP to parallelize the linear layers and MHA, which are the most time-consuming components during training. Simultaneously, it employs SP on Norm and Dropout modules, effectively reducing the activation memory of these layers by a factor of s sp , where s sp = s t p . To maintain consistency in computational results, it integrates necessary communications, including all-gather and reduce-scatter to transfer the activation in forward and backward passes. When the activation size increases with the sequence length, this way of communicating the activation will incur a high overhead. \n\nSP-2: Add Linear modules. DeepSpeed Ulysses utilizes sequence parallelism on Linear, Norm, and Dropout layers, as shown in Figure 2 (c). An all-to-all communi- cation is applied to the output of Linear qkv with a shape of B \u00d7 3 \u00d7 S/s sp \u00d7 H. This enables each GPU to receive the complete sequence of Q, K, and V (each with a shape of B \u00d7 S \u00d7 H/s sp ) for a subset of attention heads. Subsequently, FlashAttention is employed for highly efficient MHA computation with a constraint that the number of attention heads D should not exceed s sp . Another all-to-all communication is then used to gather the MHA layer results and the results are re-partitioned along the sequence dimension.",
            "score": 0.6490428437307956,
            "section_title": "Long-sequence Training",
            "char_start_offset": 9473,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2147
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68310546875
        },
        {
            "corpus_id": "220831004",
            "title": "Big Bird: Transformers for Longer Sequences",
            "text": "Models based on Transformers [92], such as BERT [23,64], are wildly successful for a wide variety of Natural Language Processing (NLP) tasks and consequently are mainstay of modern NLP research. Their versatility and robustness are the primary drivers behind the wide-scale adoption of Transformers. The model is easily adapted for a diverse range of sequence based tasks -as a seq2seq model for translation [92], summarization [67], generation [16], etc. or as a standalone encoders for sentiment analysis [84], POS tagging [66], machine reading comprehension [94], etc. -and it is known to vastly outperform previous sequence models like LSTM [38]. The key innovation in Transformers is the introduction of a self-attention mechanism, which can be evaluated in parallel for each token of the input sequence, eliminating the sequential dependency in recurrent neural networks, like LSTM. This parallelism enables Transformers to leverage the full power of modern SIMD hardware accelerators like GPUs/TPUs, thereby facilitating training of NLP models on datasets of unprecedented size. This ability to train on large scale data has led to surfacing of models like BERT [23] and T5 [76], which pretrain transformers on large general purpose corpora and transfer the knowledge to down-stream task. The pretraining has led to significant improvement in low data downstream tasks [52] as well as tasks with sufficient data [102] and thus have been a major force behind the ubiquity of transformers in contemporary NLP.\n\nThe self-attention mechanism overcomes constraints of RNNs (namely the sequential nature of RNN) by allowing each token in the input sequence to attend independently to every other token in the sequence. This design choice has several interesting repercussions. In particular, the full self-attention have computational and memory requirement that is quadratic in the sequence length. Using commonly available current hardware and model sizes, this requirement translates to roughly being able to handle input sequences of length 512 tokens. This reduces its direct applicability to tasks that require larger context, like question answering [61], document summarization [21], etc.\n\nHowever, while we know that self-attention and Transformers are useful,",
            "score": 0.6489803227888971,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 645,
                    "end": 649,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1419,
                    "end": 1424,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.423095703125
        },
        {
            "corpus_id": "236924765",
            "title": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention",
            "text": "The self-attention mechanism is used to learn long-range dependencies while enabling parallel processing of the input sequence. For a given input sequence X := [x 1 , x 2 , \u2022 \u2022 \u2022 , x N ] \u2208 R N \u00d7Dx of N feature vectors that have been encoded in a D x -dimensional vector space, self-attention transforms X into an output sequence V in the following two steps: \n\nStep 1. Project the input sequence X into three matrices via the following linear transformations \n\nwhere W Q , W K \u2208 R D\u00d7Dx , and W V \u2208 R Dv\u00d7Dx are the weight matrices. We denote \n\nwhere the vectors q i , k i , v i for i = 1, \u2022 \u2022 \u2022 , N are the query, key, and value vectors, respectively. \n\nStep 2. For each query vector q i for i = 1, \u2022 \u2022 \u2022 , N , we compute the output vector vi as follows \n\nwhere the softmax function is applied to each row of the matrix (QK )/ \u221a D. \n\nFor long sequences, the computational time and memory footprint of transformers are dominated by (1). It is evident that the memory cost is O(N 2 ) to store the attention matrix A. Also, the computational complexities of computing the matrix-matrix products QK and AV are both O(N 2 ). These limitations impede the application of transformers to many important settings that involve very long sequences [38,28,46]. When applying self-attention for long sequence modeling, we have to limit the context window to a reasonable size to make it computationally feasible, limiting the effectiveness of learning long-term dependencies. Efficient transformer models have been proposed, including leveraging sparse and low-rank attention. Many of the existing efficient transformers gain computational and memory efficiency at the cost of significant accuracy degradation.",
            "score": 0.6488448296425824,
            "section_title": "Self-attention",
            "char_start_offset": 824,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 930,
                    "end": 933,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 1236,
                    "end": 1240,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 1240,
                    "end": 1243,
                    "matchedPaperCorpusId": "54477714"
                },
                {
                    "start": 1243,
                    "end": 1246,
                    "matchedPaperCorpusId": "3353110"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1524658203125
        },
        {
            "corpus_id": "248780136",
            "title": "Long-range Sequence Modeling with Predictable Sparse Attention",
            "text": "Self-attention mechanism has been shown to be an effective approach for capturing global context dependencies in sequence modeling, but it suffers from quadratic complexity in time and memory usage. Due to the sparsity of the attention matrix, much computation is redundant. Therefore, in this paper, we design an efficient Transformer architecture, named Fourier Sparse Attention for Transformer (FSAT), for fast long-range sequence modeling. We provide a brand-new perspective for constructing sparse attention matrix, i.e. making the sparse attention matrix predictable. Two core sub-modules are: (1) A fast Fourier transform based hidden state cross module, which captures and pools L^2 semantic combinations in \\mathcal{O}(L\\log L) time complexity. (2) A sparse attention matrix estimation module, which predicts dominant elements of an attention matrix based on the output of the previous hidden state cross module. By reparameterization and gradient truncation, FSAT successfully learned the index of dominant elements. The overall complexity about the sequence length is reduced from \\mathcal{O}(L^2) to \\mathcal{O}(L\\log L). Extensive experiments (natural language, vision, and math) show that FSAT remarkably outperforms the standard multi-head attention and its variants in various long-sequence tasks with low computational costs, and achieves new state-of-the-art results on the Long Range Arena benchmark.",
            "score": 0.6473216284746952,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.173583984375
        },
        {
            "corpus_id": "276885221",
            "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
            "text": "Hybrid linear sequence modeling models, which combine linear transformer layers (leveraging LSM methods for token mixing) with standard transformer layers (utilizing conventional selfattention for token mixing), have demonstrated substantial improvements in handling long-context tasks (Lieber et al., 2024;Ren et al., 2024;Waleffe et al., 2024). This hybrid model is particularly beneficial for tasks with high recall requirements, including five-shot MMLU (Hendrycks et al., 2020), Phone-book lookup (Jelassi et al., 2024), and Needle In A Haystack (Briakou et al., 2023), etc.. Our proposed hybrid Linear-MoE models also aim to enhance performance in areas where pure Linear-MoE models have shown limitations, specifically on tasks where recall precision is critical. \n\nApplying SP on pure Linear-MoE models is straightforward, as this form of SP operates exclusively on the LSM modules, leaving the MoE layers unaffected. In hybrid Linear-MoE models, however, implementing SP becomes more complex due to the interleaving of distinct sequence modeling layers. To effectively optimize SP for these hybrid models, we introduce an integrated approach that incorporates SP across both the linear-MoE and standard transformer layers, thus enhancing overall efficiency. We illustrate the approach in Fig. 2, and explain it as below: \n\nOn LSM Module. The SP for LSM modules is implemented via a single collective communication operation on the memory state M s \u2208 R d\u00d7d . This approach ensures that the communication complexity does not depend on either the sequence or sub-sequence length; rather, it scales only linearly with the SP size T , thereby maintaining efficiency in distributed setups. \n\nOn Standard Attention Module. Context parallelism (CP) is a SP technique used in Megatron-LM that divides input data and activations along the sequence dimension, specifically designed for standard softmax attention. Traditional CP implementations in Megatron-LM rely on a ring-like communication-computation overlap (Liu et al., 2023). In contrast, our approach for standard attention modules adopts the all-gather-based strategy used in the pretraining of Llama3 (Dubey et al., 2024).",
            "score": 0.646128740262937,
            "section_title": "Hybrid Model Sequence Parallelism",
            "char_start_offset": 12599,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1329
                },
                {
                    "start": 1332,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1692
                },
                {
                    "start": 1695,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2181
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.235595703125
        },
        {
            "corpus_id": "258832477",
            "title": "FIT: Far-reaching Interleaved Transformers",
            "text": "The primary sources of computational complexity in transformers arise from self-attention, which has a complexity of O(L 2 d), and the feed-forward network (FFN) with a complexity of O(Ld 2 ). It is important to be aware of the typical scale of L (sequence length) and d (embedding dimension) that we may encounter. With the recent surge of large models, d can vary from 1024 to 18432 [3,13]. Longer contexts are often desired, leading to L ranging from a few hundred to millions or even higher. In cases where d and L are of comparable magnitudes, such as d = 4096 and L = 2048, approximations applied to the original quadratic attention may yield negligible gains as they usually shift the computational burden from O(L 2 d) to O(Ld 2 ). Furthermore, optimized implementations of self-attention, such as FlashAttention [16], can improve efficiency and eliminate the need for O(L 2 ) memory. Therefore, the full quadratic attention remains the most effective and efficient operation when dealing with relatively short sequences (e.g., in the order of hundreds, or thousands). \n\nConsidering the efficacy of full attention for shorter sequences and our objective of developing a single architecture capable of handling sequences of varying lengths, we will focus on reviewing techniques that integrate global attention into architectures with local quadratic attentions (such as window attention). One simple approach is to leverage global attention for only a few transformer layers. Although this primarily reduces the constant factor in terms of computational complexity, it has demonstrated practical effectiveness in certain applications [3,34]. Another approach is to employ techniques such as shifted window [35] or convolution [51] to propagate information across groups, with a disadvantage that it only affects adjacent groups at a time. A more general approach involves the use of sparse/axial attention [11,24], mixed or learned attention patterns [50,52,32,41,40]. These methods allow for more flexible attention patterns, but may involve sparse operations that are not always accelerator-friendly. Another family of methods [14,39,4,27] incorporates recurrent mechanisms to connect transformers on local windows.",
            "score": 0.6458331365199026,
            "section_title": "Background and related work",
            "char_start_offset": 2370,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2225
                }
            ],
            "ref_mentions": [
                {
                    "start": 385,
                    "end": 388,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1642,
                    "end": 1645,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1645,
                    "end": 1648,
                    "matchedPaperCorpusId": "247793203"
                },
                {
                    "start": 1714,
                    "end": 1718,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 1734,
                    "end": 1738,
                    "matchedPaperCorpusId": "245353951"
                },
                {
                    "start": 1959,
                    "end": 1963,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1963,
                    "end": 1966,
                    "matchedPaperCorpusId": "235422255"
                },
                {
                    "start": 1969,
                    "end": 1972,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1972,
                    "end": 1975,
                    "matchedPaperCorpusId": "235829099"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.080810546875
        },
        {
            "corpus_id": "239049478",
            "title": "Transformer Acceleration with Dynamic Sparse Attention",
            "text": "Transformers with the use of self-attention mechanism are difficult to scale with sequence length because of the quadratic time and memory complexity. Our paper focuses on the exploration of sparse attention patterns in Transformers. Other orthogonal approaches such as parameters sharing (Gong et al., 2019) can mitigate the issue. We refer readers to a survey paper for a more comprehensive view of efficient Transformers (Tay et al., 2020c). \n\nStatic Sparse Patterns. A straightforward way to exploit attention sparsity is to set static or fixed sparse patterns, such as local windows, block-wise, dilated patterns, or a combination of static patterns (Zaheer et al., 2020;Child et al., 2019;Qiu et al., 2020). However, as the sparse attention patterns are inherently dynamic depending on input sequences, those work lack the capability of capturing dynamic sparse patterns. As shown in our evaluation, the sparsity-saving trade-offs of representative methods using static sparse patterns are worse than our dynamic sparse attention approach. \n\nClustering-based methods. Building upon static blocksparse patterns, another line of research is to group similar tokens into chunks and perform local attention within chunks (Kitaev et al., 2020;Roy et al., 2021;Tay et al., 2020a). The similarity function used to group tokens can be hashing, clustering, or learned sorting. However, those methods are designed for training memory reduction and impractical at inference time when operating on each sequence. The quality of grouping, e.g., convergence of clustering, is not guaranteed at long sequences, and the overhead of onthe-fly clustering is not acceptable. \n\nApproximation methods. Recent work proposes to replace standard attention with forms of approximation of the at-tention weights (Wang et al., 2020;Katharopoulos et al., 2020;Choromanski et al., 2021;Peng et al., 2021). While we provide a comparison in our evaluation, we regard those work out the scope of our discussion for exploring sparsity in (standard) attention. Whether using a form of approximation to replace standard attention or as we suggest to predict sparse patterns explicitly is a design choice leaving up to practitioners.",
            "score": 0.6452915640593143,
            "section_title": "RELATED WORK",
            "char_start_offset": 37247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1661
                },
                {
                    "start": 1664,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2203
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 308,
                    "matchedPaperCorpusId": "174799716"
                },
                {
                    "start": 1223,
                    "end": 1244,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1244,
                    "end": 1261,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1261,
                    "end": 1279,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 1811,
                    "end": 1838,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1863,
                    "end": 1881,
                    "matchedPaperCorpusId": "232105052"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07598876953125
        },
        {
            "corpus_id": "258987968",
            "title": "Blockwise Parallel Transformer for Large Context Models",
            "text": "Section 3.1 provides a memory cost breakdown. \n\nDespite the resulting reduced memory requirements of the self-attention block in Transformer models, a significant challenge still arises from the feedforward network. This network contains a large number of parameters and produces high-dimensional intermediate vectors, resulting in substantial memory requirements. This issue is becomes the key memory challenge once employing memoryefficient attention mechanisms. Consequently, training Transformers on longer context lengths and scaling up Transformer models become significantly hindered due to the overwhelming memory demands imposed by the feedforward network. \n\nTo address this challenge, we make an important observation: when self-attention is computed in a blockwise manner to reduce memory requirements, it becomes feasible to merge the computation of the feedforward network. This eliminates the need to wait for the self-attention computation to finish before performing the feedforward step on the entire sequence. By computing the feedforward network on a block-by-block basis, we effectively reduce the memory cost associated with the feedforward network. This process involves the utilization of two nested loops over the input sequence blocks. In the outer loop, we iterate over each block and compute the query. In the inner loop, we iterate over each block to calculate the key and value. These key-value pairs, along with the query, are then used to compute the blockwise attention specific to the corresponding input block. This blockwise attention is subsequently used to calculate the output of the feedforward network, followed by a residual connection. This approach enables us to process longer input sequences while maintaining lower memory budget. Since our approach performs blockwise parallel computation and fuses the feedforward and self-attention computations, we name our method the Blockwise Parallel Transformer (BPT). \n\nWe evaluate the effectiveness of our approach on several benchmarks, including language modeling and reinforcement learning. Our experiments show that BPT can reduce the memory requirements of Transformers, enabling us to train 32 times longer sequence than vanilla attention [52] based GPT models and up to 4 times longer sequence than prior state-of-the-arts FlashAttention [14] and Memory Efficient Attention [42]. Furthermore, we demonstrate the application of BPT on the task of traning Transformer based RL agent.",
            "score": 0.6446266327132574,
            "section_title": "Introduction",
            "char_start_offset": 2176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 48,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1954
                },
                {
                    "start": 1957,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2374
                },
                {
                    "start": 2375,
                    "end": 2476
                }
            ],
            "ref_mentions": [
                {
                    "start": 2333,
                    "end": 2337,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494384765625
        },
        {
            "corpus_id": "276106883",
            "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
            "text": "As the key objective of this work is to increase context length by exploiting sparsity, we discuss relevant works that focus on either achieving efficient sequence parallelism or on achieving efficient implementation of sparse masks. Sequence Parallelism: DeepSpeed Ulysses [16] achieves sequence parallelism for the original L 2 complexity attention by distributing the partitions of the K, Q, V matrices along the sequence dimensions across computing nodes. Megatron [17] performs sequence parallelism that is tightly coupled with their tensor parallelism [16]. Ring attention achieves sequence parallelism for block sparse attention masks [14]. LongNet achieves sequence parallelism using a dilated attention mask and requires all-gather of K, Q matrices [7]. On a single node, these techniques still rely on dense matrix operations for attention calculations. Our technique, that exploits sparsity, is orthogonal to these methods and has the potential to significantly scale the sequence length that can be achieved using these techniques for a fixed number of nodes. Efficient Sparse Attention Implementations: While works such as BigBird [6], Longformer [8], and Reformer [11] proposed sparse attention masks, their implementations in libraries such as PyTorch [9] and xFormers [10] still rely on dense operations. Specifically, they employ a variant of the Scaled Dot Product (SDP) attention [5] where they first perform a dense matrix multiplication of Q and K that can incorporate block sparsity for low-resolution masking, set the excess terms corresponding to the zero entries in the attention mask to \u2212\u221e, perform a row-wise softmax (which results in \u2212\u221e getting converted to 0) and finally a sparse-dense matrix multiplication is performed between the resultant matrix (sparse) and the V matrix (dense). \n\nAs a result, FlashAttention -a highly optimized version of SDP -has remained the most efficient attention implementation despite the fact that it performs the full L 2 computations [18]- [20]. Recent works [21], [22] have enabled support for sparsity by partitioning the attention mask and the K, Q, V matrices into blocks and only computing the blocks that have at least one non-zero element using the FlashAttention algorithm.",
            "score": 0.6412553458871415,
            "section_title": "III. RELATED WORKS",
            "char_start_offset": 10382,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 1144,
                    "end": 1147,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1399,
                    "end": 1402,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81494140625
        },
        {
            "corpus_id": "277150882",
            "title": "ATTENTION2D: Communication Efficient Distributed Self-Attention Mechanism",
            "text": "Memory-efficient attention. Self-attention is a fundamental component of Transformer models, but it tends to be slow and memory-intensive, particularly for long sequences, due to its quadratic memory cost relative to sequence length. This challenge has spurred the creation of memory-efficient attention mechanisms [22,28,9,8] that achieve linear memory costs. These methods primarily address memory cost improvements on a single GPU. Our proposed solution leverages these techniques to reduce memory costs on a single GPU while introducing an efficient parallelization strategy to scale self-attention computations across multiple GPUs. \n\nParallelism methods. Various self-attention parallelism methods [32,17,19,20,18,13,12] have been proposed in the past. However, these methods mainly focus on reducing memory costs, often neglecting communication costs. As shown in Table 1, all these techniques incur a communication cost that increases linearly with the sequence length, regardless of the number of devices used. In contrast, our proposed parallelism method achieves a sub-linear communication cost that decreases as the number of devices increases. Parallelization strategies like data parallelism [11], pipeline parallelism [23], and ZeRO [29,3] focus on parallelizing the entire LLM. Many of these methods complement our approach. Therefore, similar to the strategy employed by USP [12], a hybrid approach that combines these techniques with ours can be utilized to achieve further improvements. Approximation of attention. To reduce the computational cost of attention on long contexts, numerous methods have explored approximations of attention mechanisms [4,7,16,35,15,37]. Although these methods have been used in some cases, standard attention is still often preferred because these approximations can yield sub-optimal results. In contrast, our method does not rely on any approximations. Instead, we propose improvements to the standard self-attention operation for parallel settings. Grouped Query Attention (GQA) [1] and Multi-Query Attention (MQA) [31] are variants of standard self-attention where multiple query heads attend to the same key and value head, reducing the size of the KV cache during inference. Since our parallelism method operates at the level of single attention heads, it can be directly applied to these variants. \n\nLong context inference.",
            "score": 0.6400189572087915,
            "section_title": "Discussion and Related Work",
            "char_start_offset": 33159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2230
                },
                {
                    "start": 2231,
                    "end": 2354
                },
                {
                    "start": 2357,
                    "end": 2380
                }
            ],
            "ref_mentions": [
                {
                    "start": 322,
                    "end": 324,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 324,
                    "end": 326,
                    "matchedPaperCorpusId": "259936734"
                },
                {
                    "start": 711,
                    "end": 714,
                    "matchedPaperCorpusId": "246017095"
                },
                {
                    "start": 717,
                    "end": 720,
                    "matchedPaperCorpusId": "270973564"
                },
                {
                    "start": 1206,
                    "end": 1210,
                    "matchedPaperCorpusId": "372467"
                },
                {
                    "start": 1248,
                    "end": 1252,
                    "matchedPaperCorpusId": "269617042"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36279296875
        },
        {
            "corpus_id": "265351802",
            "title": "Linear Log-Normal Attention with Unbiased Concentration",
            "text": "Transformer models, proposed by (Vaswani et al., 2017), have become widely used deep learning architectures that have achieved state-of-the-art performance in various fields, including Natural Language Processing (NLP) (Brown et al., 2020;Devlin et al., 2018), Computer Vision (CV) (Dosovitskiy et al., 2020), Neural Machine Translation (NMT) (Chen et al., 2018), Document Summarization (Zhang et al., 2019;Pilault et al., 2020), and Protein Structure Prediction (Bahdanau et al., 2015). The main component of the Transformer model is an attention mechanism that identifies complex dependencies between tokens and efficiently captures tokens' correlation. However, standard self-attention suffers from quadratic memory and computation complexity, which arises from the N \u00d7 N attention matrix, where N is the sequence length. This problem is particularly significant during training, as it requires storing the attention matrix for gradient computation. Consequently, this significantly hinders the training of Transformer models with long sequences. \n\nRecently, we have observed an increasing interest in training Transformer models with long sequences, especially when considering large language models (Scao et al., 2022;Zhang et al., 2022;Chowdhery et al., 2022). Various approaches address the quadratic memory issue in self-attention. One class of the methods is sparse-attention, which aims to perform only a subset of the attention computations while preserving the softmax function (Child et al., 2019;Zaheer et al., 2020). Another approach is Linearized Attention (LA), which replaces the softmax with a product of two functions (Choromanski et al., 2020;Katharopoulos et al., 2020). These methods reduce the computational and memory complexity of the attention mechanism while striving to maintain performance. One of LA's benefits is that it performs dense operations and does not require special HW or low-level implementation. However, despite their efficiency, LA methods often underperform compared to standard self-attention. Thus, understanding the reasons behind the superior performance of self-attention is crucial for designing an effective LA method.",
            "score": 0.6392007663455017,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1049
                },
                {
                    "start": 1052,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 239,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 463,
                    "end": 486,
                    "matchedPaperCorpusId": "11212020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.136474609375
        },
        {
            "corpus_id": "250113485",
            "title": "SALO: an efficient spatial accelerator enabling hybrid sparse attention mechanisms for long sequences",
            "text": "The attention mechanisms of transformers effectively extract pertinent information from the input sequence. However, the quadratic complexity of self-attention w.r.t the sequence length incurs heavy computational and memory burdens, especially for tasks with long sequences. Existing accelerators face performance degradation in these tasks. To this end, we propose SALO to enable hybrid sparse attention mechanisms for long sequences. SALO contains a data scheduler to map hybrid sparse attention patterns onto hardware and a spatial accelerator to perform the efficient attention computation. We show that SALO achieves 17.66x and 89.33x speedup on average compared to GPU and CPU implementations, respectively, on typical workloads, i.e., Longformer and ViL.",
            "score": 0.6385798739180781,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1605224609375
        },
        {
            "corpus_id": "268875936",
            "title": "Linear Attention Sequence Parallelism",
            "text": "Memory-Efficient Attention. Rabe & Staats (2021) first employs the online Softmax technique to efficiently compute numerically stable attention scores sequentially, resulting in a linear memory for attention, yet still needs quadratic time complexity. While FlashAttention (Dao et al., 2022;Dao, 2023) employs tiling to minimize the number of memory reads/writes between GPU's high bandwidth memory (HBM) and on-chip SRAM to reduce time and memory in the training process, PagedAttention (Kwon et al., 2023) optimizes the utilization of the KV cache memory by reducing waste and enabling adaptable sharing among batched requests during inference. Ring Attention (Liu et al., 2023) reduces memory requirements for Transformer models when handling long sequences by distributing sequences across multiple devices and overlapping the communication of key-value blocks with blockwise attention computation. \n\nSequence Parallelism. SP as a widely used method to train long sequences has been integrated into many large model training frameworks, including Megatron-LM, DeepSpeed, and Colossal-AI. Megatron-LM (Shoeybi et al., 2019) implements SP along with model (tensor) parallelism (MP) to perform large matrix multiplications on GPUs. However, MP partitions the attention heads, which limits the maximum parallelism degree to be less than the number of attention heads. DeepSpeed-Ulysses (Jacobs et al., 2023) uses an all-to-all communication primitive to reduce communication volume, but also partitions attention heads and faces similar issues as Megatron-LM.",
            "score": 0.6385612949265947,
            "section_title": "Related Work",
            "char_start_offset": 6137,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1559
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69189453125
        },
        {
            "corpus_id": "269757781",
            "title": "USP: A Unified Sequence Parallelism Approach for Long Context Generative AI",
            "text": "The field of artificial intelligence is witnessing a trend as the context length in generative AI models grows ever longer.Claude has pioneered this trend in large language models (LLMs) by extending the sequence length to 100K tokens.Following closely in its footsteps, OpenAI's GPT-4 has expanded the context length to 128K tokens.The advent of multi-modality models is propelling this trend forward, with Gemini 1.5 Pro boasting a context length of a staggering 10 million tokens, and OpenAI's Sora, a Diffusion Model, accommodating at least 1 million visual tokens.These breakthroughs underscore the imperative for generative AI techniques to adeptly handle a larger context length.\n\nSequence Parallelism (SP), a technique that partitions input sequences, has emerged as a promising approach for the training or inference of longer sequences.Following an initial exploration period of two years, by the latter of 2023, two landmark works, DeepSpeed-Ulysses [1] and Ring-Attention [2], marked the maturation of the SP technique.DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally, while Ring-Attention hides P2P communication costs introduced by SP through overlapping computation and communication.However, challenges remain, such as the SP parallel degree of DeepSpeed-Ulysses is limited to less than the number of attention heads, and the computational efficiency of Ring-Attention degrading due to the subdivision of matrix multiplications.These limitations currently hinder the broader adoption of Sequence Parallelism in distributed Transformer computation.\n\nIn this paper, we delve deeper into the realm of SP.We begin by highlighting that Ulysses and Ring are not mutually exclusive approaches; they can be combined through a hybrid parallel strategy to mitigate their drawbacks.Then, we discussed the relationship between SP and data/tensor/zero/expert/pipeline parallelism.The most complex among these is the relationship between SP and tensor parallelism.Since tensor parallelism also has its specific sequence parallel optimizations to reduce activation memory cost [3].For each parallelism approach, whether SP should replace it or is there some issue to using SP with it together, remains an open question.",
            "score": 0.637862553788059,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 123,
                    "end": 235
                },
                {
                    "start": 235,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 569
                },
                {
                    "start": 569,
                    "end": 686
                },
                {
                    "start": 688,
                    "end": 846
                },
                {
                    "start": 846,
                    "end": 1031
                },
                {
                    "start": 1031,
                    "end": 1278
                },
                {
                    "start": 1278,
                    "end": 1523
                },
                {
                    "start": 1523,
                    "end": 1642
                },
                {
                    "start": 1644,
                    "end": 1696
                },
                {
                    "start": 1696,
                    "end": 1866
                },
                {
                    "start": 1866,
                    "end": 1962
                },
                {
                    "start": 1962,
                    "end": 2045
                },
                {
                    "start": 2045,
                    "end": 2161
                },
                {
                    "start": 2161,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 2157,
                    "end": 2160,
                    "matchedPaperCorpusId": "248693351"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "269922123",
            "title": "The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving",
            "text": "We discuss works that explore the Sequence Parallelism (SP) dimension in a distributed fashion.\n\nHere, a long-context inference request is segmented into sub-sequences and distributed across nodes for parallel processing.While traditional distributed strategies like tensor parallelism (TP) or pipeline parallelism (PP) can also enhance inference performance, we omit them in this survey because, they are not specifically designed for long-context handling and generally serve as orthogonal or complementary to SP optimizations.\n\nOur analysis unfolds in two steps.First, we investigate methods to accelerate a single long-context request using SP.Second, we investigate methods to accelerate a cluster serving long-context requests.\n\nAccelerate a Single Request.\n\n\u2022 Figure 3 shows the relation among this line of research work.This line of research can be traced back to the online normalizer work [88], a mathematically equivalent method for block-wise softmax calculation that avoids materializing the full attention matrix\n\nOnline Normalizer [88] Memory Efficient Attention [89] Flash Attention [48] Blockwise Parallel Transformer [90] Ring Attention [91] Burst Attention [92] Striped Attention [93] Dist Attention [94] Figure 3: Works using sequence parallelism.Gray boxes are not tailored for long-context serving.\n\nsoftmax(QK T ).This method is a foundation for memory-efficient attention [89] and their CUDA implementations [48,96].\n\n\u2022 SP was first introduced by Li et al. [97] and has been widely used in distributed LLM training frameworks such as Megatron [98] and Deepspeed [99].In the context of LLM serving systems, new challenges emerge: (1) LLM serving is usually latency-sensitive and thus requires much smaller batch sizes than LLM training; (2) LLM serving has an autoregressive decode phase, where the sequence length is only one, but it requires large memory for KV cache storage; (3) LLM serving usually relies on large fused kernels for improving performance.While the feed-forward network (FFN) computations for each token in a sequence are linearly independent, the computations for attention are not.Consequently, substantial data exchange is involved when computing distributed attention using SP, thereby opening significant space for performance optimization.",
            "score": 0.6365913015733615,
            "section_title": "Distributed Acceleration",
            "char_start_offset": 30981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 97,
                    "end": 221
                },
                {
                    "start": 221,
                    "end": 529
                },
                {
                    "start": 531,
                    "end": 565
                },
                {
                    "start": 565,
                    "end": 648
                },
                {
                    "start": 648,
                    "end": 733
                },
                {
                    "start": 735,
                    "end": 763
                },
                {
                    "start": 765,
                    "end": 828
                },
                {
                    "start": 828,
                    "end": 1026
                },
                {
                    "start": 1028,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1320
                },
                {
                    "start": 1322,
                    "end": 1337
                },
                {
                    "start": 1337,
                    "end": 1440
                },
                {
                    "start": 1442,
                    "end": 1591
                },
                {
                    "start": 1591,
                    "end": 1982
                },
                {
                    "start": 1982,
                    "end": 2126
                },
                {
                    "start": 2126,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 1099,
                    "end": 1103,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1135,
                    "end": 1139,
                    "matchedPaperCorpusId": "266351737"
                },
                {
                    "start": 1432,
                    "end": 1436,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1567,
                    "end": 1571,
                    "matchedPaperCorpusId": "248693351"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6708984375
        },
        {
            "corpus_id": "277244620",
            "title": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh Generation",
            "text": "Transformer architectures have revolutionized sequence modeling across domains, but their quadratic computational complexity with respect to sequence length creates significant scaling challenges. This limitation has catalyzed extensive research into more computationally efficient alternatives that preserve modeling capabilities. \n\nLinear attention mechanisms fundamentally reformulate the attention operation to achieve linear complexity with sequence length. Performer [8] extended this approach by approximating the softmax kernel using random feature projections, while Cosformer [31] introduced a positionaware cosine similarity-based attention mechanism. Despite these innovations, linear attention models often strug-gle with the expressivity-efficiency trade-off, as analyzed by Qin et al. [30] in their examination of the limitations inherent to linear formulations. \n\nAn alternative approach leverages recurrent architectures with enhanced expressivity. Mamba [17] introduced selective state space modeling (S4) for efficient sequence processing, achieving linear complexity while maintaining competitive performance. Its successor, Mamba2 [12], incorporates a gated update mechanism that further improves performance on language modeling and long-context understanding tasks. Similarly, RWKV [29] combines RNN-like computation with transformer-like expressivity to achieve efficient inference. \n\nMore sophisticated hybrid designs include Griffin [14], which interleaves gated linear attention with local attention patterns, and Samba [34], which integrates state space models with standard attention layers. DeltaNet [46] employs selective memory updates based on the delta rule, demonstrating particularly strong results for in-context retrieval tasks. \n\nMinimax-text-1 [21] pioneered the implementation of an interleaving attention structure in large language models, demonstrating the scalability and effectiveness of this architectural approach. Our work further enhances the efficiency of the interleaving strategy. While many recent approaches have introduced complex gating mechanisms or specialized activation functions to improve performance, our design philosophy emphasizes architectural simplicity and computational efficiency.",
            "score": 0.635327193680353,
            "section_title": "Linear and Hybrid Attention Mechanisms",
            "char_start_offset": 7960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 331
                },
                {
                    "start": 334,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1766
                },
                {
                    "start": 1769,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2252
                }
            ],
            "ref_mentions": [
                {
                    "start": 473,
                    "end": 476,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 586,
                    "end": 590,
                    "matchedPaperCorpusId": "246904340"
                },
                {
                    "start": 800,
                    "end": 804,
                    "matchedPaperCorpusId": "252992749"
                },
                {
                    "start": 1305,
                    "end": 1309,
                    "matchedPaperCorpusId": "258832459"
                },
                {
                    "start": 1630,
                    "end": 1634,
                    "matchedPaperCorpusId": "270371554"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10894775390625
        },
        {
            "corpus_id": "267200357",
            "title": "ProtHyena: A fast and efficient foundation protein language model at single amino acid Resolution",
            "text": "The self-attention operator, a key component of Transformer models as introduced by [40], plays a pivotal role in processing sequences. When given a sequence x of length L and dimension D, each head of the self-attention mechanism transforms x into an output y using the self-attention operator A(x): \n\nwhere W q , W k , and W v are learnable linear projections for the query, key, and value, respectively. The softmax and scaling function, denoted as sof tmax, is also applied. This mechanism allows the model to capture pairwise relationships among all tokens in the sequence, thereby understanding the global context. However, a notable drawback of self-attention is its high computational cost for long sequences, with complexity scaling quadratically with the sequence length O(L 2 ). To counteract this, various methods have been developed. One such method is factorized self-attention, used in sparse Transformers. This technique reduces memory and computational demands by allowing self-attention heads to focus on a subset of tokens. Another method is the Performer, which addresses the memory complexity of self-attention by decomposing the self-attention matrix. This allows the Performer to implicitly store the attention matrix with linear memory complexity, thereby enabling the processing of longer sequences than traditional Transformers. As an alternative, linear attention methods create approximations of the self-attention operator A(x) that can be computed more quickly, with less than quadratic time complexity. However, while these methods allow for processing longer sequences due to their reduced time complexity, they often come with a trade-off, resulting in a decrease in the model's expressivity. \n\nTo match the high performance of Transformers with a lower computational burden, it's crucial to obtain operators that possess attention's three defining properties: data control, sublinear parameter scaling, and unrestricted context. [28] successfully achieves this by introducing an operator characterized by a recurrent structure, which combines two subquadratic primitives: a long convolution and an elementwise multiplicative gating. This innovative operator retains the core attributes of attention, ensuring efficient processing across lengthy sequences without compromising on the ability to model complex dependencies.",
            "score": 0.6345460660842044,
            "section_title": "Background Self-Attention",
            "char_start_offset": 6223,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2166
                },
                {
                    "start": 2167,
                    "end": 2355
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1541748046875
        },
        {
            "corpus_id": "251403135",
            "title": "A length adaptive algorithm-hardware co-design of transformer on FPGA through sparse attention and dynamic pipelining",
            "text": "In this paper, we believe that the ideal Transformers acceleration should have a coherent algorithm-hardware co-design. We believe that (i) Transformers should have their dedicated efficient algorithm designs. Since self-attention cares more about the value relativity of all the attention scores than the absolute value of any specific attention score, we propose an efficient scheme to exploit two different self-attention approximations adaptively. Note, our approximation mechanisms are quantization-based designs that are not only computation-efficient but also hardware-friendly. We also think that (ii) Transformers should efficiently support various sequence length inputs. For instance, SQuAD v2.0 [8] has an average and maximum sequence length of 171 and 975, respectively. When padding the sequence with 975, it causes 5.7\u00d7 computational and memory bandwidth overhead on average. The inputs are sorted and processed according to the order of length. Compared to existing works, we achieve 4 \u00d7 higher energy efficiency than GPU accelerator optimized through CUBLAS GEMM routine [18,19] with small accuracy loss, and comparable energy efficiency compared to ASIC accelerator designs [12,13]. Our contributions are: \n\n\u2022 We propose sparse attention which is computation-efficient and hardware-friendly to reduce the need for computational resources and memory bandwidth. \u2022 We propose a sequence length adaptive design to allocate coarse pipeline stages dynamically to eliminate pipeline bubbles and achieve the highest possible throughput under different sequence length inputs. \u2022 Transformer exhibits a highly skewed distribution of computation complexity among the operators. We further develop a loop fusion to orchestrate the multiple attention operators and re-arrange various Transformer computations to enhance temporal locality and efficient hardware design with finer granularity.",
            "score": 0.6343200519969725,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1896
                }
            ],
            "ref_mentions": [
                {
                    "start": 707,
                    "end": 710,
                    "matchedPaperCorpusId": "47018994"
                },
                {
                    "start": 1088,
                    "end": 1092,
                    "matchedPaperCorpusId": "239037005"
                },
                {
                    "start": 1092,
                    "end": 1095,
                    "matchedPaperCorpusId": "235474235"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "211296403"
                },
                {
                    "start": 1196,
                    "end": 1199,
                    "matchedPaperCorpusId": "229298088"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2020263671875
        },
        {
            "corpus_id": "276585307",
            "title": "CQS-Attention: Scaling Up the Standard Attention Computation for Infinitely Long Sequences",
            "text": "To alleviate the unaffordable memory demand issue, new attention mechanisms, especially approximate attentions, are introduced such as sparse attention [5], [6], [7], [8], [9], [10], local-global attention [11], [12], [13], [14], [15], [16], [17], dilated attention [18], etc. Optimization approaches have been developed including memory optimization [8], [19], [20], [21], IO optimization [22], exploitation of the distributive law [2], replacing batch with sequential computation [23], [24], etc. Distributed attention solutions were proposed in recent years such as data parallelism, tensor parallelism [25] and pipeline parallelism [26]. However, 3-dimensional parallelism is still insufficient to scale up the context length of large language model (LLM) [27], [28]. Sequence parallelism was developed as 4D parallelism such as Ring Self-Attention (RSA) in [29], Ring Attention [27] and DistFlashAttn [28].",
            "score": 0.6341762797446127,
            "section_title": "A. RELATED WORK",
            "char_start_offset": 1233,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 911
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 155,
                    "matchedPaperCorpusId": "11792642"
                },
                {
                    "start": 167,
                    "end": 170,
                    "matchedPaperCorpusId": "195886317"
                },
                {
                    "start": 172,
                    "end": 175,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 218,
                    "end": 222,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 224,
                    "end": 228,
                    "matchedPaperCorpusId": "234364557"
                },
                {
                    "start": 242,
                    "end": 246,
                    "matchedPaperCorpusId": "247939839"
                },
                {
                    "start": 351,
                    "end": 354,
                    "matchedPaperCorpusId": "195886317"
                },
                {
                    "start": 390,
                    "end": 394,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 482,
                    "end": 486,
                    "matchedPaperCorpusId": "244346076"
                },
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "53670168"
                },
                {
                    "start": 766,
                    "end": 770,
                    "matchedPaperCorpusId": "263671659"
                },
                {
                    "start": 906,
                    "end": 910,
                    "matchedPaperCorpusId": "263671659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24072265625
        },
        {
            "corpus_id": "271270399",
            "title": "Hybrid Dynamic Pruning: A Pathway to Efficient Transformer Inference",
            "text": "Transformer models, including BERT [1], GPT [2], T5 [3] and others [4] [5] , have transformed Natural Language Processing (NLP) with their attention mechanism, achieving top performance in tasks such as question-answering [6], text classification [3], and machine translation [7].The transformer architecture uses self-attention mechanism [8] and it is highly parallelizable on modern Graphical Processing Units (GPUs), providing major benefits over older models like Long Short Term Memories (LSTMs) and Recurrent Neural Networks (RNNs).This has led to fast progress in NLP, with models like BERT exceeding human performance in difficult tasks [9] and expanding their application to computer vision, including object recognition and detection [10], image classification [11], and segmentation [12].\n\nDeploying large transformer models on devices with limited resources is challenging due to their high computational and memory requirements.For example, BERT-Base Transformer needs 440 MB of memory and over 176 Giga FLoating Point Operations (GFLOPs) [13] .The computations are particularly difficult because of the complex attention operations and the quadratic computational complexity related to the length of input sequences [14].Attention operations in transformer models become increasingly dominant as the input sequence length grows.For BERT-Base Transformer deployed on edge platforms, with a sequence length of 512, the attention operations account for about half of the total execution time, and this figure rises to 70% when the sequence length extends to 768 [15].Therefore, finding efficient ways to handle attention operations is crucial for speeding up transformers.\n\nMany studies utilized sparsity to mitigate the quadratic time and space complexity issue.Some techniques save computational effort by using fixed or static sparse attention patterns [16] [14] [17], but their performance is limited [18], since the sparse pattern in attention is naturally dynamic, and depends only on the input.Other techniques focus on dynamic sparsity, meaning there's no fixed pattern for which parts are sparse (zero).",
            "score": 0.6331751621663351,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 538
                },
                {
                    "start": 538,
                    "end": 799
                },
                {
                    "start": 801,
                    "end": 941
                },
                {
                    "start": 941,
                    "end": 1058
                },
                {
                    "start": 1058,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1342
                },
                {
                    "start": 1342,
                    "end": 1578
                },
                {
                    "start": 1578,
                    "end": 1683
                },
                {
                    "start": 1685,
                    "end": 1774
                },
                {
                    "start": 1774,
                    "end": 2012
                },
                {
                    "start": 2012,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 55,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 222,
                    "end": 225,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 276,
                    "end": 279,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 744,
                    "end": 748,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 794,
                    "end": 798,
                    "matchedPaperCorpusId": "235829175"
                },
                {
                    "start": 1052,
                    "end": 1056,
                    "matchedPaperCorpusId": "251554643"
                },
                {
                    "start": 1573,
                    "end": 1577,
                    "matchedPaperCorpusId": "239016160"
                },
                {
                    "start": 1877,
                    "end": 1881,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1916,
                    "end": 1920,
                    "matchedPaperCorpusId": "221702858"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.272705078125
        },
        {
            "corpus_id": "257050308",
            "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
            "text": "Large Transformers have enabled a number of breakthrough advances in modeling language, vision, audio, biology and numerous other domains (Vaswani et al., 2017), (Dosovitskiy et al., 2020), (Radford et al., 2022), (Cramer, 2021). Much of the success of Transformers, powered by the attention operator (Vaswani et al., 2017), relies on their scaling properties (Hoffmann et al., 2022) and the emergence of in-context learning (Garg et al., 2022), which allows them to generalize to unseen data and tasks given context as input. The Transformer block is a powerful tool for sequence modeling, but it is not without its limitations. One of the most notable is the computational cost, which grows rapidly as the length of the input sequence increases. Specifically, the cost scales quadratically with the length L of the sequence, which places a strict limit on the amount of context that can be considered by the model. Breaking the quadratic barrier is a key step towards new possibilities for deep learning, such as using entire textbooks as context, generating long-form music or processing gigapixel scale images. \n\nEfforts to reduce the computational cost of attention in models primarily involve the use of linearized, low-rank, and sparse approximations (Child et al., 2019;Wang et al., 2020;Kitaev et al., 2020;Zhai et al., 2021;Roy et al., 2021;Schlag et al., 2021;Tu et al., 2022). These approaches introduce a trade-off between expressivity and speed, requiring hybridization with standard attention layers to reach Transformer quality (Mehta et al., 2022;Dao et al., 2022c). \n\nA growing amount of evidence suggests that attention mechanisms only utilize a small portion of their quadratic capabilities for language processing (Olsson et al., 2022;Dao et al., 2022c), leading us to question its role as the gold-standard operator for deep learning at scale. Specifically, we ask: \n\nAre there subquadratic operators that can match the quality of attention at scale? \n\nFigure 1.1:",
            "score": 0.6287496871428883,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1114
                },
                {
                    "start": 1117,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1583
                },
                {
                    "start": 1586,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 1887
                },
                {
                    "start": 1890,
                    "end": 1972
                },
                {
                    "start": 1975,
                    "end": 1986
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 228,
                    "matchedPaperCorpusId": "236977527"
                },
                {
                    "start": 1334,
                    "end": 1351,
                    "matchedPaperCorpusId": "212718077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09271240234375
        },
        {
            "corpus_id": "258987968",
            "title": "Blockwise Parallel Transformer for Large Context Models",
            "text": "Transformers [52] have become the backbone of many state-of-the-art natural language processing models [15,43,5,35]. They have demonstrated impressive performance across a wide range of AI problems, including language modeling, machine translation, image captioning, and protein folding [39,47,32,43,5,45,9]. Transformers achieve this success through their architecture design that uses self-attention and position-wise feedforward mechanisms. These components facilitate the efficient capture of long-range dependencies between input tokens, enabling scalability in terms of context length and model size through highly parallel computations. \n\nHowever, the memory requirements of Transformers limit their ability to handle long sequences, which is necessary for many AI problems, such as high-resolution images, podcasts, code, or books and especially those that involve multiple long sequences or long-term dependencies [10,7,39,7,34,29,47,32,1]. The quadratic self-attention and the large feed forward network of Transformers require a large amount of memory, which makes it challenging to scale to longer input sequences. This limitation has led to various techniques proposed to reduce the memory requirements of Transformers, including sparse-approximation, low-rank approximation, and low precision approximation [see e.g. 51,24,22,11,25,36,54]. \n\nOne distinct line of research does not rely on approximation but instead focuses on computing exact self-attention with linear memory complexity. This approach leverages the observation that the softmax matrix in self-attention can be computed without materializing the full matrix [37]. This technique has led to the development of FlashAttention [14] and Memory Efficient Attention [42]. Both methods propose a blockwise computation of the self-attention softmax, demonstrating reduced memory requirements. (A), (B), and (C) show evaluation using one, eight A100, and 64 TPUv4, respectively, with a single sequence. Our method enables training sequences 32 times longer than vanilla attention-based Transformer [52], and 2 to 4 times longer than FlashAttention [14] and Memory Efficient Attention [42]. Section 3.1 provides a memory cost breakdown.",
            "score": 0.6282041020594631,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2206
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 110,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 110,
                    "end": 112,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 291,
                    "end": 294,
                    "matchedPaperCorpusId": "237260635"
                },
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 300,
                    "end": 302,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 302,
                    "end": 305,
                    "matchedPaperCorpusId": "231939146"
                },
                {
                    "start": 940,
                    "end": 943,
                    "matchedPaperCorpusId": "237260635"
                },
                {
                    "start": 943,
                    "end": 946,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 946,
                    "end": 948,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1331,
                    "end": 1334,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1334,
                    "end": 1337,
                    "matchedPaperCorpusId": "232110866"
                },
                {
                    "start": 1704,
                    "end": 1708,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 2119,
                    "end": 2123,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3779296875
        },
        {
            "corpus_id": "274822286",
            "title": "Expansion Span: Combining Fading Memory and Retrieval in Hybrid State Space Models",
            "text": "Attention on long contexts. Since the introduction of Self-Attention [33], a significant amount of research has been made to reduce its quadratic cost in the sequence length at training time. To achieve this, state of the art methods usually approximate the Attention mechanism with sparse/linearized versions. For example, Reformer [21] uses locality-sensitive-hashing to group tokens with similar embeddings, allowing the model to only attend to a subset of tokens rather than the entire sequence. Longformer [1] combines local Attention, whereby each token is restricted to attending only to a window of neighboring tokens, and global Attention, whereby a few tokens can attend to all tokens. Meanwhile, Linformer [35] uses projection matrices to project keys and values into a lower-dimensional space along the sequence dimension. Different from these approximate Attention methods, other works have proposed to endow Transformers models with 'compressed' memory tokens that are updated dynamically and causally over sliding windows on entire sequence chunks. For example, Transformer-XL [4] and Infini-Attention [24] segment an input sequence into chunks and process them sequentially while maintaining a complementary set of tokens whose purpose is to summarize the older ones. In contrast to these works, our Attention retrieves relevant tokens 'eidetically,' i.e., we retrieve tokens rather than maintain a compressed representation of the past. Most similar to our method is Landmark Attention [23], which inserts landmark tokens into the input at fixed block intervals and trains these tokens to act as summaries of their corresponding blocks via a grouped softmax attention; these summary tokens are then used to index and retrieve relevant input tokens when processing future segments. Our Attention aims to integrate retrieval natively, without the need for external landmark tokens or complex Softmax procedures. State Space Models. While a great effort has been made to improve the efficiency of Transformer models, a recent line of work has explored efficient alternative 'linear' architectures. In particular, State Space Models (SSMs) [13-15, 30, 36] have emerged as promising competitors to Transformer models due to their efficient scaling and strong empirical performance.",
            "score": 0.6278305226150268,
            "section_title": "Background and Related Work",
            "char_start_offset": 4119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2111
                },
                {
                    "start": 2112,
                    "end": 2293
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 73,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1529541015625
        },
        {
            "corpus_id": "270370979",
            "title": "SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models",
            "text": "The primary obstacle in scaling Transformer models to handle longer sequence lengths lies in the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements.This quadratic computational burden has prompted significant research efforts focused on developing more efficient sparse Transformer models.Notable examples include Longformer [4] and BigBird [41], which utilize a combination of local, global, and sparse attention mechanisms to manage long contexts, thereby reducing the complexity to O(n).These models achieve a balance between maintaining sufficient context for understanding while managing computational load.For achieving complexity of O(n log n), several approaches have been proposed.Fixed Window Attention [7] employs a fixed-size window for attention, which confines the attention computation to a limited context window.Reformer [21] introduces locality-sensitive hashing (LSH) to approximate attention by hashing similar tokens into the same buckets, thus reducing the computational complexity.LSG Attention [9], adapted from BigBird, combines local, sparse, and global attention to effectively handle long contexts while minimizing computational overhead.\n\nEquipping Transformer [40] proposes a novel reading strategy termed random access, which enables Transformers to efficiently process long documents without needing to examine every token.This method shows promising results across pretraining, fine-tuning, and inference phases, demonstrating its efficacy in handling extended contexts.Despite these advancements, the ability of these methods to manage long-context conversations, such as those required in chat applications, remains limited.This highlights an ongoing challenge in enhancing the context-handling capabilities of Transformer models for interactive and real-time applications.",
            "score": 0.6268173311472394,
            "section_title": "Long-context Transformers",
            "char_start_offset": 7163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 224,
                    "end": 365
                },
                {
                    "start": 365,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 766
                },
                {
                    "start": 766,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1080
                },
                {
                    "start": 1080,
                    "end": 1242
                },
                {
                    "start": 1244,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1579
                },
                {
                    "start": 1579,
                    "end": 1735
                },
                {
                    "start": 1735,
                    "end": 1884
                }
            ],
            "ref_mentions": [
                {
                    "start": 417,
                    "end": 421,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1094,
                    "end": 1097,
                    "matchedPaperCorpusId": "253157377"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28271484375
        },
        {
            "corpus_id": "219401747",
            "title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers",
            "text": "Transformers [44,17] are powerful neural network architectures that have become SOTA in several areas of machine learning including Natural Language Processing (NLP) (e.g. speech recognition [32]), Neural Machine Translation (NMT) [6], document generation/summarization, time series prediction, generative modeling (e.g. image generation [36]), music generation [26], and analysis of biological sequences [39,33,30]. Transformers rely on a trainable attention mechanism that specifies complex dependencies between the elements of each input sequence (e.g. amino acids within a protein). Unfortunately, a standard Transformer scales quadratically with the number of tokens L in the input sequence, which is prohibitively expensive for large L. Several solutions have been proposed to address this issue [1,23,3,7]. Most approaches restrict the attention mechanism to attend to local neighborhoods [36] or incorporate structural priors on attention such as sparsity [7], pooling-based compression [37] clustering/binning/convolution techniques (e.g. [41] which applies k-means clustering to learn dynamic sparse attention regions, or [28], where locality sensitive hashing is used to group together tokens of similar embeddings), sliding windows [1], or truncated targeting [4]. Thus these approaches do not aim to approximate regular attention, but rather propose simpler and more tractable attention mechanisms, often by incorporating additional constraints (e.g. identical query and key sets as in [28]), or by trading regular attention with sparse attention using more layers [7]. Furthermore, many of these works require special customized GPU operations (e.g. either writing C++ CUDA kernels [7] or using TVMs [1]). Other techniques which aim to improve the time complexity of Transformers include reversible residual layers allowing for one-time activation storage in training [28] and shared attention weights [48]. These constraints may impede application to problems that involve long sequences, where approximations of the attention mechanism are not sufficient.",
            "score": 0.6262705196937545,
            "section_title": "Introduction and related work",
            "char_start_offset": 32,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2071
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 17,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 17,
                    "end": 20,
                    "matchedPaperCorpusId": "49667762"
                },
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "13747425"
                },
                {
                    "start": 338,
                    "end": 342,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "54477714"
                },
                {
                    "start": 896,
                    "end": 900,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 995,
                    "end": 999,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 1132,
                    "end": 1136,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1499,
                    "end": 1503,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1882,
                    "end": 1886,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1916,
                    "end": 1920,
                    "matchedPaperCorpusId": "195657947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.135009765625
        },
        {
            "corpus_id": "267412232",
            "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models",
            "text": "Pattern. An alternative strategy to address the computation and memory challenges of the self-attention module in transformers involves leveraging sparse patterns to handle long contexts effectively. These patterns use a sparse attention matrix, where each token attends to a limited set of other tokens. Various methods have been proposed to introduce the sparsity, which, while not specifically designed for long contexts, can effectively help manage long sequences. \n\nAmong the most straightforward yet practical instances of sparse patterns, Block-wise Self Attention [Qiu et al., 2020], stands out as an illustrative demonstrations. This method reduces the computation and memory cost by chunking the input sequence into fixed blocks. An alternative strategy involves having an individual token attend to tokens at regular, fixed internals. For instance, Longformer [Beltagy et al., 2020] is a sparsifying mechanism that utilizes dilated windows of tokens to construct the attention matrix. LogSparse [Li et al., 2019] is another method that sparsifies the attention matrix by restricting consideration to a limited window of tokens, where the window is defined by exponential steps from the token itself. This approach ensures a targeted focus range for each individual token. By employing LogSparse, it is guaranteed that any pair of tokens can exchange attention information with each other, while the memory usage of the transformer can be reduced to O(n(Log n) 2 ). LongNet [Ding et al., 2023] introduces dilated attention, in which attention allocation decreases exponentially as the distance between tokens increases. This approach exploits mixed dilated rates to accommodate both local and global dependencies between different tokens. It has been shown that by utilizing LongNet, a linear computation complexity, O(n), and a logarithm dependency between tokens can be achieved. Some other sparse transformers consider adaptive sparse patterns which are not dependent on the location of the tokens, but rather they rely on other dynamic factors such as embedding values or task-specific parameters. For instance, Routing Transformer [Roy et al., 2021] exploits dynamic keyvalue pairs to infer sparsity patterns and hence, it removes the computation and memory requirements of attending to content unrelated to the query of interest.",
            "score": 0.6257852699141087,
            "section_title": "Sparse",
            "char_start_offset": 14547,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2111
                },
                {
                    "start": 2112,
                    "end": 2345
                }
            ],
            "ref_mentions": [
                {
                    "start": 572,
                    "end": 590,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 871,
                    "end": 893,
                    "matchedPaperCorpusId": "215737171"
                },
                {
                    "start": 1006,
                    "end": 1023,
                    "matchedPaperCorpusId": "195766887"
                },
                {
                    "start": 1484,
                    "end": 1503,
                    "matchedPaperCorpusId": "255372747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1578369140625
        },
        {
            "corpus_id": "229340359",
            "title": "Sub-Linear Memory: How to Make Performers SLiM",
            "text": "The Transformer architecture (Vaswani et al., 2017) has changed the landscape of deep learning for sequential data. In contrast to more conventional methods such as recurrent neural networks (Hochreiter & Schmidhuber, 1997;Cho et al., 2014), the self-attention module, responsible for temporal information propagation, is fully-parallelizable, meaning that the training speed can be increased by simply using more compute resources. \n\n1 University of Cambridge 2 Google Brain 3 Columbia University 4 DeepMind 5 Stanford University 6 Alan Turing Institute. Correspondence to: Valerii Likhosherstov <vl304@cam.ac.uk>. \n\nHowever, this parallel-friendly structure of self-attention comes at a cost of quadratic O(L 2 ) time and memory complexity, where L is the length of the Transformer's input sequence. A recent line of work aimed to address this restriction, using either structured sparsity (Child et al., 2019), truncated back-propagation (Dai et al., 2019), clustering (Kitaev et al., 2020;Roy et al., 2020) or linear attention methods (Katharopoulos et al., 2020;Choromanski et al., 2020;Shen et al., 2018;Li et al., 2020). For a detailed overview of efficient Transformers, see (Tay et al., 2020b). We refer to the family of linear attention architectures as Performers, following Choromanski et al. (2020), since their generic kernel formulation covers all the aforementioned linear attention methods. Performers reduce time and memory complexity to linear O(L) and can provably approximate conventional quadratic Transformers (Choromanski et al., 2020), demonstrating strong performance in a systematic comparison of efficient Transformers (Tay et al., 2020a). This recent trend of feeding longer sequences into Transformers, coupled with the use of deeper models, introduces new challenges for researchers and practitioners. Whereas conventional Transformer setups benefit from large-batch optimization (You et al., 2019), long sequence modelling necessitates smaller batch sizes in order to fit the model into memory.",
            "score": 0.6254129495633101,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2026
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 51,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 191,
                    "end": 223,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 223,
                    "end": 240,
                    "matchedPaperCorpusId": "5590763"
                },
                {
                    "start": 972,
                    "end": 993,
                    "matchedPaperCorpusId": "209315300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24853515625
        },
        {
            "corpus_id": "278714643",
            "title": "Aquarius: A Family of Industry-Level Video Generation Models for Marketing Scenarios",
            "text": "Innermost TP-SP. Among 8 ranks within a node, we adopt Tensor Parallelism(TP) (Shoeybi et al., 2019) and Sequence Parallelism(SP) (Korthikanti et al., 2023) to partition the model states and activations. In each Transformer block, we employ a dedicated MLP (a Linear layer followed by a SiLU) to incorporate conditioning embedding for Adaptive Layer Normalization (AdaLN), aiming to enhance the model's capacity to capture conditional information. However, this design incurs substantial memory overhead: these dedicated Linear collectively increase the model's total parameter count by over 3B and introduce redundant model states exceeding 20GB per rank. To address this issue, we partition these Linear layers in a column parallel fashion with their computation and parameters parallelized across devices. Specifically, during the forward pass, all-gather reconstructs the complete Linear output, while reduce-scatter synchronizes gradients across the sequence-parallel group before the Linear in the backward pass. \n\nMid-Optional CP. While TP-SP supports training with moderately long contexts, Context Parallelism (CP) (Jacobs et al., 2023) is optionally enabled only for ultra-long sequences (exceeds 200k tokens). \n\nFor non-attention computations, each layer processes 1/CP of the input sequence. Before and after attention computations, all-to-all communication is applied to transpose the sequence, enabling each rank to process attention with full context length but only 1/CP of the attention heads. Since CP incurs lower communication overhead than TP-SP, we configure TP-SP as inner parallelism and CP as outer parallelism. \n\nOutermost ZeRO-DP. We employ DP as the outermost parallelization to enhance throughput. Since the exponential-moving-average (EMA) parameters and moments in AdamW contribute substantially to memory footprint, we adopt ZeRO (Rajbhandari et al., 2020) optimization to partition redundant optimizer states across data-parallel groups. Compared to optimizer states, memory consumption for model parameters and gradients proves relatively minor.",
            "score": 0.6238436413990509,
            "section_title": "Parallel Strategy",
            "char_start_offset": 21999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 1018
                },
                {
                    "start": 1021,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1220
                },
                {
                    "start": 1223,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1636
                },
                {
                    "start": 1639,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2079
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 156,
                    "matchedPaperCorpusId": "248693351"
                },
                {
                    "start": 1862,
                    "end": 1888,
                    "matchedPaperCorpusId": "269617042"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65576171875
        },
        {
            "corpus_id": "259063695",
            "title": "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention",
            "text": "Quadratic computational cost of attention in transformers. In Fig. 9 we show the runtime (forward + backward) of a transformer language model as a function of the sequence length. We separate the time taken by the attention operations from the time taken by the rest of the model. We see how the attention computation ends up dominating the runtime as the sequence length increases. We measure the forward and backward runtimes for a 12 layers transformer (see App. B.3 for implementation details). Except for the attention operations, the rest of the transformer runtime grows linearly with the sequence length. \n\nAdditional runtimes performances in a vacuum. In Fig. 10 and Fig. 11 we show the runtime details for the forward and backward methods separately for respectively the Hash-sparse and QKsparse methods. We also measure runtimes of the forward and backward passes when we assume the pre and post-processing steps aree free, see Fig. 12.  Linear QK-dropping scheduler. In the main paper we show results dropping keys and queries at random with a fixed pre-defined probability. In an additional experiment we start by dropping 80% of keys and queries at random and linearly decay this probability to 20%. Our intuition is earlier iterations just aim to learn contextual cues which are very redundant (and therefore quite immune to random dropping) before requiring more fine-grained representations. In Fig. 13 we show and analyse the results of that experiment. \n\nH-LM models speeding up during training. The speed of Hash-sparse attention is conditioned on the distribution of bucket indices over keys and queries-e.g. if all the keys and queries were to fall in  Forward+backward runtimes for our Hash-sparse method when the pre and postproceessing steps are free. While the runtimes for large sequences stays relatively the same compared to Fig. 3, the offset for short sequences is now smaller. Intrestingly, unlike for the QK-sparse method, the offset for smaller sequences stays significant.",
            "score": 0.6196806041681693,
            "section_title": "C Additional Details and Analysis",
            "char_start_offset": 39349,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 59,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 612
                },
                {
                    "start": 615,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1471
                },
                {
                    "start": 1474,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2007
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.086181640625
        },
        {
            "corpus_id": "267627468",
            "title": "FAST: Factorizable Attention for Speeding up Transformers",
            "text": "Transformers are the single deep learning architecture that underpin many recent successful applications in diverse fields, including in natural language processing, speech, computer vision, and biology. \n\nTransformers incorporate a Softmax-based all-to-all score computation mechanism denoted as \"attention\". While this mechanism has proven to be extremely effective in learning tasks, they have a cost that is quadratic in the length of the input (N ) and in the data dimension (D), and need a similar amount of memory. Our goal is to develop a more efficient transformer implementation that is as expressive using a novel attention formulation described in \u00a7 2. quadratic dependence on N , the problem of \"long range attention\" that arises needs to be tackled, and is dealt with in the literature in various ways. \n\n1. Algorithmic: Faster attention algorithms via various approaches, including spectral matrix decompositions, kernel method approximations, and sparsification via locality sensitive hashing have been proposed [6,20,10,11,9,1], but these solutions have not appeared to have gained traction. This appears to be for the most part due to perceived lower expressivity of the resulting transformers. These algorithms are further discussed in \u00a74. \n\n2. Parallelization: Efficient attention via careful parallelization and minimizing communication between the CPU and the GPU [5,4]. This is widely used, but is still quadratic. There are also quadratic frameworks to extend training across multiple nodes and GPUs to allow larger problems to fit in memory [15,16]. \n\n3. Non-transformer Models: New learning architectures are being proposed as an alternative to Transformers. Recent ones which have generated considerable interest include Mamba [8], \n\nRetentive networks [17], and CRATE [23].These do require re-engineering the model pipeline. \n\nIn practice, the really large models use quadratic vanilla attention strategies, and work on the data in batches at various stride-lengths and stitch together the results to reach token lengths to the trillions [3,19]. Previously proposed fast attention mechanisms do not appear to have been integrated into these works.",
            "score": 0.6192370282931363,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 206,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1258
                },
                {
                    "start": 1261,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 1034,
                    "end": 1037,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1386,
                    "end": 1389,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1566,
                    "end": 1570,
                    "matchedPaperCorpusId": "221191193"
                },
                {
                    "start": 1570,
                    "end": 1573,
                    "matchedPaperCorpusId": "239768728"
                },
                {
                    "start": 2066,
                    "end": 2069,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1114501953125
        },
        {
            "corpus_id": "253158021",
            "title": "Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost",
            "text": "The Transformer [40] architecture has been the go-to method for encoding sequential data, due to its superior performance in various tasks such as machine translation [30], image classification [15], and protein language modeling [34]. Its key strength stems from the multi-head attention module, where a so-called attention score matrix computes how contextually important one token is to another for all possible token pairs. Each Transformer layer simultaneously pools the token representations based on the attention scores, eventually returning contextualized features without sequentially traversing through the input sequence as its recurrent neural network-based predecessors [18]. \n\nA well-known drawback of the original Transformer is its high computational cost in time and memory that increases quadratically with sequence length. This is due to the full pairwise computation of attention scores, which prohibits applying it in tasks involving long-range dependencies such as document summarization [19] or high-resolution image processing [53]. Many works have thus focused on developing more efficient alternatives by exploiting fixed or learnable attention sparsity patterns [9,51,22,13], low-rank approximations [45,48], or kernelized attention modules [21,10]. \n\nEven though the efficient alternatives hold theoretical expressibility guarantees [50], they are far from sufficient, still failing to convince practitioners to replace the original Transformer. We believe this is mostly due to their lack of adaptability. They apply the same modifications to unanimously sparsify all the attention modules across layers, without considering the tasks at hand. Such strategy  In multi-head attention, each attention head samples a bipartite graph connecting queries to keys from an underlying SBM. The adjacency of the sampled graph is used as an attention mask to compute the dot products only for the sampled edges. \n\nimposes inductive bias too strongly and often leads to sub-optimal cost vs. performance trade-offs in downstream tasks [29]. In this work, we argue that to retain the utmost potential of Transformers, each attention module should have the ability to flexibly choose between sparse and full attention. This is especially evident when considering many state-of-the-art systems suggest the need for a mixture of dense and sparse attention layers.",
            "score": 0.6187446512443552,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1930
                },
                {
                    "start": 1933,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2233
                },
                {
                    "start": 2234,
                    "end": 2376
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 171,
                    "matchedPaperCorpusId": "44131019"
                },
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 230,
                    "end": 234,
                    "matchedPaperCorpusId": "231939146"
                },
                {
                    "start": 684,
                    "end": 688,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1011,
                    "end": 1015,
                    "matchedPaperCorpusId": "233033613"
                },
                {
                    "start": 1052,
                    "end": 1056,
                    "matchedPaperCorpusId": "232404731"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2166748046875
        },
        {
            "corpus_id": "239049478",
            "title": "Transformer Acceleration with Dynamic Sparse Attention",
            "text": "Transformers are the mainstream of NLP applications and are becoming increasingly popular in other domains such as Computer Vision. Despite the improvements in model quality, the enormous computation costs make Transformers difficult at deployment, especially when the sequence length is large in emerging applications. Processing attention mechanism as the essential component of Transformer is the bottleneck of execution due to the quadratic complexity. Prior art explores sparse patterns in attention to support long sequence modeling, but those pieces of work are on static or fixed patterns. We demonstrate that the sparse patterns are dynamic, depending on input sequences. Thus, we propose the Dynamic Sparse Attention (DSA) that can efficiently exploit the dynamic sparsity in the attention of Transformers. Compared with other methods, our approach can achieve better trade-offs between accuracy and model complexity. Moving forward, we identify challenges and provide solutions to implement DSA on existing hardware (GPUs) and specialized hardware in order to achieve practical speedup and efficiency improvements for Transformer execution.",
            "score": 0.61856952203507,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.319580078125
        },
        {
            "corpus_id": "253237189",
            "title": "XNOR-FORMER: Learning Accurate Approximations in Long Speech Transformers",
            "text": "Transformers are among the state of the art for many tasks in speech, natural language processing and vision, among other fields involving ordered sequences of data. They achieve their exceptional generalization in part due to \"multi-head self-attention\" -processing blocks which derive (a set of) updated representations for each input in a sequence as a weighted sum of values derived from all the inputs in the sequence. This computation, however, has a computational and space complexity that is quadratic in the length of the input sequence. As a result, it becomes challenging to operate on longer input sequences on modern GPUs and TPUs, and training such models from scratch is time and labor intensive. \n\nThe computational challenges arise from the formulation of selfattention. For an input sequence of length N each head of the multihead self attention block derives three N -row matrices: a value matrix V representing the latent-representation vectors for the N inputs in the sequence, a query matrix Q representing the probes with which each input derives the \"self-attention\" weights required to update itself, and a key matrix K, representing the key contribution of each input to the computation of its own weight in updating any input. The actual updated representations for the input computed by the head has the form sof tmax(QK )V . The bottleneck arises from sof tmax(QK ): both Q and K have N rows, QK requires O(N 2 ) computation. This cannot be factored, since the softmax operates on the product, which must necessarily be computed before the softmax is applied. \n\nRecently, many works have had success in developing attention mechanisms that approximate the standard self-attention in linear time and space complexity. \n\nPattern based methods [1,2,3,4] save on computation by limiting the size of the input blocks they can work on, however they potentially lose the larger context of the input in the process. Sparsity based methods [5,6] use low-rank approximations of attention to reduce compuation; however they are, by nature lossy, approaching the computational complexity of full self-attention if losslessness is to be assured. Linear transformers [7, 8, ?] do achieve linear-time and can thus operate on long sequences.",
            "score": 0.6169842697261224,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1588
                },
                {
                    "start": 1591,
                    "end": 1745
                },
                {
                    "start": 1748,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2161
                },
                {
                    "start": 2162,
                    "end": 2191
                },
                {
                    "start": 2192,
                    "end": 2254
                }
            ],
            "ref_mentions": [
                {
                    "start": 1770,
                    "end": 1773,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1773,
                    "end": 1775,
                    "matchedPaperCorpusId": "215737171"
                },
                {
                    "start": 1775,
                    "end": 1777,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10723876953125
        },
        {
            "corpus_id": "267949483",
            "title": "TLS-RWKV: Real-Time Online Action Detection with Temporal Label Smoothing",
            "text": "While the Transformer model has demonstrated remarkable capability in handling long-distance dependencies, it is hindered by the square compute complexity of cross self-attention. To address this challenge, a variety of approaches have been proposed. Some focus on optimizing the attention mechanism, employing techniques such as sparse self-attention [26], kernelization [21], low-rank approximations [27], and other methods [28,29]. Other researches explore alternative modules to replace attention. MLP-Mixer [30] replaces attention with Multilayer Perceptrons(MLPs), while the Attention Free Transformer(AFT) [31] introduces a computationally efficient alternative to the traditional dot-product self-attention mechanism. Inspired by AFT, RWKV [17] simplifies interaction weights to enable a RNN-style implementation for inference. For a comprehensive overview of more efficient Transformer variants, a survey by Tay et al. [32] can be referenced. Additionally, some approaches modify recurrent neural networks(RNN) to increase context length, such as the Recurrent Memory Transformer [33], Linear Recurrent Unit [34], and state space models(SSM) [35][36][37][38]. These techniques offer alternative strategies to enhance the context modeling capabilities of sequence-based models.",
            "score": 0.6165475797915729,
            "section_title": "Efficient Long Sequence Modeling",
            "char_start_offset": 4427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1285
                }
            ],
            "ref_mentions": [
                {
                    "start": 372,
                    "end": 376,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 430,
                    "end": 433,
                    "matchedPaperCorpusId": "247011581"
                },
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "233714958"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1089,
                    "end": 1093,
                    "matchedPaperCorpusId": "250526424"
                },
                {
                    "start": 1151,
                    "end": 1155,
                    "matchedPaperCorpusId": "240354066"
                },
                {
                    "start": 1155,
                    "end": 1159,
                    "matchedPaperCorpusId": "247762199"
                },
                {
                    "start": 1159,
                    "end": 1163,
                    "matchedPaperCorpusId": "260443992"
                },
                {
                    "start": 1163,
                    "end": 1167,
                    "matchedPaperCorpusId": "251442769"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251220703125
        },
        {
            "corpus_id": "246017095",
            "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
            "text": "Sequence parallelism is designed for training Transformerbased models with longer input sequences so we investigated the maximum sequence length it can handle. Similarly, we still compared TP with and without PP. We fixed batch size as 64 for BERT Base and no PP was used.\n\nWe show the maximum sequence length of the BERT Base model in Figure 5. If we scale up to 64 GPUs, we can achieve around 3\u00d7 maximum sequence length on BERT Base. Another observation is splitting along the number of attention heads limits the input sequence length of tensor parallelism in Megatron, but our sequence parallelism can scale easily by  Table 3: Weak scaling results. P is the tensor or sequence parallel size. B and S are global batch size and sequence length, respectively. M and T denote max allocated memory/MB and tokens processed per second. OOM means that CUDA out of memory occurs. splitting a sequence into multiple chunks. When using the same 16 GPUs, our sequence parallelism still can achieve 1.4 times larger sequence length than tensor parallelism. The gap is expected to widen if we use 32GB GPUs instead of 16GB GPUs. Also, in Appendix, we investigate the maximum sequence length our system can handle when we use a smaller batch size. Our RSA focuses on full self-attention in this paper. According to Table 2, when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences, because three terms of memory usage include L/N . We leave it as our future work.",
            "score": 0.61582621786598,
            "section_title": "Maximum sequence length",
            "char_start_offset": 15467,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5693359375
        },
        {
            "corpus_id": "229924221",
            "title": "Shortformer: Better Language Modeling using Shorter Inputs",
            "text": "Many recent papers have explored how to improve the efficiency of transformers by reducing the quadratic cost of self-attention, motivated by scaling to longer sequences (Kitaev et al., 2020;Roy et al., 2020;Tay et al., 2020). We show that longer sequences can be harmful and instead demonstrate improved results with shorter sequences, which naturally also improves efficiency.\n\nOne way to reduce the memory consumption of a transformer model is to sparsify the attention matrix by letting each token attend only to a subset of the nearby tokens Beltagy et al., 2020). Our approach, training on shorter subsequence lengths, is much more efficient since we do not sparsify an attention matrix: we use multiple, but much smaller, attention matrices, which are dense. Since attention uses memory and computation in a way that scales quadratically with input size, by splitting the inputs into multiple subsequences that are each processed independently, we use much less memory and run faster. The model of Beltagy et al. (2020) uses different, sparse attention patterns at every layer, with the number of neighboring tokens that each tokens attends to growing in each subsequent layers. In our method, we let each layer attend to the same number of tokens at every stage. Similar to our method, Beltagy et al. (2020) let each token attend a growing number of neighbors as training progresses, but they use five stages, which we found not to be superior to the two stages in our method.\n\nThe adaptive attention span model of Sukhbaatar et al. (2019) learns the maximum effective context window sizes for each head at each layer inde-pendently. Like our method, the context window sizes are smaller at the beginning of training and get longer as training progresses. We show that a simple approach of manually choosing two subsequence lengths is highly effective. In addition, keeping subsequence lengths equal across all heads and layers lets us save memory and runtime.",
            "score": 0.6151020234062025,
            "section_title": "Staged Training",
            "char_start_offset": 27425,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13671875
        },
        {
            "corpus_id": "219401765",
            "title": "GMAT: Global Memory Augmentation for Transformers",
            "text": "Transformer-based models have become ubiquitous in natural language processing thanks to their large capacity, innate parallelism and high performance. The contextualizing component of a Transformer block is the $\\textit{pairwise dot-product}$ attention that has a large $\\Omega(L^2)$ memory requirement for length $L$ sequences, limiting its ability to process long documents. This has been the subject of substantial interest recently, where multiple approximations were proposed to reduce the quadratic memory requirement using sparse attention matrices. In this work, we propose to augment sparse Transformer blocks with a dense attention-based $\\textit{global memory}$ of length $M$ ($\\ll L$) which provides an aggregate global view of the entire input sequence to each position. Our augmentation has a manageable $O(M\\cdot(L+M))$ memory overhead, and can be seamlessly integrated with prior sparse solutions. Moreover, global memory can also be used for sequence compression, by representing a long input sequence with the memory representations only. We empirically show that our method leads to substantial improvement on a range of tasks, including (a) synthetic tasks that require global reasoning, (b) masked language modeling, and (c) reading comprehension.",
            "score": 0.6146435265106387,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3369140625
        },
        {
            "corpus_id": "259982812",
            "title": "Exploring Transformer Extrapolation",
            "text": "Transformer (Vaswani et al. 2017) is advancing steadily in the areas of natural language processing (Qin et al. 2023b;Devlin et al. 2019;Liu et al. 2019;Qin et al. 2022b,a;Liu et al. 2022;Qin andZhong 2023), computer vision (Dosovitskiy et al. 2020;Sun et al. 2022b;Lu et al. 2022;Hao et al. 2024), and audio processing (Gong, Chung, and Glass 2021;Akbari et al. 2021;Gulati et al. 2020;Sun et al. 2022a). Although it outperforms other architectures such as RNNs (Cho et al. 2014;Qin, Yang, and Zhong 2023) and CNNs (Kim 2014;Hershey et al. 2016;Gehring et al. 2017) in many sequence modeling tasks, its lack of length extrapolation capability limits its ability to handle a wide range of sequence lengths, i.e., inference sequences need to be equal to or shorter than training sequences. Increasing the training sequence length is only a temporary solution because the space-time complexity grows quadratically with the sequence length. Another option is to extend the inference sequence length by converting the trained full attention blocks to sliding window attention blocks (Beltagy, Peters, and Cohan 2020), but this will result in significantly worse efficiency than the full attention speed (Press, Smith, and Lewis 2022). How to permanently resolve this issue without incurring additional costs has emerged as a new topic. \n\nA mainstream solution for length extrapolation is to design a Relative Positional Encoding (RPE) (Qin et al. 2023c) that concentrates attention on neighboring tokens. For example, ALiBi (Press, Smith, and Lewis 2022) applies linear decay biases to the attention to reduce the contribution from distant tokens. Kerple (Chi et al. 2022) investigates shiftinvariant conditionally positive definite kernels in RPEs and proposes a collection of kernels that promote the length extrapolation property. It also shows that ALiBi is one of its instances.",
            "score": 0.6143749015308135,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1331
                },
                {
                    "start": 1334,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 137,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 137,
                    "end": 153,
                    "matchedPaperCorpusId": "234762885"
                },
                {
                    "start": 172,
                    "end": 188,
                    "matchedPaperCorpusId": "234762885"
                },
                {
                    "start": 249,
                    "end": 266,
                    "matchedPaperCorpusId": "249890146"
                },
                {
                    "start": 266,
                    "end": 281,
                    "matchedPaperCorpusId": "234762885"
                },
                {
                    "start": 281,
                    "end": 297,
                    "matchedPaperCorpusId": "260926399"
                },
                {
                    "start": 320,
                    "end": 349,
                    "matchedPaperCorpusId": "233024831"
                },
                {
                    "start": 463,
                    "end": 480,
                    "matchedPaperCorpusId": "5590763"
                },
                {
                    "start": 480,
                    "end": 506,
                    "matchedPaperCorpusId": "265212770"
                },
                {
                    "start": 516,
                    "end": 526,
                    "matchedPaperCorpusId": "9672033"
                },
                {
                    "start": 526,
                    "end": 546,
                    "matchedPaperCorpusId": "8810481"
                },
                {
                    "start": 546,
                    "end": 565,
                    "matchedPaperCorpusId": "3648736"
                },
                {
                    "start": 1431,
                    "end": 1448,
                    "matchedPaperCorpusId": "265212770"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08868408203125
        },
        {
            "corpus_id": "219558319",
            "title": "$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers",
            "text": "We now state our main theorem, which shows that if the sparsity patterns {A l k } and the probability map \u03c1 satisfy Assumptions 1 and 2, sparse Transformers with h = 2 attention heads of size m = 1, and hidden layer width r = 4 are universal approximators of continuous sequence-to-sequence functions on any compact domain (recall that F denotes the class of such continuous functions). \n\nTheorem 1. Consider any f \u2208 F , and the class of sparse Transformers ST 2,1,4 (cf. (3)) with the underlying sparse attention layers satisfying Assumptions 1 and 2. Then, for any > 0 and 1 \u2264 p < \u221e, there exists a function g \u2208 ST 2,1,4 such that \n\nAs discussed earlier, dense Transformers do satisfy Assumptions 1 and 2. Thus, Theorem 1 subsumes the existing result [28] for dense Transformers. Also, we note that the required width parameters h, m, and r in Theorem 1 are completely independent of d, n, or the sparsity patterns. \n\nThe key justifying intuition for adopting sparse attention layers is that, if each token can attend to the other tokens in multiple hops 1 , then these models do not lose too much expressive power. However, there has been no formal justification for this intuition. Our theorem provides the first formal evidence that well-designed sparse attention layers do not limit Transformer's universal approximation power. In \u00a7 3.4, we show a surprising result that there exist sparse self-attention layers with only O(n) connections (as opposed to n 2 connections in regular self-attention layers) that retain enough expressive power to approximate F . This advantage of sparse Transformers over their dense counterpart becomes even stronger with increasing sequence length n, providing a theoretical support for the adoption of sparsity for the tasks with long sequence lengths. \n\nWe provide a high-level proof sketch of Theorem 1 in \u00a7 4.1. Although the outline of the proof is similar to [28], the sparsity in attention mechanism and the choice of general probability map \u03c1 pose nontrivial challenges in extending the existing result to our setting. We detail these challenges and briefly describe how we overcome them in \u00a7 4.2.",
            "score": 0.6141035430019076,
            "section_title": "Sparse Transformers are universal approximators",
            "char_start_offset": 10766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1791
                },
                {
                    "start": 1794,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2142
                }
            ],
            "ref_mentions": [
                {
                    "start": 753,
                    "end": 757,
                    "matchedPaperCorpusId": "209444410"
                },
                {
                    "start": 1902,
                    "end": 1906,
                    "matchedPaperCorpusId": "209444410"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08929443359375
        },
        {
            "corpus_id": "272593079",
            "title": "Gated Slot Attention for Efficient Linear-Time Sequence Modeling",
            "text": "Transformers [88] have emerged as the predominant architecture for most, if not all, sequence modeling tasks. Nevertheless, the quadratic complexity of softmax-based standard attention (SA) poses significant challenges for long sequence modeling (e.g., video understanding and biological sequence modeling). In the context of language modeling, where sequence lengths are moderate, training efficiency is generally not a primary concern. However, during inference, the Key-Value (KV) cache [34,64] grows linearly with the generation length, resulting in substantial memory burdens and throughput bottlenecks due to high I/O costs. Linear (kernelized) attention [42] and its gated variants [96,82,68,61,16,69] have received interest as promising alternatives to softmax attention. These models demonstrate strong performance in language modeling and understanding tasks. Notably, they can be reframed as RNNs during inference, achieving constant memory complexity and thereby significantly enhancing inference efficiency. \n\nHowever, two key issues persist with these models: (i) Performance-wise, recent research indicates that linear recurrent models still struggle with tasks requiring in-context retrieval or learning [2,1,37,28], and there is a fundamental recall-memory trade-off [3,91] where all inference-time-constantmemory models face inherent limitations. (ii) In terms of training efficiency, while linear attention supports hardware-efficient chunkwise training [96] as implemented in FlashLinearAttention (FLA [95]), training from scratch on trillions of tokens remains prohibitively expensive. A paradigm, \"finetuning pretrained Transformers to RNNs\" (short for T2R [41]), has recently gained great attention [101,10,54,13,7,90]. This approach circumvents the high cost of training from scratch by requiring only a few billion tokens for finetuning-about 1-3% of the total cost. However, linear attention uses a different kernel method from softmax, leading to performance discrepancies when finetuning pretrained softmax attention models to linear attention [101].",
            "score": 0.6139708717701879,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 17,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 490,
                    "end": 494,
                    "matchedPaperCorpusId": "214816402"
                },
                {
                    "start": 661,
                    "end": 665,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "266162792"
                },
                {
                    "start": 1225,
                    "end": 1228,
                    "matchedPaperCorpusId": "267406617"
                },
                {
                    "start": 1473,
                    "end": 1477,
                    "matchedPaperCorpusId": "266162792"
                },
                {
                    "start": 1679,
                    "end": 1683,
                    "matchedPaperCorpusId": "232335426"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08062744140625
        },
        {
            "corpus_id": "268714807",
            "title": "Proxyformer: Nystr\u00f6m-Based Linear Transformer with Trainable Proxy Tokens",
            "text": "Transformer-based models have demonstrated remarkable performance in various domains, including natural language processing, image processing and generative modeling. The most significant contributor to the successful performance of Transformer models is the self-attention mechanism, which allows for a comprehensive understanding of the interactions between tokens in the input sequence. However, there is a well-known scalability issue, the quadratic dependency (i.e. O(n^2)) of self-attention operations on the input sequence length n, making the handling of lengthy sequences challenging. To address this limitation, there has been a surge of research on efficient transformers, aiming to alleviate the quadratic dependency on the input sequence length. Among these, the Nystr\u00f6mformer, which utilizes the Nystr\u00f6m method to decompose the attention matrix, achieves superior performance in both accuracy and throughput. However, its landmark selection exhibits redundancy, and the model incurs computational overhead when calculating the pseudo-inverse matrix. We propose a novel Nystr\u00f6m method-based transformer, called Proxyformer. Unlike the traditional approach of selecting landmarks from input tokens, the Proxyformer utilizes trainable neural memory, called proxy tokens, for landmarks. By integrating contrastive learning, input injection, and a specialized dropout for the decomposed matrix, Proxyformer achieves top-tier performance for long sequence tasks in the Long Range Arena benchmark.",
            "score": 0.6098502652913156,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2413330078125
        },
        {
            "corpus_id": "236912844",
            "title": "Armour: Generalizable Compact Self-Attention for Vision Transformers",
            "text": "The attention mechanism was first introduced and used in conjunction with a recurrent network in [2] to help memorize long sequences in neural machine translation. The Transformer [17] further proposed the multi-head attention mechanism which performs multiple self-attention in parallel to achieve more effective learning at different scales. Recently, the Transformer architecture has extended its application from NLP to vision tasks, especially as ViT [8] has achieved the state-of-theart results for image classification with an architecture that closely resembles the initial NLP version. \n\nThe other examples of using the Transformer as a viable alternative to CNNs include object detection [3,7,14,28], video processing [24,26], image enhancement [4,22], etc. Due to the enormous computational and memory requirements of the transformer models, there has been increasing interest in transformer optimizations along several directions. \n\nThe regular transformer scales quadratically with the number of tokens N in the input sequence, which limits its usage in settings with limited resources. Several works [19,12,6,11] have proposed techniques to reduce the complexity from O(N 2 ) to as low as O(N ), yielding linear transformers. However, these techniques only outperform the regular method for very long sequences. \n\nAnother direction focuses on the sparsity in transformers. [5] exploits a factorized sparse representation of attention. [18] leverages token and head sparsities to reduce the attention computation and memory access for NLP tasks. For ViT, [27] is an early example that prunes channels according to the learnable importance scores. In our work, the reduction of the transformations in self-attention can be viewed as a fixed layer pruning scheme, which is unexplored by any of these works. \n\nEvolving the Transformer architecture can also significantly improve performance. For ViTs, the evolutions can be categorized by how the key components are implemented, including positional encoding [9,21], token embedding [23], projection for attention [20], and model hierarchy [20,21,9]. Selecting deployment-friendly operators for non-linearity and normalization can also greatly improve inference latency.",
            "score": 0.6094967368250803,
            "section_title": "Related Work",
            "char_start_offset": 2672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1325
                },
                {
                    "start": 1328,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1817
                },
                {
                    "start": 1820,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2110
                },
                {
                    "start": 2111,
                    "end": 2230
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 184,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1796875
        },
        {
            "corpus_id": "262826014",
            "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
            "text": "Two challenges with existing parallelism approach come to the fore. First, existing parallelism approach such as data, tensor and pipeline parallelism cannot address the scaling along sequence dimension. Second, existing sequence parallelism approaches are not effective because of memory-communication inefficiencies. Furthermore, existing approaches have limited usability requiring intrusive and error prone code refactoring. \n\nIn this paper, we introduce DeepSpeed-Ulysses (or Ulysses, a very long novel), a simple, portable, and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence lengths. DeepSpeed-Ulysses partitions individual samples along the sequence dimension among participating GPUs. Then right before the attention computation, it employs all-to-all communication collective on the partitioned queries, keys and values such that each GPU receives the full sequence but only for a non-overlapping subset of the attention heads. This allows the participating GPUs to compute attention for different attention heads in parallel. Finally, DeepSpeed-Ulysses employs another all-to-all to gather the results along the attention heads while re-partitioning along the sequence dimension. \n\nIn this work, we put forward the following contributions of DeepSpeed-Ulysses to advance state of the art in long sequence parallelism: \n\n\u2022 DeepSpeed-Ulysses trains Transformer models 4x larger sequence lengths than existing systems, while enabling training with sequences with over a million tokens. \n\n\u2022 Communication reduction of over 10x compared to existing systems, resulting in throughput improvements of up to 2.5x, and sustained throughput of over 175 TFlops/GPU (over 54% of hardware peak). \n\n\u2022 Fully general and implementation agnostic attention: DeepSpeed sequence parallelism (Ulysses) supports dense as well as sparse attention, and it works with efficient attention implementations such as FlashAttention v2 [Dao, 2023]. \n\n\u2022 Support for massive model training: DeepSpeed sequence parallelism works together with ZeRO-3 to not only support large sequence lengths but also massive model sizes. \n\n\u2022 Easy-to-use and portable, requiring minimal code changes to the existing training frameworks.",
            "score": 0.6090932191491494,
            "section_title": "Introduction",
            "char_start_offset": 2188,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1387
                },
                {
                    "start": 1390,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1751
                },
                {
                    "start": 1754,
                    "end": 1986
                },
                {
                    "start": 1989,
                    "end": 2157
                },
                {
                    "start": 2160,
                    "end": 2255
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83935546875
        },
        {
            "corpus_id": "259063695",
            "title": "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention",
            "text": "State-of-the-art sequence models have very high computational requirements. As a consequence, a lot of effort has been invested into developing methods to reduce the memory footprint in Transformers. Many efficient Transformer variants have been developed, with the main goal of taming the quadratic complexity of the attention mechanism (Tay et al., 2020). Several methods rely on kernelized attention (Katharopoulos et al., 2020;Choromanski et al., 2020), while others endow the Transformer with some auxiliary memory to increase the context (Wu et al., 2022;Borgeaud et al., 2021). \n\nIn many cases, leveraging sparsity in the attention matrix has proven useful. The Sparse Transformer (Child et al., 2019) works with a factorized sparse representation of the attention. They employ several sparse attention patterns, where each output position only computes weightings from a subset of input positions. \n\nThe Reformer (Kitaev et al., 2020) uses locality-sensitive-hashing (LSH) to sparsify the attention matrix and allow queries to restrict their context window to keys that collide with the same hash. However, to allow GPU-efficient processing, complex machinery has to be developed where the queries and keys are split into fixed-sized chunks, with the attention being applied only within the chunk and the immediate neighbor. K Q QK-sparse Hash-sparse Figure 1: Proposed sparsification of the attention matrix for a given attention head. In each depicted attention matrix, black areas indicate coefficients to compute, patterned areas those forced to zero due to the causal masking, and white areas coefficients that are ignored. We consider two main dynamic strategies to sparsify the left attention matrix. The QK-sparse attention consists of dropping some keys and queries (top, the discarded keys and queries are indicated in red), and the Hash-sparse attention computes a hash code for each key and each query, and restricts the attention matrix to blocks of keys and queries of same hash code (bottom, the three hash values are indicated for each key or query with the colors blue/green/red).",
            "score": 0.6088404758659708,
            "section_title": "Related work",
            "char_start_offset": 1868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 905
                },
                {
                    "start": 908,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 2105
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 431,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 921,
                    "end": 942,
                    "matchedPaperCorpusId": "209315300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1431884765625
        },
        {
            "corpus_id": "265552007",
            "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey",
            "text": "Recently, the Transformer family [275] has been the dominant architecture for language modeling, owing to its strong parallelism over recurrent methods such as RNNs [189]. However, its substantial computational cost renders the overall architecture inefficient in processing and handling long inputs. In particular, one of the key operations in the Transformer architecture is the attention mechanism. It typically requires quadratic complexity with respect to sequence length for computation and is thus significantly slow when processing long text inputs [100]. Reducing the computation required by the attention operation [267] becomes a direct solution to improve the architecture's efficiency, benefiting both the training and inference stages. Toward this end, researchers are exploring solutions for more efficient attention [50,58,60] along with different types of positional encoding [47,48,129,162,206,212,229,256], or leveraging the inherent sparsity within the model to avoid activating all parameters during the feedforward computation with sparse modeling [72,248]. Additionally, some recent works have directly replaced the attention mechanism with alternative architectures, introducing attention-free methods [61,204,210,260] to the fold. In this section, we cover these four primary directions with the latest progress.",
            "score": 0.6077163918800118,
            "section_title": "Introduction",
            "char_start_offset": 32131,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1337
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 38,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 165,
                    "end": 170,
                    "matchedPaperCorpusId": "17048224"
                },
                {
                    "start": 557,
                    "end": 562,
                    "matchedPaperCorpusId": "253384631"
                },
                {
                    "start": 625,
                    "end": 630,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 839,
                    "end": 842,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "248965309"
                },
                {
                    "start": 897,
                    "end": 900,
                    "matchedPaperCorpusId": "258865166"
                },
                {
                    "start": 900,
                    "end": 904,
                    "matchedPaperCorpusId": "258987259"
                },
                {
                    "start": 912,
                    "end": 916,
                    "matchedPaperCorpusId": "237347130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2496337890625
        },
        {
            "corpus_id": "239016160",
            "title": "Energon: Toward Efficient Acceleration of Transformers Using Dynamic Sparse Attention",
            "text": "B ENEFITING from the powerful attention mechanism, transformer models such as Seq2seq [65], BERT [12], GPT-2 [47], XLNet [70], T5 [48] and other variants [28], [29], [31] have achieved leading-edge performance on various NLP tasks, such as question-answering, text classification and machine translation, etc. Besides, transformers also show promising performance on many computer vision tasks including image classification [13], [64], object detection [4], [78] and even video comprehension [54], [61]. In brief, transformers are becoming the de facto substitutes for traditional RNNs and CNNs in a broad range of scenarios. \n\nDespite their effectiveness, it is still challenging to deploy transformers on resource-constrained devices. The greatest difficulty comes from the attention operations, which involve complicated data movement [15], [68] and bear quadratic computational complexity concerning the input sequence length [3], [25], [46], [52]. Therefore, for many tasks having long input sequences like question-answering and image-classification, it is always difficult to realize low-latency inference. For Z.Zhou, J. Liu   instance, we have profiled the execution time of the BERTbase model [12] on two popular edge-computing platforms. As shown in Figure 1, the attention operations take up about half of the total execution time as the sequence length grows to 512. When the input length reaches 768, attention operations even dominate the total execution (70% on average). Note that the attention overhead is becoming more prominent as weightpruning and quantization methods have significantly reduced the complexity of linear layers in transformers [55], [71], [72], [74]. Specialized NN accelerators have also been proposed to handle the non-attention parts [6], [16], [71]. Therefore, how to process attention operations efficiently is the key problem of transformers acceleration. Some recent works propose to co-design attention algorithms and accelerator architectures to mitigate the attention overhead. They mainly exploit the sparsity of attention to speed up the execution.",
            "score": 0.6060768584691656,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 626
                },
                {
                    "start": 629,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2099
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 90,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 109,
                    "end": 113,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 121,
                    "end": 125,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 130,
                    "end": 134,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 160,
                    "end": 164,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 493,
                    "end": 497,
                    "matchedPaperCorpusId": "207914263"
                },
                {
                    "start": 499,
                    "end": 503,
                    "matchedPaperCorpusId": "102483628"
                },
                {
                    "start": 839,
                    "end": 843,
                    "matchedPaperCorpusId": "211296403"
                },
                {
                    "start": 931,
                    "end": 934,
                    "matchedPaperCorpusId": "215737171"
                },
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 942,
                    "end": 946,
                    "matchedPaperCorpusId": "207847640"
                },
                {
                    "start": 948,
                    "end": 952,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1666,
                    "end": 1670,
                    "matchedPaperCorpusId": "202565587"
                },
                {
                    "start": 1672,
                    "end": 1676,
                    "matchedPaperCorpusId": "218571099"
                },
                {
                    "start": 1684,
                    "end": 1688,
                    "matchedPaperCorpusId": "221970445"
                },
                {
                    "start": 1776,
                    "end": 1779,
                    "matchedPaperCorpusId": "207209696"
                },
                {
                    "start": 1781,
                    "end": 1785,
                    "matchedPaperCorpusId": "1663491"
                },
                {
                    "start": 1787,
                    "end": 1791,
                    "matchedPaperCorpusId": "218571099"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1474609375
        },
        {
            "corpus_id": "269362649",
            "title": "Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges",
            "text": "Attention-based transformers have revolutionized natural language processing and other sequence-tosequence tasks.However, they encounter certain limitations, especially when dealing with long input sequences, especially when dependencies extend beyond the attention window size.This constraint is particularly crucial in applications such as high-resolution imagery analysis and genomics.Efforts to address these limitations have been surveyed by Efficient 360 [116], focusing on optimizing and improving efficiency in terms of computational complexity.Various aspects of transformers, including spectral analysis, fairness considerations, approximation methods, robustness enhancements, and computational complexity optimizations, have been discussed.In this report, we discuss these limitations and explore state space models (SSMs) as an alternative approach.\n\n\u2022 Computational Complexity: [138,150] Transformers exhibit high computational demands, particularly with large models.This complexity poses challenges for both training and deploying them on resource-constrained devices.\n\n\u2022 Large Memory Requirements: [138,150] Transformers necessitate significant memory resources for storing embeddings and intermediate activations.This can hinder scalability, especially for very long sequences, as they may surpass available memory capacity.\n\n\u2022 Fixed Sequence Length: [138,150] Transformers rely on fixed-size input sequences due to positional embeddings.Efficiently handling variable-length inputs presents a notable challenge in transformer-based architectures.",
            "score": 0.605861192280708,
            "section_title": "Recent Advances in State Space Models",
            "char_start_offset": 12582,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 113,
                    "end": 278
                },
                {
                    "start": 278,
                    "end": 388
                },
                {
                    "start": 388,
                    "end": 553
                },
                {
                    "start": 553,
                    "end": 752
                },
                {
                    "start": 752,
                    "end": 862
                },
                {
                    "start": 864,
                    "end": 982
                },
                {
                    "start": 982,
                    "end": 1084
                },
                {
                    "start": 1086,
                    "end": 1231
                },
                {
                    "start": 1231,
                    "end": 1342
                },
                {
                    "start": 1344,
                    "end": 1456
                },
                {
                    "start": 1456,
                    "end": 1564
                }
            ],
            "ref_mentions": [
                {
                    "start": 892,
                    "end": 897,
                    "matchedPaperCorpusId": "259075636"
                },
                {
                    "start": 1115,
                    "end": 1120,
                    "matchedPaperCorpusId": "259075636"
                },
                {
                    "start": 1369,
                    "end": 1374,
                    "matchedPaperCorpusId": "259075636"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0843505859375
        },
        {
            "corpus_id": "266110855",
            "title": "LongT5-Mulla: LongT5 With Multi-Level Local Attention for a Longer Sequence",
            "text": "Transformer [1] models have achieved dominant results in numerous text-oriented tasks and have become the foundational architecture in natural language processing (NLP). However, standard Transformer models [1], [2], [3], [4], [5] with full attention cannot efficiently handle long texts due to their quadratic complexity in relation to the input length, and consequently cause the maximum input length of 1\u223c4k for lowering computational consumption. To relieve the limitation, considerable efforts of research called efficient Transformers [6] have been done in past few years, which \n\nThe associate editor coordinating the review of this manuscript and approving it for publication was Okyay Kaynak . \n\nincluding transformers with sparse attention and transformers with hierarchical or recurrent architectures. \n\nAs the main category of efficient Transformers, Transformers with sparse attention have a key feature that their attention matrices are sparse, which means that only a small number of selected tokens, instead of all tokens in the input sequence, are attended to during the attention computation. According to the different ways of sparsification, it is called local attention [7] if we select neighbor tokens of the query token as the attended tokens, and it is called global attention [8] if we select shared special tokens as them. With the help of local and global attention alone or jointly, efficient Transformer models such as LED [7], Bigbird [9], LongT5 [10] have achieved state-of-the-art results in long text understanding and generative NLP tasks with relatively low resource consumption, and have extended the length limit from 4k to 16k. But unfortunately, on one hand, models with global attention fail to process longer documents in mainstream hardware conditions due to their essentially quadratic computational complexity, which results in a lack of efficiency. And on the other hand, other models with only local attention can process much longer documents but cannot effectively capture long-range dependencies, which results in reduced accuracy. \n\nAs another category, transformers with hierarchical or recurrent architectures [11], [12], [13], [14], [15] are based on the concept of divide-and-conquer.",
            "score": 0.6050655658811784,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2080
                },
                {
                    "start": 2083,
                    "end": 2238
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 217,
                    "end": 220,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 222,
                    "end": 225,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 541,
                    "end": 544,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1301,
                    "end": 1304,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 1465,
                    "end": 1468,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1477,
                    "end": 1481,
                    "matchedPaperCorpusId": "245144820"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251953125
        },
        {
            "corpus_id": "267499866",
            "title": "CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers",
            "text": "The Transformer architecture (Vaswani et al., 2017) has revolutionized many fields within machine learning such as translation (Vaswani et al., 2017), summarization (Miller, 2019), text generation (Chen et al., 2019), sentiment classification (Sun et al., 2019), and also tasks like image classification (Dosovitskiy et al., 2020), object detection (Liu et al., 2021b), and protein folding (Jumper et al., 2021). The self-attention mechanism stands at the core of its strengths. It allows the Transformer to directly model long-range dependencies within a sequence without the need for a hidden state like in recurrent neural networks (Hochreiter & Schmidhuber, 1997). However, the self-attention mechanism has an inherent large memory cost, since its complexity grows quadratically with the input sequence length. With these memory requirements and the ever-increasing size of large language models, such as the GPT series (Brown et al., 2020;OpenAI, 2023) and LLaMA (Touvron et al., 2023), a need for more efficient attention mechanisms has emerged (Dao et al., 2022). Current implementations of more efficient self-attention mechanisms can be roughly grouped into the following categories: (1) apply self-attention on subsets of the input sequences(sparsification) (Ainslie et al., 2020;Daras et al., 2020;Kitaev et al., 2020;Ma et al., 2023;Tay et al., 2020b;Zaheer et al., 2021), (2) approximate the self-attention mechanism with a lower complexity (Choromanski et al., 2020;Liu et al., 2021a;Wang et al., 2020), and (3) remove self-attention in favor of a lower complexity similar operation (Gu et al., 2022;Lee-Thorp et al., 2021;Smith et al., 2023;Tolstikhin et al., 2021).",
            "score": 0.6047001757099376,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1681
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 411,
                    "matchedPaperCorpusId": "235959867"
                },
                {
                    "start": 635,
                    "end": 667,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 924,
                    "end": 944,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08648681640625
        },
        {
            "corpus_id": "254854555",
            "title": "Inductive Attention for Video Action Anticipation",
            "text": "[61] proposed a memory compression module for the transformer to model long-range dependencies efficiently and has nearly linear scaling complexity, providing outstanding performance in various video tasks. \n\nTransformer has shown capability in general modeling for sequential inputs [31,30]. In line with these findings, we deploy attention in our model as a flexible mechanism for distilling temporal context. Our work is also related to the recent progress of combining sequence modeling and attention [28,29,7]. These approaches integrate recurrent with self-attention to overcome the quadratic computation complexity of transformers and to learn better representations for recurrent propagation. However, these methods have not yet been extensively evaluated on large datasets nor tested in practical application settings.",
            "score": 0.6045581629648724,
            "section_title": "Related Work",
            "char_start_offset": 6924,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 209,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 827
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 288,
                    "matchedPaperCorpusId": "232110866"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "259298549"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.125732421875
        },
        {
            "corpus_id": "262826014",
            "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
            "text": "Computation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input data along the sequence dimension and employs an efficient all-to-all collective communication for attention computation. Theoretical communication analysis shows that whereas other methods incur communication overhead as sequence length increases, DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally. Furthermore, experimental evaluations show that DeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than the existing method SOTA baseline.",
            "score": 0.6040364653326177,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "264306349",
            "title": "Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer",
            "text": "Pre-trained transformer models have manifested superior performance in various natural language processing tasks such as natural language modeling (NLM) (Dai et al., 2019;Radford et al., 2019), natural language generation (NLG) (Brown et al., 2020) and natural language understanding (NLU) (Devlin et al., 2019;Liu et al., 2019;He et al., 2021b). \n\nThese models leverage the attention mechanism (Vaswani et al., 2017) to compute the dependency score for each pair of tokens in an input sequence. \n\nSome practical tasks require these transformer models to handle long-sequence inputs like 8k tokens. For example, chatbot systems gather longterm contexts of user interactions to generate informative texts (Roller et al., 2021). Summarization for news, government reports, and academic papers request models to take inputs of long sequences to generate comprehensive summaries (Shaham et al., 2022), otherwise models often miss important information. Note that typical transformer models apply full attention to capture token dependencies pair-wise. It leads to a quadratic time and space complexity w.r.t. input length. However, such a complexity is prohibitive for long sequences. In particular, it incurs massive memory consumption during the back propagation. For example, a transformer model with 250M parameters consumes over 80G GPU memory when sequence length is 8k (Zuo et al., 2022). \n\nTo address this scalability issue, various approaches have been proposed to reduce the complexity. One approach is sparse attention, which restricts each token to attend a subset of tokens based on predefined sparsity patterns (Beltagy et al., 2020;Zaheer et al., 2020;Ainslie et al., 2020). For instance, block sparse attention (Kitaev et al., 2020;Ma et al., 2023) divides the input sequence into several blocks, and only intra-block attention is performed. Besides, sliding-window attention (Beltagy et al., 2020;Zaheer et al., 2020;Ainslie et al., 2020) allows each token to attend to its neighboring tokens within a sliding window.",
            "score": 0.6040230356769811,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 495
                },
                {
                    "start": 498,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2030
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 192,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 290,
                    "end": 311,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 328,
                    "end": 345,
                    "matchedPaperCorpusId": "219531210"
                },
                {
                    "start": 395,
                    "end": 417,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 704,
                    "end": 725,
                    "matchedPaperCorpusId": "216562425"
                },
                {
                    "start": 1643,
                    "end": 1663,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1723,
                    "end": 1744,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1744,
                    "end": 1760,
                    "matchedPaperCorpusId": "252439127"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.153564453125
        },
        {
            "corpus_id": "268875936",
            "title": "Linear Attention Sequence Parallelism",
            "text": "Linear sequence modeling methods (Katharopoulos et al., 2020;Choromanski et al., 2022;Sun et al., 2025a) including linear attention (Qin et al., 2024d), state space models (Dao & Gu, 2024) and linear RNN (Qin et al., 2024e), are becoming increasingly popular due to their faster training and inference speed and comparable modeling performance to vanilla Softmax attention-based transformer models (Vaswani et al., 2017;Zeng et al., 2022;Touvron et al., 2023a;b;Team, 2023). The hybrid architecture, which interleaves Softmax attention and linear attention Transformer layers, has proven to be an effective balance between their respective strengths. This approach has been successfully implemented in large-scale commercial models such as Minimax-01 (Li et al., 2025) and Tencent Hunyuan Turbo-S (Tencent, 2025), as well as in smaller-scale hybrid models like Samba (Ren et al., 2024), Jamba (Lieber et al., 2024). \n\nAs the size of large language models (LLMs) increases and sequence lengths extend, the capacity limitations of single GPU's memory become a significant challenge, constraining the maximum sequence length manageable by a large model. To address this, Sequence Parallelism (SP) techniques (Li et al., 2022;Korthikanti et al., 2022) are employed, which partition a long sequence into multiple sub-sequences to be processed on separate devices. However, current implementations of SP methods do not fully exploit the right-product advantages of linear-complexity attention mechanisms Qin et al. (2024b). This results in less than optimal parallelism efficiency and reduced usability on linear sequence modeling methods. \n\nIn this paper, we present Linear Attention Sequence Parallelism (LASP) approach for efficient SP on models with linear sequence modeling. Our approach takes linear attention (Katharopoulos et al., 2020) as an instance to design a sophisticated point-to-point (P2P) ring-style communication mechanism during both forward and backward among devices within a node or across multiple nodes.",
            "score": 0.6016893976547616,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1633
                },
                {
                    "start": 1636,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 61,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 86,
                    "end": 104,
                    "matchedPaperCorpusId": "267312207"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71875
        },
        {
            "corpus_id": "263671659",
            "title": "DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training",
            "text": "In this work, we introduce DISTFLASHATTN, a distributed memory-efficient attention prototype for long-context transformer training based on sequence parallelism. DISTFLASHATTN presents novel system optimizations including load balancing for causal language modelings, overlapped communication with computation in the distributed attention computation, and a re-materialization-aware checkpointing strategy. Experiments evaluate multiple families of transformer models and on different cluster types, and over four strong distributed system baselines. In particular, DISTFLASHATTN has demonstrated up to 2.01\u00d7 speedup and scales up to 8x longer sequences, compared to the popular system, Megatron-LM with FlashAttention.",
            "score": 0.6007884574508708,
            "section_title": "Conclusion",
            "char_start_offset": 25673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 719
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64111328125
        },
        {
            "corpus_id": "267406617",
            "title": "Repeat After Me: Transformers are Better than State Space Models at Copying",
            "text": "Transformers (Vaswani et al., 2017) are the workhorse of modern sequence modeling, achieving remarkable performance on a variety of tasks, but they have unavoidable inefficiencies.Specifically, they require \u2126(L) memory1 and Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024.Copyright 2024 by the author(s). 1In some naive implementations of transformers, it is common to allocate a L \u00d7 L matrix to compute the attention.However, compute to predict the next token of a sequence of length L.\n\nThis has spurred a boom in attempts to create architectures that can achieve similar performance as transformers, but with O(1) memory to predict each token.This class of models includes state space models like S4 (Gu et al., 2021) or Mamba (Gu & Dao, 2023), as well as traditional RNN models (Hochreiter & Schmidhuber, 1997) and models that can be trained in parallel like linear attention (Katharopoulos et al., 2020;Choromanski et al., 2020) and parallel RNNs (Bradbury et al., 2016;Peng et al., 2023;Sun et al., 2023).In this paper, we will refer to this entire class of models that use a fixed-size memory as \"generalized state space models\" or GSSMs (see a formal definition in Section 2).\n\nRecent work has demonstrated impressive performance of GSSMs, but it is not yet clear what these models sacrifice for their improved efficiency, if anything.In this paper, we find that one particular capability that is sacrificed is the ability to retrieve and repeat parts of the input context.As a result, transformers are better than GSSMs at a variety of tasks that require accessing arbitrary parts of the context.\n\nTo understand this gap in capabilities, we begin by presenting a theoretical analysis of the copying task2 .First, we show via construction that a simple transformer model can copy strings of length that is exponential in the number of heads of the transformer.",
            "score": 0.6006767999794691,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 327
                },
                {
                    "start": 327,
                    "end": 361
                },
                {
                    "start": 361,
                    "end": 473
                },
                {
                    "start": 473,
                    "end": 542
                },
                {
                    "start": 544,
                    "end": 701
                },
                {
                    "start": 701,
                    "end": 1066
                },
                {
                    "start": 1066,
                    "end": 1239
                },
                {
                    "start": 1241,
                    "end": 1398
                },
                {
                    "start": 1398,
                    "end": 1536
                },
                {
                    "start": 1536,
                    "end": 1660
                },
                {
                    "start": 1662,
                    "end": 1770
                },
                {
                    "start": 1770,
                    "end": 1923
                }
            ],
            "ref_mentions": [
                {
                    "start": 837,
                    "end": 869,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 935,
                    "end": 963,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09503173828125
        },
        {
            "corpus_id": "266999285",
            "title": "Extending LLMs' Context Window with 100 Samples",
            "text": "The vanilla attention mechanism in the Transformer architecture is known for its quadratic time and space complexity, which poses significant resource demands for transformer models when processing lengthy inputs. Various works have focused on conquering the complexity issue and proposing more efficient Transformers. Sparse transformers (Child et al., 2019;Ye et al., 2019;Kitaev et al., 2020;Beltagy et al., 2020;Ainslie et al., 2020;Zaheer et al., 2020;Ding et al., 2023) replace the original full attention mechanism with a sparsified version to make the computation more efficient. Linear transformers (Wang et al., 2020;Katharopoulos et al., 2020;Choromanski et al., 2020), rather than forcing the attention mechanism to attend to fewer tokens, propose an alternative approach by leveraging low-rank matrix multiplication or linear dot-product of kernel feature maps to approximate the original attention mechanism, achieving linear time complexity. Meanwhile, retrieval-augmented models (Guu et al., 2020;Lewis et al., 2020;Wu et al., 2022;Bulatov et al., 2023;Tworkowski et al., 2023) integrate retrieval with attention. During inference time, these models avoid directly modeling lengthy inputs by retrieving information from an external memory that stores previous key-value pairs. While prior research primarily focuses on reducing FLOPs, the bottleneck of transformer inference on modern computing hardware has shifted to the overhead from memory access (IO). Multi-query attention (MQA) (Shazeer, 2019) and grouped-query attention (GQA) (Ainslie et al., 2023), for instance, address the memory-bandwidth cost associated with loading the large \"keys\" and \"values\" tensors in the multi-head attention mechanism by proposing the use of fewer \"key\" and \"value\" heads. Notably, GQA is employed in LLaMA2 (Touvron et al., 2023b). Additionally, FlashAttention (Dao et al., 2022;Dao, 2023) introduces an IO-aware exact attention approach that utilizes tiling to reduce memory IOs.",
            "score": 0.6004137060098879,
            "section_title": "More Efficient Transformers",
            "char_start_offset": 24824,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1986
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 375,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 437,
                    "end": 457,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 995,
                    "end": 1013,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1013,
                    "end": 1032,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1867,
                    "end": 1885,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.136474609375
        },
        {
            "corpus_id": "221702858",
            "title": "Efficient Transformers: A Survey",
            "text": "\u2022 Downsampling -Another popular method of reducing computation cost is to reduce the resolution of the sequence, hence reducing computation costs by a commensurate factor. Examples of this class of models include Perceiver (Jaegle et al., 2021), Funnel Transformers (Dai et al., 2020), Swin Transformer (Liu et al., 2021b), and Charformer (Tay et al., 2021c) models. Notably, there might also be some form of overlap of this class of models with models that leverage memory tokens as models such as Set Transformer can also be viewed as a form of downsampling, albeit within the attention mechanism. The recent Nystr\u00f6mformer (Xiong et al., 2021b), on the surface, may seem like a low-rank or kernal-based approach. However, it is actually a downsampling approach where the 'landmarks' are simply strided based pooling -in similar spirit to Set Transformer, Funnel Transformer or Perceiever. \n\n\u2022 Sparse Models and Conditional Computation -While not targeted specifically at the attention modules, sparse models sparsely activate a subset of the parameters which generally improves the parameter to FLOPs ratio. Examples of this class of model includes Switch Transformers (Fedus et al., 2021), ST-MoE (Zoph et al., 2022), GShard (Lepikhin et al., 2020), Product-Key Memory Layers (Lample et al., 2019). Within the scope of our studied models, sparse models typically operate on an adaptive basis in which the sparsity is typically learned (via mixture-of-experts like mechanism). \n\nWithin this context, we can also consider sparsification of attention weights to fall under this paradigm. For this reason, we believe there is a close connection to fixed or learned patterns in attention. However, we believe that the emergence of an entire research direction (Roller et al., 2021;Lewis et al., 2021;Lepikhin et al., 2020;Du et al., 2021) based on sparse efficient should warrant a new category of efficient Transformers. \n\nWe note that these buckets are a broad characterization of the different efficient Transformer models.",
            "score": 0.5998997379773399,
            "section_title": "A Taxonomy of Efficient Transformers",
            "char_start_offset": 22542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1919
                },
                {
                    "start": 1922,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 284,
                    "matchedPaperCorpusId": "219401850"
                },
                {
                    "start": 339,
                    "end": 358,
                    "matchedPaperCorpusId": "235624202"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0548095703125
        },
        {
            "corpus_id": "258888224",
            "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers",
            "text": "The first replaces the attention mechanism with an alternative operation that features more favorable scaling with the input sequence length [Peng et al., 2021, Katharopoulos et al., 2020, Choromanski et al., 2020a, Schlag et al., 2021]. While several recent methods in this category show promise, none have emerged as a definitive winner, and most state-of-the-art language models still rely on the standard attention mechanism [Touvron et al., 2023, Chowdhery et al., 2022]. The second approach proposed to compress the length of the input context, controlling the complexity of the attention operation but unavoidably sacrificing potentially relevant information from the original input [Lee et al., 2019, Wang et al., 2020, Jaegle et al., 2021]. The third approach involves pruning the attention matrix, preventing each token from attending to every other token within the context [Zaheer et al., 2020, Martins et al., 2020, Lee et al., 2023]. This line of research is motivated by the theoretical finding highlighting that sparse Transformers retain the expressivity of their dense counterparts [Yun et al., 2020]. Many methods in this category employ specially designed attention masks that aim to zero out as many entries as possible, often based on principles of locality, randomness, or a combination of both. The main drawback of these methods is their mostly static nature, meaning that every token is compelled to attend to a fixed context window and disregard the rest of the context regardless of its specific role within the input sequence. Our approach falls within this last category, and enables dynamic sparsification of the attention matrix for decoder models, without resorting to any potentially restricting inductive biases about its structure. \n\nImplementation Speed-up Recently, hardware-optimized implementations [Dao et al., 2022, Touvron et al., 2023] have been proposed with the aim of optimizing computational resources during the training phase of Transformers [Hoffmann et al., 2022].",
            "score": 0.5996178052331794,
            "section_title": "Related Work",
            "char_start_offset": 7302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1767
                },
                {
                    "start": 1770,
                    "end": 2016
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 187,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 885,
                    "end": 905,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 905,
                    "end": 927,
                    "matchedPaperCorpusId": "219636190"
                },
                {
                    "start": 1100,
                    "end": 1118,
                    "matchedPaperCorpusId": "219558319"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05194091796875
        },
        {
            "corpus_id": "258841564",
            "title": "Stateful Memory-Augmented Transformers for Efficient Dialogue Modeling",
            "text": "For long documents processing, sparse Transformers are another direction. The main idea is to apply a sparse attention matrix to skip computations of tokens that are far away. Many works (Child et al., 2019;Zaheer et al., 2020;Beltagy et al., 2020) have explored different sparse attention patterns with linear complexity. Especially, Longformer extended the pre-trained BART (Lewis et al., 2020) with sparse attention and introduced Longformer-Encoder-Decoder (LED) for sequence-to-sequence tasks. However, these models are stateless, which are inefficient for dialogue modeling. They require the context to be long enough to cover enough history information. The context also needs to be re-computed at every timestep due to bidirectional attention. Besides, sparse Transformers need full attention for the local window, which makes them less competitive against non-sparse models when the context is short. In contrast, our stateful memory-augmented method can have a shorter context input while still memorizing the history information.",
            "score": 0.5995247468833376,
            "section_title": "Stateless Long-Document Models",
            "char_start_offset": 6588,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1040
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 227,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 376,
                    "end": 396,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1324462890625
        },
        {
            "corpus_id": "266999544",
            "title": "The What, Why, and How of Context Length Extension Techniques in Large Language Models - A Detailed Survey",
            "text": "Transformers designed for extended sequences address dual challenges: inadequate efficiency in processing or memory utilization for prolonged sequences and an intrinsic balance between efficacy and resource usage. Techniques such as linear attention (Wang et al., 2020;Katharopoulos et al., 2020;Choromanski et al., 2020), utilizing kernel-based or low-dimensional approximations, prioritize resource efficiency but often exhibit sub-optimal performance in typicallength scenarios. Sparse attention (Child et al., 2019b;Beltagy et al., 2020;Zaheer et al., 2020;Xiong et al., 2021), leveraging structured sparsity, provides a computational reduction strategy. Additionally, designs employing recurrent-style architectures (Dai et al., 2019b;Hutchins et al., 2022;Ma et al., 2022) for causal sequence modeling are contenders in handling these challenges. In this context, the emphasis is on addressing the extrapolation problem in language modeling-training on brief texts while evaluating extended texts (Press et al., 2021b). The training methodology aligns with conventional Transformers, encompassing training on brief sequences with concentrated attention computation. The advantage lies in seamlessly unlocking the potential for long-sequence modeling during inference without compromising training efficiency. This approach guarantees the retention of optimal performance for typical lengths, eliminating trade-offs associated with long-sequence modeling compared to earlier methodologies.",
            "score": 0.5975593523852292,
            "section_title": "Related work.",
            "char_start_offset": 46348,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1494
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 296,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 541,
                    "end": 561,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 740,
                    "end": 762,
                    "matchedPaperCorpusId": "247451135"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2149658203125
        },
        {
            "corpus_id": "273653873",
            "title": "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning",
            "text": "Efficient transformers have adopted various attention approximation including sparse and low-rank methods to extend the context length that can be processed by one attention layer. For example, sparse Transformer (Child et al., 2019), Longformer (Beltagy et al., 2020), Bigbird (Zaheer et al., 2020) and Diffusers (Feng et al., 2022) adopt combinations of random, window, and global sparse attention targeting the long-sequence scenario. Dataadaptive sparsity has also been considered in recent works including Reformer (Kitaev et al., 2020) BiFormer (Zhu et al., 2023b), DAT (Xia et al., 2022), and SparseViT (Chen et al., 2023b). The proposed tensorization is compatible with such sparisty masks. Low-rank attention is based on the assumption that the attention matrix has an intrinsic low-rank structure. Linformer (Wang et al., 2020), Performer (Katharopoulos et al., 2020), Linear Transformer (Katharopoulos et al., 2020), Synthesizer (Tay et al., 2021), andLRT (Winata et al., 2020) achieve efficient attention with linear complexity by projecting attention matrix to a lower dimension in the column vector space along the query or key dimension. However, these low-rank assumptions are mostly based on the vector space by directly reducing q,k,v sequence length, ignoring the hierarchical structure embedded in the attention matrix. Given the observed structure of attention matrices, we show that attention representations in the tensor space which is constructed by a set of hierarchically decomposed attention blocks can be better approximated with low-rank matrices.",
            "score": 0.5973581107467217,
            "section_title": "Context Window Extension",
            "char_start_offset": 4807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1577
                }
            ],
            "ref_mentions": [
                {
                    "start": 278,
                    "end": 299,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 576,
                    "end": 594,
                    "matchedPaperCorpusId": "245650206"
                },
                {
                    "start": 849,
                    "end": 877,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 898,
                    "end": 926,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 940,
                    "end": 963,
                    "matchedPaperCorpusId": "218487423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.120361328125
        },
        {
            "corpus_id": "238407884",
            "title": "ABC: Attention with Bounded-memory Control",
            "text": "Transformer architectures have achieved state- of-the-art results on a variety of natural language processing (NLP) tasks. However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token taking a slot. Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it. One way to improve the efficiency is to bound the memory size. We show that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and they vary in their organization of the memory. ABC reveals new, unexplored possibilities. First, it connects several efficient attention variants that would otherwise seem apart. Second, this abstraction gives new insights\u2014an established approach (Wang et al., 2020b) previously thought to not be applicable in causal attention, actually is. Last, we present a new instance of ABC, which draws inspiration from existing ABC approaches, but replaces their heuristic memory-organizing functions with a learned, contextualized one. Our experiments on language modeling, machine translation, and masked language model finetuning show that our approach outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.",
            "score": 0.596395726921326,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1114501953125
        },
        {
            "corpus_id": "270095115",
            "title": "Optimizing Foundation Model Inference on a Many-Tiny-Core Open-Source RISC-V Platform",
            "text": "In the last few years, the field of artificial intelligence (AI) has seen a paradigm shift towards transformer-based models such as Large Language Models (LLMs) [1], Vision Transformers (ViTs) [2], and, more in general, Foundation Models (FMs) [3].Transformer-based FMs are large-scale models based on the so-called attention mechanism [4], trained on enormous generic datasets in a self-supervised fashion.FMs can then be fine-tuned to perform specialized downstream tasks [5].The big breakthrough in FMs came with the Generative Pretrained Transformer (GPT), which exploited the transformer architecture for question-answering problems with natural language.After the immense success of GPT-3 [6], transformer-based FMs have then been adapted to all aforementioned domains, with models such as ViTs [2] for computer vision or Whispers [7] for Automatic Speech Recognition (ASR).\n\nThe broad adoption of FMs makes their efficient execution a high-priority target.However, the computational patterns at the core of attention layers, i.e., the fundamental building blocks of FMs, exhibit several key bottlenecks.For each input element, these layers compute an attention score with respect to every other input obtained through a Softmax-normalized dotproduct operation [4].Attention complexity scales quadratically with the input sequence length, limiting both the practical size of the models and the length of the inputs they can handle.Moreover, the internal Softmax is crucial for normalizing the attention scores to a pseudo-probabilistic distribution, but it introduces significant computational challenges.Namely, the computation of exponentials for each attention score prior to normalization can be expensive and susceptible to numerical stability issues, particularly under large-scale or low-precision arithmetic conditions.\n\nAt the algorithmic level, these bottlenecks can be addressed with innovations such as sparse and linearized attention mechanisms [8], [9], as well as approximations and alternatives to the Softmax function [10].All these approaches aim to mitigate the computational overhead and enhance the model's capacity to handle longer sequences more effectively, thereby extending the practical applications of FMs without compromising their performance.",
            "score": 0.5961782946355169,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 407
                },
                {
                    "start": 407,
                    "end": 478
                },
                {
                    "start": 478,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 880
                },
                {
                    "start": 882,
                    "end": 963
                },
                {
                    "start": 963,
                    "end": 1110
                },
                {
                    "start": 1110,
                    "end": 1271
                },
                {
                    "start": 1271,
                    "end": 1437
                },
                {
                    "start": 1437,
                    "end": 1611
                },
                {
                    "start": 1611,
                    "end": 1833
                },
                {
                    "start": 1835,
                    "end": 2046
                },
                {
                    "start": 2046,
                    "end": 2279
                }
            ],
            "ref_mentions": [
                {
                    "start": 336,
                    "end": 339,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 695,
                    "end": 698,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1267,
                    "end": 1270,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1969,
                    "end": 1972,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.116943359375
        },
        {
            "corpus_id": "237412971",
            "title": "\\infty-former: Infinite Memory Transformer",
            "text": "Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the \\infty-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the \\infty-former\u2019s attention complexity becomes independent of the context length, trading off memory length with precision.In order to control where precision is more important, \\infty-former maintains \u201csticky memories,\u201d being able to model arbitrarily long contexts while keeping the computation budget fixed.Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the \\infty-former\u2019s ability to retain information from long sequences.",
            "score": 0.5954577698955086,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07171630859375
        },
        {
            "corpus_id": "273323477",
            "title": "Extra Global Attention Designation Using Keyword Detection in Sparse Transformer Architectures",
            "text": "The transformer uses the concept of attention, which in loose terms can be thought of as similar to human attention; i.e., identifying parts of the input that are interesting and the connection between them. Early transformers, such as the original transformer (Vaswani et al., 2017), BERT (Kenton and Toutanova, 2019), and BART (Lewis et al., 2020) were severely limited by input lengths, often 512 or 1024 tokens, due to the O(n 2 ) time and space complexity required by the self-attention mechanism in the standard transformer. Several modifications to the transformer have been proposed to work around this, with one popular family of strategies using a sparse self-attention where many of the input pairs are selectively ignored. Some examples of this family include Big Bird (Zaheer et al., 2020), which uses a sliding window attention combined with randomly selected global attention; Longformer (Beltagy et al., 2020), which uses a sliding window attention and a dilated sliding window attention, along with some global attention tokens; and Smart Bird (Wu et al., 2021), which uses a low-dimension full attention transformer to determine which attentions to calculate in the full-depth transformer. A representation of these different sparsity patterns is depicted in Figure 2 for an original transformer, Longformer, Big Bird, and our modifications to Longformer. The diagrams are meant to be illustrative only and exact numbers of cells may not be proportional to the actual sparse attention matrices used in each case. \n\nOne area of concern with these sparse transformer methods, as highlighted by (Tay et al., 2020), is that they can lack long range context. The tasks laid out in (Tay et al., 2020) are varied and substantially different from summarization, but the concept is still similar; if relevant information is contained at a longer distance (or lags) in the input sequence than the sliding window, the model will not be able to compute the attention between those inputs. Other NLP specific tasks that require longer range context have also been proposed, such as LAMBADA, which tries to predict a word given context from sentences preceding the sentence missing the word.",
            "score": 0.5953582706161111,
            "section_title": "Introduction",
            "char_start_offset": 1768,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2195
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 283,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 329,
                    "end": 349,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 1610,
                    "end": 1628,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 1694,
                    "end": 1712,
                    "matchedPaperCorpusId": "260440449"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10089111328125
        },
        {
            "corpus_id": "274234686",
            "title": "A Method for Building Large Language Models with Predefined KV Cache Capacity",
            "text": "The Bounded-Cache Transformer (BCT) introduces several key implementation aspects that enhance its efficiency and effectiveness in sequence modeling: \n\n\u2022 Fixed-Length Cache: BCT utilizes a fixed-length key-value (KV) cache, which contrasts with the expanding cache in traditional Transformers. This design is essential for managing memory efficiency and ensuring stable performance across varying context lengths. \n\n\u2022 Adaptive Update Rule: BCT features an adaptive update rule that refines the hidden state through a self-supervised learning approach during inference. This enables the model to dynamically adjust to new sequences, improving its capacity to capture long-range dependencies within the bounded cache. \n\n\u2022 Optimized for Parallelism: The BCT is designed to take full advantage of parallel processing capabilities. Since the update of KV content at each time step is independent, it allows for token-level parallel training. This is achieved through the computation of all KV elements via concurrent traversal and accumulation operations, significantly enhancing training efficiency. \n\n\u2022 Learning Rate Adaptation: BCT incorporates an adaptive learning rate mechanism, which adjusts the step size for each token. This is vital for stabilizing the learning process and can lead to faster convergence. \n\n\u2022 Hardware Efficiency: The BCT's design is mindful of modern hardware constraints, aligning its operations with the strengths of GPUs and TPUs. This includes the use of efficient matrix operations and strategic memory management to ensure high-performance computing. \n\nThese aspects highlight BCT's innovative approach to sequence modeling, focusing on efficiency, adaptability, and hardware utilization, which are critical for advancing the state of the art in language model design.",
            "score": 0.5951494515310396,
            "section_title": "Key Implementation Aspects",
            "char_start_offset": 7958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 152,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1310
                },
                {
                    "start": 1313,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1579
                },
                {
                    "start": 1582,
                    "end": 1797
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.437255859375
        },
        {
            "corpus_id": "259360659",
            "title": "Scaling In-Context Demonstrations with Structured Attention",
            "text": "Therefore, a variety of efficient Transformer models have been proposed to mitigate this issue (Child et al., 2019;Beltagy et al., 2020;Wang et al., 2020;Kitaev et al., 2019;Katharopoulos et al., 2020;Zaheer et al., 2020;Qiu et al., 2020;Tay et al., 2020;Peng et al., 2021;Qin et al., 2021;Zhou et al., 2021). We refer the readers to Tay et al. (2022a) for a comprehensive survey. In this paper, we draw inspiration from the design of sparse attention mechanisms in efficient Transformers (Beltagy et al., 2020;Zaheer et al., 2020). However, instead of using sparse attention to approximate the ability of full attention to process any sequences, we exploit the inherent structure of in-context learning (test input follows a number of demonstrations), and design a structured attention mechanism tailored for the problem. \n\nComparison with concurrent works. Concurrent with our work, there are several works (Hao et al., 2022;Ratner et al., 2022;Ye et al., 2022a) exploring architectural designs to improve the in-context learning ability of LLMs. Among them, Hao et al. (2022); Ratner et al. (2022) focus on tackling the limitation of context length by independently processing several prompts within the max length constraint and then combining them. This idea is similar to our adaptation of FiD while Hao et al. (2022); Ratner et al. (2022) use fixed schemes (e.g., average) for fusing parallel prompts so that their methods can be used directly on top of pretrained models without further fine-tuning. Compared to their methods, we, in addition, seek to address the efficiency and instability issues while exploring better encoding to retain the dependencies between demonstrations. These goals cannot be directly achieved by processing several grouped prompts in parallel.",
            "score": 0.5949871477382138,
            "section_title": "Related Work",
            "char_start_offset": 8050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1779
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 174,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 174,
                    "end": 201,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 201,
                    "end": 221,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 221,
                    "end": 238,
                    "matchedPaperCorpusId": "207847640"
                },
                {
                    "start": 255,
                    "end": 273,
                    "matchedPaperCorpusId": "232105052"
                },
                {
                    "start": 273,
                    "end": 290,
                    "matchedPaperCorpusId": "246904340"
                },
                {
                    "start": 290,
                    "end": 308,
                    "matchedPaperCorpusId": "229156802"
                },
                {
                    "start": 334,
                    "end": 352,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 511,
                    "end": 531,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.190673828125
        },
        {
            "corpus_id": "272310408",
            "title": "HLogformer: A Hierarchical Transformer for Representing Log Data",
            "text": "The related work in this area can be categorized into 2 main groups: efficient transformers including incorporating global memory tokens, sparse attention mechanisms, segment-based recurrence methods, and hierarchical architectures. Each category offers distinct approaches to addressing the challenges of processing long sequences with transformers.",
            "score": 0.5947145698645686,
            "section_title": "Related Works",
            "char_start_offset": 5916,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 350
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1634521484375
        },
        {
            "corpus_id": "246430990",
            "title": "Fast Monte-Carlo Approximation of the Attention Mechanism",
            "text": "Attention mechanisms have become a predominant modeling paradigm for state-of-the-art deep neural networks. Among many, the Transformer architecture (Vaswani et al. 2017) is a major contribution that drove this success, which demonstrated unprecedented performance in various domains such as neural machine translation (Chen et al. 2018), question answering (Raffel et al. 2020), image classification (Parmar et al. 2019;Dosovitskiy et al. 2021), and timeseries forecasting (Lim et al. 2021).\n\nA typical Transformer can be thought as a stack of selfattention layers. Self-attention can be viewed as an inductive bias with graph-like dependencies among input tokens, where important elements hold stronger influence on the final output. On an algorithmic perspective, this is a three step process: (i) computing bidirectional attention scores between input tokens, (ii) element-wise encoding of input token sequences, and (iii) computing the weighted sum of encoded sequences with respect to attention scores.\n\nRecently, Transformer parameter sizes are ever-increasing with current trends of exploiting self-supervised pre-training using massive data. This has introduced significant computational cost when employing Transformers; thus, recent re-Copyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. search has moved on to designing more computationally efficient attention mechanisms.\n\nTo alleviate the computational cost of self-attention mechanisms, two mainstream approaches have been taken in previous work, where each optimize the process in different dimensions. First is to improve the quadratic time complexity of vanilla dot-product attentions relative to the input sequence length. Since computing attention scores for an input of n elements takes O(n 2 ) time and memory (typical attention being bidirectional), applying Transformers to long input sequences can be a challenge. A number of techniques have been proposed in this dimension of optimization, such as leveraging fixed or learnable patterns to sparsify the attention matrix (Child et al. 2019;Beltagy, Peters, and Cohan 2020) and approximating attention matrices with mathematical methods (Wang et al. 2020b;Choromanski et al. 2021).\n\nSecond, there have also been efforts to reduce cost by trim",
            "score": 0.594410157012156,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 169,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 358,
                    "end": 378,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 401,
                    "end": 421,
                    "matchedPaperCorpusId": "189897750"
                },
                {
                    "start": 421,
                    "end": 445,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 474,
                    "end": 491,
                    "matchedPaperCorpusId": "209414891"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0823974609375
        },
        {
            "corpus_id": "229923177",
            "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
            "text": "Transformers (Vaswani et al., 2017) have achieved remarkable improvements in a wide range of natural language tasks, including language modeling , text classification , and question answering (Devlin et al., 2018;. This success is largely due to the self-attention mechanism, which enables the network to capture contextual information from the *indicates equal contribution. 1 Source code and pre-trained checkpoints can be found at https://github.com/PaddlePaddle/ERNIE/ tree/repro/ernie-doc. entire input sequence. Nevertheless, the memory usage and computation complexity caused by the self-attention mechanism grows quadratically with the sequence length, incurring excessive cost when processing a long document on existing hardware. Currently, the most prominent pretrained models, such as BERT (Devlin et al., 2018), are used on fixed-length input segments of a maximum of 512 tokens owing to the aforementioned limitation. Thus, a long document input must be partitioned into smaller segments of manageable sizes. However, this leads to the loss of important crosssegment information, that is, the context fragmentation problem , as shown in Fig. 1(a). To mitigate the problem of insufficient interactions among the partitioned segments of long documents, Recurrence Transformers Rae et al., 2019) permit the use of contextual information from previous segments in computing the hidden states for a new segment by maintaining a memory component from the previous activation; this enables the modeling of long documents. In addition, Sparse Attention Transformers Tay et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020) focus on reducing the complexity of self-attention operations to explicitly improve the modeling length, but only up to a restricted context length (4,096) due to resource limitations.\n\nWe argue that existing strategies are not sufficiently effective or reliable, because the contextual information of a complete document is still not available for each segment during the training phase. As depicted in Fig. 1, when training on segment S 2 , the model is ideally optimized by maximizing P (y | (S 1 , S 2 , S 3 )) conditioned on the contextual information of the entire document D = {S 1 , S 2 , S 3 }, in contrast to the following sub",
            "score": 0.5939017690944386,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.095703125
        },
        {
            "corpus_id": "268384915",
            "title": "BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences",
            "text": "Transformer-based LLMs such as GPT (Brown et al., 2020;Ouyang et al., 2022), LLaMA (Touvron et al., 2023a;b), and PaLM (Chowdhery et al., 2022;Anil et al., 2023) have achieved great success (Han et al., 2021;Bommasani et al., 2021;Zhao et al., 2023).Despite the success of these LLMs, they still face efficiency challenges: one is that as these models continue to grow in size, the time and memory costs associated with training and inference have become bottlenecks.Another is that the quadratic attention computational complexity of the Transformer architecture makes these LLMs difficult to handle long sequences.Up to now, various parallelism strategies (Valiant, 1990;Huang et al., 2019;Rajbhandari et al., 2020;Narayanan et al., 2021) and memory optimization strategies (Ren et al., 2021;Chen et al., 2016;Korthikanti et al., 2023), have well solved the bottleneck caused by the model size growth, but it is still challenging to solve the efficiency issue caused by the sequence growth.\n\nTo enable LLMs to process longer sequences more efficiently, several attention solutions have been proposed.Korthikanti et al. (2023) adopt selective activation recomputation to avoid storing attention softmax logits during the forward pass, and then recompute these logits during the backward pass to build a computation graph for backpropagation, significantly reducing memory overheads of attention modules to process long sequences.Rabe & Staats (2021) formalize the computation of attention modules at the block level and make each thread block in devices handle the attention computation of a subsequence, further reducing temporary memory consumptions and achieving a logarithmic memory complexity relative to the sequence length.\n\nBased on these works, Dao et al. (2022) introduce FlashAttention, a CUDA implementation of attention modules that leverages the fast I/O capabilities of the SRAM in devices for further speedup.",
            "score": 0.5933586659739354,
            "section_title": "Related Work",
            "char_start_offset": 26625,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 250,
                    "end": 467
                },
                {
                    "start": 467,
                    "end": 616
                },
                {
                    "start": 616,
                    "end": 992
                },
                {
                    "start": 994,
                    "end": 1102
                },
                {
                    "start": 1102,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1731
                },
                {
                    "start": 1733,
                    "end": 1926
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 55,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 190,
                    "end": 208,
                    "matchedPaperCorpusId": "235421816"
                },
                {
                    "start": 658,
                    "end": 673,
                    "matchedPaperCorpusId": "15655597"
                },
                {
                    "start": 673,
                    "end": 692,
                    "matchedPaperCorpusId": "53670168"
                },
                {
                    "start": 692,
                    "end": 717,
                    "matchedPaperCorpusId": "269617042"
                },
                {
                    "start": 717,
                    "end": 740,
                    "matchedPaperCorpusId": "236635565"
                },
                {
                    "start": 776,
                    "end": 794,
                    "matchedPaperCorpusId": "231632857"
                },
                {
                    "start": 812,
                    "end": 837,
                    "matchedPaperCorpusId": "248693351"
                },
                {
                    "start": 1102,
                    "end": 1127,
                    "matchedPaperCorpusId": "248693351"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.247802734375
        },
        {
            "corpus_id": "268875948",
            "title": "How Sparse Attention Approximates Exact Attention? Your Attention is Naturally $n^C$-Sparse",
            "text": "Large Language Models (LLMs) [VSP + 17, RNS + 18, DCLT18, RWC + 19, BMR + 20, CND + 22, ZRG + 22, Cha22] have emerged as a cornerstone of contemporary artificial intelligence, exhibiting remarkable capabilities across a plethora of AI domains.Their prowess is grounded in their ability to comprehend and generate human language with a level of sophistication that is unprecedented.This has catalyzed transformative applications in natural language processing, including machine translation [HWL21], content creation [Cha22,Ope23], and beyond, underscoring the profound impact of LLMs on the field of AI.\n\nHowever, the architectural backbone of these models, particularly those built on the transformer framework [VSP + 17], presents a significant challenge: computational efficiency [TDBM22].The essence of the transformer architecture, the Attention mechanism, necessitates a computational and memory complexity of O(n 2 ), where n represents the sequence length.This quadratic dependency limits the scalability of LLMs, especially as we venture into processing longer sequences or expanding model capacities.\n\nIn an effort to mitigate this bottleneck, the AI research community has pivoted towards innovative solutions, one of which is sparse transformers [CGRS19,CNM19].Sparse attention mechanisms aim to approximate the results of the full attention computation by selectively focusing on a subset of the input data points.This is typically achieved by omitting certain interactions in the Query and Key multiplications within the attention mechanism, thereby inducing sparsity in the attention matrix.The overarching goal is to preserve the model's performance while alleviating the computational and memory demands.\n\nA noteworthy advancement in this domain is the introduction of the Reformer model [KKL20], which adeptly reduces the complexity from O(n 2 ) to O(n log n) through the adoption of the Locality Sensitive Hashing (LSH) technique.LSH enables the Reformer to efficiently approximate the attention mechanism, significantly curtailing the computational overhead without substantially compromising the model's efficacy.\n\nDespite these advancements, the theoretical underpinnings of sparse attention mechanisms and their implications on model performance and behavior remain an area of active inquiry.",
            "score": 0.5918724741327348,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 243,
                    "end": 381
                },
                {
                    "start": 381,
                    "end": 603
                },
                {
                    "start": 605,
                    "end": 792
                },
                {
                    "start": 792,
                    "end": 964
                },
                {
                    "start": 964,
                    "end": 1110
                },
                {
                    "start": 1112,
                    "end": 1273
                },
                {
                    "start": 1273,
                    "end": 1427
                },
                {
                    "start": 1427,
                    "end": 1606
                },
                {
                    "start": 1606,
                    "end": 1721
                },
                {
                    "start": 1723,
                    "end": 1949
                },
                {
                    "start": 1949,
                    "end": 2134
                },
                {
                    "start": 2136,
                    "end": 2315
                }
            ],
            "ref_mentions": [
                {
                    "start": 490,
                    "end": 497,
                    "matchedPaperCorpusId": "243766712"
                },
                {
                    "start": 783,
                    "end": 791,
                    "matchedPaperCorpusId": "221702858"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1357421875
        },
        {
            "corpus_id": "256847015",
            "title": "A Unified View of Long-Sequence Models towards Modeling Million-Scale Dependencies",
            "text": "Ever since their conception, Transformers have taken over traditional sequence models in many tasks, such as NLP, image classification, and video/audio processing, for their fast training and superior performance. Much of the merit is attributable to positional encoding and multi-head attention. However, Transformers fall short in learning long-range dependencies mainly due to the quadratic complexity scaled with context length, in terms of both time and space. Consequently, over the past five years, a myriad of methods has been proposed to make Transformers more efficient. In this work, we first take a step back, study and compare existing solutions to long-sequence modeling in terms of their pure mathematical formulation. Specifically, we summarize them using a unified template, given their shared nature of token mixing. Through benchmarks, we then demonstrate that long context length does yield better performance, albeit application-dependent, and traditional Transformer models fall short in taking advantage of long-range dependencies. Next, inspired by emerging sparse models of huge capacity, we propose a machine learning system for handling million-scale dependencies. As a proof of concept, we evaluate the performance of one essential component of this system, namely, the distributed multi-head attention. We show that our algorithm can scale up attention computation by almost $40\\times$ using four GeForce RTX 4090 GPUs, compared to vanilla multi-head attention mechanism. We believe this study is an instrumental step towards modeling million-scale dependencies.",
            "score": 0.5911413306130029,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.274658203125
        },
        {
            "corpus_id": "129945531",
            "title": "Generating Long Sequences with Sparse Transformers",
            "text": "Separately, the Transformer (Vaswani et al., 2017) has been shown to excel on many natural language tasks, which may be in part due to its ability to model arbitrary dependencies in a constant number of layers. As each self-attention layer has a global receptive field, the network can allocate representational capacity to the input regions for which it is most useful. Thus the architecture may be more flexible at generating diverse data types than networks with fixed connectivity patterns. \n\nHowever, the memory and computational requirements of such networks grows quadratically with sequence length, which excludes their use on long sequences. \n\nThe main contribution of this work is to introduce several sparse factorizations of the attention matrix, which scale as O(n p \u221a n) with the sequence length without sacrificing performance. These work by separating the full attention computation into several faster attention operations which, when combined, can approximate the dense attention operation. We use this to apply self-attention to sequences of unprecedented length. \n\nAdditionally, we introduce several other changes to the Transformer, including: \n\n\u2022 A restructured residual block and weight initialization to improve training of very deep networks \n\n\u2022 A set of sparse attention kernels which efficiently compute subsets of the attention matrix \n\n\u2022 Recomputation of attention weights during the backwards pass to reduce memory usage \n\nWe empirically validate that models augmented in this manner can achieve state-of-the-art compression and generation of natural language, raw audio, and natural images. The simplicity of the architecture leads us to believe it may be useful for many problems of interest.",
            "score": 0.5907556819577925,
            "section_title": "Introduction",
            "char_start_offset": 1715,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 650
                },
                {
                    "start": 653,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1082
                },
                {
                    "start": 1085,
                    "end": 1164
                },
                {
                    "start": 1167,
                    "end": 1266
                },
                {
                    "start": 1269,
                    "end": 1362
                },
                {
                    "start": 1365,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1724
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 50,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.240478515625
        },
        {
            "corpus_id": "270703226",
            "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers",
            "text": "Transformer models [72] have been considered as a de facto backbone of modeling arbitrary sequences, pretraining foundation models [8,21], and more recently, constructing large language models (LLMs) [9,69].Despite the inspiring success of their wide applications on both Natural Language Processing (NLP) and Machine Learning (ML) downstream tasks, extending the context window size to long sequences with computation and memory efficiently poses significant challenges [1,20,19], owing to the quadratic computation complexity and large amounts of key/value vectors associated with self-attention, especially on resource-constrained devices.\n\nMany recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths.However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs.Clustering-based methods [39,61] allow queries to attend to different sets of KV pairs.In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81,42,78].(ii) Previous learnable sparse attention often has super-linear complexity, especially during training.For example, clustering-based methods usually cost O(n log n) to maintain clusters.Ainslie et al. [1]  KV pairs are scored by u.SPARSEK computes a threshold for each query (\u03c4 (u)) such that the sum of normalized scores is k, which is 3 in this example.We select top-k KV pairs (orange cells) to perform attention.Right: the SPARSEK attention module.We fuse selection and attention in one kernel for efficiency.incorporates a SOFTTOPK operator [41] to compute soft masks in Transformer encoders.Meanwhile, migrating SOFTTOPK to Transformer decoders is less advantageous because solving SOFTTOPK for variable-length context associated with different queries requires quadratic time in total.",
            "score": 0.5907430241111706,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 207,
                    "end": 642
                },
                {
                    "start": 644,
                    "end": 777
                },
                {
                    "start": 777,
                    "end": 1008
                },
                {
                    "start": 1008,
                    "end": 1095
                },
                {
                    "start": 1095,
                    "end": 1305
                },
                {
                    "start": 1305,
                    "end": 1408
                },
                {
                    "start": 1408,
                    "end": 1491
                },
                {
                    "start": 1491,
                    "end": 1536
                },
                {
                    "start": 1536,
                    "end": 1660
                },
                {
                    "start": 1660,
                    "end": 1721
                },
                {
                    "start": 1721,
                    "end": 1757
                },
                {
                    "start": 1757,
                    "end": 1818
                },
                {
                    "start": 1818,
                    "end": 1902
                },
                {
                    "start": 1902,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 19,
                    "end": 23,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 200,
                    "end": 203,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 471,
                    "end": 474,
                    "matchedPaperCorpusId": "257622671"
                },
                {
                    "start": 1037,
                    "end": 1040,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1301,
                    "end": 1304,
                    "matchedPaperCorpusId": "264439578"
                },
                {
                    "start": 1506,
                    "end": 1509,
                    "matchedPaperCorpusId": "257622671"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1231689453125
        },
        {
            "corpus_id": "265034082",
            "title": "Ultra-Long Sequence Distributed Transformer",
            "text": "To address this issue, there are three research approaches: hierarchical training, attention approximation, and distributed sequence parallelism. Hierarchical training involves training multiple transformers at different levels of abstraction (Si & Roberts, 2021;Chen et al., 2022;2021b;Yu et al., 2023). The transformer at the lowest abstraction level trains on the shortest sequence segments. Then, the transformer at the next higher level uses the previous level outputs as additional input to train on longer segments. The process then repeats until reaching the highest abstraction level. However, training multiple transformers at different abstraction levels significantly increases training time and memory footprint. In addition, hyper-parameter tuning is required for the optimal model architecture at each abstraction level. \n\nThe approximation approach, in contrast, aims to reduce the computations and memory usage by approximating the self-  (Kitaev et al., 2020;Roy et al., 2021) (Child et al., 2019;Beltagy et al., 2020) Yes Serial O(Z) O(Z 3 ) N/A N/A N/A Low sparsity",
            "score": 0.5906958859984591,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2098,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 1085
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 281,
                    "matchedPaperCorpusId": "249395419"
                },
                {
                    "start": 281,
                    "end": 287,
                    "matchedPaperCorpusId": "232404237"
                },
                {
                    "start": 956,
                    "end": 977,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 977,
                    "end": 993,
                    "matchedPaperCorpusId": "212718077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.388671875
        },
        {
            "corpus_id": "248780136",
            "title": "Long-range Sequence Modeling with Predictable Sparse Attention",
            "text": "Models based on the Transformer architecture (Vaswani et al., 2017) have been firmly established as state of the art approaches across a range of domains like language (Brown et al., 2020;Clark et al., 2020;Devlin et al., 2018), and vision (Carion et al., 2020;Dosovitskiy et al., 2020). The Transformer architecture perceiving long-range context heavily relies on the multi-head self-attention mechanism, in which the relevance of every token pairs is computed to decide the attention scores and token's representations are the weighted average of all tokens using the attention scores. \n\nDespite its effectiveness, self-attention mechanism's quadratic time and memory complexity about the sequence length is an obstacle to extend Transformer for very long sequences, such as document-level text tasks, high-resolution images, videos, etc. Shen et al. (2021Shen et al. ( , 2018) ) elaborate the issue of high computational complexity. For instance, more than 68GB GPU memory and 1.6T multiply-accumulation operations are required for a 64 \u00d7 64 \u00d7 32 3D feature volume. \n\nGreat efforts have been made to develop Transformer's variants for long-range sequence modeling tasks. Tay et al. (2020c) categorize the researches of efficient Transformers: (a) Fixed patterns or combination of patterns (Beltagy et al., 2020;Zaheer et al., 2020), in which the field to be attended is pre-defined by fixed pattern. (b) Learnable patterns (Kitaev et al., 2020;Tay et al., 2020a), in which tokens are sorted or clustered in a data-driven fashion. (c) Memory (Ma et al., 2021;Lee et al., 2019), in which spacial tokens with global view are introduced to compress the input sequence. (d) Low-rank methods (Tay et al., 2021;Wang et al., 2020), which adopt low-rank approximations of the self-attention matrix. (e) Kernels (Katharopoulos et al., 2020;Choromanski et al., 2020a,b), which view the attention mechanism through kernelization.",
            "score": 0.5906402601674101,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1068
                },
                {
                    "start": 1071,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1920
                }
            ],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 67,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 240,
                    "end": 261,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 841,
                    "end": 858,
                    "matchedPaperCorpusId": "215999966"
                },
                {
                    "start": 1314,
                    "end": 1334,
                    "matchedPaperCorpusId": "28972310"
                },
                {
                    "start": 1447,
                    "end": 1465,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 1689,
                    "end": 1707,
                    "matchedPaperCorpusId": "218487423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12335205078125
        },
        {
            "corpus_id": "258832477",
            "title": "FIT: Far-reaching Interleaved Transformers",
            "text": "These methods allow for more flexible attention patterns, but may involve sparse operations that are not always accelerator-friendly. Another family of methods [14,39,4,27] incorporates recurrent mechanisms to connect transformers on local windows. However, similar to RNNs, these methods often require truncation due to reduced parallelism, which limits the sequence length during training. Lastly, a recent approach [26] incorporates linear attention [31,48,12] to bridge the information gap between local windows, but linear attention over all tokens in long sequences can still be expensive. For a comprehensive overview of various efficient transformer variants, we refer readers to [44]. Indeed, the aforementioned techniques are able to enhance long-range dependencies in local attention models. However, the communication channel across groups may be rigid, inefficient with accelerators, or not easily scalable. In contrast, we take a different approach by introducing a small set of adaptive latent tokens specifically designed for global attention. This allows for more flexible and efficient information exchange across groups. Additionally, we propose an interleaved mechanism, utilizing two types of transformer layers to encapsulate local and global processing, creating a dynamic interplay resembling top-down and bottom-up interactions [23], and ensuring the model remains expressive and scalable. \n\nAn alternative approach to improving the computational efficiency of transformers is to shift the computation to a reduced set of tokens (i.e. more computation on shorter sequences). Traditional methods often employ fixed-pattern downsampling techniques such as max pooling or average pooling [15], which have shown reasonable effectiveness. More recent approaches [33,19,20,2,1,30,29,28,22,5,21,42,18] explore the use of latent tokens that dynamically attend to data tokens and perform additional computations. Latent tokens are not tied to specific data tokens and remain small in number, thereby offering a compression effect and effectively handling redundancy or non-uniform information distribution in the data [28]. Our work builds upon similar ideas, but with a unique design. We incorporate grouping and local/window attention, seamlessly combining them to optimize efficiency for both encoding and generation tasks.",
            "score": 0.5903723781709432,
            "section_title": "Background and related work",
            "char_start_offset": 4347,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1414
                },
                {
                    "start": 1417,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2201
                },
                {
                    "start": 2202,
                    "end": 2342
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 169,
                    "matchedPaperCorpusId": "250526424"
                },
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "247011581"
                },
                {
                    "start": 453,
                    "end": 457,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 688,
                    "end": 692,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1353,
                    "end": 1357,
                    "matchedPaperCorpusId": "232046121"
                },
                {
                    "start": 1710,
                    "end": 1714,
                    "matchedPaperCorpusId": "219401850"
                },
                {
                    "start": 1796,
                    "end": 1799,
                    "matchedPaperCorpusId": "232110866"
                },
                {
                    "start": 1805,
                    "end": 1808,
                    "matchedPaperCorpusId": "246867214"
                },
                {
                    "start": 1810,
                    "end": 1813,
                    "matchedPaperCorpusId": "232076027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.201171875
        },
        {
            "corpus_id": "269982033",
            "title": "Attention as an RNN",
            "text": "Advancements in sequence modelling are highly impactful due to the wide range of applications, including reinforcement learning (e.g., robotics and autonomous driving), time series classification (e.g., financial fraud detection and medical diagnoses), and time series forecasting (e.g., weather and energy consumption predictions).Over the past several years, an extensively explored topic in sequence modelling is that of Transformer-based models (Vaswani et al., 2017).This is due to Transformers' strong performance and ability to leverage GPU parallelism.As a result, numerous Transformer-specific survey papers have been written for various sequential settings such as reinforcement learning (Agarwal et al., 2023;Li et al., 2023), time series (Lin et al., 2022;Jiang et al., 2024), and speech processing (Latif et al., 2023).\n\nWith the rapid increase in low-resource domains such as battery-powered devices, deployed models in these domains must be computationally efficient.Transformers, on the other hand, are expensive due to their quadratic scaling in memory and computation.Although their efficiency can be enhanced at inference time using techniques such as KV-caching (Pope et al., 2023), Transformers remain expensive for low-resource domains due to requiring (1) linear memory in the number of tokens and (2) the caching of all preceding tokens to the model.The effect of these limitations is exacerbated in settings with long contexts (i.e., large number of tokens) such as those common in time series (e.g., climate and economics).\n\nTo address this issue, we begin by examining attention, the component contributing to Transformers quadratic computational complexity.We show that (1) attention can be viewed as a special Recurrent Neural Network (RNN) with the ability to compute its many-to-one RNN output efficiently.Leveraging the RNN formulation of attention, we (2) show that popular attention-based models (e.g., Transformers and Perceivers) can be viewed as RNNs.",
            "score": 0.5898993663046881,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 472
                },
                {
                    "start": 472,
                    "end": 560
                },
                {
                    "start": 560,
                    "end": 832
                },
                {
                    "start": 834,
                    "end": 982
                },
                {
                    "start": 982,
                    "end": 1086
                },
                {
                    "start": 1086,
                    "end": 1374
                },
                {
                    "start": 1374,
                    "end": 1549
                },
                {
                    "start": 1551,
                    "end": 1685
                },
                {
                    "start": 1685,
                    "end": 1837
                },
                {
                    "start": 1837,
                    "end": 1988
                }
            ],
            "ref_mentions": [
                {
                    "start": 449,
                    "end": 471,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1182,
                    "end": 1201,
                    "matchedPaperCorpusId": "253420623"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1573486328125
        },
        {
            "corpus_id": "271334727",
            "title": "MINI-SEQUENCE TRANSFORMER: Optimizing Intermediate Memory for Long Sequences Training",
            "text": "We present our MINI-SEQUENCE TRANSFORMER (MST) mechanism with the idea of partitioning the input sequence into M \u00d7 mini-sequences. We show how to compute the exact transformer block by storing small intermediate matrices for the backward pass. Then, we analyze its IO complexity, showing that our method is throughput-equalized compared to the standard transformer. We further show how MST can work on distributed settings by integrating with DeepSpeed [19]. \n\nWe focus here on the forward pass for ease of exposition; Appendix B contains details for the backward. These implementations are partially inspired by Triton [49] and Transformer in Triton [1]. As flash attention and group query attention have minimized the intermediate value of attention, we put our focus on the MLP block and LM-Head block. Therefore, our implementation of MST is general enough to work with any attention: self-attention [52], cross-attention [4], causal attention [37], their sparse counterparts [9,55,43], and their various optimized kernels such as different versions of FlashAttention [13,12]. Our implementation adopts FlashAttention2 [12] for the experiments. \n\nInput Partition. We apply the mini-sequence technique to overcome the technical challenge of large intermediate values occupying HBM memory. We describe this in Algorithms 1, and 2, which represent MLP blocks and LM-Head from Llama2 [51] and Llama3 [33]. Their MLP block consists of three linear layers and SiLU function [41], and their LM-Head block consists of one linear layer and CrossEntropyLoss function [44]. The corresponding backward implementations can be referred to in Appendix B for more details. The main idea is to partition the input X into mini-sequence X i as Algorithm 1 line 1 and Algorithm 2 line 1, then compute the output with respect to those minisequences. We get the exact same result as standard implementation by contacting all mini-sequence outputs. \n\nGradient Accumulation. One of our goals is to reduce intermediate values for backward passes.",
            "score": 0.5894635400150082,
            "section_title": "MINI-SEQUENCE TRANSFORMER (MST): Algorithm, Analysis, and Distributed Extensions",
            "char_start_offset": 7211,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1929
                },
                {
                    "start": 1932,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2025
                }
            ],
            "ref_mentions": [
                {
                    "start": 620,
                    "end": 624,
                    "matchedPaperCorpusId": "184488182"
                },
                {
                    "start": 983,
                    "end": 986,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 986,
                    "end": 989,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1072,
                    "end": 1076,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1561,
                    "end": 1565,
                    "matchedPaperCorpusId": "18838459"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10302734375
        },
        {
            "corpus_id": "278715063",
            "title": "MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production",
            "text": "Different parallelism strategies for self-attention. \"TP\" denotes partitioning along the dimension of hidden size, while \"SP\" denotes partitioning along the dimension of sequence length. \n\napproach has limitations that prevent relying on a single method for effective scaling. \n\nData parallelism uniformly distributes the training data across all devices, with each device replicating the model parameters and optimizer states. To synchronize the parameters after each training iteration, data parallelism performs an all-reduce communication operation. Zero Redundancy Optimizer (ZeRO) [38] improves over data parallelism by distributing model states across all participating devices. ZeRO unfolds across three progressive stages, each designed to increasingly conserve memory, though this comes with the trade-off of elevated communication. \n\nTensor parallelism distributes compute-intensive tensor operations over multiple devices, enabling parallel computation and significantly accelerating the training process. The specific partitioning strategy and the dependencies among operators within the model dictate that tensor parallelism may necessitate gathering split inputs (all-gather) or merging outputs (reduce-scatter). In LLM training, operators like Lay-erNorm and Dropout, though less compute-intensive, require substantial activation memory. To tackle this problem, a variant of tensor parallelism known as sequence parallelism [18] is proposed, which partitions these operators along the dimension of sequence length. For long-context training, several works [1,14,46] apply sequence parallelism or tensor parallelism to different operators in self-attention. Figure 3 illustrates the mainstream parallelism strategies for attention, namely tensor, sequence, and context parallelism (TP, SP, and CP), which we analyze in \u00a73.1. model layers into stages that are processed on different devices, enabling pipelined execution. Each batch is split into several micro-batches for this purpose. To minimize pipeline bubbles, various scheduling strategies have been developed, e.g., GPipe [12], PipeDream 1F1B [31] and Interleaved 1F1B [32], etc. Megatron-LM adopts Interleaved 1F1B pipeline scheduling, further dividing each stage on one device into multiple virtual stages to reduce the pipeline bubble rate.",
            "score": 0.5886972890428341,
            "section_title": "Figure 3",
            "char_start_offset": 6671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 186
                },
                {
                    "start": 189,
                    "end": 276
                },
                {
                    "start": 279,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2315
                }
            ],
            "ref_mentions": [
                {
                    "start": 587,
                    "end": 591,
                    "matchedPaperCorpusId": "269617042"
                },
                {
                    "start": 1440,
                    "end": 1444,
                    "matchedPaperCorpusId": "248693351"
                },
                {
                    "start": 2115,
                    "end": 2119,
                    "matchedPaperCorpusId": "202488191"
                },
                {
                    "start": 2141,
                    "end": 2145,
                    "matchedPaperCorpusId": "236635565"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3330078125
        },
        {
            "corpus_id": "238408063",
            "title": "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences",
            "text": "Transformer (Vaswani et al., 2017) has become the state-of-the-art (SOTA) architecture for sequence modeling in a wide variety of fields, including natural language processing (NLP), computer vision, speech processing, applications to genomics data, etc. The key reason for Transformer's success is its self-attention mechanism, which computes dot-product between input representations for each pair of positions in the full sequence. Proved to be greatly effective in learning contextualized representations, Transformer becomes the backbone for dominant pre-trained language models (PLM) in NLP, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). These PLMs demonstrate strong transfer learning capabilities and have achieved SOTA widely on NLP tasks. However, self-attention has quadratic time and memory complexity to the input sequence length (Vaswani et al., 2017), which becomes the bottleneck for applying the vanilla Transformer to long sequence modeling tasks and for scaling up Transformer-based PLMs. \n\nA broad spectrum of approaches have been proposed to address the efficiency problem of selfattention, as summarized in (Tay et al., 2020). One major direction approximates the dense full self-attention using techniques such as introducing sparsity (Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020;Zhang et al., 2021a;Yang et al., 2021;Zhang et al., 2021b), low-rank approximations of the softmax attention matrix (Katharopoulos et al., 2020;Wang et al., 2020a;Zhu & Soricut, 2021;Xiong et al., 2021;Peng et al., 2021;Choromanski et al., 2021), and locality sensitive hashing (Kitaev et al., 2020). These approximation approaches exploit observations that token interactions have strong locality, hence the importance and in turn the attention should decrease with the increase of the distance between a query token and a key token. Several of these works achieve O(N ) theoretical complexity.",
            "score": 0.5885408746412856,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 1935
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 34,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 611,
                    "end": 632,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 645,
                    "end": 663,
                    "matchedPaperCorpusId": "198953378"
                },
                {
                    "start": 864,
                    "end": 886,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1320,
                    "end": 1340,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1340,
                    "end": 1360,
                    "matchedPaperCorpusId": "234339280"
                },
                {
                    "start": 1378,
                    "end": 1398,
                    "matchedPaperCorpusId": "235743105"
                },
                {
                    "start": 1456,
                    "end": 1484,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1503,
                    "end": 1523,
                    "matchedPaperCorpusId": "236428421"
                },
                {
                    "start": 1523,
                    "end": 1542,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 1542,
                    "end": 1560,
                    "matchedPaperCorpusId": "232105052"
                },
                {
                    "start": 1560,
                    "end": 1585,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1618,
                    "end": 1639,
                    "matchedPaperCorpusId": "209315300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1871337890625
        },
        {
            "corpus_id": "262217165",
            "title": "SPION: Layer-Wise Sparse Training of Transformer via Convolutional Flood Filling",
            "text": "The Transformer is a state-of-the-art deep neural network developed for addressing sequence tasks, originally proposed by Vaswani et al. [17]. One of the main advantages of the Transformer is that, given a sequence of input data points (e.g., a sentence of word tokens), it is able to compute the multi-head attention (MHA) operation in parallel, thereby quickly and accurately capturing long-term dependencies of data points. However, as the sequence length increases, it is true that the overall computational cost and memory space required for training the Transformer also increase quadratically [1]. Especially, a MHA sub-layer in the Transformer occupies a substantial portion of the total execution time and becomes the main bottleneck as the sequence length increases. The MHA operation requires a large number of dot-product operations to compute the similarity between all data points in the sequence. However, the dot-product operation inherently has limitations in memory bandwidth since it performs only two floating-point operations for each pair of data elements read from memory. Therefore, reducing the number of operations involving dot-product operations is an important consideration for accelerating the training of the Transformer. \n\nIn addition, the attention matrices used in the MHA computation have two main challenges. First, as the pattern of non-zero entries in the attention matrices varies across different layers, the irregular access of these non-zero entries within the matrices leads to an inefficient utilization of global memory bandwidth and low cache hit rates while performing dot-product operations. Second, the irregular distribution of non-zero entries in attention matrices across different layers results in load imbalance and a reduced level of exploitable parallelism. Hence, in order to mitigate computational complexity and improve data locality of the Transformer, several approaches have addressed the sparsification of computations associated with the MHA operation [2,14,18,20,21]. However, previous approaches suffer from two primary limitations. Firstly, when the Transformer adopts identical fixed patterns of non-zero entries in the attention matrices across all layers during training, it becomes difficult to effectively capture key features within the sequence. Secondly, when the Transformer employs additional parameters to learn the sparsity pattern in the attention matrices, both the model size and the computational overhead increase.",
            "score": 0.5881524015519495,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2321
                },
                {
                    "start": 2322,
                    "end": 2500
                }
            ],
            "ref_mentions": [
                {
                    "start": 600,
                    "end": 603,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 2021,
                    "end": 2024,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 2027,
                    "end": 2030,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 2030,
                    "end": 2033,
                    "matchedPaperCorpusId": "234339280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.318115234375
        },
        {
            "corpus_id": "220250819",
            "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
            "text": "Existing works seek to improve memory efficiency in transformers through weight pruning (Michel et al., 2019), weight factorization (Lan et al., 2020), weight quantization (Zafrir et al., 2019) or knowledge distillation. Clark et al. (2020) proposed a new pretraining objective called replaced token detection that is more sample efficient and reduces the overall computation. Lample et al. (2019) used product-key attention to increase the capacity of any layer with negligible computational overhead. \n\nReducing the memory or computational requirements with these methods leads to training or inference time speedups, but, fundamentally, the time complexity is still quadratic with respect to the sequence length which hinders scaling to long sequences. In contrast, we show that our method reduces both memory and time complexity of transformers both theoretically ( \u00a7 3.2) and empirically ( \u00a7 4.1). \n\nAnother line of research aims at increasing the \"context\" of self-attention in transformers. Context refers to the maximum part of the sequence that is used for computing selfattention. Dai et al. (2019) introduced Transformer-XL which achieves state-of-the-art in language modeling by learning dependencies beyond a fixed length context without disrupting the temporal coherence. However, maintaining previous contexts in memory introduces significant additional computational cost. In contrast, Sukhbaatar et al. (2019) extended the context length significantly by learning the optimal attention span per attention head, while maintaining control over the memory footprint and computation time. Note that both approaches have the same asymptotic complexity as the vanilla model. In contrast, we improve the asymptotic complexity of the self-attention, which allows us to use significantly larger context. \n\nMore related to our model are the works of Child et al. (2019) and Kitaev et al. (2020). The former (Child et al., 2019) introduced sparse factorizations of the attention matrix reducing the overall complexity from quadratic to O N \u221a N for generative modeling of long sequences. More recently, Kitaev et al. (2020) proposed Reformer. This method further reduces complexity to O (N log N ) by using locality-sensitive hashing (LSH) to perform fewer dot products.",
            "score": 0.5872295799837196,
            "section_title": "Efficient Transformers",
            "char_start_offset": 3393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1811
                },
                {
                    "start": 1814,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2275
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 109,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 132,
                    "end": 150,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 221,
                    "end": 240,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 377,
                    "end": 397,
                    "matchedPaperCorpusId": "195886317"
                },
                {
                    "start": 1091,
                    "end": 1108,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1402,
                    "end": 1426,
                    "matchedPaperCorpusId": "159041867"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27587890625
        },
        {
            "corpus_id": "269982604",
            "title": "LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions",
            "text": "Transformers (Vaswani et al., 2017) are dominant in the natural language processing (NLP) solution space, demonstrating state-of-the-art performance for a range of applications.With the advent of widely accessible large language models (LLMs), transformers as a class of models are being studied Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024.Copyright 2024 by the author(s).\n\n2 https://github.com/OSU-STARLAB/LeaPformer Figure 1.Contrasting accuracy-throughput trade-offs between LeaPformers and baselines.Throughput for 4k sequence length tasks (x-axis) vs. average accuracy score (y-axis) across the five tasks in the Long-Range Arena benchmark.The memory footprint of each mechanisim is specified in labels and represented by circle size.LeaPformers provide the best average inference accuracythroughput trade-off while achieving the second best overall score with only up to a 1.5% increase to parameter count.more closely than ever.Unfortunately, the quadratic complexity of the attention mechanisms of typical transformers limits the lengths of the sequences that they can process, rendering them sub-optimal or even impossible to apply for tasks with long sequences.\n\nNaturally, an active area of possible improvement for classical transformers are efficient attention mechanisms that reduce the sometimes prohibitive quadratic run-time and memory complexity of softmax attention with respect to sequence lengths.Many efficient transformer variants have been proposed, including both sub-quadratic attention mechanisms, usually with key assumptions or experimental bounds surrounding their construction, and truly linear attention mechanisms with no prior environmental assumptions (Katharopoulos et al., 2020;Choromanski et al., 2020;Peng et al., 2021;Chen et al., 2021;Qin et al., 2022b).While the aforementioned linear transformers are often effective for specific tasks, they tend to exhibit varying degrees of performance degradation when generalized.\n\nTo address this issue, re-weighting functions have been recently formalized (Su et al., 2022;Qin et al., 2022b) in linear transformers and serve to concentrate attention scores.",
            "score": 0.5872289399198009,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 177,
                    "end": 399
                },
                {
                    "start": 399,
                    "end": 431
                },
                {
                    "start": 433,
                    "end": 466
                },
                {
                    "start": 466,
                    "end": 486
                },
                {
                    "start": 486,
                    "end": 563
                },
                {
                    "start": 563,
                    "end": 704
                },
                {
                    "start": 704,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 971
                },
                {
                    "start": 971,
                    "end": 994
                },
                {
                    "start": 994,
                    "end": 1230
                },
                {
                    "start": 1232,
                    "end": 1477
                },
                {
                    "start": 1477,
                    "end": 1854
                },
                {
                    "start": 1854,
                    "end": 2020
                },
                {
                    "start": 2022,
                    "end": 2199
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 35,
                    "matchedPaperCorpusId": "209516262"
                },
                {
                    "start": 1817,
                    "end": 1835,
                    "matchedPaperCorpusId": "240354799"
                },
                {
                    "start": 1835,
                    "end": 1853,
                    "matchedPaperCorpusId": "246904340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.051177978515625
        },
        {
            "corpus_id": "273963185",
            "title": "Accelerating Large Language Model Training with 4D Parallelism and Memory Consumption Estimator",
            "text": "Unlike Sequence Parallelism (Korthikanti et al., 2023), which only parallelizes the activations of Dropout and LayerNorm, Context Parallelism enables partitioning of the model's network inputs and all activations along the sequence dimension. \n\nLlama 2 (Touvron et al., 2023) had a sequence length of 4,096 tokens, but Llama 3 (Dubey et al., 2024) increased this to 8,192 tokens, and Llama 3.1 further extended it to 131,072 tokens. As efficient training that supports long contexts is increasingly demanded, context parallelismwhich allows partitioning along the sequence dimensionis extremely useful for reducing the activations per GPU. \n\nIn components other than self-attention, there are no intertoken operations; thus, introducing context parallelism does not alter the operation. However, in self-attention layers, inter-token operations occur, necessitating the gathering of the full sequence, which requires additional all-gather communications between GPUs in the forward pass. During backpropagation, reduce-scatter is applied to the activation gradients, and each GPU stores only its sequence chunk to reduce the activation memory footprint. \n\nTo date, there is no comprehensive performance evaluation of 4D parallelism (DP, TP, PP, CP) utilizing context parallelism, and knowledge for applying it to actual LLM training is lacking.",
            "score": 0.5869973283977934,
            "section_title": "Parallelism",
            "char_start_offset": 6612,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 245,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1153
                },
                {
                    "start": 1156,
                    "end": 1344
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 54,
                    "matchedPaperCorpusId": "248693351"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5078125
        },
        {
            "corpus_id": "270370789",
            "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
            "text": "The attention mechanism (Vaswani et al., 2017) plays as a crucial component in transformers. Suppose the sequence length is L and the hidden dimension is d. In causal language modeling, an attention block receives query, key, and value matrices Q, K, V \u2208 R d\u00d7L , and computes outputs as: \n\nwhere M is a lower-triangular causal mask. Intuitively, causal attention only allows tokens to aggregate information from past tokens. Given a sequence with L tokens X = x 1 \u2022 \u2022 \u2022 x L \u2208 R d\u00d7L , the query, key, value matrices are computed as the linear projections of X: \n\nAs illustrated in the Figure 1 (a), acquiring full attention matrix softmax(K \u22a4 Q) requires O(L 2 ) peak memory cost. \n\nAlgorithm 1 Segment-level Attention (Training Time) \n\nStep 2 -Calculate key, query, values: \n\nStep 3 -Perform attention as: \n\nWhen L is large, i.e. handling long sequential data, full attention computation tend to run out of GPU memory rapidly. \n\nContext chunking is a common practice for reducing peak memory usage during training. Owing to the causality, Transformer-XL (Dai et al., 2019) introduces a recurrent computation mechanism by caching and reusing the hidden states to extend the context length for both training and inference. Specifically, the whole sequence is divided into a couple of segments, each then processed sequentially. The intermediate key and value states will be stored in the memory. Previously cached KV pairs will be used for computing the token representation in the subsequent segments. \n\nAlgorithm 1 presents the detailed procedure for the trainingtime attention computation. Suppose the input sequence of length L can be divided into N segments, where each block has B tokens, i.e. L = N B. The symbol [\u2022, \u2022] therein denotes the concatenation of two matrices' columns. \n\nNote that auto-regressive generation is a special case of segment-level attention at B = 1. This is, tokens come in sequel and attention is only computed between the incoming query and past KV pairs. The cached KV pairs K, V are known as KV cache for short in the inference mode1 .",
            "score": 0.5861235049410979,
            "section_title": "Segment-Level Attention with Long Sequences",
            "char_start_offset": 8403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 733
                },
                {
                    "start": 736,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1500
                },
                {
                    "start": 1503,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1784
                },
                {
                    "start": 1787,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2068
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0850830078125
        },
        {
            "corpus_id": "268714807",
            "title": "Proxyformer: Nystr\u00f6m-Based Linear Transformer with Trainable Proxy Tokens",
            "text": "Self-attention operation. The self-attention operation (Vaswani et al. 2017) stands as a pivotal element within transformer models. Fig. 2 illustrates a comparison of various attention techniques. In the conventional approach, shown in Fig. 2(a), each token attends to every other token, including itself. In the self-attention operation, the hidden states X = (x 1 , x 2 , ..., x n ) \u2208 R n\u00d7d of an input sequence have n tokens in d dimensions. They are projected to query (Q), key (K), and value (V ) by three weight matrices, \n\n, respectively, as follows: \n\nThen, the attention values between tokens are computed as: \n\nHere, S(\u2022) denotes the row-wise softmax normalization operation, and A \u2208 R n\u00d7n is the attention matrix, which contains the attention weights between tokens. The selfattention operation represents the value of a token as a weighted sum of all token values, based on their attention weights. The computation in Eq. 2 exhibits a quadratic complexity of O(n 2 d k + n 2 d v ) for an input sequence length n. As the input sequence length grows, this computation turns into a significant computational and memory challenge for the transformer model, thereby restricting its ability to handle longer input sequences. \n\nEfficient Transformers. Numerous studies have been conducted on efficient transformers with the goal of alleviating the quadratic dependency on input sequence length. The first approach is to exploit the sparsity of attention maps. To learn the sparse access pattern from the data set, Reformer (Kitaev, Kaiser, and Levskaya 2020) trains the query and key matrices to be clustered into buckets by locality sensitive hashing and considers the relationship between tokens only within the same bucket at attention operations, as depicted in Fig. 2(c). It reduces dot-product operations and achieves a complexity of O(nlogn). There are also similar approaches using sorting or clustering-based methods (Tay et al. 2020;Vyas, Katharopoulos, and Fleuret 2020). The second approach is to leverage low-rank approximation and kernelization.",
            "score": 0.5858647251962622,
            "section_title": "Background and Motivation",
            "char_start_offset": 5097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 527
                },
                {
                    "start": 530,
                    "end": 557
                },
                {
                    "start": 560,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 1528,
                    "end": 1563,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1948,
                    "end": 1986,
                    "matchedPaperCorpusId": "220424511"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1417236328125
        },
        {
            "corpus_id": "278237209",
            "title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing",
            "text": "Modern transformer architectures [1] have proven to be highly effective for sequence modeling tasks and are the key to the success of large language models (LLMs; [2,3,4,5]). One of the key components of their success is the attention mechanism, which enables dynamic information propagation by computing weighted sums of past states when processing each token. This results in high computational and memory complexity, both quadratic in sequence length. The key to the success of LLMs is the ever-increasing model sizes and context windows. Training and deploying these models becomes increasingly prohibitive. Furthermore, the KV-cache memory footprint during inference presents a significant bottleneck, limiting practical deployment scenarios and increasing operational costs. This led the researchers to explore alternative approaches. State Space Models [6,7,8,9,10] capture long-range dependencies with just a handful of state variables rather than relying on full attention matrices. They, however, fall short of full self-attention in terms of practical performance. To counteract lossy compression of State Space Models, a recent line of work investigates hybrids that  MoSA replaces each dense head with multiple heads with a learnable sparsity pattern. Each head selects its own k tokens to process. MoSA calculates query, key, and value projections only for the selected token and computes the attention only between them. It drops the rest of the tokens, leading to more efficient compute utilization. This reduces the computational and memory complexity on a sequence of length T from O(T 2 ) to O(k 2 + T ). The saved compute budget can be used to scale up the number of heads. combine quadratic attention and linearized memories [11,12,13]. Linear attention [14,15,16] * optimizes the attention cost by changing the order of the operations in the attention after removing nonlinearity. However, it also performs poorly compared to quadratic attention [17]. \n\nAs an alternative, static sparse attention methods [18] reduce the quadratic complexity by selectively attending to a subset of tokens to be used in the attention. They use hand-defined coarse-grained patterns that are not data-dependent. Typical examples of these methods are the block-sparse and strided attention [18,19,20].",
            "score": 0.5857363507147544,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 1973
                },
                {
                    "start": 1976,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2214
                },
                {
                    "start": 2215,
                    "end": 2303
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 36,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 163,
                    "end": 166,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 860,
                    "end": 863,
                    "matchedPaperCorpusId": "221150566"
                },
                {
                    "start": 863,
                    "end": 865,
                    "matchedPaperCorpusId": "240354066"
                },
                {
                    "start": 869,
                    "end": 872,
                    "matchedPaperCorpusId": "274598177"
                },
                {
                    "start": 1746,
                    "end": 1750,
                    "matchedPaperCorpusId": "267499935"
                },
                {
                    "start": 1775,
                    "end": 1779,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1779,
                    "end": 1782,
                    "matchedPaperCorpusId": "235377069"
                },
                {
                    "start": 1968,
                    "end": 1972,
                    "matchedPaperCorpusId": "252992749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1336669921875
        },
        {
            "corpus_id": "271310083",
            "title": "TorchGT: A Holistic System for Large-Scale Graph Transformer Training",
            "text": "Motivated by all the observations in \u00a7II-C, our key idea is to co-design an accuracy-maintained and compute-efficient attention module with a graph-parallelism-enabled system framework to support long sequence training. As shown in Figure 3 three levels from the top to bottom hierarchy: algorithm, runtime and kernel. We propose a topology-induced and accurate attention algorithm in the algorithm level. We present a novel cluster-aware graph parallelism to scale the training in the runtime level. In the kernel level, we design a memoryoptimized computation pattern specialized for clustered attention. Specifically, TORCHGT consists of three key modules: \n\n\u2022 Dual-interleaved Attention: In the algorithm level, it integrates locally graph-induced topology into the attention computation pattern and periodically overlays it with the fully-connected information, which efficiently reduces the computation burden while maintaining the model's quality as much as possible. It is tailored for versatile graph transformer models with local-global interleaved attention. \u2022 Cluster-aware Graph Parallelism: From the distributed runtime perspective, we design a cluster-aware graph parallelism tailored for graph transformers. It splits the graph tokens in sequences according to the clustering nature of graphs, thus computing attention with locality and facilitating the system scalability. \u2022 Elastic Computation Reformation: It reformats the graphinduced pattern obtained at the runtime level into our customized and fine-grained cluster-sparse pattern at the underlying execution kernel level. It further improves the attention computation throughput by greatly alleviating irregular memory access. To balance the efficiency-quality trade-off, we build an Auto Tuner to make an elastic transfer of cluster sparsity.",
            "score": 0.58520100510666,
            "section_title": "A. System Overview",
            "char_start_offset": 19208,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 659
                },
                {
                    "start": 662,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1816
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.242919921875
        },
        {
            "corpus_id": "269741084",
            "title": "HMT: Hierarchical Memory Transformer for Efficient Long Context Language Processing",
            "text": "Since one of the bottlenecks of transformers is the quadratic computational complexity of selfattention, a natural approach is sparsifying attention computation. A naive sparse attention pattern is the sliding window attention (Kovaleva et al., 2019), where each token attends to neighbors within a local window. However, this neglects longrange interaction between words. Existing works such as Longformer (Beltagy et al., 2020) and Poolingformer (Zhang et al., 2021) extend the sliding window attention by adding global attending tokens and applying pooling to expand the receptive field area. Unlimiformer (Bertsch et al., 2023) adopts the retrieval-augmented generative method by searching the top K most important tokens for the incoming sequence. It then applies attention to just those tokens in the decoders, resulting in pruned computations with minor losses. Nevertheless, the contribution of less relevant tokens may accumulate over time and impact the overall sequence generation. Although these methods extend the attainable context length, they cannot prevent increasing memory consumption as the input length increases. Alternatively, compressing past tokens using a recurrent sequence model can potentially reduce memory consumption by condensing the information into a fixed-size embedding.",
            "score": 0.5851405033156519,
            "section_title": "Long Context Transformers",
            "char_start_offset": 4732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1307
                }
            ],
            "ref_mentions": [
                {
                    "start": 448,
                    "end": 468,
                    "matchedPaperCorpusId": "234339280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18994140625
        },
        {
            "corpus_id": "273821735",
            "title": "The Evolution of RWKV: Advancements in Efficient Language Modeling",
            "text": "The success of Transformers, coupled with their limitations, led to research into more efficient attention mechanisms. These mechanisms aim to reduce the quadratic complexity of standard self-attention while retaining its ability to model long-range dependencies. Some notable examples include: \n\n\u2022 Sparse Attention: Methods like Sparse Transformer [Child et al., 2019] and Longformer [Beltagy et al., 2020] restrict attention to a subset of tokens, reducing the number of attention scores computed. This can be achieved by attending only to local neighborhoods, using fixed patterns, or learning the sparse attention structure. \u2022 Linear Attention: Techniques like Performers [Choromanski et al., 2020] and Linear Transformers [Katharopoulos et al., 2020] reformulate attention to achieve linear complexity. They typically use kernel methods or other approximations to avoid explicit computation of the full attention matrix. These methods often involve factorizing the attention matrix or using feature maps to represent token interactions. \u2022 Local Attention: Models like Transformer-XL [Dai et al., 2019] and Big Bird [Zaheer et al., 2020] combine local attention patterns with a small number of global tokens that attend to all other tokens. This allows for efficient processing of long sequences while still maintaining some global context. \u2022 Efficient Transformers: Architectures like Reformer [Kitaev et al., 2020] and Linformer [Wang et al., 2020] employ various techniques such as locality-sensitive hashing (LSH) or low-rank approximations to reduce the complexity of attention. LSH allows for efficient approximate nearest neighbor search, while low-rank approximations reduce the dimensionality of the attention matrices. \n\nThese approaches aim to maintain the strong performance of Transformers while reducing their computational and memory requirements, especially for long sequences.",
            "score": 0.5845959005141439,
            "section_title": "Efficient Attention Mechanisms",
            "char_start_offset": 4184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1897
                }
            ],
            "ref_mentions": [
                {
                    "start": 727,
                    "end": 755,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1120,
                    "end": 1141,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1561279296875
        },
        {
            "corpus_id": "269362347",
            "title": "CORM: Cache Optimization with Recent Message for Large Language Model Inference",
            "text": "Attention Let x \u2208 R n\u00d7d denote the input embeddings from a sequence of n feature vectors of dimension d.The multi-head self-attention [17], as a core module of Transformer model, facilitates contextual information interaction within each head in the following manner:\n\nQ, K, V represent the query, key, and value matrices, which are obtained by linearly mapping x using weight matrices W q , W k , and W v \u2208 R d\u00d7d h , respectively.d h is the dimension of each individual head.\n\nKV Cache Compression A common paradigm for inference of Transformer models is to retain the key-value pairs of previous tokens for subsequent reuse, to avoid inefficiency of recomputation.Thus, the consumption of KV cache becomes linearly correlated with sequence length, potentially resulting in excessive memory and latency issues when dealing with long input or output.\n\nMany efforts have been made to improve efficiency for LLMs.Multi-query attention (MQA) [8] and grouped-query attention (GQA) [9] are proposed to reduce key-value heads to decrease memory usage and the number of memory swaps.Adaptively Sparse Attention [18] learns to drop uninformative tokens from the context at any point across the generation process.Dynamic Memory Compression (DMC) [19] learns to apply different compression rates in different heads and layers with continual training.However, these methods require additional training to keep model performance due to the incapacity of direct conversion.\n\nAnother set of efforts is dedicated to addressing the balance between model efficiency and inference cost, achieved without extra training and architectural changes, by evicting the KV cache using different algorithms.StreamingLLM [10] and LM-Infinite [20] keep initial token and recent tokens throughout decoding process to align with the training window.InfLLM [21] stores evicted tokens into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation.Scissorhands [11] maintains pivotal tokens and recent tokens based on the persistence of importance hypothesis.H 2 O [12] utilizes accumulated attention score to maintain heavy hitters and recent tokens for inference.",
            "score": 0.5844143270523343,
            "section_title": "Related Work",
            "char_start_offset": 4647,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 104,
                    "end": 267
                },
                {
                    "start": 269,
                    "end": 431
                },
                {
                    "start": 431,
                    "end": 476
                },
                {
                    "start": 478,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 850
                },
                {
                    "start": 852,
                    "end": 911
                },
                {
                    "start": 911,
                    "end": 1076
                },
                {
                    "start": 1076,
                    "end": 1205
                },
                {
                    "start": 1205,
                    "end": 1341
                },
                {
                    "start": 1341,
                    "end": 1461
                },
                {
                    "start": 1463,
                    "end": 1681
                },
                {
                    "start": 1681,
                    "end": 1819
                },
                {
                    "start": 1819,
                    "end": 1974
                },
                {
                    "start": 1974,
                    "end": 2085
                },
                {
                    "start": 2085,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 1104,
                    "end": 1108,
                    "matchedPaperCorpusId": "258888224"
                },
                {
                    "start": 1987,
                    "end": 1991,
                    "matchedPaperCorpusId": "258947558"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12451171875
        },
        {
            "corpus_id": "278327448",
            "title": "CodeSSM: Towards State Space Models for Code Understanding",
            "text": "Transformer models have been used in code intelligence tasks (Feng et al., 2020;Guo et al., 2022a;Wang et al., 2021Wang et al., , 2023b)). However, they struggle to scale with long sequence lengths due to the quadratic complexity of self-attention (Vaswani et al., 2017). Various approaches have been proposed to address quadratic complexity, including linear attention (Katharopoulos et al., 2020) and methods using sparse attention patterns (Guo et al., 2022b;Zaheer et al., 2020;Condevaux and Harispe, 2022). LongCoder (Guo et al., 2023) uses the sparse attention mechanism. However, efficient transformers have seen limited practical adoption due to a minimal gain in computational cost in practice (Yang et al., 2025) and a potential degradation in performance (Qin et al., 2022). Dao et al. (2022;2024) introduced hardware-level optimizations, but the benefits of these optimizations are hardwarespecific and do not necessarily apply to newer or different hardware architectures (Shah et al., 2024). \n\nMoreover, self-attention models are trained with a fixed context window and cannot be used beyond the pretraining context window. Techniques such as ALiBi (Press et al., 2022) and RoPE (Su et al., 2024) have been proposed to alleviate this problem. HiRoPE (Zhang et al., 2024) adapted RoPE to code models. However, these methods come with their own set of challenges (Chen et al., 2023b;Hua et al., 2025a;Lin et al., 2024). In contrast, SSMs, and hence CodeSSM, are efficient by design and have been shown to better capture long-range dependencies compared to self-attention (Gu et al., 2022b). Additionally, CodeSSM does not have positional embedding, alleviating the issues of position-specific biases and length generalization.",
            "score": 0.5843554077612857,
            "section_title": "Related Work",
            "char_start_offset": 23550,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1005
                },
                {
                    "start": 1008,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1738
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 80,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 80,
                    "end": 98,
                    "matchedPaperCorpusId": "247315559"
                },
                {
                    "start": 98,
                    "end": 115,
                    "matchedPaperCorpusId": "237386541"
                },
                {
                    "start": 248,
                    "end": 270,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 370,
                    "end": 398,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 443,
                    "end": 462,
                    "matchedPaperCorpusId": "245144820"
                },
                {
                    "start": 462,
                    "end": 482,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 482,
                    "end": 510,
                    "matchedPaperCorpusId": "253157377"
                },
                {
                    "start": 522,
                    "end": 540,
                    "matchedPaperCorpusId": "259262301"
                },
                {
                    "start": 703,
                    "end": 722,
                    "matchedPaperCorpusId": "267770686"
                },
                {
                    "start": 766,
                    "end": 784,
                    "matchedPaperCorpusId": "252992749"
                },
                {
                    "start": 786,
                    "end": 803,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 803,
                    "end": 808,
                    "matchedPaperCorpusId": "259936734"
                },
                {
                    "start": 985,
                    "end": 1004,
                    "matchedPaperCorpusId": "271098045"
                },
                {
                    "start": 1163,
                    "end": 1183,
                    "matchedPaperCorpusId": "237347130"
                },
                {
                    "start": 1193,
                    "end": 1210,
                    "matchedPaperCorpusId": "233307138"
                },
                {
                    "start": 1375,
                    "end": 1395,
                    "matchedPaperCorpusId": "266053571"
                },
                {
                    "start": 1413,
                    "end": 1430,
                    "matchedPaperCorpusId": "270845965"
                },
                {
                    "start": 1583,
                    "end": 1601,
                    "matchedPaperCorpusId": "240354066"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05450439453125
        },
        {
            "corpus_id": "265034082",
            "title": "Ultra-Long Sequence Distributed Transformer",
            "text": "Transformer models trained on long sequences often achieve higher accuracy than short sequences. Unfortunately, conventional transformers struggle with long sequence training due to the overwhelming computation and memory requirements. Existing methods for long sequence training offer limited speedup and memory reduction, and may compromise accuracy. This paper presents a novel and efficient distributed training method, the Long Short-Sequence Transformer (LSS Transformer), for training transformer with long sequences. It distributes a long sequence into segments among GPUs, with each GPU computing a partial self-attention for its segment. Then, it uses a fused communication and a novel double gradient averaging technique to avoid the need to aggregate partial self-attention and minimize communication overhead. We evaluated the performance between LSS Transformer and the state-of-the-art Nvidia sequence parallelism on a Wikipedia enwik8 dataset. Results show that our proposed method lead to 5.6x faster and 10.2x more memory-efficient implementation compared to state-of-the-art sequence parallelism on 144 Nvidia V100 GPUs. Moreover, our algorithm scales to an extreme sequence length of 50,112 at 3,456 GPUs, achieving 161% super-linear parallel efficiency and a throughput of 32 petaflops.",
            "score": 0.5841051777787966,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72509765625
        },
        {
            "corpus_id": "273025930",
            "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference with Partial Contexts",
            "text": "This paper argues that integrating the two stages not only builds an efficient long-context LLM with improved long-context performance, but also reduces the cost of length extension training. \n\nOur proposed approach LONGGEN, as illustrated in Figure 1, is a simple and effective hybrid transformer architecture that could be built upon any pretrained transformer to extend its context length. LONGGEN incorporates two key designs: (1) It conducts context extension with various GPU-friendly KV cache-saving designs, including but not limited to window attention (Jiang et al., 2023), attention sink (Xiao et al., 2024), and blockwise strided attention (Zaheer et al., 2020b). \n\nCompared with delicately curated KV cache reduction methods, LONGGEN employs a simpler and more efficient KV design, which maintains uniform memory access patterns for attention heads and achieves load balance among token blocks. Practically, our customized triton training kernel inherits from FlashAttention-2 (Dao, 2023), but achieves faster speed in sparse settings, and the inference kernel is well-suited to vLLM (Kwon et al., 2023) for high-throughput serving. \n\n(2) LONGGEN uses this sparse attention in a hybrid architecture, where 2/3 of the attention layers use sparse attention while the remaining 1/3 retain full attention, which we find is essential for handling complex tasks requiring direct access to long-context information( \u00a74.4). \n\nExperiments are conducted on a Llama-2-7B base model (Touvron et al., 2023) and its 70B counterpart with group query attention. We extend their context length from 4K to 128K and perform evaluations on multiple long-context benchmarks, including the needle-in-a-haystack (NIAH) retrieval, and two more distinguishing benchmarks, BABILong (Kuratov et al., 2024) and RULER (Hsieh et al., 2024), where long-context reasoning is also needed. Through ablations on the position and number of sparse attention layers, we study how sparsity level affects long-context capability.",
            "score": 0.5839461821548999,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1818,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1428
                },
                {
                    "start": 1431,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 2002
                }
            ],
            "ref_mentions": [
                {
                    "start": 599,
                    "end": 618,
                    "matchedPaperCorpusId": "263310483"
                },
                {
                    "start": 652,
                    "end": 674,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1097,
                    "end": 1116,
                    "matchedPaperCorpusId": "261697361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2447509765625
        },
        {
            "corpus_id": "271431900",
            "title": "S2-Attention: Hardware-Aware Context Sharding Among Attention Heads",
            "text": "Efficient Transformers Numerous of attempts have been made to make the training and serving of LLMs more efficient. FlashAttention family (Dao et al., 2022;Dao, 2023) are the most widelyadopted attention acceleration framework. Dao et al. (2022) breaks down the attention computation into smaller blockwise computation to reduce the IO between SRAM and HBM. Dao (2023) further improve the performance by reducing the non-matmul operations, extra parallelization over sequence length, and better warp organization. Our implementation is based on the FlashAttention kernel, and leverage its parallelization over the number of heads. \n\nSparse Self-Attention reduce the computational complexity of self-attention by only attending to a subset of tokens (Child et al., 2019;Katharopoulos et al., 2020;Kitaev et al., 2020;Zaheer et al., 2020;Beltagy et al., 2020). Child et al. (2019) factorized attention computation into local and stride patterns to reduce the computation. Ho et al. (2019) saves computation by applying multiple attentions along each axis of the input tensor. Beltagy et al. (2020) adds more sparsity on top of Child et al. (2019) by taking advantage of the receptive field accumulation with dilated sliding window. Zaheer et al. (2020) replaces the one-to-all attention in transformers with sliding window attention and random attention. Qiu et al. (2020) chunks the sequence into blocks, which reduces FLOPs by performing attention on a larger granularity. However, many of these methods can't bring wall-clock speed-up due to ignorance of realistic memory access cost (Dao et al., 2022). Meanwhile, we argue that some sparsity pattern are not inference-friendly for generative models due to inefficient KV caching. Also, as they underutilized the parallelization over head, there's actually great room to further reduce FLOPs while improving context coverage. We will discuss this further in the following sections.",
            "score": 0.5838611639218247,
            "section_title": "RELATED WORKS",
            "char_start_offset": 4762,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 156,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 228,
                    "end": 245,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 796,
                    "end": 816,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 816,
                    "end": 836,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1230,
                    "end": 1250,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1353,
                    "end": 1370,
                    "matchedPaperCorpusId": "207847640"
                },
                {
                    "start": 1585,
                    "end": 1603,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15869140625
        },
        {
            "corpus_id": "272366594",
            "title": "Designing Large Foundation Models for Efficient Training and Inference: A Survey",
            "text": "The core operation for a sequence X with kernel W is: \n\nTo accelerate training, Fast Fourier Transforms (FFT) reduce computational complexity from O(N 2 ) to O(N log N ). However, causal convolutions require caching all prior activations, leading to high memory usage in autoregressive applications. \n\nHybrid Models: Combining Attention and SSMs While alternative architectures provide computational benefits, self-attention remains superior in capturing in-context relationships and performing few-shot generalization. Hybrid models, such as Mamba-2-Hybrid Waleffe et al. ( 2024), integrate selective self-attention with state-space models, achieving a balance between efficiency and model expressivity. Empirical studies show that hybrid architectures outperform both transformers and pure SSMs in long-context tasks, efficiently handling sequences up to 128K tokens. Recent advances in sequence modeling reduce computational overhead. Linear attention employs kernel approximations to optimize efficiency, state-space models leverage structured recurrence for constant-time token generation, and long convolutional introduces FFT-based acceleration, etc. These efficient sequence processing techniques help reduce memory usage and speed up computation while keeping the model's ability to understand long sequences. They make it possible to handle extremely long contexts in large-scale language models. The concept of the Mixture of experts, first introduced in (Jacobs et al., 1991;Jordan and Jacobs, 1994) has been thoroughly studied and developed through numerous subsequent works (Collobert et al., 2001;Rasmussen and Ghahramani, 2001;Eigen et al., 2014;Theis and Bethge, 2015;Aljundi et al., 2017). The advent of sparsely-gated MoE (Shazeer et al., 2017), especially in combination with transformer-based large language models (Lepikhin et al., 2020a), has revitalized this technology, which has been evolving for over three decades. The MoE framework operates on a straightforward but impactful principle: distinct components of the model, referred to as experts, are designed to specialize in specific tasks or data aspects. In this approach, only the relevant experts are activated for a given input, balancing computational efficiency with access to a vast pool of specialized expertise.",
            "score": 0.5837095963598736,
            "section_title": "Efficient Sequence Modeling",
            "char_start_offset": 42529,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 56,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 299
                },
                {
                    "start": 302,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2300
                }
            ],
            "ref_mentions": [
                {
                    "start": 1466,
                    "end": 1487,
                    "matchedPaperCorpusId": "572361"
                },
                {
                    "start": 1487,
                    "end": 1511,
                    "matchedPaperCorpusId": "67000854"
                },
                {
                    "start": 1588,
                    "end": 1612,
                    "matchedPaperCorpusId": "266933338"
                },
                {
                    "start": 1612,
                    "end": 1643,
                    "matchedPaperCorpusId": "231632857"
                },
                {
                    "start": 1662,
                    "end": 1685,
                    "matchedPaperCorpusId": "2865509"
                },
                {
                    "start": 1685,
                    "end": 1706,
                    "matchedPaperCorpusId": "914027"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10723876953125
        },
        {
            "corpus_id": "273502767",
            "title": "Revealing and Mitigating the Local Pattern Shortcuts of Mamba",
            "text": "H3 (Fu et al., 2023) was developed by combining S4 with linear attention (Kaul, 2020), generalizing this formulation to more complex recurrent structures. This model is the first to extend linear attention to broader recurrences, which form the foundation for later architectures. By integrating the strengths of SSMs and linear attention, H3 effectively balances scalability and performance in long-sequence processing tasks. \n\nHyena (Poli et al., 2023) was designed to close the perplexity gap between attention-based Transformers and sub-quadratic alternatives. While traditional attention mechanisms face quadratic complexity with increasing sequence length, Hyena overcomes this by using implicitly parameterized long convolutions and data-gating, achieving subquadratic complexity. The model also shows that previous sub-quadratic attention approaches, such as those based on low-rank or sparse approximations, often still rely on dense attention layers to reach Transformer-level performance. \n\nRetnet (Sun et al., 2023) is an efficient model for long-range sequence modeling, built on linear attention with a simplified internal structure that reduces the state dimension to one. Its recurrence mechanism can be viewed as a specific case of linear state space models (SSMs). RetNet introduces the retention mechanism, which supports three computation modes: parallel, recurrent, and chunkwise recurrent. These modes enable parallel training, low-cost O(1) inference for faster decoding, and efficient long-sequence modeling with linear complexity by processing chunks in parallel and summarizing them recurrently. \n\nRWKV (Peng et al., 2023) is a recent RNN architecture designed specifically for language modeling, featuring a linear attention approximation mechanism called the \"WKV\" mechanism. RWKV utilizes linear time-invariant recurrences and can be viewed as the ratio of two state space models. Unlike traditional Transformers, which have quadratic complexity in terms of computation and memory, RWKV offers linear scalability, combining the efficiency of RNNs with the performance of Transformers.",
            "score": 0.5829975607847186,
            "section_title": "A.2 Efficient Model Architecture",
            "char_start_offset": 26415,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 85,
                    "matchedPaperCorpusId": "203603396"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1259765625
        },
        {
            "corpus_id": "272770661",
            "title": "Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey",
            "text": "Overcoming sequence length limitations in language models has several advantages, including improved interactions with human language, better capture of complex causality and reasoning, and reduced catastrophic forgetting. However, scaling up sequence length poses a challenge in balancing computational complexity and model expressivity. RNN-style models and state space models (Gu et al., 2021), (Smith et al., 2022), (Fu et al., 2022), (Poli et al., 2023) have been proposed, but they have limitations from the perspective of parallelization and model adaptability (Fathi et al., 2023). An alternative approach is to reduce the complexity of Transformers (Vaswani et al., 2017), such as using sliding windows or convolution modules for attention, or sparse attention. LongNet (Ding et al., 2023), a novel approach, replaces the attention mechanism with \"dilated attention\", which achieves linear computational complexity and logarithmic dependency between tokens. This allows LongNet to efficiently scale sequence lengths to 1 billion tokens, overcoming the constraints of computation and memory.",
            "score": 0.582773052009883,
            "section_title": "LongNET",
            "char_start_offset": 13089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1099
                }
            ],
            "ref_mentions": [
                {
                    "start": 439,
                    "end": 458,
                    "matchedPaperCorpusId": "257050308"
                },
                {
                    "start": 658,
                    "end": 680,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1905517578125
        },
        {
            "corpus_id": "218487799",
            "title": "Multi-scale Transformer Language Models",
            "text": "Transformer-based language models (Vaswani et al., 2017;Radford et al., 2018;Al-Rfou et al., 2019) have become the model of choice for most large-scale language modeling benchmarks. Kaplan et al. (2020) report power-law scaling of transformer language models with model capacity and data size. As models get bigger, the amount of computational resources, especially memory, grows quickly. In the next paragraph we review some recent efforts attempting to address this issue. \n\nMemory-Efficient Transformers. Sukhbaatar et al. (2019) present an adaptive attention mechanism that learns how far back into the past each head in a transformer should look, and if implemented efficiently with sparse matrix operations, can help save memory. Liu et al. (2018); Rae et al. (2019) present approaches that compress the transformer's memory with strided convolutions. Specifically, Liu et al. (2018) compress the keys and values in the multi-headed attention by a factor of 3 for long-text abstractive summarization. Child et al. (2019) present sparse transformers along with efficient CUDA kernels for sparse attention demonstrating the ability to generate very long sequences. Rae et al. (2019) compress the recurrent memory for a transformer-XL (Dai et al., 2019), but find that the best performing variant is one that does not learn the compression function end-to-end. Liu & Lapata (2019) proposes a hierarchical extension of the architecture proposed in Liu et al. (2018) that attends over very long sequences, with the aim to better model paragraph-and document-level contexts. Kitaev et al. (2019) describe the different factors that contribute to large memory footprints in vanilla transformers. They use reversible layers to remove the need to store activations at every layer in the forward pass, LSH attention to decrease memory requirements from O(N 2 ) to O(N log N ) where N is the sequence length, and they split activations in the feedforward layers.",
            "score": 0.5821686433562014,
            "section_title": "Related Work",
            "char_start_offset": 3186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 474
                },
                {
                    "start": 477,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1957
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 56,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 77,
                    "end": 98,
                    "matchedPaperCorpusId": "52004855"
                },
                {
                    "start": 1575,
                    "end": 1595,
                    "matchedPaperCorpusId": "209315300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1663818359375
        },
        {
            "corpus_id": "250390967",
            "title": "Fine-tuning Transformers with Additional Context to Classify Discursive Moves in Mathematics Classrooms",
            "text": "Similarly, to address the problem of processing long sequences with transformers, (Beltagy et al., 2020) introduced Longformer, which extends vanilla transformers with a modified self-attention mechanism to process long documents. The classic self-attention mechanism in BERT is computationally expensive, which explains the restriction of the maximum sequence length of 512 tokens. Instead, Longformer combines dilated sliding windows with global attention to achieve similar performance. As a result of reducing the computational complexity, Longformer can process long input sequences beyond the previously defined segment length of 512 tokens. Like Longfomers, Big-Bird (Zaheer et al., 2020) uses a sparse attention mechanism that includes a random attention component. \n\nOver the past few years, we have seen an increasing trend in other approaches to supporting transformers to learn long-term dependencies, such as modifying pre-training methods and the classic attention mechanism. For example, to learn dependencies across documents, (Xie et al., 2020) adopted a simple approach to truncate the document used for classification. Similarly, (Joshi et al., 2019)) used a chunking approach where documents were broken down into multiple chunks, and the activations were then combined to perform the tasks. Another recent example is the BERT-Seq model for classifying Collaborative Problem Solving (Pugh et al., 2021). The BERT-Seq model uses a special input representation that combines embeddings from adjacent utterances as contextual cues for the model. Building on the prior work, we explored new ways to enrich transformers with additional contextual cues.",
            "score": 0.5821452534535134,
            "section_title": "Transformers for additional context and long-term dependencies",
            "char_start_offset": 7370,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1667
                }
            ],
            "ref_mentions": [
                {
                    "start": 1043,
                    "end": 1061,
                    "matchedPaperCorpusId": "195873898"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.171142578125
        },
        {
            "corpus_id": "276574608",
            "title": "Neural Attention: A Novel Mechanism for Enhanced Expressive Power in Transformer Models",
            "text": "The attention mechanism was first introduced within the context of machine translation by Bahdamau et al. [1], when it was used for dynamic alignment of words between input and output sequences. This work introduced the concept of \"additive attention\", in which two vectors are independently processed by their own feed-forward networks to produce scalars that are summed to calculate the final attention score. With Neural Attention, instead of processing each vector separately, we concatenate the two vectors and perform a nonlinear operation on the combined representation, projecting them jointly into a space that encodes their relationship. This approach captures richer dependencies between vectors, whereas additive attention ultimately relies on a linear combination of independently projected representations. \n\nBuilding upon this foundation, Vaswani et al. [9] proposed the transformer architecture, eliminating the need for recurrence while still allowing flexibility of input sequence length. This innovation significantly improved the ability of sequence processing to be parallelized, addressing some of the bottlenecks in recurrent models. In this work, they adopted Dot-Product Attention as the mechanism for calculating attention scores, citing its practical advantages over additive attention when considering computational and memory efficiency. However, their work did not explore whether additive attention could offer advantages in expressivity, leaving the representational limitations of Dot-Product Attention largely unexamined. Neural Attention addresses this gap by prioritizing enhanced expressivity, even as we tackle the associated complexity challenges, as explained in section 3. \n\nWhile these foundational innovations laid the groundwork for modern transformers, much of the subsequent research has shifted toward improving computational efficiency, rather than addressing representational limitations. Fastformer [11] explored additive attention to achieve faster processing, while SwiftFormer [7] revisited additive attention for real-time applications in mobile vision tasks. Similarly, Xu et al. [12] proposed additive and convolution-based attention mechanisms tailored for vision transformers, focusing on practical implementation benefits. Linformer [10] addressed the O(n 2 ) complexity of self-attention by employing low-rank approximations to reduce computational and memory costs. More recently, Mahmood and Huang [5] introduced a segmented attention approach, computing attention only on pairs of consecutive overlapping segments to further optimize computational efficiency.",
            "score": 0.5817991952971578,
            "section_title": "Related Work",
            "char_start_offset": 6162,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1713
                },
                {
                    "start": 1716,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2281
                },
                {
                    "start": 2282,
                    "end": 2426
                },
                {
                    "start": 2427,
                    "end": 2622
                }
            ],
            "ref_mentions": [
                {
                    "start": 869,
                    "end": 872,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 2030,
                    "end": 2033,
                    "matchedPaperCorpusId": "257766532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.159423828125
        },
        {
            "corpus_id": "261557684",
            "title": "TFBEST: Dual-Aspect Transformer with Learnable Positional Encoding for Failure Prediction",
            "text": "It also exploits parallelism to increase model training and performance. \n\nwhere W o is a learnable weight matrix used to linearly combine the outputs of all attention heads and head i = Attention(Q, K, V ) \n\n3) Feed-forward network: The feed-forward network in a Transformer is an important component that captures complicated patterns in the input data. It is made up of numerous fully-connected layers and non-linear activation functions that allow the model to process and convert information gained via attention mechanisms, boosting its ability to learn and express intricate relationships within sequences. \n\n4) Encoder-Decoder Structure: Using self-attention mechanisms, the encoder analyses the input sequence, acquiring contextual information. The decoder then constructs the output sequence, using contextual information from the encoder and previously created tokens to make coherent translations or predictions. Using self-attention mechanisms, the encoder analyses the input sequence, acquiring contextual information. The decoder then constructs the output sequence, using contextual information from the encoder and previously created tokens to make coherent translations or predictions.",
            "score": 0.5817991952971578,
            "section_title": "IV. PRELIMINARIES OF THE TRANSFORMER A. The Vanilla Transformer",
            "char_start_offset": 11816,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 75,
                    "end": 206
                },
                {
                    "start": 209,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 613
                },
                {
                    "start": 616,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1203
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.145751953125
        },
        {
            "corpus_id": "266818228",
            "title": "Training and Serving System of Foundation Models: A Comprehensive Survey",
            "text": "Tensor parallelism is developed to address the challenges of training foundation models that exceed the memory capacity of a single device. In tensor parallelism (As shown in Figure 3b), the parameters and computations of the model are divided (or sliced) across multiple computing devices, effectively reducing the memory load on each device. \n\nMegatron-LM [19] introduced an efficient form of 1D tensor parallelism. For a given computational task, it involves two GEMM operations and a GeLU non-linearity: \n\nThe second weight matrix B can be split into \n\n. With this approach, Y i B i can be computed separately on individual processors. In Transformer models, the method of 1D tensor parallelism is effectively applied to the computation of multi-head attention. This allows multiple processing units to simultaneously calculate different attention heads without waiting for the results of others. Optimus [20] proposed an efficient and scalable 2D tensor parallelism. It is introduced based on the scalable universal matrix multiplication algorithm (SUMMA) [21]. Compared to 1D tensor parallelism, 2D parallelism distributes the computational load across more processing units, significantly enhancing overall computational efficiency. Although 2D parallelism offers a more fine-grained model partitioning approach, it can introduce higher communication overhead. To solve this, 2.5D tensor parallelism [22] is introduced, building upon 2.5D matrix multiplication [23], which leverages additional devices to minimize communication requirements. \n\nTo balance computation, memory, and communication loads effectively, 3D tensor parallelism [24] employs a 3-D Parallel Matrix Multiplication algorithm to accurately map and execute the computation process of the Transformer model. This algorithm optimizes the use of computational resources by intelligently distributing and computing different parts of the input and weight matrices on designated processors.",
            "score": 0.5816165059215647,
            "section_title": "2) Tensor Parallelism: Scaling Foundation Models",
            "char_start_offset": 7578,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 343
                },
                {
                    "start": 346,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1548
                },
                {
                    "start": 1551,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1960
                }
            ],
            "ref_mentions": [
                {
                    "start": 909,
                    "end": 913,
                    "matchedPaperCorpusId": "233210609"
                },
                {
                    "start": 1061,
                    "end": 1065,
                    "matchedPaperCorpusId": "611285"
                },
                {
                    "start": 1407,
                    "end": 1411,
                    "matchedPaperCorpusId": "251979875"
                },
                {
                    "start": 1468,
                    "end": 1472,
                    "matchedPaperCorpusId": "13931921"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058990478515625
        },
        {
            "corpus_id": "212718077",
            "title": "Efficient Content-Based Sparse Attention with Routing Transformers",
            "text": "Attention with Temporal Sparsity: Research on efficient attention neural models parallels the advent of attention-based architectures. In the context of speech recognition, (Jaitly et al., 2015) proposed the Neural Transducer which segments sequences in nonoverlapping chunks and attention is performed in each chunk independently. Limiting attention to a fixed temporal context around the current prediction has also been explored in (Chorowski et al., 2015), while (Chiu and Raffel, 2017) dynamically segment the sequence into variable sized-chunks. \n\nHierarchical attention strategies have also been explored: the model first considers which part of the inputs should be attended to before computing full attention in a contiguous neighborhood of the selected area (Gregor et al., 2015;Xu et al., 2015;Luong et al., 2015). Later, hierarchical attention has been simplified by (Liu et al., 2018) that alternates coarse layers (attending to the whole sequence at a lower temporal resolution) with local layers (attending to a neighborhood of the current prediction). This alternating strategy is also employed by (Child et al., 2019), which introduces bounded and strided attention, i.e. attending to a fixed context in the past at a subsampled temporal resolution. This work formalizes such a strategy using a sparse attention formalism, showing how it relates to full attention with a specific sparsity pattern in the attention matrix. It shows that sparse attention is sufficient to get stateof-the-art results in modeling long sequences over language modeling, image generation and music generation. (Sukhbaatar et al., 2019) builds upon this work and shows that is it is possible to obtain further sparsity by letting the model learn the length of the temporal context for each attention module. This work also makes use of the attention cache introduced in (Dai et al., 2019), a memory mechanism to train models over temporal contexts which extend beyond the length of the training batches.",
            "score": 0.5816046490257452,
            "section_title": "Related Work",
            "char_start_offset": 5531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 551
                },
                {
                    "start": 554,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1997
                }
            ],
            "ref_mentions": [
                {
                    "start": 435,
                    "end": 459,
                    "matchedPaperCorpusId": "1921173"
                },
                {
                    "start": 789,
                    "end": 805,
                    "matchedPaperCorpusId": "1055111"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0831298828125
        },
        {
            "corpus_id": "234336004",
            "title": "FNet: Mixing Tokens with Fourier Transforms",
            "text": "The standard self-attention mechanism (Vaswani et al., 2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in text tasks involving long range dependencies, character-level modelling, speech processing, image and video processing.\n\nMost efforts to improve attention efficiency are based on sparsifying the attention matrix. Tay et al. (2020d) survey many of the recent efficient attention works, which they broadly categorize as: (1) Data independent sparsity approaches that use fixed attention patterns (Child et al., 2019;Qiu et al., 2019;Parmar et al., 2018;Beltagy et al., 2020;Ainslie et al., 2020;Zaheer et al., 2020); (2) Data dependent sparsity approaches that dynamically compress the attention matrix Tay et al., 2020b,a;Kitaev et al., 2020;Vyas et al., 2020;; and (3) Linearization of the attention matrix using kernel decompositions (Katharopoulos et al., 2020;Choromanski et al., 2020b;Peng et al., 2021). Several of these works achieve O(N \u221a N ) or even O(N ) theoretical complexity. However, the scaling constants hidden by this notation can be large. For example, in models such as Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), andBigBird (Zaheer et al., 2020), attention is O(N ) as a function of the input length, but quadratic in the number of \"global tokens\"; the latter must be sufficiently large to ensure good performance.\n\nThe Long-Range Arena benchmark (Tay et al., 2020c) attempts to compare many of the above \"efficient\" Transformers in a series of tasks requiring long range dependencies, finding that the Performer (Choromanski et al., 2020b), Linear Transformer (Katharopoulos et al., 2020), Linformer , and Image Transformer (Local Attention) (Parmar et al., 2018) were the fastest on 4 \u00d7 4 TPU v3",
            "score": 0.5812990176625638,
            "section_title": "Efficient Transformers and long sequence models",
            "char_start_offset": 9304,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 601,
                    "end": 621,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 642,
                    "end": 663,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 1197,
                    "end": 1224,
                    "matchedPaperCorpusId": "221845203"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046905517578125
        },
        {
            "corpus_id": "260899828",
            "title": "Attention Is Not All You Need Anymore",
            "text": "It has been over six years since the introduction of the widely adopted Transformer architecture Vaswani et al. [2017]. While initially proposed for sequence transduction tasks, the Transformer has become the de-facto model architecture for a broad range of natural language processing tasks in recent years Kim et al. [2023] and has been widely applied in many other areas including computer vision Khan et al. [2022] and speech processing Latif et al. [2023]. \n\nAs of July 2023, the standard Transformer with the self-attention mechanism still prevails, especially in large language models (LLM) Touvron et al. [2023], due to its excellent parallelizability and capacity Zhao et al. [2023]. \n\nThe key to the success of the Transformer lies in the self-attention mechanism Dowdell and Zhang [2019], a type of the attention mechanism first introduced for machine translation in Bahdanau, Cho, and Bengio [2015]. \n\nHowever, a notable limitation of the Transformer is its quadratic computation and memory complexity in relation to sequence length. This limitation arises from the self-attention mechanism Ren et al. [2021], posing a challenge for applying the Transformer in scenarios involving long input sequences. \n\nTherefore, many variants, including \"efficient Transformers\" or \"x-formers\", have been proposed to address this issue in the past few years Tay et al. [2022a]. In general, these variants leverage pre-determined patterns or learnable patterns, low-rank approximations, kernels, downsampling, or sparsification to reduce the computation and memory complexity in the self-attention mechanism. As a result, they generally underperform the vanilla self-attention mechanism Dong et al. [2023]. \n\nTruly, the Transformer with the self-attention mechanism is a remarkable piece of artwork. It is quite challenging to surpass it in terms of performance. In order to improve its performance, we need to comprehensively understand the problem we face and every bit of the Transformer and consider replacing the self-attention mechanism, which is its",
            "score": 0.5811430457466772,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 692
                },
                {
                    "start": 695,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1704
                },
                {
                    "start": 1707,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2054
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 118,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 400,
                    "end": 418,
                    "matchedPaperCorpusId": "230435805"
                },
                {
                    "start": 878,
                    "end": 910,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 1103,
                    "end": 1120,
                    "matchedPaperCorpusId": "235829099"
                },
                {
                    "start": 1357,
                    "end": 1375,
                    "matchedPaperCorpusId": "221702858"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1806640625
        },
        {
            "corpus_id": "270710876",
            "title": "A Survey on Transformers in NLP with Focus on Efficiency",
            "text": "To achieve desirable performance on the text representations obtained, the model design should be such that it delivers optimal results with minimum complexity.The contributions towards model design can be grouped under the following categories:\n\n\u2022 Chunking: It deals with chunking into several blocks, processing each block individually, and connecting the representations of these blocks through recurrence or by some other mechanism.ABSA BERT [56] breaks down each sequence based on significant phrases contained in it while filtering out irrelevant chunks of tokens before being fed into the BERT model.An extension to the chunking approach has been proposed in the case of Transformer-XL [46] wherein multiple blocks are connected through a recurrence mechanism.This helps to efficiently compute attention for long sequences by breaking them down into multiple blocks.\u2022 Sparse Attention: A few contributions attempt sparsification of the attention matrix to reduce the complexity of computing attention in transformer-based models.This implies limiting the count of keys to be attended by queries based either on certain pre-defined patterns or input-conditioned connections.Some common patterns might be global attention, band attention, dilated attention, random attention, and block attention as illustrated in Figure 7.This technique exploits the inherent sparsity in the attention matrix in real-life applications even after computing attention on all possible query-key pairs.Sparse Transformer [87] factorizes the attention matrix to attain sparse patterns where connectivity is established between a pre-defined set of tokens.This reduces the complexity of attention to O(n \u221a n).Longformer [84] employs attention at fixed intervals in a strided fashion.It adopts a blend of band attention, dilated attention, and global attention to achieve a near linear scaling factor with respect to the sequence length.Extended Transformer Construction (ETC) [156] follows a similar approach agglomerating global attention and local band attention with relative positional encoding.Additionally, it employs masking through Contrastive Predictive Coding as a pre-training objective.BigBird [157] builds upon the ETC model by applying random patterns of sparse-attention.It can handle sequences 8 times the length and achieve linear complexity compared to the conventional attention mechanism.",
            "score": 0.5802397456649856,
            "section_title": "Model Design",
            "char_start_offset": 51117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 160,
                    "end": 245
                },
                {
                    "start": 247,
                    "end": 436
                },
                {
                    "start": 436,
                    "end": 607
                },
                {
                    "start": 607,
                    "end": 767
                },
                {
                    "start": 767,
                    "end": 873
                },
                {
                    "start": 873,
                    "end": 1036
                },
                {
                    "start": 1036,
                    "end": 1180
                },
                {
                    "start": 1180,
                    "end": 1328
                },
                {
                    "start": 1328,
                    "end": 1487
                },
                {
                    "start": 1487,
                    "end": 1639
                },
                {
                    "start": 1639,
                    "end": 1692
                },
                {
                    "start": 1692,
                    "end": 1766
                },
                {
                    "start": 1766,
                    "end": 1919
                },
                {
                    "start": 1919,
                    "end": 2082
                },
                {
                    "start": 2082,
                    "end": 2181
                },
                {
                    "start": 2181,
                    "end": 2269
                },
                {
                    "start": 2269,
                    "end": 2391
                }
            ],
            "ref_mentions": [
                {
                    "start": 446,
                    "end": 450,
                    "matchedPaperCorpusId": "234266431"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1959,
                    "end": 1964,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 2189,
                    "end": 2194,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1588134765625
        },
        {
            "corpus_id": "276575796",
            "title": "GenAI at the Edge: Comprehensive Survey on Empowering Edge Devices",
            "text": "Transformers have become the backbone of many GenAI models, but their multi-head self-attention mechanism can dominate runtime and memory usage. Therefore, researchers have explored a range of strategies to optimize attention on hardware and algorithmic levels. \n\nHardware-based. FlashAttention (Dao et al. 2022) reorders attention operations to reduce the number of reads and writes between GPU high bandwidth memory (HBM) and on-chip static RAM (SRAM) by splitting queries, keys, and values into smaller blocks, recomputing attention onchip during the backward pass, and fusing multiple GPU kernels into one. Built on this, FlashAttention-2 (Dao 2023) takes the foundation of memory efficiency and adds better parallelism and work distribution to further increase speed and GPU utilization, especially for longer sequences. Then, FlashAttention-3 (Shah et al. 2024) introduces asynchrony and low-precision computation to further optimize the attention mechanism for modern GPU architectures, which allows for even higher performance and efficiency, along with reduced error for low-precision (FP8) computing. Besides these, xFormers (Lefaudeux et al. 2022), a PyTorchbased library, provides a collection of optimized attention and Transformer blocks, including custom GPU kernels and memory-efficient attention implementations. \n\nAlgorithmic-based. Work on sparse attention reduces the quadratic complexity of self-attention by ignoring parts of the input that do not affect the result significantly. Child et al. (Child et al. 2019) pioneered this approach by limiting attention to strided patterns using sparse factorizations of the attention matrix to reduce computation cost while maintaining performance on sequence models. Subsequent techniques like Longformer (Beltagy et al. 2020) by using a combination of sliding window local attention and taskmotivated global attention, Big Bird (Zaheer et al. 2020) by combining random, windowed, and global attention to create a sparse attention mechanism, and Linformer (Wang et al. 2020a) by decomposing attention with linear projections to achieve linear complexity introduced various structured sparsity patterns.",
            "score": 0.579984334251487,
            "section_title": "Attention Optimization",
            "char_start_offset": 22202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1329
                },
                {
                    "start": 1332,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 2166
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.281494140625
        },
        {
            "corpus_id": "219558319",
            "title": "$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers",
            "text": "Transformer networks use pairwise attention to compute contextual embeddings of inputs, and have redefined the state of the art in many NLP tasks. However, these models suffer from quadratic computational cost in the input sequence length $n$ to compute attention in each layer. This has prompted recent research into faster attention models, with a predominant approach involving sparsifying the connections in the attention layers. While empirically promising for long sequences, fundamental questions remain unanswered: Can sparse transformers approximate any arbitrary sequence-to-sequence function, similar to their dense counterparts? How does the sparsity pattern and the sparsity level affect their performance? In this paper, we address these questions and provide a unifying framework that captures existing sparse attention models. Our analysis proposes sufficient conditions under which we prove that a sparse attention model can universally approximate any sequence-to-sequence function. Surprisingly, our results show the existence of models with only $O(n)$ connections per attention layer that can approximate the same function class as the dense model with $n^2$ connections. Lastly, we present experiments comparing different patterns/levels of sparsity on standard NLP tasks.",
            "score": 0.5798890247941026,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2181396484375
        },
        {
            "corpus_id": "250279747",
            "title": "Efficient Representation Learning via Adaptive Context Pooling",
            "text": "Context in Transformers is captured by the attention mechanism between all pairs of tokens from the entire input sequence. When the network goes deeper, high-level contextual dependencies emerge. However, full attention scales quadratically with the sequence length as existing attention models are trained to attend to individual tokens with a fixed granularity, e.g., text characters and image pixels. Hence the vanilla Transformer (Vaswani et al., 2017) is prohibitive for learning long sequences such as long documents or highresolution images (modeled as long sequences of image patches). \n\nRecent works build on the hierarchical architecture to improve the capability of long-range context modeling. In the vision domain for example, hierarchical transformers such as Swin transformer (Liu et al., 2021), PVT (Wang et al., 2021) and ViL (Zhang et al., 2021) rely on predefined image pyramids to compute self-attention at multiple scales, and can thus model long sequences of image patches at a much higher resolution. However, both the scaling scheme and effective attention granularities remain fixed in these methods. In a similar spirit, 'area attention' (Li et al., 2019b) computes multi-scale attention which is generic for both language and vision tasks. Specifically, attention is computed against a multi-scale memory formed by pooling the original memory with predetermined pool sizes. This not only requires larger memory but also does not adapt the context range based on content. Finally, the BP-Transformer (Ye et al., 2019) computes attention using multi-scale attention spans that encode fine-to-coarse contexts, but it imposes a sparsity prior on the attention mechanism, which is adaptive, but which might hurt its capacity. \n\nEfficient Transformers mostly use sparsity or low-rank assumptions on the attention matrix to reduce cost.",
            "score": 0.5798483543923737,
            "section_title": "Related Work",
            "char_start_offset": 4888,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1747
                },
                {
                    "start": 1750,
                    "end": 1856
                }
            ],
            "ref_mentions": [
                {
                    "start": 791,
                    "end": 809,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 815,
                    "end": 834,
                    "matchedPaperCorpusId": "232035922"
                },
                {
                    "start": 1164,
                    "end": 1182,
                    "matchedPaperCorpusId": "53085166"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1217041015625
        },
        {
            "corpus_id": "277621915",
            "title": "Foundation Models for Time Series: A Survey",
            "text": "Advantages Over Traditional Architectures. One of the most transformative aspects of the Transformer model is its ability to leverage parallelism, making it significantly more scalable compared to traditional sequential architectures [69]. This scalability stems from two critical factors: \n\n\u2022 Non-Sequential Processing: Unlike RNNs, which process inputs one time step at a time, Transformers operate on the entire sequence simultaneously [50]. This non-sequential processing eliminates the dependency on previous computations, allowing for efficient utilization of modern hardware accelerators like GPUs and TPUs. \u2022 Efficient Handling of Long Sequences: Time series datasets often involve long sequences, posing challenges for RNNs [40] and LSTMs [45] due to their inherent limitations in memory and computation. Transformers, on the other hand, process all time steps in parallel, ensuring consistent computational efficiency regardless of sequence length [69]. This  advantage is especially relevant in applications like highfrequency financial trading, where datasets consist of millions of observations. \n\nHowever, the scalability of Transformers is not without challenges. The self-attention mechanism requires O(n2) memory and computation for a sequence of length n, which can become prohibitive for extremely long time series [70]. To address this, recent innovations such as sparse attention and linear Transformers have been proposed, reducing the complexity to O(n) [71], [72] in certain cases. These advancements extend the applicability of Transformers to resource-constrained environments.",
            "score": 0.5798295144776457,
            "section_title": "Scalability and Parallelism",
            "char_start_offset": 21613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 289
                },
                {
                    "start": 292,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1603
                }
            ],
            "ref_mentions": [
                {
                    "start": 439,
                    "end": 443,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 733,
                    "end": 737,
                    "matchedPaperCorpusId": "205001834"
                },
                {
                    "start": 748,
                    "end": 752,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1334,
                    "end": 1338,
                    "matchedPaperCorpusId": "235623791"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.552734375
        },
        {
            "corpus_id": "237421288",
            "title": "PermuteFormer: Efficient Relative Position Encoding for Long Sequences",
            "text": "The Transformer architecture (Vaswani et al., 2017) has achieved state-of-the-art on various fields of research, including natural language processing (Devlin et al., 2019;Raffel et al., 2020), speech processing (Baevski et al., 2020) and image processing (Dosovitskiy et al., 2020;Tan and Bansal, 2019). But Transformer does not scale well to long sequences, because the time complexity and memory complexity of the attention module in Transformer are both quadratic to the sequence length. Recently, several efficient Transformers (Kitaev et al., 2020;Wang et al., 2020;Zaheer et al., 2020;Xiong et al., 2021) have been proposed to speed up the model from quadratic complexity to linear complexity without significant performance loss. Generally, they utilize efficient algorithms to approximate attention. \u00a72 briefly introduces these efficient Transformers and a more thorough review can be found in Tay et al. (2020c).\n\nAmong these efficient Transformers, it is suggested that Performer  is the fastest one (Tay et al., 2020b). In this paper, we denote as Performer the family of efficient Transformers similar to , e.g., Katharopoulos et al. (2020); Peng et al. (2021); Kasai et al. (2021); , not only  itself. Performer utilizes kernel method to avoid explicit calculation of attention weights. It applies a non-linear feature map to queries and keys to get query features and key features respectively and then multiplies query features, key features, and values together directly, without applying softmax. With the appropriate ordering of matrix multiplications, Performer achieves complexity linear of the sequence length. Moreover, some implementation of unidirectional Performer  even reduces memory footprint to constant at both training time and inference time.\n\nAlthough Performer accelerates attention to linear complexity, the existing relative position encoding (Shaw et al., 2018;Dai et al., 2019;Raffel et al., 2020) still has quadratic complexity with respect to the sequence length. So Performer cannot benefit from relative position encoding, which has already been a common practice for a bunch of state-of-the-art Transformers (Yang et al., 2019;Raffe",
            "score": 0.5798203063409113,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 51,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 172,
                    "end": 192,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 212,
                    "end": 234,
                    "matchedPaperCorpusId": "219966759"
                },
                {
                    "start": 282,
                    "end": 303,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 533,
                    "end": 554,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1880,
                    "end": 1899,
                    "matchedPaperCorpusId": "3725815"
                },
                {
                    "start": 1899,
                    "end": 1916,
                    "matchedPaperCorpusId": "57759363"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12359619140625
        },
        {
            "corpus_id": "215828216",
            "title": "ETC: Encoding Long and Structured Data in Transformers",
            "text": "Since the publication of the original Transformer model (Vaswani et al., 2017), and especially after the great success of BERT (Devlin et al., 2018), a number of variations of the model have been proposed in the literature. For example, work exists on scaling up the training process (RoBERTa (Liu et al., 2019)), scaling the internal representation of the model (ALBERT (Lan et al., 2019)), or both (T5 (Raffel et al., 2019)), outperforming the original BERT model in tasks such as GLUE (Wang et al., 2018), SQuAD (Rajpurkar et al., 2016) or RACE (Lai et al., 2017). However, these models use input sequences of length up to 512 tokens due to computational and memory constraints: namely, the O(n 2 ) computational and memory cost of attention in Transformers, where n is the length of the input sequences. This work builds on prior efforts to scale up the attention mechanism and network architecture to accommodate longer input sequences. We classify these approaches into four broad categories: sparse attention, recurrence, hierarchical mechanisms, and compressed attention, which we elaborate on below. \n\nSparse Attention involves limiting each token to attend only to a subset of the other tokens in the input. For example, the Sparse Transformer (Child et al., 2019) used predefined attention patterns with applications to both natural language tasks and image generation. For example, they showed that attending only to previous pixels in the same row or column was enough for generating high quality images. This allows attention to go from \n\nAnother idea is that of Adaptive Attention Span Transformer (Sukhbaatar et al., 2019), where each attention head is associated with a decaying masking function, which limits the number of tokens it can attend to. Making those masking functions learnable, they show that lower layers tend to learn to use short attention spans, and it is not until the higher layers of the model, that attention spans are longer.",
            "score": 0.5797287814646074,
            "section_title": "Background",
            "char_start_offset": 3068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1550
                },
                {
                    "start": 1553,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1964
                }
            ],
            "ref_mentions": [
                {
                    "start": 56,
                    "end": 78,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 293,
                    "end": 311,
                    "matchedPaperCorpusId": "52044834"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1607666015625
        },
        {
            "corpus_id": "273345158",
            "title": "LoLCATs: On Low-Rank Linearizing of Large Language Models",
            "text": "Subquadratic Attention Alternatives. Many prior works study more efficient sequence modeling modules compared to Transformer self-attention, commonly training proposed architectures from scratch. While our work is most similar to and compatible to linear attentions [11,13,30,44,49,62,63,66] and sparse attentions [6,9,65], other efficient architectures include convolutional models [25,42], recurrent neural nets (RNNs) (not strictly following the attention inductive bias) [5,18,39], and state-space model (SSM)-inspired neural net layers [16,24,25]. \n\nImproving LLMs for Long Contexts. We share motivation various works focus on improving context length quality [1,10,41]. They that the rotary embeddings [53] of popular LLMs [3,7,28,57] struggle to handle contexts longer than those used in training. [12] notably study the training efficiency of this via LoRA. Our work is complementary, where we reduce the efficiency bottleneck by replacing self-attentions. \n\nFurther work lies in how to improve long context quality of linearized LLMs, e.g., by integrating these methods. Other works also improve efficiency by optimizing the KV Cache from a systems standpoint [32]. These may present complementary optimizations to linear attention's algorithmic improvements.",
            "score": 0.5793040505836408,
            "section_title": "D.2 Efficient Architectures",
            "char_start_offset": 60287,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1268
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "240354799"
                },
                {
                    "start": 273,
                    "end": 276,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 276,
                    "end": 279,
                    "matchedPaperCorpusId": "252992749"
                },
                {
                    "start": 279,
                    "end": 282,
                    "matchedPaperCorpusId": "235377069"
                },
                {
                    "start": 282,
                    "end": 285,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 288,
                    "end": 291,
                    "matchedPaperCorpusId": "267523164"
                },
                {
                    "start": 319,
                    "end": 322,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 708,
                    "end": 712,
                    "matchedPaperCorpusId": "233307138"
                },
                {
                    "start": 732,
                    "end": 734,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 1169,
                    "end": 1173,
                    "matchedPaperCorpusId": "261697361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0863037109375
        },
        {
            "corpus_id": "266999544",
            "title": "The What, Why, and How of Context Length Extension Techniques in Large Language Models - A Detailed Survey",
            "text": "Pre-training is executed on The Stack dataset (Kocetkov et al., 2022), a compilation of source code in over 300 programming languages. Data preprocessing employs the tiktoken tokenizer with cl100k_base encoding, and models undergo training with a batch size of 0.5M tokens for 300K steps. LongNet undergoes comparison with vanilla Transformer and sparse Transformers, experimenting with sequence lengths ranging from 2K to 32K. Segment lengths for LongNet are defined as w = {2048, 4096, 8192, 16384, 32768}, and dilated ratios are denoted as r = {1, 2, 4, 6, 12}. Sparse attention, following a fixed pattern (Child et al., 2019b), adjusts ratios to match computation flops with LongNet. Dense attention in vanilla Transformers is restricted to a 32K sequence length due to higher computation costs. Attention variants are derived from FlashAttention3 for training efficiency, incorporating customized flash attention kernels for both sparse and dilated attention. \n\nFor sequences surpassing the model's support, block-wise causal attention (BCA) (Sun et al., 2022) is implemented for language model inference, accompanied by the removal of absolute position encoding. Results indicate that augmenting sequence length during training generally enhances language models. However, the extrapolation of sequence length in inference encounters limitations when the length significantly surpasses the model's support. LongNet consistently outperforms baseline models, affirming its efficacy in language modeling. \n\nAdvantages. LongNet presents notable advantages, encompassing linear computation complexity and a logarithmic dependency between any two tokens in a sequence. Its applicability extends to serving as a distributed trainer for exceptionally long sequences, offering a seamless integration of its dilated attention as a drop-in replacement for standard attention within the existing Transformerbased optimization framework. The linear complexity of LongNet facilitates parallelized training across nodes, overcoming computational and memory constraints through a distributed algorithm. This scalability enables efficient training on sequences of up to 1 billion tokens with nearly constant runtime, a significant improvement over the quadratic complexity limitations experienced by vanilla Transformer.",
            "score": 0.5785167687732871,
            "section_title": "LongNet",
            "char_start_offset": 49963,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2309
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28173828125
        },
        {
            "corpus_id": "256847015",
            "title": "A Unified View of Long-Sequence Models towards Modeling Million-Scale Dependencies",
            "text": "Despite their great advantages, Transformers have critical disadvantages as well, and chief among them is their resource efficiency. Although Transformers are much faster compared to traditional sequence models, the attention matrix incurs a O(L 2 ) complexity, where L is the context length, in terms of both compute and storage. Such a complexity is especially prohibitive when dealing with context of thousands of tokens since the time and memory it takes scale quadratically, for example, summarizing books, processing audio/video, dealing with high-resolution images, etc. In these tasks, the context can easily scale to thousands or even millions of tokens as making a decision may need information from far earlier steps in the sequence (e.g., a name in the first chapter of a book). To counter this challenge, over the past five years a great deal of studies has focused on making Transformers more efficient. The proposed methods include (but not limited to) for example, approximating the attention matrix with sparsity [24,3,46], clustering before computing attention [35,29], making assumptions via conditional probability [34], low-rank estimation [42], better memory I/O [12], matrix orthogonality and associativity [7], etc. Apart from tackling the efficiency problem of Transformers directly, many other models have been proposed to cater the need for long-context learning. To name a few, MLP-Mixer [37], FNet [30] and SGConv [31] learn from and (solely) utilize the token-mixing paradigm from Transformers; Memorizing Transformers [44] take Neural Turing Machines [17] to extreme and make Transformers a huge LSTM-like structure; S4 [20,21] rejuvenates traditional State Space Models combined with orthogonal polynomial projection to reconstruct histories. \n\nAcknowledging the significance in both the novelty and volume of prior work, we take a step back and compile this work with the following objectives: \n\n1. Categorizing existing solutions to long-range dependency problems purely by their mathematical formulations. 2. Comparing various methods using a unified template, with which we study the tradeoffs in capturing global and local dependencies in input sequence.",
            "score": 0.5784587226553661,
            "section_title": "Introduction",
            "char_start_offset": 2921,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1774
                },
                {
                    "start": 1777,
                    "end": 1926
                },
                {
                    "start": 1929,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 1036,
                    "end": 1039,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1079,
                    "end": 1083,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 1135,
                    "end": 1139,
                    "matchedPaperCorpusId": "235829099"
                },
                {
                    "start": 1185,
                    "end": 1189,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1416,
                    "end": 1420,
                    "matchedPaperCorpusId": "233714958"
                },
                {
                    "start": 1651,
                    "end": 1655,
                    "matchedPaperCorpusId": "240354066"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1416015625
        },
        {
            "corpus_id": "266110855",
            "title": "LongT5-Mulla: LongT5 With Multi-Level Local Attention for a Longer Sequence",
            "text": "Standard Transformer [1] adopts the full attention mechanism, that is, each token in the input sequence needs to be calculated attention from all the other tokens, and consequently causes O(N 2 ) complexity and consumes huge resources when processing long sequences. For this reason, these models that are built on vanilla Transformer, for example, BERT [2], BART [3], GPT2 [5], and T5 [4], generally process no more than input length of 1\u223c4k at a time. \n\nTo deal with this problem, a lot of research has been done to make the attention mechanism more efficient and have lower complexity, so called efficient Transformers [6]. Except for low-rank kernel optimizations such as Linformer [19], Performer [20], and down-sampling models such as Charformer [21], BP Transformer [22], a great branch of this research is about Transformers with sparse attention, which directly deletes some attention relations from the original attention computation, and makes the attention matrix appear sparse. \n\nIn these contributions to Transformers with sparse attention, LED [7] and ETC [8] concurrently propose sliding window local attention and a small-scale variant of full attention called global attention, which makes attention focus only on neighboring and selected tokens. BigBird [9] jointly uses local, global and random attention to cover as much useful information as possible over short and long distances, and achieves state-of-the-art results on many question answering and summarization tasks. LongT5 [10] proposes a modified local-global attention called transient global attention that dynamically constructs global tokens and discards them after the attention operation. However, a major challenge of these works is how to maintain efficiency and accuracy while scaling up the input length. Because in the scenarios of longer sequences with length of 16\u223c48k, they are either computationally intensive due to a linear increasing global memory (e.g. global attention), or informationally lossy because of a fixed pattern that only fits for common long texts of 8\u223c16k (e.g. local attention and random attention).",
            "score": 0.5781967028307609,
            "section_title": "II. RELATED WORK A. TRANSFORMERS WITH SPARSE ATTENTION",
            "char_start_offset": 5891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 990
                },
                {
                    "start": 993,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 354,
                    "end": 357,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 364,
                    "end": 367,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 374,
                    "end": 377,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 622,
                    "end": 625,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 702,
                    "end": 706,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 752,
                    "end": 756,
                    "matchedPaperCorpusId": "235624202"
                },
                {
                    "start": 1071,
                    "end": 1074,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 1273,
                    "end": 1276,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "245144820"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1541748046875
        },
        {
            "corpus_id": "273186571",
            "title": "Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective",
            "text": "Transformer-XL [83] adopts a segment-level recurrence mechanism and a novel positional encoding scheme to learn dependencies beyond a fixed length without disrupting temporal coherence. Linear Transformer [84] represents selfattention as a linear dot product of kernel feature maps and alters the computation order by leveraging the associativity of matrix multiplication. This modification reduces the complexity from O(L 2 ) to O(L), where L is the context length, significantly accelerating the computation of autoregressive Transformers. Another efficient structure is the Attention-Free Transformer (AFT) [85]. Unlike vanilla transformers, which first compute the query-key product, AFT combines the key and value with a set of learned positional biases before performing element-wise multiplication with the query. As a result, the memory complexity of AFT is linear with respect to both the context size and feature dimensions, enabling support for larger input lengths and model sizes. Based on AFT, the Receptance Weighted Key Value (RWKV) [86] combines the efficient parallel training capabilities of Transformers with the efficient inference of RNNs. It leverages linear attention mechanisms and allows the model to be expressed as either a transformer or an RNN. It also enables parallel computation during training while maintaining constant computational and memory complexity during inference. DiJiang [87] introduces a novel frequency-domain kernelization method based on the Discrete Cosine Transform (DCT). It points out that improving attention mechanisms often requires extensive retraining, which is impractical for large language models with vast numbers of parameters. This approach enables the conversion of a pre-trained standard Transformer into a model with linear complexity and low training costs, utilizing a weighted quasi-Monte Carlo method for sampling. Extensive experiments demonstrate that this method achieves performance comparable to the vanilla transformer while significantly reducing training costs and substantially increasing inference speed.",
            "score": 0.5770423008901057,
            "section_title": "Transformer-based LLM",
            "char_start_offset": 10176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2086
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2037353515625
        },
        {
            "corpus_id": "269033427",
            "title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention",
            "text": "This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.",
            "score": 0.5770326818965127,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18505859375
        },
        {
            "corpus_id": "268041490",
            "title": "Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling",
            "text": "In modern deep neural networks, attention mechanisms have emerged as a gold standard, pivotal in domains such as natural language processing, image, and audio processing, and even complex fields like biology [Vaswani et al., 2017, Dosovitskiy et al., 2020, Dwivedi and Bresson, 2020]. However, despite their strong sequence analysis capabilities, these sequence modeling mechanisms suffer from their high computational complexity, which scales quadratically with sequence length, hindering their application to long-context tasks. This complexity has driven a shift towards innovative solutions to overcome this computational barrier, enabling analysis of long sequences in areas like genomics, DNA sequencing, and the creation of long musical compositions. \n\nIn the past years, researchers have explored various strategies to tackle the computational bottleneck of traditional dense attention layers [Tay et al., 2022]. One key strategy involves sparsifying the dense attention matrix. Instead of calculating the entire matrix, Qiu et al. [2019], Parmar et al. [2018] focus on specific local blocks of the receptive fields of sequences by chunking them into fixed-size blocks. Moreover, Sparse Transformer [Child et al., 2019], Longformer [Beltagy et al., 2020] and BigBird [Zaheer et al., 2020] use strided attention patterns combined with local sliding windows to reduce computation. In contrast to using pre-determined patterns, other techniques include learning to cluster/sort tokens based on a similarity function, thereby enhancing the global view of the sequence, as seen in Reformer [Kitaev et al., 2020], Routing Transformer [Roy et al., 2020] Sparse Sinkhorn attention [Tay et al., 2020]. Another approach involves low-rank approximations of the self-attention matrix, leveraging the insight that these matrices often exhibit low-rank properties, as demonstrated by Linformer [Wang et al., 2020] which projects keys and values matrices to lowerdimensional representation matrices. Another paradigm to reduce quadratic computation cost, is to replace the dot-product similarity between keys and query matrices of attention mechanism with a kernel function and avoid explicitly computing the attention matrix [Katharopoulos et al., 2020].",
            "score": 0.5764957032467927,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2248
                }
            ],
            "ref_mentions": [
                {
                    "start": 901,
                    "end": 919,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1048,
                    "end": 1068,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 1275,
                    "end": 1296,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1593,
                    "end": 1614,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1636,
                    "end": 1653,
                    "matchedPaperCorpusId": "212718077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1195068359375
        },
        {
            "corpus_id": "271212416",
            "title": "SOFA: A Compute-Memory Optimized Sparsity Accelerator via Cross-Stage Coordinated Tiling",
            "text": "Remarkable success has been witnessed recently in the development of Transformer architecture [1], for both natural language processing (NLP) [2]- [10] and computer vision (CV) tasks [11]- [19].The impressive capabilities of Transformers greatly stems from their self-attention module, which excels at extracting global context information [20].Typically, selfattention modules take three matrices as their inputs: namely, Q (query), K (key) and V (value).First, an attention matrix A\u2208 R S\u00d7S is obtained by multiplying Q and K, where S is sequence length.Next, A goes through the softmax function for normalization, then is multiplied by V for the final output.\n\nLarge language models (LLMs) have driven the transformer architecture to unprecedented levels of complexity and capability, particularly in handling extended sequence lengths [21].This evolution places heightened demands on inference capabilities and throughput [22], critically impacting the performance of key transformer components: the attention module, feed-forward network (FFN) module, and the querykey-value (QKV) computations.\n\nTraditionally, in Transformers designed for smaller sequence lengths(\u22642k), the FFN module typically presented the main bottleneck due to its dense computational requirements.However, with recent advancements in processing long text, where sequence lengths can exceed 128,000 characters [23]- [25], the performance bottleneck is shifting from the FFN to the attention module.Our detailed profiling indicates that as sequence lengths surpass 32,000 characters, the attention module becomes the dominant factor affecting inference time, as shown in Fig. 1.This shift is primarily because the complexity of the attention mechanism scales quadratically with sequence length, making it increasingly challenging to manage as sequences extend.\n\nDynamic sparsity (DS) acceleration [26]- [33] have emerged as a promising solution to mitigate the latency issue of selfattention.The key idea is to predict vital Q-K pairs at runtime and calculate attention based on these vital pairs to reduce the inference latency.Typically, it consists of three stages.A precompute stage firstly estimates the matrix A (denoted as \u00c2).",
            "score": 0.576454133989461,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 194,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 456
                },
                {
                    "start": 456,
                    "end": 555
                },
                {
                    "start": 555,
                    "end": 661
                },
                {
                    "start": 663,
                    "end": 843
                },
                {
                    "start": 843,
                    "end": 1098
                },
                {
                    "start": 1100,
                    "end": 1274
                },
                {
                    "start": 1274,
                    "end": 1474
                },
                {
                    "start": 1474,
                    "end": 1653
                },
                {
                    "start": 1653,
                    "end": 1835
                },
                {
                    "start": 1837,
                    "end": 1967
                },
                {
                    "start": 1967,
                    "end": 2104
                },
                {
                    "start": 2104,
                    "end": 2143
                },
                {
                    "start": 2143,
                    "end": 2208
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 340,
                    "end": 344,
                    "matchedPaperCorpusId": "212996548"
                },
                {
                    "start": 1872,
                    "end": 1876,
                    "matchedPaperCorpusId": "211296403"
                },
                {
                    "start": 1878,
                    "end": 1882,
                    "matchedPaperCorpusId": "239016160"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.193359375
        },
        {
            "corpus_id": "232105052",
            "title": "Random Feature Attention",
            "text": "One common motivation across the following studies, that is shared by this work and the research we have already discussed, is to scale transformers to long sequences. Note that there are plenty orthogonal choices for improving efficiency such as weight sharing (Dehghani et al., 2019), quantization (Shen et al., 2020), knowledge distillation (Sanh et al., 2020), and adapters (Houlsby et al., 2019). For a detailed overview we refer the reader to Tay et al. (2020c). \n\nSparse attention patterns. The idea behind these methods is to limit the reception field of attention computation. It motivates earlier attempts in improving attention's efficiency, and still receives lots of interest. The sparse patterns can be set a priori (Liu et al., 2018;Qiu et al., 2020;Ho et al., 2020;You et al., 2020, inter alia) or learned from data (Sukhbaatar et al., 2019;Roy et al., 2020, inter alia). For most of these approaches, it is yet to be empirically verified that they are suitable for large-scale sequence-to-sequence learning; few of them have recorded decoding speed benefits. \n\nCompressed context. Wang et al. (2020) compress the context along the timesteps so that the effective sequence length for attention computation is reduced. Another line of work aims to store past context into a memory module with limited size (Lee et al., 2019;Ainslie et al., 2020;Rae et al., 2020, inter alia), so that accessing longer history only moderately increases the overhead. Reminiscent of RNN language models, RFA attends beyond a fixed context window through a stateful computation, without increasing time or memory overhead.",
            "score": 0.5761970063719207,
            "section_title": "RELATED WORK",
            "char_start_offset": 23461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1617
                }
            ],
            "ref_mentions": [
                {
                    "start": 262,
                    "end": 285,
                    "matchedPaperCorpusId": "49667762"
                },
                {
                    "start": 300,
                    "end": 319,
                    "matchedPaperCorpusId": "202565587"
                },
                {
                    "start": 378,
                    "end": 400,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 730,
                    "end": 748,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 748,
                    "end": 765,
                    "matchedPaperCorpusId": "207847640"
                },
                {
                    "start": 832,
                    "end": 857,
                    "matchedPaperCorpusId": "159041867"
                },
                {
                    "start": 1339,
                    "end": 1360,
                    "matchedPaperCorpusId": "221845203"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1407470703125
        },
        {
            "corpus_id": "263828829",
            "title": "Scaling Laws of RoPE-based Extrapolation",
            "text": "The second challenge is parallelism for sequence length, that is, Sequence Parallelism (SP) (Li et al., 2023c). Since the memory and calculation of a single device are limited, ColossalAI-SP (Li et al., 2023c) first proposes the segmentation and parallelism for the sequence dimension in addition to tensor parallelism for the hidden dimension and pipeline parallelism for model depth. On this basis, Ring Attention (Liu et al., 2023) uses blockwise self-attention to split long sequences into different devices and overlap the communication of key-value blocks. Besides, LightSeq (Li et al., 2023b) further improves the efficiency of long sequence modeling through load balancing for causal language modelings and a re-materialization-aware checkpointing strategy. Although the above sequence parallelism can achieve infinitely long sequence modeling, they ignore the compatibility with the existing efficient self-attention mechanisms such as FlashAttention (Dao et al., 2022;Dao, 2023) to achieve input with almost infinity. On the contrary, Megatron-SP (Korthikanti et al., 2022) only uses sequence parallelism during Dropout and Layernorm operations, thereby reducing activation redundancy. In addition, DeepSpeed-Ulysses (Jacobs et al., 2023) uses an alternative all-to-all collective communication gathering weight for attention computation when segmenting the sequence, avoiding communication overhead that originally increases with length. \n\nRecently, there have been a great deal of efforts devoted to solving the collapse of the performance beyond the training length and expanding the context length of RoPE-based LLMs to 16K, 32K, or even 100K (Chen et al., 2023;Su, 2023c;bloc97, 2023b). Considering that LLMs, such as LLaMA2 (Touvron et al., 2023b), have already acquired sufficient knowledge in the pre-training stage and demonstrated excellence in short-context tasks, the emerging extrapolation improvements have primarily focused on the fine-tuning and the testing phase. Initially, the context window is a well-discussed strategy for any LLM without further training.",
            "score": 0.5758763397795322,
            "section_title": "RELATED WORK",
            "char_start_offset": 18552,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1448
                },
                {
                    "start": 1451,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 110,
                    "matchedPaperCorpusId": "246017095"
                },
                {
                    "start": 191,
                    "end": 209,
                    "matchedPaperCorpusId": "246017095"
                },
                {
                    "start": 416,
                    "end": 434,
                    "matchedPaperCorpusId": "265551519"
                },
                {
                    "start": 960,
                    "end": 978,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1657,
                    "end": 1676,
                    "matchedPaperCorpusId": "265551519"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56689453125
        },
        {
            "corpus_id": "251223993",
            "title": "Momentum Transformer: Closing the Performance Gap Between Self-attention and Its Linearization",
            "text": "Transformers learn long-term dependencies in sequences effectively and concurrently through the self-attention mechanism. By denoting k(q i , k j ) := exp(q i k j / \u221a D), we can rewrite (1) as \n\nIn linear transformers (Wang et al., 2020;Katharopoulos et al., 2020;Choromanski and et al., 2021;Shen et al., 2021), the feature map k(q i , k j ) is linearized as the product of feature maps \u03c6(\u2022) on the vectors q i and k j , i.e., k(q i , k j ) = \u03c6(q i ) \u03c6(k j ). The associative property of matrix multiplication is then utilized to derive the following efficient computation of the attention map \n\n. \n\n(5) \n\nIn the matrix-product form, we can further write (5) as follows \n\nreduces the memory and computational cost of computing the attention map from O(N 2 ) to O(N ), making linear transformers scalable to very long sequences. Furthermore, causal masking can be easily implemented in the linearized attention by truncating the summation term in the last equation of (5), resulting in \n\nwhere \n\nThe states s i and z i can be computed in a recurrent fashion. \n\nEfficient inference via the RNN formulation. Self-attention processes tokens of a sequence concurrently, enabling fast training of transformers via layerwise parallelism. However, during inference, the output for timestep i is the input for timestep i + 1. As a result, the inference in standard transformers cannot be parallelized and is thus computationally inefficient. Linear transformers provide an elegant approach to fixing this issue by leveraging their RNN formulation. In particular, we can further write the linear attention with causal masking in (7) into the following RNN form1 \n\nwhere s 0 = 0 and z 0 = 0. Note that this RNN formulation of linear transformers with causal masking contains two memory states s i and z i .",
            "score": 0.5758763397795322,
            "section_title": "Linear Transformer",
            "char_start_offset": 9992,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 192
                },
                {
                    "start": 195,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 993
                },
                {
                    "start": 996,
                    "end": 1058
                },
                {
                    "start": 1061,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1652
                },
                {
                    "start": 1655,
                    "end": 1796
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 264,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 264,
                    "end": 293,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 293,
                    "end": 311,
                    "matchedPaperCorpusId": "215999966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.137939453125
        },
        {
            "corpus_id": "276585307",
            "title": "CQS-Attention: Scaling Up the Standard Attention Computation for Infinitely Long Sequences",
            "text": "Transformer models suffer from unaffordable high memory consumption when the sequence is long and standard self-attention is utilized. We developed a sequence parallelism scheme called CQS-Attention that can break the limit of sequence length. A long sequence is divided into multiple overlapping subsequences. The attention of each subsequence is independently computed and gathered as the final exact attention of the original long sequence. CQS-Attention is a fork-join parallel model comprising three components: Scheduler, Workers, and Tiler. The Scheduler equally partitions computation responsibility in a completely mutually exclusive manner and ensures the local subsequence length is minimum. Each worker independently computes the standard attention of the assigned subsequence and transfers local results to the Tiler, which produces the final attention. CQS-Attention makes attention computation embarrassingly parallel. Hence, it enjoys great performance regarding single-device memory and computation time consumption, mathematical stability and scalability. More importantly, it is fully compatible with all state-of-the-art attention optimizations. Our code and supplementary information (SI) are available at https://github.com/CQS-Attention/CQS_Attention.",
            "score": 0.5758763397795322,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83056640625
        },
        {
            "corpus_id": "268510227",
            "title": "DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers",
            "text": "Megatron-SP. Korthikanti et al. ( 2022) further optimizes activation usage in the attention based on tensor parallelism. \n\nTo transit between tensor parallelism and sequence parallelism in the transformer block, additional all-gather and reduce-scatter operations are introduced. But it's limited by the number of attention heads, as self-attention relies on the parallelism of the head dimension of the sequence. \n\nDeepSpeed-Ulysses. Jacobs et al. ( 2023) introduces an innovative approach for training long sequences by utilizing all-to-all collective communication. This method partitions the query, key, and value matrices across attention heads while preserving the original attention computation structure. The process is facilitated by two sets of all-to-all communications that alternate between sequence splitting and attention head splitting. Nevertheless, it is constrained by the number of attention heads as well. \n\nMoreover, these sequence parallelism methods are designed for parallelism within a single sequence dimension. For multi-dimensional transformers, this strategy becomes inefficient due to unnecessary communication. While specialized parallelism for multi-dimensional sequences has been explored in specific domains (Cheng et al., 2024), their applicability remains limited.",
            "score": 0.5758763397795322,
            "section_title": "Related Work",
            "char_start_offset": 8537,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 13,
                    "end": 120
                },
                {
                    "start": 123,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1301
                }
            ],
            "ref_mentions": [
                {
                    "start": 1243,
                    "end": 1263,
                    "matchedPaperCorpusId": "267789157"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.277587890625
        },
        {
            "corpus_id": "207930593",
            "title": "Compressive Transformers for Long-Range Sequence Modelling",
            "text": "There have been a variety of recent attempts to extend the range of attention, particularly in the Transformer, or to replace the attention operation with something less expensive. Wu et al. (2019) show that a convolution-like operator that runs in linear time can actually exceed the performance of the quadratic-time self-attention layer in the Transformer at sentence-to-sentence translation and sentence-level language modelling. However such a mechanism inhibits the flow of information across a large number of time-steps for a given layer, and has not shown to be beneficial for longrange sequence modelling. Dai et al. (2019) propose the TransformerXL, which keeps past activations around in memory. They also propose a novel relative positional embedding scheme which they see outperforms the Transformer's original absolute positional system. Our model incorporates both of these ideas, the use of a memory to preserve prior activations and their relative positional embedding scheme. \n\nThe Sparse Transformer (Child et al., 2019) uses fixed sparse attention masks to attend to roughly \u221a n locations in memory. This approach still requires keeping all memories around during training, however with careful re-materialization of activations and custom kernels, the authors are able to train the model with a reasonable budget of memory and compute. When run on Enwik8, the much larger attention window of 8, 000 improves model performance, but overall it does not significantly outperform a simpler TransformerXL with a much smaller attention window. \n\nThe use of dynamic attention spans is explored in Sukhbaatar et al. (2019). Different attention heads can learn to have shorter or longer spans of attention -and they observe this achieves state-ofthe-art in character-based language modelling. This idea could easily be combined with our contribution -a compressive memory. However an efficient implementation is not possible on current dense-linear-algebra accelerators, such as Google's TPUs, due to the need for dynamic and sparse computation. Our approach builds on simple dense linear algebra components, such as convolutions.",
            "score": 0.5757581170236427,
            "section_title": "RELATED WORK",
            "char_start_offset": 4641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 994
                },
                {
                    "start": 997,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2143
                }
            ],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 633,
                    "matchedPaperCorpusId": "57759363"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.103759765625
        },
        {
            "corpus_id": "248498407",
            "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation",
            "text": "Recent advances in efficient Transformers have exploited either the sparsity or low-rank properties of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences. However, it is still challenging to balance the trade-off between model quality and efficiency to perform a one-size-fits-all approximation for different tasks. To better understand this trade-off, we observe that sparse and low-rank approximations excel in different regimes, determined by the softmax temperature in attention, and sparse + low-rank can outperform each individually. Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, we propose Scatterbrain, a novel way to unify sparse (via locality sensitive hashing) and low-rank (via kernel feature map) attention for accurate and efficient approximation. The estimation is unbiased with provably low error. We empirically show that Scatterbrain can achieve 2.1x lower error than baselines when serving as a drop-in replacement in BigGAN image generation and pre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without fine-tuning, Scatterbrain can reduce 98% of attention memory at the cost of only 1% drop in accuracy. We demonstrate Scatterbrain for end-to-end training with up to 4 points better perplexity and 5 points better average accuracy than sparse or low-rank efficient transformers on language modeling and long-range-arena tasks.",
            "score": 0.575211106313578,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1710205078125
        },
        {
            "corpus_id": "273025630",
            "title": "Were RNNs All We Needed?",
            "text": "Over the past few years, Transformers (Vaswani et al., 2017) have been the dominant architecture in many areas, leading to advancements in tasks like machine translation (Devlin et al., 2019), text generation (Brown et al., 2020), and more. However, Transformers have a quadratic computational complexity in the sequence length, making them prohibitively expensive for long sequences, especially in low-resource settings. As such, numerous works have investigated the design of more efficient alternatives that achieve competitive performance with that of Transformers. Recently, there has been a renewed interest in recurrent sequence models that can be trained efficiently processing their context in parallel. These models (1) during training require only linear memory in the sequence length and (2) at inference time are rolled out recurrently token-by-token, requiring only constant memory. As a result, these models can scale to significantly longer sequences than Transformers1 . \n\nA family of efficiently trainable recurrent sequence models that has recently gained much traction is that of state-space models, specifically the recently proposed Mamba (Gu & Dao, 2024). Mamba (S6) is a state-space model that differentiates itself from prior works by leveraging input-dependent transitions. The recent success of Mamba and the proposals of many new variants of state-space models has led to several survey papers (Wang et al., 2024;Patro & Agneeswaran, 2024;Qu et al., 2024). Another extensively explored group of methods is those based on attention. Peng et al. (2023) proposed a linear attention model that can be written recurrently while being trained in parallel. Feng et al. (2024) showed that softmax attention (and Transformers) can be viewed as a recurrent neural network (RNN). Building on their RNN formulation of attention, they proposed Aaren, a softmax attention model, that can be computed in parallel for efficient training or unrolled sequentially as an RNN for efficient inference. Although many recurrent models have been proposed with vastly different architectures, these recent state-of-the-art methods are all efficiently trainable using the same algorithm -the parallel prefix scan algorithm (Blelloch, 1990).",
            "score": 0.574935979623479,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2242
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 190,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 209,
                    "end": 229,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1441,
                    "end": 1467,
                    "matchedPaperCorpusId": "269362649"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275390625
        },
        {
            "corpus_id": "236975917",
            "title": "Adaptive Multi-Resolution Attention with Linear Complexity",
            "text": "The recent emergence of the Transformer has drastically reshaped the landscape of natural language processing research. Transformers have demonstrated superior performance in a wide variety of tasks, such as machine translation [1], text classification [2], question answering [3], automatic speech recognition [4], image generation [5], and image captioning [6]. The key innovation in Transformers is the introduction of a multi-head self-attention mechanism, which models the pairwise interactions between all positions in the sequence, regardless of their distance from each other. This operation has been shown quite effective.\n\nNonetheless, despite several notable successes of Transformers, computing the attention matrix, which is their key component, also turns out to be a major efficiency bottleneck due to its quadratic time and space complexity with respect to the sequence length. Therefore, the maximum sequence length is restricted by the amount of memory available. This inherent limitation of Transformers has prevented them from being successfully applied to domains requiring longer sequence lengths, like document classification. Furthermore, building large Transformer-based models in practice takes an enormous amount of time. Although fine-tuning pre-trained Transformers \u00a7 Equal contribution  Table III for more details. is relatively inexpensive, the memory issue still restricts the scenarios in which these models can be used. Besides the computational cost, qualitative analysis of attention heads [1] suggests that heads tend to favor flatter or more peaked distributions, depending on what phenomena they capture. Thus, using an extremely long sequence may limit the power of the model. To this end, a wide spectrum of efficient, fast Transformers has been proposed to tackle these limitations. For instance, [7]- [12] addresses the problematic complexity by limiting the number of keys that each query attends to. However, these methods either break long-term dependency or hurt time efficiency. There is also a long line of research on using dense attention matrix but defined by low-rank kernels substituting softmax [13]- [16]. Although these approaches have achieved better speed-memory-accuracy trade-off, they still suffer from the aforementioned limitations of the selfattention mechanism. Another prominent line of work is to increase the memory capacity [17]- [19]. However, these works still suffer from problematic complexity issues.\n\nBesides the computational cost, the self-attention mechanism of all models mentioned",
            "score": 0.5736192325001749,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 231,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 277,
                    "end": 280,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 311,
                    "end": 314,
                    "matchedPaperCorpusId": "52287921"
                },
                {
                    "start": 333,
                    "end": 336,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 359,
                    "end": 362,
                    "matchedPaperCorpusId": "1055111"
                },
                {
                    "start": 1526,
                    "end": 1529,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1844,
                    "end": 1848,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 2150,
                    "end": 2154,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 2156,
                    "end": 2160,
                    "matchedPaperCorpusId": "232105052"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.093017578125
        },
        {
            "corpus_id": "237260051",
            "title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer",
            "text": "Transformer (Vaswani et al., 2017) has achieved great success in NLP by serving as the basic architecture of many popular models such as BERT (Devlin et al., 2019), RoBERTa (Radford et al., 2019) and XLM (Conneau and Lample, 2019). In addition, Transformers also show great potentials in other fields like computer vision (Dosovitskiy et al., 2021) and speech recognition (Dong et al., 2018) to understand data in other modalities. Self-attention is the core of Transformer (Parikh et al., 2016). It aims to model the interaction between each pair of tokens to help capture their contexts, where the   computational complexity is quadratic with respect to the input sequence length (Vaswani et al., 2017). Thus, Transformer is inefficient in handling long sequences (Tay et al., 2020). \n\nAn intuitive way to improve the efficiency of Transformer is computing a sparse self-attention matrix to reduce the number of tokens to be attended (Child et al., 2019). For example, Beltagy et al. (2020) proposed Longformer, which computes sliding window attention to capture local contexts and global attention at a few positions to capture global contexts. Zaheer et al. (2020) proposed BidBird, which further incorporates random attention that models the interactions between each token with a certain number of randomly selected tokens. However, attending to the tokens that are randomly sampled or selected by certain rules may not be actually helpful for context modeling. \n\nInstead of attending to the heuristically or randomly selected tokens, attending to those potentially important tokens may help build higherquality sparse attention for text modeling. Fortunately, these potentially important tokens can be efficiently and effectively identified by a tiny Transformer model with very low dimensions. Fig. 1 show the attention heatmaps learned by a tiny arXiv:2108.09193v3 [cs.CL] 2 Sep 2021 \n\nTransformer with 4 hidden dimensions and a standard 256-dim Transformer. We find the attention heatmap produced by the low-dimensional Transformer is very similar to the heatmap learned by a Transformer with a much larger size.",
            "score": 0.5734489346981889,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 785
                },
                {
                    "start": 788,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1467
                },
                {
                    "start": 1470,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 34,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 142,
                    "end": 163,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 173,
                    "end": 195,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 204,
                    "end": 230,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 322,
                    "end": 348,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 474,
                    "end": 495,
                    "matchedPaperCorpusId": "8495258"
                },
                {
                    "start": 682,
                    "end": 704,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1368408203125
        },
        {
            "corpus_id": "272689550",
            "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval",
            "text": "Sparse Transformers. Since the quadratic complexity of attention has become the bottleneck of LLM efficiency for long context applications, numerous works have studied to design sparse transformers to reduce the computational and memory complexity of the self-attention mechanism. Some works restrict the attention computation to predefined patterns, including sliding windows (Child et al., 2019), dilated windows (Beltagy et al., 2020), or a mixture of different patterns (Zaheer et al., 2020;Ainslie et al., 2020). Some approaches use cluster-based sparsity based on hash value (Kitaev et al., 2020) or KNN algorithms (Bertsch et al., 2024;Mao et al., 2024). These solutions either require pre-training a model from scratch or target limited scenarios like CPU-only, which do not work for our target to out-of-box usage of LLMs on the GPU-CPU architecture. Although some approaches (Xiao et al., 2024a;Ribar et al., 2024) exploit the dynamic sparse nature of LLMs, they often use some estimation using low-rank hidden states or post-statistical approaches, which incurs high overhead but with low accuracy. Moreover, all these approaches have to maintain full KV vectors on GPU with only accelerated inference by reduced memory movement, which does not solve the challenge of commodity GPUs with limited GPU memory. \n\nAdditionally, some approaches accelerate the inference by employing dynamically sparse attention patterns (Jiang et al., 2024), separating the prefill and decoding stages (Zhong et al., 2024;Qin et al., 2024), and utilizing sequence parallelism (Jacobs et al., 2023;Liu et al., 2024a). These methods are orthogonal to ours and can be in conjunction with our approach.",
            "score": 0.5733494154171599,
            "section_title": "D ADDITIONAL RELATED WORK",
            "char_start_offset": 37638,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1688
                }
            ],
            "ref_mentions": [
                {
                    "start": 474,
                    "end": 495,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 495,
                    "end": 516,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 581,
                    "end": 602,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 621,
                    "end": 643,
                    "matchedPaperCorpusId": "258436892"
                },
                {
                    "start": 643,
                    "end": 660,
                    "matchedPaperCorpusId": "269605811"
                },
                {
                    "start": 905,
                    "end": 924,
                    "matchedPaperCorpusId": "266149707"
                },
                {
                    "start": 1492,
                    "end": 1512,
                    "matchedPaperCorpusId": "267034664"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2861328125
        },
        {
            "corpus_id": "259187506",
            "title": "Block-State Transformers",
            "text": "Transformers have shown impressive performance on a wide range of natural language processing (NLP) tasks. While they have been primarily used for language modeling the Transformer architecture [40] has also been successfully applied to other tasks outside of the NLP and have mostly replaced Recurrent Neural Networks (RNNs). Several factors contribute to this success, including computational efficiency and architectural inductive biases that are well-suited for training on natural language tasks at scale. On the computational upside, Transformers are able to process tokens of a given input sequence in parallel, making the most of modern accelerator hardware. Moreover, the attention mechanism enables Transformers to find relationships in longer sequences by providing ready access to all the extracted information from past tokens when inferring the next token. Compared to RNNs and LSTMs [19], the benefits of self-attention are two-fold: (i) the capacity of what could be stored and directly accessible as context is drastically increased, and (ii) training on longer sequences is more stable [18,23]. \n\nGiven the remarkable achievements of Transformers in language modeling tasks, and their improved performance at scale on hard NLP tasks such as reasoning and question answering [2,39,6], the demand for deploying even deeper and larger networks is greater than ever before. An orthogonal scaling dimension, which could be potentially even more consequential, is the size of the input sequence. Despite the several advantages of Transformers over RNNs, it is still problematic to scale the input sequence length, again for both computational performance and quality reasons. Further, the Transformer's runtime is quadratic with respect to the input sequence length, which makes training these models increasingly expensive. Furthermore, Transformers with attention, that is local [8], sparse [4,43,36], low-rank approximated [41] or linearized via kernel methods [5,22], notoriously struggle on long-input classification tasks [37]. Vanilla transformers can be unstable when trained on long sequences [26] and token importance is concentrated in a local receptive field of around 50 tokens around the current time step [35].",
            "score": 0.5724550408118958,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1112
                },
                {
                    "start": 1115,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2237
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 898,
                    "end": 902,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1104,
                    "end": 1108,
                    "matchedPaperCorpusId": "18452318"
                },
                {
                    "start": 1292,
                    "end": 1295,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1908,
                    "end": 1911,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1911,
                    "end": 1914,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 1979,
                    "end": 1982,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32958984375
        },
        {
            "corpus_id": "256080791",
            "title": "ATP: Adaptive Tensor Parallelism for Foundation Models",
            "text": "Transformer has become the bedrock of foundation models due to their excellent ability to model sequences. Taking a language model as an example, the input tokens are first fed into an embedding layer and then passed through a singlestack transformer encoder or decoder with the L layers (see Figure 1). The input of transformer layer is a 3D tensor of size [b, s, h] where b, s, h are batch, sequence and hidden-size dimensions. Each transformer layer consists of a multi-head attention (MHA) block followed by a feed-forward block. In the MHA block, the sequence data is fed into three different MLP layers to obtain Query(Q), Key(K), Value(V). Then divided into a heads, each with a hidden size of d = h/a. For each head, the attention score is calculated using Formula 1. The feed-forward block has two layers of multi-layer perceptron (MLP). The first layer increases the hidden size to 4h and the second layer reduces it back to h. \n\nTensor Parallelism, which is commonly used for training foundation models, was proposed by Megatron-LM [15]. As shown in Figure 1, tensor parallelism parallelizes the MHA and feed-forward blocks. In the feed-forward block, there are two MLP layers and an activation function (GeLU): \n\nwhere A and B are the weight of two MLP layers. We can split A by columns and B by rows and parallelize the computation into two parts: \n\nMHA blocks can be parallelized in a similar way by dividing the weight matrix of Q, K,V by columns and the output linear layer by rows. \n\nThis approach introduces two all-reduce operations in both the forward and backward passes of each layer, distributing computation and memory footprints across devices. Inside the MHA and feed-forward blocks, tensor parallelism parallelizes model parameters, optimizer state, and activations. The Lay-erNorm, including the input/output activations, is duplicated in each tensor-parallel worker. \n\nSeveral recent works, such as 2D tensor parallelism [32] use the Scalable Universal Matrix Multiplication Algorithm (SUMMA) [29].",
            "score": 0.572038156565181,
            "section_title": "Tensor Parallelism on Transformer",
            "char_start_offset": 4891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 937
                },
                {
                    "start": 940,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1222
                },
                {
                    "start": 1225,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1360
                },
                {
                    "start": 1363,
                    "end": 1498
                },
                {
                    "start": 1501,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1895
                },
                {
                    "start": 1898,
                    "end": 2027
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.132080078125
        },
        {
            "corpus_id": "246017095",
            "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
            "text": "We analyzed and compared our SP with TP in both theoretical modeling and experiments, although this is not our direct baseline. To our best knowledge, SP is the first system designed for breaking the length limitation of sequence, so there is actually no direct baseline. Therefore, as a distributed training system designed for attention-based models, we compare with a SoTA model parallelism. TP [Narayanan et al., 2021] is compatible with DP, PP. Our SP is compatible with them. We expect our system can outperform TP with and without PP. In the future, we will integrate SP with DP, PP and TP into 4D parallelism.\n\nHere, we mainly focus on memory usage and communication cost. According to the architecture of Transformer, the comparison is divided into two parts, MLP block and attention block. In this part, we consider multi-head attention instead of self-attention for a fair and accurate comparison. We assume the optimizer is Adam used in Megatron.   Table 1, for the MLP blocks, TP stores the matrices after row or column-style splitting of the whole sequence. Our SP stores the matrices without row or column-style splitting of only one single sub-sequence on each GPU. If we assume that our sequence parallelism is more memory-efficient:\n\nWe can find that, in MLP blocks, SP is more memory-efficient when BL > 32H.\n\nAs for communication, an all-reduce operation is needed in both the forward pass and backward pass in the MLP block of Megatron due to tensor splitting. As our SP does not split the linear layer weights, no additional communication is required.",
            "score": 0.5713079399812329,
            "section_title": "Modelling",
            "char_start_offset": 9398,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 398,
                    "end": 422,
                    "matchedPaperCorpusId": "233289729"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.681640625
        },
        {
            "corpus_id": "269982033",
            "title": "Attention as an RNN",
            "text": "The advent of Transformers marked a significant breakthrough in sequence modelling, providing a highly performant architecture capable of leveraging GPU parallelism. However, Transformers are computationally expensive at inference time, limiting their applications, particularly in low-resource settings (e.g., mobile and embedded devices). Addressing this, we (1) begin by showing that attention can be viewed as a special Recurrent Neural Network (RNN) with the ability to compute its \\textit{many-to-one} RNN output efficiently. We then (2) show that popular attention-based models such as Transformers can be viewed as RNN variants. However, unlike traditional RNNs (e.g., LSTMs), these models cannot be updated efficiently with new tokens, an important property in sequence modelling. Tackling this, we (3) introduce a new efficient method of computing attention's \\textit{many-to-many} RNN output based on the parallel prefix scan algorithm. Building on the new attention formulation, we (4) introduce \\textbf{Aaren}, an attention-based module that can not only (i) be trained in parallel (like Transformers) but also (ii) be updated efficiently with new tokens, requiring only constant memory for inferences (like traditional RNNs). Empirically, we show Aarens achieve comparable performance to Transformers on $38$ datasets spread across four popular sequential problem settings: reinforcement learning, event forecasting, time series classification, and time series forecasting tasks while being more time and memory-efficient.",
            "score": 0.5712542501546892,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2132568359375
        },
        {
            "corpus_id": "271916182",
            "title": "Macformer: Transformer with Random Maclaurin Feature Attention",
            "text": "One common motivation shared across several studies is the need to scale transformers to process long sequences efficiently. To address this limitation, various approaches have been proposed. \n\nSoftmax-Free Linear Transformers (Lu et al. 2024) aim to approximate self-attention at linear complexity by replacing the dot-product similarity with a Gaussian kernel function. This innovation allows for a full self-attention matrix to be approximated under low-rank matrix decomposition. Linformer (Wang et al. 2020) approximates the selfattention mechanism with a low-rank matrix, reducing its complexity from O(n 2 ) to O(n). Methods such as Swin Transformer (Liu et al. 2021) have explored shifted windowbased self-attention to reduce computational costs. However, while Swin Transformer addresses some of the challenges associated with traditional transformers, it may still struggle when training on small datasets. ESwin Transformer (Yao and Shao 2023) further improves upon this by redesigning the modules of Swin Transformer and introducing simple convolutional components, enhancing the model's performance on small datasets. \n\nSparse attention patterns have been another area of focus for optimizing transformer models (Qiu et al. 2019;Ho et al. 2019) by limiting the reception field of attention computation. Efficient Transformer models in domains such as speech recognition (Luo 2023), audio tagging (Schmid, Koutini, and Widmer 2023), and remote sensing image segmentation (Xu et al. 2021) often leverage techniques like knowledge distillation, quantization, and sparse attention (Child et al. 2019) patterns. Informer (Zhou et al. 2020) introduces the ProbSparse self-attention mechanism and selfattention distillation techniques, reducing time complexity and memory usage to O(n log n). \n\nRandom feature attention (RFA) (Peng et al. 2021) proposes reducing computational costs by decomposing Softmax into linear operations. This method uses random feature map (Rahimi and Recht 2007) to approximate the attention.",
            "score": 0.5710476012505772,
            "section_title": "Related Work",
            "char_start_offset": 2889,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1798
                },
                {
                    "start": 1801,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2025
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 242,
                    "matchedPaperCorpusId": "250334278"
                },
                {
                    "start": 935,
                    "end": 954,
                    "matchedPaperCorpusId": "260363331"
                },
                {
                    "start": 1383,
                    "end": 1393,
                    "matchedPaperCorpusId": "266480229"
                },
                {
                    "start": 1409,
                    "end": 1443,
                    "matchedPaperCorpusId": "253420462"
                },
                {
                    "start": 1483,
                    "end": 1498,
                    "matchedPaperCorpusId": "238244662"
                },
                {
                    "start": 1629,
                    "end": 1647,
                    "matchedPaperCorpusId": "229156802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1275634765625
        },
        {
            "corpus_id": "272969212",
            "title": "Cottention: Linear Transformers With Cosine Attention",
            "text": "Transformer models have achieved unprecedented success in various applications ranging from natural language processing to computer vision [1,2,3,4]. Central to these model's capability is the attention mechanism, a powerful computation that allows models to adapt representations based on focused context of the entire sequence [5,6]. However, the attention mechanism's expressiveness comes at the cost of computation as the sequence length increases, due to the quadratic complexity in both time and memory of the softmax operation [7,8]. This limitation has spurred interest in developing more efficient attention mechanisms that can handle longer sequences without such a steep cost. \n\nSeveral attempts have been made to address this issue, including the introduction of sub-quadratic time architectures like linear attention [9,10,11], gated convolutional and recurrent models [12,13], and structured state space models (SSMs) [14,15,16]. While these efforts have yielded improvements in computational efficiency, they often fall short in matching the performance of traditional attention mechanisms on key tasks, particularly in the domain of language processing. A critical examination of these models reveals that their primary limitation could be diminished capacity for content-based reasoning [7], although additional investigation is needed to fully examine the performance gap. \n\nRecent works have explored the potential of alternative similarity measures to replace the softmax operation in attention mechanisms [17,10,18]. These approaches have shown promising results in reducing computational complexity while maintaining competitive performance. Other works have also investigated the use of cosine similarity in various contexts [19,20,21,22,23]. However, for each of these, their application has been limited to specific domains or doesn't explore optimal stabilization techniques. \n\nIn this work, we propose a novel attention mechanism, Cottention (Cosine Attention), which leverages the properties of cosine similarity to achieve linear complexity concerning sequence length. We generalize cosine attention to work with arbitrary-length sequences and apply it to the text domain, demonstrating its effectiveness on a range of language tasks. Our approach addresses the stability issues encountered in previous works without the need for additional constraints or modifications [24]. Additionally, we show that although cosine attention is linear with respect to sequence length, it retains similar accuracy to softmax attention.",
            "score": 0.5709279630669932,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 687
                },
                {
                    "start": 690,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1390
                },
                {
                    "start": 1393,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1901
                },
                {
                    "start": 1904,
                    "end": 2097
                },
                {
                    "start": 2098,
                    "end": 2263
                },
                {
                    "start": 2264,
                    "end": 2404
                },
                {
                    "start": 2405,
                    "end": 2550
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 146,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 534,
                    "end": 537,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 537,
                    "end": 539,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 830,
                    "end": 833,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 882,
                    "end": 886,
                    "matchedPaperCorpusId": "16119010"
                },
                {
                    "start": 936,
                    "end": 939,
                    "matchedPaperCorpusId": "239998472"
                },
                {
                    "start": 939,
                    "end": 942,
                    "matchedPaperCorpusId": "247762199"
                },
                {
                    "start": 1304,
                    "end": 1307,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1752,
                    "end": 1755,
                    "matchedPaperCorpusId": "253510640"
                },
                {
                    "start": 1755,
                    "end": 1758,
                    "matchedPaperCorpusId": "1505432"
                },
                {
                    "start": 1761,
                    "end": 1764,
                    "matchedPaperCorpusId": "244346076"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09686279296875
        },
        {
            "corpus_id": "231934213",
            "title": "TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models",
            "text": "A growing set of works start to focus on increasing the input sequence length of the Transformers (Tay et al., 2020;Zaheer et al., 2020;Kitaev et al., 2020). Long sequence length enables Transformers to reason about long-term dependencies and thus extends its applicability to more complex applications such as modeling documents. However, longer sequences increases the memory usage of a single input sequence, and decreases the maximum batch size allowed, which limits the pipeline efficiency of previous microbatchbased pipeline parallelism methods. \n\nIn this subsection, we vary the sequence length from 2048 to 8192 for the GPT3-13B model (setting ( 5)) and evaluate the training iteration latency. Because of the growth in memory usage, the batch sizes for sequence length 4096, 6144, 8196 are reduced to 8, 4, 2, respectively. We show the results in Figure 7. TeraPipe achieves 2.76x, 4.97x, 7.83x speedup for sequence length 4096, 6144, and 8196, respectively. As the sequence length grows, the gap between the performance with and without TeraPipe significantly increases, as expected. Meanwhile, longer sequence length provides more space on the token dimension and thus TeraPipe can perform even better -TeraPipe enables efficient training of future-emerging LMs with growing sequence lengths.",
            "score": 0.5707885331620706,
            "section_title": "Longer Sequence Length",
            "char_start_offset": 29598,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1304
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33447265625
        },
        {
            "corpus_id": "259858862",
            "title": "Adaptive Attention for Sparse-based Long-sequence Transformer",
            "text": "Different from traditional sparse Transformer (Martins and Astudillo, 2016;Correia et al., 2019;Peters et al., 2019) with different softmax and pattern-related quadratic computation, recent works mainly adopt sliding windows to achieve linear complexity. For example, Longformer (Beltagy et al., 2020) employs an attention pattern that combines local windowed attention with task-motivated global attention while also scaling linearly with the sequence length. BigBird (Zaheer et al., 2020) incorporates random attention (queries attend to random keys) besides global tokens and local sliding windows. However, these hand-crafted attention patterns mentioned above are usually selected empirically or randomly. It is not an ideal solution for modeling long sequences. How to adaptively select useful tokens for sparse attention according to the context is still an important problem to be considered. \n\nTo address these issues, we propose A 2 -Former with adaptive attention to model longer sequences in this paper. It can select useful tokens automatically in sparse attention by learnable position vectors, which consist of meta position and offset position vectors. Because each element in the learnable offset position vector is not an integer, we utilize linear interpolation to gather discrete vectors from original the input embedding matrix. Position visualization further shows that traditional attention patterns are not enough to cover the valuable positions automatically selected by models. Experiments on Long Range Arena, a systematic and unified benchmark with different tasks, show that our model has achieved further improvement in performance compared with other sparse-based Transformers. \n\nOverall, the main contributions are as follows: \n\n\u2022 We propose a novel efficient Transformer, A 2 -Former, which replaces hand-crafted attention patterns with learnable adaptive attention in sparse attention. Besides, position visualization (Figure 3) further shows that traditional attention patterns are not enough to cover the useful positions automatically selected by models. \n\n\u2022 We adopt an interpolation technique to help the model gather discrete positions with a continuous weight matrix. By combining the meta position and generated offset position, the position of tokens can be selected dynamically according to the context. \n\n\u2022 Experiments on different long sequence tasks validate the effectiveness of our model.",
            "score": 0.570670202166112,
            "section_title": "Introduction",
            "char_start_offset": 1517,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1708
                },
                {
                    "start": 1711,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2091
                },
                {
                    "start": 2094,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2347
                },
                {
                    "start": 2350,
                    "end": 2437
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 75,
                    "matchedPaperCorpusId": "16432551"
                },
                {
                    "start": 75,
                    "end": 96,
                    "matchedPaperCorpusId": "202538495"
                },
                {
                    "start": 96,
                    "end": 116,
                    "matchedPaperCorpusId": "153313159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.148681640625
        },
        {
            "corpus_id": "258840932",
            "title": "WavSpA: Wavelet Space Attention for Boosting Transformers' Long Sequence Learning Ability",
            "text": "There has been plenty of prior work to enable transformers to handle long input more effectively and efficiently. Since the inefficiency comes from the quadratic dependency on sequence length because of the dense attention operation, a large portion of research simulates the attention operation with certain approximations, for example, replacing the dense attention matrix with a sparse version, or assume that it satisfies certain low-rank structures. We briefly review some methods on this topic in this section. For a more detailed survey, we refer the readers to [36]. \n\nSparse Attention. Perhaps the most intuitive solution to alleviate the quadratic cost, Sparse Attention only calculates a portion of the full n 2 attention matrix. Early stage methods include Local Attention [27] and Multi-passage BERT [41] use sliding windows or chunked blocks to speed up computation. Longformer [1] and BigBird [46] further combine global attention, sliding window attention, dilated sliding window attention, and random attention together to form strong sparse attention mechanisms, and BigBird showed that their method is a universal approximator of sequence functions. On the other front, Orthogonal Transformer [13] utilizes an iterative approach to construct an orthogonal vector basis in Euclidean space, then perform windowed attention on grouped tokens after orthogonal projection. \n\nLow-rank Approximation. The self-attention matrix, at the center of transformer, has been found to display low-rank behaviors after pre-training. Linformer [40] performed spectrum analysis on the pre-trained attention matrix, and the results indicate that the top 128 singular values composite 88%-96% of the entire 512 singular values across attention heads and layers. Based on this observation, Linformer added low-rank projection matrices in attention to approximate the original attention matrix. On a similar notion, Drone [4] extended the low-rank approximation scope to all matrices in transformer via data-driven optimal compression.",
            "score": 0.5693542420168097,
            "section_title": "Attention Methods",
            "char_start_offset": 23458,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2031
                }
            ],
            "ref_mentions": [
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 785,
                    "end": 789,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "201307832"
                },
                {
                    "start": 908,
                    "end": 912,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1212,
                    "end": 1216,
                    "matchedPaperCorpusId": "258509623"
                },
                {
                    "start": 1918,
                    "end": 1921,
                    "matchedPaperCorpusId": "245003869"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15185546875
        },
        {
            "corpus_id": "250113485",
            "title": "SALO: an efficient spatial accelerator enabling hybrid sparse attention mechanisms for long sequences",
            "text": "Nowadays, models based on Transformer [13] have achieved an extremely high performance in deep learning research. In the area of natural language processing, Transformer and its variants, like BERT [5] and GPT-3 [3], have outperformed other models based on RNN and CNN in most tasks. Inspired by the great ability that transformers have shown in NLP tasks, researchers have started to migrate the network to other fields like computer vision and recommender system recently. Vision Transformer (ViT) [6] is one of the representative works which apply the transformer directly to sequences of image patches. \n\nTransformer and its variants achieve state-of-the-art results by capturing contextual information from the entire sequence using the self-attention mechanism. Each self-attention block takes three matrices, namely  (query),  (key), and  (value), as its inputs. These three matrices will be used to capture the relation between tokens in a given sequence. The computation of attention includes two matrix multiplications, a scaling operation and a softmax operation. The two matrix multiplications introduce the quadratic computational complexity with respect to the sequence length. As transformers are being extensively applied in many areas, the model size increases greatly, together with the lengths of input sequences. For example, BERT-base [5] and GPT-3 [3] have 110 million and 175 billion parameters, respectively. Due to the quadratic complexity of self-attention w.r.t the sequence length, the model complexity is further exacerbated in scenarios with long input sequences. Such considerable workload brings about heavy computational and memory burdens, making it difficult to train and deploy these models, especially for tasks with long input sequences. \n\nTo remedy this issue, several models with hybrid sparse attention mechanisms are proposed to handle long sequences, such as Longformer [2] (up to 16384 tokens in a sequence) and ViL [16] (up to 96 \u00d7 96 patches in an image). They hybridize two variants of the attention mechanism and perform the computation of a local window attention and a task-motivated global attention. Such a hybrid sparse attention mechanism successfully reduce the complexity of attention to a linear level. This linear complexity significantly alleviates the memory burden, making it possible to train models with long sequences.",
            "score": 0.5692881565761575,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1775
                },
                {
                    "start": 1778,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2259
                },
                {
                    "start": 2260,
                    "end": 2382
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 42,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259765625
        },
        {
            "corpus_id": "271709558",
            "title": "BioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba",
            "text": "The Mamba model is a state-of-the-art sequence model designed to address the computational inefficiencies of Transformers on long sequences [16]. Traditional Transformers [32] rely heavily on the attention mechanism, which, while powerful, suffers from quadratic complexity in sequence length. This makes Transformers computationally expensive and memory-intensive, particularly for long sequences. To overcome these limitations, Mamba leverages structured state space models (SSMs) with parameters that are functions of the input. \n\nSSMs offer a compelling alternative to attention mechanisms by providing a framework for modeling sequences with linear complexity in sequence length. This makes them more efficient for handling long sequences. Additionally, SSMs are well-suited for content-based reasoning, which is crucial for discrete modalities such as language. The core innovation of Mamba lies in its ability to selectively propagate or forget information along the sequence length dimension based on the current token. This is achieved by making the SSM parameters dynamic and dependent on the input sequence. Mathematically, this can be represented as: \n\nwhere h t is the hidden state at time step t, x t is the input token at time step t, and \u03b8(x t ) represents the dynamic parameters of the SSM that are functions of the input token x t . Despite the dynamic nature of the SSM parameters, which prevents the use of efficient convolutions, Mamba employs a hardware-aware parallel algorithm in recurrent mode to maintain computational efficiency. This design choice ensures that Mamba can achieve fast inference with linear scaling in sequence length. \n\nFurthermore, Mamba integrates these selective SSMs into a simplified end-to-end neural network architecture that does not rely on attention mechanisms or even multi-layer perceptron (MLP) blocks. This streamlined architecture contributes to Mamba's high throughput and scalability. \n\nIn summary, the Mamba model offers a novel approach to sequence modeling by combining the strengths of SSMs with dynamic parameterization and hardware-aware parallelism. By addressing the inefficiencies of Transformers and leveraging the advantages of SSMs, Mamba achieves state-of-the-art performance across various modalities, including language, audio, and genomics, while maintaining computational efficiency.",
            "score": 0.5692716295800793,
            "section_title": "Mamba Model",
            "char_start_offset": 13474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 531
                },
                {
                    "start": 534,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1162
                },
                {
                    "start": 1165,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1661
                },
                {
                    "start": 1664,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1945
                },
                {
                    "start": 1948,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2361
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 175,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3447265625
        },
        {
            "corpus_id": "256105131",
            "title": "AQuaMaM: An Autoregressive, Quaternion Manifold Model for Rapidly Estimating Complex SO(3) Distributions",
            "text": "In this section, I briefly describe the Transformer (Vaswani et al., 2017) architecture at a high level, and define the Transformer-specific terms used in the main text. Readers who seek a deeper understanding of Transformers are encouraged to explore the following excellent pedagogical materials: To accomplish this parallelism, Transformers use the attention mechanism (Graves, 2013;Graves et al., 2014;Weston et al., 2015;Bahdanau et al., 2015). 23 Intuitively, the attention mechanism is a function that tells a neural network how much to \"focus\" on the different elements of a sequence when processing a specific element of that sequence. The pure attention approach to sequence modeling provides two main benefits: (1) it can greatly speed up training because sequences are processed in parallel through the network, and (2) long-term dependencies are much easier to model because distant elements of the sequence do not need to communicate through many neural network layers (which is the case for RNNs). \n\nWhile the attention mechanism is extremely powerful, it is also permutation invariant. As a result, Transformers must encode positional information and conditional dependencies through other means, i.e., with position embeddings and attention masks. Consider the following sequences of word embeddings:24 \n\nBecause the attention mechanism is permutation invariant, W 1 and W 2 look identical to a Transformer. Therefore, to encode the order of the word embeddings, the Transformer adds a position embedding t i to each word embedding where i indicates the word embedding's position in the sequence. As a result, W 1 becomes: \n\nand the attention mechanism would be applied to W 1 and W 2 . \n\nIn order to fully understand how Transformers model conditional dependencies, additional technical details about the attention mechanism are necessary. The basic attention mechanism in Transformers consists of two architectural pieces: (1) a query/key/value function \u03c6 and (2) a score function \u03c8. The query/key/value function \u03c6 independently maps each element of a (position encoded) sequence W[i] to a query, key, and value vector, i.e.:",
            "score": 0.5687985347063886,
            "section_title": "A.3 BACKGROUND ON TRANSFORMERS",
            "char_start_offset": 29051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1319
                },
                {
                    "start": 1322,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1639
                },
                {
                    "start": 1642,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2144
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 74,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 406,
                    "end": 426,
                    "matchedPaperCorpusId": "2926851"
                },
                {
                    "start": 426,
                    "end": 448,
                    "matchedPaperCorpusId": "11212020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2135009765625
        },
        {
            "corpus_id": "268032287",
            "title": "Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study",
            "text": "The self-attention mechanism in Transformers is a key component of recent pretrained language models. It allows each token in an input sequence to interact independently and in parallel with all other tokens. However, the computational and memory demands of the Transformer's self-attention grow quadratically with the sequence length. This makes processing long sequences computationally expensive. Consequently, models like BERT and its variants are limited to input sequence lengths of 512 tokens, which can be limiting when working with long documents for specific tasks. \n\nVarious adaptation of the Transformers' selfattention mechanism have been proposed to reduce this complexity (Tay et al., 2021(Tay et al., , 2022)). One of the possible approaches is sparse attention and involves using non-full attention patterns to reduce the scope of attention. The sparse attention mechanisms implemented in various pretrained models often comprise several atomic patterns, including global attention, band attention (such as sliding window or local attention), dilated attention, random attention, and block local attention. \n\nThe Sinkhorn attention (Tay et al., 2020) combines a content-based block sparse attention along with block local attention. Longformer (Beltagy et al., 2020) rely on a sliding window and a global attention. In addition to sliding window and a global attention, BigBird (Zaheer et al., 2020) use an additional random attention to approximate full attention. More recently, the LSG (Local Sparse Global) attention was introduced (Condevaux and Harispe, 2023). This attention relies on three components: block local attention, sparse attention to capture extended context and global attention. All these attention mechanisms allow re-ducing the attention complexity from O(n 2 ) to O(nlogn) or O(n). Many other methods can improve the efficiency by replacing the self-attention matrix by a low-rank approximation, such as Linformer (Wang et al., 2020), Performer (Choromanski et al., 2020a,b), Random Feature Attention (Peng et al., 2021), and LUNA (Ma et al., 2021).",
            "score": 0.5681315372071787,
            "section_title": "Attention mechanism for long-sequence transformers",
            "char_start_offset": 2746,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1123
                },
                {
                    "start": 1126,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 2090
                }
            ],
            "ref_mentions": [
                {
                    "start": 687,
                    "end": 704,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 704,
                    "end": 725,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1149,
                    "end": 1167,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 1395,
                    "end": 1416,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1553,
                    "end": 1582,
                    "matchedPaperCorpusId": "253157377"
                },
                {
                    "start": 2042,
                    "end": 2061,
                    "matchedPaperCorpusId": "232105052"
                },
                {
                    "start": 2072,
                    "end": 2089,
                    "matchedPaperCorpusId": "235313355"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1187744140625
        },
        {
            "corpus_id": "273821735",
            "title": "The Evolution of RWKV: Advancements in Efficient Language Modeling",
            "text": "Despite their success, Transformer models face several significant challenges: \n\n1. Quadratic Complexity: The self-attention mechanism in Transformers has a time and memory complexity of O(n 2 ) with respect to sequence length n. This quadratic scaling becomes problematic for very long sequences, limiting the practical maximum context length. 2. Memory Requirements: The need to store attention matrices for all layers leads to high memory usage, especially for long sequences or large batch sizes. 3. Inference Speed: While Transformers excel in parallel processing during training, their inference speed for autoregressive tasks can be slower than RNNs. \n\n4. Positional Encoding Limitations: Transformers are inherently permutation-invariant, meaning they don't understand the order of words in a sequence without explicit positional information. \n\nThese limitations have motivated research into more efficient architectures that can maintain the strong performance of Transformers while addressing their computational challenges.",
            "score": 0.5681238381646062,
            "section_title": "Limitations of Transformer Models",
            "char_start_offset": 2118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 81,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 1034
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2264404296875
        },
        {
            "corpus_id": "247451135",
            "title": "Block-Recurrent Transformers",
            "text": "The quadratic cost of attention is well known in the literature, and a great deal of work has been done on efficient long-range attention mechanisms; see [4,5] for recent surveys. Sparse strategies such as Big Bird [6], Routing Transformers [7], and Reformer [8] select only a subset of tokens to attend to. Hierarchical mechanisms [9] combine multiple tokens into phrases or sentences to reduce sequence length. Expire-span [10] learns to prune far-away tokens that the model has labelled as \"unimportant\". \n\nMemorizing transformers [11] replace dense attention with k-nearest-neighbor lookup. \n\nYet another approach is to reduce the sequence length by pooling, averaging, or compressing it in some way. Hierarchical 1D attention [12], and Combiner [13] apply pooling or averaging over tokens at longer distances. Linformer [14] applies a linear transformation to the key and value matrices to reduce the sequence length. Compressive transformers [15] and funnel transformers [16] apply additional learned compression layers to compress the sequence. The equation for attention is (roughly) softmax(QK T )V where Q, K, and V are the query, key, and value matrices of the attention layer. If the softmax operation is removed from this equation or somehow \"linearized\", the equation can be rearranged as Q(K T V ), where (K T V ) can be computed incrementally (i.e., in a recurrent fashion) as a cumulative sum over the sequence [17]. Linearized attention thus has linear rather than quadratic complexity with respect to sequence length. Following this line of reasoning, there have been several proposals that approximate the softmax [18,19] or replace it [20,21]. Linear transformers are related to earlier work on fast weight programmers [20] [22], and can be extended with other forms of recurrence [23]. \n\nOur work differs from all of the above mechanisms, because we rely only on standard dense attention with softmax. \n\nA few other lines of research have combined the transformer architecture with recurrence in some way.",
            "score": 0.5680209676569085,
            "section_title": "Related Work",
            "char_start_offset": 3814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1807
                },
                {
                    "start": 1810,
                    "end": 1923
                },
                {
                    "start": 1926,
                    "end": 2027
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "260440449"
                },
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 259,
                    "end": 262,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 332,
                    "end": 335,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 425,
                    "end": 429,
                    "matchedPaperCorpusId": "234681615"
                },
                {
                    "start": 534,
                    "end": 538,
                    "matchedPaperCorpusId": "247519194"
                },
                {
                    "start": 750,
                    "end": 754,
                    "matchedPaperCorpusId": "235829099"
                },
                {
                    "start": 1428,
                    "end": 1432,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1634,
                    "end": 1638,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1656,
                    "end": 1660,
                    "matchedPaperCorpusId": "235377069"
                },
                {
                    "start": 1740,
                    "end": 1744,
                    "matchedPaperCorpusId": "235377069"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07586669921875
        },
        {
            "corpus_id": "251403135",
            "title": "A length adaptive algorithm-hardware co-design of transformer on FPGA through sparse attention and dynamic pipelining",
            "text": "According to our preliminary study, the time consumption ratio of self-attention is projected to climb if the number of tokens in the input sequence increases, especially in the NLP field [8] further. There exist several initial attention-aware optimization attempts, such as sparse attention [9] and attention approximation [10][11][12][13][14], to leverage runtime approximations or domain knowledge, i.e., tokens only attend their nearby tokens & a few sampled tokens as the summary of the sentence instead of attending all tokens. However, these explorations, unfortunately, fall short by either lacking generality or high computation overheads. \n\nTo make it worse, Transformer brings the challenge of a wide variety of input lengths, where inputs need to be zero-padded to the maximum sentence length in the batch to accommodate the parallel computing platforms such as GPU and Field-Programmable Gate Array (FPGA) [17]. By nature, RNN-based models, e.g., GRU and LSTM, process the inputs sequentially. Thus, the inputs could be divided into unified fixed-length sub-inputs and processed independently. Transformers leverages parallel processing and therefore cannot enjoy the benefit of fixed-length sub-inputs. Existing works on sequence length standardization fall into two categories. The first one is padding or truncation, which forces the sequence length to be the same. It leads to enormous computation overhead due to the unnecessary computation of the padding part. The second category divides a sequence batch into micro-batches (padding within the micro-batch) to mitigate the computation overhead. However, the various and irregular sequence length undermines overall performance and throughput at the inter micro-batch level. Together with the prolonged turnaround time, achieving fast and efficient Transformer models becomes a grand challenge. \n\nAcross all the popular hardware, e.g., CPUs, GPUs, FPGAs, and Application-Specific Integrated Circuits (ASICs), FPGAs strike an effective balance among massive parallelism, high energy efficiency and short development cycle, hence lend themselves as the top choice to expedite the Transformer architecture. In this paper, we believe that the ideal Transformers acceleration should have a coherent algorithm-hardware co-design.",
            "score": 0.5680137697935418,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 649
                },
                {
                    "start": 652,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1864
                },
                {
                    "start": 1867,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2293
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 191,
                    "matchedPaperCorpusId": "47018994"
                },
                {
                    "start": 329,
                    "end": 333,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 333,
                    "end": 337,
                    "matchedPaperCorpusId": "211296403"
                },
                {
                    "start": 337,
                    "end": 341,
                    "matchedPaperCorpusId": "229298088"
                },
                {
                    "start": 341,
                    "end": 345,
                    "matchedPaperCorpusId": "235414966"
                },
                {
                    "start": 920,
                    "end": 924,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2010498046875
        },
        {
            "corpus_id": "267770311",
            "title": "CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory",
            "text": "Long-range self-attention. Many efficient self-attention techniques have been proposed for long context modeling in transformer models, including low-rank factorization (Wang et al., 2020), local attention (Ramachandran et al., 2019), dilated attention (Ding et al., 2023), sparsity (Beltagy et al., 2020;Zaheer et al., 2020;Kitaev et al., 2020), and hardwareaware attention mechanisms such as FlashAttention (Dao et al., 2022;Dao, 2023). Despite notable progress, these methods are unable to handle unbounded context window sizes and struggle to retrieve information in the middle of the input (Liu et al., 2023). They can be used in tandem with our proposed approach for longer context modeling. \n\nMemory-augmented LLMs. Memory-augmented language models have emerged as a promising way to model extended context window sizes (Packer et al., 2023;Dai et al., 2019;Wu et al., 2022;Tworkowski et al., 2023;Weston et al., 2014). In particular, Wu et al. (2022) show that a kNN lookup into a memory cache bank containing (key, value) pairs of past inputs can improve language modeling. Tworkowski et al. (2023) further improved this approach using contrastive learning. In the same vein, Wang et al. (2023) addressed the memory staleness limitation of these works by training a side network model, while keeping the LLM frozen. Unlike these methods, our approach relies on consolidated representations of past tokens which are dynamically updated, enabling the context window to be arbitrarily large, without being limited by the number of memory slots. Moreover, different from these approaches, our method is training-free (memory updates occur solely at runtime), making it easier to integrate our memory module into any existing LLM architecture. (Ge et al., 2023;Mu et al., 2023;Chevalier et al., 2023) have been recently explored for extending the context length in transformer models.",
            "score": 0.5677510521252507,
            "section_title": "Related Work",
            "char_start_offset": 5301,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1888
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 325,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 409,
                    "end": 427,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1783447265625
        },
        {
            "corpus_id": "272753750",
            "title": "WallFacer: Harnessing Multi-dimensional Ring Parallelism for Efficient Long Sequence Model Training",
            "text": "Attention Optimization. Traditional full attention mechanisms necessitate O(n 2 ) memory for storing the outputs of QK T , leading to significant computational and memory demands. To address these challenges within the GPU, several approaches have been devised to reduce both memory and computational requirements. Memory-efficient attention [35] introduces a straightforward algorithm that requires only O(1) memory relative to the sequence length, with an extension for self-attention that needs only O(log n) memory. Flash Attention further minimizes I/O overhead and enhances overall efficiency. Additionally, optimization methods specifically tailored for inference, such as PagedAttention [21], are also being developed to improve the efficiency of attention computations. In this work, we utilize Flash Attention within each iteration to reduce the computation overhead. \n\nLong-Sequence Training Techniques. Sequence Parallelism [23] was initially introduced to enhance the efficiency of parallel long-sequence training. Ring Attention [26] improved communication efficiency through memory-efficient methods [35], supporting near-infinite sequence lengths. Deep-Speed Ulysses [15] employs attention head splitting to achieve high efficiency, though it is constrained by the number of heads. Megatron Sequence Parallelism focuses on reducing memory costs during Tensor Parallelism, while DistFlashAttention [22] features a load-balance scheme and a novel gradient checkpoint method. Our work builds on these innovations, introducing a system that supports large-scale training with an efficient communication scheme. \n\nTechniques for Distributed Model Training. Distributed model training encompasses two primary areas: 1) Memory Management: Various techniques aim to conserve GPU memory during distributed training, such as mixed precision training [28] and the ZeRO series [37]. In this work, we implement ZeRO-2 to manage optimizer states and gradients efficiently. 2) Hybrid Parallelism: Frameworks like Megatron [30] and Colossal AI [4] integrate multiple forms of parallelism. There are various existing Parallelism techniques like Pipeline Parallelism [14,11,24,27] and Tensor Parallelism [40], which can be combined with WallFacer Parallelism to facilitate largescale training. We are also considering the integration of additional frameworks such as [6] to enhance overlapping capabilities in future implementations.",
            "score": 0.5671767886759387,
            "section_title": "Related Work",
            "char_start_offset": 40056,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2291
                },
                {
                    "start": 2292,
                    "end": 2431
                }
            ],
            "ref_mentions": [
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "246017095"
                },
                {
                    "start": 2172,
                    "end": 2175,
                    "matchedPaperCorpusId": "235898937"
                },
                {
                    "start": 2175,
                    "end": 2178,
                    "matchedPaperCorpusId": "261339639"
                },
                {
                    "start": 2365,
                    "end": 2368,
                    "matchedPaperCorpusId": "269363779"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5859375
        },
        {
            "corpus_id": "266162792",
            "title": "Gated Linear Attention Transformers with Hardware-Efficient Training",
            "text": "Sun et al. (2023a) and Qin et al. (2024a) generalize this form to linear attention with exponential decay (or ALiBi). Kacham et al. (2023); Lingle (2023) also derive similar chunkwise forms. However, most chunkwise linear attention is not I/O-aware. To the best of our knowledge, only LIGHTNINGATTENTION2 (Qin et al., 2024a) (concurrent to our work) is I/O aware, and it is very similar to the non-materialization version of our FLASHLINEARATTENTION. We additionally propose a materialization version, which leverages sequence-level parallelism and thus allows for higher training throughput at the cost of a slightly increasing memory footprint. \n\nOther subquadratic models. Besides the Linear attention Transformer (Katharopoulos et al., 2020;Schlag et al., 2021) discussed in this work, previous studies have explored sparsifying attention with either a predefined fixed pattern (Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020) or a context-aware learnable pattern (Roy et al., 2020;Kitaev et al., 2020;Ren et al., 2023) for sequence modeling with subquadratic complexity in the sequence length dimension. Leveraging convolutions for efficient sequence modeling has also been studied in works such as Dynamic Convolution (Wu et al., 2019), Long Convolution (Fu et al., 2023c;Qin et al., 2023a;Poli et al., 2023;Massaroli et al., 2023;Li et al., 2023d;Romero et al., 2021), and State Space Models (Gu et al., 2021a;Gupta & Berant, 2022;Gu et al., 2021b;Hasani et al., 2022;Smith et al., 2023;Ma et al., 2023).",
            "score": 0.5669800597292531,
            "section_title": "A Extended Related Work",
            "char_start_offset": 36179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 646
                },
                {
                    "start": 649,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1524
                }
            ],
            "ref_mentions": [
                {
                    "start": 717,
                    "end": 745,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 745,
                    "end": 765,
                    "matchedPaperCorpusId": "235377069"
                },
                {
                    "start": 923,
                    "end": 943,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1019,
                    "end": 1036,
                    "matchedPaperCorpusId": "259203690"
                },
                {
                    "start": 1237,
                    "end": 1254,
                    "matchedPaperCorpusId": "59310641"
                },
                {
                    "start": 1273,
                    "end": 1291,
                    "matchedPaperCorpusId": "256846960"
                },
                {
                    "start": 1291,
                    "end": 1309,
                    "matchedPaperCorpusId": "258557266"
                },
                {
                    "start": 1309,
                    "end": 1327,
                    "matchedPaperCorpusId": "257050308"
                },
                {
                    "start": 1327,
                    "end": 1350,
                    "matchedPaperCorpusId": "264590326"
                },
                {
                    "start": 1350,
                    "end": 1367,
                    "matchedPaperCorpusId": "252917984"
                },
                {
                    "start": 1412,
                    "end": 1430,
                    "matchedPaperCorpusId": "240354066"
                },
                {
                    "start": 1451,
                    "end": 1468,
                    "matchedPaperCorpusId": "239998472"
                },
                {
                    "start": 1488,
                    "end": 1507,
                    "matchedPaperCorpusId": "251442769"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1788330078125
        },
        {
            "corpus_id": "265552007",
            "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey",
            "text": "In the quest to optimize Transformers for efficiency, another key area of research focuses on integrating sparse modeling within these attention-based architectures. This approach is pivotal in reducing computational demands, especially in models with a large number of parameters. Two primary directions have emerged in sparse modeling: the Mixture of Experts (MoE) [43,72,153,193,219,244,352] and Sparsefinder [272] from different manners. \n\nThe MoE approach [43,44,72,78,248,313], incorporates multiple branches or 'experts' in the model, each specializing in different subtasks. During inference, only a subset of these paths is activated, maintaining computational efficiency while potentially enhancing performance. This design enables models like GLaM to scale impressively, activating only 99 billion parameters during inference despite having over 1.2 trillion parameters in total. Further developments in MoE, such as Sparse MoE [43], address issues like representation collapse, ensuring more equal activation of experts and efficient information processing. On the other hand, Sparsefinder [272] takes a different approach by focusing on uncovering sparsity within the Table 1. Comparisons of time and memory cost in inference evaluation between Transformer and attention-free methods when using a sequence of length  as input. Models with more + under performance are of better performance of the perplexity numbers on the in-domain validation set and other out-of-domain corpora.",
            "score": 0.566597790419268,
            "section_title": "Sparse Modeling",
            "char_start_offset": 45502,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1493
                }
            ],
            "ref_mentions": [
                {
                    "start": 378,
                    "end": 382,
                    "matchedPaperCorpusId": "249394802"
                },
                {
                    "start": 382,
                    "end": 386,
                    "matchedPaperCorpusId": "245986500"
                },
                {
                    "start": 471,
                    "end": 474,
                    "matchedPaperCorpusId": "231573431"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1451416015625
        },
        {
            "corpus_id": "248177894",
            "title": "Revisiting Transformer-based Models for Long Document Classification",
            "text": "Vanilla transformer relies on the multi-head selfattention mechanism, which scales poorly with the length of the input sequence, requiring quadratic computation time and memory to store all scores that are used to compute the gradients during back-propagation (Qiu et al., 2020). Several Transformer-based models (Kitaev et al., 2020;Tay et al., 2020;Choromanski et al., 2021)  in-between a window of neighbour (consecutive) tokens. Global attention relies on the idea of global tokens that are able to attend and be attended by any other token in the sequence (Figure 3). BigBird of Zaheer et al. (2020) is another sparse-attention based Transformer that uses a combination of a local, global and random attention, i.e., all tokens also attend a number of random tokens on top of those in the same neighbourhood. Both models are warm-started from the public RoBERTa checkpoint and are further pre-trained on masked language modelling. They have been reported to outperform RoBERTa on a range of tasks that require modelling long sequences. \n\nWe choose Longformer (Beltagy et al., 2020) in this study and refer readers to Xiong et al. (2021) for a systematic comparison of recent proposed efficient attention variants.",
            "score": 0.5665102011597861,
            "section_title": "Sparse-Attention Transformers",
            "char_start_offset": 6042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1218
                }
            ],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 278,
                    "matchedPaperCorpusId": "207847640"
                },
                {
                    "start": 351,
                    "end": 376,
                    "matchedPaperCorpusId": "222067132"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11065673828125
        },
        {
            "corpus_id": "268692192",
            "title": "Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model",
            "text": "Since the Transformer was introduced, the quadratic time cost of the attention operation has been numerously researched.Optimizing this operation can improve the efficiency when training and inferencing long context for large language models (Xiao et al., 2023).Recently, many transformer variants have been introduced to reduce the complexity of attentions (Tay et al., 2022), including sparse attention (Beltagy et al., 2020;Kitaev et al., 2020;Guo et al., 2021), kernel-based methods (Choromanski et al., 2020;Kasai et al., 2021;Peng et al., 2021), chunked attention with gating (Hua et al., 2022;Ma et al., 2022) and other efficient methods (Wang et al., 2020;Dao et al., 2022).Another line of research tries to replace the attention mechanism with other modules for long sequences and avoid quadratic time costs.A dizzying number of attention-free models have emerged, where SSMs are becoming one of the most promising models among them.SSMs.S4 first investigated the SSM for long sequence modeling (Gu et al., 2021).They showed that naive instantiations of the SSM did not perform well but HiPPO-LegS matrix did (Gu et al., 2020), and hence introduced the DPLR that efficiently computed the complex diagonal plus low-rank matrix.DSS observed that a fully diagonal matrix could preserve the performance of the original S4 (Gupta et al., 2022), and S4D (Gu et al., 2022) then showed that the initialization is critical for DSS.Inspired by S4, many SSM variants emerged recently.S5 replaced single-input, single-output (SISO) SSMs in S4 with multi-input, multi-output (MIMO) (Smith et al., 2022).SGConv viewed the SSM as a global convolution model and suggested that the convolution kernel's sub-linear decay in sequence length is indispensable (Li et al., 2022).",
            "score": 0.56646857280621,
            "section_title": "Related Works",
            "char_start_offset": 20243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 120,
                    "end": 262
                },
                {
                    "start": 262,
                    "end": 682
                },
                {
                    "start": 682,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 942
                },
                {
                    "start": 942,
                    "end": 947
                },
                {
                    "start": 947,
                    "end": 1022
                },
                {
                    "start": 1022,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1482
                },
                {
                    "start": 1482,
                    "end": 1599
                },
                {
                    "start": 1599,
                    "end": 1766
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 376,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 582,
                    "end": 600,
                    "matchedPaperCorpusId": "247011581"
                },
                {
                    "start": 664,
                    "end": 681,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1118,
                    "end": 1135,
                    "matchedPaperCorpusId": "221150566"
                },
                {
                    "start": 1327,
                    "end": 1347,
                    "matchedPaperCorpusId": "247762199"
                },
                {
                    "start": 1357,
                    "end": 1374,
                    "matchedPaperCorpusId": "249953875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08209228515625
        },
        {
            "corpus_id": "230435805",
            "title": "Transformers in Vision: A Survey",
            "text": "Oscar [44] Transformer layer to jointly process triplet      complexity of O(nc) with respect to the input sequence length n (where c is the number of clusters). We refer interested readers to a survey on efficient Transformers in NLP [34]. Similar to the NLP domain, computer vision models also suffer from the high computational cost of Transformer models. For example, image generators that are based on sequence-based Transformers (e.g., iGPT) have a high compute cost limiting their applicability to high-resolution inputs. The time and memory cost of core self-attention operation in Transformers increases quadratically with the number of patches, i.e. O(n 2 ), for n image patches (in some applications, e.g., low-level vision, n = H \u00d7 W where H, W denote the height and width of the image). This is a major drawback of existing Transformers that hinders their application to most tasks involving high-resolution (HR) images, such as object detection and segmentation (in highlevel vision), and super-resolution, deblurring, denoising, etc. (in low-level vision). Numerous methods have been proposed that make special design choices to perform selfattention more 'efficiently', for instance employing pooling/downsampling in self-attention [97], [219], [249], local window-based attention [36], [250], axial-attention [179], [251], low-rank projection attention [38], [252], [253], ker-nelizable attention [254], [255], and similarity-clustering based methods [246], [256]. However, almost all of these approaches either come with a trade-off between complexity and accuracy, require special hardware specifications or are still not applicable to very large images. Therefore, there is a pressing need to develop an efficient self-attention mechanism that can be applied to HR images on resourcelimited systems without compromising accuracy. It will be interesting to explore how existing models can be extended to high-dimensional cases e.g., using a multi-scale transformer design with a somewhat local context modeling.",
            "score": 0.5662838830111423,
            "section_title": "Multi-Model Learning",
            "char_start_offset": 111437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 2030
                }
            ],
            "ref_mentions": [
                {
                    "start": 6,
                    "end": 10,
                    "matchedPaperCorpusId": "215754208"
                },
                {
                    "start": 1303,
                    "end": 1308,
                    "matchedPaperCorpusId": "232320340"
                },
                {
                    "start": 1376,
                    "end": 1381,
                    "matchedPaperCorpusId": "231847231"
                },
                {
                    "start": 1383,
                    "end": 1388,
                    "matchedPaperCorpusId": "218487423"
                },
                {
                    "start": 1421,
                    "end": 1426,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1468,
                    "end": 1473,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1475,
                    "end": 1480,
                    "matchedPaperCorpusId": "211505992"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058563232421875
        },
        {
            "corpus_id": "248965487",
            "title": "Linearizing Transformer with Key-Value Memory",
            "text": "(Qiu et al., 2020) and strided attention (Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020). (Sukhbaatar et al., 2019) learned sparse attention patterns in a data-driven manner. These sparse local attention approaches reduced the computation at a cost of potentially harming the modeling capacity. Another family of efficient transformers compresses the context via low-rank projections to reduce memory overhead Tay et al., 2020a). Other methods add \"global tokens\" as surrogates for global information exchange (Rae et al., 2020;Ma et al., 2021) or employ clustering-based attention (Kitaev et al., 2020;Roy et al., 2020;Tay et al., 2020b). We compared MemSizer in detail with some of these efficient Transformers in the Appendix A.\n\nPrior work also suggested many other strategies to improve efficiency in transformers, such as factorization (Dehghani et al., 2019;Lan et al., 2020), pruning (Michel et al., 2019;Fan et al., 2020), and quantization (Zafrir et al., 2019;Shen et al., 2020). Some of these methods present orthogonal design choices and can be integrated into our MemSizer model to gain further efficiency.",
            "score": 0.5660484283674867,
            "section_title": "Related Work",
            "char_start_offset": 25250,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 102,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 104,
                    "end": 129,
                    "matchedPaperCorpusId": "159041867"
                },
                {
                    "start": 524,
                    "end": 542,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 542,
                    "end": 558,
                    "matchedPaperCorpusId": "235313355"
                },
                {
                    "start": 596,
                    "end": 617,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 634,
                    "end": 652,
                    "matchedPaperCorpusId": "211505992"
                },
                {
                    "start": 879,
                    "end": 896,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 906,
                    "end": 927,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 927,
                    "end": 944,
                    "matchedPaperCorpusId": "202750230"
                },
                {
                    "start": 963,
                    "end": 984,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 984,
                    "end": 1002,
                    "matchedPaperCorpusId": "202565587"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.092529296875
        },
        {
            "corpus_id": "259937453",
            "title": "Retentive Network: A Successor to Transformer for Large Language Models",
            "text": "Transformer [VSP + 17] has become the de facto architecture for large language models [BMR + 20], which was initially proposed to overcome the sequential training issue of recurrent models [HS97]. However, training parallelism of Transformers is at the cost of inefficient inference, because of the O(N ) complexity per step and memory-bound key-value cache [Sha19], which renders Transformers unfriendly to deployment. The growing sequence length increases GPU memory consumption as well as latency and reduces inference speed. \n\nNumerous efforts have continued to develop the next-generation architecture, aiming at retaining training parallelism and competitive performance as Transformers while having efficient O(1) inference. It is challenging to achieve the above goals simultaneously, i.e., the so-called \"impossible triangle\" as shown in Figure 2. \n\nThere have been three main strands of research. First, linearized attention [KVPF20] approximates standard attention scores exp(q \u2022 k) with kernels \u03d5(q) \u2022 \u03d5(k), so that autoregressive inference can be rewritten in a recurrent form. However, the modeling capability and performance are worse than Transformers, which hinders the method's popularity. The second strand returns to recurrent models for efficient inference while sacrificing training parallelism. As a remedy, element-wise operators [PAA + 23] are used for acceleration, however, representation capacity and performance are harmed. The third line of research explores replacing attention with other mechanisms, such as S4 [GGR21], and its variants [DFS + 22, PMN + 23]. None of the previous work can break through the impossible triangle, resulting in no clear winner compared with Transformers. \n\nIn this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient longsequence modeling, Transformer-comparable performance, and parallel model training simultaneously. Specifically, we introduce a multi-scale retention mechanism to substitute multi-head attention, which has three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent representations. First, the parallel representation empowers training parallelism to utilize GPU devices fully.",
            "score": 0.565372856312799,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 856
                },
                {
                    "start": 859,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2214
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 195,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 935,
                    "end": 943,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1541748046875
        },
        {
            "corpus_id": "272310408",
            "title": "HLogformer: A Hierarchical Transformer for Representing Log Data",
            "text": "Global Memory Tokens in Transformers. Models like Longformer (Beltagy, Peters, and Cohan 2020), ETC (Extended Transformer Construction) (Ainslie et al. 2020), and Big Bird (Zaheer et al. 2020) introduce global memory tokens to address the limitations of traditional transformers with long sequences. These tokens maintain attention connections to all other tokens in the sequence, allowing the models to capture broader contextual understanding while avoiding the quadratic memory and computational overhead of standard self-attention mechanisms. Sparse Attention Mechanisms. Sparse transformers (Child et al. 2019) employ fixed patterns with local and strided attention to address the inefficiencies of traditional transformers in processing long sequences. Other methods, such as those proposed by (Roy et al. 2021) and (Kitaev, Kaiser, and Levskaya 2020), enhance this concept by making the sparsity pattern learnable. These approaches adapt the attention patterns during training to better capture the data structure. Segment-based Recurrence. Segment-based recurrence methods, such as Transformer-XL (Dai et al. 2019) and Compressive Transformer (Rae et al. 2019), introduce mechanisms to maintain and leverage contextual information across segments, significantly reducing memory and computational costs. \n\nDespite their effectiveness, these approaches are not specifically tailored to the unique characteristics of log data, which often exhibit a hierarchical, dictionary-like structure. This gap underscores the need for models designed to capture and leverage the intrinsic structure of log data.",
            "score": 0.5650182984033489,
            "section_title": "Efficient Transformers",
            "char_start_offset": 6293,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1310
                },
                {
                    "start": 1313,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1605
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 94,
                    "matchedPaperCorpusId": "215737171"
                },
                {
                    "start": 136,
                    "end": 157,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 172,
                    "end": 192,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 800,
                    "end": 816,
                    "matchedPaperCorpusId": "212718077"
                },
                {
                    "start": 822,
                    "end": 857,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 1151,
                    "end": 1168,
                    "matchedPaperCorpusId": "5987139"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1943359375
        },
        {
            "corpus_id": "266164124",
            "title": "Efficient Classification of Long Documents via State-Space Models",
            "text": "Since the emergence of large-scale pre-trained language models such as BERT (Devlin et al., 2019) and GPT3 (Brown et al., 2020), these transformerbased models have become popular solutions for many text classification and generation tasks. However, their benefit is constrained to short-length inputs when the computation resource is limited because attention module requires quadratic computation time and space. More specifically, each token in a sequence of length N requires pairwise computation with all N tokens, which results in O(N 2 ) complexity. Such limitation makes transformerbased models hard to process long sequential data efficiently. There are many works aiming to improve the performance on Long Document Classi-fication for transformers (Dai et al., 2022). One of the common approaches is to simply truncate long texts to a pre-defined length, e.g. 512, which makes pre-trained models to be applicable for them. Some work demonstrated this technique is not sufficient for long documents due to the missing of important information (Dai et al., 2022). \n\nAnother sort of technique attempts to reduce the computation overhead of attention-based systems. This problem has several relevant solutions, e.g. Sparse Attention models (Beltagy et al., 2020) and Hierarchical Attention models (Chalkidis et al., 2022). One of the important sparse attention methods is Longformer, which leverages local and global attention to reduce the computational complexity of the models and increases the input length up to 4096 tokens. Another popular sparse attention method is BigBird (Zaheer et al., 2020): besides the global and local attention, it includes extra random attention modules to attend to a predefined number of random tokens. Apart from designing sparse attention mechanisms, Hierarchical Transformers (HAN) like ToBERT (Pappagari et al., 2019) propose to construct systems on top of the conventional transformer (Chalkidis et al., 2022). Basically, the long text is first split into several chunks less than a fixed number, e.g. 200. \n\nNext, every chunk is encoded by a vanilla transformer one by one to form a collection of chunk representations and then another transformer processes the sequence of chuck representations.",
            "score": 0.5648668295058207,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2051
                },
                {
                    "start": 2054,
                    "end": 2242
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 97,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 757,
                    "end": 775,
                    "matchedPaperCorpusId": "248177894"
                },
                {
                    "start": 1051,
                    "end": 1069,
                    "matchedPaperCorpusId": "248177894"
                },
                {
                    "start": 1302,
                    "end": 1326,
                    "matchedPaperCorpusId": "248177894"
                },
                {
                    "start": 1586,
                    "end": 1607,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1837,
                    "end": 1861,
                    "matchedPaperCorpusId": "204852089"
                },
                {
                    "start": 1930,
                    "end": 1954,
                    "matchedPaperCorpusId": "248177894"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1475830078125
        },
        {
            "corpus_id": "273812148",
            "title": "Context Parallelism for Scalable Million-Token Inference",
            "text": "In this work, we mainly address the challenges with extremely large (128K-1M) context lengths. \n\n\u2022 Compute: While an W -parameter Transformer model requires 2 \u2022 W matrix multiplication FLOPs for each token during inference or forward pass (Kaplan et al., 2020), the pairwise attention architecture found in mainstream transformers (Vaswani, 2017) incurs a quadratic cost in FLOPs w.r.t. context lengths, which would be dominating in long context cases. Several approximate and sparse methods were proposed, including focusing attention on a subset of tokens, and employing a combination of local and global attention strategies. Techniques such as window attention (Liu et al., 2021), local attention (Xiong et al., 2021), Linformer (Wang et al., 2020), and semi-local sparse attention (Jiang et al., 2024;Beltagy et al., 2020) are examples of such innovations that help manage the computational cost. \n\n\u2022 Memory: Memory usage for LLMs, particularly the KV cache (Pope et al., 2023), scales linearly with the context length. Model compression techniques such as KV cache quantization are crucial for bending the growth curve: lower precision formats like 3-bit, INT4/8 or FP8 can achieve a 2\u00d7 to 4\u00d7 reduction in memory requirements compared to using 16bit (Hooper et al., 2024;Lin et al., 2024). Grouped Query Attention (GQA) (Ainslie et al., 2023) and MQA (Shazeer, 2019) were widely adopted to reduce memory usage by reducing the number of KV heads by 8\u00d7 to 64\u00d7. Additionally, strategies like paged attention (Kwon et al., 2023) have been developed to provide efficient page-like memory management for large numbers of tokens.",
            "score": 0.5648287221332272,
            "section_title": "Challenges with Serving Long Context LLM",
            "char_start_offset": 4926,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 97,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1628
                }
            ],
            "ref_mentions": [
                {
                    "start": 331,
                    "end": 346,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 665,
                    "end": 683,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 963,
                    "end": 982,
                    "matchedPaperCorpusId": "253420623"
                },
                {
                    "start": 1511,
                    "end": 1530,
                    "matchedPaperCorpusId": "261697361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.224365234375
        },
        {
            "corpus_id": "235422257",
            "title": "Memory-efficient Transformers via Top-k Attention",
            "text": "We benchmark a 12-layer model to examine the cumulative utility of not caching QK in all N layers compared to the Performer. We use the same architecture as BERT-base with batch size 1 and vary the input length. We use a Transformer decoder with top-64 attention and chunk size 1, 024 at the self-attention layers, and simple query chunking with chunk size 4, 096 at the feed-forward layers.\n\nWe easily fit a 32K-long input on a 30GiB GPU, improving memory consumption by more than 8\u00d7 compared to vanilla Transformer and 2\u00d7 compared to Performer. Moreover, top-k attention outperforms query chunking in terms of both memory and runtime. As top-k attention targets memory consumption but not runtime, a current limitation is that runtime, unlike Performer, is still quadratic. Thus, running multi-layer models on long sequences is reasonable in a fine-tuning or zero-shot inference setup, but further work is required for training from scratch 12-layer models over large datasets that contain long sequences.\n\nOverall, our benchmarking results over multihead attention, feed-forward, and multi-layer Transformer establish top-k attention as a strong baseline for future work on efficient Transformers that dramatically improves memory consumption. Next, we evaluate top-k attention on downstream tasks and show that top-k attention can be used as a drop-in replacement for vanilla attention without additional pre-training, which can allow resourceconstrained research groups experiment with Transformers over long sequences or models with a large feed-forward dimension.",
            "score": 0.5648171247165495,
            "section_title": "12-layer model",
            "char_start_offset": 18422,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.148681640625
        },
        {
            "corpus_id": "235417174",
            "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers",
            "text": "The Transformer [1] has become one of the most popular neural networks (NNs) for processing sequential data. Its success on neural machine translation quickly transferred to other problems in natural language processing, such as language modelling [2,3] or question answering [4]. Recently, it has also been applied in other domains, including image processing [5,6] or mathematical problem solving [7,8,9]. \n\nConceptually, the Transformer is a deep feedforward NN that processes all elements of a sequence in parallel: unlike in recurrent NNs (RNNs), the computations of a layer for the entire sequence can be packed into one big matrix multiplication. This scales well with the number of parallel processors. \n\nDespite the benefits of parallelisation, a major drawback of Transformers is that their computational complexity in time and space is quadratic in sequence length. Furthermore, in the auto-regressive version [1,2] -the focus of our work -the state size increases linearly with sequence length. This makes Transformers infeasible for auto-regressive settings dealing with very long or potentially infinite sequences, forcing practitioners to truncate temporal contexts and ignore long-term dependencies beyond fixed-size time windows. Although recent work tries to address this issue [10,11], this limitation makes some applications of Transformers challenging, e.g., reinforcement learning (RL) in partially observable environments [12,13], which is still dominated by RNNs such as the Long Short-Term Memory (LSTM; [14]) trained by policy gradients [15,16,17,18]. \n\nTo scale Transformers to longer sequences, recent works have proposed to linearise the softmax in the self-attention computation and reorganise the latter in a sequential way [19]. Such models include Our experiments on the language modelling dataset Wikitext-103 [28] show that our RFWPs are competitive compared to regular Transformers. We then study various properties of the proposed models on two synthetic algorithmic tasks: code execution [29] and sequential ListOps [30]. Finally, it is straightforward to apply our models to RL problems as a drop-in replacement for LSTMs.",
            "score": 0.5648129056261189,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 407
                },
                {
                    "start": 410,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1577
                },
                {
                    "start": 1580,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2161
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 19,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 248,
                    "end": 251,
                    "matchedPaperCorpusId": "52004855"
                },
                {
                    "start": 251,
                    "end": 253,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 276,
                    "end": 279,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 361,
                    "end": 364,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 364,
                    "end": 366,
                    "matchedPaperCorpusId": "222208633"
                },
                {
                    "start": 399,
                    "end": 402,
                    "matchedPaperCorpusId": "85504763"
                },
                {
                    "start": 404,
                    "end": 406,
                    "matchedPaperCorpusId": "235613478"
                },
                {
                    "start": 921,
                    "end": 924,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 924,
                    "end": 926,
                    "matchedPaperCorpusId": "52004855"
                },
                {
                    "start": 1296,
                    "end": 1300,
                    "matchedPaperCorpusId": "57759363"
                },
                {
                    "start": 1300,
                    "end": 1303,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 1445,
                    "end": 1449,
                    "matchedPaperCorpusId": "204578308"
                },
                {
                    "start": 1449,
                    "end": 1452,
                    "matchedPaperCorpusId": "233025422"
                },
                {
                    "start": 1529,
                    "end": 1533,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1563,
                    "end": 1567,
                    "matchedPaperCorpusId": "7162236"
                },
                {
                    "start": 1567,
                    "end": 1570,
                    "matchedPaperCorpusId": "14039355"
                },
                {
                    "start": 1573,
                    "end": 1576,
                    "matchedPaperCorpusId": "204972004"
                },
                {
                    "start": 1755,
                    "end": 1759,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 1844,
                    "end": 1848,
                    "matchedPaperCorpusId": "16299141"
                },
                {
                    "start": 2054,
                    "end": 2058,
                    "matchedPaperCorpusId": "4942335"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32421875
        },
        {
            "corpus_id": "266690973",
            "title": "PanGu-\u03c0: Enhancing Language Model Architectures via Nonlinearity Compensation",
            "text": "While Transformer architectures have gained significant prominence in LLMs recently, there continues to be a surge of interest in their effective utilization in diverse domains, including computer vision tasks. In light of this, we review classical works dedicated to enhancing Transformer structures, with a particular focus on augmenting model nonlinearity and efficiency. \n\nNatural language processing domain. The conventional selfattention mechanism, with quadratic computational complexity, poses challenges for handling long input sequences during training and inference. To mitigate this, various structural priors on attention, including sparsity [33], [34], [35], [36], [37] and linear attention [15], [38], have been proposed. Notably, Reformer [39] employs locality-sensitive hashing to approximate full attention. Longformer [40] integrates local windowed attention with taskmotivated global attention. Models such as GPT-3 [41] incorporate locally banded sparse attention methods, such as Factorized Attention [34]. There are also works focusing on replacing the attention module by incorporating recurrent models [42], [43], [44]. Hyena [45] trained a recurrence of gating units and implicitly parametrized long convolutions, which serves as an attention-free drop-in replacement for the traditional Transformer architecture. RWKV [46] replaced the quadratic QK attention with a scalar formulation that has linear cost. RetNet [44] theoretically derived the connection between recurrence and attention and proposed the retention mechanism for sequence modeling. There are also efficient enhancements focused on the Feed-Forward Network (FFN). Mixture-of-Experts (MoE) [47], [17], [48], [49], [50] has demonstrated effectiveness in the pre-training of LLMs. In addition to MoE, PaLM [51] and LLaMA [23] leverage the SwiGLU activation for original FFN intermediate activations. This choice is grounded in the observation that SwiGLU activations, as demonstrated in compute-equivalent experiments [52], substantially enhance quality compared to standard activation functions like ReLU, GeLU, or Swish. \n\nComputer vision domain.",
            "score": 0.5647466476242005,
            "section_title": "Enhanced Transformer Architectures",
            "char_start_offset": 11409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2112
                },
                {
                    "start": 2115,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 705,
                    "end": 709,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1682,
                    "end": 1686,
                    "matchedPaperCorpusId": "245124124"
                },
                {
                    "start": 1688,
                    "end": 1692,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 1694,
                    "end": 1698,
                    "matchedPaperCorpusId": "235367626"
                },
                {
                    "start": 1700,
                    "end": 1704,
                    "matchedPaperCorpusId": "248266346"
                },
                {
                    "start": 1706,
                    "end": 1710,
                    "matchedPaperCorpusId": "232428341"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.094482421875
        },
        {
            "corpus_id": "271693376",
            "title": "A Survey of Mamba",
            "text": "However, the attention mechanism also introduces a significant computational overhead that increases quadratically with the input size [125,242], which presents challenges in processing lengthy inputs. For example, the rapid growth in computational cost makes Transformers impractical or infeasible to process substantial sequences, thereby limiting their applicability in tasks like document-level machine translation [132] or long document summarization [95]. \n\nRecently, a promising architecture, structured state space sequence models (SSMs) [59], have emerged to efficiently capture complex dependencies in sequential data, becoming a formidable competitor to Transformer. These models, inspired by classical state space models [90], can be considered a fusion of recurrent neural networks and convolutional neural networks. They can be computed efficiently using either recurrence or convolution operations, achieving linear or near-linear scaling with sequence length, thus significantly reducing the computational costs. More specifically, as one of the most successful SSM variants, Mamba achieves comparable modeling capabilities to Transformers while maintaining linear scalability with sequence length [56], propelling it into the realm of focal topics. Mamba first introduces a simple yet effective selection mechanism that enables the model to filter out irrelevant information while retaining necessary and relevant data indefinitely by parameterizing the SSM parameters based on the input. Then, Mamba proposes a hardware-aware algorithm to compute the model recurrently with a scan instead of convolution, achieving up to 3\u00d7faster computation on A100 GPUs. As shown in Figure 1, the powerful modeling capabilities for Manuscript submitted to ACM complex and lengthy sequential data, along with near-linear scalability, position Mamba as an emerging foundation model, poised to revolutionize various domains of research and applications, such as computer vision [207,243], natural language processing [112,235], healthcare [156,184,205], etc. For example, Zhu et al. [243] propose Vim, which is 2.8\u00d7faster than DeiT [178] and saves 86.8% GPU memory when extracting features for high-resolution images.",
            "score": 0.5643366140703676,
            "section_title": "Introduction",
            "char_start_offset": 2249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 140,
                    "matchedPaperCorpusId": "239616022"
                },
                {
                    "start": 140,
                    "end": 144,
                    "matchedPaperCorpusId": "235743105"
                },
                {
                    "start": 419,
                    "end": 424,
                    "matchedPaperCorpusId": "222295593"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "250118028"
                },
                {
                    "start": 733,
                    "end": 737,
                    "matchedPaperCorpusId": "1242324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1083984375
        },
        {
            "corpus_id": "250113485",
            "title": "SALO: an efficient spatial accelerator enabling hybrid sparse attention mechanisms for long sequences",
            "text": "This linear complexity significantly alleviates the memory burden, making it possible to train models with long sequences. However, since the hybrid sparse attention mechanism is not directly supported by the highly optimized GEMM kernels on CPU and GPU, the inference speed is limited, requiring efficient hardware acceleration to achieve a higher speed. \n\nExisting attention accelerators [8][9][10]14] meet performance bottlenecks given long input sequences. FTRANS [9] compresses weights of transformers while computing the full attention without sparsity. SpAtten [14] applies coarse-grained pruning to remove tokens and heads with a relatively low pruning ratio, which cannot effectively shorten the sequence length.  3 [8] and Sanger [10] accelerate the attention by utilizing dynamic sparse patterns, which can incur large additional memory/computation overhead in the case of long sequences. To this end, we propose SALO to efficiently accelerate the attention in transformers for tasks with long input sequences. The main contributions are summarized as follows: \n\n\u2022 We survey popular sparse attention mechanisms and explore the common computation patterns they share. Moreover, we analyze their potential of data reuse during computation.",
            "score": 0.5642473185802919,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1071
                },
                {
                    "start": 1074,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1248
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 393,
                    "matchedPaperCorpusId": "211296403"
                },
                {
                    "start": 393,
                    "end": 396,
                    "matchedPaperCorpusId": "220633179"
                },
                {
                    "start": 396,
                    "end": 400,
                    "matchedPaperCorpusId": "239012114"
                },
                {
                    "start": 400,
                    "end": 403,
                    "matchedPaperCorpusId": "229298088"
                },
                {
                    "start": 468,
                    "end": 471,
                    "matchedPaperCorpusId": "220633179"
                },
                {
                    "start": 568,
                    "end": 572,
                    "matchedPaperCorpusId": "229298088"
                },
                {
                    "start": 725,
                    "end": 728,
                    "matchedPaperCorpusId": "211296403"
                },
                {
                    "start": 740,
                    "end": 744,
                    "matchedPaperCorpusId": "239012114"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1656494140625
        },
        {
            "corpus_id": "240070340",
            "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training",
            "text": "Tensor parallelism mainly tackles the memory bottleneck brought by model data. However, the non-model data can be the bottleneck in applications such as AlphaFold and document-level text understanding. This is because these applications rely on long-sequence data. As the self-attention module in the Transformer layer is of quadratic complexity with respect to the sequence length, longsequence data will increase the memory usage consumed by the intermediate activation, limiting the training capability of the devices. \n\nSequence parallelism [21] is proposed to enable long-sequence modeling by breaking the memory wall brought by the large sequence dimension. In sequence parallelism, the model is replicated across devices just like data parallelism. The input data is split along the sequence dimension and each device only keeps a sub-sequence. The self-attention module is replaced with the Ring Self-Attention module such that the partial query, key, and value embeddings are exchanged among devices to complete the self-attention calculation.",
            "score": 0.5642473185802919,
            "section_title": "Sequence Parallelism",
            "char_start_offset": 11663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1052
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80712890625
        },
        {
            "corpus_id": "221702858",
            "title": "Efficient Transformers: A Survey",
            "text": "Modifications to strides, such as using dilated windows was also proposed in the Longformer work. \n\nThe most recent wave of models we've been seeing is models that are based on low-rank approximation or kernel methods, e.g., models such as Low-Rank Transformer (Winata et al., 2020), Linformer (Wang et al., 2020c), Performer (Choromanski et al., 2020a) and/or Linear Transformers (Katharopoulos et al., 2020). Although due to the state of evaluation and the high parallelism of research, it is quite unclear if this low-rank or kernel paradigm is actually better than the learnable pattern (LP) or model memory based efficient Transformer models. \n\nMore recently, there have been more models that propose a two-pronged or two-step attention mechanism combining models from different techniques. The Long Short Transformer (Zhu et al., 2021) is a dynamic form of Linformer combined with Fixed Pattern attention mechanisms. On the other hand, models like Poolingformer also explicitly construct a two-level attention mechanism with techniques reminiscent of memory-based approaches and local attention. Scatter Brain is a new work (Chen et al., 2021) attempts to unify sparse (fixed pattern) attention with low-rank attention. Two stage attention mechanisms are also proposed by Luna (Ma et al., 2021) On the side, it is important to note that the recurrent based models (Transformer-XL and Compressive Transformers) seem to operate orthogonally and are not as directly comparable to the other models. We also observe that Sparse models (Lepikhin et al., 2020;Fedus et al., 2021) that are not only applicable to attention modules, are also recently emerging and becoming more popular and have demonstrated considerable success in the recent months (Du et al., 2021).",
            "score": 0.5636764571281857,
            "section_title": "On Model Design Trends",
            "char_start_offset": 65175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 100,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 647
                },
                {
                    "start": 650,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1765
                }
            ],
            "ref_mentions": [
                {
                    "start": 823,
                    "end": 841,
                    "matchedPaperCorpusId": "235743105"
                },
                {
                    "start": 1130,
                    "end": 1149,
                    "matchedPaperCorpusId": "235743105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03656005859375
        },
        {
            "corpus_id": "277150882",
            "title": "ATTENTION2D: Communication Efficient Distributed Self-Attention Mechanism",
            "text": "We integrated both non-overlapping ATTENTION2D-NO and overlapping ATTENTION2D-O parallelization methods into the Megatron-LM framework [24] and evaluated them against Nvidia Transformer Engine's [25] efficient implementation of Ring Attention [20], also known as Context Parallelism. The evaluation was performed with the GPT-3-like model architecture [5,38], featuring parameters of 760M, 2.7B, 13B, 66B and 175B. Details of the model architecture are provided in Table 3. GPT-3 is a causal language model. The original Ring Attention's [20] block distribution caused load imbalances when applying causal attention. However, Transformer Engine's [25] implementation of Ring Attention efficiently addresses this issue by reordering the input sequence tokens as follows: The tokens are divided into two equal halves along the sequence length dimension. The first half is block distributed equally among all processors from p 0 to p P \u22121 , and the second half is block distributed equally among all processors in reverse order, from p P \u22121 to p 0 . This results in a fully load-balanced computation. In our ATTENTION2D-NO and ATTENTION2D-O methods, we use a cyclic distribution instead of block distribution, which inherently balances the computation load. The experiments were carried out on a multi-node/multi-GPU configuration, with each node equipped with 4 NVIDIA A100 GPUs connected via PCIe, and inter-node communication facilitated through an Ethernet network. We used global batch sizes of 64, 32, and 16 for model sizes of 760M, 2.7B, and 13B, respectively. Sequence lengths were varied from 2 13 = 8192 to 2 17 = 131072, increasing in powers of 2. We configured the degrees of pipeline, data and tensor parallelism to 1, and allocated all available GPUs to context parallelism for both Ring Attention and our 2D parallel methods during our experiments. This configuration allows us to compare our method directly with Ring Attention, without the influence of pipeline and data parallelism on the comparison.",
            "score": 0.5636725320902822,
            "section_title": "Results",
            "char_start_offset": 26407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2016
                }
            ],
            "ref_mentions": [
                {
                    "start": 352,
                    "end": 355,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.255859375
        },
        {
            "corpus_id": "274280741",
            "title": "Star Attention: Efficient LLM Inference over Long Sequences",
            "text": "In this paper, we introduced Star Attention, a novel block-sparse attention mechanism designed to enable efficient inference on long sequences in transformer-based LLMs. The method operates in two phases: (1) context tokens are processed using blockwise-local attention, with the context segmented into blocks where each block is prefixed with an anchor block; and (2) then the query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention delivers up to 11x speedup over Ring Attention while maintaining 97-100% accuracy, significantly enhancing both memory efficiency and inference speed. Despite these advances, several open questions remain. The role and optimal size of anchor blocks relative to context blocks require further exploration. Additionally, while Star Attention performs effectively with block sizes set to onequarter of the sequence length, accuracy degrades when using smaller blocks on longer sequences. Future work will focus on refining the anchor block mechanism and improving performance on more complex long-context tasks to enhance the scalability and robustness of Star Attention. for each transformer layer do \u25b7 Process through all transformer layers 7: \n\nfor each host h concurrently do \u25b7 Execute parallel computations on each host 8: \n\nCompute query, key, and value vectors (Q, K, V ) using input tokens  (Wolf et al., 2020), which currently lacks support for multi-node inference. As a result, when performing inference with the Llama-3.1 8B model using standard causal autoregressive generation on sequences exceeding 64K tokens with bfloat16 precision across 8 A100 GPUs, we encounter out-of-memory (OOM) errors. Given these limitations, we adopt Ring Attention as a practical and relevant baseline for evaluating Star Attention's performance on sequences up to 1 million tokens in length.",
            "score": 0.5633270541356501,
            "section_title": "CONCLUSION",
            "char_start_offset": 20776,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1312
                },
                {
                    "start": 1315,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1871
                }
            ],
            "ref_mentions": [
                {
                    "start": 1384,
                    "end": 1403,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.286376953125
        },
        {
            "corpus_id": "265034082",
            "title": "Ultra-Long Sequence Distributed Transformer",
            "text": "The LSS Transformer's sequence parallelism has three limitations. First, it still requires 2 global inter-GPU communications per attention layer, which degrades parallel efficiency at many GPUs. Second, while sequence parallelism tackles the long sequence issue, it does not address computation challenge for training large dataset. Three, sequence parallelism is only one source of parallelism. To scale to a large supercomputer for training, the LSS Transformer needs more sources of parallelism to achieve better scalability. To address these issues, this section introduces a method to integrate the LSS Transformer's sequence parallelism with data parallelism. With the integration, the parallel algorithm can (1) achieve better scalability; (2) simultaneously tackle long sequence and large dataset challenges; and (3) constrain the self-attention communications among local communicative groups for reduced overhead. \n\nDespite that sequence and data parallelisms are mostly orthogonal, one technical challenge to overcome is that both parallelisms require model parameter synchronization, but among GPUs in different communicative groups and communicate in different ways. Sequence parallelism requires model parameter synchronization among sequence parallel GPUs, but excludes positional embedding parameters from synchronization given that positional embeddings are distributed in sequence dimension. Data parallelism requires model parameter synchronization among data parallel GPUs, but must include positional embeddings given that data parallel GPUs have the same copy of the positional embedding parameters, but train them with different data batches. \n\nTo address this issue, we use an innovative double gradient averaging technique to avoid synchronization conflicts for positional embeddings. Fig. 3. illustrates an example of how the integrated sequence and data parallelism uses double gradient averaging. In this example, GPUs 1 and 2 process a sequence x 1 together using sequence parallelism, with the first segment x 1 1 assigned to GPU 1 and the second segment x 1 2 assigned to GPU 2. The positional embedding parameters are distributed in the same way with the first half P E 1 assigned to GPU 1 and the second half P E 2 assigned to GPU 2. Similarly, GPUs 3 and 4 handle a difference sequence x 2 using sequence parallelism.",
            "score": 0.5632034889714013,
            "section_title": "INTEGRATED SEQUENCE & DATA PARALLELISM",
            "char_start_offset": 21806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1665
                },
                {
                    "start": 1668,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2351
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88427734375
        },
        {
            "corpus_id": "237266377",
            "title": "Fastformer: Additive Attention Can Be All You Need",
            "text": "In recent years, there are many approaches to improving the efficiency of Transformer architecture (Tay et al., 2020). Some methods use sparse attention mechanisms to reduce the complexity of self-attention (Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020;Zhang et al., 2021). For example, Longformer (Beltagy et al., 2020) uses sliding window attention to attend local contexts and uses global attention on a few pre-selected input locations to capture global contexts. Big-Bird (Zaheer et al., 2020) combines local attention, global attention at certain positions and random attention on several randomly selected token pairs. However, these methods may need a relative larger number of attended tokens to reduce the performance degradation on longer sequences, which usually leads to a limited speed-up ratio. \n\nAnother way is using hashing technique to accelerate self-attention computation. For example, Reformer (Kitaev et al., 2020) uses a multiround hashing scheme to put similar representations into same buckets when computing selfattention, which can theoretically reduce the complexity to O(N log(N )). However, the computational complexity constant of Reformer is quite large, making it inefficient in processing common sequences with rather limited lengths. \n\nThere are also several methods that aim to reduce the computational cost by computing approximated self-attention (Choromanski et al., 2020;Wang et al., 2020b;Tay et al., 2021). For instance, Linformer (Wang et al., 2020b) projects the attention key and value into low-rank matrices to approximate the self-attention mechanism. Linear Transformer (Katharopoulos et al., 2020) uses a kernelbased formulation of self-attention and the associative property of matrix multiplication to approximate the dot-product attention. However, these methods approximate self-attention in a contextagnostic manner, which may not be optimal for text modeling. In addition, they still bring heavy computational cost when the sequence length is very long.",
            "score": 0.5630205363661298,
            "section_title": "Efficient Transformer",
            "char_start_offset": 4363,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 824
                },
                {
                    "start": 827,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2023
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 287,
                    "matchedPaperCorpusId": "234339280"
                },
                {
                    "start": 1445,
                    "end": 1462,
                    "matchedPaperCorpusId": "218487423"
                },
                {
                    "start": 1633,
                    "end": 1661,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1114501953125
        },
        {
            "corpus_id": "273707418",
            "title": "PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference",
            "text": "PAB (Zhao et al., 2024) exploits the U-shaped attention pattern to mitigate temporal redundancy through a pyramid-style broadcasting approach. Finally, DiTFastAttn (Yuan et al., 2024) identifies three types of redundancies, such as spatial redundancy, temporal redundancy, and conditional redundancy, and presents an attention compression method to speed up generation. \n\nParallel Inference for Diffusion Model: Given the similar transformer architecture, tensor parallelism (TP) (Shoeybi et al., 2019) and sequence parallelism (SP) (Jacobs et al., 2023;Liu et al., 2023;Li et al., 2023), which are commonly employed for efficient inference in LLMs, can be adapted for DiTs. TP partitions model parameters across multiple devices, enabling parallel computation and reducing memory requirements on individual devices. However, it necessitates an AllReduce operation for the outputs of both the Attention and Feedforward Network modules, resulting in communication overhead proportional to sequence length. For DiTs with exceptionally long sequences, this communication overhead becomes substantial. In contrast, SP partitions the input image across multiple devices, utilizing methods such as Ulysses or Ring to communicate the Attention input and output tensors. SP demonstrates su-perior communication efficiency compared to TP, but each device stores the entire model parameters, which can be memory intensive. DistriFusion (Li et al., 2024) leverages Input Temporal Redundancy to design an asynchronous sequence parallel method that utilizes stable activations. It is specifically designed for with a U-Net-based models.",
            "score": 0.562983059215619,
            "section_title": "VAE",
            "char_start_offset": 9727,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1623
                }
            ],
            "ref_mentions": [
                {
                    "start": 1426,
                    "end": 1443,
                    "matchedPaperCorpusId": "268091166"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64892578125
        },
        {
            "corpus_id": "221655697",
            "title": "Cluster-Former: Clustering-based Sparse Transformer for Question Answering",
            "text": "Transformer has become ubiquitous in the deep learning field. One of the key ingredients that destined its success is the self-attention mechanism, which allows fully-connected contextual encoding over input tokens. However, despite its effectiveness in modeling short sequences, self-attention suffers when handling inputs with extreme long-range dependencies, as its complexity grows quadratically with respect to the sequence length. Therefore, long sequences are often encoded by Transformer in chunks using a sliding window. In this paper, we propose Cluster-Former, a novel clustering-based sparse Transformer to perform attention across chunked sequences. The proposed framework is pivoted on two unique types of Transformer layer: Sliding-Window Layer and Cluster-Former Layer, which encode local sequence information and global context jointly and iteratively. This new design allows information integration beyond local windows, which is especially beneficial for question answering (QA) tasks that rely on long-range dependencies. Experiments show that Cluster-Former achieves state-of-the-art performance on several major QA benchmarks.",
            "score": 0.5627909758229699,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2235107421875
        },
        {
            "corpus_id": "276961696",
            "title": "Radar: Fast Long-Context Decoding for Any Transformer",
            "text": "Context reduction. Previous literature shows that Transformers with sparse attention patterns can generate with fewer context tokens while not sacrificing the generation quality (Child et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020;Roy et al., 2020;Chen et al., 2021). In addition to the success of sparse attention architectures, researchers also empirically demonstrate that the vanilla Transformers do not need all tokens in the context to correctly generate the current token (O'Connor & Andreas, 2021;Sun et al., 2021;Liu et al., 2024). Our work entails this discovery. \n\nBased on the discovery, there are some closely related methods proposed Han et al. (2023). For instance, StreamingLLM (Xiao et al., 2024) uses only the local sliding window along with a few beginning tokens to generate the current token. H 2 O (Zhang et al., 2023) proposes to additionally retain middle tokens based on a scoring function to maximize hit rate. SnapKV (Li et al., 2024) proposes to retain middle tokens based on the attention pooling in a parallel manner. All these methods, albeit reducing the context length, cannot recover the lost information once the context token is evicted. Moreover, they still suffer from the quadratic time complexity. By contrast, our method is able to dynamically retrieve any tokens in the context with a low time complexity. Recently, Zandieh et al. (2024) propose to compress the KV cache for memory and time efficiency, posing an opportunity for better algorithm designs. \n\nEfficient attention. The quadratic computing time of the dot-product attention becomes a bottleneck in long-context generation. Searching for an efficient design of the attention mechanisms is a long-standing topic for Transformers (Wang et al., 2020;Kitaev et al., 2020). Recently, Landmark Attention (Mohtashami & Jaggi, 2023) proposes to use grouped softmax with a special token to reduce computation.",
            "score": 0.5627421557276957,
            "section_title": "RELATED WORK",
            "char_start_offset": 19516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1504
                },
                {
                    "start": 1507,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1911
                }
            ],
            "ref_mentions": [
                {
                    "start": 487,
                    "end": 513,
                    "matchedPaperCorpusId": "235446334"
                },
                {
                    "start": 513,
                    "end": 530,
                    "matchedPaperCorpusId": "237572264"
                },
                {
                    "start": 656,
                    "end": 673,
                    "matchedPaperCorpusId": "263831240"
                },
                {
                    "start": 702,
                    "end": 721,
                    "matchedPaperCorpusId": "263310483"
                },
                {
                    "start": 952,
                    "end": 969,
                    "matchedPaperCorpusId": "269303164"
                },
                {
                    "start": 1758,
                    "end": 1778,
                    "matchedPaperCorpusId": "209315300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0948486328125
        },
        {
            "corpus_id": "276574945",
            "title": "Compression Barriers for Autoregressive Transformers",
            "text": "Generating long sequences with sparse transformers. arXiv preprint arXiv: . , . \n\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv: . , . \n\nGonc \u00b8alo M Correia, Vlad Niculae, and Andr\u00e9 FT Martins. Adaptively sparse transformers. arXiv preprint arXiv: . , . \n\nTri Dao. Flashattention-: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv: . , . \n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R \u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, : -, . \n\nAlexey Dosovitskiy. An image is worth x words: Transformers for image recognition at scale. arXiv preprint arXiv: . , . \n\nHaojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng Zhang, and Dahua Lin. Skvq: Slidingwindow key and value cache quantization for large language models. In First Conference on Language Modeling, . \n\nQuentin Fournier, Ga\u00e9tan Marceau Caron, and Daniel Aloise. A practical survey on faster and lighter transformers. ACM Computing Surveys, ( s): -, . \n\nSuyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. In International Conference for Learning Representations, . \n\nInsu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David P Woodruff, and Amir Zandieh. Hyperattention: Long-context attention in near-linear time. arXiv preprint arXiv: . , . \n\nThemistoklis Haris.",
            "score": 0.562400611849026,
            "section_title": "Conclusion",
            "char_start_offset": 28110,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 79
                },
                {
                    "start": 82,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 756
                },
                {
                    "start": 759,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1455
                },
                {
                    "start": 1458,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1637
                },
                {
                    "start": 1640,
                    "end": 1659
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0266571044921875
        },
        {
            "corpus_id": "246017095",
            "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
            "text": "Attention-based models (e.g., Transformer) have achieved impressive performance on various natural language processing tasks (e.g., Q&A [Qu et al., 2019], relation extraction [Xue et al., 2020b;Xue et al., 2020a]). Recently, Transformer also achieved promising results on computer vision tasks [Dosovitskiy et al., 2020;Zhang et al., 2021] and even on bioinformatics tasks [Wang et al., 2021]. These Transformer-based models learn powerful context-aware representation by applying self-attention to all pairs of tokens from the input sequence. This mechanism captures longterm dependencies at the token level for sequence modeling. However, self-attention suffers from quadratic memory requirements with respect to sequence length. Existing system requires us to hold the whole sequence in one GPU, which limits the length of input sequence. Unfortunately, the long sequence is common in real-world applications. For instance, when we train Transformer for medical image classification, each image is much larger than it is in usual (e.g., 512\u00d7512\u00d7512 vs 256\u00d7256\u00d73). Then, each medical image includes much more tokens, that is, each input sequence is much longer than usual. In this case, it is challenging to hold the whole sequence within single GPU.\n\nIn this paper, we designed and implemented sequence parallelism (SP), a novel parallelism aiming at breaking the limitation that we need to store the whole sequence in one GPU. The proposed system can train transformer-based models with longer sequences and a larger batch size. We first split the input sequence into multiple chunks along the sequence dimension and feed each sub-sequence chunk to one corresponding GPU. Each GPU thus only holds a part of the full sequence. To apply self-attention to the tokens from different chunks, the main challenge is to compute attention scores and outputs across GPUs efficiently. To tackle this problem, we proposed Ring Self-Attention (RSA), which circulates key and value embeddings across GPUs in a ring manner. In this case, each device is just required to keep the attention embeddings corresponding to its own sub-sequence. As a",
            "score": 0.5621823497311355,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 212,
                    "matchedPaperCorpusId": "229678726"
                },
                {
                    "start": 294,
                    "end": 320,
                    "matchedPaperCorpusId": "233289729"
                },
                {
                    "start": 320,
                    "end": 339,
                    "matchedPaperCorpusId": "232036688"
                },
                {
                    "start": 373,
                    "end": 392,
                    "matchedPaperCorpusId": "233289729"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.908203125
        },
        {
            "corpus_id": "270521583",
            "title": "BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack",
            "text": "Attending to a memory state is much cheaper than to all contexts. Many different architectures adding recurrence to transformers have been proposed (Wu et al., 2022a;Lei et al., 2020;Fan et al., 2020). For example, Compressive Transformer (Rae et al., 2020) updates recurrent memory by compressing hidden activation's from the previous segment to a smaller set of representations. Recurrent Memory Transformer (Bulatov et al., 2022) recurrently passes states of special memory tokens added to the input of Transformer. \n\nActivation Beacon (Zhang et al., 2024a) compresses activations from prior segments using separate parameters and integrates a sliding window mechanism, handling up to 400k tokens. Temporal Latent Bottleneck (Didolkar et al., 2022) Transformer splits computation into two streams: recurrent slow stream and fast stream with self-attention between tokens. Block-Recurrent Transformer (Hutchins et al., 2022) employs LSTM-style (Hochreiter & Schmidhuber, 1997) gates to update its recurrent state. \n\nWe use RMT in our experiments because of its simplicity, plug-and-play compatibility with pretrained transformer-based language models, and promising scaling capabilities (Bulatov et al., 2024). \n\nBig Bird (Zaheer et al., 2020), Longformer (Beltagy et al., 2020), LongNet (Ding et al., 2023) help extend context length for Transformers by switching from full self-attention to sparse self-attention mechanisms with linear complexity. Works like RWKV (Peng et al., 2023a), S4 (Gu et al., 2021), Mamba (Gu & Dao, 2023), take another approach and focus on advancing recurrent networks to reach high parallelism levels available to Transformers while retaining the linear complexity of RNN. These works show promising results on long sequences but are still lagging behind the best transformer models in natural language processing tasks. Mamba, however, seeks to bridge this gap.",
            "score": 0.5616852824038175,
            "section_title": "Limitations",
            "char_start_offset": 32825,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1212
                },
                {
                    "start": 1215,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1894
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 166,
                    "matchedPaperCorpusId": "248157404"
                },
                {
                    "start": 239,
                    "end": 257,
                    "matchedPaperCorpusId": "207930593"
                },
                {
                    "start": 410,
                    "end": 432,
                    "matchedPaperCorpusId": "250526424"
                },
                {
                    "start": 728,
                    "end": 751,
                    "matchedPaperCorpusId": "249192344"
                },
                {
                    "start": 903,
                    "end": 926,
                    "matchedPaperCorpusId": "247451135"
                },
                {
                    "start": 946,
                    "end": 978,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1189,
                    "end": 1211,
                    "matchedPaperCorpusId": "268709587"
                },
                {
                    "start": 1224,
                    "end": 1245,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1493,
                    "end": 1510,
                    "matchedPaperCorpusId": "240354066"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17822265625
        },
        {
            "corpus_id": "258564860",
            "title": "The emergence of clusters in self-attention dynamics",
            "text": "Given the widespread use of Transformers in natural language processing, there has been a surge of interest in understanding the function and significance of attention layers within these models. In [36], the authors show that when treated as discrete-time systems with additional dense layers and multiple heads appended to the core attention mechanism, Transformers exhibit the universal approximation property. In [22], the authors present, to the best of our knowledge, the first interacting particle systems perspective on Transformers. They then leverage the similarities between Transformers (with an additional feed-forward layer compared to (1.1)) and convection-diffusion equations to slightly improve the performance of Transformers by employing a Strang-Marchuk splitting scheme for time discretization. In [28], the authors interpret system (1.1) as the characteristics of a continuity equation. Drawing on the similarities between (1.1) and Sinkhorn iterations, they propose a novel architecture dubbed Sinkformer, which possesses the desirable property of being a Wasserstein gradient flow. 1.2.2. Quadratic complexity of Transformers. The major computational challenge of Transformers is their high computational complexity, particularly when processing long sequences. Transformers require quadratic time and space complexity to process sequences, because each self-attention layer contains n 2 products of the form xQx i , Kx j y (for i, j P rns). The empirical observation that the self-attention matrix P is close to a low rank matrix-see [21,Section 4.4] for references-is cited as the inspiration behind Linformers [33] and the fine-tuning algorithm LoRA [16]. For both approaches, the low-rank structure is imposed rather than extracted from P itself. Other methods called sparse attention and block attention have been proposed to reduce the quadratic complexity-see [33,Section 2.2] for references. In the spirit of these works, a foreshadowing of the clustering mechanism was invoked in [32], where queries are clustered into groups, again in view of reducing the quadratic complexity of self-attention. We point out that [8] previously demonstrated that without skip connections, the dynamics trivializes and all tokens quickly lump together into a single tight cluster.",
            "score": 0.5611729817745287,
            "section_title": "Introduction",
            "char_start_offset": 8573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2129
                },
                {
                    "start": 2130,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 203,
                    "matchedPaperCorpusId": "209444410"
                },
                {
                    "start": 417,
                    "end": 421,
                    "matchedPaperCorpusId": "174801126"
                },
                {
                    "start": 819,
                    "end": 823,
                    "matchedPaperCorpusId": "239616044"
                },
                {
                    "start": 1559,
                    "end": 1563,
                    "matchedPaperCorpusId": "235368340"
                },
                {
                    "start": 2013,
                    "end": 2017,
                    "matchedPaperCorpusId": "220424511"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0531005859375
        },
        {
            "corpus_id": "258947457",
            "title": "Randomized Positional Encodings Boost Length Generalization of Transformers",
            "text": "Transformers are emerging as the new workhorse of machine learning as they underpin many recent breakthroughs, including sequence-to-sequence modeling (Vaswani et al., 2017), image recognition (Dosovitskiy et al., 2021), and multi-task learning (Reed et al., 2022). However, recent work (Del\u00e9tang et al., 2023) demonstrated that Transformers fail to generalize to longer sequences on seemingly simple tasks such as binary addition. Thus, while certain problems can be solved without length generalization, algorithmic reasoning generally requires this ability, similar to many real-world settings such as online or continual learning.\n\nWhile the Transformer's attention mechanism can recognize complex relationships amongst to-* Equal contribution. 1 DeepMind. 2 The Swiss AI Lab, IDSIA, USI & SUPSI. \u2020 Work performed while the author was at DeepMind. Correspondence to {anianr, gdelt}@deepmind.com. Figure 1: Test-time evaluation with longer inputs. The standard positional encoding vector has values larger than those observed during training. Our approach avoids this problem by assigning a random (ordered) positional encoding vector using the full range of possible test positions to each training example. kens in the input sequence, it is limited by its lack of positional awareness. Thus, the input sequence is generally augmented with positional encodings to inject position information into the computation. However, current approaches only consider positions up to the maximum training sequence length N , and thus all the positions N + 1, . . . , M for test sequences of length up to M will appear out-ofdistribution during evaluation (top of Fig. 1).\n\nThis work We introduce a novel family of randomized positional encodings, which significantly improves Transformers' length generalization capabilities on algorithmic reasoning tasks. Our approach is compatible with any existing positional encoding scheme and augments the existing methods by subsampling an ordered set of positions from a much larger range of positions than those observed during training or evaluation (i.e., up to L \u226b M ; bottom of Fig. 1). Thus, over the course of training, the Transformer will learn to handle very large positional encodings and, therefore no longer encounter out-of-distribution inputs",
            "score": 0.5607993955099984,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0615234375
        },
        {
            "corpus_id": "272826811",
            "title": "Efficiently Dispatching Flash Attention For Partially Filled Attention Masks",
            "text": "Transformers [25] revolutionized sequence modeling by using self-attention mechanisms, enabling efficient parallelization and handling of long-range dependencies. A key component of transformers is the attention mechanism. However, in many practical scenarios-such as when processing very long sequences, reducing computational costs, or working with structured data like graphs -it becomes advantageous to restrict the range of elements each item can attend to. This leads to the development of masked attention mechanisms, which limit interactions to a subset of elements. \n\nOne application that leads to such partially filled masks is the creation of efficient transformers. Approaches like LongFormer and BigBird [2,28] employ fixed sparsity patterns to alleviate the quadratic complexity inherent in self-attention, while other approaches exploit low-rank approximations to achieve similar objectives [26,9]. Despite the theoretically lower computational complexity of these methods, many results fail to translate to reduced wall-clock runtimes. The currently prevalent practical solution is instead to simply implement dense attention more efficiently. Flash Attention [10,8,20] is a such a hardware-aware algorithm sufficient to boost the speed of dense attention and obviate the need for a more efficient attention in many applications. \n\nHowever, because the initial flash attention implementation was written with only causal masks in mind, research using alternative attention patterns has slowed down. A previous approach to remedy this, [17], enables a few types of mask in flash attention, but only considers a small subset of cases and requires the user to carefully change the GPU kernel parameters for every new mask. To overcome these limitations, we introduce Binary Block Masking (BinBlkMsk) -a novel algorithm that extends Flash Attention to support any attention masks while also being user-friendly. Our approach builds on two key insights: (1) Flash Attention processes attention matrices in blocks, so we only need to handle blocks containing at least one non-zero mask value, and (2) these blocks are independent Preprint. Under review. and can be preprocessed in parallel with minimal runtime overhead and then shared across multiple heads, layers, or runs depending on the use case. We further enhance the algorithm's efficiency for masks with contiguous non-zero blocks.",
            "score": 0.5607552829611853,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2163
                },
                {
                    "start": 2164,
                    "end": 2311
                },
                {
                    "start": 2312,
                    "end": 2400
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 17,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 720,
                    "end": 723,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 910,
                    "end": 912,
                    "matchedPaperCorpusId": "76661331"
                },
                {
                    "start": 1176,
                    "end": 1180,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1551,
                    "end": 1555,
                    "matchedPaperCorpusId": "268064867"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1783447265625
        },
        {
            "corpus_id": "251223993",
            "title": "Momentum Transformer: Closing the Performance Gap Between Self-attention and Its Linearization",
            "text": "Transformers have achieved remarkable success in sequence modeling and beyond but suffer from quadratic computational and memory complexities with respect to the length of the input sequence. Leveraging techniques include sparse and linear attention and hashing tricks; efficient transformers have been proposed to reduce the quadratic complexity of transformers but significantly degrade the accuracy. In response, we first interpret the linear attention and residual connections in computing the attention map as gradient descent steps. We then introduce momentum into these components and propose the \\emph{momentum transformer}, which utilizes momentum to improve the accuracy of linear transformers while maintaining linear memory and computational complexities. Furthermore, we develop an adaptive strategy to compute the momentum value for our model based on the optimal momentum for quadratic optimization. This adaptive momentum eliminates the need to search for the optimal momentum value and further enhances the performance of the momentum transformer. A range of experiments on both autoregressive and non-autoregressive tasks, including image generation and machine translation, demonstrate that the momentum transformer outperforms popular linear transformers in training efficiency and accuracy.",
            "score": 0.5604654707609302,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10284423828125
        },
        {
            "corpus_id": "273323882",
            "title": "On-Chip Learning via Transformer In-Context Learning",
            "text": "Recurrent Neural Networks (RNNs) and in particular transformers have become essential components for a wide variety of scalable sequence processing models [3], [11], [12], [31], [37]. Transformers and their RNN-based variants have drawn significant attention thanks to their ability to be massively scaled [14], [15] and their ability to be used as foundation models [5]. Their emergent ability to learn new input-output functions during inference, also called in-context learning, is believed to be at the heart of their performance capabilities [34]. \n\nDuring inference, the self-attention mechanism of transformers requires loading prior token caches from memory, scaling poorly with the sequence length and leading to very low arithmetic intensity (i.e. few compute per memory operations) [17]. Thus, there is a growing interest in designing dedicated in and near-memory processors to accelerate transformer models [18], [20], [29], [35]. \n\nTransformer models consist almost exclusively of operations that are easily realizable on neuromorphic substrates in a local fashion. Specifically, they do not require weight sharing or non-linearities except for the normalization and softmax operation and activation functions. \n\nChips designed for in-and near-memory processing enable the local storage of token caches, specifically the key and value representations. This allows for immediate reuse of those representations as weight matrices during subsequent token processing steps, eliminating the need to transfer them to other parts of the chip or retrieve them from external memory. Additionally, autoregressive inference avoids the instantiation of a full attention mask, which typically scales quadratically with sequence length, thereby significantly reducing peak memory requirements. These advantages underscore the potential of in-and near-memory processing for autoregressive transformer inference, while also introducing the need for dynamic weightmemory adjustment mechanisms. \n\nFurthermore, several studies have demonstrated activation sparsity that naturally arises from softmax and rectified linear unit (ReLU) activation functions [2], [22], [24], [27] which can be leveraged for more efficient processing in event-based architectures.",
            "score": 0.559655702557156,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1989
                },
                {
                    "start": 1992,
                    "end": 2252
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 176,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 919,
                    "end": 923,
                    "matchedPaperCorpusId": "236151453"
                },
                {
                    "start": 937,
                    "end": 941,
                    "matchedPaperCorpusId": "264146175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1756591796875
        },
        {
            "corpus_id": "278310868",
            "title": "Compact Recurrent Transformer with Persistent Memory",
            "text": "Numerous innovative architectures and algorithms aimed at enhancing various aspects of language modeling have been proposed [2,12,13,24,26]), but Transformers [33] have become the dominant approach to language/sequence modeling. \n\nEffective language models require incorporating a broader context; e.g., enriched memory plays a pivotal role in improving predictions. Prior approaches leveraged recurrent neural network (RNN) architectures such as GRU [6] and LSTM [16]. These architectures process data in sequence and maintain history/context information through a hidden state, which is updated every time step. However, training these models using backpropagation-throughtime (BPTT) can result in vanishing/exploding gradients during training. Various solutions to this problem have emerged where recurrent weights are updated to maintain orthogonal/unitary attributes (e.g., multiplicative updates [35], Givens rotations [20], Householder reflections [25], and Cayley transforms [14,15,21,22,27]). \n\nThe introduction of Transformers in 2017 brought in new methodologies, showcasing state-of-the-art results [7,28,32]. However, standard Transformers are restricted to processing a fixed-length context. In response to this challenge, various approaches have been proposed that introduce the concept of memory into Transformers to correlate information from different segments of a longer sequence. Transformer-XL [8] extends the Transformer architecture by incorporating a segment-level recurrence mechanism, allowing the model to capture longer-term dependencies beyond a fixed length. However, this ability comes at a cost: a sizeable increase in the number of tokens used to compute the keys and values of the self-attention mechanism. Due to the quadratic complexity of self-attention, this results in an increase in compute and subsequently negatively effects power consumption and processing latency. Thus, while Transformer-XL typically outperforms the Transformer model in terms of performance, it generally is ill-suited for applications and devices requiring limited resource use. \n\nRecurrent Memory Transformer (RMT) [4] builds upon Transformer-XL as a memory-augmented segment-level recurrent Transformer.",
            "score": 0.5596307891516821,
            "section_title": "Related Works",
            "char_start_offset": 8551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 231,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2093
                },
                {
                    "start": 2096,
                    "end": 2220
                }
            ],
            "ref_mentions": [
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 902,
                    "end": 906,
                    "matchedPaperCorpusId": "14702380"
                },
                {
                    "start": 925,
                    "end": 929,
                    "matchedPaperCorpusId": "13383426"
                },
                {
                    "start": 955,
                    "end": 959,
                    "matchedPaperCorpusId": "11182533"
                },
                {
                    "start": 983,
                    "end": 987,
                    "matchedPaperCorpusId": "208157916"
                },
                {
                    "start": 987,
                    "end": 990,
                    "matchedPaperCorpusId": "44778792"
                },
                {
                    "start": 990,
                    "end": 993,
                    "matchedPaperCorpusId": "59222698"
                },
                {
                    "start": 993,
                    "end": 996,
                    "matchedPaperCorpusId": "53286454"
                },
                {
                    "start": 1111,
                    "end": 1114,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1436767578125
        },
        {
            "corpus_id": "254275267",
            "title": "Document-Level Abstractive Summarization",
            "text": "Finally, sparse attention mechanisms directly tackle the issue of time and memory quadratic complexity with sequence length. Instead of using a full attention pattern, primacy is given to the local context (local attention window), while also incorporating some global attention elements that provide access to the global context. This sparsity approach provides a considerable context of the full sequence while significantly decreasing complexity. Beltagy et al. [17] and Zaheer et al. [18] propose drop-in replacements for the standard attention mechanisms, reporting results for the standard Transformer [3] and PEGASUS [6] architectures, respectively. Similarly, Guo et al. [19] extends the original T5 architecture [5] with an attention sparsity pattern, applied to the encoder layer only. \n\nWhile all approaches achieved state-of-the-art performances on the arXiv dataset, not all models are designed to handle the same input length, as illustrated in Table 1. Considering shorter input lengths as a limitation for the specific task of document-length summarization, the LongT5 approach proposed in [19] reports the most satisfactory results in both domains (performance and input length).",
            "score": 0.5594613866941227,
            "section_title": "Long document summarization",
            "char_start_offset": 4834,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1196
                }
            ],
            "ref_mentions": [
                {
                    "start": 488,
                    "end": 492,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 608,
                    "end": 611,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 624,
                    "end": 627,
                    "matchedPaperCorpusId": "209405420"
                },
                {
                    "start": 721,
                    "end": 724,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1187744140625
        },
        {
            "corpus_id": "271097926",
            "title": "HDT: Hierarchical Document Transformer",
            "text": "HDT resolves these problems by simultaneous contextualization over all hierarchy levels in a single layer using an efficient hierarchical attention pattern.\n\nDue to the quadratic memory and time complexity of the self-attention block of Transformer models (Vaswani et al., 2017), major efforts have been devoted to reducing complexity, in particular when applying attention to longer sequences.One branch of work focuses on using sparse attention patterns to reduce computation while maintaining expressiveness including Sparse Transformer (Rewon et al., 2019), ETC (Ainslie et al., 2020), BigBird (Zaheer et al., 2020), Longformer (Beltagy et al., 2020) and CoLT5 (Ainslie et al., 2023) among others.Another branch of work aims at speeding up attention computation by considering GPU hardware characteristics.FlashAttention 1 & 2 (Dao et al., 2022;Dao, 2023) propose tiling with block-wise attention to reduce memory IOs and compute attention blocks in parallel in Static Random Access Memory (SRAM).Matteo et al. (2023) extends the idea of FlashAttention to key/query dropping and hashing-based attention that was originally proposed by Kitaev et al. (2020).Our work builds upon both branches: We propose a dynamic sample-dependent sparse attention pattern which exploits the structure of text documents.For efficiency, we follow Dao et al. (2022) and implement this pattern as a customized memory-aware kernel.",
            "score": 0.5592753086218976,
            "section_title": "Related Work",
            "char_start_offset": 5946,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 158,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 701
                },
                {
                    "start": 701,
                    "end": 810
                },
                {
                    "start": 810,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1160
                },
                {
                    "start": 1160,
                    "end": 1306
                },
                {
                    "start": 1306,
                    "end": 1413
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 278,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 566,
                    "end": 587,
                    "matchedPaperCorpusId": "221845203"
                },
                {
                    "start": 831,
                    "end": 849,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1139,
                    "end": 1159,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1332,
                    "end": 1349,
                    "matchedPaperCorpusId": "249151871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17822265625
        },
        {
            "corpus_id": "276885221",
            "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
            "text": "The existing methods, LASP (Sun et al., 2024a) and its improved version LASP-2 (Sun et al., 2025), are designed specifically to leverage the right-productfirst property of linear attention techniques for efficient sequence parallelism (SP). LASP employs a point-to-point ring-style communication pattern, facilitating the exchange of incremental memory states across devices. This communication pattern is particularly effective for managing dependencies while minimizing the data transferred between devices, enhancing the scalability of SP. LASP-2 further refines this approach by replacing the ring-style communication with an all-gather collective communication operation, streamlining the entire communication process. This modification not only simplifies the communication structure but also improves the parallelism of computation and communication. \n\nIn this work, we extend the capabilities of LASP series to the Linear-MoE system, allowing for the efficient SP training on LSM modules, particularly when dealing with extremely long sequences across large-scale distributed clusters. This extension significantly enhances the scalability and efficiency of training large-scale Linear-MoE models with long-context sequences on extensive compute re- \n\nFigure 2: Sequence Parallelism Approach on Hybrid Linear-MoE models. We exemplify the parallelism on the hybrid layers of LSM and standard attention with both TP and SP (both have a dimension of 2). The communication operations colored in yellow and green are for TP and SP, respectively. AG/RS: all-gather in forward and reducescatter in backward, RS/AG: reduce-scatter in forward and all-gather in backward, AG/No: all-gather in forward and no-op in backward, No/AG: no-op in forward and all-gather in backward. Note that the SP communication operations for linear attention operate on the memory state M s \u2208 R d\u00d7d , while for standard attention, they operate on states K s , V s \u2208 R C\u00d7d . \n\nsources. A detailed breakdown of the SP algorithm on Linear-MoE, with and without masking, is provided in Appendix A.3.",
            "score": 0.5589986310671853,
            "section_title": "Sequence Parallelism on Linear-MoE",
            "char_start_offset": 10488,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2073
                }
            ],
            "ref_mentions": [
                {
                    "start": 27,
                    "end": 46,
                    "matchedPaperCorpusId": "276259019"
                },
                {
                    "start": 79,
                    "end": 97,
                    "matchedPaperCorpusId": "276259019"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75830078125
        },
        {
            "corpus_id": "273186571",
            "title": "Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective",
            "text": "Some other LLMs integrate transformer-based and SSM-based LLMs, leveraging the complete information extraction ability of attention and the information compression capability of SSM to enhance the performance for long inputs. The Block-State-Transformer (BST) [92] integrates an SSM sublayer for long-range contextualization with a blocktransformer sublayer for short-term sequence representation. This architecture combines the strengths of SSMs and block attention, and explores three distinct, fully parallelizable variants. Griffin [93] combines gated linear recurrence with local attention, featuring the Hawk layer (a type of RNN with gated linear recurrence). Jamba [94] interleaves blocks of transformer and Mamba layers, harnessing the strengths of both model families. In some of these layers, mixture of expert (MoE) is added to increase model capacity while keeping the number of active parameters manageable. Unlike BST, Infini-Transformer [95] combines masked local attention and long-term linear attention within a single Transformer block. This Infini-Attention mechanism incorporates compressed memory into the original attention mechanism within the constraints of limited memory and computational resources. MEGALODON [96] is a neural architecture designed for efficient sequence modeling with infinite context length. It builds on the MEGA architecture (exponential moving average with gated attention) and introduces complex exponential moving average (CEMA), timestep normalization layer, normalized attention mechanism, and a pre-norm configuration with two-hop residuals to enhance its capability and stability.",
            "score": 0.5588141006077865,
            "section_title": "Hybrid LLM",
            "char_start_offset": 14744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1635
                }
            ],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 264,
                    "matchedPaperCorpusId": "259187506"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10540771484375
        },
        {
            "corpus_id": "277151262",
            "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
            "text": "The transformative impact of Large Language Models (LLMs) (Dubey et al., 2024;OpenAI, 2023) is expanding beyond natural language processing, steering in a new era of multimodal capabilities. Long-Context Transformer Models (LCTMs) are emerging as essential tools in this evolution, particularly for tasks like video understanding (Lin et al., 2023;Wang et al., 2024) and video generation (Kong et al., 2025) that demand processing and generating exceptionally long sequences of information. These models hold the key to unlocking brilliant systems capable of interacting with the world in a human-like way, understanding and generating not just text, but also visual information over extended periods. Imagine AI agents engaging in seamless, multimodal, day-long interactions, or powerful world simulators generating hours of coherent video-tasks that hinge on processing a tremendous number of tokens. \n\nHowever, realizing this vision requires overcoming a significant challenge: the computational burden of the attention mechanism (Vaswani et al., 2017). While crucial for capturing relationships within sequences, attention's cost scales quadratically with sequence length. This quadratic scaling creates a substantial bottleneck during the pre-filling stage, hindering the practical deployment of LCTMs for complex, real-world applications. \n\nIn the pursuit of more efficient Transformers, block-sparse attention (Zaheer et al., 2020;Guo et al., 2024) has emerged as a promising avenue. The core idea is appealing: instead of computing attention between all token pairs, focus resources on the most crucial regions of the attention map, creating \"blocks\" of relevant information. This selective computation promises to drastically reduce computational burden while preserving the model's ability to capture essential longrange dependencies. \n\nYet, existing block-sparse methods have struggled to deliver on their full potential, often grappling with a trade-off between accuracy and efficiency. This stems from the lack of lightweight yet effective mechanisms for identifying and prioritizing truly important attention blocks. The overhead involved in determining block importance can negate the gains achieved through sparsity, rendering these methods impractical for real-world deployment.",
            "score": 0.5587457193724543,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1344
                },
                {
                    "start": 1347,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1844
                },
                {
                    "start": 1847,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2295
                }
            ],
            "ref_mentions": [
                {
                    "start": 1417,
                    "end": 1438,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.162109375
        },
        {
            "corpus_id": "267770686",
            "title": "Do Efficient Transformers Really Save Computation?",
            "text": "Sparse Transformer. Unlike the standard Transformer where each token x (l) can attend to all previous positions {x (l) j : j \u2208 [i]} (see Equation ( 1)), in a Sparse Transformer it only attends to a subset of previous tokens {x \n\nIn this paper, we study a standard design paradigm proposed in Child et al. (2019), which employs a block-wise pattern as shown in the following: \n\nwhere B is called the block size and k, c are constant integers. When B = \u0398( \u221a L), the Sparse Transformer achieves a minimal complexity of \u0398(M (L \n\nWe note that GPT-3 adopted the above design paradigm (Brown et al., 2020). \n\nLinear Transformer. Another line of work proposed to accelerate the attention computation (Equation ( 3)) using kernel-based approximations. A representative approach is the Linear Transformer (Katharopoulos et al., 2020), which approximates Attn (l) with the following formula: \n\nwhere they choose \u03d5(x) = elu(x) + 1. The above computation can be accelerated by rearranging the order of computation so that the intermediate results \n\n\u22a4 associated with different S can be jointly computed using prefix sum, finally yielding a complexity of \u0398(M LD 2 ) which is linear in L.",
            "score": 0.5585385263552101,
            "section_title": "Efficient Transformers",
            "char_start_offset": 12065,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 226
                },
                {
                    "start": 229,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 599
                },
                {
                    "start": 602,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1173
                }
            ],
            "ref_mentions": [
                {
                    "start": 578,
                    "end": 598,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 795,
                    "end": 823,
                    "matchedPaperCorpusId": "220250819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08282470703125
        },
        {
            "corpus_id": "262826014",
            "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
            "text": "Similarly, we evaluate DeepSpeed sequence parallelism on 7 billion and 30 billion parameter sparse attention models and benchmark against Megatron-LM sequence parallelism. Results of our evaluation are shown in Figures 6 and 7. \n\nWe observe similar trends with sparse attention as dense attention experiments. We observe more than 2x throughput performance of DeepSpeed sequence parallelism compared to Megatron-LM. For memory saving, DeepSpeed sequence parallelism leveraging ZeRO-3 scales to 4x longer sequence lengths than Megatron-LM. \n\nDeepSpeed sequence parallelism outperforms Megatron-LM for sequence length that can be run with both. In fact, the current DeepSpeed throughput is bottlenecked by the local sparse attention implementation, and as a result DeepSpeed throughput decreases as the sequence length increases. We expect this gap in performance between DeepSpeed and Megatron-LM to increase further for larger sequence lengths as we improve the performance of the local sparse attention implementation in future. Furthermore, we conduct parallel scaling studies of DeepSpeed-Ulysses along two axes. First, we fix sequence length at 131,072 tokens and increase GPU count from 64 to 256. Second, we increase the GPU count proportionally to the increase in sequence length. The results of these experiments are shown in Tables 2 and 3 respectively. For both evaluations, we used GPT-7B dense model at global batch size of 8. The tables show iteration time in microseconds as well as the achieved throughput measured in per GPU TFLOPs. Table 2 can be interpreted as strong scaling and shows that execution time decreases almost linearly as we increase the GPU count. Table 3 on the other hand, is a form of weak scaling (not in the traditional sense) with caveat that attention computation, a function of sequence length, is quadratic in complexity. In other words, as we increase sequence length, the work increases quadratically.",
            "score": 0.5585385263552101,
            "section_title": "Sparse Attention Evaluation",
            "char_start_offset": 17038,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 227
                },
                {
                    "start": 230,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1944
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70556640625
        },
        {
            "corpus_id": "270371554",
            "title": "Parallelizing Linear Transformers with the Delta Rule over Sequence Length",
            "text": "Given a sequence of d-dimensional input vectors x 1 , . . . , x L , transformers use the softmax attention mechanism to attend over the entire past, \n\n(Here we assume a single attention head for simplicity). Linear attention [48] replaces the exponential kernel exp(k T i q t ) with the dot-product \u03c6(k i ) \n\nT \u03c6(q t ) where \u03c6 : R d \u2192 R n is a feature map. This makes it possible to rearrange computations to represent linear attention as a linear RNN with matrix-valued hidden states, \n\n, \n\nIf we allow n to go to infinity, linear attention can use feature maps associated with polynomial kernels to compute a polynomial approximation to the exponential kernel as a dot product, and can thus approximate softmax attention arbitrarily well [6]. The denominator z T t \u03c6(q t ) \u2208 R can result in numerical instabilities [86] and is removed in recent works [101,63]. It is also common to use the identity mapping for \u03c6 [63,108], which results in the following simplified linear transformer: \n\nEfficient training. Let Q, K, V \u2208 R L\u00d7d be the stacked query, key, value vectors, e.g., Q i = q i . We can then compute the output O \u2208 R L\u00d7d in parallel via O = QK T \u2299 M L V, where M L \u2208 R L\u00d7L is the causal mask. This fully \"parallel form\" and the above \"recurrent form\" have different FLOPs and parallelization tradeoffs. The parallel form takes O(L 2 d + Ld 2 ) and thus requires more FLOPs than the recurrent form, which takes O(Ld 2 ). However, the parallel form is often much faster in practice for moderate-length sequences as it can be done in O(1) steps. This sequence-level parallellism also enables high GPU occupancy. The recurrent form requires fewer FLOPs but cannot be parallelized across sequence length 1 and the elementwise operations involved in recurrence moreover cannot make use of specialized matmul accelerators (e.g., tensor cores). \n\nChunkwise parallel form.",
            "score": 0.5585385263552101,
            "section_title": "Linear Transformer: Transformers with Linear Attention",
            "char_start_offset": 4040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 148
                },
                {
                    "start": 151,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 306
                },
                {
                    "start": 309,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1845
                },
                {
                    "start": 1848,
                    "end": 1872
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 817,
                    "end": 821,
                    "matchedPaperCorpusId": "252992749"
                },
                {
                    "start": 853,
                    "end": 858,
                    "matchedPaperCorpusId": "235377069"
                },
                {
                    "start": 858,
                    "end": 861,
                    "matchedPaperCorpusId": "252781097"
                },
                {
                    "start": 915,
                    "end": 919,
                    "matchedPaperCorpusId": "252781097"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27001953125
        },
        {
            "corpus_id": "264796520",
            "title": "Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities",
            "text": "At the heart of the Transformer model lies the pivotal self-attention mechanism, which computes a weighted sum of all words in a sequence relative to each other. This empowers the model to capture the intricate relationships between words, regardless of their positions in the sequence. Unlike recurrent models such as RNNs [25] or LSTMs [26] which process sequences iteratively, Transformers handle the entire sequence in parallel. This approach, coupled with additional design elements like positional encodings [27] and residual connections [28], empowers Transformers to deliver both efficiency and effectiveness, even when confronted with lengthy sequences.",
            "score": 0.5585385263552101,
            "section_title": "Transformer Architectures",
            "char_start_offset": 10874,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 662
                }
            ],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 328,
                    "matchedPaperCorpusId": "51968707"
                },
                {
                    "start": 514,
                    "end": 518,
                    "matchedPaperCorpusId": "3648736"
                },
                {
                    "start": 544,
                    "end": 548,
                    "matchedPaperCorpusId": "15611611"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.182373046875
        },
        {
            "corpus_id": "273502767",
            "title": "Revealing and Mitigating the Local Pattern Shortcuts of Mamba",
            "text": "Unlike traditional Transformers, which have quadratic complexity in terms of computation and memory, RWKV offers linear scalability, combining the efficiency of RNNs with the performance of Transformers. While RWKV is presented as a hybrid model of RNNs and Transformers, it primarily relies on linear attention and lacks the recurrent properties of traditional RNNs, making it more similar to attention-based models. \n\nBased (Arora et al., 2024b) is a simple yet flexible architecture that integrates linear attention with sliding window mechanisms. By varying the window size and the feature dimension of the linear attention layer, Based can navigate the Pareto frontier of the recall-memory tradeoff. This allows it to effectively achieve full attention-like quality on one end while providing a compact state size for memory-efficient alternatives on the other. \n\nGLA (Yang et al., 2024) introduces an efficient training algorithm for linear attention Transformers that integrates data-dependent gating mechanisms. This algorithm strikes a balance between floatingpoint operations (FLOPs) and parallelism, enabling the use of half-precision matrix multiplications to leverage modern GPU tensor cores. GLA exhibits competitive performance on language modeling tasks, demonstrating that gated linear attention Transformers can compete with strong baselines while ensuring computational efficiency.",
            "score": 0.5582480463452432,
            "section_title": "A.2 Efficient Model Architecture",
            "char_start_offset": 28325,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 417
                },
                {
                    "start": 420,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1400
                }
            ],
            "ref_mentions": [
                {
                    "start": 426,
                    "end": 446,
                    "matchedPaperCorpusId": "268063190"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06536865234375
        },
        {
            "corpus_id": "247618722",
            "title": "ERNIE-SPARSE: Learning Hierarchical Efficient Transformer Through Regularized Self-Attention",
            "text": "Transformer (Vaswani et al., 2017) architecture is a key component for many pretrained language models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), ERNIE (Sun et al., 2020b), ALBERT (Lan et al., 2019), ELECTRA (Clark et al., 2020), T5 (Raffel et al., 2020). Self-attention is one of the most important modules in transformer. It eliminates the sequential dependency constraints of recurrent neural networks by introducing interactions between each token pair to capture contextual information. However, the self-attention's computational complexity and memory cost grow quadratically with sequence length, which comes with complexity O(N 2 ) for processing contexts of N inputs. \n\nOne way to optimize self-attention complexity is introducing sparsity into attention layers (Child et al., 2019;Qiu et al., 2019;Beltagy et al., 2020) by having each token attend to only a subset of tokens in the whole sequence. Recent sparseattention works (Child et al., 2019;Beltagy et al., 2020;Ainslie et al., 2020;Zaheer et al., 2020) introduce global tokens that can attend to the whole sequence. Those global tokens are used as a form of memory to strengthen global information. While this method reduces the complexity of full selfattention, there are two issues with Sparse Transformer that affect performance. \n\nThe first issue is information bottleneck sensitivity as shown in Figure 1. Information bottleneck is a phenomenon caused by the low number of global tokens -the model had to encapsulate the entire input sequence into those global tokens. If the size of the information bottleneck becomes smaller, performance can suffer. The second issue is inconsistency between different attention topologies. Normally, a sparse pattern is pre-defined and fixed in practical training.",
            "score": 0.5580281931895734,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1347
                },
                {
                    "start": 1350,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1820
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 34,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 200,
                    "end": 219,
                    "matchedPaperCorpusId": "198968327"
                },
                {
                    "start": 256,
                    "end": 276,
                    "matchedPaperCorpusId": "208229926"
                },
                {
                    "start": 281,
                    "end": 302,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1047,
                    "end": 1067,
                    "matchedPaperCorpusId": "220831004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1312255859375
        },
        {
            "corpus_id": "277065826",
            "title": "Intra-neuronal attention within language models Relationships between activation and semantics",
            "text": "Studies on attention mechanisms have shown that some heads are specialized, while others are redundant. Luong et al. [129] demonstrated that certain heads capture precise syntactic relations, whereas others focus on global semantic relationships. [130,131] observed that removing several heads does not significantly impact model performance, suggesting compensatory mechanisms among the remaining heads. \n\nStatistical mechanics approaches have been used to analyze interactions between attention paths. [132] modeled the contribution of attention heads via a kernel decomposition : \n\nEach kernel K i corresponds to a specific head, allowing an evaluation of its role in the model's final representation. Results indicate that some heads play a structuring role, while others can be eliminated without significant impact. This observation paves the way for transformer architecture optimizations by reducing redundant heads and improving model interpretability. \n\nThe computational efficiency of transformers has been extensively researched. Sparse Transformers [23] reduce attention complexity to O(n log n) by introducing a sparse attention structure. Reformer [133] optimizes memory management via key-value factorization and local attention. Performer [134] replaces standard attention with a linear approximation, reducing complexity to O(n). This method relies on random projections of keys and queries into a lower-dimensional space, where dot products are computed approximately using kernels favoring efficient factorization. This avoids costly dense matrix multiplications while maintaining high accuracy, making attention scalable even for long sequences. Longformer and BigBird [135] combine local and global attentions to efficiently process long sequences. \n\nOther studies have analyzed attention head specialization in specific contexts. Clark et al. [136] examined BERT attention matrices and found that some heads learn specific syntactic relationships, such as subject-verb dependencies or anaphoric relations. Transformer-XL [137] introduced a recurrent memory mechanism that captures longer-term dependencies, improving text generation and dialogue modeling. \n\nFinally, attention mechanisms have extended to other domains, including computer vision with Vision Transformer (ViT) [138] and Swin Transformer [139], as well as neuroscience and cognitive process modeling [140].",
            "score": 0.5576331846661573,
            "section_title": "body",
            "char_start_offset": 2435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1770
                },
                {
                    "start": 1773,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2178
                },
                {
                    "start": 2181,
                    "end": 2394
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 122,
                    "matchedPaperCorpusId": "1998416"
                },
                {
                    "start": 1163,
                    "end": 1168,
                    "matchedPaperCorpusId": "209315300"
                },
                {
                    "start": 1256,
                    "end": 1261,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1690,
                    "end": 1695,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1866,
                    "end": 1871,
                    "matchedPaperCorpusId": "184486746"
                },
                {
                    "start": 2326,
                    "end": 2331,
                    "matchedPaperCorpusId": "232352874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09222412109375
        },
        {
            "corpus_id": "270226431",
            "title": "LongSSM: On the Length Extension of State-space Models in Language Modelling",
            "text": "Large language models [1] are usually trained on large corpus with a fixed context length (e.g., 2048 tokens).However, attention-based transformer [1] has an O(T 2 ) asymptotic growth with respect to the sequence length.The cost for training and inference is even higher when we are working with long sequences.Recently, state-space models [2,3,4,5] and linear-attention-based transformers [6,7,8] have shown the potential to replace the attention-based transformers [1].SSMs are recurrent models characterized by parallelism in sequence length and inference cost that remains independent of length.\n\nDespite state-space models having a recurrent form and thereby inducing an \"infinitein-time\" memory of the input history, they tend to exhibit limited length extension beyond the training sequence length in mamba [4].In practical applications, where the target inference context often exceeds the length of the training sequence and can even be infinite, a pertinent question arises: Is it possible to train a model with the ability to extend its memory beyond the constraints of a finite training sequence length?The assumption of a finite training sequence length is both reasonable and necessary, given the constraints of GPU memory and the comparatively short training length, especially when compared with the infinite inference length found in real-world applications.\n\nIn the earliest transformer model [9], achieving length extension is challenging, usually constrained by the limitations of absolute position encoding [9,10].Press et al. [10] have demonstrated that introducing attention with linear bias serves as an effective solution to address this limitation and enable length extension.Apart from the additive bias, another stream of works is constructing relative position embedding [11,12,13].In this paper, we adopt the backpropagation through time method that is orthogonal to these previous approaches, and can be used to improve state-space models' length extension capability.Moreover, our method shows that the length extension capability can be achieved without using a long training sequence (Figure 4).\n\nWe summarize our main contributions as follow:\n\n1. We show why the zero hidden states initialization scheme has difficulty doing length extension.",
            "score": 0.5573678133685114,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 110,
                    "end": 220
                },
                {
                    "start": 220,
                    "end": 311
                },
                {
                    "start": 311,
                    "end": 471
                },
                {
                    "start": 471,
                    "end": 599
                },
                {
                    "start": 601,
                    "end": 818
                },
                {
                    "start": 818,
                    "end": 1115
                },
                {
                    "start": 1115,
                    "end": 1375
                },
                {
                    "start": 1377,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1702
                },
                {
                    "start": 1702,
                    "end": 1811
                },
                {
                    "start": 1811,
                    "end": 1999
                },
                {
                    "start": 1999,
                    "end": 2129
                },
                {
                    "start": 2131,
                    "end": 2177
                },
                {
                    "start": 2179,
                    "end": 2277
                }
            ],
            "ref_mentions": [
                {
                    "start": 22,
                    "end": 25,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 340,
                    "end": 343,
                    "matchedPaperCorpusId": "221150566"
                },
                {
                    "start": 343,
                    "end": 345,
                    "matchedPaperCorpusId": "240354066"
                },
                {
                    "start": 467,
                    "end": 470,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1411,
                    "end": 1414,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1528,
                    "end": 1531,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.154541015625
        },
        {
            "corpus_id": "253553171",
            "title": "Token Turing Machines",
            "text": "Our TTM model is related to prior work on designing Transformers to process long sequence lengths and temporal context (e.g., videos), and also models to store and retrieve relevant information from internal/external memory. Transformers for sequences. Pairwise self-attention mechanism proposed in Transformer [64] has been very successful in many vision tasks, including the understanding of image sequences. Extending ViT [23], ViViT [3] and TimeSformer [4] represented video data, a series of space-time tokens. Transformers have since become state-of-the-art in video modeling, being able to handle multiple modalities like audio/text [2,49] and scaling efficiently [24,47].\n\nOne of the major challenges in using Transformers for sequential data is the well-known quadratic computation cost of self-attention. That is, as the number of frames in a video sequence increase, the computation grows quadratically which often soon becomes intractable. There is a wide body of work on reducing this to enable transformers to handle longer sequence lengths, as summarized in surveys such as [60,61]. Common themes include local-or sparseattention [11,46,71], pooling or reducing the number of tokens within the network [37,54,56] and approximations of the attention matrix [12,50,65].\n\nHowever, in the sequential inference problems considered in this paper, efficient operations for handing longer sequence lengths are often not sufficient themselves, as we do not want to perform redundant operations at every new time step, when new input tokens are given.\n\nTransformers with memories. One manner of reducing redundancy over time-steps is to leverage models with memory. There are a number of works using Transformers to retrieve relevant information from external memories/knowledge bases [5,33,43,67] or historical observations [7,45,54]. In comparison, the memory of our model is also based on historical observations of the model, which informs the current and future predictions; we learn to maintain/read/write to the memory. Another method for reusing computation from previous time steps is to perform causal attention. In this case, the previous activations of the model can be cached, as done in the original implementation of the Transformer [64].",
            "score": 0.5572371238170242,
            "section_title": "Related Work",
            "char_start_offset": 3338,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 315,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 437,
                    "end": 440,
                    "matchedPaperCorpusId": "232417054"
                },
                {
                    "start": 640,
                    "end": 643,
                    "matchedPaperCorpusId": "233346984"
                },
                {
                    "start": 643,
                    "end": 646,
                    "matchedPaperCorpusId": "235694621"
                },
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "233346705"
                },
                {
                    "start": 675,
                    "end": 678,
                    "matchedPaperCorpusId": "235624247"
                },
                {
                    "start": 1093,
                    "end": 1096,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1149,
                    "end": 1152,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 1152,
                    "end": 1155,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "matchedPaperCorpusId": "232110866"
                },
                {
                    "start": 1224,
                    "end": 1227,
                    "matchedPaperCorpusId": "245122526"
                },
                {
                    "start": 1790,
                    "end": 1793,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1793,
                    "end": 1796,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1799,
                    "end": 1802,
                    "matchedPaperCorpusId": "247519194"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.191650390625
        },
        {
            "corpus_id": "273653873",
            "title": "Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning",
            "text": "In the swiftly advancing field of natural language processing (NLP), large language models (LLMs) like GPT-4 and Llama have emerged as vital tools, demonstrating expertise in comprehending and producing human language. These complex scenarios often involve context lengths longer than those LLMs were trained on, posing significant challenges to Transformer-based architectures in handling long sequences. For example, LLMs are expected to read long paragraphs or books to answer questions while they are usually trained with a much smaller context length (like 8k for Llama-3 (Touvron et al., 2023)). \n\nScaling up LLMs for long sequences presents significant challenges related to limited attention windows during training and length extrapolation during inference. First, transformers usually adopt a bounded context window in the training phase due to quadratic computation costs (Vaswani et al., 2017;Tay et al., 2022). To mitigate these costs and expand the training context window, existing works usually adopt attention approximation methods like sparse, low-rank, and softmax-free attention. Despite their efficiency and scalability with relatively long sequences, these methods gain less popularity due to inefficient implementations and incompatibility with existing pre-trained LLMs. \n\nGiven such pretrained LLMs, researchers start to focus more on the length extrapolation challenge: leveraging the short-range modeling power acquired during pre-training to handle unseen longrange dependencies. Addressing this challenge requires methods that take into account both efficiency and effectiveness perspectives. One line of thought is to employ full attention with hardwareefficient implementations such as FlashAttention and quantization for efficiency, paired with positional extrapolation (Su et al., 2024) or interpolation (Chen et al., 2023a) for enhanced performance. \n\nWhile full attention captures all potential correlations, it significantly increases running time, and many correlations in long sequences are in fact unnecessary and distracting. The second line of approaches involves using segmented or sliding windows for efficiency, supplemented by additional recurrent memory modules (Bulatov et al., 2023;Wang et al., 2023) or global sinks (Xiao et al., 2023;Han et al., 2023) to integrate segments.",
            "score": 0.5568807283678021,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1294
                },
                {
                    "start": 1297,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1883
                },
                {
                    "start": 1886,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2324
                }
            ],
            "ref_mentions": [
                {
                    "start": 883,
                    "end": 905,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 905,
                    "end": 922,
                    "matchedPaperCorpusId": "221702858"
                },
                {
                    "start": 1802,
                    "end": 1819,
                    "matchedPaperCorpusId": "233307138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1822509765625
        },
        {
            "corpus_id": "235743105",
            "title": "Long-Short Transformer: Efficient Transformers for Language and Vision",
            "text": "However, due to the loss of high-fidelity token-wise information, their performance sometimes is not as good as full attention or sparse attention on tasks that require fine-grained local information, including standard benchmarks in language [20] and vision [21]. \n\nDespite the rapid progress in efficient Transformers, some proposed architectures can only be applied to bidirectional models [e.g., 15,16,18]. Transformer-based autoregressive models have achieved great successes in language modeling [22], image synthesis [23], and text-to-image synthesis [24], which also involve long texts or high-resolution images. It is desirable to design an efficient transformer that can be applied to both autoregressive and bidirectional models. \n\nIn this work, we unify a local window attention and a novel long-range attention into a single efficient attention mechanism. We show that these two kinds of attention have complementary effects that together yield the state-of-the-art results on a range of tasks in language and vision, for both autoregressive and bidirectional models. Specifically, we make the following contributions: \n\n\u2022 We propose Long-Short Transformer (Transformer-LS), an efficient Transformer that integrates a dynamic projection based attention to model long-range correlations, and a local window attention to capture fine-grained correlations. Transformer-LS can be applied to both autoregressive and bidirectional models with linear time and memory complexity. \n\n\u2022 We compute a dynamic low-rank projection, which depends on the content of the input sequence. \n\nIn contrast to previous low-rank projection methods, our dynamic projection method is more flexible and robust to semantic-preserving positional variations (e.g., insertion, paraphrasing). We demonstrate that it outperforms previous low-rank methods [17,18] on Long Range Arena benchmark [20]. \n\n\u2022 We identify a scale mismatch problem between the embeddings from the long-range and shortterm attentions, and design a simple but effective dual normalization strategy, termed DualLN, to account for the mismatch and enhance the effectiveness of the aggregation.",
            "score": 0.5568476371767767,
            "section_title": "Introduction",
            "char_start_offset": 1961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 267,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1582
                },
                {
                    "start": 1585,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1878
                },
                {
                    "start": 1881,
                    "end": 2144
                }
            ],
            "ref_mentions": [
                {
                    "start": 400,
                    "end": 403,
                    "matchedPaperCorpusId": "221845203"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06646728515625
        },
        {
            "corpus_id": "266174680",
            "title": "SCCA: Shifted Cross Chunk Attention for long contextual semantic expansion",
            "text": "The Transformer architecture is rapidly becoming one of the most widely applied deep learning architectures, and the emergence of Large Language Models (LLMs) using Transformer has brought improvements to many tasks.However, a significant challenge lies in the quadratic computation complexity introduced by the vanilla transformer, which hinders the increase in input length.\n\nSome researchers opt for using sparse attention patterns to reduce computing complexity and save memory.While sparse transformers like local attention (Qiu et al., 2020) and sliding window context (Beltagy et al., 2020) based on window size are proposed, these attention pattern face a limitation in information flow within the window or chunk.Other approaches, such as dilated window attention (Beltagy et al., 2020) and sparse Transformer (Child et al., 2019), require changes to the model structure and lack a corresponding CUDA-friendly implementation.Swin Transformer (Liu et al., 2021) and Dilated Neighborhood Attention (DNA) provide a cross-layer attention pattern in chunk-based attention, introducing information flow between different chunks or windows.However, global information flow remains lacking in these methods.\n\nWhile current LLMs have revolutionized language modeling and showcased impressive task performance (Dasigi et al., 2021;Cohan et al., 2018;Ko\u010disk\u00fd et al., 2018;Shi et al., 2023;Huang et al., 2021;Shaham et al., 2022;Bai et al., 2023), they are constrained by pre-defined context window size.The performance significantly declines when input tokens exceed these limited context length.Direct context extrapolation in LLMs using positional embedding, such as RoPE, can lead to catastrophic consequences.To address this out-of-distribution problem, various Position Interpolation algorithms (Chen et al., 2023a;Peng and Quesnelle, 2023;Peng et al., 2023) have been introduced.While these methods effectively extrapolate the length of LLMs using RoPE, full fine-tuning is still required.",
            "score": 0.5567599371275802,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 376
                },
                {
                    "start": 378,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 722
                },
                {
                    "start": 722,
                    "end": 934
                },
                {
                    "start": 934,
                    "end": 1142
                },
                {
                    "start": 1142,
                    "end": 1208
                },
                {
                    "start": 1210,
                    "end": 1501
                },
                {
                    "start": 1501,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1711
                },
                {
                    "start": 1711,
                    "end": 1883
                },
                {
                    "start": 1883,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 529,
                    "end": 547,
                    "matchedPaperCorpusId": "207847640"
                },
                {
                    "start": 951,
                    "end": 969,
                    "matchedPaperCorpusId": "232352874"
                },
                {
                    "start": 1330,
                    "end": 1349,
                    "matchedPaperCorpusId": "4894594"
                },
                {
                    "start": 1349,
                    "end": 1370,
                    "matchedPaperCorpusId": "2593903"
                },
                {
                    "start": 1370,
                    "end": 1387,
                    "matchedPaperCorpusId": "259370527"
                },
                {
                    "start": 1387,
                    "end": 1406,
                    "matchedPaperCorpusId": "233033613"
                },
                {
                    "start": 1406,
                    "end": 1426,
                    "matchedPaperCorpusId": "245836939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0975341796875
        },
        {
            "corpus_id": "258740790",
            "title": "CageViT: Convolutional Activation Guided Efficient Vision Transformer",
            "text": "Transformer [37] and its variations have achieved great success in both the visual and linguistic areas. The Transformer, for instance, is the backbone architecture of several advanced pre-trained language models in natural language processing, including BERT [9] and GPT [25]. In vision-related tasks, such as classification [10], object detection [3], semantic segmentation [50], pose estimation [17] and video captioning [52], Transformer also shows significant potential. Core to a Transformer model is the self-attention mechanism, which allows the Transformer to represent the contexts within an input sequence [21]. Due to the fact that self-attention computes the dot-product between each pair of input representations, its complexity is quadratic to the length of the input sequence [37]. Therefore, it is challenging for standard Transformer models to effectively process lengthy input sequences [35]. In computer vision, however, many tasks [10,42,41] demand high-resolution images that are transformed into lengthy series of image patches ahead of being processed using the Transformer model. Consequently, it is essential to construct an effective attention mechanism capable of modeling lengthy sequences. The overview of activation guided attention in our model. After partitioning the input image into patches of fixed size, the heat map produced by Grad-CAM++ via the auxiliary convolutional model M conv is utilized to choose the major tokens and the minor tokens. Then, for each patch, we perform a linear embedding. The minor tokens (grey tokens) are combined into several representation tokens (N f < N m ) via a fusion network in order to decrease computation while preserving background knowledge. After incorporating positional embedding and fusion embedding into tokens, the output is fed into an efficient transformer encoder. \n\nUtilizing more efficient attention processes, many approaches have been proposed to accelerate the vanilla Transformer [8,15,22]. In one direction, the approaches lower the length of input tokens, such as [39,32] by leveraging low-rank projections. These methods employ complex projection techniques to reduce computing effort while retaining feature representation.",
            "score": 0.5566909838965673,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 16,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 349,
                    "end": 352,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 376,
                    "end": 380,
                    "matchedPaperCorpusId": "229924195"
                },
                {
                    "start": 398,
                    "end": 402,
                    "matchedPaperCorpusId": "229297707"
                },
                {
                    "start": 424,
                    "end": 428,
                    "matchedPaperCorpusId": "4564155"
                },
                {
                    "start": 792,
                    "end": 796,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 956,
                    "end": 959,
                    "matchedPaperCorpusId": "232417787"
                },
                {
                    "start": 959,
                    "end": 962,
                    "matchedPaperCorpusId": "235652212"
                },
                {
                    "start": 1980,
                    "end": 1983,
                    "matchedPaperCorpusId": "3353110"
                },
                {
                    "start": 2064,
                    "end": 2067,
                    "matchedPaperCorpusId": "218487423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10595703125
        },
        {
            "corpus_id": "211132628",
            "title": "Low-Rank Bottleneck in Multi-head Attention Models",
            "text": "Given the significance of self attention models, there has been work trying to both improve the performance and speed up the computation in Transformers. Ott et al. [2018] and You et al. [2019] reduce precision and use large batch training to reduce the training time of the attention models. Child et al. [2019] propose sparse self attention models to speed up the computation in the attention layer for long sequence data generation tasks. They show that these sparse attention models can be trained on tasks with sequence length greater than 10k without sacrificing the accuracy. Dehghani et al. [2018] propose a depth recurrent Transformer network that reuses the parameters across layers. They show that this modification makes the Transformer networks Turing complete even with finite precision weights. Yang et al. [2019] propose a new way to increase the effective sequence length that the Transformer attends to, by reusing the intermediate embeddings across sequences. They show that the modified architecture performs better on tasks that require computing context over longer sequence lengths. We note that most of these modifications rely on the multi-head self attention, the same building block of the Transformers. Our work is studying this basic multi-head attention layer, and suggesting a new way to set the head size, which can be easily applied along with any of the above architectural modifications. Wu et al. [2019] propose to replace the self-attention layer with lightweight dynamic convolutions and show improved performance on machine translation and language modeling. Even though the resulting model has faster inference time, it still needs to use a large embedding size (1024), as big as the original attention models. We believe the techniques in this paper can be combined with these results to realize both smaller embedding size and faster inference time. Sun et al. [2019] perform neural architecture search using evolutionary methods on sequence to sequence models and find an evolved transformer architecture, which in addition to multi-head attention units, has convolution filter and gated linear units. Our proposed modifications stay closer to Transformers in spirit and can be used as seed units for this architecture search. Yang et al. [2017] have studied the effect of rank constraint caused by the small projection sizes in computing the softmax loss. The situation in self attention layers is a bit different.",
            "score": 0.5561817427024329,
            "section_title": "Related Works",
            "char_start_offset": 6912,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2269
                },
                {
                    "start": 2270,
                    "end": 2399
                },
                {
                    "start": 2400,
                    "end": 2458
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 171,
                    "matchedPaperCorpusId": "44131019"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1688232421875
        },
        {
            "corpus_id": "270620100",
            "title": "DeciMamba: Exploring the Length Extrapolation Potential of Mamba",
            "text": "Lengthy sequences, which can span up to millions of tokens, are common in real-world applications including long books, high-resolution video and audio signals, and genomic data. Consequently, developing Deep Learning (DL) sequence models capable of effectively managing long contexts is a critical objective. Transformers (Vaswani et al., 2017), despite their current dominance in general DL tasks, still face challenges in processing long sequences. Specifically, their quadratic complexity in sequence length makes them computationally demanding, restricting the ability to train them over long sequences and very large datasets. \n\nIn recent years, substantial efforts have been made in order to tackle this challenge. The most significant advancements include efficient implementations that increase the model's context length during training (Dao et al., 2022;Liu et al.), and context-extension methods (Chen et al., 2023b;Peng et al., 2023b) designed to effectively expand the context after training. However, recent studies suggest that long-range processing is still an unresolved problem (Li et al., 2024a;Liu et al., 2024a). \n\nOne promising approach in this domain is the development of attention-free networks with subquadratic complexity, which can be trained more efficiently over long sequence data. In a recent line of works (Gu et al., 2021;Gu et al.;Gupta et al., 2022), the family of state-space layers has been introduced. These layers can be seen as theoretically grounded linear RNNs that can be efficiently computed in parallel via convolutions, thanks to a closed-form formulation of their linear recurrent rule. A recent advancement by Gu & Dao (2024) presented Mamba, which builds on top of an expressive variant of SSMs called Selective State-Space Layers (S6).",
            "score": 0.5554862906565334,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1787
                }
            ],
            "ref_mentions": [
                {
                    "start": 847,
                    "end": 865,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1115,
                    "end": 1133,
                    "matchedPaperCorpusId": "259360665"
                },
                {
                    "start": 1367,
                    "end": 1386,
                    "matchedPaperCorpusId": "247762199"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1578369140625
        },
        {
            "corpus_id": "229923177",
            "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
            "text": "Sparse Attention Transformers have been extensively explored Tay et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). The key idea is to sparsify the self-attention operation, which scales quadratically with the sequence length. For instance, the Sparse Transformer  uses a dilated sliding window that reduces the complexity to O(L \u221a L), where L is the sequence length. Reformer (Kitaev et al., 2020) further reduces the complexity to O(L log L) using locality-sensitive hashing attention to compute the nearest neighbors. BP-Transformers (Ye et al., 2019) employs a binary partition for the input sequence. Recently, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) have been proposed, and both achieved state-of-the-art performance on a variety of long-document tasks. They reduce the complexity of self-attention to O(L) by combining random attention, window attention, and global attention. However, it has been proven in Zaheer et al. (2020) that sparse attention mech-anisms cannot universally replace dense attention mechanisms; moreover, solving the simple problem of finding the furthest vector requires \u2126(n)-layers of a sparse attention mechanism but only O(1)layers of a dense attention mechanism. In addition, the aforementioned methods require customized CUDA kernels or TVM programming to implement sparse attention, which are not maintainable and are difficult to use. In this study, we adopt a different approach to adapting Recurrence Transformers for a pretraining-then-finetuning setting, to model a long document.\n\nRecurrence Transformers Rae et al., 2019) have been successfully applied in generative language modeling. They employ the Transformer decoder as a parametric model for each conditional distribution in p(x) = L t=1 p(x t |x <t ), where x denotes a text sequence. To capture long dependencies, they process the text in segments from left to right based on the segment recurrence mechanism . This mechanism maintains a memory bank of past activations at each layer to preserve a history of context. Compressive Transformer (Rae et al., 2019) adds a",
            "score": 0.5549893090080279,
            "section_title": "Related Work",
            "char_start_offset": 5099,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 382,
                    "end": 403,
                    "matchedPaperCorpusId": "209315300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.132568359375
        },
        {
            "corpus_id": "219530577",
            "title": "Linformer: Self-Attention with Linear Complexity",
            "text": "Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.",
            "score": 0.5545499109534037,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11376953125
        },
        {
            "corpus_id": "272911381",
            "title": "On the Optimal Memorization Capacity of Transformers",
            "text": "While Corollary 4.1 demonstrates that a Transformer with a single-layer self-attention can achieve memorization in the seq-to-seq prediction setting, it inevitably requires O(n 2 ) computational complexity due to the self-attention mechanism. In line with recent efforts to improve the scalability of Transformers by making attention maps sparse (Zaheer et al., 2020;Yun et al., 2020), using two self-attention layers and appending an additional token to the input sequence allows us to achieve the same behavior with an O(n) connections without affecting the order of parameter counts. This idea of aggregating global information into the additional token has gained interest in recent studies (Darcet et al., 2023;Wang et al., 2024a). \n\nThis corollary shows that at least \u00d5( \u221a nN ) parameters with bit complexity \u00d5( \u221a nN ) are enough to memorize N input sequences of input length n. The next question is: is this order optimal for the seq-to-seq prediction setting? As in the case of next-token prediction setting (Remark 4.4), we can leverage a similar argument to show that this is optimal, at least in terms of bit counts. Unlike the next-token prediction setting, it is challenging to analyze the optimal lower bound on the number of parameters necessary to memorize N input sequences with input length n for the seq-toseq prediction setting, mainly due to the presence of the softmax function. However, we partially answer this question by considering a Transformer that uses not the softmax function, but instead the hardmax function, often viewed as an approximation of the softmax. \n\nMore rigorously, we introduce the following self-attention layer with the hardmax function, which we call the hard attention layer. For each block l \u2208 [L] and its input Z \u2208 R m\u00d7n , the hard attention layer at block l calculates \n\nwhere \u03c3 H : R n\u00d7n \u2192 [0, 1] n\u00d7n is the column-wise hardmax function (see eq. ( 1) for its definition), and \n\n\u2208 R m\u00d7s are value, key, query and projection matrices at head h \u2208 [H] with head size s, respectively.",
            "score": 0.5544354456649513,
            "section_title": "SEQUENCE-TO-SEQUENCE PREDICTION SETTING",
            "char_start_offset": 24556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 736
                },
                {
                    "start": 739,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1591
                },
                {
                    "start": 1594,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1821
                },
                {
                    "start": 1824,
                    "end": 1929
                },
                {
                    "start": 1932,
                    "end": 2033
                }
            ],
            "ref_mentions": [
                {
                    "start": 695,
                    "end": 716,
                    "matchedPaperCorpusId": "263134283"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0631103515625
        },
        {
            "corpus_id": "267522774",
            "title": "PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels",
            "text": "The quadratic time and memory complexity inherent to self-attention mechanisms, with respect to sequence length, presents a critical computational bottleneck in the training and deployment of large-scale Transformer-based language models. Recent theoretical results indicate the intractability of sub-quadratic softmax attention approximation under reasonable complexity assumptions. This paper addresses this challenge by first demonstrating that polynomial attention with high degree can effectively replace softmax without sacrificing model quality. Next, we develop polynomial sketching techniques from numerical linear algebra to achieve linear-time polynomial attention with approximation guarantees. Crucially, our approach achieves this speedup without requiring the sparsification of attention matrices. We also present a block-based algorithm to apply causal masking efficiently. Combining these techniques, we provide \\emph{PolySketchFormer}, a practical linear-time Transformer architecture for language modeling that offers provable guarantees. We validate PolySketchFormer empirically by training language models capable of handling long contexts. These experiments utilize both synthetic and real-world datasets (PG19, Wikipedia and C4) on Google Cloud TPUs. For context lengths of 32k and GPT-2 style models, our model achieves a 2.5-4x speedup in training compared to FlashAttention, with no observed degradation in quality across our experiments.",
            "score": 0.5536743664231306,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1246337890625
        },
        {
            "corpus_id": "278237532",
            "title": "From Attention to Atoms: Spectral Dictionary Learning for Fast, Interpretable Language Models",
            "text": "The advent of the Transformer architecture [18] revolutionized sequence modeling by replacing recurrent and convolutional operations with self-attention mechanisms that directly capture dependencies across arbitrary token distances. Building on this foundation, bi-directional encoders like BERT [6] and autoregressive language models such as the GPT series [16] have achieved state-of-the-art results on a wide range of natural language processing tasks. These models rely on the full L \u00d7 L attention matrix, where L is the input sequence length, to compute pairwise interactions between tokens. Although highly expressive, this quadratic complexity in both computation and memory becomes prohibitive when scaling to very long contexts, such as entire documents or long code sequences [3,20]. \n\nTo mitigate the cost of full self-attention, a variety of approximations have been proposed. Sparse attention patterns exploit locality or fixed windowing, as in Longformer [2] and the blocksparse model of Child et al. [3]; kernel-based methods like Performer [4] use randomized feature maps to approximate softmax attention in linear time; low-rank factorization approaches such 2 Related Work Fourier and Spectral Methods Fourier transforms have been employed in efficient sequence modeling [14,8], often as submodules within attention blocks or as fixed transformations replacing the attention mechanism entirely. FNet [14] used unparameterized 2D Fourier transforms. Our work extends spectral approaches by learning an explicit, parameterized Fourier dictionary optimized end-to-end specifically for language modeling reconstruction and generation. While spectral methods have found applications in image generation [11] and wavelet transforms have been explored as attention alternatives [12], our approach uniquely adapts spectral dictionary learning, with learnable sinusoidal parameters and per-token coefficients, to the sequential nature of language. \n\nAttention Alternatives Numerous techniques aim to reduce attention's O(L 2 ) computational cost, including sparse attention [2,3], kernel methods like Performer [4], low-rank projections like Linformer [19], and hashing methods like Reformer [13]. Unlike these approaches that primarily approximate the standard attention mechanism, SDGM replaces attention entirely with a learned spectral mixing paradigm, offering a fundamentally different approach to sequence interaction modeling.",
            "score": 0.5531557842279138,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1956
                },
                {
                    "start": 1959,
                    "end": 2206
                },
                {
                    "start": 2207,
                    "end": 2443
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 47,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 296,
                    "end": 299,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 786,
                    "end": 789,
                    "matchedPaperCorpusId": "129945531"
                },
                {
                    "start": 789,
                    "end": 792,
                    "matchedPaperCorpusId": "220831004"
                },
                {
                    "start": 969,
                    "end": 972,
                    "matchedPaperCorpusId": "215737171"
                },
                {
                    "start": 1015,
                    "end": 1018,
                    "matchedPaperCorpusId": "129945531"
                },
                {
                    "start": 1056,
                    "end": 1059,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 1289,
                    "end": 1293,
                    "matchedPaperCorpusId": "234336004"
                },
                {
                    "start": 1418,
                    "end": 1422,
                    "matchedPaperCorpusId": "234336004"
                },
                {
                    "start": 2083,
                    "end": 2086,
                    "matchedPaperCorpusId": "215737171"
                },
                {
                    "start": 2086,
                    "end": 2088,
                    "matchedPaperCorpusId": "129945531"
                },
                {
                    "start": 2120,
                    "end": 2123,
                    "matchedPaperCorpusId": "222067132"
                },
                {
                    "start": 2201,
                    "end": 2205,
                    "matchedPaperCorpusId": "209315300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1693115234375
        },
        {
            "corpus_id": "246017095",
            "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
            "text": "efficiently. To tackle this problem, we proposed Ring Self-Attention (RSA), which circulates key and value embeddings across GPUs in a ring manner. In this case, each device is just required to keep the attention embeddings corresponding to its own sub-sequence. As a result, our sequence parallelism is memory-efficient, especially for long input sequences.\n\nTo model long sequences, existing works mainly focus on sparse attention (e.g., [Zaheer et al., 2020]) with linear instead of quadratic space complexity. In this paper, we aim to solve the long sequence modeling problem from the distributed system perspective. Compared with sparse attention, we devote ourselves to designing and implementing a system instead of a deep learning algorithm to train attentionbased models with longer sequences. Existing pipeline parallelism (PP) [Huang et al., 2018] and tensor parallelism (TP) [Shoeybi et al., 2019]) are designed to cope with a larger model size instead of longer sequences, although they can still process longer sequences to some extent. However, the challenge is, these existing parallelism methods keep the whole sequence on a single device, which limits the maximum length of the input sequence. In contrast, our approach splits the whole sequence into multiple devices, making it possible to fit longer input data.\n\nIn summary, our main contributions are three folds: (1) Our system breaks the length limitation of Transformer model training. SP splits long sequences into multiple chunks and feeds them into different devices. It is memory-efficient because each device only keeps the attention embeddings corresponding to its own sub-sequences. Theoretically, with linear space complexity attention, SP can help us train the at-tention model with infinite long sequences. (2) To our best knowledge, our work first proposed to use distributed system to handle long sequence training for attention-based models. Our implementation is fully based on PyTorch and is compatible with data parallelism (DP), PP, and TP without any extra compiler or library. This makes it possible to integrate SP with DP, PP and TP into 4D parallelism, and pave the way to train large-scale models with long sequences. (3) Our system achieves 3.0\u00d7 maximum sequence length than",
            "score": 0.5530848211482116,
            "section_title": "Introduction",
            "char_start_offset": 1880,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 838,
                    "end": 858,
                    "matchedPaperCorpusId": "233289729"
                },
                {
                    "start": 887,
                    "end": 909,
                    "matchedPaperCorpusId": "233289729"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "253080899",
            "title": "Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences",
            "text": "Efficient Transformers have been developed for long sequence modeling, due to their subquadratic memory and time complexity. Sparse Transformer is a popular approach to improving the efficiency of Transformers by restricting self-attention to locations specified by the predefined sparse patterns. However, leveraging sparsity may sacrifice expressiveness compared to full-attention, when important token correlations are multiple hops away. To combine advantages of both the efficiency of sparse transformer and the expressiveness of full-attention Transformer, we propose Diffuser, a new state-of-the-art efficient Transformer. Diffuser incorporates all token interactions within one attention layer while maintaining low computation and memory costs. The key idea is to expand the receptive field of sparse attention using Attention Diffusion, which computes multi-hop token correlations based on all paths between corresponding disconnected tokens, besides attention among neighboring tokens. Theoretically, we show the expressiveness of Diffuser as a universal sequence approximator for sequence-to-sequence modeling, and investigate its ability to approximate full-attention by analyzing the graph expander property from the spectral perspective. Experimentally, we investigate the effectiveness of Diffuser with extensive evaluations, including language modeling, image modeling, and Long Range Arena (LRA). Evaluation results show that Diffuser achieves improvements by an average of 0.94% on text classification tasks and 2.30% on LRA, with 1.67x memory savings compared to state-of-the-art benchmarks, which demonstrates superior performance of Diffuser in both expressiveness and efficiency aspects.",
            "score": 0.553020576224269,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18798828125
        },
        {
            "corpus_id": "263620755",
            "title": "scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis in Brain",
            "text": "The self-attention operator (Vaswani et al., 2017) is a fundamental mechanism of Transformers. Specifically, given a sequence u \u2208 R L\u00d7D with a length of L, each head of the scaled self-attention operator maps u to y \u2208 R L\u00d7D through a self-attention operator A(u): \n\nwhere W q , W k , W v \u2208 R D\u00d7D represent learnable linear projections for query, key, and value, respectively, and \u03c3 denotes the softmax and scaling operator. Through the self-attention operator, it becomes possible to capture pairwise relationships among all tokens and learn the global context of the input sequence. However, one limitation is that self-attention becomes computationally expensive for long sequences, with a complexity of O(L 2 ). \n\nTo address this computational challenge, several approaches have been developed to reduce the cost of self-attention. For instance, the factorized self-attention has been proposed for the sparse Transformer (Child et al., 2019). The factorized self-attention reduces the memory and computational requirements of self-attention by allowing self-attention heads to attend only to a subset of tokens. \n\nAnother approach is the Performer (Choromanski et al., 2021) which aims to reduce the memory complexity of self-attention. By decomposing the self-attention matrix, Performer can store the implicit attention matrix with linear memory complexity, enabling it to handle longer sequences compared to Transformers with the original self-attention mechanism. However, these approaches require custom kernels that are difficult to reproduce and may involve trade-offs between memory complexity and model expressivity.",
            "score": 0.552931320501374,
            "section_title": "SELF-ATTENTION",
            "char_start_offset": 5664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 263
                },
                {
                    "start": 266,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1114
                },
                {
                    "start": 1117,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1628
                }
            ],
            "ref_mentions": [
                {
                    "start": 1151,
                    "end": 1177,
                    "matchedPaperCorpusId": "222067132"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09857177734375
        },
        {
            "paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336",
            "corpusId": 249151871,
            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
            "venue": "Neural Information Processing Systems",
            "year": 2022,
            "referenceCount": 111,
            "citationCount": 2285,
            "influentialCitationCount": 213,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.14135, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "24593911",
                    "name": "Tri Dao"
                },
                {
                    "authorId": "49577833",
                    "name": "Daniel Y. Fu"
                },
                {
                    "authorId": "2490652",
                    "name": "Stefano Ermon"
                },
                {
                    "authorId": "1755572",
                    "name": "A. Rudra"
                },
                {
                    "authorId": "2061444681",
                    "name": "Christopher R'e"
                }
            ],
            "abstract": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",
            "corpus_id": "249151871",
            "text": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.17431640625
        }
    ],
    "quotes": {
        "cost": 0.1524,
        "quotes": [
            {
                "idx": 0,
                "key": "[240070340 | Li et al. | 2021 | Citations: 148]",
                "snippets": "Sequence parallelism [21] is proposed to enable long-sequence modeling by breaking the memory wall brought by the large sequence dimension. In sequence parallelism, the model is replicated across devices just like data parallelism. The input data is split along the sequence dimension and each device only keeps a sub-sequence. The self-attention module is replaced with the Ring Self-Attention module such that the partial query, key, and value embeddings are exchanged among devices to complete the self-attention calculation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Sequence Parallelism",
                        "pdf_hash": "",
                        "start": 524,
                        "end": 1052,
                        "sentence_offsets": [
                            {
                                "start": 524,
                                "end": 663
                            },
                            {
                                "start": 664,
                                "end": 755
                            },
                            {
                                "start": 756,
                                "end": 851
                            },
                            {
                                "start": 852,
                                "end": 1052
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Sequence parallelism [21] is proposed to enable long-sequence modeling by breaking the memory wall brought by the large sequence dimension. In sequence parallelism, the model is replicated across devices just like data parallelism. The input data is split along the sequence dimension and each device only keeps a sub-sequence. The self-attention module is replaced with the Ring Self-Attention module such that the partial query, key, and value embeddings are exchanged among devices to complete the self-attention calculation."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[246017095 | Li et al. | 2021 | Citations: 102]",
                "snippets": "To model long sequences, existing works mainly focus on sparse attention (e.g., [Zaheer et al., 2020]) with linear instead of quadratic space complexity. In this paper, we aim to solve the long sequence modeling problem from the distributed system perspective. Compared with sparse attention, we devote ourselves to designing and implementing a system instead of a deep learning algorithm to train attentionbased models with longer sequences...Our RSA focuses on full self-attention in this paper. According to Table 2, when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences, because three terms of memory usage include L/N.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 360,
                        "end": 801,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "To model long sequences, existing works mainly focus on sparse attention (e.g., [Zaheer et al., 2020]) with linear instead of quadratic space complexity. In this paper, we aim to solve the long sequence modeling problem from the distributed system perspective. Compared with sparse attention, we devote ourselves to designing and implementing a system instead of a deep learning algorithm to train attentionbased models with longer sequences"
                    },
                    {
                        "section_title": "Maximum sequence length",
                        "pdf_hash": "",
                        "start": 986,
                        "end": 1237,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Our RSA focuses on full self-attention in this paper. According to Table 2, when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences, because three terms of memory usage include L/N."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[258987968 | Liu et al. | 2023 | Citations: 11]",
                "snippets": "Several studies explored sharding large model on distributed devices tensor, data, or sequence parallelism [49,16,55,27,59,31,(Rasley et al., 2020). Ours shares similarities with the sequence parallelism [27] where sequences are distributed across devices, in contrast, ours implements blockwise computation on sequences for each device. This creates an orthogonal relationship between our method and sequence parallelism, allowing for straightforward combination. In addition, our methodology is compatible with both tensor and data parallelism.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[221191193 | Rasley et al. | 2020 | Citations: 1392]": "Explore new techniques in Microsoft's open source library called DeepSpeed, which advances large model training by improving scale, speed, cost, and usability, unlocking the ability to train 100-billion-parameter models. DeepSpeed is compatible with PyTorch. One piece of our library, called ZeRO, is a new parallelized optimizer that greatly reduces the resources needed for model and data parallelism while massively increasing the number of parameters that can be trained. Researchers have used these breakthroughs to create Turing Natural Language Generation (Turing-NLG), which at the time of its release was the largest publicly known language model at 17 billion parameters. In addition we will also go over our latest transformer kernel advancements that led the DeepSpeed team to achieve the world fastest BERT pretraining record. The Zero Redundancy Optimizer (ZeRO) is a novel memory optimization technology for large-scale distributed deep learning. ZeRO can train deep learning models with over 100 billion parameters on the current generation of GPU clusters at three to five times the throughput of the current best system. It also presents a clear path to training models with trillions of parameters, demonstrating an unprecedented leap in deep learning system technology. DeepSpeed brings state-of-the-art training techniques, such as ZeRO, optimized kernels, distributed training, mixed precision, and checkpointing, through lightweight APIs compatible with PyTorch. With just a few lines of code changes to your PyTorch model, you can leverage DeepSpeed to address underlying performance challenges and boost the speed and scale of your training."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1016,
                        "end": 1544,
                        "sentence_offsets": [
                            {
                                "start": 1016,
                                "end": 1146
                            },
                            {
                                "start": 1147,
                                "end": 1335
                            },
                            {
                                "start": 1336,
                                "end": 1462
                            },
                            {
                                "start": 1463,
                                "end": 1544
                            }
                        ],
                        "ref_mentions": [
                            "221191193"
                        ],
                        "quote": "Several studies explored sharding large model on distributed devices tensor, data, or sequence parallelism [49,16,55,27,59,31,(Rasley et al., 2020). Ours shares similarities with the sequence parallelism [27] where sequences are distributed across devices, in contrast, ours implements blockwise computation on sequences for each device. This creates an orthogonal relationship between our method and sequence parallelism, allowing for straightforward combination. In addition, our methodology is compatible with both tensor and data parallelism."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[262826014 | Jacobs et al. | 2023 | Citations: 120]",
                "snippets": "Fully general and implementation agnostic attention: DeepSpeed sequence parallelism (Ulysses) supports dense as well as sparse attention, and it works with efficient attention implementations such as FlashAttention v2 [Dao, 2023]...We observe similar trends with sparse attention as dense attention experiments. We observe more than 2x throughput performance of DeepSpeed sequence parallelism compared to Megatron-LM. For memory saving, DeepSpeed sequence parallelism leveraging ZeRO-3 scales to 4x longer sequence lengths than Megatron-LM...DeepSpeed sequence parallelism outperforms Megatron-LM for sequence length that can be run with both. In fact, the current DeepSpeed throughput is bottlenecked by the local sparse attention implementation, and as a result DeepSpeed throughput decreases as the sequence length increases. We expect this gap in performance between DeepSpeed and Megatron-LM to increase further for larger sequence lengths as we improve the performance of the local sparse attention implementation in future.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1756,
                        "end": 1985,
                        "sentence_offsets": [
                            {
                                "start": 1754,
                                "end": 1986
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Fully general and implementation agnostic attention: DeepSpeed sequence parallelism (Ulysses) supports dense as well as sparse attention, and it works with efficient attention implementations such as FlashAttention v2 [Dao, 2023]"
                    },
                    {
                        "section_title": "Sparse Attention Evaluation",
                        "pdf_hash": "",
                        "start": 230,
                        "end": 538,
                        "sentence_offsets": [
                            {
                                "start": 230,
                                "end": 309
                            },
                            {
                                "start": 310,
                                "end": 415
                            },
                            {
                                "start": 416,
                                "end": 538
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We observe similar trends with sparse attention as dense attention experiments. We observe more than 2x throughput performance of DeepSpeed sequence parallelism compared to Megatron-LM. For memory saving, DeepSpeed sequence parallelism leveraging ZeRO-3 scales to 4x longer sequence lengths than Megatron-LM"
                    },
                    {
                        "section_title": "Sparse Attention Evaluation",
                        "pdf_hash": "",
                        "start": 541,
                        "end": 1030,
                        "sentence_offsets": [
                            {
                                "start": 541,
                                "end": 642
                            },
                            {
                                "start": 643,
                                "end": 827
                            },
                            {
                                "start": 828,
                                "end": 1029
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "DeepSpeed sequence parallelism outperforms Megatron-LM for sequence length that can be run with both. In fact, the current DeepSpeed throughput is bottlenecked by the local sparse attention implementation, and as a result DeepSpeed throughput decreases as the sequence length increases. We expect this gap in performance between DeepSpeed and Megatron-LM to increase further for larger sequence lengths as we improve the performance of the local sparse attention implementation in future."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[263608461 | Liu et al. | 2023 | Citations: 258]",
                "snippets": "Tensor parallelism can only reduce parts of activations memory and sequence parallelism introduces a significant communication overhead that cannot be fully overlapped with computation. Prior work has studied sharding along sequence and attention heads, and gathering sequences via an optimized all-to-all topology, achieving reduced communication [17]. However, this method is restricted by the number of attention heads and requires gathering the full sequence on each device. In comparison, our approach fully overlaps communication with blockwise computation, enhancing its scalability. Prior work extends sequence parallelism for computing self-attention using a ring topology (Li et al., 2021), which reduces the communication cost compared to standard sequence parallelism. However, overlapping communication with computation remains challenging due to the constraints of arithmetic intensity. The communication overheads render this approach infeasible for training and inference in largecontext scenarios.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[246017095 | Li et al. | 2021 | Citations: 102]": "Transformer achieves promising results on various tasks. However, self-attention suffers from quadratic memory requirements with respect to the sequence length. Existing work focuses on reducing time and space complexity from an algorithm perspective. In this work, we propose sequence parallelism, a memory-efficient parallelism to solve this issue from system perspective instead. Our approach is compatible with most existing parallelisms (e.g., data, pipeline, and tensor parallelism), which means our sequence parallelism makes 4D parallelism possible. More importantly, we no longer require a single device to hold the whole sequence. Besides, using efficient attention with linear complexity, our sequence parallelism enables us to train transformer with infinite long sequence. Specifically, we split the input sequence into multiple chunks and feed each chunk into its corresponding device (i.e., GPU). To compute the attention output, we integrated ring-style communication with self-attention calculation and proposed Ring Self-Attention (RSA). Experiments show that sequence parallelism performs well when scaling with batch size and sequence length. Compared with tensor parallelism, our approach achieved 13.7\\times and 3.0\\times maximum batch size and sequence length respectively when scaling up to 64 NVIDIA P100 GPUs. With efficient attention, sequence can handle sequence with over 114K tokens, which is over 27\\times longer than existing efficient attention works holding the whole sequence on a single device."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1367,
                        "end": 2368,
                        "sentence_offsets": [
                            {
                                "start": 1367,
                                "end": 1552
                            },
                            {
                                "start": 1553,
                                "end": 1720
                            },
                            {
                                "start": 1721,
                                "end": 1845
                            },
                            {
                                "start": 1846,
                                "end": 1957
                            },
                            {
                                "start": 1958,
                                "end": 2134
                            },
                            {
                                "start": 2135,
                                "end": 2254
                            },
                            {
                                "start": 2255,
                                "end": 2368
                            }
                        ],
                        "ref_mentions": [
                            "246017095"
                        ],
                        "quote": "Tensor parallelism can only reduce parts of activations memory and sequence parallelism introduces a significant communication overhead that cannot be fully overlapped with computation. Prior work has studied sharding along sequence and attention heads, and gathering sequences via an optimized all-to-all topology, achieving reduced communication [17]. However, this method is restricted by the number of attention heads and requires gathering the full sequence on each device. In comparison, our approach fully overlaps communication with blockwise computation, enhancing its scalability. Prior work extends sequence parallelism for computing self-attention using a ring topology (Li et al., 2021), which reduces the communication cost compared to standard sequence parallelism. However, overlapping communication with computation remains challenging due to the constraints of arithmetic intensity. The communication overheads render this approach infeasible for training and inference in largecontext scenarios."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[263671659 | Li et al. | 2023 | Citations: 22]",
                "snippets": "Memory-efficient attention. (Dao et al., 2022) and Lefaudeux et al. (2022) propose to use an online normalizer (Milakov & Gimelshein, 2018) to compute the attention in a blockwise and memory-efficient way. It reduces peak memory usage by not materializing large intermediate states, e.g. the attention softmax matrix. In addition, research on sparse attention computes only a sparse subset of the attention score, which also reduces the memory footprints yet may lead to inferior performance (Beltagy et al., 2020;Sun et al., 2022;(Zaheer et al., 2020). In this work, we limit our scope to exact attention. \n\nSequence parallelism and ring attention Ring Self-Attention (Li et al., 2021) is among the first to parallelize Transformers in the sequence dimension. However, its distributed attention design is not optimized for causal language modeling and incompatible with memory-efficient attention, which are crucial for long-context LLM training. Ring Attention (Liu et al., 2023) proposes to compute distributed attention in a memory-efficient blockwise pattern. However, it is also not optimized for causal language modeling, leading to 2\u00d7 extra computation. DISTFLASHATTN optimizes for both memory-efficient attention and causal language modeling.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220831004 | Zaheer et al. | 2020 | Citations: 2103]": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
                    "[249151871 | Dao et al. | 2022 | Citations: 2285]": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy)."
                },
                "metadata": [
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1249,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 27
                            },
                            {
                                "start": 28,
                                "end": 204
                            },
                            {
                                "start": 205,
                                "end": 316
                            },
                            {
                                "start": 317,
                                "end": 551
                            },
                            {
                                "start": 552,
                                "end": 604
                            },
                            {
                                "start": 607,
                                "end": 758
                            },
                            {
                                "start": 759,
                                "end": 945
                            },
                            {
                                "start": 946,
                                "end": 1062
                            },
                            {
                                "start": 1063,
                                "end": 1159
                            },
                            {
                                "start": 1160,
                                "end": 1249
                            }
                        ],
                        "ref_mentions": [
                            "249151871",
                            "220831004"
                        ],
                        "quote": "Memory-efficient attention. (Dao et al., 2022) and Lefaudeux et al. (2022) propose to use an online normalizer (Milakov & Gimelshein, 2018) to compute the attention in a blockwise and memory-efficient way. It reduces peak memory usage by not materializing large intermediate states, e.g. the attention softmax matrix. In addition, research on sparse attention computes only a sparse subset of the attention score, which also reduces the memory footprints yet may lead to inferior performance (Beltagy et al., 2020;Sun et al., 2022;(Zaheer et al., 2020). In this work, we limit our scope to exact attention. \n\nSequence parallelism and ring attention Ring Self-Attention (Li et al., 2021) is among the first to parallelize Transformers in the sequence dimension. However, its distributed attention design is not optimized for causal language modeling and incompatible with memory-efficient attention, which are crucial for long-context LLM training. Ring Attention (Liu et al., 2023) proposes to compute distributed attention in a memory-efficient blockwise pattern. However, it is also not optimized for causal language modeling, leading to 2\u00d7 extra computation. DISTFLASHATTN optimizes for both memory-efficient attention and causal language modeling."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[263828829 | Liu et al. | 2023 | Citations: 89]",
                "snippets": "The second challenge is parallelism for sequence length, that is, Sequence Parallelism (SP) (Li et al., 2021). Since the memory and calculation of a single device are limited, ColossalAI-SP (Li et al., 2021) first proposes the segmentation and parallelism for the sequence dimension in addition to tensor parallelism for the hidden dimension and pipeline parallelism for model depth. On this basis, Ring Attention (Lv et al., 2023) uses blockwise self-attention to split long sequences into different devices and overlap the communication of key-value blocks. Besides, LightSeq (Li et al., 2023b) further improves the efficiency of long sequence modeling through load balancing for causal language modelings and a re-materialization-aware checkpointing strategy. Although the above sequence parallelism can achieve infinitely long sequence modeling, they ignore the compatibility with the existing efficient self-attention mechanisms such as FlashAttention (Dao et al., 2022)Dao, 2023) to achieve input with almost infinity. On the contrary, Megatron-SP (Korthikanti et al., 2022) only uses sequence parallelism during Dropout and Layernorm operations, thereby reducing activation redundancy. In addition, DeepSpeed-Ulysses (Jacobs et al., 2023) uses an alternative all-to-all collective communication gathering weight for attention computation when segmenting the sequence, avoiding communication overhead that originally increases with length.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[246017095 | Li et al. | 2021 | Citations: 102]": "Transformer achieves promising results on various tasks. However, self-attention suffers from quadratic memory requirements with respect to the sequence length. Existing work focuses on reducing time and space complexity from an algorithm perspective. In this work, we propose sequence parallelism, a memory-efficient parallelism to solve this issue from system perspective instead. Our approach is compatible with most existing parallelisms (e.g., data, pipeline, and tensor parallelism), which means our sequence parallelism makes 4D parallelism possible. More importantly, we no longer require a single device to hold the whole sequence. Besides, using efficient attention with linear complexity, our sequence parallelism enables us to train transformer with infinite long sequence. Specifically, we split the input sequence into multiple chunks and feed each chunk into its corresponding device (i.e., GPU). To compute the attention output, we integrated ring-style communication with self-attention calculation and proposed Ring Self-Attention (RSA). Experiments show that sequence parallelism performs well when scaling with batch size and sequence length. Compared with tensor parallelism, our approach achieved 13.7\\times and 3.0\\times maximum batch size and sequence length respectively when scaling up to 64 NVIDIA P100 GPUs. With efficient attention, sequence can handle sequence with over 114K tokens, which is over 27\\times longer than existing efficient attention works holding the whole sequence on a single device.",
                    "[249151871 | Dao et al. | 2022 | Citations: 2285]": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",
                    "[265551519 | Lv et al. | 2023 | Citations: 7]": "Large language models (LLMs) are increasingly pivotal in a wide range of natural language processing tasks. Access to pre-trained models, courtesy of the open-source community, has made it possible to adapt these models to specific applications for enhanced performance. However, the substantial resources required for training these models necessitate efficient solutions. This paper introduces CoLLiE, an efficient library that facilitates collaborative training of large language models using 3D parallelism, parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion, Adan, Sophia, LOMO and AdaLomo. With its modular design and comprehensive functionality, CoLLiE offers a balanced blend of efficiency, ease of use, and customization. CoLLiE has proven superior training efficiency in comparison with prevalent solutions in pre-training and fine-tuning scenarios. Furthermore, we provide an empirical evaluation of the correlation between model size and GPU memory consumption under different optimization methods, as well as an analysis of the throughput. Lastly, we carry out a comprehensive comparison of various optimizers and PEFT methods within the instruction-tuning context. CoLLiE is available at https://github.com/OpenLMLab/collie."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1448,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 111
                            },
                            {
                                "start": 112,
                                "end": 385
                            },
                            {
                                "start": 386,
                                "end": 562
                            },
                            {
                                "start": 563,
                                "end": 765
                            },
                            {
                                "start": 766,
                                "end": 1027
                            },
                            {
                                "start": 1028,
                                "end": 1195
                            },
                            {
                                "start": 1196,
                                "end": 1448
                            }
                        ],
                        "ref_mentions": [
                            "246017095",
                            "246017095",
                            "265551519",
                            "249151871"
                        ],
                        "quote": "The second challenge is parallelism for sequence length, that is, Sequence Parallelism (SP) (Li et al., 2021). Since the memory and calculation of a single device are limited, ColossalAI-SP (Li et al., 2021) first proposes the segmentation and parallelism for the sequence dimension in addition to tensor parallelism for the hidden dimension and pipeline parallelism for model depth. On this basis, Ring Attention (Lv et al., 2023) uses blockwise self-attention to split long sequences into different devices and overlap the communication of key-value blocks. Besides, LightSeq (Li et al., 2023b) further improves the efficiency of long sequence modeling through load balancing for causal language modelings and a re-materialization-aware checkpointing strategy. Although the above sequence parallelism can achieve infinitely long sequence modeling, they ignore the compatibility with the existing efficient self-attention mechanisms such as FlashAttention (Dao et al., 2022)Dao, 2023) to achieve input with almost infinity. On the contrary, Megatron-SP (Korthikanti et al., 2022) only uses sequence parallelism during Dropout and Layernorm operations, thereby reducing activation redundancy. In addition, DeepSpeed-Ulysses (Jacobs et al., 2023) uses an alternative all-to-all collective communication gathering weight for attention computation when segmenting the sequence, avoiding communication overhead that originally increases with length."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[267027729 | Chen et al. | 2024 | Citations: 8]",
                "snippets": "Processing long sequences is crucial for supporting important applications such as dealing with longer histories in chat applications. To this end, sequence parallelism (SP) has emerged as a technique aimed at alleviating activation memory footprints during the training of Transformers. In SP, the input tensor of each Transformer layer is divided along the sequence dimension, allowing for parallel computation across multiple GPUs. This segmentation, in conjunction with activation recomputation, results in a substantial reduction in activation memory requirements by a factor of s sp...DeepSpeed Ulysses utilizes sequence parallelism on Linear, Norm, and Dropout layers, as shown in Figure 2 (c). An all-to-all communi- cation is applied to the output of Linear qkv with a shape of B \u00d7 3 \u00d7 S/s sp \u00d7 H. This enables each GPU to receive the complete sequence of Q, K, and V (each with a shape of B \u00d7 S \u00d7 H/s sp ) for a subset of attention heads. Subsequently, FlashAttention is employed for highly efficient MHA computation with a constraint that the number of attention heads D should not exceed s sp .",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Long-sequence Training",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 588,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 134
                            },
                            {
                                "start": 135,
                                "end": 287
                            },
                            {
                                "start": 288,
                                "end": 434
                            },
                            {
                                "start": 435,
                                "end": 590
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Processing long sequences is crucial for supporting important applications such as dealing with longer histories in chat applications. To this end, sequence parallelism (SP) has emerged as a technique aimed at alleviating activation memory footprints during the training of Transformers. In SP, the input tensor of each Transformer layer is divided along the sequence dimension, allowing for parallel computation across multiple GPUs. This segmentation, in conjunction with activation recomputation, results in a substantial reduction in activation memory requirements by a factor of s sp"
                    },
                    {
                        "section_title": "Long-sequence Training",
                        "pdf_hash": "",
                        "start": 1489,
                        "end": 2005,
                        "sentence_offsets": [
                            {
                                "start": 1489,
                                "end": 1599
                            },
                            {
                                "start": 1600,
                                "end": 1704
                            },
                            {
                                "start": 1705,
                                "end": 1846
                            },
                            {
                                "start": 1847,
                                "end": 2004
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "DeepSpeed Ulysses utilizes sequence parallelism on Linear, Norm, and Dropout layers, as shown in Figure 2 (c). An all-to-all communi- cation is applied to the output of Linear qkv with a shape of B \u00d7 3 \u00d7 S/s sp \u00d7 H. This enables each GPU to receive the complete sequence of Q, K, and V (each with a shape of B \u00d7 S \u00d7 H/s sp ) for a subset of attention heads. Subsequently, FlashAttention is employed for highly efficient MHA computation with a constraint that the number of attention heads D should not exceed s sp ."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[269922123 | Zeng et al. | 2024 | Citations: 4]",
                "snippets": "\u2022 SP was first introduced by Li et al. [97] and has been widely used in distributed LLM training frameworks such as Megatron (Korthikanti et al., 2022) and Deepspeed [99].In the context of LLM serving systems, new challenges emerge: (1) LLM serving is usually latency-sensitive and thus requires much smaller batch sizes than LLM training; (2) LLM serving has an autoregressive decode phase, where the sequence length is only one, but it requires large memory for KV cache storage; (3) LLM serving usually relies on large fused kernels for improving performance.While the feed-forward network (FFN) computations for each token in a sequence are linearly independent, the computations for attention are not.Consequently, substantial data exchange is involved when computing distributed attention using SP, thereby opening significant space for performance optimization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248693351 | Korthikanti et al. | 2022 | Citations: 275]": "Training large transformer models is one of the most important computational challenges of modern AI. In this paper, we show how to significantly accelerate training of large transformer models by reducing activation recomputation. Activation recomputation is commonly used to work around memory capacity constraints. Rather than storing activations for backpropagation, they are traditionally recomputed, which saves memory but adds redundant compute. In this work, we show most of this redundant compute is unnecessary because we can reduce memory consumption sufficiently without it. We present two novel yet very simple techniques: sequence parallelism and selective activation recomputation. In conjunction with tensor parallelism, these techniques almost eliminate the need to recompute activations. We evaluate our approach on language models up to one trillion parameters in scale and show that our method reduces activation memory by 5x, while reducing execution time overhead from activation recomputation by over 90%. For example, when training a 530B parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using recomputation. Our implementation will be available in both Megatron-LM and NeMo-Megatron."
                },
                "metadata": [
                    {
                        "section_title": "Distributed Acceleration",
                        "pdf_hash": "",
                        "start": 1442,
                        "end": 2288,
                        "sentence_offsets": [
                            {
                                "start": 1442,
                                "end": 1591
                            },
                            {
                                "start": 1591,
                                "end": 1982
                            },
                            {
                                "start": 1982,
                                "end": 2126
                            },
                            {
                                "start": 2126,
                                "end": 2288
                            }
                        ],
                        "ref_mentions": [
                            "248693351"
                        ],
                        "quote": "\u2022 SP was first introduced by Li et al. [97] and has been widely used in distributed LLM training frameworks such as Megatron (Korthikanti et al., 2022) and Deepspeed [99].In the context of LLM serving systems, new challenges emerge: (1) LLM serving is usually latency-sensitive and thus requires much smaller batch sizes than LLM training; (2) LLM serving has an autoregressive decode phase, where the sequence length is only one, but it requires large memory for KV cache storage; (3) LLM serving usually relies on large fused kernels for improving performance.While the feed-forward network (FFN) computations for each token in a sequence are linearly independent, the computations for attention are not.Consequently, substantial data exchange is involved when computing distributed attention using SP, thereby opening significant space for performance optimization."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[270062939 | Tsaris et al. | 2024 | Citations: 1]",
                "snippets": "Sequence parallelism, which distributes the self-attention computation in transformer models across the sequence dimension, has been an active area of research, with many developed methods employing various approaches.DeepSpeed-Ulysses [15]introduces a suite of system optimizations specifically designed for the efficient training of extreme long sequence transformer models.DeepSpeed-Ulysses partitions the input data along the sequence dimension, which allows the system to handle much longer sequences.For attention computation, it employs an all-to-all collective communication to ensure that each GPU receives a complete sequence, but only for a non-overlapping subset of the attention heads, allowing for parallel computation of attention across GPUs.LSS [16] divides a long sequence into segments distributed among GPUs, with each GPU computing a partial selfattention for its segment.It also introduces a fused communication strategy to minimize the communication overhead.Dynamic Sequence Parallelism (DSP) [22] enables efficient sequence parallelism for multi-dimensional transformers by dynamically switching the parallelism dimension according to the current computation stage.Ring attention [23], [24] is another variant that organizes tokens in a ring structure, allowing each token to attend to a fixed number of adjacent tokens in the sequence.This method offers a compromise between dense and sparse attention, balancing computational efficiency with the ability to capture local and semi-distant relationships.However, ring attention may still fall short in tasks where long-range dependencies are crucial.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "E. Sequence Parallel (SP)",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1625,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 218
                            },
                            {
                                "start": 218,
                                "end": 376
                            },
                            {
                                "start": 376,
                                "end": 506
                            },
                            {
                                "start": 506,
                                "end": 758
                            },
                            {
                                "start": 758,
                                "end": 893
                            },
                            {
                                "start": 893,
                                "end": 982
                            },
                            {
                                "start": 982,
                                "end": 1190
                            },
                            {
                                "start": 1190,
                                "end": 1361
                            },
                            {
                                "start": 1361,
                                "end": 1529
                            },
                            {
                                "start": 1529,
                                "end": 1625
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Sequence parallelism, which distributes the self-attention computation in transformer models across the sequence dimension, has been an active area of research, with many developed methods employing various approaches.DeepSpeed-Ulysses [15]introduces a suite of system optimizations specifically designed for the efficient training of extreme long sequence transformer models.DeepSpeed-Ulysses partitions the input data along the sequence dimension, which allows the system to handle much longer sequences.For attention computation, it employs an all-to-all collective communication to ensure that each GPU receives a complete sequence, but only for a non-overlapping subset of the attention heads, allowing for parallel computation of attention across GPUs.LSS [16] divides a long sequence into segments distributed among GPUs, with each GPU computing a partial selfattention for its segment.It also introduces a fused communication strategy to minimize the communication overhead.Dynamic Sequence Parallelism (DSP) [22] enables efficient sequence parallelism for multi-dimensional transformers by dynamically switching the parallelism dimension according to the current computation stage.Ring attention [23], [24] is another variant that organizes tokens in a ring structure, allowing each token to attend to a fixed number of adjacent tokens in the sequence.This method offers a compromise between dense and sparse attention, balancing computational efficiency with the ability to capture local and semi-distant relationships.However, ring attention may still fall short in tasks where long-range dependencies are crucial."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[272310078 | Yao et al. | 2024 | Citations: 2]",
                "snippets": "Megatron-SP (Korthikanti et al., 2022)) adopts a sequence parallelism technique which is tightly integrated with its tensor parallelism. In this approach, sequences are partitioned along the sequence dimension, and all-gather and reduce-scatter collectives are employed to aggregate the QKV (query, key, value) projections for attention computation. The communication complexity analysis indicates that, in contrast to our approach, the communication volume in Megatron-SP's sequence parallelism increases linearly with the sequence length regardless of the number of compute devices...Ring Attention (Liu et al., 2023) enhances Transformer's scalability by distributing long sequences across multiple devices. This innovative approach overlaps the communication of key-value pairs with the computation of blockwise attention, effectively increasing the feasible sequence length proportionally to the number of available devices...DeepSpeed Ulysses (Jacobs et al., 2023) tackles the challenges of sequence parallelism by partitioning input data along the sequence dimension and utilizing an efficient allto-all collective communication strategy for attention computations. Although this method maintains a constant communication volume regardless of the increase in sequence lengths and device counts, achieving significant speedups and scalability, it may still encounter practical hurdles in deployment related to large-scale clusters and the optimization of communication patterns across diverse computing environments.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248693351 | Korthikanti et al. | 2022 | Citations: 275]": "Training large transformer models is one of the most important computational challenges of modern AI. In this paper, we show how to significantly accelerate training of large transformer models by reducing activation recomputation. Activation recomputation is commonly used to work around memory capacity constraints. Rather than storing activations for backpropagation, they are traditionally recomputed, which saves memory but adds redundant compute. In this work, we show most of this redundant compute is unnecessary because we can reduce memory consumption sufficiently without it. We present two novel yet very simple techniques: sequence parallelism and selective activation recomputation. In conjunction with tensor parallelism, these techniques almost eliminate the need to recompute activations. We evaluate our approach on language models up to one trillion parameters in scale and show that our method reduces activation memory by 5x, while reducing execution time overhead from activation recomputation by over 90%. For example, when training a 530B parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using recomputation. Our implementation will be available in both Megatron-LM and NeMo-Megatron."
                },
                "metadata": [
                    {
                        "section_title": "Long context training",
                        "pdf_hash": "",
                        "start": 398,
                        "end": 980,
                        "sentence_offsets": [
                            {
                                "start": 398,
                                "end": 533
                            },
                            {
                                "start": 534,
                                "end": 746
                            },
                            {
                                "start": 747,
                                "end": 981
                            }
                        ],
                        "ref_mentions": [
                            "248693351"
                        ],
                        "quote": "Megatron-SP (Korthikanti et al., 2022)) adopts a sequence parallelism technique which is tightly integrated with its tensor parallelism. In this approach, sequences are partitioned along the sequence dimension, and all-gather and reduce-scatter collectives are employed to aggregate the QKV (query, key, value) projections for attention computation. The communication complexity analysis indicates that, in contrast to our approach, the communication volume in Megatron-SP's sequence parallelism increases linearly with the sequence length regardless of the number of compute devices"
                    },
                    {
                        "section_title": "Long context training",
                        "pdf_hash": "",
                        "start": 1429,
                        "end": 1772,
                        "sentence_offsets": [
                            {
                                "start": 1429,
                                "end": 1553
                            },
                            {
                                "start": 1554,
                                "end": 1772
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Ring Attention (Liu et al., 2023) enhances Transformer's scalability by distributing long sequences across multiple devices. This innovative approach overlaps the communication of key-value pairs with the computation of blockwise attention, effectively increasing the feasible sequence length proportionally to the number of available devices"
                    },
                    {
                        "section_title": "Long context training",
                        "pdf_hash": "",
                        "start": 2046,
                        "end": 2638,
                        "sentence_offsets": [
                            {
                                "start": 2046,
                                "end": 2287
                            },
                            {
                                "start": 2288,
                                "end": 2637
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "DeepSpeed Ulysses (Jacobs et al., 2023) tackles the challenges of sequence parallelism by partitioning input data along the sequence dimension and utilizing an efficient allto-all collective communication strategy for attention computations. Although this method maintains a constant communication volume regardless of the increase in sequence lengths and device counts, achieving significant speedups and scalability, it may still encounter practical hurdles in deployment related to large-scale clusters and the optimization of communication patterns across diverse computing environments."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[273707418 | Fang et al. | 2024 | Citations: 6]",
                "snippets": "In contrast, SP partitions the input image across multiple devices, utilizing methods such as Ulysses or Ring to communicate the Attention input and output tensors. SP demonstrates su-perior communication efficiency compared to TP, but each device stores the entire model parameters, which can be memory intensive.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "VAE",
                        "pdf_hash": "",
                        "start": 1098,
                        "end": 1412,
                        "sentence_offsets": [
                            {
                                "start": 1098,
                                "end": 1262
                            },
                            {
                                "start": 1263,
                                "end": 1412
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In contrast, SP partitions the input image across multiple devices, utilizing methods such as Ulysses or Ring to communicate the Attention input and output tensors. SP demonstrates su-perior communication efficiency compared to TP, but each device stores the entire model parameters, which can be memory intensive."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[273963185 | Fujii et al. | 2024 | Citations: 2]",
                "snippets": "Unlike Sequence Parallelism (Korthikanti et al., 2022), which only parallelizes the activations of Dropout and LayerNorm, Context Parallelism enables partitioning of the model's network inputs and all activations along the sequence dimension.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248693351 | Korthikanti et al. | 2022 | Citations: 275]": "Training large transformer models is one of the most important computational challenges of modern AI. In this paper, we show how to significantly accelerate training of large transformer models by reducing activation recomputation. Activation recomputation is commonly used to work around memory capacity constraints. Rather than storing activations for backpropagation, they are traditionally recomputed, which saves memory but adds redundant compute. In this work, we show most of this redundant compute is unnecessary because we can reduce memory consumption sufficiently without it. We present two novel yet very simple techniques: sequence parallelism and selective activation recomputation. In conjunction with tensor parallelism, these techniques almost eliminate the need to recompute activations. We evaluate our approach on language models up to one trillion parameters in scale and show that our method reduces activation memory by 5x, while reducing execution time overhead from activation recomputation by over 90%. For example, when training a 530B parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using recomputation. Our implementation will be available in both Megatron-LM and NeMo-Megatron."
                },
                "metadata": [
                    {
                        "section_title": "Parallelism",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 242,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 242
                            }
                        ],
                        "ref_mentions": [
                            "248693351"
                        ],
                        "quote": "Unlike Sequence Parallelism (Korthikanti et al., 2022), which only parallelizes the activations of Dropout and LayerNorm, Context Parallelism enables partitioning of the model's network inputs and all activations along the sequence dimension."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[276106883 | Tomczak et al. | 2025 | Citations: 0]",
                "snippets": "Our technique, that exploits sparsity, is orthogonal to these methods and has the potential to significantly scale the sequence length that can be achieved using these techniques for a fixed number of nodes.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "III. RELATED WORKS",
                        "pdf_hash": "",
                        "start": 864,
                        "end": 1071,
                        "sentence_offsets": [
                            {
                                "start": 864,
                                "end": 1071
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our technique, that exploits sparsity, is orthogonal to these methods and has the potential to significantly scale the sequence length that can be achieved using these techniques for a fixed number of nodes."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[276585307 | Bian et al. | 2025 | Citations: 0]",
                "snippets": "More importantly, it is fully compatible with all state-of-the-art attention optimizations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 1074,
                        "end": 1165,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "More importantly, it is fully compatible with all state-of-the-art attention optimizations."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[276885221 | Sun et al. | 2025 | Citations: 4]",
                "snippets": "The existing methods, LASP (Sun et al., 2025) and its improved version LASP-2 (Sun et al., 2025), are designed specifically to leverage the right-productfirst property of linear attention techniques for efficient sequence parallelism (SP). LASP employs a point-to-point ring-style communication pattern, facilitating the exchange of incremental memory states across devices. This communication pattern is particularly effective for managing dependencies while minimizing the data transferred between devices, enhancing the scalability of SP. LASP-2 further refines this approach by replacing the ring-style communication with an all-gather collective communication operation, streamlining the entire communication process. This modification not only simplifies the communication structure but also improves the parallelism of computation and communication.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[276259019 | Sun et al. | 2025 | Citations: 4]": "Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE."
                },
                "metadata": [
                    {
                        "section_title": "Sequence Parallelism on Linear-MoE",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 857,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 240
                            },
                            {
                                "start": 241,
                                "end": 375
                            },
                            {
                                "start": 376,
                                "end": 542
                            },
                            {
                                "start": 543,
                                "end": 723
                            },
                            {
                                "start": 724,
                                "end": 857
                            }
                        ],
                        "ref_mentions": [
                            "276259019",
                            "276259019"
                        ],
                        "quote": "The existing methods, LASP (Sun et al., 2025) and its improved version LASP-2 (Sun et al., 2025), are designed specifically to leverage the right-productfirst property of linear attention techniques for efficient sequence parallelism (SP). LASP employs a point-to-point ring-style communication pattern, facilitating the exchange of incremental memory states across devices. This communication pattern is particularly effective for managing dependencies while minimizing the data transferred between devices, enhancing the scalability of SP. LASP-2 further refines this approach by replacing the ring-style communication with an all-gather collective communication operation, streamlining the entire communication process. This modification not only simplifies the communication structure but also improves the parallelism of computation and communication."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[277621915 | Kottapalli et al. | 2025 | Citations: 0]",
                "snippets": "To address this, recent innovations such as sparse attention and linear Transformers have been proposed, reducing the complexity to O(n) [71], [72] in certain cases. These advancements extend the applicability of Transformers to resource-constrained environments.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Scalability and Parallelism",
                        "pdf_hash": "",
                        "start": 1340,
                        "end": 1603,
                        "sentence_offsets": [
                            {
                                "start": 1340,
                                "end": 1505
                            },
                            {
                                "start": 1506,
                                "end": 1603
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To address this, recent innovations such as sparse attention and linear Transformers have been proposed, reducing the complexity to O(n) [71], [72] in certain cases. These advancements extend the applicability of Transformers to resource-constrained environments."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[278714643 | Shi et al. | 2025 | Citations: 0]",
                "snippets": "Innermost TP-SP. Among 8 ranks within a node, we adopt Tensor Parallelism(TP) (Shoeybi et al., 2019) and Sequence Parallelism(SP) (Korthikanti et al., 2022) to partition the model states and activations. In each Transformer block, we employ a dedicated MLP (a Linear layer followed by a SiLU) to incorporate conditioning embedding for Adaptive Layer Normalization (AdaLN), aiming to enhance the model's capacity to capture conditional information. However, this design incurs substantial memory overhead: these dedicated Linear collectively increase the model's total parameter count by over 3B and introduce redundant model states exceeding 20GB per rank. To address this issue, we partition these Linear layers in a column parallel fashion with their computation and parameters parallelized across devices. Specifically, during the forward pass, all-gather reconstructs the complete Linear output, while reduce-scatter synchronizes gradients across the sequence-parallel group before the Linear in the backward pass. \n\nMid-Optional CP. While TP-SP supports training with moderately long contexts, Context Parallelism (CP) (Jacobs et al., 2023) is optionally enabled only for ultra-long sequences (exceeds 200k tokens). \n\nFor non-attention computations, each layer processes 1/CP of the input sequence. Before and after attention computations, all-to-all communication is applied to transpose the sequence, enabling each rank to process attention with full context length but only 1/CP of the attention heads. Since CP incurs lower communication overhead than TP-SP, we configure TP-SP as inner parallelism and CP as outer parallelism.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248693351 | Korthikanti et al. | 2022 | Citations: 275]": "Training large transformer models is one of the most important computational challenges of modern AI. In this paper, we show how to significantly accelerate training of large transformer models by reducing activation recomputation. Activation recomputation is commonly used to work around memory capacity constraints. Rather than storing activations for backpropagation, they are traditionally recomputed, which saves memory but adds redundant compute. In this work, we show most of this redundant compute is unnecessary because we can reduce memory consumption sufficiently without it. We present two novel yet very simple techniques: sequence parallelism and selective activation recomputation. In conjunction with tensor parallelism, these techniques almost eliminate the need to recompute activations. We evaluate our approach on language models up to one trillion parameters in scale and show that our method reduces activation memory by 5x, while reducing execution time overhead from activation recomputation by over 90%. For example, when training a 530B parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using recomputation. Our implementation will be available in both Megatron-LM and NeMo-Megatron."
                },
                "metadata": [
                    {
                        "section_title": "Parallel Strategy",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1636,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 16
                            },
                            {
                                "start": 17,
                                "end": 203
                            },
                            {
                                "start": 204,
                                "end": 447
                            },
                            {
                                "start": 448,
                                "end": 656
                            },
                            {
                                "start": 657,
                                "end": 808
                            },
                            {
                                "start": 809,
                                "end": 1018
                            },
                            {
                                "start": 1021,
                                "end": 1037
                            },
                            {
                                "start": 1038,
                                "end": 1220
                            },
                            {
                                "start": 1223,
                                "end": 1303
                            },
                            {
                                "start": 1304,
                                "end": 1510
                            },
                            {
                                "start": 1511,
                                "end": 1636
                            }
                        ],
                        "ref_mentions": [
                            "248693351"
                        ],
                        "quote": "Innermost TP-SP. Among 8 ranks within a node, we adopt Tensor Parallelism(TP) (Shoeybi et al., 2019) and Sequence Parallelism(SP) (Korthikanti et al., 2022) to partition the model states and activations. In each Transformer block, we employ a dedicated MLP (a Linear layer followed by a SiLU) to incorporate conditioning embedding for Adaptive Layer Normalization (AdaLN), aiming to enhance the model's capacity to capture conditional information. However, this design incurs substantial memory overhead: these dedicated Linear collectively increase the model's total parameter count by over 3B and introduce redundant model states exceeding 20GB per rank. To address this issue, we partition these Linear layers in a column parallel fashion with their computation and parameters parallelized across devices. Specifically, during the forward pass, all-gather reconstructs the complete Linear output, while reduce-scatter synchronizes gradients across the sequence-parallel group before the Linear in the backward pass. \n\nMid-Optional CP. While TP-SP supports training with moderately long contexts, Context Parallelism (CP) (Jacobs et al., 2023) is optionally enabled only for ultra-long sequences (exceeds 200k tokens). \n\nFor non-attention computations, each layer processes 1/CP of the input sequence. Before and after attention computations, all-to-all communication is applied to transpose the sequence, enabling each rank to process attention with full context length but only 1/CP of the attention heads. Since CP incurs lower communication overhead than TP-SP, we configure TP-SP as inner parallelism and CP as outer parallelism."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.024993,
        "cot": "For this query about sequence parallelism interaction with other efficiency methods for extending Transformer context length, I need to create a logical structure that clearly addresses both the interactions/integrations and the synergies/trade-offs among these approaches.\n\n1. \"Introduction\" - This should be a synthesis paragraph providing background on sequence parallelism and the general problem of extending Transformer context length. This will help users understand the basic concepts before diving into details.\n\n2. \"Sequence Parallelism Fundamentals\" - A synthesis section explaining what sequence parallelism is and how it works generally. This provides the foundation for understanding its interactions with other methods.\n\n3. \"Integration with Sparse Attention Methods\" - This should be a synthesis paragraph as it requires explaining how sequence parallelism combines with sparse attention techniques, which several quotes mention directly.\n\n4. \"Compatibility with Memory-Efficient Algorithms\" - Another synthesis section focused on how sequence parallelism works alongside memory-efficient attention implementations like FlashAttention.\n\n5. \"Notable Implementation Frameworks\" - A list format is appropriate here as we're enumerating specific frameworks/systems that implement sequence parallelism with various optimizations.\n\n6. \"Technical Challenges and Solutions\" - This should be synthesis as it requires explaining the technical issues that arise when combining these approaches and how they're addressed.\n\n7. \"Synergies Among Approaches\" - A synthesis section highlighting the positive complementary aspects when combining sequence parallelism with other methods.\n\n8. \"Trade-offs and Limitations\" - A synthesis section addressing the compromises and constraints that emerge when integrating sequence parallelism with other approaches.\n\nThis structure logically addresses the query while ensuring we can incorporate all the provided quotes.",
        "plan": {
            "Introduction (synthesis)": [],
            "Sequence Parallelism Fundamentals (synthesis)": [
                0,
                1,
                7,
                8,
                11,
                12
            ],
            "Integration with Sparse Attention Methods (synthesis)": [
                1,
                3,
                5,
                9,
                13,
                15,
                16
            ],
            "Compatibility with Memory-Efficient Algorithms (synthesis)": [
                3,
                5,
                6,
                14
            ],
            "Notable Implementation Frameworks (list)": [
                3,
                6,
                9,
                10,
                17
            ],
            "Technical Challenges and Solutions (synthesis)": [
                4,
                5,
                6,
                8,
                9
            ],
            "Synergies Among Approaches (synthesis)": [
                2,
                3,
                5,
                13,
                14,
                15
            ],
            "Trade-offs and Limitations (synthesis)": [
                4,
                7,
                10,
                11
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction",
                "tldr": "Modern Transformer models face significant computational challenges when scaling to longer context lengths, requiring innovative efficiency techniques. Sequence parallelism works alongside other methods like sparse attention and memory-efficient algorithms to overcome these limitations, each offering distinct benefits and trade-offs. (LLM Memory)",
                "text": "\nThe extension of context length in Transformer models has become a critical frontier in natural language processing, enabling tasks that require understanding and generating longer sequences of text. However, this extension comes with substantial computational challenges, particularly in terms of memory usage and processing time. The standard Transformer architecture, with its quadratic complexity in self-attention mechanisms relative to sequence length, quickly becomes prohibitively expensive as context length increases. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nSequence parallelism has emerged as one of several complementary approaches to address these challenges. It involves distributing the sequence dimension across multiple processing units, allowing parallel computation on different segments of the input sequence. This technique works alongside other efficiency-focused methods, creating a rich ecosystem of approaches that can be combined to optimize Transformer performance. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nUnderstanding how sequence parallelism integrates with approaches like sparse attention patterns (which reduce the quadratic complexity by attending to only a subset of tokens) and memory-efficient algorithms (which optimize how activations and gradients are stored and computed) is crucial for practitioners. These methods can be deployed individually or in combination, with each configuration offering different advantages in terms of throughput, memory usage, training stability, and implementation complexity. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Sequence Parallelism Fundamentals",
                "tldr": "Sequence parallelism addresses the memory constraints of long-sequence Transformer models by splitting input sequences across multiple devices. This technique distributes activation memory along the sequence dimension while maintaining full model parameters on each device, requiring specialized communication patterns like Ring Self-Attention to complete attention calculations. (7 sources)",
                "text": "\nSequence parallelism (SP) emerged as a distributed computing approach to overcome memory limitations when processing long sequences in Transformer models. Unlike algorithmic solutions that modify the attention mechanism, SP tackles the problem from a systems perspective by distributing computations across multiple devices <Paper corpusId=\"246017095\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>. The core idea is conceptually straightforward: the input sequence is partitioned along the sequence dimension, with each device processing only a sub-sequence while maintaining a full copy of the model parameters <Paper corpusId=\"240070340\" paperTitle=\"(Li et al._1, 2021)\" isShortName></Paper>.\n\nThe primary innovation of sequence parallelism lies in how it handles the interdependent nature of self-attention computations. Since attention requires interactions between all tokens in a sequence, SP implements specialized communication mechanisms like the Ring Self-Attention (RSA) module, where partial query, key, and value embeddings are exchanged among devices to complete the attention calculation <Paper corpusId=\"240070340\" paperTitle=\"(Li et al._1, 2021)\" isShortName></Paper>. This approach enables the processing of significantly longer sequences by dividing the activation memory requirements by the number of parallel devices (s_sp) <Paper corpusId=\"267027729\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>.\n\nIn practice, sequence parallelism can be applied to different components of the Transformer architecture. DeepSpeed Ulysses, for example, applies SP to Linear, Normalization, and Dropout layers, with all-to-all communication for query, key, and value tensors to enable efficient multi-head attention computation <Paper corpusId=\"267027729\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. When combined with tensor parallelism and selective activation recomputation, SP can reduce activation memory by up to 5x while eliminating over 90% of the computation overhead traditionally associated with activation recomputation <Paper corpusId=\"269922123\" paperTitle=\"(Zeng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"248693351\" paperTitle=\"(Korthikanti et al., 2022)\" isShortName></Paper>.\n\nIt's important to note that different implementations of sequence parallelism vary in scope. The original implementation in Megatron-LM focused primarily on parallelizing the activations of Dropout and LayerNorm layers <Paper corpusId=\"273963185\" paperTitle=\"(Fujii et al., 2024)\" isShortName></Paper> <Paper corpusId=\"248693351\" paperTitle=\"(Korthikanti et al., 2022)\" isShortName></Paper>, while more recent approaches like Context Parallelism extend this to partition all activations along the sequence dimension <Paper corpusId=\"273963185\" paperTitle=\"(Fujii et al., 2024)\" isShortName></Paper>. Compared to tensor parallelism (TP), sequence parallelism offers superior communication efficiency but requires each device to store the entire model parameters, which can be memory intensive for very large models <Paper corpusId=\"273707418\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>.\n\nWhile originally developed for training, sequence parallelism also presents unique challenges in inference scenarios, particularly for LLM serving systems. These include the need for lower latency with smaller batch sizes, managing the KV cache during autoregressive decoding, and optimizing performance with large fused kernels <Paper corpusId=\"269922123\" paperTitle=\"(Zeng et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Li et al., 2021)",
                        "snippets": [
                            "To model long sequences, existing works mainly focus on sparse attention (e.g., [Zaheer et al., 2020]) with linear instead of quadratic space complexity. In this paper, we aim to solve the long sequence modeling problem from the distributed system perspective. Compared with sparse attention, we devote ourselves to designing and implementing a system instead of a deep learning algorithm to train attentionbased models with longer sequences",
                            "Our RSA focuses on full self-attention in this paper. According to Table 2, when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences, because three terms of memory usage include L/N."
                        ],
                        "paper": {
                            "corpus_id": 246017095,
                            "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
                            "authors": [
                                {
                                    "authorId": "2153703322",
                                    "name": "Shenggui Li"
                                },
                                {
                                    "authorId": "2144332771",
                                    "name": "Fuzhao Xue"
                                },
                                {
                                    "authorId": "2180329661",
                                    "name": "Yongbin Li"
                                },
                                {
                                    "authorId": "144259229",
                                    "name": "Yang You"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 102
                        },
                        "score": 0.93017578125
                    },
                    {
                        "id": "(Li et al._1, 2021)",
                        "snippets": [
                            "Sequence parallelism [21] is proposed to enable long-sequence modeling by breaking the memory wall brought by the large sequence dimension. In sequence parallelism, the model is replicated across devices just like data parallelism. The input data is split along the sequence dimension and each device only keeps a sub-sequence. The self-attention module is replaced with the Ring Self-Attention module such that the partial query, key, and value embeddings are exchanged among devices to complete the self-attention calculation."
                        ],
                        "paper": {
                            "corpus_id": 240070340,
                            "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training",
                            "authors": [
                                {
                                    "authorId": "2153703322",
                                    "name": "Shenggui Li"
                                },
                                {
                                    "authorId": "1996240097",
                                    "name": "Zhengda Bian"
                                },
                                {
                                    "authorId": "2110176595",
                                    "name": "Hongxin Liu"
                                },
                                {
                                    "authorId": "49292014",
                                    "name": "Boxiang Wang"
                                },
                                {
                                    "authorId": "2146286212",
                                    "name": "Haichen Huang"
                                },
                                {
                                    "authorId": "2180329661",
                                    "name": "Yongbin Li"
                                },
                                {
                                    "authorId": "2118397214",
                                    "name": "Chuan-Qing Wang"
                                },
                                {
                                    "authorId": "2211340476",
                                    "name": "Fan Cui"
                                },
                                {
                                    "authorId": "144259229",
                                    "name": "Yang You"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Parallel Processing",
                            "n_citations": 148
                        },
                        "score": 0.80712890625
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "Processing long sequences is crucial for supporting important applications such as dealing with longer histories in chat applications. To this end, sequence parallelism (SP) has emerged as a technique aimed at alleviating activation memory footprints during the training of Transformers. In SP, the input tensor of each Transformer layer is divided along the sequence dimension, allowing for parallel computation across multiple GPUs. This segmentation, in conjunction with activation recomputation, results in a substantial reduction in activation memory requirements by a factor of s sp",
                            "DeepSpeed Ulysses utilizes sequence parallelism on Linear, Norm, and Dropout layers, as shown in Figure 2 (c). An all-to-all communi- cation is applied to the output of Linear qkv with a shape of B \u00d7 3 \u00d7 S/s sp \u00d7 H. This enables each GPU to receive the complete sequence of Q, K, and V (each with a shape of B \u00d7 S \u00d7 H/s sp ) for a subset of attention heads. Subsequently, FlashAttention is employed for highly efficient MHA computation with a constraint that the number of attention heads D should not exceed s sp ."
                        ],
                        "paper": {
                            "corpus_id": 267027729,
                            "title": "InternEvo: Efficient Long-sequence Large Language Model Training via Hybrid Parallelism and Redundant Sharding",
                            "authors": [
                                {
                                    "authorId": "2189935212",
                                    "name": "Qiaoling Chen"
                                },
                                {
                                    "authorId": "1393328013",
                                    "name": "Diandian Gu"
                                },
                                {
                                    "authorId": "2263696698",
                                    "name": "Guoteng Wang"
                                },
                                {
                                    "authorId": "2293554318",
                                    "name": "Xun Chen"
                                },
                                {
                                    "authorId": "2279899091",
                                    "name": "Yingtong Xiong"
                                },
                                {
                                    "authorId": "2279784429",
                                    "name": "Ting Huang"
                                },
                                {
                                    "authorId": "2150570711",
                                    "name": "Qi Hu"
                                },
                                {
                                    "authorId": "2279869979",
                                    "name": "Xin Jin"
                                },
                                {
                                    "authorId": "2114783855",
                                    "name": "Yonggang Wen"
                                },
                                {
                                    "authorId": "2146333441",
                                    "name": "Tianwei Zhang"
                                },
                                {
                                    "authorId": "2075416290",
                                    "name": "Peng Sun"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.68310546875
                    },
                    {
                        "id": "(Zeng et al., 2024)",
                        "snippets": [
                            "\u2022 SP was first introduced by Li et al. [97] and has been widely used in distributed LLM training frameworks such as Megatron (Korthikanti et al., 2022) and Deepspeed [99].In the context of LLM serving systems, new challenges emerge: (1) LLM serving is usually latency-sensitive and thus requires much smaller batch sizes than LLM training; (2) LLM serving has an autoregressive decode phase, where the sequence length is only one, but it requires large memory for KV cache storage; (3) LLM serving usually relies on large fused kernels for improving performance.While the feed-forward network (FFN) computations for each token in a sequence are linearly independent, the computations for attention are not.Consequently, substantial data exchange is involved when computing distributed attention using SP, thereby opening significant space for performance optimization."
                        ],
                        "paper": {
                            "corpus_id": 269922123,
                            "title": "The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving",
                            "authors": [
                                {
                                    "authorId": "2302327872",
                                    "name": "Pai Zeng"
                                },
                                {
                                    "authorId": "2302322712",
                                    "name": "Zhenyu Ning"
                                },
                                {
                                    "authorId": "2302350607",
                                    "name": "Jieru Zhao"
                                },
                                {
                                    "authorId": "1492129615",
                                    "name": "Weihao Cui"
                                },
                                {
                                    "authorId": "2239403838",
                                    "name": "Mengwei Xu"
                                },
                                {
                                    "authorId": "2286749413",
                                    "name": "Liwei Guo"
                                },
                                {
                                    "authorId": "2280208638",
                                    "name": "XuSheng Chen"
                                },
                                {
                                    "authorId": "2280137293",
                                    "name": "Yizhou Shan"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.6708984375
                    },
                    {
                        "id": "(Korthikanti et al., 2022)",
                        "snippets": [
                            "Training large transformer models is one of the most important computational challenges of modern AI. In this paper, we show how to significantly accelerate training of large transformer models by reducing activation recomputation. Activation recomputation is commonly used to work around memory capacity constraints. Rather than storing activations for backpropagation, they are traditionally recomputed, which saves memory but adds redundant compute. In this work, we show most of this redundant compute is unnecessary because we can reduce memory consumption sufficiently without it. We present two novel yet very simple techniques: sequence parallelism and selective activation recomputation. In conjunction with tensor parallelism, these techniques almost eliminate the need to recompute activations. We evaluate our approach on language models up to one trillion parameters in scale and show that our method reduces activation memory by 5x, while reducing execution time overhead from activation recomputation by over 90%. For example, when training a 530B parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using recomputation. Our implementation will be available in both Megatron-LM and NeMo-Megatron."
                        ],
                        "paper": {
                            "corpus_id": 248693351,
                            "title": "Reducing Activation Recomputation in Large Transformer Models",
                            "authors": [
                                {
                                    "authorId": "3111334",
                                    "name": "V. Korthikanti"
                                },
                                {
                                    "authorId": "48991386",
                                    "name": "J. Casper"
                                },
                                {
                                    "authorId": "3448779",
                                    "name": "Sangkug Lym"
                                },
                                {
                                    "authorId": "20957879",
                                    "name": "Lawrence C. McAfee"
                                },
                                {
                                    "authorId": "47304697",
                                    "name": "M. Andersch"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "2301680",
                                    "name": "Bryan Catanzaro"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Machine Learning and Systems",
                            "n_citations": 275
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fujii et al., 2024)",
                        "snippets": [
                            "Unlike Sequence Parallelism (Korthikanti et al., 2022), which only parallelizes the activations of Dropout and LayerNorm, Context Parallelism enables partitioning of the model's network inputs and all activations along the sequence dimension."
                        ],
                        "paper": {
                            "corpus_id": 273963185,
                            "title": "Accelerating Large Language Model Training with 4D Parallelism and Memory Consumption Estimator",
                            "authors": [
                                {
                                    "authorId": "2298889885",
                                    "name": "Kazuki Fujii"
                                },
                                {
                                    "authorId": "2330230064",
                                    "name": "Kohei Watanabe"
                                },
                                {
                                    "authorId": "2294362068",
                                    "name": "Rio Yokota"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.5078125
                    },
                    {
                        "id": "(Fang et al., 2024)",
                        "snippets": [
                            "In contrast, SP partitions the input image across multiple devices, utilizing methods such as Ulysses or Ring to communicate the Attention input and output tensors. SP demonstrates su-perior communication efficiency compared to TP, but each device stores the entire model parameters, which can be memory intensive."
                        ],
                        "paper": {
                            "corpus_id": 273707418,
                            "title": "PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference",
                            "authors": [
                                {
                                    "authorId": "2303233521",
                                    "name": "Jiarui Fang"
                                },
                                {
                                    "authorId": "2329908310",
                                    "name": "Jinzhe Pan"
                                },
                                {
                                    "authorId": "2302815939",
                                    "name": "Jiannan Wang"
                                },
                                {
                                    "authorId": "2303334242",
                                    "name": "Aoyu Li"
                                },
                                {
                                    "authorId": "2328615626",
                                    "name": "Xibo Sun"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 6
                        },
                        "score": 0.64892578125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Integration with Sparse Attention Methods",
                "tldr": "Sequence parallelism can be combined with sparse attention mechanisms to further extend context length capabilities, with implementations like DeepSpeed Ulysses demonstrating compatibility with both dense and sparse attention patterns. This integration offers multiplicative benefits in handling extremely long sequences, though optimizing sparse attention implementations within distributed frameworks remains an active research challenge. (8 sources)",
                "text": "\nSequence parallelism and sparse attention represent two complementary approaches to address the computational challenges of long-context Transformer models. While sequence parallelism tackles the problem from a distributed systems perspective by partitioning sequences across devices, sparse attention methods approach it algorithmically by reducing the quadratic complexity of attention calculations to linear complexity <Paper corpusId=\"246017095\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper> <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>.\n\nThe integration of these methods creates powerful synergies. As noted by Li et al., \"when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences\" because the memory requirements are reduced by both the sparse pattern and the sequence partitioning factor <Paper corpusId=\"246017095\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>. DeepSpeed Ulysses exemplifies this integration by supporting both dense and sparse attention patterns while maintaining compatibility with efficient attention implementations like FlashAttention v2 <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>.\n\nPerformance comparisons demonstrate the benefits of this integration. DeepSpeed sequence parallelism with sparse attention can achieve more than 2x throughput compared to Megatron-LM implementations and scale to 4x longer sequence lengths when leveraging ZeRO-3 memory optimizations <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>. However, researchers have identified that current implementations still face bottlenecks: \"DeepSpeed throughput is bottlenecked by the local sparse attention implementation, and as a result DeepSpeed throughput decreases as the sequence length increases\" <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>.\n\nRecent research has focused on specialized sequence parallelism approaches for different attention patterns. For linear attention models, which naturally offer advantages like linear-time training and constant-memory inference, methods such as LASP and LASP-2 optimize communication patterns specifically for the right-product-first property of linear attention <Paper corpusId=\"276885221\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper> <Paper corpusId=\"276259019\" paperTitle=\"(Sun et al._1, 2025)\" isShortName></Paper>. LASP-2 replaces ring-style communication with an all-gather collective operation, improving both communication and computation parallelism for linear attention implementations <Paper corpusId=\"276885221\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>.\n\nThe landscape of sequence parallelism implementations continues to evolve with various approaches for distributing self-attention computation. DeepSpeed-Ulysses partitions input data along the sequence dimension and uses all-to-all collective communication for attention computation, while other methods like LSS (Long Sequence Segmentation) divide sequences into segments distributed among GPUs with fused communication strategies <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>. Ring attention represents yet another variant that organizes tokens in a ring structure, balancing computational efficiency with the ability to capture local and semi-distant relationships <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>.\n\nImportantly, techniques that exploit sparsity are orthogonal to sequence parallelism methods, offering the potential to significantly scale sequence lengths with a fixed number of nodes <Paper corpusId=\"276106883\" paperTitle=\"(Tomczak et al., 2025)\" isShortName></Paper>. This complementary nature allows researchers to combine sparse attention patterns, which reduce complexity to O(n) in certain cases, with distributed computation strategies to extend the applicability of Transformers even in resource-constrained environments <Paper corpusId=\"277621915\" paperTitle=\"(Kottapalli et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Li et al., 2021)",
                        "snippets": [
                            "To model long sequences, existing works mainly focus on sparse attention (e.g., [Zaheer et al., 2020]) with linear instead of quadratic space complexity. In this paper, we aim to solve the long sequence modeling problem from the distributed system perspective. Compared with sparse attention, we devote ourselves to designing and implementing a system instead of a deep learning algorithm to train attentionbased models with longer sequences",
                            "Our RSA focuses on full self-attention in this paper. According to Table 2, when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences, because three terms of memory usage include L/N."
                        ],
                        "paper": {
                            "corpus_id": 246017095,
                            "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
                            "authors": [
                                {
                                    "authorId": "2153703322",
                                    "name": "Shenggui Li"
                                },
                                {
                                    "authorId": "2144332771",
                                    "name": "Fuzhao Xue"
                                },
                                {
                                    "authorId": "2180329661",
                                    "name": "Yongbin Li"
                                },
                                {
                                    "authorId": "144259229",
                                    "name": "Yang You"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 102
                        },
                        "score": 0.93017578125
                    },
                    {
                        "id": "(Zaheer et al., 2020)",
                        "snippets": [
                            "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."
                        ],
                        "paper": {
                            "corpus_id": 220831004,
                            "title": "Big Bird: Transformers for Longer Sequences",
                            "authors": [
                                {
                                    "authorId": "1771307",
                                    "name": "M. Zaheer"
                                },
                                {
                                    "authorId": "1947314",
                                    "name": "Guru Guruganesh"
                                },
                                {
                                    "authorId": "89890133",
                                    "name": "Kumar Avinava Dubey"
                                },
                                {
                                    "authorId": "1643737606",
                                    "name": "J. Ainslie"
                                },
                                {
                                    "authorId": "114577307",
                                    "name": "Chris Alberti"
                                },
                                {
                                    "authorId": "1722671",
                                    "name": "Santiago Onta\u00f1\u00f3n"
                                },
                                {
                                    "authorId": "38552691",
                                    "name": "Philip Pham"
                                },
                                {
                                    "authorId": "101210026",
                                    "name": "Anirudh Ravula"
                                },
                                {
                                    "authorId": "145196279",
                                    "name": "Qifan Wang"
                                },
                                {
                                    "authorId": "113906155",
                                    "name": "Li Yang"
                                },
                                {
                                    "authorId": "143629707",
                                    "name": "Amr Ahmed"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2103
                        },
                        "score": 0.423095703125
                    },
                    {
                        "id": "(Jacobs et al., 2023)",
                        "snippets": [
                            "Fully general and implementation agnostic attention: DeepSpeed sequence parallelism (Ulysses) supports dense as well as sparse attention, and it works with efficient attention implementations such as FlashAttention v2 [Dao, 2023]",
                            "We observe similar trends with sparse attention as dense attention experiments. We observe more than 2x throughput performance of DeepSpeed sequence parallelism compared to Megatron-LM. For memory saving, DeepSpeed sequence parallelism leveraging ZeRO-3 scales to 4x longer sequence lengths than Megatron-LM",
                            "DeepSpeed sequence parallelism outperforms Megatron-LM for sequence length that can be run with both. In fact, the current DeepSpeed throughput is bottlenecked by the local sparse attention implementation, and as a result DeepSpeed throughput decreases as the sequence length increases. We expect this gap in performance between DeepSpeed and Megatron-LM to increase further for larger sequence lengths as we improve the performance of the local sparse attention implementation in future."
                        ],
                        "paper": {
                            "corpus_id": 262826014,
                            "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
                            "authors": [
                                {
                                    "authorId": "1683824",
                                    "name": "S. A. Jacobs"
                                },
                                {
                                    "authorId": "2118817779",
                                    "name": "Masahiro Tanaka"
                                },
                                {
                                    "authorId": "2248958850",
                                    "name": "Chengming Zhang"
                                },
                                {
                                    "authorId": "67016465",
                                    "name": "Minjia Zhang"
                                },
                                {
                                    "authorId": "2145202945",
                                    "name": "L. Song"
                                },
                                {
                                    "authorId": "32817044",
                                    "name": "Samyam Rajbhandari"
                                },
                                {
                                    "authorId": "2145020341",
                                    "name": "Yuxiong He"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 120
                        },
                        "score": 0.87060546875
                    },
                    {
                        "id": "(Sun et al., 2025)",
                        "snippets": [
                            "The existing methods, LASP (Sun et al., 2025) and its improved version LASP-2 (Sun et al., 2025), are designed specifically to leverage the right-productfirst property of linear attention techniques for efficient sequence parallelism (SP). LASP employs a point-to-point ring-style communication pattern, facilitating the exchange of incremental memory states across devices. This communication pattern is particularly effective for managing dependencies while minimizing the data transferred between devices, enhancing the scalability of SP. LASP-2 further refines this approach by replacing the ring-style communication with an all-gather collective communication operation, streamlining the entire communication process. This modification not only simplifies the communication structure but also improves the parallelism of computation and communication."
                        ],
                        "paper": {
                            "corpus_id": 276885221,
                            "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
                            "authors": [
                                {
                                    "authorId": "2346291295",
                                    "name": "Weigao Sun"
                                },
                                {
                                    "authorId": "2344833625",
                                    "name": "Disen Lan"
                                },
                                {
                                    "authorId": "2349575238",
                                    "name": "Tong Zhu"
                                },
                                {
                                    "authorId": "2265753258",
                                    "name": "Xiaoye Qu"
                                },
                                {
                                    "authorId": "2344895705",
                                    "name": "Yu Cheng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.75830078125
                    },
                    {
                        "id": "(Sun et al._1, 2025)",
                        "snippets": [
                            "Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE."
                        ],
                        "paper": {
                            "corpus_id": 276259019,
                            "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid",
                            "authors": [
                                {
                                    "authorId": "2225238340",
                                    "name": "Weigao Sun"
                                },
                                {
                                    "authorId": "2344833625",
                                    "name": "Disen Lan"
                                },
                                {
                                    "authorId": "2266275708",
                                    "name": "Yiran Zhong"
                                },
                                {
                                    "authorId": "2265753258",
                                    "name": "Xiaoye Qu"
                                },
                                {
                                    "authorId": "2344895705",
                                    "name": "Yu Cheng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.767578125
                    },
                    {
                        "id": "(Tsaris et al., 2024)",
                        "snippets": [
                            "Sequence parallelism, which distributes the self-attention computation in transformer models across the sequence dimension, has been an active area of research, with many developed methods employing various approaches.DeepSpeed-Ulysses [15]introduces a suite of system optimizations specifically designed for the efficient training of extreme long sequence transformer models.DeepSpeed-Ulysses partitions the input data along the sequence dimension, which allows the system to handle much longer sequences.For attention computation, it employs an all-to-all collective communication to ensure that each GPU receives a complete sequence, but only for a non-overlapping subset of the attention heads, allowing for parallel computation of attention across GPUs.LSS [16] divides a long sequence into segments distributed among GPUs, with each GPU computing a partial selfattention for its segment.It also introduces a fused communication strategy to minimize the communication overhead.Dynamic Sequence Parallelism (DSP) [22] enables efficient sequence parallelism for multi-dimensional transformers by dynamically switching the parallelism dimension according to the current computation stage.Ring attention [23], [24] is another variant that organizes tokens in a ring structure, allowing each token to attend to a fixed number of adjacent tokens in the sequence.This method offers a compromise between dense and sparse attention, balancing computational efficiency with the ability to capture local and semi-distant relationships.However, ring attention may still fall short in tasks where long-range dependencies are crucial."
                        ],
                        "paper": {
                            "corpus_id": 270062939,
                            "title": "Sequence Length Scaling in Vision Transformers for Scientific Images on Frontier",
                            "authors": [
                                {
                                    "authorId": "3395528",
                                    "name": "A. Tsaris"
                                },
                                {
                                    "authorId": "2303676556",
                                    "name": "Chengming Zhang"
                                },
                                {
                                    "authorId": "2265563075",
                                    "name": "Xiao Wang"
                                },
                                {
                                    "authorId": "2244593486",
                                    "name": "Junqi Yin"
                                },
                                {
                                    "authorId": "2257073765",
                                    "name": "Siyan Liu"
                                },
                                {
                                    "authorId": "9259766",
                                    "name": "M. Ashfaq"
                                },
                                {
                                    "authorId": "2180823649",
                                    "name": "Ming Fan"
                                },
                                {
                                    "authorId": "2298006038",
                                    "name": "Jong-Youl Choi"
                                },
                                {
                                    "authorId": "1700692",
                                    "name": "M. Wahib"
                                },
                                {
                                    "authorId": "2258305751",
                                    "name": "Dan Lu"
                                },
                                {
                                    "authorId": "2138151793",
                                    "name": "Prasanna Balaprakash"
                                },
                                {
                                    "authorId": "2244140922",
                                    "name": "Feiyi Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.8359375
                    },
                    {
                        "id": "(Tomczak et al., 2025)",
                        "snippets": [
                            "Our technique, that exploits sparsity, is orthogonal to these methods and has the potential to significantly scale the sequence length that can be achieved using these techniques for a fixed number of nodes."
                        ],
                        "paper": {
                            "corpus_id": 276106883,
                            "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
                            "authors": [
                                {
                                    "authorId": "2276535724",
                                    "name": "Nathaniel Tomczak"
                                },
                                {
                                    "authorId": "2873546",
                                    "name": "S. Kuppannagari"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.81494140625
                    },
                    {
                        "id": "(Kottapalli et al., 2025)",
                        "snippets": [
                            "To address this, recent innovations such as sparse attention and linear Transformers have been proposed, reducing the complexity to O(n) [71], [72] in certain cases. These advancements extend the applicability of Transformers to resource-constrained environments."
                        ],
                        "paper": {
                            "corpus_id": 277621915,
                            "title": "Foundation Models for Time Series: A Survey",
                            "authors": [
                                {
                                    "authorId": "2354177739",
                                    "name": "Siva Rama Krishna Kottapalli"
                                },
                                {
                                    "authorId": "2354177337",
                                    "name": "Karthik Hubli"
                                },
                                {
                                    "authorId": "2354178763",
                                    "name": "Sandeep Chandrashekhara"
                                },
                                {
                                    "authorId": "2354142186",
                                    "name": "Garima Jain"
                                },
                                {
                                    "authorId": "2354177341",
                                    "name": "Sunayana Hubli"
                                },
                                {
                                    "authorId": "2354179172",
                                    "name": "Gayathri Botla"
                                },
                                {
                                    "authorId": "2354179043",
                                    "name": "Ramesh Doddaiah"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.552734375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Compatibility with Memory-Efficient Algorithms",
                "tldr": "Sequence parallelism can be effectively integrated with memory-efficient algorithms like FlashAttention to create multiplicative benefits for processing longer sequences. This compatibility requires specialized implementations that optimize communication patterns while preserving the memory benefits of both approaches. (6 sources)",
                "text": "\nMemory-efficient attention mechanisms have emerged as critical components for scaling Transformer models to longer sequences. These algorithms, exemplified by FlashAttention, reduce peak memory usage by avoiding materialization of large intermediate states like the attention softmax matrix <Paper corpusId=\"263671659\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>. The integration of these memory-efficient approaches with sequence parallelism presents both significant opportunities and technical challenges.\n\nDeepSpeed-Ulysses has demonstrated compatibility with state-of-the-art memory-efficient attention implementations such as FlashAttention v2, achieving both distributed processing and algorithmic efficiency simultaneously <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>. This integration enables multiplicative memory savings: sequence parallelism reduces memory requirements by distributing across devices, while memory-efficient attention algorithms minimize the memory footprint within each device. When combined with ZeRO-3 optimizations, this approach enables scaling to sequence lengths 4x longer than alternative implementations like Megatron-LM <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>.\n\nHowever, not all sequence parallelism implementations properly integrate with memory-efficient attention algorithms. As Liu et al. note, many sequence parallelism approaches \"ignore the compatibility with the existing efficient self-attention mechanisms such as FlashAttention\" <Paper corpusId=\"263828829\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This limitation has motivated the development of specialized implementations like DISTFLASHATTN, which optimizes for both memory-efficient attention and causal language modeling while maintaining distributed processing capabilities <Paper corpusId=\"263671659\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nThe communication patterns employed in sequence parallelism significantly impact compatibility with memory-efficient algorithms. DeepSpeed-Ulysses uses an all-to-all collective communication approach for attention computation when segmenting sequences, which helps avoid communication overhead that would otherwise increase with sequence length <Paper corpusId=\"263828829\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. In contrast, Ring Attention employs blockwise self-attention to split long sequences across devices and overlaps communication of key-value blocks to improve efficiency <Paper corpusId=\"263828829\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"265551519\" paperTitle=\"(Lv et al., 2023)\" isShortName></Paper>.\n\nRecent innovations continue to enhance the compatibility between sequence parallelism and memory-efficient algorithms. For example, Bian et al. have developed sequence parallelism implementations that are \"fully compatible with all state-of-the-art attention optimizations\" <Paper corpusId=\"276585307\" paperTitle=\"(Bian et al., 2025)\" isShortName></Paper>. This suggests a trend toward frameworks that seamlessly integrate these complementary approaches, maximizing both memory efficiency and computational parallelism.\n\nThe compatibility between sequence parallelism and memory-efficient algorithms represents a critical advantage in scaling Transformer models to extreme sequence lengths. By combining the distributed processing capabilities of sequence parallelism with the algorithmic efficiencies of approaches like FlashAttention, researchers can extend context lengths beyond what would be possible with either approach in isolation.",
                "citations": [
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "Memory-efficient attention. (Dao et al., 2022) and Lefaudeux et al. (2022) propose to use an online normalizer (Milakov & Gimelshein, 2018) to compute the attention in a blockwise and memory-efficient way. It reduces peak memory usage by not materializing large intermediate states, e.g. the attention softmax matrix. In addition, research on sparse attention computes only a sparse subset of the attention score, which also reduces the memory footprints yet may lead to inferior performance (Beltagy et al., 2020;Sun et al., 2022;(Zaheer et al., 2020). In this work, we limit our scope to exact attention. \n\nSequence parallelism and ring attention Ring Self-Attention (Li et al., 2021) is among the first to parallelize Transformers in the sequence dimension. However, its distributed attention design is not optimized for causal language modeling and incompatible with memory-efficient attention, which are crucial for long-context LLM training. Ring Attention (Liu et al., 2023) proposes to compute distributed attention in a memory-efficient blockwise pattern. However, it is also not optimized for causal language modeling, leading to 2\u00d7 extra computation. DISTFLASHATTN optimizes for both memory-efficient attention and causal language modeling."
                        ],
                        "paper": {
                            "corpus_id": 263671659,
                            "title": "DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training",
                            "authors": [
                                {
                                    "authorId": "2117961435",
                                    "name": "Dacheng Li"
                                },
                                {
                                    "authorId": "2254264970",
                                    "name": "Rulin Shao"
                                },
                                {
                                    "authorId": "2254221083",
                                    "name": "Anze Xie"
                                },
                                {
                                    "authorId": "2243336934",
                                    "name": "Eric P. Xing"
                                },
                                {
                                    "authorId": "2378954",
                                    "name": "Xuezhe Ma"
                                },
                                {
                                    "authorId": "2055174324",
                                    "name": "Ion Stoica"
                                },
                                {
                                    "authorId": "2254681613",
                                    "name": "Joseph Gonzalez"
                                },
                                {
                                    "authorId": "2257340589",
                                    "name": "Hao Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "",
                            "n_citations": 22
                        },
                        "score": 0.64111328125
                    },
                    {
                        "id": "(Dao et al., 2022)",
                        "snippets": [
                            "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy)."
                        ],
                        "paper": {
                            "corpus_id": 249151871,
                            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                            "authors": [
                                {
                                    "authorId": "24593911",
                                    "name": "Tri Dao"
                                },
                                {
                                    "authorId": "49577833",
                                    "name": "Daniel Y. Fu"
                                },
                                {
                                    "authorId": "2490652",
                                    "name": "Stefano Ermon"
                                },
                                {
                                    "authorId": "1755572",
                                    "name": "A. Rudra"
                                },
                                {
                                    "authorId": "2061444681",
                                    "name": "Christopher R'e"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2285
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jacobs et al., 2023)",
                        "snippets": [
                            "Fully general and implementation agnostic attention: DeepSpeed sequence parallelism (Ulysses) supports dense as well as sparse attention, and it works with efficient attention implementations such as FlashAttention v2 [Dao, 2023]",
                            "We observe similar trends with sparse attention as dense attention experiments. We observe more than 2x throughput performance of DeepSpeed sequence parallelism compared to Megatron-LM. For memory saving, DeepSpeed sequence parallelism leveraging ZeRO-3 scales to 4x longer sequence lengths than Megatron-LM",
                            "DeepSpeed sequence parallelism outperforms Megatron-LM for sequence length that can be run with both. In fact, the current DeepSpeed throughput is bottlenecked by the local sparse attention implementation, and as a result DeepSpeed throughput decreases as the sequence length increases. We expect this gap in performance between DeepSpeed and Megatron-LM to increase further for larger sequence lengths as we improve the performance of the local sparse attention implementation in future."
                        ],
                        "paper": {
                            "corpus_id": 262826014,
                            "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
                            "authors": [
                                {
                                    "authorId": "1683824",
                                    "name": "S. A. Jacobs"
                                },
                                {
                                    "authorId": "2118817779",
                                    "name": "Masahiro Tanaka"
                                },
                                {
                                    "authorId": "2248958850",
                                    "name": "Chengming Zhang"
                                },
                                {
                                    "authorId": "67016465",
                                    "name": "Minjia Zhang"
                                },
                                {
                                    "authorId": "2145202945",
                                    "name": "L. Song"
                                },
                                {
                                    "authorId": "32817044",
                                    "name": "Samyam Rajbhandari"
                                },
                                {
                                    "authorId": "2145020341",
                                    "name": "Yuxiong He"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 120
                        },
                        "score": 0.87060546875
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "The second challenge is parallelism for sequence length, that is, Sequence Parallelism (SP) (Li et al., 2021). Since the memory and calculation of a single device are limited, ColossalAI-SP (Li et al., 2021) first proposes the segmentation and parallelism for the sequence dimension in addition to tensor parallelism for the hidden dimension and pipeline parallelism for model depth. On this basis, Ring Attention (Lv et al., 2023) uses blockwise self-attention to split long sequences into different devices and overlap the communication of key-value blocks. Besides, LightSeq (Li et al., 2023b) further improves the efficiency of long sequence modeling through load balancing for causal language modelings and a re-materialization-aware checkpointing strategy. Although the above sequence parallelism can achieve infinitely long sequence modeling, they ignore the compatibility with the existing efficient self-attention mechanisms such as FlashAttention (Dao et al., 2022)Dao, 2023) to achieve input with almost infinity. On the contrary, Megatron-SP (Korthikanti et al., 2022) only uses sequence parallelism during Dropout and Layernorm operations, thereby reducing activation redundancy. In addition, DeepSpeed-Ulysses (Jacobs et al., 2023) uses an alternative all-to-all collective communication gathering weight for attention computation when segmenting the sequence, avoiding communication overhead that originally increases with length."
                        ],
                        "paper": {
                            "corpus_id": 263828829,
                            "title": "Scaling Laws of RoPE-based Extrapolation",
                            "authors": [
                                {
                                    "authorId": "2257094943",
                                    "name": "Xiaoran Liu"
                                },
                                {
                                    "authorId": "146948229",
                                    "name": "Hang Yan"
                                },
                                {
                                    "authorId": "2257086624",
                                    "name": "Shuo Zhang"
                                },
                                {
                                    "authorId": "2064164220",
                                    "name": "Chen An"
                                },
                                {
                                    "authorId": "1767521",
                                    "name": "Xipeng Qiu"
                                },
                                {
                                    "authorId": "2258618409",
                                    "name": "Dahua Lin"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 89
                        },
                        "score": 0.56689453125
                    },
                    {
                        "id": "(Lv et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) are increasingly pivotal in a wide range of natural language processing tasks. Access to pre-trained models, courtesy of the open-source community, has made it possible to adapt these models to specific applications for enhanced performance. However, the substantial resources required for training these models necessitate efficient solutions. This paper introduces CoLLiE, an efficient library that facilitates collaborative training of large language models using 3D parallelism, parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion, Adan, Sophia, LOMO and AdaLomo. With its modular design and comprehensive functionality, CoLLiE offers a balanced blend of efficiency, ease of use, and customization. CoLLiE has proven superior training efficiency in comparison with prevalent solutions in pre-training and fine-tuning scenarios. Furthermore, we provide an empirical evaluation of the correlation between model size and GPU memory consumption under different optimization methods, as well as an analysis of the throughput. Lastly, we carry out a comprehensive comparison of various optimizers and PEFT methods within the instruction-tuning context. CoLLiE is available at https://github.com/OpenLMLab/collie."
                        ],
                        "paper": {
                            "corpus_id": 265551519,
                            "title": "CoLLiE: Collaborative Training of Large Language Models in an Efficient Way",
                            "authors": [
                                {
                                    "authorId": "2055634356",
                                    "name": "Kai Lv"
                                },
                                {
                                    "authorId": "2257086624",
                                    "name": "Shuo Zhang"
                                },
                                {
                                    "authorId": "2269141855",
                                    "name": "Tianle Gu"
                                },
                                {
                                    "authorId": "2269141152",
                                    "name": "Shuhao Xing"
                                },
                                {
                                    "authorId": "2269567728",
                                    "name": "Jiawei Hong"
                                },
                                {
                                    "authorId": "2269551684",
                                    "name": "Keyu Chen"
                                },
                                {
                                    "authorId": "2257094943",
                                    "name": "Xiaoran Liu"
                                },
                                {
                                    "authorId": "2145435513",
                                    "name": "Yuqing Yang"
                                },
                                {
                                    "authorId": "2269420111",
                                    "name": "Honglin Guo"
                                },
                                {
                                    "authorId": "2136108329",
                                    "name": "Tengxiao Liu"
                                },
                                {
                                    "authorId": "2269176065",
                                    "name": "Yu Sun"
                                },
                                {
                                    "authorId": "3187768",
                                    "name": "Qipeng Guo"
                                },
                                {
                                    "authorId": "146948229",
                                    "name": "Hang Yan"
                                },
                                {
                                    "authorId": "2256661980",
                                    "name": "Xipeng Qiu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 7
                        },
                        "score": 0
                    },
                    {
                        "id": "(Bian et al., 2025)",
                        "snippets": [
                            "More importantly, it is fully compatible with all state-of-the-art attention optimizations."
                        ],
                        "paper": {
                            "corpus_id": 276585307,
                            "title": "CQS-Attention: Scaling Up the Standard Attention Computation for Infinitely Long Sequences",
                            "authors": [
                                {
                                    "authorId": "2248690560",
                                    "name": "Yiming Bian"
                                },
                                {
                                    "authorId": "35088844",
                                    "name": "Arun Somani"
                                }
                            ],
                            "year": 2025,
                            "venue": "IEEE Access",
                            "n_citations": 0
                        },
                        "score": 0.83056640625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Notable Implementation Frameworks",
                "tldr": "Several major frameworks have implemented sequence parallelism for long-sequence Transformer models, each with distinct approaches to communication patterns and integration with other parallelism strategies. These implementations vary in their compatibility with memory-efficient algorithms and attention mechanisms, reflecting different trade-offs between communication efficiency, memory usage, and computational performance. (7 sources)",
                "text": "\n- **DeepSpeed-Ulysses**: A comprehensive framework that supports both dense and sparse attention patterns while maintaining compatibility with efficient attention implementations like FlashAttention v2 <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>. It partitions input data along the sequence dimension and uses all-to-all collective communication for attention computation, avoiding communication overhead that would otherwise increase with sequence length <Paper corpusId=\"272310078\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>. DeepSpeed-Ulysses can achieve more than 2x throughput compared to Megatron-LM implementations, and when leveraging ZeRO-3 memory optimizations, it can scale to 4x longer sequence lengths <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>.\n\n- **Megatron-SP**: Adopts a sequence parallelism technique that integrates tightly with tensor parallelism, partitioning sequences along the sequence dimension and using all-gather and reduce-scatter collectives to aggregate the query, key, value (QKV) projections for attention computation <Paper corpusId=\"272310078\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>. Unlike other implementations, Megatron-SP only applies sequence parallelism during Dropout and Layernorm operations, reducing activation redundancy <Paper corpusId=\"263828829\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. When combined with tensor parallelism and selective activation recomputation, it can reduce activation memory by 5x while eliminating over 90% of the computation overhead traditionally associated with activation recomputation <Paper corpusId=\"248693351\" paperTitle=\"(Korthikanti et al., 2022)\" isShortName></Paper>.\n\n- **Ring Attention**: Organizes tokens in a ring structure, allowing each token to attend to a fixed number of adjacent tokens in the sequence <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>. This approach enhances Transformer scalability by distributing long sequences across multiple devices and overlapping the communication of key-value pairs with the computation of blockwise attention <Paper corpusId=\"272310078\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>. Ring Attention offers a balance between computational efficiency and the ability to capture local and semi-distant relationships, though it may still have limitations for tasks requiring long-range dependencies <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>.\n\n- **Long Sequence Segmentation (LSS)**: Divides long sequences into segments distributed among GPUs, with each GPU computing partial self-attention for its segment. LSS introduces a fused communication strategy specifically designed to minimize communication overhead <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>.\n\n- **ColossalAI-SP**: One of the first frameworks to propose segmentation and parallelism for the sequence dimension, complementing tensor parallelism for the hidden dimension and pipeline parallelism for model depth <Paper corpusId=\"263828829\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"246017095\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\n- **Context Parallelism (CP)**: Often implemented as an extension of sequence parallelism, particularly for ultra-long sequences (e.g., exceeding 200k tokens). CP can be configured as an outer parallelism layer surrounding inner TP-SP implementations. During non-attention computations, each layer processes a fraction of the input sequence, while attention computations use all-to-all communication to transpose the sequence, enabling each rank to process attention with full context length but only a fraction of the attention heads <Paper corpusId=\"278714643\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper>.\n\n- **Dynamic Sequence Parallelism (DSP)**: Enables efficient sequence parallelism for multi-dimensional transformers by dynamically switching the parallelism dimension according to the current computation stage <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Jacobs et al., 2023)",
                        "snippets": [
                            "Fully general and implementation agnostic attention: DeepSpeed sequence parallelism (Ulysses) supports dense as well as sparse attention, and it works with efficient attention implementations such as FlashAttention v2 [Dao, 2023]",
                            "We observe similar trends with sparse attention as dense attention experiments. We observe more than 2x throughput performance of DeepSpeed sequence parallelism compared to Megatron-LM. For memory saving, DeepSpeed sequence parallelism leveraging ZeRO-3 scales to 4x longer sequence lengths than Megatron-LM",
                            "DeepSpeed sequence parallelism outperforms Megatron-LM for sequence length that can be run with both. In fact, the current DeepSpeed throughput is bottlenecked by the local sparse attention implementation, and as a result DeepSpeed throughput decreases as the sequence length increases. We expect this gap in performance between DeepSpeed and Megatron-LM to increase further for larger sequence lengths as we improve the performance of the local sparse attention implementation in future."
                        ],
                        "paper": {
                            "corpus_id": 262826014,
                            "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
                            "authors": [
                                {
                                    "authorId": "1683824",
                                    "name": "S. A. Jacobs"
                                },
                                {
                                    "authorId": "2118817779",
                                    "name": "Masahiro Tanaka"
                                },
                                {
                                    "authorId": "2248958850",
                                    "name": "Chengming Zhang"
                                },
                                {
                                    "authorId": "67016465",
                                    "name": "Minjia Zhang"
                                },
                                {
                                    "authorId": "2145202945",
                                    "name": "L. Song"
                                },
                                {
                                    "authorId": "32817044",
                                    "name": "Samyam Rajbhandari"
                                },
                                {
                                    "authorId": "2145020341",
                                    "name": "Yuxiong He"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 120
                        },
                        "score": 0.87060546875
                    },
                    {
                        "id": "(Yao et al., 2024)",
                        "snippets": [
                            "Megatron-SP (Korthikanti et al., 2022)) adopts a sequence parallelism technique which is tightly integrated with its tensor parallelism. In this approach, sequences are partitioned along the sequence dimension, and all-gather and reduce-scatter collectives are employed to aggregate the QKV (query, key, value) projections for attention computation. The communication complexity analysis indicates that, in contrast to our approach, the communication volume in Megatron-SP's sequence parallelism increases linearly with the sequence length regardless of the number of compute devices",
                            "Ring Attention (Liu et al., 2023) enhances Transformer's scalability by distributing long sequences across multiple devices. This innovative approach overlaps the communication of key-value pairs with the computation of blockwise attention, effectively increasing the feasible sequence length proportionally to the number of available devices",
                            "DeepSpeed Ulysses (Jacobs et al., 2023) tackles the challenges of sequence parallelism by partitioning input data along the sequence dimension and utilizing an efficient allto-all collective communication strategy for attention computations. Although this method maintains a constant communication volume regardless of the increase in sequence lengths and device counts, achieving significant speedups and scalability, it may still encounter practical hurdles in deployment related to large-scale clusters and the optimization of communication patterns across diverse computing environments."
                        ],
                        "paper": {
                            "corpus_id": 272310078,
                            "title": "Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer",
                            "authors": [
                                {
                                    "authorId": "2266686678",
                                    "name": "Jinghan Yao"
                                },
                                {
                                    "authorId": "2297768912",
                                    "name": "Sam Ade Jacobs"
                                },
                                {
                                    "authorId": "2226706029",
                                    "name": "Masahiro Tanaka"
                                },
                                {
                                    "authorId": "2537545",
                                    "name": "Olatunji Ruwase"
                                },
                                {
                                    "authorId": "1685408",
                                    "name": "A. Shafi"
                                },
                                {
                                    "authorId": "1802958",
                                    "name": "H. Subramoni"
                                },
                                {
                                    "authorId": "2222521323",
                                    "name": "Dhabaleswar K. Panda"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.81103515625
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "The second challenge is parallelism for sequence length, that is, Sequence Parallelism (SP) (Li et al., 2021). Since the memory and calculation of a single device are limited, ColossalAI-SP (Li et al., 2021) first proposes the segmentation and parallelism for the sequence dimension in addition to tensor parallelism for the hidden dimension and pipeline parallelism for model depth. On this basis, Ring Attention (Lv et al., 2023) uses blockwise self-attention to split long sequences into different devices and overlap the communication of key-value blocks. Besides, LightSeq (Li et al., 2023b) further improves the efficiency of long sequence modeling through load balancing for causal language modelings and a re-materialization-aware checkpointing strategy. Although the above sequence parallelism can achieve infinitely long sequence modeling, they ignore the compatibility with the existing efficient self-attention mechanisms such as FlashAttention (Dao et al., 2022)Dao, 2023) to achieve input with almost infinity. On the contrary, Megatron-SP (Korthikanti et al., 2022) only uses sequence parallelism during Dropout and Layernorm operations, thereby reducing activation redundancy. In addition, DeepSpeed-Ulysses (Jacobs et al., 2023) uses an alternative all-to-all collective communication gathering weight for attention computation when segmenting the sequence, avoiding communication overhead that originally increases with length."
                        ],
                        "paper": {
                            "corpus_id": 263828829,
                            "title": "Scaling Laws of RoPE-based Extrapolation",
                            "authors": [
                                {
                                    "authorId": "2257094943",
                                    "name": "Xiaoran Liu"
                                },
                                {
                                    "authorId": "146948229",
                                    "name": "Hang Yan"
                                },
                                {
                                    "authorId": "2257086624",
                                    "name": "Shuo Zhang"
                                },
                                {
                                    "authorId": "2064164220",
                                    "name": "Chen An"
                                },
                                {
                                    "authorId": "1767521",
                                    "name": "Xipeng Qiu"
                                },
                                {
                                    "authorId": "2258618409",
                                    "name": "Dahua Lin"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 89
                        },
                        "score": 0.56689453125
                    },
                    {
                        "id": "(Korthikanti et al., 2022)",
                        "snippets": [
                            "Training large transformer models is one of the most important computational challenges of modern AI. In this paper, we show how to significantly accelerate training of large transformer models by reducing activation recomputation. Activation recomputation is commonly used to work around memory capacity constraints. Rather than storing activations for backpropagation, they are traditionally recomputed, which saves memory but adds redundant compute. In this work, we show most of this redundant compute is unnecessary because we can reduce memory consumption sufficiently without it. We present two novel yet very simple techniques: sequence parallelism and selective activation recomputation. In conjunction with tensor parallelism, these techniques almost eliminate the need to recompute activations. We evaluate our approach on language models up to one trillion parameters in scale and show that our method reduces activation memory by 5x, while reducing execution time overhead from activation recomputation by over 90%. For example, when training a 530B parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using recomputation. Our implementation will be available in both Megatron-LM and NeMo-Megatron."
                        ],
                        "paper": {
                            "corpus_id": 248693351,
                            "title": "Reducing Activation Recomputation in Large Transformer Models",
                            "authors": [
                                {
                                    "authorId": "3111334",
                                    "name": "V. Korthikanti"
                                },
                                {
                                    "authorId": "48991386",
                                    "name": "J. Casper"
                                },
                                {
                                    "authorId": "3448779",
                                    "name": "Sangkug Lym"
                                },
                                {
                                    "authorId": "20957879",
                                    "name": "Lawrence C. McAfee"
                                },
                                {
                                    "authorId": "47304697",
                                    "name": "M. Andersch"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "2301680",
                                    "name": "Bryan Catanzaro"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Machine Learning and Systems",
                            "n_citations": 275
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tsaris et al., 2024)",
                        "snippets": [
                            "Sequence parallelism, which distributes the self-attention computation in transformer models across the sequence dimension, has been an active area of research, with many developed methods employing various approaches.DeepSpeed-Ulysses [15]introduces a suite of system optimizations specifically designed for the efficient training of extreme long sequence transformer models.DeepSpeed-Ulysses partitions the input data along the sequence dimension, which allows the system to handle much longer sequences.For attention computation, it employs an all-to-all collective communication to ensure that each GPU receives a complete sequence, but only for a non-overlapping subset of the attention heads, allowing for parallel computation of attention across GPUs.LSS [16] divides a long sequence into segments distributed among GPUs, with each GPU computing a partial selfattention for its segment.It also introduces a fused communication strategy to minimize the communication overhead.Dynamic Sequence Parallelism (DSP) [22] enables efficient sequence parallelism for multi-dimensional transformers by dynamically switching the parallelism dimension according to the current computation stage.Ring attention [23], [24] is another variant that organizes tokens in a ring structure, allowing each token to attend to a fixed number of adjacent tokens in the sequence.This method offers a compromise between dense and sparse attention, balancing computational efficiency with the ability to capture local and semi-distant relationships.However, ring attention may still fall short in tasks where long-range dependencies are crucial."
                        ],
                        "paper": {
                            "corpus_id": 270062939,
                            "title": "Sequence Length Scaling in Vision Transformers for Scientific Images on Frontier",
                            "authors": [
                                {
                                    "authorId": "3395528",
                                    "name": "A. Tsaris"
                                },
                                {
                                    "authorId": "2303676556",
                                    "name": "Chengming Zhang"
                                },
                                {
                                    "authorId": "2265563075",
                                    "name": "Xiao Wang"
                                },
                                {
                                    "authorId": "2244593486",
                                    "name": "Junqi Yin"
                                },
                                {
                                    "authorId": "2257073765",
                                    "name": "Siyan Liu"
                                },
                                {
                                    "authorId": "9259766",
                                    "name": "M. Ashfaq"
                                },
                                {
                                    "authorId": "2180823649",
                                    "name": "Ming Fan"
                                },
                                {
                                    "authorId": "2298006038",
                                    "name": "Jong-Youl Choi"
                                },
                                {
                                    "authorId": "1700692",
                                    "name": "M. Wahib"
                                },
                                {
                                    "authorId": "2258305751",
                                    "name": "Dan Lu"
                                },
                                {
                                    "authorId": "2138151793",
                                    "name": "Prasanna Balaprakash"
                                },
                                {
                                    "authorId": "2244140922",
                                    "name": "Feiyi Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.8359375
                    },
                    {
                        "id": "(Li et al., 2021)",
                        "snippets": [
                            "To model long sequences, existing works mainly focus on sparse attention (e.g., [Zaheer et al., 2020]) with linear instead of quadratic space complexity. In this paper, we aim to solve the long sequence modeling problem from the distributed system perspective. Compared with sparse attention, we devote ourselves to designing and implementing a system instead of a deep learning algorithm to train attentionbased models with longer sequences",
                            "Our RSA focuses on full self-attention in this paper. According to Table 2, when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences, because three terms of memory usage include L/N."
                        ],
                        "paper": {
                            "corpus_id": 246017095,
                            "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
                            "authors": [
                                {
                                    "authorId": "2153703322",
                                    "name": "Shenggui Li"
                                },
                                {
                                    "authorId": "2144332771",
                                    "name": "Fuzhao Xue"
                                },
                                {
                                    "authorId": "2180329661",
                                    "name": "Yongbin Li"
                                },
                                {
                                    "authorId": "144259229",
                                    "name": "Yang You"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 102
                        },
                        "score": 0.93017578125
                    },
                    {
                        "id": "(Shi et al., 2025)",
                        "snippets": [
                            "Innermost TP-SP. Among 8 ranks within a node, we adopt Tensor Parallelism(TP) (Shoeybi et al., 2019) and Sequence Parallelism(SP) (Korthikanti et al., 2022) to partition the model states and activations. In each Transformer block, we employ a dedicated MLP (a Linear layer followed by a SiLU) to incorporate conditioning embedding for Adaptive Layer Normalization (AdaLN), aiming to enhance the model's capacity to capture conditional information. However, this design incurs substantial memory overhead: these dedicated Linear collectively increase the model's total parameter count by over 3B and introduce redundant model states exceeding 20GB per rank. To address this issue, we partition these Linear layers in a column parallel fashion with their computation and parameters parallelized across devices. Specifically, during the forward pass, all-gather reconstructs the complete Linear output, while reduce-scatter synchronizes gradients across the sequence-parallel group before the Linear in the backward pass. \n\nMid-Optional CP. While TP-SP supports training with moderately long contexts, Context Parallelism (CP) (Jacobs et al., 2023) is optionally enabled only for ultra-long sequences (exceeds 200k tokens). \n\nFor non-attention computations, each layer processes 1/CP of the input sequence. Before and after attention computations, all-to-all communication is applied to transpose the sequence, enabling each rank to process attention with full context length but only 1/CP of the attention heads. Since CP incurs lower communication overhead than TP-SP, we configure TP-SP as inner parallelism and CP as outer parallelism."
                        ],
                        "paper": {
                            "corpus_id": 278714643,
                            "title": "Aquarius: A Family of Industry-Level Video Generation Models for Marketing Scenarios",
                            "authors": [
                                {
                                    "authorId": "2362191870",
                                    "name": "Huafeng Shi"
                                },
                                {
                                    "authorId": "2362279072",
                                    "name": "Jianzhong Liang"
                                },
                                {
                                    "authorId": "2332574955",
                                    "name": "Rongchang Xie"
                                },
                                {
                                    "authorId": "2357119154",
                                    "name": "Xian Wu"
                                },
                                {
                                    "authorId": "2362091271",
                                    "name": "Cheng Chen"
                                },
                                {
                                    "authorId": "2356750113",
                                    "name": "Chang Liu"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.65576171875
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Technical Challenges and Solutions",
                "tldr": "Sequence parallelism implementations face significant challenges including communication overhead, compatibility with memory-efficient algorithms, and balancing computation with communication. Recent advances have addressed these issues through optimized communication patterns, blockwise attention computation, and specialized algorithms for causal language modeling. (9 sources)",
                "text": "\nImplementing sequence parallelism for Transformer models introduces several technical challenges that researchers have worked to overcome. One of the primary obstacles is the substantial communication overhead required for distributed attention computation, which can significantly impact performance. As Liu et al. note, \"sequence parallelism introduces a significant communication overhead that cannot be fully overlapped with computation,\" limiting scalability for training and inference in large-context scenarios <Paper corpusId=\"263608461\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>.\n\nDifferent approaches to managing this communication overhead have emerged. Ring Self-Attention (RSA) was an early attempt to reduce communication costs compared to standard sequence parallelism by using ring-style communication patterns <Paper corpusId=\"246017095\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>. However, this approach still struggles with overlapping communication and computation due to \"constraints of arithmetic intensity\" <Paper corpusId=\"263608461\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>. More recent implementations like Ring Attention have improved upon this by using blockwise self-attention to split long sequences across devices while overlapping the communication of key-value blocks <Paper corpusId=\"265551519\" paperTitle=\"(Lv et al., 2023)\" isShortName></Paper>, enhancing scalability through better communication-computation balance.\n\nAnother significant challenge is achieving compatibility between sequence parallelism and memory-efficient attention algorithms like FlashAttention <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>. Many sequence parallelism implementations \"ignore the compatibility with the existing efficient self-attention mechanisms such as FlashAttention\" <Paper corpusId=\"263828829\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>, limiting their ability to fully optimize memory usage. This has led to specialized solutions like DISTFLASHATTN, which is specifically designed to be compatible with both memory-efficient attention and causal language modeling <Paper corpusId=\"263671659\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nFor LLM serving systems, sequence parallelism introduces unique challenges compared to training scenarios. These include: \"(1) LLM serving is usually latency-sensitive and thus requires much smaller batch sizes than LLM training; (2) LLM serving has an autoregressive decode phase, where the sequence length is only one, but it requires large memory for KV cache storage; (3) LLM serving usually relies on large fused kernels for improving performance\" <Paper corpusId=\"269922123\" paperTitle=\"(Zeng et al., 2024)\" isShortName></Paper>. These considerations necessitate different optimization strategies for inference compared to training.\n\nVarious communication patterns have been explored to address these challenges. DeepSpeed-Ulysses employs \"an all-to-all collective communication to ensure that each GPU receives a complete sequence, but only for a non-overlapping subset of the attention heads, allowing for parallel computation of attention across GPUs\" <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>. This approach avoids communication overhead that would otherwise increase with sequence length. In contrast, Megatron-SP takes a more targeted approach, applying \"sequence parallelism only during Dropout and Layernorm operations, thereby reducing activation redundancy\" <Paper corpusId=\"263828829\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"248693351\" paperTitle=\"(Korthikanti et al., 2022)\" isShortName></Paper>.\n\nThe Long Sequence Segmentation (LSS) approach tackles communication overhead by introducing \"a fused communication strategy to minimize the communication overhead\" when dividing long sequences into segments distributed among GPUs <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>. Additionally, Dynamic Sequence Parallelism (DSP) provides flexibility by \"dynamically switching the parallelism dimension according to the current computation stage\" for multi-dimensional transformers <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>.\n\nThese technical solutions demonstrate the ongoing evolution of sequence parallelism to address the computational and memory challenges of processing increasingly long sequences in Transformer models. By optimizing communication patterns, improving compatibility with memory-efficient algorithms, and developing specialized approaches for different use cases, researchers continue to push the boundaries of what's possible with long-context Transformer models.",
                "citations": [
                    {
                        "id": "(Liu et al._1, 2023)",
                        "snippets": [
                            "Tensor parallelism can only reduce parts of activations memory and sequence parallelism introduces a significant communication overhead that cannot be fully overlapped with computation. Prior work has studied sharding along sequence and attention heads, and gathering sequences via an optimized all-to-all topology, achieving reduced communication [17]. However, this method is restricted by the number of attention heads and requires gathering the full sequence on each device. In comparison, our approach fully overlaps communication with blockwise computation, enhancing its scalability. Prior work extends sequence parallelism for computing self-attention using a ring topology (Li et al., 2021), which reduces the communication cost compared to standard sequence parallelism. However, overlapping communication with computation remains challenging due to the constraints of arithmetic intensity. The communication overheads render this approach infeasible for training and inference in largecontext scenarios."
                        ],
                        "paper": {
                            "corpus_id": 263608461,
                            "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context",
                            "authors": [
                                {
                                    "authorId": "2256317240",
                                    "name": "Hao Liu"
                                },
                                {
                                    "authorId": "2253469012",
                                    "name": "Matei Zaharia"
                                },
                                {
                                    "authorId": "2253464956",
                                    "name": "Pieter Abbeel"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 258
                        },
                        "score": 0.646484375
                    },
                    {
                        "id": "(Li et al., 2021)",
                        "snippets": [
                            "To model long sequences, existing works mainly focus on sparse attention (e.g., [Zaheer et al., 2020]) with linear instead of quadratic space complexity. In this paper, we aim to solve the long sequence modeling problem from the distributed system perspective. Compared with sparse attention, we devote ourselves to designing and implementing a system instead of a deep learning algorithm to train attentionbased models with longer sequences",
                            "Our RSA focuses on full self-attention in this paper. According to Table 2, when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences, because three terms of memory usage include L/N."
                        ],
                        "paper": {
                            "corpus_id": 246017095,
                            "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
                            "authors": [
                                {
                                    "authorId": "2153703322",
                                    "name": "Shenggui Li"
                                },
                                {
                                    "authorId": "2144332771",
                                    "name": "Fuzhao Xue"
                                },
                                {
                                    "authorId": "2180329661",
                                    "name": "Yongbin Li"
                                },
                                {
                                    "authorId": "144259229",
                                    "name": "Yang You"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 102
                        },
                        "score": 0.93017578125
                    },
                    {
                        "id": "(Lv et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) are increasingly pivotal in a wide range of natural language processing tasks. Access to pre-trained models, courtesy of the open-source community, has made it possible to adapt these models to specific applications for enhanced performance. However, the substantial resources required for training these models necessitate efficient solutions. This paper introduces CoLLiE, an efficient library that facilitates collaborative training of large language models using 3D parallelism, parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion, Adan, Sophia, LOMO and AdaLomo. With its modular design and comprehensive functionality, CoLLiE offers a balanced blend of efficiency, ease of use, and customization. CoLLiE has proven superior training efficiency in comparison with prevalent solutions in pre-training and fine-tuning scenarios. Furthermore, we provide an empirical evaluation of the correlation between model size and GPU memory consumption under different optimization methods, as well as an analysis of the throughput. Lastly, we carry out a comprehensive comparison of various optimizers and PEFT methods within the instruction-tuning context. CoLLiE is available at https://github.com/OpenLMLab/collie."
                        ],
                        "paper": {
                            "corpus_id": 265551519,
                            "title": "CoLLiE: Collaborative Training of Large Language Models in an Efficient Way",
                            "authors": [
                                {
                                    "authorId": "2055634356",
                                    "name": "Kai Lv"
                                },
                                {
                                    "authorId": "2257086624",
                                    "name": "Shuo Zhang"
                                },
                                {
                                    "authorId": "2269141855",
                                    "name": "Tianle Gu"
                                },
                                {
                                    "authorId": "2269141152",
                                    "name": "Shuhao Xing"
                                },
                                {
                                    "authorId": "2269567728",
                                    "name": "Jiawei Hong"
                                },
                                {
                                    "authorId": "2269551684",
                                    "name": "Keyu Chen"
                                },
                                {
                                    "authorId": "2257094943",
                                    "name": "Xiaoran Liu"
                                },
                                {
                                    "authorId": "2145435513",
                                    "name": "Yuqing Yang"
                                },
                                {
                                    "authorId": "2269420111",
                                    "name": "Honglin Guo"
                                },
                                {
                                    "authorId": "2136108329",
                                    "name": "Tengxiao Liu"
                                },
                                {
                                    "authorId": "2269176065",
                                    "name": "Yu Sun"
                                },
                                {
                                    "authorId": "3187768",
                                    "name": "Qipeng Guo"
                                },
                                {
                                    "authorId": "146948229",
                                    "name": "Hang Yan"
                                },
                                {
                                    "authorId": "2256661980",
                                    "name": "Xipeng Qiu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 7
                        },
                        "score": 0
                    },
                    {
                        "id": "(Dao et al., 2022)",
                        "snippets": [
                            "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy)."
                        ],
                        "paper": {
                            "corpus_id": 249151871,
                            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                            "authors": [
                                {
                                    "authorId": "24593911",
                                    "name": "Tri Dao"
                                },
                                {
                                    "authorId": "49577833",
                                    "name": "Daniel Y. Fu"
                                },
                                {
                                    "authorId": "2490652",
                                    "name": "Stefano Ermon"
                                },
                                {
                                    "authorId": "1755572",
                                    "name": "A. Rudra"
                                },
                                {
                                    "authorId": "2061444681",
                                    "name": "Christopher R'e"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2285
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "The second challenge is parallelism for sequence length, that is, Sequence Parallelism (SP) (Li et al., 2021). Since the memory and calculation of a single device are limited, ColossalAI-SP (Li et al., 2021) first proposes the segmentation and parallelism for the sequence dimension in addition to tensor parallelism for the hidden dimension and pipeline parallelism for model depth. On this basis, Ring Attention (Lv et al., 2023) uses blockwise self-attention to split long sequences into different devices and overlap the communication of key-value blocks. Besides, LightSeq (Li et al., 2023b) further improves the efficiency of long sequence modeling through load balancing for causal language modelings and a re-materialization-aware checkpointing strategy. Although the above sequence parallelism can achieve infinitely long sequence modeling, they ignore the compatibility with the existing efficient self-attention mechanisms such as FlashAttention (Dao et al., 2022)Dao, 2023) to achieve input with almost infinity. On the contrary, Megatron-SP (Korthikanti et al., 2022) only uses sequence parallelism during Dropout and Layernorm operations, thereby reducing activation redundancy. In addition, DeepSpeed-Ulysses (Jacobs et al., 2023) uses an alternative all-to-all collective communication gathering weight for attention computation when segmenting the sequence, avoiding communication overhead that originally increases with length."
                        ],
                        "paper": {
                            "corpus_id": 263828829,
                            "title": "Scaling Laws of RoPE-based Extrapolation",
                            "authors": [
                                {
                                    "authorId": "2257094943",
                                    "name": "Xiaoran Liu"
                                },
                                {
                                    "authorId": "146948229",
                                    "name": "Hang Yan"
                                },
                                {
                                    "authorId": "2257086624",
                                    "name": "Shuo Zhang"
                                },
                                {
                                    "authorId": "2064164220",
                                    "name": "Chen An"
                                },
                                {
                                    "authorId": "1767521",
                                    "name": "Xipeng Qiu"
                                },
                                {
                                    "authorId": "2258618409",
                                    "name": "Dahua Lin"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 89
                        },
                        "score": 0.56689453125
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "Memory-efficient attention. (Dao et al., 2022) and Lefaudeux et al. (2022) propose to use an online normalizer (Milakov & Gimelshein, 2018) to compute the attention in a blockwise and memory-efficient way. It reduces peak memory usage by not materializing large intermediate states, e.g. the attention softmax matrix. In addition, research on sparse attention computes only a sparse subset of the attention score, which also reduces the memory footprints yet may lead to inferior performance (Beltagy et al., 2020;Sun et al., 2022;(Zaheer et al., 2020). In this work, we limit our scope to exact attention. \n\nSequence parallelism and ring attention Ring Self-Attention (Li et al., 2021) is among the first to parallelize Transformers in the sequence dimension. However, its distributed attention design is not optimized for causal language modeling and incompatible with memory-efficient attention, which are crucial for long-context LLM training. Ring Attention (Liu et al., 2023) proposes to compute distributed attention in a memory-efficient blockwise pattern. However, it is also not optimized for causal language modeling, leading to 2\u00d7 extra computation. DISTFLASHATTN optimizes for both memory-efficient attention and causal language modeling."
                        ],
                        "paper": {
                            "corpus_id": 263671659,
                            "title": "DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training",
                            "authors": [
                                {
                                    "authorId": "2117961435",
                                    "name": "Dacheng Li"
                                },
                                {
                                    "authorId": "2254264970",
                                    "name": "Rulin Shao"
                                },
                                {
                                    "authorId": "2254221083",
                                    "name": "Anze Xie"
                                },
                                {
                                    "authorId": "2243336934",
                                    "name": "Eric P. Xing"
                                },
                                {
                                    "authorId": "2378954",
                                    "name": "Xuezhe Ma"
                                },
                                {
                                    "authorId": "2055174324",
                                    "name": "Ion Stoica"
                                },
                                {
                                    "authorId": "2254681613",
                                    "name": "Joseph Gonzalez"
                                },
                                {
                                    "authorId": "2257340589",
                                    "name": "Hao Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "",
                            "n_citations": 22
                        },
                        "score": 0.64111328125
                    },
                    {
                        "id": "(Zeng et al., 2024)",
                        "snippets": [
                            "\u2022 SP was first introduced by Li et al. [97] and has been widely used in distributed LLM training frameworks such as Megatron (Korthikanti et al., 2022) and Deepspeed [99].In the context of LLM serving systems, new challenges emerge: (1) LLM serving is usually latency-sensitive and thus requires much smaller batch sizes than LLM training; (2) LLM serving has an autoregressive decode phase, where the sequence length is only one, but it requires large memory for KV cache storage; (3) LLM serving usually relies on large fused kernels for improving performance.While the feed-forward network (FFN) computations for each token in a sequence are linearly independent, the computations for attention are not.Consequently, substantial data exchange is involved when computing distributed attention using SP, thereby opening significant space for performance optimization."
                        ],
                        "paper": {
                            "corpus_id": 269922123,
                            "title": "The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving",
                            "authors": [
                                {
                                    "authorId": "2302327872",
                                    "name": "Pai Zeng"
                                },
                                {
                                    "authorId": "2302322712",
                                    "name": "Zhenyu Ning"
                                },
                                {
                                    "authorId": "2302350607",
                                    "name": "Jieru Zhao"
                                },
                                {
                                    "authorId": "1492129615",
                                    "name": "Weihao Cui"
                                },
                                {
                                    "authorId": "2239403838",
                                    "name": "Mengwei Xu"
                                },
                                {
                                    "authorId": "2286749413",
                                    "name": "Liwei Guo"
                                },
                                {
                                    "authorId": "2280208638",
                                    "name": "XuSheng Chen"
                                },
                                {
                                    "authorId": "2280137293",
                                    "name": "Yizhou Shan"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.6708984375
                    },
                    {
                        "id": "(Tsaris et al., 2024)",
                        "snippets": [
                            "Sequence parallelism, which distributes the self-attention computation in transformer models across the sequence dimension, has been an active area of research, with many developed methods employing various approaches.DeepSpeed-Ulysses [15]introduces a suite of system optimizations specifically designed for the efficient training of extreme long sequence transformer models.DeepSpeed-Ulysses partitions the input data along the sequence dimension, which allows the system to handle much longer sequences.For attention computation, it employs an all-to-all collective communication to ensure that each GPU receives a complete sequence, but only for a non-overlapping subset of the attention heads, allowing for parallel computation of attention across GPUs.LSS [16] divides a long sequence into segments distributed among GPUs, with each GPU computing a partial selfattention for its segment.It also introduces a fused communication strategy to minimize the communication overhead.Dynamic Sequence Parallelism (DSP) [22] enables efficient sequence parallelism for multi-dimensional transformers by dynamically switching the parallelism dimension according to the current computation stage.Ring attention [23], [24] is another variant that organizes tokens in a ring structure, allowing each token to attend to a fixed number of adjacent tokens in the sequence.This method offers a compromise between dense and sparse attention, balancing computational efficiency with the ability to capture local and semi-distant relationships.However, ring attention may still fall short in tasks where long-range dependencies are crucial."
                        ],
                        "paper": {
                            "corpus_id": 270062939,
                            "title": "Sequence Length Scaling in Vision Transformers for Scientific Images on Frontier",
                            "authors": [
                                {
                                    "authorId": "3395528",
                                    "name": "A. Tsaris"
                                },
                                {
                                    "authorId": "2303676556",
                                    "name": "Chengming Zhang"
                                },
                                {
                                    "authorId": "2265563075",
                                    "name": "Xiao Wang"
                                },
                                {
                                    "authorId": "2244593486",
                                    "name": "Junqi Yin"
                                },
                                {
                                    "authorId": "2257073765",
                                    "name": "Siyan Liu"
                                },
                                {
                                    "authorId": "9259766",
                                    "name": "M. Ashfaq"
                                },
                                {
                                    "authorId": "2180823649",
                                    "name": "Ming Fan"
                                },
                                {
                                    "authorId": "2298006038",
                                    "name": "Jong-Youl Choi"
                                },
                                {
                                    "authorId": "1700692",
                                    "name": "M. Wahib"
                                },
                                {
                                    "authorId": "2258305751",
                                    "name": "Dan Lu"
                                },
                                {
                                    "authorId": "2138151793",
                                    "name": "Prasanna Balaprakash"
                                },
                                {
                                    "authorId": "2244140922",
                                    "name": "Feiyi Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.8359375
                    },
                    {
                        "id": "(Korthikanti et al., 2022)",
                        "snippets": [
                            "Training large transformer models is one of the most important computational challenges of modern AI. In this paper, we show how to significantly accelerate training of large transformer models by reducing activation recomputation. Activation recomputation is commonly used to work around memory capacity constraints. Rather than storing activations for backpropagation, they are traditionally recomputed, which saves memory but adds redundant compute. In this work, we show most of this redundant compute is unnecessary because we can reduce memory consumption sufficiently without it. We present two novel yet very simple techniques: sequence parallelism and selective activation recomputation. In conjunction with tensor parallelism, these techniques almost eliminate the need to recompute activations. We evaluate our approach on language models up to one trillion parameters in scale and show that our method reduces activation memory by 5x, while reducing execution time overhead from activation recomputation by over 90%. For example, when training a 530B parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using recomputation. Our implementation will be available in both Megatron-LM and NeMo-Megatron."
                        ],
                        "paper": {
                            "corpus_id": 248693351,
                            "title": "Reducing Activation Recomputation in Large Transformer Models",
                            "authors": [
                                {
                                    "authorId": "3111334",
                                    "name": "V. Korthikanti"
                                },
                                {
                                    "authorId": "48991386",
                                    "name": "J. Casper"
                                },
                                {
                                    "authorId": "3448779",
                                    "name": "Sangkug Lym"
                                },
                                {
                                    "authorId": "20957879",
                                    "name": "Lawrence C. McAfee"
                                },
                                {
                                    "authorId": "47304697",
                                    "name": "M. Andersch"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "2301680",
                                    "name": "Bryan Catanzaro"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Machine Learning and Systems",
                            "n_citations": 275
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Synergies Among Approaches",
                "tldr": "The combination of sequence parallelism with other efficiency techniques creates multiplicative benefits for extending Transformer context lengths. These approaches complement each other in orthogonal ways, with sequence parallelism addressing distributed computing challenges while sparse attention and memory-efficient algorithms tackle algorithmic complexity. (9 sources)",
                "text": "\nThe integration of sequence parallelism with other efficiency techniques creates powerful synergies that enable Transformer models to process significantly longer contexts than would be possible with any single approach. Sequence parallelism works orthogonally to other parallelism strategies, allowing for straightforward combination with tensor parallelism, data parallelism, and other distributed training techniques <Paper corpusId=\"258987968\" paperTitle=\"(Liu et al._2, 2023)\" isShortName></Paper>. This orthogonality means that the benefits of sequence parallelism multiply with those of other approaches rather than merely adding to them.\n\nWhen sequence parallelism is combined with sparse attention patterns, the results are particularly impressive. DeepSpeed-Ulysses demonstrates that sequence parallelism works effectively with both dense and sparse attention implementations, maintaining compatibility with efficient attention implementations like FlashAttention v2 <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>. The performance benefits are substantial, with DeepSpeed's sequence parallelism showing more than 2x throughput compared to Megatron-LM when using sparse attention patterns <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>. This combination addresses both the memory requirements and computational complexity of long-sequence processing simultaneously.\n\nTechniques that exploit sparsity in attention patterns are particularly complementary to sequence parallelism, with the potential to \"significantly scale the sequence length that can be achieved using these techniques for a fixed number of nodes\" <Paper corpusId=\"276106883\" paperTitle=\"(Tomczak et al., 2025)\" isShortName></Paper>. Sparse attention methods like those proposed by Zaheer et al. reduce memory footprints by computing only a subset of the attention score, though this approach may impact model performance <Paper corpusId=\"263671659\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>.\n\nRecent innovations have focused on optimizing the compatibility between sequence parallelism and memory-efficient algorithms. Some implementations are now \"fully compatible with all state-of-the-art attention optimizations\" <Paper corpusId=\"276585307\" paperTitle=\"(Bian et al., 2025)\" isShortName></Paper>, ensuring that the memory efficiency benefits of algorithms like FlashAttention can be fully realized within a distributed sequence parallelism framework. FlashAttention itself uses tiling to reduce memory reads/writes between GPU high bandwidth memory and on-chip SRAM, complementing the distributed memory approach of sequence parallelism <Paper corpusId=\"263671659\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>.\n\nFor linear attention models, which naturally provide linear-time training and constant-memory inference advantages, specialized sequence parallelism implementations like LASP and LASP-2 have been developed <Paper corpusId=\"276885221\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>. These implementations leverage the unique \"right-product-first\" property of linear attention to create efficient communication patterns. LASP-2, in particular, replaces ring-style communication with an all-gather collective operation, improving both communication and computation parallelism for linear attention implementations <Paper corpusId=\"276885221\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper> <Paper corpusId=\"276259019\" paperTitle=\"(Sun et al._1, 2025)\" isShortName></Paper>.\n\nThe synergistic integration of sequence parallelism with other efficiency techniques has enabled researchers to push the boundaries of what's possible with Transformer models, creating systems capable of processing extremely long sequences that would be infeasible with any single approach alone. This combination of distributed computing strategies with algorithmic optimizations represents a powerful framework for addressing the computational challenges of next-generation language models.",
                "citations": [
                    {
                        "id": "(Liu et al._2, 2023)",
                        "snippets": [
                            "Several studies explored sharding large model on distributed devices tensor, data, or sequence parallelism [49,16,55,27,59,31,(Rasley et al., 2020). Ours shares similarities with the sequence parallelism [27] where sequences are distributed across devices, in contrast, ours implements blockwise computation on sequences for each device. This creates an orthogonal relationship between our method and sequence parallelism, allowing for straightforward combination. In addition, our methodology is compatible with both tensor and data parallelism."
                        ],
                        "paper": {
                            "corpus_id": 258987968,
                            "title": "Blockwise Parallel Transformer for Large Context Models",
                            "authors": [
                                {
                                    "authorId": "2143856672",
                                    "name": "Hao Liu"
                                },
                                {
                                    "authorId": "1689992",
                                    "name": "P. Abbeel"
                                }
                            ],
                            "year": 2023,
                            "venue": "",
                            "n_citations": 11
                        },
                        "score": 0.74169921875
                    },
                    {
                        "id": "(Jacobs et al., 2023)",
                        "snippets": [
                            "Fully general and implementation agnostic attention: DeepSpeed sequence parallelism (Ulysses) supports dense as well as sparse attention, and it works with efficient attention implementations such as FlashAttention v2 [Dao, 2023]",
                            "We observe similar trends with sparse attention as dense attention experiments. We observe more than 2x throughput performance of DeepSpeed sequence parallelism compared to Megatron-LM. For memory saving, DeepSpeed sequence parallelism leveraging ZeRO-3 scales to 4x longer sequence lengths than Megatron-LM",
                            "DeepSpeed sequence parallelism outperforms Megatron-LM for sequence length that can be run with both. In fact, the current DeepSpeed throughput is bottlenecked by the local sparse attention implementation, and as a result DeepSpeed throughput decreases as the sequence length increases. We expect this gap in performance between DeepSpeed and Megatron-LM to increase further for larger sequence lengths as we improve the performance of the local sparse attention implementation in future."
                        ],
                        "paper": {
                            "corpus_id": 262826014,
                            "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
                            "authors": [
                                {
                                    "authorId": "1683824",
                                    "name": "S. A. Jacobs"
                                },
                                {
                                    "authorId": "2118817779",
                                    "name": "Masahiro Tanaka"
                                },
                                {
                                    "authorId": "2248958850",
                                    "name": "Chengming Zhang"
                                },
                                {
                                    "authorId": "67016465",
                                    "name": "Minjia Zhang"
                                },
                                {
                                    "authorId": "2145202945",
                                    "name": "L. Song"
                                },
                                {
                                    "authorId": "32817044",
                                    "name": "Samyam Rajbhandari"
                                },
                                {
                                    "authorId": "2145020341",
                                    "name": "Yuxiong He"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 120
                        },
                        "score": 0.87060546875
                    },
                    {
                        "id": "(Tomczak et al., 2025)",
                        "snippets": [
                            "Our technique, that exploits sparsity, is orthogonal to these methods and has the potential to significantly scale the sequence length that can be achieved using these techniques for a fixed number of nodes."
                        ],
                        "paper": {
                            "corpus_id": 276106883,
                            "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
                            "authors": [
                                {
                                    "authorId": "2276535724",
                                    "name": "Nathaniel Tomczak"
                                },
                                {
                                    "authorId": "2873546",
                                    "name": "S. Kuppannagari"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.81494140625
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "Memory-efficient attention. (Dao et al., 2022) and Lefaudeux et al. (2022) propose to use an online normalizer (Milakov & Gimelshein, 2018) to compute the attention in a blockwise and memory-efficient way. It reduces peak memory usage by not materializing large intermediate states, e.g. the attention softmax matrix. In addition, research on sparse attention computes only a sparse subset of the attention score, which also reduces the memory footprints yet may lead to inferior performance (Beltagy et al., 2020;Sun et al., 2022;(Zaheer et al., 2020). In this work, we limit our scope to exact attention. \n\nSequence parallelism and ring attention Ring Self-Attention (Li et al., 2021) is among the first to parallelize Transformers in the sequence dimension. However, its distributed attention design is not optimized for causal language modeling and incompatible with memory-efficient attention, which are crucial for long-context LLM training. Ring Attention (Liu et al., 2023) proposes to compute distributed attention in a memory-efficient blockwise pattern. However, it is also not optimized for causal language modeling, leading to 2\u00d7 extra computation. DISTFLASHATTN optimizes for both memory-efficient attention and causal language modeling."
                        ],
                        "paper": {
                            "corpus_id": 263671659,
                            "title": "DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training",
                            "authors": [
                                {
                                    "authorId": "2117961435",
                                    "name": "Dacheng Li"
                                },
                                {
                                    "authorId": "2254264970",
                                    "name": "Rulin Shao"
                                },
                                {
                                    "authorId": "2254221083",
                                    "name": "Anze Xie"
                                },
                                {
                                    "authorId": "2243336934",
                                    "name": "Eric P. Xing"
                                },
                                {
                                    "authorId": "2378954",
                                    "name": "Xuezhe Ma"
                                },
                                {
                                    "authorId": "2055174324",
                                    "name": "Ion Stoica"
                                },
                                {
                                    "authorId": "2254681613",
                                    "name": "Joseph Gonzalez"
                                },
                                {
                                    "authorId": "2257340589",
                                    "name": "Hao Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "",
                            "n_citations": 22
                        },
                        "score": 0.64111328125
                    },
                    {
                        "id": "(Zaheer et al., 2020)",
                        "snippets": [
                            "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."
                        ],
                        "paper": {
                            "corpus_id": 220831004,
                            "title": "Big Bird: Transformers for Longer Sequences",
                            "authors": [
                                {
                                    "authorId": "1771307",
                                    "name": "M. Zaheer"
                                },
                                {
                                    "authorId": "1947314",
                                    "name": "Guru Guruganesh"
                                },
                                {
                                    "authorId": "89890133",
                                    "name": "Kumar Avinava Dubey"
                                },
                                {
                                    "authorId": "1643737606",
                                    "name": "J. Ainslie"
                                },
                                {
                                    "authorId": "114577307",
                                    "name": "Chris Alberti"
                                },
                                {
                                    "authorId": "1722671",
                                    "name": "Santiago Onta\u00f1\u00f3n"
                                },
                                {
                                    "authorId": "38552691",
                                    "name": "Philip Pham"
                                },
                                {
                                    "authorId": "101210026",
                                    "name": "Anirudh Ravula"
                                },
                                {
                                    "authorId": "145196279",
                                    "name": "Qifan Wang"
                                },
                                {
                                    "authorId": "113906155",
                                    "name": "Li Yang"
                                },
                                {
                                    "authorId": "143629707",
                                    "name": "Amr Ahmed"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2103
                        },
                        "score": 0.423095703125
                    },
                    {
                        "id": "(Bian et al., 2025)",
                        "snippets": [
                            "More importantly, it is fully compatible with all state-of-the-art attention optimizations."
                        ],
                        "paper": {
                            "corpus_id": 276585307,
                            "title": "CQS-Attention: Scaling Up the Standard Attention Computation for Infinitely Long Sequences",
                            "authors": [
                                {
                                    "authorId": "2248690560",
                                    "name": "Yiming Bian"
                                },
                                {
                                    "authorId": "35088844",
                                    "name": "Arun Somani"
                                }
                            ],
                            "year": 2025,
                            "venue": "IEEE Access",
                            "n_citations": 0
                        },
                        "score": 0.83056640625
                    },
                    {
                        "id": "(Dao et al., 2022)",
                        "snippets": [
                            "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy)."
                        ],
                        "paper": {
                            "corpus_id": 249151871,
                            "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
                            "authors": [
                                {
                                    "authorId": "24593911",
                                    "name": "Tri Dao"
                                },
                                {
                                    "authorId": "49577833",
                                    "name": "Daniel Y. Fu"
                                },
                                {
                                    "authorId": "2490652",
                                    "name": "Stefano Ermon"
                                },
                                {
                                    "authorId": "1755572",
                                    "name": "A. Rudra"
                                },
                                {
                                    "authorId": "2061444681",
                                    "name": "Christopher R'e"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 2285
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sun et al., 2025)",
                        "snippets": [
                            "The existing methods, LASP (Sun et al., 2025) and its improved version LASP-2 (Sun et al., 2025), are designed specifically to leverage the right-productfirst property of linear attention techniques for efficient sequence parallelism (SP). LASP employs a point-to-point ring-style communication pattern, facilitating the exchange of incremental memory states across devices. This communication pattern is particularly effective for managing dependencies while minimizing the data transferred between devices, enhancing the scalability of SP. LASP-2 further refines this approach by replacing the ring-style communication with an all-gather collective communication operation, streamlining the entire communication process. This modification not only simplifies the communication structure but also improves the parallelism of computation and communication."
                        ],
                        "paper": {
                            "corpus_id": 276885221,
                            "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
                            "authors": [
                                {
                                    "authorId": "2346291295",
                                    "name": "Weigao Sun"
                                },
                                {
                                    "authorId": "2344833625",
                                    "name": "Disen Lan"
                                },
                                {
                                    "authorId": "2349575238",
                                    "name": "Tong Zhu"
                                },
                                {
                                    "authorId": "2265753258",
                                    "name": "Xiaoye Qu"
                                },
                                {
                                    "authorId": "2344895705",
                                    "name": "Yu Cheng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.75830078125
                    },
                    {
                        "id": "(Sun et al._1, 2025)",
                        "snippets": [
                            "Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE."
                        ],
                        "paper": {
                            "corpus_id": 276259019,
                            "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid",
                            "authors": [
                                {
                                    "authorId": "2225238340",
                                    "name": "Weigao Sun"
                                },
                                {
                                    "authorId": "2344833625",
                                    "name": "Disen Lan"
                                },
                                {
                                    "authorId": "2266275708",
                                    "name": "Yiran Zhong"
                                },
                                {
                                    "authorId": "2265753258",
                                    "name": "Xiaoye Qu"
                                },
                                {
                                    "authorId": "2344895705",
                                    "name": "Yu Cheng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.767578125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Trade-offs and Limitations",
                "tldr": "While sequence parallelism offers significant benefits for extending Transformer context lengths, it comes with notable trade-offs including substantial communication overhead, practical implementation challenges in large-scale environments, and memory limitations when storing full model parameters on each device. (6 sources)",
                "text": "\nSequence parallelism, despite its advantages for processing long sequences, introduces significant communication overhead that represents one of its primary limitations. Liu et al. note that \"sequence parallelism introduces a significant communication overhead that cannot be fully overlapped with computation,\" which severely restricts its applicability for training and inference in large-context scenarios <Paper corpusId=\"263608461\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>. Earlier approaches like Ring Self-Attention (RSA) attempted to mitigate this issue by using ring-style communication to reduce costs compared to standard sequence parallelism, but these methods still struggle with effectively overlapping communication and computation due to \"constraints of arithmetic intensity\" <Paper corpusId=\"263608461\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"246017095\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\nAnother significant limitation stems from implementation complexity and practical deployment challenges. DeepSpeed Ulysses, while achieving impressive performance through partitioning along the sequence dimension and using all-to-all collective communication, \"may still encounter practical hurdles in deployment related to large-scale clusters and the optimization of communication patterns across diverse computing environments\" <Paper corpusId=\"272310078\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>. These implementation challenges can limit the practical adoption of sequence parallelism approaches in production environments.\n\nMemory utilization presents another trade-off when considering sequence parallelism relative to other parallel computation approaches. While sequence parallelism demonstrates \"superior communication efficiency compared to TP (tensor parallelism), each device stores the entire model parameters, which can be memory intensive\" <Paper corpusId=\"273707418\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>. This requirement means that sequence parallelism may not be suitable for extremely large models where parameter storage becomes a bottleneck, even if activation memory is effectively distributed.\n\nImplementation design choices can also create constraints. DeepSpeed Ulysses, for example, applies \"sequence parallelism on Linear, Norm, and Dropout layers\" with \"an all-to-all communication... applied to the output of Linear qkv\" to enable efficient computation across GPUs <Paper corpusId=\"267027729\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. However, this approach imposes a constraint \"that the number of attention heads D should not exceed s_sp\" (the sequence parallelism factor), limiting flexibility in model architecture design <Paper corpusId=\"267027729\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>.\n\nCommunication patterns represent another key trade-off area among different sequence parallelism implementations. Megatron-SP's approach tightly integrates with tensor parallelism and uses all-gather and reduce-scatter collectives, but its \"communication volume... increases linearly with the sequence length regardless of the number of compute devices\" <Paper corpusId=\"272310078\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"248693351\" paperTitle=\"(Korthikanti et al., 2022)\" isShortName></Paper>. This scaling behavior contrasts with approaches like DeepSpeed Ulysses that maintain constant communication volume as sequence length increases, highlighting the different trade-offs in communication efficiency among implementation approaches.",
                "citations": [
                    {
                        "id": "(Liu et al._1, 2023)",
                        "snippets": [
                            "Tensor parallelism can only reduce parts of activations memory and sequence parallelism introduces a significant communication overhead that cannot be fully overlapped with computation. Prior work has studied sharding along sequence and attention heads, and gathering sequences via an optimized all-to-all topology, achieving reduced communication [17]. However, this method is restricted by the number of attention heads and requires gathering the full sequence on each device. In comparison, our approach fully overlaps communication with blockwise computation, enhancing its scalability. Prior work extends sequence parallelism for computing self-attention using a ring topology (Li et al., 2021), which reduces the communication cost compared to standard sequence parallelism. However, overlapping communication with computation remains challenging due to the constraints of arithmetic intensity. The communication overheads render this approach infeasible for training and inference in largecontext scenarios."
                        ],
                        "paper": {
                            "corpus_id": 263608461,
                            "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context",
                            "authors": [
                                {
                                    "authorId": "2256317240",
                                    "name": "Hao Liu"
                                },
                                {
                                    "authorId": "2253469012",
                                    "name": "Matei Zaharia"
                                },
                                {
                                    "authorId": "2253464956",
                                    "name": "Pieter Abbeel"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 258
                        },
                        "score": 0.646484375
                    },
                    {
                        "id": "(Li et al., 2021)",
                        "snippets": [
                            "To model long sequences, existing works mainly focus on sparse attention (e.g., [Zaheer et al., 2020]) with linear instead of quadratic space complexity. In this paper, we aim to solve the long sequence modeling problem from the distributed system perspective. Compared with sparse attention, we devote ourselves to designing and implementing a system instead of a deep learning algorithm to train attentionbased models with longer sequences",
                            "Our RSA focuses on full self-attention in this paper. According to Table 2, when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences, because three terms of memory usage include L/N."
                        ],
                        "paper": {
                            "corpus_id": 246017095,
                            "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
                            "authors": [
                                {
                                    "authorId": "2153703322",
                                    "name": "Shenggui Li"
                                },
                                {
                                    "authorId": "2144332771",
                                    "name": "Fuzhao Xue"
                                },
                                {
                                    "authorId": "2180329661",
                                    "name": "Yongbin Li"
                                },
                                {
                                    "authorId": "144259229",
                                    "name": "Yang You"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 102
                        },
                        "score": 0.93017578125
                    },
                    {
                        "id": "(Yao et al., 2024)",
                        "snippets": [
                            "Megatron-SP (Korthikanti et al., 2022)) adopts a sequence parallelism technique which is tightly integrated with its tensor parallelism. In this approach, sequences are partitioned along the sequence dimension, and all-gather and reduce-scatter collectives are employed to aggregate the QKV (query, key, value) projections for attention computation. The communication complexity analysis indicates that, in contrast to our approach, the communication volume in Megatron-SP's sequence parallelism increases linearly with the sequence length regardless of the number of compute devices",
                            "Ring Attention (Liu et al., 2023) enhances Transformer's scalability by distributing long sequences across multiple devices. This innovative approach overlaps the communication of key-value pairs with the computation of blockwise attention, effectively increasing the feasible sequence length proportionally to the number of available devices",
                            "DeepSpeed Ulysses (Jacobs et al., 2023) tackles the challenges of sequence parallelism by partitioning input data along the sequence dimension and utilizing an efficient allto-all collective communication strategy for attention computations. Although this method maintains a constant communication volume regardless of the increase in sequence lengths and device counts, achieving significant speedups and scalability, it may still encounter practical hurdles in deployment related to large-scale clusters and the optimization of communication patterns across diverse computing environments."
                        ],
                        "paper": {
                            "corpus_id": 272310078,
                            "title": "Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer",
                            "authors": [
                                {
                                    "authorId": "2266686678",
                                    "name": "Jinghan Yao"
                                },
                                {
                                    "authorId": "2297768912",
                                    "name": "Sam Ade Jacobs"
                                },
                                {
                                    "authorId": "2226706029",
                                    "name": "Masahiro Tanaka"
                                },
                                {
                                    "authorId": "2537545",
                                    "name": "Olatunji Ruwase"
                                },
                                {
                                    "authorId": "1685408",
                                    "name": "A. Shafi"
                                },
                                {
                                    "authorId": "1802958",
                                    "name": "H. Subramoni"
                                },
                                {
                                    "authorId": "2222521323",
                                    "name": "Dhabaleswar K. Panda"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.81103515625
                    },
                    {
                        "id": "(Fang et al., 2024)",
                        "snippets": [
                            "In contrast, SP partitions the input image across multiple devices, utilizing methods such as Ulysses or Ring to communicate the Attention input and output tensors. SP demonstrates su-perior communication efficiency compared to TP, but each device stores the entire model parameters, which can be memory intensive."
                        ],
                        "paper": {
                            "corpus_id": 273707418,
                            "title": "PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference",
                            "authors": [
                                {
                                    "authorId": "2303233521",
                                    "name": "Jiarui Fang"
                                },
                                {
                                    "authorId": "2329908310",
                                    "name": "Jinzhe Pan"
                                },
                                {
                                    "authorId": "2302815939",
                                    "name": "Jiannan Wang"
                                },
                                {
                                    "authorId": "2303334242",
                                    "name": "Aoyu Li"
                                },
                                {
                                    "authorId": "2328615626",
                                    "name": "Xibo Sun"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 6
                        },
                        "score": 0.64892578125
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "Processing long sequences is crucial for supporting important applications such as dealing with longer histories in chat applications. To this end, sequence parallelism (SP) has emerged as a technique aimed at alleviating activation memory footprints during the training of Transformers. In SP, the input tensor of each Transformer layer is divided along the sequence dimension, allowing for parallel computation across multiple GPUs. This segmentation, in conjunction with activation recomputation, results in a substantial reduction in activation memory requirements by a factor of s sp",
                            "DeepSpeed Ulysses utilizes sequence parallelism on Linear, Norm, and Dropout layers, as shown in Figure 2 (c). An all-to-all communi- cation is applied to the output of Linear qkv with a shape of B \u00d7 3 \u00d7 S/s sp \u00d7 H. This enables each GPU to receive the complete sequence of Q, K, and V (each with a shape of B \u00d7 S \u00d7 H/s sp ) for a subset of attention heads. Subsequently, FlashAttention is employed for highly efficient MHA computation with a constraint that the number of attention heads D should not exceed s sp ."
                        ],
                        "paper": {
                            "corpus_id": 267027729,
                            "title": "InternEvo: Efficient Long-sequence Large Language Model Training via Hybrid Parallelism and Redundant Sharding",
                            "authors": [
                                {
                                    "authorId": "2189935212",
                                    "name": "Qiaoling Chen"
                                },
                                {
                                    "authorId": "1393328013",
                                    "name": "Diandian Gu"
                                },
                                {
                                    "authorId": "2263696698",
                                    "name": "Guoteng Wang"
                                },
                                {
                                    "authorId": "2293554318",
                                    "name": "Xun Chen"
                                },
                                {
                                    "authorId": "2279899091",
                                    "name": "Yingtong Xiong"
                                },
                                {
                                    "authorId": "2279784429",
                                    "name": "Ting Huang"
                                },
                                {
                                    "authorId": "2150570711",
                                    "name": "Qi Hu"
                                },
                                {
                                    "authorId": "2279869979",
                                    "name": "Xin Jin"
                                },
                                {
                                    "authorId": "2114783855",
                                    "name": "Yonggang Wen"
                                },
                                {
                                    "authorId": "2146333441",
                                    "name": "Tianwei Zhang"
                                },
                                {
                                    "authorId": "2075416290",
                                    "name": "Peng Sun"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.68310546875
                    },
                    {
                        "id": "(Korthikanti et al., 2022)",
                        "snippets": [
                            "Training large transformer models is one of the most important computational challenges of modern AI. In this paper, we show how to significantly accelerate training of large transformer models by reducing activation recomputation. Activation recomputation is commonly used to work around memory capacity constraints. Rather than storing activations for backpropagation, they are traditionally recomputed, which saves memory but adds redundant compute. In this work, we show most of this redundant compute is unnecessary because we can reduce memory consumption sufficiently without it. We present two novel yet very simple techniques: sequence parallelism and selective activation recomputation. In conjunction with tensor parallelism, these techniques almost eliminate the need to recompute activations. We evaluate our approach on language models up to one trillion parameters in scale and show that our method reduces activation memory by 5x, while reducing execution time overhead from activation recomputation by over 90%. For example, when training a 530B parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using recomputation. Our implementation will be available in both Megatron-LM and NeMo-Megatron."
                        ],
                        "paper": {
                            "corpus_id": 248693351,
                            "title": "Reducing Activation Recomputation in Large Transformer Models",
                            "authors": [
                                {
                                    "authorId": "3111334",
                                    "name": "V. Korthikanti"
                                },
                                {
                                    "authorId": "48991386",
                                    "name": "J. Casper"
                                },
                                {
                                    "authorId": "3448779",
                                    "name": "Sangkug Lym"
                                },
                                {
                                    "authorId": "20957879",
                                    "name": "Lawrence C. McAfee"
                                },
                                {
                                    "authorId": "47304697",
                                    "name": "M. Andersch"
                                },
                                {
                                    "authorId": "1911755",
                                    "name": "M. Shoeybi"
                                },
                                {
                                    "authorId": "2301680",
                                    "name": "Bryan Catanzaro"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Machine Learning and Systems",
                            "n_citations": 275
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.22999199999999997
    }
}