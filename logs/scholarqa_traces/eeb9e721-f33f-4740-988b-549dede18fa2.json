{
    "query": "What are the technical implementation details of transferring 'dark knowledge' in knowledge distillation from teacher to student networks, including temperature scaling, soft label generation, and specific loss function formulations?",
    "user_id": "lib_user",
    "task_id": "eeb9e721-f33f-4740-988b-549dede18fa2",
    "timestamp": "2025-06-24T00:12:10.188209",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 49,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.6061529999999999,
    "decomposed_query": {
        "rewritten_query": "Technical implementation details of transferring 'dark knowledge' in knowledge distillation from teacher to student networks, including temperature scaling, soft label generation, and specific loss function formulations.",
        "keyword_query": "dark knowledge knowledge distillation teacher student networks temperature scaling soft label generation loss function formulations",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010086,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Annealing Knowledge Distillation",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 39,
            "citation_count": 79,
            "influential_citation_count": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.eacl-main.212.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.07163, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "31036999",
                    "name": "A. Jafari"
                },
                {
                    "authorId": "1924511",
                    "name": "Mehdi Rezagholizadeh"
                },
                {
                    "authorId": "31705350",
                    "name": "Pranav Sharma"
                },
                {
                    "authorId": "38565890",
                    "name": "A. Ghodsi"
                }
            ],
            "abstract": "Significant memory and computational requirements of large deep neural networks restricts their application on edge devices. Knowledge distillation (KD) is a prominent model compression technique for deep neural networks in which the knowledge of a trained large teacher model is transferred to a smaller student model. The success of knowledge distillation is mainly attributed to its training objective function, which exploits the soft-target information (also known as \u201cdark knowledge\u201d) besides the given regular hard labels in a training set. However, it is shown in the literature that the larger the gap between the teacher and the student networks, the more difficult is their training using knowledge distillation. To address this shortcoming, we propose an improved knowledge distillation method (called Annealing-KD) by feeding the rich information provided by teacher\u2019s soft-targets incrementally and more efficiently. Our Annealing-KD technique is based on a gradual transition over annealed soft-targets generated by the teacher at different temperatures in an iterative process; and therefore, the student is trained to follow the annealed teacher output in a step-by-step manner. This paper includes theoretical and empirical evidence as well as practical experiments to support the effectiveness of our Annealing-KD method. We did a comprehensive set of experiments on different tasks such as image classification (CIFAR-10 and 100) and NLP language inference with BERT-based models on the GLUE benchmark and consistently got superior results.",
            "corpus_id": 233189631,
            "sentences": [
                {
                    "corpus_id": "233189631",
                    "title": "Annealing Knowledge Distillation",
                    "text": "Significant memory and computational requirements of large deep neural networks restricts their application on edge devices. Knowledge distillation (KD) is a prominent model compression technique for deep neural networks in which the knowledge of a trained large teacher model is transferred to a smaller student model. The success of knowledge distillation is mainly attributed to its training objective function, which exploits the soft-target information (also known as \u201cdark knowledge\u201d) besides the given regular hard labels in a training set. However, it is shown in the literature that the larger the gap between the teacher and the student networks, the more difficult is their training using knowledge distillation. To address this shortcoming, we propose an improved knowledge distillation method (called Annealing-KD) by feeding the rich information provided by teacher\u2019s soft-targets incrementally and more efficiently. Our Annealing-KD technique is based on a gradual transition over annealed soft-targets generated by the teacher at different temperatures in an iterative process; and therefore, the student is trained to follow the annealed teacher output in a step-by-step manner. This paper includes theoretical and empirical evidence as well as practical experiments to support the effectiveness of our Annealing-KD method. We did a comprehensive set of experiments on different tasks such as image classification (CIFAR-10 and 100) and NLP language inference with BERT-based models on the GLUE benchmark and consistently got superior results.",
                    "score": 0.5887075868561957,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8955078125
                }
            ],
            "relevance_judgement": 0.8955078125,
            "relevance_judgment_input_expanded": "# Title: Annealing Knowledge Distillation\n# Venue: Conference of the European Chapter of the Association for Computational Linguistics\n# Authors: A. Jafari, Mehdi Rezagholizadeh, Pranav Sharma, A. Ghodsi\n## Abstract\nSignificant memory and computational requirements of large deep neural networks restricts their application on edge devices. Knowledge distillation (KD) is a prominent model compression technique for deep neural networks in which the knowledge of a trained large teacher model is transferred to a smaller student model. The success of knowledge distillation is mainly attributed to its training objective function, which exploits the soft-target information (also known as \u201cdark knowledge\u201d) besides the given regular hard labels in a training set. However, it is shown in the literature that the larger the gap between the teacher and the student networks, the more difficult is their training using knowledge distillation. To address this shortcoming, we propose an improved knowledge distillation method (called Annealing-KD) by feeding the rich information provided by teacher\u2019s soft-targets incrementally and more efficiently. Our Annealing-KD technique is based on a gradual transition over annealed soft-targets generated by the teacher at different temperatures in an iterative process; and therefore, the student is trained to follow the annealed teacher output in a step-by-step manner. This paper includes theoretical and empirical evidence as well as practical experiments to support the effectiveness of our Annealing-KD method. We did a comprehensive set of experiments on different tasks such as image classification (CIFAR-10 and 100) and NLP language inference with BERT-based models on the GLUE benchmark and consistently got superior results.\n",
            "reference_string": "[233189631 | Jafari et al. | 2021 | Citations: 79]"
        },
        {
            "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 42,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2302.08038",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.08038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135847074",
                    "name": "Xiongtao Zhang"
                },
                {
                    "authorId": "2206403890",
                    "name": "Zezong Yin"
                },
                {
                    "authorId": "3247526",
                    "name": "Yunliang Jiang"
                },
                {
                    "authorId": "1390650781",
                    "name": "Yizhang Jiang"
                },
                {
                    "authorId": "2340356",
                    "name": "Da-Song Sun"
                },
                {
                    "authorId": "2189281",
                    "name": "Yong Liu"
                }
            ],
            "abstract": "High-order Takagi-Sugeno-Kang (TSK) fuzzy classifiers possess powerful classification performance yet have fewer fuzzy rules, but always be impaired by its exponential growth training time and poorer interpretability owing to High-order polynomial used in consequent part of fuzzy rule, while Low-order TSK fuzzy classifiers run quickly with high interpretability, however they usually require more fuzzy rules and perform relatively not very well. Address this issue, a novel TSK fuzzy classifier embeded with knowledge distillation in deep learning called HTSK-LLM-DKD is proposed in this study. HTSK-LLM-DKD achieves the following distinctive characteristics: 1) It takes High-order TSK classifier as teacher model and Low-order TSK fuzzy classifier as student model, and leverages the proposed LLM-DKD (Least Learning Machine based Decoupling Knowledge Distillation) to distill the fuzzy dark knowledge from High-order TSK fuzzy classifier to Low-order TSK fuzzy classifier, which resulting in Low-order TSK fuzzy classifier endowed with enhanced performance surpassing or at least comparable to High-order TSK classifier, as well as high interpretability; specifically 2) The Negative Euclidean distance between the output of teacher model and each class is employed to obtain the teacher logits, and then it compute teacher/student soft labels by the softmax function with distillating temperature parameter; 3) By reformulating the Kullback-Leibler divergence, it decouples fuzzy dark knowledge into target class knowledge and non-target class knowledge, and transfers them to student model. The advantages of HTSK-LLM-DKD are verified on the benchmarking UCI datasets and a real dataset Cleveland heart disease, in terms of classification performance and model interpretability.",
            "corpus_id": 256900863,
            "sentences": [
                {
                    "corpus_id": "256900863",
                    "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK",
                    "text": "Knowledge distillation [14] transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence [14], the mean squared error [37] and the Jensen-Shannon divergence [38], etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. [32] pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence. Furlanello et al. [33] demonstrated that the decoupled dark knowledge of teacher model can guide student model to have stronger generalization ability than that of teacher model. In this paper, we attempt to distill fuzzy dark knowledge from High-order TSK fuzzy classifier, and propose a novel born-again TSK fuzzy classifier endowed with the powerful classification performance as well as high interpretability.",
                    "score": 0.7568996490814457,
                    "section_title": "B. Knowledge Distillation",
                    "char_start_offset": 8754,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 209
                        },
                        {
                            "start": 210,
                            "end": 617
                        },
                        {
                            "start": 620,
                            "end": 946
                        },
                        {
                            "start": 947,
                            "end": 1169
                        },
                        {
                            "start": 1170,
                            "end": 1348
                        },
                        {
                            "start": 1349,
                            "end": 1583
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 23,
                            "end": 27,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 730,
                            "end": 734,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 759,
                            "end": 763,
                            "matchedPaperCorpusId": "233033712"
                        },
                        {
                            "start": 798,
                            "end": 802,
                            "matchedPaperCorpusId": "209405263"
                        },
                        {
                            "start": 959,
                            "end": 963,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 1188,
                            "end": 1192,
                            "matchedPaperCorpusId": "4110009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88525390625
                }
            ],
            "relevance_judgement": 0.88525390625,
            "relevance_judgment_input_expanded": "# Title: Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK\n# Venue: arXiv.org\n# Authors: Xiongtao Zhang, Zezong Yin, Yunliang Jiang, Yizhang Jiang, Da-Song Sun, Yong Liu\n## Abstract\nHigh-order Takagi-Sugeno-Kang (TSK) fuzzy classifiers possess powerful classification performance yet have fewer fuzzy rules, but always be impaired by its exponential growth training time and poorer interpretability owing to High-order polynomial used in consequent part of fuzzy rule, while Low-order TSK fuzzy classifiers run quickly with high interpretability, however they usually require more fuzzy rules and perform relatively not very well. Address this issue, a novel TSK fuzzy classifier embeded with knowledge distillation in deep learning called HTSK-LLM-DKD is proposed in this study. HTSK-LLM-DKD achieves the following distinctive characteristics: 1) It takes High-order TSK classifier as teacher model and Low-order TSK fuzzy classifier as student model, and leverages the proposed LLM-DKD (Least Learning Machine based Decoupling Knowledge Distillation) to distill the fuzzy dark knowledge from High-order TSK fuzzy classifier to Low-order TSK fuzzy classifier, which resulting in Low-order TSK fuzzy classifier endowed with enhanced performance surpassing or at least comparable to High-order TSK classifier, as well as high interpretability; specifically 2) The Negative Euclidean distance between the output of teacher model and each class is employed to obtain the teacher logits, and then it compute teacher/student soft labels by the softmax function with distillating temperature parameter; 3) By reformulating the Kullback-Leibler divergence, it decouples fuzzy dark knowledge into target class knowledge and non-target class knowledge, and transfers them to student model. The advantages of HTSK-LLM-DKD are verified on the benchmarking UCI datasets and a real dataset Cleveland heart disease, in terms of classification performance and model interpretability.\n## B. Knowledge Distillation\nKnowledge distillation [14] transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence [14], the mean squared error [37] and the Jensen-Shannon divergence [38], etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. [32] pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence. Furlanello et al. [33] demonstrated that the decoupled dark knowledge of teacher model can guide student model to have stronger generalization ability than that of teacher model. In this paper, we attempt to distill fuzzy dark knowledge from High-order TSK fuzzy classifier, and propose a novel born-again TSK fuzzy classifier endowed with the powerful classification performance as well as high interpretability.",
            "reference_string": "[256900863 | Zhang et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
            "venue": "Fire",
            "year": 2023,
            "reference_count": 48,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2571-6255/6/12/446/pdf?version=1700635738",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/fire6120446?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/fire6120446, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268047558",
                    "name": "Jinzhou Xie"
                },
                {
                    "authorId": "2268031861",
                    "name": "Hongmin Zhao"
                }
            ],
            "abstract": "This paper investigates the application of the YOLOv7 object detection model combined with knowledge distillation techniques in forest fire detection. As an advanced object detection model, YOLOv7 boasts efficient real-time detection capabilities. However, its performance may be constrained in resource-limited environments. To address this challenge, this research proposes a novel approach: considering that deep neural networks undergo multi-layer mapping from the input to the output space, we define the knowledge propagation between layers by evaluating the dot product of features extracted from two different layers. To this end, we utilize the Flow of Solution Procedure (FSP) matrix based on the Gram matrix and redesign the distillation loss using the Pearson correlation coefficient, presenting a new knowledge distillation method termed ILKDG (Intermediate Layer Knowledge Distillation with Gram Matrix-based Feature Flow). Compared with the classical knowledge distillation algorithm, KD, ILKDG achieved a significant performance improvement on a self-created forest fire detection dataset. Specifically, without altering the student network\u2019s parameters or network layers, mAP@0.5 improved by 2.9%, and mAP@0.5:0.95 increased by 2.7%. These results indicate that the proposed ILKDG method effectively enhances the accuracy and performance of forest fire detection without introducing additional parameters. The ILKDG method, based on the Gram matrix and Pearson correlation coefficient, presents a novel knowledge distillation approach, providing a fresh avenue for future research. Researchers can further optimize and refine this method to achieve superior results in fire detection.",
            "corpus_id": 265384964,
            "sentences": [
                {
                    "corpus_id": "265384964",
                    "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
                    "text": "Knowledge distillation [34] involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements. In the process of knowledge distillation, logits are used as a basis for comparing the outputs of the student model with those of the teacher model. By examining logits, the model can measure the certainty or confidence of its predictions before applying the softmax function to obtain the final probability. The standard cross-entropy loss depends on the predicted probabilities and ground truth labels. Additionally, the loss function is extended to include the standard cross-entropy loss between the predictions of the student model and the ground truth labels, as well as an additional loss term that measures the discrepancy between the softened probabilities (obtained through a higher temperature softmax) of the teacher model's predictions and the corresponding predictions of the student model. This additional term ensures that the student model not only learns to predict the correct labels but also aims to replicate the softened outputs of the teacher model, effectively transferring its knowledge to the student model. By jointly minimizing these two loss terms, the student model can learn to generalize better and imitate the behavior of the more complex teacher model. The calculation of the probability for the class is as follows: \n\nHere, T represents the \"temperature\" of knowledge distillation. When T = 1, it corresponds to a normalized exponential function. With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\"",
                    "score": 0.8160733512936827,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 9617,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 510
                        },
                        {
                            "start": 511,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 915
                        },
                        {
                            "start": 916,
                            "end": 1315
                        },
                        {
                            "start": 1316,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1697
                        },
                        {
                            "start": 1698,
                            "end": 1761
                        },
                        {
                            "start": 1764,
                            "end": 1827
                        },
                        {
                            "start": 1828,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 2131
                        },
                        {
                            "start": 2132,
                            "end": 2255
                        },
                        {
                            "start": 2256,
                            "end": 2403
                        },
                        {
                            "start": 2404,
                            "end": 2463
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 23,
                            "end": 27,
                            "matchedPaperCorpusId": "11253972"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88232421875
                }
            ],
            "relevance_judgement": 0.88232421875,
            "relevance_judgment_input_expanded": "# Title: Forest Fire Object Detection Analysis Based on Knowledge Distillation\n# Venue: Fire\n# Authors: Jinzhou Xie, Hongmin Zhao\n## Abstract\nThis paper investigates the application of the YOLOv7 object detection model combined with knowledge distillation techniques in forest fire detection. As an advanced object detection model, YOLOv7 boasts efficient real-time detection capabilities. However, its performance may be constrained in resource-limited environments. To address this challenge, this research proposes a novel approach: considering that deep neural networks undergo multi-layer mapping from the input to the output space, we define the knowledge propagation between layers by evaluating the dot product of features extracted from two different layers. To this end, we utilize the Flow of Solution Procedure (FSP) matrix based on the Gram matrix and redesign the distillation loss using the Pearson correlation coefficient, presenting a new knowledge distillation method termed ILKDG (Intermediate Layer Knowledge Distillation with Gram Matrix-based Feature Flow). Compared with the classical knowledge distillation algorithm, KD, ILKDG achieved a significant performance improvement on a self-created forest fire detection dataset. Specifically, without altering the student network\u2019s parameters or network layers, mAP@0.5 improved by 2.9%, and mAP@0.5:0.95 increased by 2.7%. These results indicate that the proposed ILKDG method effectively enhances the accuracy and performance of forest fire detection without introducing additional parameters. The ILKDG method, based on the Gram matrix and Pearson correlation coefficient, presents a novel knowledge distillation approach, providing a fresh avenue for future research. Researchers can further optimize and refine this method to achieve superior results in fire detection.\n## Knowledge Distillation\nKnowledge distillation [34] involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements. In the process of knowledge distillation, logits are used as a basis for comparing the outputs of the student model with those of the teacher model. By examining logits, the model can measure the certainty or confidence of its predictions before applying the softmax function to obtain the final probability. The standard cross-entropy loss depends on the predicted probabilities and ground truth labels. Additionally, the loss function is extended to include the standard cross-entropy loss between the predictions of the student model and the ground truth labels, as well as an additional loss term that measures the discrepancy between the softened probabilities (obtained through a higher temperature softmax) of the teacher model's predictions and the corresponding predictions of the student model. This additional term ensures that the student model not only learns to predict the correct labels but also aims to replicate the softened outputs of the teacher model, effectively transferring its knowledge to the student model. By jointly minimizing these two loss terms, the student model can learn to generalize better and imitate the behavior of the more complex teacher model. The calculation of the probability for the class is as follows: \n\nHere, T represents the \"temperature\" of knowledge distillation. When T = 1, it corresponds to a normalized exponential function. With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\"",
            "reference_string": "[265384964 | Xie et al. | 2023 | Citations: 6]"
        },
        {
            "title": "Efficient One Pass Self-distillation with Zipf's Label Smoothing",
            "venue": "European Conference on Computer Vision",
            "year": 2022,
            "reference_count": 36,
            "citation_count": 19,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2207.12980",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.12980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1387852255",
                    "name": "Jiajun Liang"
                },
                {
                    "authorId": "2304362074",
                    "name": "Linze Li"
                },
                {
                    "authorId": "2056410266",
                    "name": "Z. Bing"
                },
                {
                    "authorId": "2112526021",
                    "name": "Borui Zhao"
                },
                {
                    "authorId": "2179286812",
                    "name": "Yao Tang"
                },
                {
                    "authorId": "2179192518",
                    "name": "Bo Lin"
                },
                {
                    "authorId": "1934546",
                    "name": "Haoqiang Fan"
                }
            ],
            "abstract": "Self-distillation exploits non-uniform soft supervision from itself during training and improves performance without any runtime cost. However, the overhead during training is often overlooked, and yet reducing time and memory overhead during training is increasingly important in the giant models' era. This paper proposes an efficient self-distillation method named Zipf's Label Smoothing (Zipf's LS), which uses the on-the-fly prediction of a network to generate soft supervision that conforms to Zipf distribution without using any contrastive samples or auxiliary parameters. Our idea comes from an empirical observation that when the network is duly trained the output values of a network's final softmax layer, after sorting by the magnitude and averaged across samples, should follow a distribution reminiscent to Zipf's Law in the word frequency statistics of natural languages. By enforcing this property on the sample level and throughout the whole training period, we find that the prediction accuracy can be greatly improved. Using ResNet50 on the INAT21 fine-grained classification dataset, our technique achieves +3.61% accuracy gain compared to the vanilla baseline, and 0.88% more gain against the previous label smoothing or self-distillation strategies. The implementation is publicly available at https://github.com/megvii-research/zipfls.",
            "corpus_id": 251066725,
            "sentences": [
                {
                    "corpus_id": "251066725",
                    "title": "Efficient One Pass Self-distillation with Zipf's Label Smoothing",
                    "text": "Instead of imposing a fixed prior distribution, knowledge distillation was first proposed by Hinton in [12] to provide sample-level non-uniform soft labels. They demonstrated that the \"dark knowledge\" lies in the output distributions from a large capacity teacher network and benefits the student's representation learning. Recent works mainly explored to better transfer the \"dark knowledge\" and improve the efficiency from various aspects, such as reducing the difference between the teacher and student [3,5,18,34], designing student-friendly architecture [16,20], improving the distillation efficiency [7,14,27,29] and explaining the distillation's working mechanism [1,23]. \n\nIn this work, we focus on how to transfer the \"dark knowledge\" in an almost free manner. Furlanello et al. [7] proposed to improve the performance of the student network by distilling a teacher network with the same architecture. However, it is still a two-stage approach, which first trains the teacher and then distills knowledge to the student. To reduce the training time, many selfdistillation methods were proposed. They gain soft label supervision on the fly without the pretraining step.",
                    "score": 0.9397752214692279,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 5454,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 156
                        },
                        {
                            "start": 157,
                            "end": 323
                        },
                        {
                            "start": 324,
                            "end": 678
                        },
                        {
                            "start": 681,
                            "end": 769
                        },
                        {
                            "start": 770,
                            "end": 910
                        },
                        {
                            "start": 911,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1102
                        },
                        {
                            "start": 1103,
                            "end": 1176
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 509,
                            "end": 511,
                            "matchedPaperCorpusId": "203642130"
                        },
                        {
                            "start": 511,
                            "end": 514,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 514,
                            "end": 517,
                            "matchedPaperCorpusId": "244680427"
                        },
                        {
                            "start": 563,
                            "end": 566,
                            "matchedPaperCorpusId": "231925118"
                        },
                        {
                            "start": 606,
                            "end": 609,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 609,
                            "end": 612,
                            "matchedPaperCorpusId": "233714221"
                        },
                        {
                            "start": 612,
                            "end": 615,
                            "matchedPaperCorpusId": "70335318"
                        },
                        {
                            "start": 615,
                            "end": 618,
                            "matchedPaperCorpusId": "214727822"
                        },
                        {
                            "start": 674,
                            "end": 677,
                            "matchedPaperCorpusId": "235390933"
                        },
                        {
                            "start": 788,
                            "end": 791,
                            "matchedPaperCorpusId": "4110009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.869140625
                }
            ],
            "relevance_judgement": 0.869140625,
            "relevance_judgment_input_expanded": "# Title: Efficient One Pass Self-distillation with Zipf's Label Smoothing\n# Venue: European Conference on Computer Vision\n# Authors: Jiajun Liang, Linze Li, Z. Bing, Borui Zhao, Yao Tang, Bo Lin, Haoqiang Fan\n## Abstract\nSelf-distillation exploits non-uniform soft supervision from itself during training and improves performance without any runtime cost. However, the overhead during training is often overlooked, and yet reducing time and memory overhead during training is increasingly important in the giant models' era. This paper proposes an efficient self-distillation method named Zipf's Label Smoothing (Zipf's LS), which uses the on-the-fly prediction of a network to generate soft supervision that conforms to Zipf distribution without using any contrastive samples or auxiliary parameters. Our idea comes from an empirical observation that when the network is duly trained the output values of a network's final softmax layer, after sorting by the magnitude and averaged across samples, should follow a distribution reminiscent to Zipf's Law in the word frequency statistics of natural languages. By enforcing this property on the sample level and throughout the whole training period, we find that the prediction accuracy can be greatly improved. Using ResNet50 on the INAT21 fine-grained classification dataset, our technique achieves +3.61% accuracy gain compared to the vanilla baseline, and 0.88% more gain against the previous label smoothing or self-distillation strategies. The implementation is publicly available at https://github.com/megvii-research/zipfls.\n## Knowledge Distillation\nInstead of imposing a fixed prior distribution, knowledge distillation was first proposed by Hinton in [12] to provide sample-level non-uniform soft labels. They demonstrated that the \"dark knowledge\" lies in the output distributions from a large capacity teacher network and benefits the student's representation learning. Recent works mainly explored to better transfer the \"dark knowledge\" and improve the efficiency from various aspects, such as reducing the difference between the teacher and student [3,5,18,34], designing student-friendly architecture [16,20], improving the distillation efficiency [7,14,27,29] and explaining the distillation's working mechanism [1,23]. \n\nIn this work, we focus on how to transfer the \"dark knowledge\" in an almost free manner. Furlanello et al. [7] proposed to improve the performance of the student network by distilling a teacher network with the same architecture. However, it is still a two-stage approach, which first trains the teacher and then distills knowledge to the student. To reduce the training time, many selfdistillation methods were proposed. They gain soft label supervision on the fly without the pretraining step.",
            "reference_string": "[251066725 | Liang et al. | 2022 | Citations: 19]"
        },
        {
            "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2020,
            "reference_count": 35,
            "citation_count": 35,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/6509/6365",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/AAAI.V34I05.6509?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/AAAI.V34I05.6509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47295143",
                    "name": "Zhenyu Zhang"
                },
                {
                    "authorId": "2269366",
                    "name": "Xiaobo Shu"
                },
                {
                    "authorId": "48613402",
                    "name": "Yu Bowen"
                },
                {
                    "authorId": "2079682",
                    "name": "Tingwen Liu"
                },
                {
                    "authorId": "48019474",
                    "name": "Jiapeng Zhao"
                },
                {
                    "authorId": "2108645146",
                    "name": "Quangang Li"
                },
                {
                    "authorId": "48358041",
                    "name": "Li Guo"
                }
            ],
            "abstract": "Extracting relations from plain text is an important task with wide application. Most existing methods formulate it as a supervised problem and utilize one-hot hard labels as the sole target in training, neglecting the rich semantic information among relations. In this paper, we aim to explore the supervision with soft labels in relation extraction, which makes it possible to integrate prior knowledge. Specifically, a bipartite graph is first devised to discover type constraints between entities and relations based on the entire corpus. Then, we combine such type constraints with neural networks to achieve a knowledgeable model. Furthermore, this model is regarded as teacher to generate well-informed soft labels and guide the optimization of a student network via knowledge distillation. Besides, a multi-aspect attention mechanism is introduced to help student mine latent information from text. In this way, the enhanced student inherits the dark knowledge (e.g., type constraints and relevance among relations) from teacher, and directly serves the testing scenarios without any extra constraints. We conduct extensive experiments on the TACRED and SemEval datasets, the experimental results justify the effectiveness of our approach.",
            "corpus_id": 212855595,
            "sentences": [
                {
                    "corpus_id": "212855595",
                    "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
                    "text": "This subsection illustrates the objective functions of teacher and student networks, we introduce two additional loss functions to help our model efficiently transfer dark knowledge (i.e., the global type constraints and the relevance among relations) from teacher to student. \n\nThe Teacher Network In vanilla knowledge distillation, the teacher network is trained to fit one-hot labels. However, the ultimate goal of teacher is to provide better guidances for student, rather than achieve high accuracy simply. Inspired by Yang et al. (2019), we introduce a top score difference (TSD) loss to make the teacher's distribution softer. More concretely, we first pick up K classes with the highest confidence scores from the teacher's output, and then compute the gap between the confidence scores of the primary class and other K-1 classes: \n\nwhere \u03c1 k refers to the value of k-th largest element in the output distribution of teacher. Based on the global statistics, K is set to 3 empirically. We add the penalty term to standard ground-truth loss L T GT when training the teacher, facilitating it to distribute confidence to a few secondary relations: \n\nThe Student Network Typically, knowledge distillation transfers dark knowledge from the final output of teacher. Chen et al. (2017) demonstrate that using the intermediate representation of teacher as hint can stabilize the training process and improve the final performance of student. Here, we utilize the Kullback-Leibler divergence to measure the differences of corresponding branches between the teacher and student networks as hint learning loss: \n\nwhere L T L and L T G are the local and global logits of the teacher network respectively, L S L and L S A are the local and auxiliary logits of the student network respectively. Intuitively, it encourages the results of MAA to be similar with the scaled global logits of the teacher network. The loss of knowledge distillation is calculated as the sum of L HT and L KD with a weight factor \u03bb ht . As a result, the updated loss of student network is defined as follows:",
                    "score": 0.7444051655534826,
                    "section_title": "Objective Functions",
                    "char_start_offset": 15526,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 276
                        },
                        {
                            "start": 279,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 511
                        },
                        {
                            "start": 512,
                            "end": 633
                        },
                        {
                            "start": 634,
                            "end": 838
                        },
                        {
                            "start": 841,
                            "end": 933
                        },
                        {
                            "start": 934,
                            "end": 992
                        },
                        {
                            "start": 993,
                            "end": 1151
                        },
                        {
                            "start": 1154,
                            "end": 1266
                        },
                        {
                            "start": 1267,
                            "end": 1440
                        },
                        {
                            "start": 1441,
                            "end": 1606
                        },
                        {
                            "start": 1609,
                            "end": 1787
                        },
                        {
                            "start": 1788,
                            "end": 1901
                        },
                        {
                            "start": 1902,
                            "end": 2006
                        },
                        {
                            "start": 2007,
                            "end": 2078
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 524,
                            "end": 542,
                            "matchedPaperCorpusId": "54986302"
                        },
                        {
                            "start": 1267,
                            "end": 1285,
                            "matchedPaperCorpusId": "29308926"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.869140625
                }
            ],
            "relevance_judgement": 0.869140625,
            "relevance_judgment_input_expanded": "# Title: Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Zhenyu Zhang, Xiaobo Shu, Yu Bowen, Tingwen Liu, Jiapeng Zhao, Quangang Li, Li Guo\n## Abstract\nExtracting relations from plain text is an important task with wide application. Most existing methods formulate it as a supervised problem and utilize one-hot hard labels as the sole target in training, neglecting the rich semantic information among relations. In this paper, we aim to explore the supervision with soft labels in relation extraction, which makes it possible to integrate prior knowledge. Specifically, a bipartite graph is first devised to discover type constraints between entities and relations based on the entire corpus. Then, we combine such type constraints with neural networks to achieve a knowledgeable model. Furthermore, this model is regarded as teacher to generate well-informed soft labels and guide the optimization of a student network via knowledge distillation. Besides, a multi-aspect attention mechanism is introduced to help student mine latent information from text. In this way, the enhanced student inherits the dark knowledge (e.g., type constraints and relevance among relations) from teacher, and directly serves the testing scenarios without any extra constraints. We conduct extensive experiments on the TACRED and SemEval datasets, the experimental results justify the effectiveness of our approach.\n## Objective Functions\nThis subsection illustrates the objective functions of teacher and student networks, we introduce two additional loss functions to help our model efficiently transfer dark knowledge (i.e., the global type constraints and the relevance among relations) from teacher to student. \n\nThe Teacher Network In vanilla knowledge distillation, the teacher network is trained to fit one-hot labels. However, the ultimate goal of teacher is to provide better guidances for student, rather than achieve high accuracy simply. Inspired by Yang et al. (2019), we introduce a top score difference (TSD) loss to make the teacher's distribution softer. More concretely, we first pick up K classes with the highest confidence scores from the teacher's output, and then compute the gap between the confidence scores of the primary class and other K-1 classes: \n\nwhere \u03c1 k refers to the value of k-th largest element in the output distribution of teacher. Based on the global statistics, K is set to 3 empirically. We add the penalty term to standard ground-truth loss L T GT when training the teacher, facilitating it to distribute confidence to a few secondary relations: \n\nThe Student Network Typically, knowledge distillation transfers dark knowledge from the final output of teacher. Chen et al. (2017) demonstrate that using the intermediate representation of teacher as hint can stabilize the training process and improve the final performance of student. Here, we utilize the Kullback-Leibler divergence to measure the differences of corresponding branches between the teacher and student networks as hint learning loss: \n\nwhere L T L and L T G are the local and global logits of the teacher network respectively, L S L and L S A are the local and auxiliary logits of the student network respectively. Intuitively, it encourages the results of MAA to be similar with the scaled global logits of the teacher network. The loss of knowledge distillation is calculated as the sum of L HT and L KD with a weight factor \u03bb ht . As a result, the updated loss of student network is defined as follows:",
            "reference_string": "[212855595 | Zhang et al. | 2020 | Citations: 35]"
        },
        {
            "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback",
            "venue": "IEEE Transactions on Communications",
            "year": 2022,
            "reference_count": 63,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.17113",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.17113, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2172485573",
                    "name": "Yiming Cui"
                },
                {
                    "authorId": "47093519",
                    "name": "Jiajia Guo"
                },
                {
                    "authorId": "2113999930",
                    "name": "Zheng Cao"
                },
                {
                    "authorId": "120710335",
                    "name": "Huaze Tang"
                },
                {
                    "authorId": "2257212132",
                    "name": "Chao-Kai Wen"
                },
                {
                    "authorId": "2227268421",
                    "name": "Shi Jin"
                },
                {
                    "authorId": "2288090155",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2240356164",
                    "name": "Xiaolin Hou"
                }
            ],
            "abstract": "Deep learning has shown promise in enhancing channel state information (CSI) feedback. However, many studies indicate that better feedback performance often accompanies higher computational complexity. Pursuing better performance-complexity tradeoffs is crucial to facilitate practical deployment, especially on computation-limited devices, which may have to use lightweight autoencoder with unfavorable performance. To achieve this goal, this paper introduces knowledge distillation (KD) to achieve better tradeoffs, where knowledge from a complicated teacher autoencoder is transferred to a lightweight student autoencoder for performance improvement. Specifically, two methods are proposed for implementation. Firstly, an autoencoder KD-based method is introduced by training a student autoencoder to mimic the reconstructed CSI of a pretrained teacher autoencoder. Secondly, an encoder KD-based method is proposed to reduce training overhead by performing KD only on the student encoder. Additionally, a variant of encoder KD is introduced to protect user equipment and base station vendor intellectual property. Numerical simulations demonstrate that the proposed methods can significantly improve the student autoencoder\u2019s performance, while reducing the number of floating point operations and inference time to 3.05%\u20135.28% and 13.80%\u201314.76% of the teacher network, respectively. Furthermore, the variant encoder KD method effectively enhances the student autoencoder\u2019s generalization capability across different scenarios, environments, and bandwidths.",
            "corpus_id": 260447668,
            "sentences": [
                {
                    "corpus_id": "260447668",
                    "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback",
                    "text": "In this case, the form of knowledge is converted to a simpler one while the knowledge itself is kept the same. This approach can help avoid using cumbersome networks in actual deployment. \n\nCompared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part. \n\n3) Loss Function for KD. The proposed KD-based neural network lightweight method is depicted in Fig. 3. After the teacher network is trained, the student network is trained using a combination of distillation loss and ordinary MSE loss.",
                    "score": 0.6649727084486374,
                    "section_title": "B. Key Idea and Training Framework",
                    "char_start_offset": 18765,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 110
                        },
                        {
                            "start": 111,
                            "end": 187
                        },
                        {
                            "start": 190,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 462
                        },
                        {
                            "start": 463,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 897
                        },
                        {
                            "start": 898,
                            "end": 1079
                        },
                        {
                            "start": 1080,
                            "end": 1205
                        },
                        {
                            "start": 1208,
                            "end": 1382
                        },
                        {
                            "start": 1383,
                            "end": 1468
                        },
                        {
                            "start": 1469,
                            "end": 1543
                        },
                        {
                            "start": 1546,
                            "end": 1711
                        },
                        {
                            "start": 1712,
                            "end": 1912
                        },
                        {
                            "start": 1913,
                            "end": 2061
                        },
                        {
                            "start": 2062,
                            "end": 2175
                        },
                        {
                            "start": 2178,
                            "end": 2202
                        },
                        {
                            "start": 2203,
                            "end": 2281
                        },
                        {
                            "start": 2282,
                            "end": 2414
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83837890625
                }
            ],
            "relevance_judgement": 0.83837890625,
            "relevance_judgment_input_expanded": "# Title: Lightweight Neural Network With Knowledge Distillation for CSI Feedback\n# Venue: IEEE Transactions on Communications\n# Authors: Yiming Cui, Jiajia Guo, Zheng Cao, Huaze Tang, Chao-Kai Wen, Shi Jin, Xin Wang, Xiaolin Hou\n## Abstract\nDeep learning has shown promise in enhancing channel state information (CSI) feedback. However, many studies indicate that better feedback performance often accompanies higher computational complexity. Pursuing better performance-complexity tradeoffs is crucial to facilitate practical deployment, especially on computation-limited devices, which may have to use lightweight autoencoder with unfavorable performance. To achieve this goal, this paper introduces knowledge distillation (KD) to achieve better tradeoffs, where knowledge from a complicated teacher autoencoder is transferred to a lightweight student autoencoder for performance improvement. Specifically, two methods are proposed for implementation. Firstly, an autoencoder KD-based method is introduced by training a student autoencoder to mimic the reconstructed CSI of a pretrained teacher autoencoder. Secondly, an encoder KD-based method is proposed to reduce training overhead by performing KD only on the student encoder. Additionally, a variant of encoder KD is introduced to protect user equipment and base station vendor intellectual property. Numerical simulations demonstrate that the proposed methods can significantly improve the student autoencoder\u2019s performance, while reducing the number of floating point operations and inference time to 3.05%\u20135.28% and 13.80%\u201314.76% of the teacher network, respectively. Furthermore, the variant encoder KD method effectively enhances the student autoencoder\u2019s generalization capability across different scenarios, environments, and bandwidths.\n## B. Key Idea and Training Framework\nIn this case, the form of knowledge is converted to a simpler one while the knowledge itself is kept the same. This approach can help avoid using cumbersome networks in actual deployment. \n\nCompared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part. \n\n3) Loss Function for KD. The proposed KD-based neural network lightweight method is depicted in Fig. 3. After the teacher network is trained, the student network is trained using a combination of distillation loss and ordinary MSE loss.",
            "reference_string": "[260447668 | Cui et al. | 2022 | Citations: 3]"
        },
        {
            "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
            "venue": "2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 20,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2405.11704",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.11704, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2298916636",
                    "name": "Taiyuan Mei"
                },
                {
                    "authorId": "2298918720",
                    "name": "Yun Zi"
                },
                {
                    "authorId": "2222987403",
                    "name": "X. Cheng"
                },
                {
                    "authorId": "2297725659",
                    "name": "Zijun Gao"
                },
                {
                    "authorId": "2297735971",
                    "name": "Qi Wang"
                },
                {
                    "authorId": "2302372513",
                    "name": "Haowei Yang"
                }
            ],
            "abstract": "The internal structure and operation mechanism of large-scale language models are analyzed theoretically, especially how Transformer and its derivative architectures can restrict computing efficiency while capturing long-term dependencies. Further, we dig deep into the efficiency bottleneck of the training phase, and evaluate in detail the contribution of adaptive optimization algorithms (such as AdamW), massively parallel computing techniques, and mixed precision training strategies to accelerate convergence and reduce memory footprint. By analyzing the mathematical principles and implementation details of these algorithms, we reveal how they effectively improve training efficiency in practice. In terms of model deployment and inference optimization, this paper systematically reviews the latest advances in model compression techniques, focusing on strategies such as quantification, pruning, and knowledge distillation. By comparing the theoretical frameworks of these techniques and their effects in different application scenarios, we demonstrate their ability to significantly reduce model size and inference delay while maintaining model prediction accuracy. In addition, this paper critically examines the limitations of current efficiency optimization methods, such as the increased risk of overfitting, the control of performance loss after compression, and the problem of algorithm generality, and proposes some prospects for future research. In conclusion, this study provides a comprehensive theoretical framework for understanding the efficiency optimization of large-scale language models.",
            "corpus_id": 269921267,
            "sentences": [
                {
                    "corpus_id": "269921267",
                    "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
                    "text": "This process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.\n\nThe core idea of knowledge distillation is that while large teacher models may have high predictive accuracy due to the large number of parameters, these advantages are not always directly applicable to resource-constrained environments.Knowledge distillation therefore aims to extract the \"dark knowledge\" of the teacher model -not just the final predictions, but also its decision-making processes and uncertainty estimates for the input data -and then infuse this knowledge into a more concise student model:  The knowledge distillation technique adopts a typical teacher-student training framework, as shown in Figure 2. In this framework, the \"teacher\" model represents a highly complex deep neural network.In contrast, the Student model is designed to be more streamlined and aims to achieve a similar level of performance with fewer parameters.The process begins with the pre-training of the Teacher model, a step that allows it to accumulate broad and deep knowledge on the specified data set.Instead of directly replicating the weights of the \"teacher\" model, the subsequent knowledge transfer phase uses its output -particularly soft probability distributions (rather than hard classification labels).This process combines the rich experience of the \"teacher\" model with direct guidance from real-world data to form a dual oversight mechanism designed to efficiently extract and transfer deep learning capabilities, while reducing resource burdens through model compression techniques and enhancing the feasibility and flexibility of the model in practical applications.\n\nKnowledge distillation is generally categorized into two main types: output-based and feature-based distillation.In this approach, the student model acquires similar predictive abilities by assimilating the output probability distribution from the teacher model, essentially grasping and transforming high-level abstract concepts.This process is achieved by designing specialized distillation loss functions that ensure that the student model can capture and mimic the high-level transformation logic of the teacher model to the input information, thus maintaining consistent or close performance to the teacher model on the prediction task.\n\nFeature-based distillation emphasizes the assimilation of intermediate layer features from the teacher model.",
                    "score": 0.6426790735200186,
                    "section_title": "B. Knowledge distillation",
                    "char_start_offset": 14355,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 182,
                            "end": 419
                        },
                        {
                            "start": 419,
                            "end": 894
                        },
                        {
                            "start": 894,
                            "end": 1033
                        },
                        {
                            "start": 1033,
                            "end": 1183
                        },
                        {
                            "start": 1183,
                            "end": 1393
                        },
                        {
                            "start": 1393,
                            "end": 1762
                        },
                        {
                            "start": 1764,
                            "end": 1877
                        },
                        {
                            "start": 1877,
                            "end": 2405
                        },
                        {
                            "start": 2407,
                            "end": 2516
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8330078125
                }
            ],
            "relevance_judgement": 0.8330078125,
            "relevance_judgment_input_expanded": "# Title: Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks\n# Venue: 2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)\n# Authors: Taiyuan Mei, Yun Zi, X. Cheng, Zijun Gao, Qi Wang, Haowei Yang\n## Abstract\nThe internal structure and operation mechanism of large-scale language models are analyzed theoretically, especially how Transformer and its derivative architectures can restrict computing efficiency while capturing long-term dependencies. Further, we dig deep into the efficiency bottleneck of the training phase, and evaluate in detail the contribution of adaptive optimization algorithms (such as AdamW), massively parallel computing techniques, and mixed precision training strategies to accelerate convergence and reduce memory footprint. By analyzing the mathematical principles and implementation details of these algorithms, we reveal how they effectively improve training efficiency in practice. In terms of model deployment and inference optimization, this paper systematically reviews the latest advances in model compression techniques, focusing on strategies such as quantification, pruning, and knowledge distillation. By comparing the theoretical frameworks of these techniques and their effects in different application scenarios, we demonstrate their ability to significantly reduce model size and inference delay while maintaining model prediction accuracy. In addition, this paper critically examines the limitations of current efficiency optimization methods, such as the increased risk of overfitting, the control of performance loss after compression, and the problem of algorithm generality, and proposes some prospects for future research. In conclusion, this study provides a comprehensive theoretical framework for understanding the efficiency optimization of large-scale language models.\n## B. Knowledge distillation\nThis process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.\n\nThe core idea of knowledge distillation is that while large teacher models may have high predictive accuracy due to the large number of parameters, these advantages are not always directly applicable to resource-constrained environments.Knowledge distillation therefore aims to extract the \"dark knowledge\" of the teacher model -not just the final predictions, but also its decision-making processes and uncertainty estimates for the input data -and then infuse this knowledge into a more concise student model:  The knowledge distillation technique adopts a typical teacher-student training framework, as shown in Figure 2. In this framework, the \"teacher\" model represents a highly complex deep neural network.In contrast, the Student model is designed to be more streamlined and aims to achieve a similar level of performance with fewer parameters.The process begins with the pre-training of the Teacher model, a step that allows it to accumulate broad and deep knowledge on the specified data set.Instead of directly replicating the weights of the \"teacher\" model, the subsequent knowledge transfer phase uses its output -particularly soft probability distributions (rather than hard classification labels).This process combines the rich experience of the \"teacher\" model with direct guidance from real-world data to form a dual oversight mechanism designed to efficiently extract and transfer deep learning capabilities, while reducing resource burdens through model compression techniques and enhancing the feasibility and flexibility of the model in practical applications.\n\nKnowledge distillation is generally categorized into two main types: output-based and feature-based distillation.In this approach, the student model acquires similar predictive abilities by assimilating the output probability distribution from the teacher model, essentially grasping and transforming high-level abstract concepts.This process is achieved by designing specialized distillation loss functions that ensure that the student model can capture and mimic the high-level transformation logic of the teacher model to the input information, thus maintaining consistent or close performance to the teacher model on the prediction task.\n\nFeature-based distillation emphasizes the assimilation of intermediate layer features from the teacher model.",
            "reference_string": "[269921267 | Mei et al. | 2024 | Citations: 20]"
        },
        {
            "title": "On-Device Learning Systems for Edge Intelligence: A Software and Hardware Synergy Perspective",
            "venue": "IEEE Internet of Things Journal",
            "year": 2021,
            "reference_count": 123,
            "citation_count": 52,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://zenodo.org/records/5105970/files/09366901.pdf",
                "status": "GREEN",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JIOT.2021.3063147?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JIOT.2021.3063147, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1993679419",
                    "name": "Qihua Zhou"
                },
                {
                    "authorId": "2034348867",
                    "name": "Zhihao Qu"
                },
                {
                    "authorId": "144123438",
                    "name": "Song Guo"
                },
                {
                    "authorId": "2150444053",
                    "name": "Boyuan Luo"
                },
                {
                    "authorId": "8572939",
                    "name": "Jingcai Guo"
                },
                {
                    "authorId": "150358457",
                    "name": "Zhenda Xu"
                },
                {
                    "authorId": "1854500",
                    "name": "R. Akerkar"
                }
            ],
            "abstract": "Modern machine learning (ML) applications are often deployed in the cloud environment to exploit the computational power of clusters. However, this in-cloud computing scheme cannot satisfy the demands of emerging edge intelligence scenarios, including providing personalized models, protecting user privacy, adapting to real-time tasks, and saving resource cost. In order to conquer the limitations of conventional in-cloud computing, there comes the rise of on-device learning, which makes the end-to-end ML procedure totally on user devices, without unnecessary involvement of the cloud. In spite of the promising advantages of on-device learning, implementing a high-performance on-device learning system still faces with many severe challenges, such as insufficient user training data, backward propagation (BP) blocking, and limited peak processing speed. Observing the substantial improvement space in the implementation and acceleration of on-device learning systems, we intend to present a comprehensive analysis of the latest research progress and point out potential optimization directions from the system perspective. This survey presents a software and hardware synergy of on-device learning techniques, covering the scope of model-level neural network design, algorithm-level training optimization, and hardware-level instruction acceleration. We hope this survey could bring fruitful discussions and inspire the researchers to further promote the field of edge intelligence.",
            "corpus_id": 233777216,
            "sentences": [
                {
                    "corpus_id": "233777216",
                    "title": "On-Device Learning Systems for Edge Intelligence: A Software and Hardware Synergy Perspective",
                    "text": "However, the key difference is that knowledge distillation requires a pre-defined student model and needs to modify its loss function. \n\n1) Combination of Loss Functions: Although the basic idea is to extract knowledge from the teacher model, we cannot simply employ its vanilla loss function. As the teach model is well trained with high prediction accuracy, the output class corresponding to the ground-truth label dominates in the classification distribution, while other classes are approximately to zero after the softmax block. In this case, even employing the teacher's loss function cannot bring extra useful information to the student, over the vanilla loss function based on the groundtruth labels. An effective method to solve this problem is to employ the temperature-based softmax function [82] to adjust the original logits from FC layers and balance the unscaled log probabilities of shape. The temperature-based softmax can be described as: \n\nwhere p i is the classification probability of each class i and z is corresponding logits inside the softmax function. Besides, T is the hyper-parameter called temperature to control the probability distribution of the output. Note that we will get the standard softmax function when T is 1 and the probability distribution will become softer when setting a higher value of T, so as to make the teacher model provide more effective information for the student model's learning. The key that a teacher can successfully transfer the knowledge to the student is to get the prediction experience from the teacher's loss function, which is called the dark knowledge [83]. The entire distillation procedure is actually based on the transferring of this dark knowledge, from the teacher to the student. Therefore, the core modification of the student's loss function (i.e., overall loss) can be formulated as: \n\nwhere x, y, w and \u03c3 represent the input data, ground-truth label, model parameters and the softmax block, respectively. Besides, z s and z d are the logits of the student and teacher, respectively. Note that the overall loss function L contains two parts: the student loss L s and distillation loss the L d , under the control of the coefficient hyper-parameters \u03b1 and \u03b2. We use Fig. 4 to better illustrate the workflow of knowledge distillation with these two kinds of loss functions.",
                    "score": 0.6658232738163063,
                    "section_title": "B. Knowledge Distillation",
                    "char_start_offset": 29410,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 137,
                            "end": 293
                        },
                        {
                            "start": 294,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 708
                        },
                        {
                            "start": 709,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 956
                        },
                        {
                            "start": 959,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1185
                        },
                        {
                            "start": 1186,
                            "end": 1436
                        },
                        {
                            "start": 1437,
                            "end": 1625
                        },
                        {
                            "start": 1626,
                            "end": 1754
                        },
                        {
                            "start": 1755,
                            "end": 1861
                        },
                        {
                            "start": 1864,
                            "end": 1983
                        },
                        {
                            "start": 1984,
                            "end": 2061
                        },
                        {
                            "start": 2062,
                            "end": 2235
                        },
                        {
                            "start": 2236,
                            "end": 2349
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1620,
                            "end": 1624,
                            "matchedPaperCorpusId": "198179476"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8310546875
                }
            ],
            "relevance_judgement": 0.8310546875,
            "relevance_judgment_input_expanded": "# Title: On-Device Learning Systems for Edge Intelligence: A Software and Hardware Synergy Perspective\n# Venue: IEEE Internet of Things Journal\n# Authors: Qihua Zhou, Zhihao Qu, Song Guo, Boyuan Luo, Jingcai Guo, Zhenda Xu, R. Akerkar\n## Abstract\nModern machine learning (ML) applications are often deployed in the cloud environment to exploit the computational power of clusters. However, this in-cloud computing scheme cannot satisfy the demands of emerging edge intelligence scenarios, including providing personalized models, protecting user privacy, adapting to real-time tasks, and saving resource cost. In order to conquer the limitations of conventional in-cloud computing, there comes the rise of on-device learning, which makes the end-to-end ML procedure totally on user devices, without unnecessary involvement of the cloud. In spite of the promising advantages of on-device learning, implementing a high-performance on-device learning system still faces with many severe challenges, such as insufficient user training data, backward propagation (BP) blocking, and limited peak processing speed. Observing the substantial improvement space in the implementation and acceleration of on-device learning systems, we intend to present a comprehensive analysis of the latest research progress and point out potential optimization directions from the system perspective. This survey presents a software and hardware synergy of on-device learning techniques, covering the scope of model-level neural network design, algorithm-level training optimization, and hardware-level instruction acceleration. We hope this survey could bring fruitful discussions and inspire the researchers to further promote the field of edge intelligence.\n## B. Knowledge Distillation\nHowever, the key difference is that knowledge distillation requires a pre-defined student model and needs to modify its loss function. \n\n1) Combination of Loss Functions: Although the basic idea is to extract knowledge from the teacher model, we cannot simply employ its vanilla loss function. As the teach model is well trained with high prediction accuracy, the output class corresponding to the ground-truth label dominates in the classification distribution, while other classes are approximately to zero after the softmax block. In this case, even employing the teacher's loss function cannot bring extra useful information to the student, over the vanilla loss function based on the groundtruth labels. An effective method to solve this problem is to employ the temperature-based softmax function [82] to adjust the original logits from FC layers and balance the unscaled log probabilities of shape. The temperature-based softmax can be described as: \n\nwhere p i is the classification probability of each class i and z is corresponding logits inside the softmax function. Besides, T is the hyper-parameter called temperature to control the probability distribution of the output. Note that we will get the standard softmax function when T is 1 and the probability distribution will become softer when setting a higher value of T, so as to make the teacher model provide more effective information for the student model's learning. The key that a teacher can successfully transfer the knowledge to the student is to get the prediction experience from the teacher's loss function, which is called the dark knowledge [83]. The entire distillation procedure is actually based on the transferring of this dark knowledge, from the teacher to the student. Therefore, the core modification of the student's loss function (i.e., overall loss) can be formulated as: \n\nwhere x, y, w and \u03c3 represent the input data, ground-truth label, model parameters and the softmax block, respectively. Besides, z s and z d are the logits of the student and teacher, respectively. Note that the overall loss function L contains two parts: the student loss L s and distillation loss the L d , under the control of the coefficient hyper-parameters \u03b1 and \u03b2. We use Fig. 4 to better illustrate the workflow of knowledge distillation with these two kinds of loss functions.",
            "reference_string": "[233777216 | Zhou et al. | 2021 | Citations: 52]"
        },
        {
            "title": "Decoupling Dark Knowledge via Block-Wise Logit Distillation for Feature-Level Alignment",
            "venue": "IEEE Transactions on Artificial Intelligence",
            "year": 2024,
            "reference_count": 67,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2411.01547",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.01547, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2163284473",
                    "name": "Chengting Yu"
                },
                {
                    "authorId": "2274194299",
                    "name": "Fengzhao Zhang"
                },
                {
                    "authorId": "2255346632",
                    "name": "Ruizhe Chen"
                },
                {
                    "authorId": "2311458018",
                    "name": "Zuozhu Liu"
                },
                {
                    "authorId": "2110408145",
                    "name": "Shurun Tan"
                },
                {
                    "authorId": "2326182494",
                    "name": "Erping Li"
                },
                {
                    "authorId": "2115787618",
                    "name": "Aili Wang"
                }
            ],
            "abstract": "Knowledge distillation (KD), a learning manner with a larger teacher network guiding a smaller student network, transfers dark knowledge from the teacher to the student via logits or intermediate features, with the aim of producing a well-performed lightweight model. Notably, many subsequent feature-based KD methods outperformed the earliest logit-based KD method and iteratively generated numerous state-of-the-art distillation methods. Nevertheless, recent work has uncovered the potential of the logit-based method, bringing the simple KD form based on logits back into the limelight. Features or logits? They partially implement the KD with entirely distinct perspectives; therefore, choosing between logits and features is not straightforward. This article provides a unified perspective of feature alignment to obtain a better comprehension of their fundamental distinction. Inheriting the design philosophy and insights of feature-based and logit-based methods, we introduce a block-wise logit distillation framework to apply implicit logit-based feature alignment by gradually replacing teacher's blocks as intermediate stepping-stone models to bridge the gap between the student and the teacher. Our method obtains comparable or superior results to state-of-the-art distillation methods. This article demonstrates the great potential of combining logit and features, and we hope it will inspire future research to revisit KD from a higher vantage point.",
            "corpus_id": 273811396,
            "sentences": [
                {
                    "corpus_id": "273811396",
                    "title": "Decoupling Dark Knowledge via Block-Wise Logit Distillation for Feature-Level Alignment",
                    "text": "The concept of knowledge distillation [16] was first proposed as a learning strategy that employs a larger teacher network to steer the training process of a smaller student network for various tasks [27,28]. The working mechanism of vanilla KD is often comprehended via the process of \"teaching\" [51], whereby the transfer of \"dark knowledge\" occurs through the use of soft labels, known as logits, provided by the teacher to the student. Recent studies examine how to select, express, and transfer the \"dark knowledge\" more practically and effectively, which can be categorized into two main types: logit-and feature-based distillations. \n\nLogits Distillation. In the wake of the earliest KD method based on temperature-regulated distillation [16], previous logit-based methods have concentrated mainly on effective regularization and optimization methods. DML [64] proposes a mutual learning method to train students and teachers simultaneously. TAKD [34] proposes using intermediary \"teacher assistants\" to transmit knowledge in a step-by-step manner in order to narrow the performance disparity between teachers and students. Additionally, several works [6,39] focus on interpreting the principles underlying KD. Recently, DKD [65] introduces an improved logit-based objective by decoupling the classical KD loss, which re-explores the potential of logitbased methods with comparable performance gains. \n\nFeature Distillation. To further enhance knowledge distillation, feature distillation is proposed to perform alignments on intermediate features as well as logit outputs, which can directly transfer teacher representations [5,14,15,41] or the correlation [36,37,47,48] from the teacher to the student. Feature methods are more likely to obtain high performance with extensive information from the teacher; however, tight feature alignment frequently relies on prior empirical observation and meticulous adjustment of hyperparameters.",
                    "score": 0.6954746289714078,
                    "section_title": "II. RELATED WORK",
                    "char_start_offset": 5421,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 208
                        },
                        {
                            "start": 209,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 639
                        },
                        {
                            "start": 642,
                            "end": 662
                        },
                        {
                            "start": 663,
                            "end": 858
                        },
                        {
                            "start": 859,
                            "end": 948
                        },
                        {
                            "start": 949,
                            "end": 1130
                        },
                        {
                            "start": 1131,
                            "end": 1217
                        },
                        {
                            "start": 1218,
                            "end": 1407
                        },
                        {
                            "start": 1410,
                            "end": 1431
                        },
                        {
                            "start": 1432,
                            "end": 1711
                        },
                        {
                            "start": 1712,
                            "end": 1943
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 200,
                            "end": 204,
                            "matchedPaperCorpusId": "267213"
                        },
                        {
                            "start": 204,
                            "end": 207,
                            "matchedPaperCorpusId": "236912875"
                        },
                        {
                            "start": 297,
                            "end": 301,
                            "matchedPaperCorpusId": "215745611"
                        },
                        {
                            "start": 863,
                            "end": 867,
                            "matchedPaperCorpusId": "26071966"
                        },
                        {
                            "start": 954,
                            "end": 958,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 1159,
                            "end": 1162,
                            "matchedPaperCorpusId": "212633769"
                        },
                        {
                            "start": 1162,
                            "end": 1165,
                            "matchedPaperCorpusId": "174800711"
                        },
                        {
                            "start": 1232,
                            "end": 1236,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 1633,
                            "end": 1636,
                            "matchedPaperCorpusId": "233296935"
                        },
                        {
                            "start": 1636,
                            "end": 1639,
                            "matchedPaperCorpusId": "102483181"
                        },
                        {
                            "start": 1665,
                            "end": 1669,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 1669,
                            "end": 1672,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 1675,
                            "end": 1678,
                            "matchedPaperCorpusId": "198179476"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8291015625
                }
            ],
            "relevance_judgement": 0.8291015625,
            "relevance_judgment_input_expanded": "# Title: Decoupling Dark Knowledge via Block-Wise Logit Distillation for Feature-Level Alignment\n# Venue: IEEE Transactions on Artificial Intelligence\n# Authors: Chengting Yu, Fengzhao Zhang, Ruizhe Chen, Zuozhu Liu, Shurun Tan, Erping Li, Aili Wang\n## Abstract\nKnowledge distillation (KD), a learning manner with a larger teacher network guiding a smaller student network, transfers dark knowledge from the teacher to the student via logits or intermediate features, with the aim of producing a well-performed lightweight model. Notably, many subsequent feature-based KD methods outperformed the earliest logit-based KD method and iteratively generated numerous state-of-the-art distillation methods. Nevertheless, recent work has uncovered the potential of the logit-based method, bringing the simple KD form based on logits back into the limelight. Features or logits? They partially implement the KD with entirely distinct perspectives; therefore, choosing between logits and features is not straightforward. This article provides a unified perspective of feature alignment to obtain a better comprehension of their fundamental distinction. Inheriting the design philosophy and insights of feature-based and logit-based methods, we introduce a block-wise logit distillation framework to apply implicit logit-based feature alignment by gradually replacing teacher's blocks as intermediate stepping-stone models to bridge the gap between the student and the teacher. Our method obtains comparable or superior results to state-of-the-art distillation methods. This article demonstrates the great potential of combining logit and features, and we hope it will inspire future research to revisit KD from a higher vantage point.\n## II. RELATED WORK\nThe concept of knowledge distillation [16] was first proposed as a learning strategy that employs a larger teacher network to steer the training process of a smaller student network for various tasks [27,28]. The working mechanism of vanilla KD is often comprehended via the process of \"teaching\" [51], whereby the transfer of \"dark knowledge\" occurs through the use of soft labels, known as logits, provided by the teacher to the student. Recent studies examine how to select, express, and transfer the \"dark knowledge\" more practically and effectively, which can be categorized into two main types: logit-and feature-based distillations. \n\nLogits Distillation. In the wake of the earliest KD method based on temperature-regulated distillation [16], previous logit-based methods have concentrated mainly on effective regularization and optimization methods. DML [64] proposes a mutual learning method to train students and teachers simultaneously. TAKD [34] proposes using intermediary \"teacher assistants\" to transmit knowledge in a step-by-step manner in order to narrow the performance disparity between teachers and students. Additionally, several works [6,39] focus on interpreting the principles underlying KD. Recently, DKD [65] introduces an improved logit-based objective by decoupling the classical KD loss, which re-explores the potential of logitbased methods with comparable performance gains. \n\nFeature Distillation. To further enhance knowledge distillation, feature distillation is proposed to perform alignments on intermediate features as well as logit outputs, which can directly transfer teacher representations [5,14,15,41] or the correlation [36,37,47,48] from the teacher to the student. Feature methods are more likely to obtain high performance with extensive information from the teacher; however, tight feature alignment frequently relies on prior empirical observation and meticulous adjustment of hyperparameters.",
            "reference_string": "[273811396 | Yu et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Zero-Shot Knowledge Distillation in Deep Networks",
            "venue": "International Conference on Machine Learning",
            "year": 2019,
            "reference_count": 28,
            "citation_count": 245,
            "influential_citation_count": 28,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1905.08114, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "143747407",
                    "name": "Gaurav Kumar Nayak"
                },
                {
                    "authorId": "2217000",
                    "name": "Konda Reddy Mopuri"
                },
                {
                    "authorId": "7155274",
                    "name": "Vaisakh Shaj"
                },
                {
                    "authorId": "144682140",
                    "name": "R. Venkatesh Babu"
                },
                {
                    "authorId": "1429640900",
                    "name": "Anirban Chakraborty"
                }
            ],
            "abstract": "Knowledge distillation deals with the problem of training a smaller model (Student) from a high capacity source model (Teacher) so as to retain most of its performance. Existing approaches use either the training data or meta-data extracted from it in order to train the Student. However, accessing the dataset on which the Teacher has been trained may not always be feasible if the dataset is very large or it poses privacy or safety concerns (e.g., bio-metric or medical data). Hence, in this paper, we propose a novel data-free method to train the Student from the Teacher. Without even using any meta-data, we synthesize the Data Impressions from the complex Teacher model and utilize these as surrogates for the original training data samples to transfer its learning to Student via knowledge distillation. We, therefore, dub our method \"Zero-Shot Knowledge Distillation\" and demonstrate that our framework results in competitive generalization performance as achieved by distillation using the actual training data samples on multiple benchmark datasets.",
            "corpus_id": 159041346,
            "sentences": [
                {
                    "corpus_id": "159041346",
                    "title": "Zero-Shot Knowledge Distillation in Deep Networks",
                    "text": "Knowledge Distillation (Hinton et al., 2015) enables to transfer the complex mapping functions learned by cumbersome models to relatively simpler models. The cumbersome model can be an ensemble of multiple large models or a single model with large capacity and strong regualrizers such as Dropout (Srivastava et al., 2014), BatchNorm (Ioffe & Szegedy, 2015), etc. Typically the complex and small models are referred to as Teacher (T) and Student (S) models respectively. Generally the Teacher models deliver excellent performance, but they can be huge and computationally expensive. Hence, these models can not be deployed in The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability. For performing the knowledge distillation, one can use the training data from the target distribution or an arbitrary data. Typically, the data used to perform the distillation is called 'Transfer set'. In order to maximize the information provided per sample, we can make the soft targets to have a high entropy (non-peaky). This is generally achieved by using a high temperature at the softmax layer (Hinton et al., 2015). Also, because of non-peaky soft-labels, the training gradients computed on the loss will have less variance and enable to use higher learning rates leading to quick convergence. \n\nThe existing approaches use natural data either from the target data distribution or a different transfer set to perform the distillation. It is found by (Hinton et al., 2015) that using original training data performs relatively better. They also suggest to have an additional term in the objective for the Student to predict correct labels on the training data along with matching the soft-labels from the Teacher (as shown in eq. ( 1)). However, accessing the samples over which the Teacher had been trained may not always be feasible.",
                    "score": 0.6831973876440309,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 153
                        },
                        {
                            "start": 154,
                            "end": 470
                        },
                        {
                            "start": 471,
                            "end": 582
                        },
                        {
                            "start": 583,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1128
                        },
                        {
                            "start": 1129,
                            "end": 1252
                        },
                        {
                            "start": 1253,
                            "end": 1331
                        },
                        {
                            "start": 1332,
                            "end": 1454
                        },
                        {
                            "start": 1455,
                            "end": 1553
                        },
                        {
                            "start": 1554,
                            "end": 1731
                        },
                        {
                            "start": 1734,
                            "end": 1872
                        },
                        {
                            "start": 1873,
                            "end": 1971
                        },
                        {
                            "start": 1972,
                            "end": 2173
                        },
                        {
                            "start": 2174,
                            "end": 2272
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 297,
                            "end": 322,
                            "matchedPaperCorpusId": "6844431"
                        },
                        {
                            "start": 334,
                            "end": 357,
                            "matchedPaperCorpusId": "5808102"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82470703125
                }
            ],
            "relevance_judgement": 0.82470703125,
            "relevance_judgment_input_expanded": "# Title: Zero-Shot Knowledge Distillation in Deep Networks\n# Venue: International Conference on Machine Learning\n# Authors: Gaurav Kumar Nayak, Konda Reddy Mopuri, Vaisakh Shaj, R. Venkatesh Babu, Anirban Chakraborty\n## Abstract\nKnowledge distillation deals with the problem of training a smaller model (Student) from a high capacity source model (Teacher) so as to retain most of its performance. Existing approaches use either the training data or meta-data extracted from it in order to train the Student. However, accessing the dataset on which the Teacher has been trained may not always be feasible if the dataset is very large or it poses privacy or safety concerns (e.g., bio-metric or medical data). Hence, in this paper, we propose a novel data-free method to train the Student from the Teacher. Without even using any meta-data, we synthesize the Data Impressions from the complex Teacher model and utilize these as surrogates for the original training data samples to transfer its learning to Student via knowledge distillation. We, therefore, dub our method \"Zero-Shot Knowledge Distillation\" and demonstrate that our framework results in competitive generalization performance as achieved by distillation using the actual training data samples on multiple benchmark datasets.\n## Introduction\nKnowledge Distillation (Hinton et al., 2015) enables to transfer the complex mapping functions learned by cumbersome models to relatively simpler models. The cumbersome model can be an ensemble of multiple large models or a single model with large capacity and strong regualrizers such as Dropout (Srivastava et al., 2014), BatchNorm (Ioffe & Szegedy, 2015), etc. Typically the complex and small models are referred to as Teacher (T) and Student (S) models respectively. Generally the Teacher models deliver excellent performance, but they can be huge and computationally expensive. Hence, these models can not be deployed in The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability. For performing the knowledge distillation, one can use the training data from the target distribution or an arbitrary data. Typically, the data used to perform the distillation is called 'Transfer set'. In order to maximize the information provided per sample, we can make the soft targets to have a high entropy (non-peaky). This is generally achieved by using a high temperature at the softmax layer (Hinton et al., 2015). Also, because of non-peaky soft-labels, the training gradients computed on the loss will have less variance and enable to use higher learning rates leading to quick convergence. \n\nThe existing approaches use natural data either from the target data distribution or a different transfer set to perform the distillation. It is found by (Hinton et al., 2015) that using original training data performs relatively better. They also suggest to have an additional term in the objective for the Student to predict correct labels on the training data along with matching the soft-labels from the Teacher (as shown in eq. ( 1)). However, accessing the samples over which the Teacher had been trained may not always be feasible.",
            "reference_string": "[159041346 | Nayak et al. | 2019 | Citations: 245]"
        },
        {
            "title": "Self Supervision to Distillation for Long-Tailed Visual Recognition",
            "venue": "IEEE International Conference on Computer Vision",
            "year": 2021,
            "reference_count": 50,
            "citation_count": 103,
            "influential_citation_count": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2109.04075",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2109.04075, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1652032884",
                    "name": "Tianhao Li"
                },
                {
                    "authorId": "2109120086",
                    "name": "Limin Wang"
                },
                {
                    "authorId": "39914710",
                    "name": "Gangshan Wu"
                }
            ],
            "abstract": "Deep learning has achieved remarkable progress for visual recognition on large-scale balanced datasets but still performs poorly on real-world long-tailed data. Previous methods often adopt class re-balanced training strategies to effectively alleviate the imbalance issue, but might be a risk of over-fitting tail classes. The recent decoupling method overcomes over-fitting issues by using a multi-stage training scheme, yet, it is still incapable of capturing tail class information in the feature learning stage. In this paper, we show that soft label can serve as a powerful solution to incorporate label correlation into a multi-stage training scheme for long-tailed recognition. The intrinsic relation between classes embodied by soft labels turns out to be helpful for long-tailed recognition by transferring knowledge from head to tail classes.Specifically, we propose a conceptually simple yet particularly effective multi-stage training scheme, termed as Self Supervised to Distillation (SSD). This scheme is composed of two parts. First, we introduce a self-distillation framework for long-tailed recognition, which can mine the label relation automatically. Second, we present a new distillation label generation module guided by self-supervision. The distilled labels integrate information from both label and data domains that can model long-tailed distribution effectively. We conduct extensive experiments and our method achieves the state-of-the-art results on three long-tailed recognition benchmarks: ImageNet-LT, CIFAR100-LT and iNaturalist 2018. Our SSD outperforms the strong LWS baseline by from 2.7% to 4.5% on various datasets.",
            "corpus_id": 237453494,
            "sentences": [
                {
                    "corpus_id": "237453494",
                    "title": "Self Supervision to Distillation for Long-Tailed Visual Recognition",
                    "text": "Knowledge distillation [16] is first introduced for transferring knowledge from high-capability networks (teacher models) to small networks (student models) via soft labels. Our SSD method is inspired by knowledge distillation, yet exhibits essential difference with it. In our SSD method, student models are identical to teacher models, but are learned under different sampling strategies. Also, in particular for long-tailed recognition, the dark knowledge in soft labels can be helpful by transferring knowledge from head classes to tail classes. Due to the complementary properties of soft and hard labels, we propose a customized design by applying two separate classifiers supervised by hard and soft labels, respectively.\n\nMore formally, we denote x a training image with its hard label y and soft label y. We aim to learning an embedding function F that encodes x into feature vector f = F(x; \u03b8), as well as two classifiers G hard and G sof t . The feature vector f will be sent to two linear classifiers G hard and G sof t to get output logits z hard = G hard (f ) and z sof t = G sof t (f ). Let z denote the output logits of teacher model, then the soft label is given by:\n\nwhere i is the category index and T is the temperature which is set to 2 by default. Then, the knowledge distillation loss is written as:\n\n(4) For hard label supervision, we utilize the standard cross entropy loss L ce . Thus, the final loss is the combination of these two losses:\n\nwhere both of \u03bb 1 and \u03bb 2 are the weight of each loss and set to 1 in our experiments.",
                    "score": 0.5919342381285473,
                    "section_title": "Long-tailed recognition via self-distillation",
                    "char_start_offset": 16788,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 23,
                            "end": 27,
                            "matchedPaperCorpusId": "7200347"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82470703125
                }
            ],
            "relevance_judgement": 0.82470703125,
            "relevance_judgment_input_expanded": "# Title: Self Supervision to Distillation for Long-Tailed Visual Recognition\n# Venue: IEEE International Conference on Computer Vision\n# Authors: Tianhao Li, Limin Wang, Gangshan Wu\n## Abstract\nDeep learning has achieved remarkable progress for visual recognition on large-scale balanced datasets but still performs poorly on real-world long-tailed data. Previous methods often adopt class re-balanced training strategies to effectively alleviate the imbalance issue, but might be a risk of over-fitting tail classes. The recent decoupling method overcomes over-fitting issues by using a multi-stage training scheme, yet, it is still incapable of capturing tail class information in the feature learning stage. In this paper, we show that soft label can serve as a powerful solution to incorporate label correlation into a multi-stage training scheme for long-tailed recognition. The intrinsic relation between classes embodied by soft labels turns out to be helpful for long-tailed recognition by transferring knowledge from head to tail classes.Specifically, we propose a conceptually simple yet particularly effective multi-stage training scheme, termed as Self Supervised to Distillation (SSD). This scheme is composed of two parts. First, we introduce a self-distillation framework for long-tailed recognition, which can mine the label relation automatically. Second, we present a new distillation label generation module guided by self-supervision. The distilled labels integrate information from both label and data domains that can model long-tailed distribution effectively. We conduct extensive experiments and our method achieves the state-of-the-art results on three long-tailed recognition benchmarks: ImageNet-LT, CIFAR100-LT and iNaturalist 2018. Our SSD outperforms the strong LWS baseline by from 2.7% to 4.5% on various datasets.\n## Long-tailed recognition via self-distillation\nKnowledge distillation [16] is first introduced for transferring knowledge from high-capability networks (teacher models) to small networks (student models) via soft labels. Our SSD method is inspired by knowledge distillation, yet exhibits essential difference with it. In our SSD method, student models are identical to teacher models, but are learned under different sampling strategies. Also, in particular for long-tailed recognition, the dark knowledge in soft labels can be helpful by transferring knowledge from head classes to tail classes. Due to the complementary properties of soft and hard labels, we propose a customized design by applying two separate classifiers supervised by hard and soft labels, respectively.\n\nMore formally, we denote x a training image with its hard label y and soft label y. We aim to learning an embedding function F that encodes x into feature vector f = F(x; \u03b8), as well as two classifiers G hard and G sof t . The feature vector f will be sent to two linear classifiers G hard and G sof t to get output logits z hard = G hard (f ) and z sof t = G sof t (f ). Let z denote the output logits of teacher model, then the soft label is given by:\n\nwhere i is the category index and T is the temperature which is set to 2 by default. Then, the knowledge distillation loss is written as:\n\n(4) For hard label supervision, we utilize the standard cross entropy loss L ce . Thus, the final loss is the combination of these two losses:\n\nwhere both of \u03bb 1 and \u03bb 2 are the weight of each loss and set to 1 in our experiments.",
            "reference_string": "[237453494 | Li et al. | 2021 | Citations: 103]"
        },
        {
            "title": "Relational Representation Distillation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 77,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.12073, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2196360101",
                    "name": "Nikolaos Giakoumoglou"
                },
                {
                    "authorId": "2292259667",
                    "name": "Tania Stathaki"
                }
            ],
            "abstract": "Knowledge distillation involves transferring knowledge from large, cumbersome teacher models to more compact student models. The standard approach minimizes the Kullback-Leibler (KL) divergence between the probabilistic outputs of a teacher and student network. However, this approach fails to capture important structural relationships in the teacher's internal representations. Recent advances have turned to contrastive learning objectives, but these methods impose overly strict constraints through instance-discrimination, forcing apart semantically similar samples even when they should maintain similarity. This motivates an alternative objective by which we preserve relative relationships between instances. Our method employs separate temperature parameters for teacher and student distributions, with sharper student outputs, enabling precise learning of primary relationships while preserving secondary similarities. We show theoretical connections between our objective and both InfoNCE loss and KL divergence. Experiments demonstrate that our method significantly outperforms existing knowledge distillation methods across diverse knowledge transfer tasks, achieving better alignment with teacher models, and sometimes even outperforms the teacher network.",
            "corpus_id": 271244914,
            "sentences": [
                {
                    "corpus_id": "271244914",
                    "title": "Relational Representation Distillation",
                    "text": "The concept of Knowledge Distillation (KD) was first introduced by Hinton et al. [20]. It involves extracting \"dark knowledge\" from accurate teacher models to guide the learning process of student models. This is achieved by utilizing the Kullback-Leibler (KL) loss to regularize the output probabilities of student models, aligning them with those of their teacher models when given the same inputs. This simple yet effective approach significantly improves the generalization ability of smaller models and finds extensive applications in various domains. Since the initial success of KD [20], several advanced methods, including logit distillation [21,35,52] and feature distillation [42,47,53,55], have been introduced. \n\nLogit distillation. Earlier methods on logit-based distillation primarily focused on improving student learning by directly mimicking the teacher's output probabilities. Examples included hierarchical supervision using intermediary teacher networks [52], multi-step student training to 2 Average relative improvement is calculated as: \n\n, where Acc i RRD , Acc i KD , and Acc i van represent the accuracies of RRD, KD, and vanilla training of the i-th student model, respectively [47]. \n\nenhance compatibility [35], collaborative learning among multiple students to improve generalization [58] and mechanisms that separately handle different types of logit information [59]. Recent advancements have sought to refine the quality of knowledge transfer. Some methods modify the distillation target: label decoupling [61] separately processes hard and soft labels, while instance-specific label smoothing [54] adapts the smoothing factor per example. Additional approaches focus on refining probability distributions, including probability reweighting to emphasize important outputs [37] and logit normalization to mitigate overconfidence [46]. Other methods include dynamic temperature scaling to adjust teacher-student similarity [25], specialized transformations to align teacher-student logits more effectively [60], and approaches that adapt teacher logits to better fit weaker students [21]. \n\nFeature distillation. Earlier methods on feature-based distillation emphasized utilizing intermediate feature representations to facilitate learning.",
                    "score": 0.6116685122573274,
                    "section_title": "Related Work",
                    "char_start_offset": 4321,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 86
                        },
                        {
                            "start": 87,
                            "end": 204
                        },
                        {
                            "start": 205,
                            "end": 400
                        },
                        {
                            "start": 401,
                            "end": 556
                        },
                        {
                            "start": 557,
                            "end": 722
                        },
                        {
                            "start": 725,
                            "end": 744
                        },
                        {
                            "start": 745,
                            "end": 894
                        },
                        {
                            "start": 895,
                            "end": 1059
                        },
                        {
                            "start": 1062,
                            "end": 1210
                        },
                        {
                            "start": 1213,
                            "end": 1399
                        },
                        {
                            "start": 1400,
                            "end": 1476
                        },
                        {
                            "start": 1477,
                            "end": 1672
                        },
                        {
                            "start": 1673,
                            "end": 1866
                        },
                        {
                            "start": 1867,
                            "end": 2119
                        },
                        {
                            "start": 2122,
                            "end": 2143
                        },
                        {
                            "start": 2144,
                            "end": 2271
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 686,
                            "end": 690,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 693,
                            "end": 696,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 696,
                            "end": 699,
                            "matchedPaperCorpusId": "829159"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82177734375
                }
            ],
            "relevance_judgement": 0.82177734375,
            "relevance_judgment_input_expanded": "# Title: Relational Representation Distillation\n# Venue: arXiv.org\n# Authors: Nikolaos Giakoumoglou, Tania Stathaki\n## Abstract\nKnowledge distillation involves transferring knowledge from large, cumbersome teacher models to more compact student models. The standard approach minimizes the Kullback-Leibler (KL) divergence between the probabilistic outputs of a teacher and student network. However, this approach fails to capture important structural relationships in the teacher's internal representations. Recent advances have turned to contrastive learning objectives, but these methods impose overly strict constraints through instance-discrimination, forcing apart semantically similar samples even when they should maintain similarity. This motivates an alternative objective by which we preserve relative relationships between instances. Our method employs separate temperature parameters for teacher and student distributions, with sharper student outputs, enabling precise learning of primary relationships while preserving secondary similarities. We show theoretical connections between our objective and both InfoNCE loss and KL divergence. Experiments demonstrate that our method significantly outperforms existing knowledge distillation methods across diverse knowledge transfer tasks, achieving better alignment with teacher models, and sometimes even outperforms the teacher network.\n## Related Work\nThe concept of Knowledge Distillation (KD) was first introduced by Hinton et al. [20]. It involves extracting \"dark knowledge\" from accurate teacher models to guide the learning process of student models. This is achieved by utilizing the Kullback-Leibler (KL) loss to regularize the output probabilities of student models, aligning them with those of their teacher models when given the same inputs. This simple yet effective approach significantly improves the generalization ability of smaller models and finds extensive applications in various domains. Since the initial success of KD [20], several advanced methods, including logit distillation [21,35,52] and feature distillation [42,47,53,55], have been introduced. \n\nLogit distillation. Earlier methods on logit-based distillation primarily focused on improving student learning by directly mimicking the teacher's output probabilities. Examples included hierarchical supervision using intermediary teacher networks [52], multi-step student training to 2 Average relative improvement is calculated as: \n\n, where Acc i RRD , Acc i KD , and Acc i van represent the accuracies of RRD, KD, and vanilla training of the i-th student model, respectively [47]. \n\nenhance compatibility [35], collaborative learning among multiple students to improve generalization [58] and mechanisms that separately handle different types of logit information [59]. Recent advancements have sought to refine the quality of knowledge transfer. Some methods modify the distillation target: label decoupling [61] separately processes hard and soft labels, while instance-specific label smoothing [54] adapts the smoothing factor per example. Additional approaches focus on refining probability distributions, including probability reweighting to emphasize important outputs [37] and logit normalization to mitigate overconfidence [46]. Other methods include dynamic temperature scaling to adjust teacher-student similarity [25], specialized transformations to align teacher-student logits more effectively [60], and approaches that adapt teacher logits to better fit weaker students [21]. \n\nFeature distillation. Earlier methods on feature-based distillation emphasized utilizing intermediate feature representations to facilitate learning.",
            "reference_string": "[271244914 | Giakoumoglou et al. | 2024 | Citations: 0]"
        },
        {
            "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
            "venue": "",
            "year": 2023,
            "reference_count": 0,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.13621, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268314644",
                    "name": "Chi-Ping Su"
                },
                {
                    "authorId": "1569686364",
                    "name": "Ching-Hsun Tseng"
                },
                {
                    "authorId": "2338265886",
                    "name": "Bin Pu"
                },
                {
                    "authorId": "2338506240",
                    "name": "Lei Zhao"
                },
                {
                    "authorId": "2328588941",
                    "name": "Zhuangzhuang Chen"
                },
                {
                    "authorId": "2116351339",
                    "name": "Shin-Jye Lee"
                }
            ],
            "abstract": "Knowledge distillation (KD) enables a smaller\"student\"model to mimic a larger\"teacher\"model by transferring knowledge from the teacher's output or features. However, most KD methods treat all samples uniformly, overlooking the varying learning value of each sample and thereby limiting effectiveness. In this paper, we propose Entropy-based Adaptive Knowledge Distillation (EA-KD), a simple yet effective plug-and-play KD method that prioritizes learning from valuable samples. EA-KD quantifies each sample's learning value by strategically combining the entropy of the teacher and student output, then dynamically reweights the distillation loss to place greater emphasis on high-value samples. Extensive experiments across diverse KD frameworks and tasks$\\unicode{x2014}$including image classification, object detection, and large language model (LLM) distillation$\\unicode{x2014}$demonstrate that EA-KD consistently enhances performance, achieving state-of-the-art results with negligible computational cost. Our code will be publicly available.",
            "corpus_id": 265444951,
            "sentences": [
                {
                    "corpus_id": "265444951",
                    "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
                    "text": "The goal of KD [8] is to transfer the dark knowledge encapsulated in the soft probability output of the teacher model to the student model. In classification tasks, the softened probabilities are computed via the temperature-scaled softmax function, given by \n\nwhere p i (T ) is the probability output for class i softened by the temperature hyperparameter T , y i represents the logit for class i, and C is the total number of classes. Typically, T is set to greater than 1 in KD. The higher value of T produces softer probabilities, which are crucial for unveiling the dark knowledge hidden in the inter-class relationships captured by the teacher. The core idea of KD lies in minimizing the KL divergence loss function to align the soft logits of the teacher and student. The loss of KD L KD is defined as \n\nwhere p T and p S are the output logits of the teacher T and the student S, respectively. A notable limitation of KD is its uniform treatment of all samples, regardless of their inherent difficulty. In practice, different samples may present varying levels of challenge, a one-size-fits-all approach might not optimally transfer the insights of the teacher. This paper aims to address this limitation by introducing a sample-wise reweighting scheme. The original KD method computes the distillation loss using the logits from both teacher and student. ER-KD introduces a novel step by reweighting the loss with the entropy of the teacher's predictions at the instance level. This entropy serves as an indicator of sample difficulty, guiding the student to focus more on challenging samples. By ensuring a balanced knowledge transfer, ER-KD reduces the student's overconfidence prediction and aligns it more closely with the teacher's.",
                    "score": 0.5841359109301532,
                    "section_title": "Preliminaries",
                    "char_start_offset": 9275,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 139
                        },
                        {
                            "start": 140,
                            "end": 258
                        },
                        {
                            "start": 261,
                            "end": 436
                        },
                        {
                            "start": 437,
                            "end": 481
                        },
                        {
                            "start": 482,
                            "end": 650
                        },
                        {
                            "start": 651,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 808
                        },
                        {
                            "start": 811,
                            "end": 900
                        },
                        {
                            "start": 901,
                            "end": 1009
                        },
                        {
                            "start": 1010,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1260
                        },
                        {
                            "start": 1261,
                            "end": 1362
                        },
                        {
                            "start": 1363,
                            "end": 1485
                        },
                        {
                            "start": 1486,
                            "end": 1601
                        },
                        {
                            "start": 1602,
                            "end": 1745
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8193359375
                },
                {
                    "corpus_id": "265444951",
                    "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
                    "text": "KD aims to transfer the dark knowledge from a complex teacher model to a lightweight student model by aligning their logits that are softened with a temperature hyperparameter. This enables the student to approximate the teacher's performance in a compact form. \n\nLogit and Feature Distillation Logit distillation methods aim to align the teacher and student output logits, valued for their simplicity and wide applicability. In contrast, feature distillation focuses on minimizing divergence in intermediate feature representations, offering enhanced learning but at the cost of increased computational demands. \n\nBoth pathways have demonstrated state-of-the-art performance across various tasks and domains. While most of these methods follow a static training approach, recent techniques have further refined the distillation process through dynamic approaches. \n\nAdaptive Distillation Diverging from the static approaches, adaptive distillation methods pave the way for more dynamic and tailored knowledge transfer [3,13,18,20,31,37,38]. These methods either dynamically provide prior knowledge [38], modulate hyperparameters [13,18,20], or adjust distillation strategies [3,31,37] based on the teacher-student performance gap. Within this landscape, the Curriculum Temperature for Knowledge Distillation (CTKD) [13] emerges as a simple yet effective approach. This approach introduces curriculum training and adversarial temperature learning to KD, progressively exposes the student to complexities, and pushes it to address harder challenges through instance-wise temperature modulation. Nonetheless, CTKD relies on temperature modulation which dictates the overall softness of labels. In contrast, our ER-KD reweights the KD loss with the entropy of the teacher's predictions, precisely emphasizing challenging samples while reducing the focus on simpler ones.",
                    "score": 0.6155290522506045,
                    "section_title": "Related work",
                    "char_start_offset": 5957,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 177,
                            "end": 261
                        },
                        {
                            "start": 264,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 612
                        },
                        {
                            "start": 615,
                            "end": 709
                        },
                        {
                            "start": 710,
                            "end": 864
                        },
                        {
                            "start": 867,
                            "end": 1041
                        },
                        {
                            "start": 1042,
                            "end": 1231
                        },
                        {
                            "start": 1232,
                            "end": 1364
                        },
                        {
                            "start": 1365,
                            "end": 1593
                        },
                        {
                            "start": 1594,
                            "end": 1691
                        },
                        {
                            "start": 1692,
                            "end": 1867
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1019,
                            "end": 1022,
                            "matchedPaperCorpusId": "239024317"
                        },
                        {
                            "start": 1022,
                            "end": 1025,
                            "matchedPaperCorpusId": "254069919"
                        },
                        {
                            "start": 1028,
                            "end": 1031,
                            "matchedPaperCorpusId": "244119770"
                        },
                        {
                            "start": 1034,
                            "end": 1037,
                            "matchedPaperCorpusId": "244680427"
                        },
                        {
                            "start": 1037,
                            "end": 1040,
                            "matchedPaperCorpusId": "249626454"
                        },
                        {
                            "start": 1099,
                            "end": 1103,
                            "matchedPaperCorpusId": "249626454"
                        },
                        {
                            "start": 1130,
                            "end": 1134,
                            "matchedPaperCorpusId": "254069919"
                        },
                        {
                            "start": 1137,
                            "end": 1140,
                            "matchedPaperCorpusId": "244119770"
                        },
                        {
                            "start": 1176,
                            "end": 1179,
                            "matchedPaperCorpusId": "239024317"
                        },
                        {
                            "start": 1182,
                            "end": 1185,
                            "matchedPaperCorpusId": "244680427"
                        },
                        {
                            "start": 1316,
                            "end": 1320,
                            "matchedPaperCorpusId": "254069919"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79052734375
                }
            ],
            "relevance_judgement": 0.8193359375,
            "relevance_judgment_input_expanded": "# Title: EA-KD: Entropy-based Adaptive Knowledge Distillation\n# Venue: \n# Authors: Chi-Ping Su, Ching-Hsun Tseng, Bin Pu, Lei Zhao, Zhuangzhuang Chen, Shin-Jye Lee\n## Abstract\nKnowledge distillation (KD) enables a smaller\"student\"model to mimic a larger\"teacher\"model by transferring knowledge from the teacher's output or features. However, most KD methods treat all samples uniformly, overlooking the varying learning value of each sample and thereby limiting effectiveness. In this paper, we propose Entropy-based Adaptive Knowledge Distillation (EA-KD), a simple yet effective plug-and-play KD method that prioritizes learning from valuable samples. EA-KD quantifies each sample's learning value by strategically combining the entropy of the teacher and student output, then dynamically reweights the distillation loss to place greater emphasis on high-value samples. Extensive experiments across diverse KD frameworks and tasks$\\unicode{x2014}$including image classification, object detection, and large language model (LLM) distillation$\\unicode{x2014}$demonstrate that EA-KD consistently enhances performance, achieving state-of-the-art results with negligible computational cost. Our code will be publicly available.\n## Related work\nKD aims to transfer the dark knowledge from a complex teacher model to a lightweight student model by aligning their logits that are softened with a temperature hyperparameter. This enables the student to approximate the teacher's performance in a compact form. \n\nLogit and Feature Distillation Logit distillation methods aim to align the teacher and student output logits, valued for their simplicity and wide applicability. In contrast, feature distillation focuses on minimizing divergence in intermediate feature representations, offering enhanced learning but at the cost of increased computational demands. \n\nBoth pathways have demonstrated state-of-the-art performance across various tasks and domains. While most of these methods follow a static training approach, recent techniques have further refined the distillation process through dynamic approaches. \n\nAdaptive Distillation Diverging from the static approaches, adaptive distillation methods pave the way for more dynamic and tailored knowledge transfer [3,13,18,20,31,37,38]. These methods either dynamically provide prior knowledge [38], modulate hyperparameters [13,18,20], or adjust distillation strategies [3,31,37] based on the teacher-student performance gap. Within this landscape, the Curriculum Temperature for Knowledge Distillation (CTKD) [13] emerges as a simple yet effective approach. This approach introduces curriculum training and adversarial temperature learning to KD, progressively exposes the student to complexities, and pushes it to address harder challenges through instance-wise temperature modulation. Nonetheless, CTKD relies on temperature modulation which dictates the overall softness of labels. In contrast, our ER-KD reweights the KD loss with the entropy of the teacher's predictions, precisely emphasizing challenging samples while reducing the focus on simpler ones.\n\n## Preliminaries\nThe goal of KD [8] is to transfer the dark knowledge encapsulated in the soft probability output of the teacher model to the student model. In classification tasks, the softened probabilities are computed via the temperature-scaled softmax function, given by \n\nwhere p i (T ) is the probability output for class i softened by the temperature hyperparameter T , y i represents the logit for class i, and C is the total number of classes. Typically, T is set to greater than 1 in KD. The higher value of T produces softer probabilities, which are crucial for unveiling the dark knowledge hidden in the inter-class relationships captured by the teacher. The core idea of KD lies in minimizing the KL divergence loss function to align the soft logits of the teacher and student. The loss of KD L KD is defined as \n\nwhere p T and p S are the output logits of the teacher T and the student S, respectively. A notable limitation of KD is its uniform treatment of all samples, regardless of their inherent difficulty. In practice, different samples may present varying levels of challenge, a one-size-fits-all approach might not optimally transfer the insights of the teacher. This paper aims to address this limitation by introducing a sample-wise reweighting scheme. The original KD method computes the distillation loss using the logits from both teacher and student. ER-KD introduces a novel step by reweighting the loss with the entropy of the teacher's predictions at the instance level. This entropy serves as an indicator of sample difficulty, guiding the student to focus more on challenging samples. By ensuring a balanced knowledge transfer, ER-KD reduces the student's overconfidence prediction and aligns it more closely with the teacher's.",
            "reference_string": "[265444951 | Su et al. | 2023 | Citations: 2]"
        },
        {
            "title": "Harmonizing knowledge Transfer in Neural Network with Unified Distillation",
            "venue": "European Conference on Computer Vision",
            "year": 2024,
            "reference_count": 35,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.18565, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2180087917",
                    "name": "Yaomin Huang"
                },
                {
                    "authorId": "2257133494",
                    "name": "Zaoming Yan"
                },
                {
                    "authorId": "2242115637",
                    "name": "Chaomin Shen"
                },
                {
                    "authorId": "152786529",
                    "name": "Faming Fang"
                },
                {
                    "authorId": "2323414943",
                    "name": "Guixu Zhang"
                }
            ],
            "abstract": "Knowledge distillation (KD), known for its ability to transfer knowledge from a cumbersome network (teacher) to a lightweight one (student) without altering the architecture, has been garnering increasing attention. Two primary categories emerge within KD methods: feature-based, focusing on intermediate layers' features, and logits-based, targeting the final layer's logits. This paper introduces a novel perspective by leveraging diverse knowledge sources within a unified KD framework. Specifically, we aggregate features from intermediate layers into a comprehensive representation, effectively gathering semantic information from different stages and scales. Subsequently, we predict the distribution parameters from this representation. These steps transform knowledge from the intermediate layers into corresponding distributive forms, thereby allowing for knowledge distillation through a unified distribution constraint at different stages of the network, ensuring the comprehensiveness and coherence of knowledge transfer. Numerous experiments were conducted to validate the effectiveness of the proposed method.",
            "corpus_id": 272969060,
            "sentences": [
                {
                    "corpus_id": "272969060",
                    "title": "Harmonizing knowledge Transfer in Neural Network with Unified Distillation",
                    "text": "Logits-based KD was first proposed in [9], where the student learns the logits knowledge from the teacher through KL divergence. In contrast to direct supervised learning through ground truth, logits-based KD using soft labels can elicit more 'dark knowledge' [9], thereby enhancing the performance of the student network without altering its architecture. The logits distillation losses are represented as follows: \n\nP j denotes the class probability derived from the logits z \u2208 R C after undergoing Softmax. \u03c4 is the temperature scaling hyper-parameter which enables the production of different probability distributions: \n\nFeature-based KD transfers the knowledge of intermediate layers features from the teacher to the student. Generally, we can formulate such distillation methods as: \n\nwhere F t i and F s i are the corresponding features of the teacher and student at i-th layer. w(\u2022) is a mapping function that aligns the dimensions of features from the student to the teacher. \n\nFrom the above distillation process, it can be seen that logits-based knowledge distillation focuses on constraining the overall knowledge distribution. In contrast, feature-based knowledge distillation imposes pixel-level constraints on the student network. These two methods have distinct optimization objectives during the distillation process, and directly integrating them can lead to suboptimal solutions. In this study, we aim to achieve comprehensive knowledge transfer by conducting knowledge distillation on the intermediate layer features and the final layer logits. To address this issue, we introduce UniKD, a meticulously designed framework that adjusts the intermediate layer features for a thorough and consistent knowledge transfer, using the same constraints employed for the final layer's logits.",
                    "score": 0.6727024900126358,
                    "section_title": "Preliminaries",
                    "char_start_offset": 8573,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 356
                        },
                        {
                            "start": 357,
                            "end": 415
                        },
                        {
                            "start": 418,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 623
                        },
                        {
                            "start": 626,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 789
                        },
                        {
                            "start": 792,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 985
                        },
                        {
                            "start": 988,
                            "end": 1140
                        },
                        {
                            "start": 1141,
                            "end": 1246
                        },
                        {
                            "start": 1247,
                            "end": 1399
                        },
                        {
                            "start": 1400,
                            "end": 1565
                        },
                        {
                            "start": 1566,
                            "end": 1803
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8115234375
                }
            ],
            "relevance_judgement": 0.8115234375,
            "relevance_judgment_input_expanded": "# Title: Harmonizing knowledge Transfer in Neural Network with Unified Distillation\n# Venue: European Conference on Computer Vision\n# Authors: Yaomin Huang, Zaoming Yan, Chaomin Shen, Faming Fang, Guixu Zhang\n## Abstract\nKnowledge distillation (KD), known for its ability to transfer knowledge from a cumbersome network (teacher) to a lightweight one (student) without altering the architecture, has been garnering increasing attention. Two primary categories emerge within KD methods: feature-based, focusing on intermediate layers' features, and logits-based, targeting the final layer's logits. This paper introduces a novel perspective by leveraging diverse knowledge sources within a unified KD framework. Specifically, we aggregate features from intermediate layers into a comprehensive representation, effectively gathering semantic information from different stages and scales. Subsequently, we predict the distribution parameters from this representation. These steps transform knowledge from the intermediate layers into corresponding distributive forms, thereby allowing for knowledge distillation through a unified distribution constraint at different stages of the network, ensuring the comprehensiveness and coherence of knowledge transfer. Numerous experiments were conducted to validate the effectiveness of the proposed method.\n## Preliminaries\nLogits-based KD was first proposed in [9], where the student learns the logits knowledge from the teacher through KL divergence. In contrast to direct supervised learning through ground truth, logits-based KD using soft labels can elicit more 'dark knowledge' [9], thereby enhancing the performance of the student network without altering its architecture. The logits distillation losses are represented as follows: \n\nP j denotes the class probability derived from the logits z \u2208 R C after undergoing Softmax. \u03c4 is the temperature scaling hyper-parameter which enables the production of different probability distributions: \n\nFeature-based KD transfers the knowledge of intermediate layers features from the teacher to the student. Generally, we can formulate such distillation methods as: \n\nwhere F t i and F s i are the corresponding features of the teacher and student at i-th layer. w(\u2022) is a mapping function that aligns the dimensions of features from the student to the teacher. \n\nFrom the above distillation process, it can be seen that logits-based knowledge distillation focuses on constraining the overall knowledge distribution. In contrast, feature-based knowledge distillation imposes pixel-level constraints on the student network. These two methods have distinct optimization objectives during the distillation process, and directly integrating them can lead to suboptimal solutions. In this study, we aim to achieve comprehensive knowledge transfer by conducting knowledge distillation on the intermediate layer features and the final layer logits. To address this issue, we introduce UniKD, a meticulously designed framework that adjusts the intermediate layer features for a thorough and consistent knowledge transfer, using the same constraints employed for the final layer's logits.",
            "reference_string": "[272969060 | Huang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.00382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327960212",
                    "name": "Chengyu Li"
                },
                {
                    "authorId": "2320341",
                    "name": "Debo Cheng"
                },
                {
                    "authorId": "2152233401",
                    "name": "Guixian Zhang"
                },
                {
                    "authorId": "2316406036",
                    "name": "Yi Li"
                },
                {
                    "authorId": "2186753918",
                    "name": "Shichao Zhang"
                }
            ],
            "abstract": "Graph Neural Networks (GNNs) have demonstrated strong performance in graph representation learning across various real-world applications. However, they often produce biased predictions caused by sensitive attributes, such as religion or gender, an issue that has been largely overlooked in existing methods. Recently, numerous studies have focused on reducing biases in GNNs. However, these approaches often rely on training with partial data (e.g., using either node features or graph structure alone), which can enhance fairness but frequently compromises model utility due to the limited utilization of available graph information. To address this tradeoff, we propose an effective strategy to balance fairness and utility in knowledge distillation. Specifically, we introduce FairDTD, a novel Fair representation learning framework built on Dual-Teacher Distillation, leveraging a causal graph model to guide and optimize the design of the distillation process. Specifically, FairDTD employs two fairness-oriented teacher models: a feature teacher and a structure teacher, to facilitate dual distillation, with the student model learning fairness knowledge from the teachers while also leveraging full data to mitigate utility loss. To enhance information transfer, we incorporate graph-level distillation to provide an indirect supplement of graph information during training, as well as a node-specific temperature module to improve the comprehensive transfer of fair knowledge. Experiments on diverse benchmark datasets demonstrate that FairDTD achieves optimal fairness while preserving high model utility, showcasing its effectiveness in fair representation learning for GNNs.",
            "corpus_id": 274436184,
            "sentences": [
                {
                    "corpus_id": "274436184",
                    "title": "Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation",
                    "text": "In this subsection, we introduce the concept of learning node-specific temperatures to enhance knowledge transfer from teacher to student models. While dual-teacher distillation and graphlevel distillation ensure that teacher models encode fair and information-rich representations, a critical remaining challenge is ensuring effective transfer of this knowledge to the student model, thereby improving performance. \n\nLearning node-specific temperatures addresses the limitations inherent in the temperature mechanism used in traditional distillation methods. In conventional knowledge distillation, a predefined temperature parameter is used to generate soft targets. The teacher model's informative dark knowledge is embedded in the soft targets, which infer the probability that a node belongs to a given class. The softmax function with a fixed temperature is defined as follows: \n\nwhere the temperature parameter \u03c4 is used to control the softness of each prediction. The distribution of predictions becomes smoother as the temperature rises, and sharper as the temperature falls. Thus, the temperature is responsible for regulating the balance between the true label knowledge and the dark knowledge. However, using a fixed temperature treats the softening of all teacher logits equally, ignoring variations in node representations. This uniform approach fails to provide precise guidance for individual nodes, which can hinder knowledge transfer. Moreover, existing research has shown that temperature scaling can improve model performance (Z. Zhang and Sabuncu, 2020). \n\nTo confront these issues, we propose an entropy-based approach to learn node-specific temperatures. This approach assigns a unique temperature \u03c4i to each node, controlling the degree of softening individually. The softmax function with node-specific temperatures is defined as: \n\nwhere the temperature parameter \u03c4i is the specific temperature of the i-th node. Obviously, \u03c4i determines the softening direction of each node. The temperature of the node is related to the teacher's confidence for each respective node (Zhichun Guo et al., 2023). Therefore, we can judge the teacher's confidence for each node through the entropy of the teacher's logits. The lower the entropy value, the higher the teacher's confidence for this node. Specifically, we calculate the node-specific temperature through the probability distribution of the teacher and the confidence of the teacher, defined as follows:",
                    "score": 0.639935571852821,
                    "section_title": "Learning node-specific temperatures",
                    "char_start_offset": 17591,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 415
                        },
                        {
                            "start": 418,
                            "end": 559
                        },
                        {
                            "start": 560,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 814
                        },
                        {
                            "start": 815,
                            "end": 883
                        },
                        {
                            "start": 886,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1084
                        },
                        {
                            "start": 1085,
                            "end": 1205
                        },
                        {
                            "start": 1206,
                            "end": 1337
                        },
                        {
                            "start": 1338,
                            "end": 1452
                        },
                        {
                            "start": 1453,
                            "end": 1549
                        },
                        {
                            "start": 1550,
                            "end": 1575
                        },
                        {
                            "start": 1578,
                            "end": 1677
                        },
                        {
                            "start": 1678,
                            "end": 1787
                        },
                        {
                            "start": 1788,
                            "end": 1855
                        },
                        {
                            "start": 1858,
                            "end": 1938
                        },
                        {
                            "start": 1939,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2121
                        },
                        {
                            "start": 2122,
                            "end": 2229
                        },
                        {
                            "start": 2230,
                            "end": 2309
                        },
                        {
                            "start": 2310,
                            "end": 2473
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1550,
                            "end": 1574,
                            "matchedPaperCorpusId": "219558831"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80810546875
                }
            ],
            "relevance_judgement": 0.80810546875,
            "relevance_judgment_input_expanded": "# Title: Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation\n# Venue: arXiv.org\n# Authors: Chengyu Li, Debo Cheng, Guixian Zhang, Yi Li, Shichao Zhang\n## Abstract\nGraph Neural Networks (GNNs) have demonstrated strong performance in graph representation learning across various real-world applications. However, they often produce biased predictions caused by sensitive attributes, such as religion or gender, an issue that has been largely overlooked in existing methods. Recently, numerous studies have focused on reducing biases in GNNs. However, these approaches often rely on training with partial data (e.g., using either node features or graph structure alone), which can enhance fairness but frequently compromises model utility due to the limited utilization of available graph information. To address this tradeoff, we propose an effective strategy to balance fairness and utility in knowledge distillation. Specifically, we introduce FairDTD, a novel Fair representation learning framework built on Dual-Teacher Distillation, leveraging a causal graph model to guide and optimize the design of the distillation process. Specifically, FairDTD employs two fairness-oriented teacher models: a feature teacher and a structure teacher, to facilitate dual distillation, with the student model learning fairness knowledge from the teachers while also leveraging full data to mitigate utility loss. To enhance information transfer, we incorporate graph-level distillation to provide an indirect supplement of graph information during training, as well as a node-specific temperature module to improve the comprehensive transfer of fair knowledge. Experiments on diverse benchmark datasets demonstrate that FairDTD achieves optimal fairness while preserving high model utility, showcasing its effectiveness in fair representation learning for GNNs.\n## Learning node-specific temperatures\nIn this subsection, we introduce the concept of learning node-specific temperatures to enhance knowledge transfer from teacher to student models. While dual-teacher distillation and graphlevel distillation ensure that teacher models encode fair and information-rich representations, a critical remaining challenge is ensuring effective transfer of this knowledge to the student model, thereby improving performance. \n\nLearning node-specific temperatures addresses the limitations inherent in the temperature mechanism used in traditional distillation methods. In conventional knowledge distillation, a predefined temperature parameter is used to generate soft targets. The teacher model's informative dark knowledge is embedded in the soft targets, which infer the probability that a node belongs to a given class. The softmax function with a fixed temperature is defined as follows: \n\nwhere the temperature parameter \u03c4 is used to control the softness of each prediction. The distribution of predictions becomes smoother as the temperature rises, and sharper as the temperature falls. Thus, the temperature is responsible for regulating the balance between the true label knowledge and the dark knowledge. However, using a fixed temperature treats the softening of all teacher logits equally, ignoring variations in node representations. This uniform approach fails to provide precise guidance for individual nodes, which can hinder knowledge transfer. Moreover, existing research has shown that temperature scaling can improve model performance (Z. Zhang and Sabuncu, 2020). \n\nTo confront these issues, we propose an entropy-based approach to learn node-specific temperatures. This approach assigns a unique temperature \u03c4i to each node, controlling the degree of softening individually. The softmax function with node-specific temperatures is defined as: \n\nwhere the temperature parameter \u03c4i is the specific temperature of the i-th node. Obviously, \u03c4i determines the softening direction of each node. The temperature of the node is related to the teacher's confidence for each respective node (Zhichun Guo et al., 2023). Therefore, we can judge the teacher's confidence for each node through the entropy of the teacher's logits. The lower the entropy value, the higher the teacher's confidence for this node. Specifically, we calculate the node-specific temperature through the probability distribution of the teacher and the confidence of the teacher, defined as follows:",
            "reference_string": "[274436184 | Li et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 64,
            "citation_count": 26,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.13298, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "152988335",
                    "name": "Yixiao Ge"
                },
                {
                    "authorId": "2087535968",
                    "name": "Ching Lam Choi"
                },
                {
                    "authorId": "2115477215",
                    "name": "Xiao Zhang"
                },
                {
                    "authorId": "46737362",
                    "name": "Peipei Zhao"
                },
                {
                    "authorId": "2075369514",
                    "name": "Feng Zhu"
                },
                {
                    "authorId": "145638781",
                    "name": "Rui Zhao"
                },
                {
                    "authorId": "47893312",
                    "name": "Hongsheng Li"
                }
            ],
            "abstract": "The recent studies of knowledge distillation have discovered that ensembling the\"dark knowledge\"from multiple teachers or students contributes to creating better soft targets for training, but at the cost of significantly more computations and/or parameters. In this work, we present BAtch Knowledge Ensembling (BAKE) to produce refined soft targets for anchor images by propagating and ensembling the knowledge of the other samples in the same mini-batch. Specifically, for each sample of interest, the propagation of knowledge is weighted in accordance with the inter-sample affinities, which are estimated on-the-fly with the current network. The propagated knowledge can then be ensembled to form a better soft target for distillation. In this way, our BAKE framework achieves online knowledge ensembling across multiple samples with only a single network. It requires minimal computational and memory overhead compared to existing knowledge ensembling methods. Extensive experiments demonstrate that the lightweight yet effective BAKE consistently boosts the classification performance of various architectures on multiple datasets, e.g., a significant +0.7% gain of Swin-T on ImageNet with only +1.5% computational overhead and zero additional parameters. BAKE does not only improve the vanilla baselines, but also surpasses the single-network state-of-the-arts on all the benchmarks.",
            "corpus_id": 233407431,
            "sentences": [
                {
                    "corpus_id": "233407431",
                    "title": "Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification",
                    "text": "Knowledge distillation [21] can be considered as regularizing the training of the student network using soft targets that carry the \"dark knowledge\" of the teacher network. \n\nThe student network generally consists of a backbone encoder F and a classifier C to perform classification. For each training sample x, its logit vector is encoded as z = C(F (x)). The predictive probability vector p \u03c4 can be obtained via a softmax function on the logits, i.e., the probability of class k can be formulated as \n\nwhere \u03c4 is a temperature hyper-parameter, and K is the number of total classes. Let y \u2208 {1, . . . , K} denotes the ground truth label and q \u03c4 is the soft target produced by the teacher network. The cross-entropy loss and the KL divergence between the predictions and soft targets are minimized jointly to train the student via \n\nwhere p(y) denotes the probability normalized without a temperature, and \u03bb weights the two terms. \n\nRecent works [14,28,40,47] found that ensembling diverse \"dark knowledge\" from multiple teachers or students can form better soft targets, leading to better final performance (see Figure 2 (a)&(b) for details). However, this strategy would increase much more computational and memory overhead to enable multiple networks or branches training. To tackle the challenge, we introduce batch knowledge ensembling in a single network via self-distillation, as illustrated in Figure 2 (c).",
                    "score": 0.6820748678385891,
                    "section_title": "Revisit of Knowledge Distillation",
                    "char_start_offset": 8697,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 175,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 356
                        },
                        {
                            "start": 357,
                            "end": 502
                        },
                        {
                            "start": 505,
                            "end": 584
                        },
                        {
                            "start": 585,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 698
                        },
                        {
                            "start": 699,
                            "end": 831
                        },
                        {
                            "start": 834,
                            "end": 931
                        },
                        {
                            "start": 934,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1276
                        },
                        {
                            "start": 1277,
                            "end": 1416
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 23,
                            "end": 27,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 947,
                            "end": 951,
                            "matchedPaperCorpusId": "219965421"
                        },
                        {
                            "start": 951,
                            "end": 954,
                            "matchedPaperCorpusId": "48352434"
                        },
                        {
                            "start": 954,
                            "end": 957,
                            "matchedPaperCorpusId": "54447578"
                        },
                        {
                            "start": 957,
                            "end": 960,
                            "matchedPaperCorpusId": "204838340"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.794921875
                }
            ],
            "relevance_judgement": 0.794921875,
            "relevance_judgment_input_expanded": "# Title: Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification\n# Venue: arXiv.org\n# Authors: Yixiao Ge, Ching Lam Choi, Xiao Zhang, Peipei Zhao, Feng Zhu, Rui Zhao, Hongsheng Li\n## Abstract\nThe recent studies of knowledge distillation have discovered that ensembling the\"dark knowledge\"from multiple teachers or students contributes to creating better soft targets for training, but at the cost of significantly more computations and/or parameters. In this work, we present BAtch Knowledge Ensembling (BAKE) to produce refined soft targets for anchor images by propagating and ensembling the knowledge of the other samples in the same mini-batch. Specifically, for each sample of interest, the propagation of knowledge is weighted in accordance with the inter-sample affinities, which are estimated on-the-fly with the current network. The propagated knowledge can then be ensembled to form a better soft target for distillation. In this way, our BAKE framework achieves online knowledge ensembling across multiple samples with only a single network. It requires minimal computational and memory overhead compared to existing knowledge ensembling methods. Extensive experiments demonstrate that the lightweight yet effective BAKE consistently boosts the classification performance of various architectures on multiple datasets, e.g., a significant +0.7% gain of Swin-T on ImageNet with only +1.5% computational overhead and zero additional parameters. BAKE does not only improve the vanilla baselines, but also surpasses the single-network state-of-the-arts on all the benchmarks.\n## Revisit of Knowledge Distillation\nKnowledge distillation [21] can be considered as regularizing the training of the student network using soft targets that carry the \"dark knowledge\" of the teacher network. \n\nThe student network generally consists of a backbone encoder F and a classifier C to perform classification. For each training sample x, its logit vector is encoded as z = C(F (x)). The predictive probability vector p \u03c4 can be obtained via a softmax function on the logits, i.e., the probability of class k can be formulated as \n\nwhere \u03c4 is a temperature hyper-parameter, and K is the number of total classes. Let y \u2208 {1, . . . , K} denotes the ground truth label and q \u03c4 is the soft target produced by the teacher network. The cross-entropy loss and the KL divergence between the predictions and soft targets are minimized jointly to train the student via \n\nwhere p(y) denotes the probability normalized without a temperature, and \u03bb weights the two terms. \n\nRecent works [14,28,40,47] found that ensembling diverse \"dark knowledge\" from multiple teachers or students can form better soft targets, leading to better final performance (see Figure 2 (a)&(b) for details). However, this strategy would increase much more computational and memory overhead to enable multiple networks or branches training. To tackle the challenge, we introduce batch knowledge ensembling in a single network via self-distillation, as illustrated in Figure 2 (c).",
            "reference_string": "[233407431 | Ge et al. | 2021 | Citations: 26]"
        },
        {
            "title": "PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models",
            "venue": "IEEE Conference on High Performance Extreme Computing",
            "year": 2023,
            "reference_count": 32,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13441, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2276489527",
                    "name": "Neelesh Gupta"
                },
                {
                    "authorId": "2110123989",
                    "name": "Pengmiao Zhang"
                },
                {
                    "authorId": "2243055156",
                    "name": "Rajgopal Kannan"
                },
                {
                    "authorId": "2202866066",
                    "name": "Viktor K. Prasanna"
                }
            ],
            "abstract": "Deep neural networks (DNNs) have proven to be effective models for accurate Memory Access Prediction (MAP), a critical task in mitigating memory latency through data prefetching. However, existing DNN-based MAP models suffer from the challenges such as significant physical storage space and poor inference latency, primarily due to their large number of parameters. These limitations render them impractical for deployment in real-world scenarios. In this paper, we propose PaCKD, a Pattern-Clustered Knowledge Distillation approach to compress MAP models while maintaining the prediction performance. The PaCKD approach encompasses three steps: clustering memory access sequences into distinct partitions involving similar patterns, training large pattern-specific teacher models for memory access prediction for each partition, and training a single lightweight student model by distilling the knowledge from the trained pattern-specific teachers. We evaluate our approach on LSTM, MLP-Mixer, and ResNet models, as they exhibit diverse structures and are widely used for image classification tasks in order to test their effectiveness in four widely used graph applications. Compared to the teacher models with 5.406M parameters and an F1-score of 0.4626, our student models achieve a 552 x model size compression while maintaining an Fl-score of 0.4538 (with a 1.92% performance drop). Our approach yields an 8.70 % higher result compared to student models trained with standard knowledge distillation and an 8.88 % higher result compared to student models trained without any form of knowledge distillation.",
            "corpus_id": 266555759,
            "sentences": [
                {
                    "corpus_id": "266555759",
                    "title": "PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models",
                    "text": "Knowledge Distillation aims to transfer knowledge from a large, complex model (referred to as the teacher model) to a smaller, more lightweight model (referred to as the student model) [15]. The student model is trained to mimic the behavior and predictions of the teacher model by learning the embedded information in the teacher's logits. This process  has been shown to be effective in improving the performance of compact models in hopes of achieving comparable performance to their larger counterparts. The problem definition of using KD for model compression is as below. Problem Definition. Given a trained large teacher network f (x; \u03b8), where x is the input to the network and \u03b8 is the parameters. The goal of knowledge distillation is to learn a new set of parameters \u03b8 \u2032 for a shallower student network f (x; \u03b8 \u2032 ), such that the student network achieves similar performance to the teacher, with much lower computational cost. \n\nThe original knowledge distillation work primarily focuses on single-label classification. In this context, a custom loss function L is designed that considers both the hard labels from ground truth and soft labels from a teacher model. Importantly, the soft labels are obtained using a T-temperature softmax activation function. \n\nThe softmax activation function returns probabilities P (z i , T ) is defined as: \n\nTherefore, the complete loss L can be represented as: \n\nL = \u03b1L soft + \u03b2L hard (6) where \u03b1 and \u03b2 are hyperparameters, t i are the logits from the teacher model, s i are the logits from the student model, c i is the ground truth for the i-th instance, N is the total number of instances, and T is the temperature parameter for softmax. \n\nIn this work, we extend the original KD approach by training pattern-specific teacher models and designing a softsigmoid activation function for multi-label knowledge distillation (Section III-D1). By implementing the proposed approach in memory access prediction models, achieving efficient compression becomes possible by transferring the expertise of a large and intricate teacher model to a smaller and more streamlined student model. This approach allows for significant memory savings without compromising on performance and accuracy, as the student model can closely match the capabilities of the teacher model [25].",
                    "score": 0.6024818950671004,
                    "section_title": "C. Knowledge Distillation",
                    "char_start_offset": 8704,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 190
                        },
                        {
                            "start": 191,
                            "end": 340
                        },
                        {
                            "start": 341,
                            "end": 507
                        },
                        {
                            "start": 508,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 597
                        },
                        {
                            "start": 598,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 937
                        },
                        {
                            "start": 940,
                            "end": 1030
                        },
                        {
                            "start": 1031,
                            "end": 1176
                        },
                        {
                            "start": 1177,
                            "end": 1269
                        },
                        {
                            "start": 1272,
                            "end": 1353
                        },
                        {
                            "start": 1356,
                            "end": 1409
                        },
                        {
                            "start": 1412,
                            "end": 1689
                        },
                        {
                            "start": 1692,
                            "end": 1889
                        },
                        {
                            "start": 1890,
                            "end": 2130
                        },
                        {
                            "start": 2131,
                            "end": 2315
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1434,
                            "end": 1437,
                            "matchedPaperCorpusId": "3759373"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79345703125
                }
            ],
            "relevance_judgement": 0.79345703125,
            "relevance_judgment_input_expanded": "# Title: PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models\n# Venue: IEEE Conference on High Performance Extreme Computing\n# Authors: Neelesh Gupta, Pengmiao Zhang, Rajgopal Kannan, Viktor K. Prasanna\n## Abstract\nDeep neural networks (DNNs) have proven to be effective models for accurate Memory Access Prediction (MAP), a critical task in mitigating memory latency through data prefetching. However, existing DNN-based MAP models suffer from the challenges such as significant physical storage space and poor inference latency, primarily due to their large number of parameters. These limitations render them impractical for deployment in real-world scenarios. In this paper, we propose PaCKD, a Pattern-Clustered Knowledge Distillation approach to compress MAP models while maintaining the prediction performance. The PaCKD approach encompasses three steps: clustering memory access sequences into distinct partitions involving similar patterns, training large pattern-specific teacher models for memory access prediction for each partition, and training a single lightweight student model by distilling the knowledge from the trained pattern-specific teachers. We evaluate our approach on LSTM, MLP-Mixer, and ResNet models, as they exhibit diverse structures and are widely used for image classification tasks in order to test their effectiveness in four widely used graph applications. Compared to the teacher models with 5.406M parameters and an F1-score of 0.4626, our student models achieve a 552 x model size compression while maintaining an Fl-score of 0.4538 (with a 1.92% performance drop). Our approach yields an 8.70 % higher result compared to student models trained with standard knowledge distillation and an 8.88 % higher result compared to student models trained without any form of knowledge distillation.\n## C. Knowledge Distillation\nKnowledge Distillation aims to transfer knowledge from a large, complex model (referred to as the teacher model) to a smaller, more lightweight model (referred to as the student model) [15]. The student model is trained to mimic the behavior and predictions of the teacher model by learning the embedded information in the teacher's logits. This process  has been shown to be effective in improving the performance of compact models in hopes of achieving comparable performance to their larger counterparts. The problem definition of using KD for model compression is as below. Problem Definition. Given a trained large teacher network f (x; \u03b8), where x is the input to the network and \u03b8 is the parameters. The goal of knowledge distillation is to learn a new set of parameters \u03b8 \u2032 for a shallower student network f (x; \u03b8 \u2032 ), such that the student network achieves similar performance to the teacher, with much lower computational cost. \n\nThe original knowledge distillation work primarily focuses on single-label classification. In this context, a custom loss function L is designed that considers both the hard labels from ground truth and soft labels from a teacher model. Importantly, the soft labels are obtained using a T-temperature softmax activation function. \n\nThe softmax activation function returns probabilities P (z i , T ) is defined as: \n\nTherefore, the complete loss L can be represented as: \n\nL = \u03b1L soft + \u03b2L hard (6) where \u03b1 and \u03b2 are hyperparameters, t i are the logits from the teacher model, s i are the logits from the student model, c i is the ground truth for the i-th instance, N is the total number of instances, and T is the temperature parameter for softmax. \n\nIn this work, we extend the original KD approach by training pattern-specific teacher models and designing a softsigmoid activation function for multi-label knowledge distillation (Section III-D1). By implementing the proposed approach in memory access prediction models, achieving efficient compression becomes possible by transferring the expertise of a large and intricate teacher model to a smaller and more streamlined student model. This approach allows for significant memory savings without compromising on performance and accuracy, as the student model can closely match the capabilities of the teacher model [25].",
            "reference_string": "[266555759 | Gupta et al. | 2023 | Citations: 4]"
        },
        {
            "title": "Label driven Knowledge Distillation for Federated Learning with non-IID Data",
            "venue": "",
            "year": 2022,
            "reference_count": 69,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.14520, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2162621330",
                    "name": "Minh-Duong Nguyen"
                },
                {
                    "authorId": "145436642",
                    "name": "Viet Quoc Pham"
                },
                {
                    "authorId": "2233724",
                    "name": "D. Hoang"
                },
                {
                    "authorId": "1389575160",
                    "name": "Long Tran-Thanh"
                },
                {
                    "authorId": "30479205",
                    "name": "Diep N. Nguyen"
                },
                {
                    "authorId": "2054135428",
                    "name": "W. Hwang"
                }
            ],
            "abstract": "In real-world applications, Federated Learning (FL) meets two challenges: (1) scalability, especially when applied to massive IoT networks; and (2) how to be robust against an environment with heterogeneous data. Realizing the first problem, we aim to design a novel FL framework named Full-stack FL (F2L). More specifically, F2L utilizes a hierarchical network architecture, making extending the FL network accessible without reconstructing the whole network system. Moreover, leveraging the advantages of hierarchical network design, we propose a new label-driven knowledge distillation (LKD) technique at the global server to address the second problem. As opposed to current knowledge distillation techniques, LKD is capable of training a student model, which consists of good knowledge from all teachers' models. Therefore, our proposed algorithm can effectively extract the knowledge of the regions' data distribution (i.e., the regional aggregated models) to reduce the divergence between clients' models when operating under the FL system with non-independent identically distributed data. Extensive experiment results reveal that: (i) our F2L method can significantly improve the overall FL efficiency in all global distillations, and (ii) F2L rapidly achieves convergence as global distillation stages occur instead of increasing on each communication cycle.",
            "corpus_id": 252596141,
            "sentences": [
                {
                    "corpus_id": "252596141",
                    "title": "Label driven Knowledge Distillation for Federated Learning with non-IID Data",
                    "text": "Knowledge Distillation (KD) Hinton et al. (2015) employs a technique to transfer the learned knowledge from a pre-trained teacher model to another model with less or similar complexity (student model). The model transfer process is implemented in two steps. In the first step, a surrogate output probability function, namely a temperature-softmax function, is utilized. By adding a temperature scaling variable T , the conventional softmax function then becomes: \n\nwhere z j is the output set corresponding to class j of the given DNN with batch of input data X. \n\nThe subscript l denotes the index of softmax output which corresponds to the prediction on class l of the DNN. The intention of adding the variable T is to adjust the slope of the softmax function in the classifier as shown in equation 16. As we can see, when we increase the value of T , the slope of the softmax function will decrease significantly. With large temperature scale values, over the same output range of data, the range of values represented by input z j is larger. Then, the output value tuple created by the teacher and student carries considerably more information. Therefore, the learning process between teacher and student is more effective. \n\nIn the second step, to help implement the transfer of knowledge from teacher to student, the authors in Hinton et al. (2015) presented a new loss function, called distillation loss function (also known as soft-loss function). This loss function comprises two terms: the intrinsic entropy function of the teacher and the cross-entropy function between the teacher and the student's outputs, which can be expressed as follows: \n\nThe purpose of this function is to compare the output distribution between teacher p(X) and student model q(X). In terms of information theory, this measurement shows the under-performance of the distribution set created by student, when the output distribution set of the teacher is taken as the sample distribution set, with precision of 100%. By minimizing this function, we reduce the functional difference between the two deep networks. As a result, the student model tends to make it's output distribution become more identical to the teacher model's behavior.",
                    "score": 0.5806646816246094,
                    "section_title": "A.2 KNOWLEDGE DISTILLATION",
                    "char_start_offset": 25127,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 201
                        },
                        {
                            "start": 202,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 369
                        },
                        {
                            "start": 370,
                            "end": 462
                        },
                        {
                            "start": 465,
                            "end": 562
                        },
                        {
                            "start": 565,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 804
                        },
                        {
                            "start": 805,
                            "end": 916
                        },
                        {
                            "start": 917,
                            "end": 1045
                        },
                        {
                            "start": 1046,
                            "end": 1148
                        },
                        {
                            "start": 1149,
                            "end": 1227
                        },
                        {
                            "start": 1230,
                            "end": 1455
                        },
                        {
                            "start": 1456,
                            "end": 1654
                        },
                        {
                            "start": 1657,
                            "end": 1768
                        },
                        {
                            "start": 1769,
                            "end": 2002
                        },
                        {
                            "start": 2003,
                            "end": 2098
                        },
                        {
                            "start": 2099,
                            "end": 2223
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.791015625
                }
            ],
            "relevance_judgement": 0.791015625,
            "relevance_judgment_input_expanded": "# Title: Label driven Knowledge Distillation for Federated Learning with non-IID Data\n# Venue: \n# Authors: Minh-Duong Nguyen, Viet Quoc Pham, D. Hoang, Long Tran-Thanh, Diep N. Nguyen, W. Hwang\n## Abstract\nIn real-world applications, Federated Learning (FL) meets two challenges: (1) scalability, especially when applied to massive IoT networks; and (2) how to be robust against an environment with heterogeneous data. Realizing the first problem, we aim to design a novel FL framework named Full-stack FL (F2L). More specifically, F2L utilizes a hierarchical network architecture, making extending the FL network accessible without reconstructing the whole network system. Moreover, leveraging the advantages of hierarchical network design, we propose a new label-driven knowledge distillation (LKD) technique at the global server to address the second problem. As opposed to current knowledge distillation techniques, LKD is capable of training a student model, which consists of good knowledge from all teachers' models. Therefore, our proposed algorithm can effectively extract the knowledge of the regions' data distribution (i.e., the regional aggregated models) to reduce the divergence between clients' models when operating under the FL system with non-independent identically distributed data. Extensive experiment results reveal that: (i) our F2L method can significantly improve the overall FL efficiency in all global distillations, and (ii) F2L rapidly achieves convergence as global distillation stages occur instead of increasing on each communication cycle.\n## A.2 KNOWLEDGE DISTILLATION\nKnowledge Distillation (KD) Hinton et al. (2015) employs a technique to transfer the learned knowledge from a pre-trained teacher model to another model with less or similar complexity (student model). The model transfer process is implemented in two steps. In the first step, a surrogate output probability function, namely a temperature-softmax function, is utilized. By adding a temperature scaling variable T , the conventional softmax function then becomes: \n\nwhere z j is the output set corresponding to class j of the given DNN with batch of input data X. \n\nThe subscript l denotes the index of softmax output which corresponds to the prediction on class l of the DNN. The intention of adding the variable T is to adjust the slope of the softmax function in the classifier as shown in equation 16. As we can see, when we increase the value of T , the slope of the softmax function will decrease significantly. With large temperature scale values, over the same output range of data, the range of values represented by input z j is larger. Then, the output value tuple created by the teacher and student carries considerably more information. Therefore, the learning process between teacher and student is more effective. \n\nIn the second step, to help implement the transfer of knowledge from teacher to student, the authors in Hinton et al. (2015) presented a new loss function, called distillation loss function (also known as soft-loss function). This loss function comprises two terms: the intrinsic entropy function of the teacher and the cross-entropy function between the teacher and the student's outputs, which can be expressed as follows: \n\nThe purpose of this function is to compare the output distribution between teacher p(X) and student model q(X). In terms of information theory, this measurement shows the under-performance of the distribution set created by student, when the output distribution set of the teacher is taken as the sample distribution set, with precision of 100%. By minimizing this function, we reduce the functional difference between the two deep networks. As a result, the student model tends to make it's output distribution become more identical to the teacher model's behavior.",
            "reference_string": "[252596141 | Nguyen et al. | 2022 | Citations: 2]"
        },
        {
            "title": "Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2021,
            "reference_count": 52,
            "citation_count": 64,
            "influential_citation_count": 17,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2103.04705",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.04705, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3461777",
                    "name": "Shuaijun Chen"
                },
                {
                    "authorId": "1641711590",
                    "name": "Xu Jia"
                },
                {
                    "authorId": "12874945",
                    "name": "Jianzhong He"
                },
                {
                    "authorId": "6658208",
                    "name": "Yongjie Shi"
                },
                {
                    "authorId": "2144167531",
                    "name": "Jianzhuang Liu"
                }
            ],
            "abstract": "Data-driven based approaches, in spite of great success in many tasks, have poor generalization when applied to unseen image domains, and require expensive cost of annotation especially for dense pixel prediction tasks such as semantic segmentation. Recently, both unsupervised domain adaptation (UDA) from large amounts of synthetic data and semi-supervised learning (SSL) with small set of labeled data have been studied to alleviate this issue. However, there is still a large gap on performance compared to their supervised counterparts. We focus on a more practical setting of semi-supervised domain adaptation (SSDA) where both a small set of labeled target data and large amounts of labeled source data are available. To address the task of SSDA, a novel framework based on dual-level domain mixing is proposed. The proposed framework consists of three stages. First, two kinds of data mixing methods are proposed to reduce domain gap in both region-level and sample-level respectively. We can obtain two complementary domain-mixed teachers based on dual-level mixed data from holistic and partial views respectively. Then, a student model is learned by distilling knowledge from these two teachers. Finally, pseudo labels of unlabeled data are generated in a self-training manner for another few rounds of teachers training. Extensive experimental results have demonstrated the effectiveness of our proposed framework on synthetic-to-real semantic segmentation benchmarks.",
            "corpus_id": 232147231,
            "sentences": [
                {
                    "corpus_id": "232147231",
                    "title": "Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation",
                    "text": "After obtaining two pre-trained domain-mixed teachers, we employ knowledge distillation (KD), a technique to distilling knowledge by minimizing the KL-divergence between outputs of these two models. Here we adapt it to extract \"dark knowledge\" from these two complementary teachers. The pipeline of multi-teacher KD is shown in Fig. 1 (b), including two pre-trained domain-mixed teachers and one student with the same network architecture as teacher. The outputs of two teachers are ensembled as a stronger guidance to supervise the training of the student model on unlabeled target data. Besides, the student model is also supervised by the labels on the small amount of labeled target data. The objective function of learning student M S is defined as below. \n\nwhere \u03bb kl and \u03bb ce are the weights of KL-divergence loss and cross entropy loss respectively, E denotes the ensemble operation of two models. In experiments, the ensemble operation is implemented by averaging the outputs of two complementary teachers. By integrating knowledge from two views and making full use of unlabeled data, we can obtain one student with even superior performance than any one of its teachers.",
                    "score": 0.6462930549294406,
                    "section_title": "Multi-teacher Knowledge Distillation",
                    "char_start_offset": 13820,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 198
                        },
                        {
                            "start": 199,
                            "end": 282
                        },
                        {
                            "start": 283,
                            "end": 450
                        },
                        {
                            "start": 451,
                            "end": 588
                        },
                        {
                            "start": 589,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 760
                        },
                        {
                            "start": 763,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 1015
                        },
                        {
                            "start": 1016,
                            "end": 1181
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78564453125
                }
            ],
            "relevance_judgement": 0.78564453125,
            "relevance_judgment_input_expanded": "# Title: Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Shuaijun Chen, Xu Jia, Jianzhong He, Yongjie Shi, Jianzhuang Liu\n## Abstract\nData-driven based approaches, in spite of great success in many tasks, have poor generalization when applied to unseen image domains, and require expensive cost of annotation especially for dense pixel prediction tasks such as semantic segmentation. Recently, both unsupervised domain adaptation (UDA) from large amounts of synthetic data and semi-supervised learning (SSL) with small set of labeled data have been studied to alleviate this issue. However, there is still a large gap on performance compared to their supervised counterparts. We focus on a more practical setting of semi-supervised domain adaptation (SSDA) where both a small set of labeled target data and large amounts of labeled source data are available. To address the task of SSDA, a novel framework based on dual-level domain mixing is proposed. The proposed framework consists of three stages. First, two kinds of data mixing methods are proposed to reduce domain gap in both region-level and sample-level respectively. We can obtain two complementary domain-mixed teachers based on dual-level mixed data from holistic and partial views respectively. Then, a student model is learned by distilling knowledge from these two teachers. Finally, pseudo labels of unlabeled data are generated in a self-training manner for another few rounds of teachers training. Extensive experimental results have demonstrated the effectiveness of our proposed framework on synthetic-to-real semantic segmentation benchmarks.\n## Multi-teacher Knowledge Distillation\nAfter obtaining two pre-trained domain-mixed teachers, we employ knowledge distillation (KD), a technique to distilling knowledge by minimizing the KL-divergence between outputs of these two models. Here we adapt it to extract \"dark knowledge\" from these two complementary teachers. The pipeline of multi-teacher KD is shown in Fig. 1 (b), including two pre-trained domain-mixed teachers and one student with the same network architecture as teacher. The outputs of two teachers are ensembled as a stronger guidance to supervise the training of the student model on unlabeled target data. Besides, the student model is also supervised by the labels on the small amount of labeled target data. The objective function of learning student M S is defined as below. \n\nwhere \u03bb kl and \u03bb ce are the weights of KL-divergence loss and cross entropy loss respectively, E denotes the ensemble operation of two models. In experiments, the ensemble operation is implemented by averaging the outputs of two complementary teachers. By integrating knowledge from two views and making full use of unlabeled data, we can obtain one student with even superior performance than any one of its teachers.",
            "reference_string": "[232147231 | Chen et al. | 2021 | Citations: 64]"
        },
        {
            "title": "Multistage feature fusion knowledge distillation",
            "venue": "Scientific Reports",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41598-024-64041-4.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11166915, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2307186661",
                    "name": "Gang Li"
                },
                {
                    "authorId": "2307436738",
                    "name": "Kun Wang"
                },
                {
                    "authorId": "2305765306",
                    "name": "Pengfei Lv"
                },
                {
                    "authorId": "2305752302",
                    "name": "Pan He"
                },
                {
                    "authorId": "2287501699",
                    "name": "Zheng Zhou"
                },
                {
                    "authorId": "2817613",
                    "name": "Chuanyun Xu"
                }
            ],
            "abstract": "Generally, the recognition performance of lightweight models is often lower than that of large models. Knowledge distillation, by teaching a student model using a teacher model, can further enhance the recognition accuracy of lightweight models. In this paper, we approach knowledge distillation from the perspective of intermediate feature-level knowledge distillation. We combine a cross-stage feature fusion symmetric framework, an attention mechanism to enhance the fused features, and a contrastive loss function for teacher and student models at the same stage to comprehensively implement a multistage feature fusion knowledge distillation method. This approach addresses the problem of significant differences in the intermediate feature distributions between teacher and student models, making it difficult to effectively learn implicit knowledge and thus improving the recognition accuracy of the student model. Compared to existing knowledge distillation methods, our method performs at a superior level. On the CIFAR100 dataset, it boosts the recognition accuracy of ResNet20 from 69.06% to 71.34%, and on the TinyImagenet dataset, it increases the recognition accuracy of ResNet18 from 66.54% to 68.03%, demonstrating the effectiveness and generalizability of our approach. Furthermore, there is room for further optimization of the overall distillation structure and feature extraction methods in this approach, which requires further research and exploration.",
            "corpus_id": 270389751,
            "sentences": [
                {
                    "corpus_id": "270389751",
                    "title": "Multistage feature fusion knowledge distillation",
                    "text": "Knowledge distillation (KD), as initially proposed by Hinton et al. 10 , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge.This approach has given rise to variations, including intermediate feature layer distillation and multistage soft label distillation.\n\nIn the context of intermediate feature layer distillation, the challenge of inconsistent multistage feature knowledge distribution is a critical issue.FitNet 11 employs squared distance constraints to measure the similarity of intermediate layer features between teacher and student networks.AT 15 uses multi-layer attention maps to extract features between the teacher network and the student network, and builds a knowledge transfer mechanism between the two.CC 16 proposed a correlation congruence method to reduce the correlation consistency distribution between teachers and students across multiple sample instances, and improve the distribution consistency between student models and teacher models in the classification output of multiple instances.AB 17 proposed a method for knowledge transfer by extracting the activation boundaries formed by hidden neurons, which enables students to learn the separation boundaries between different activation regions formed by each neuron in the teacher, thereby reducing the differences between the student network and the teacher network.FT 18 proposed two convolutional modules, the reader and the translator, which are used to extract the feature information of teachers and the translator to extract the feature information of students.Through distillation training, the differences between the two modules are reduced, achieving the imitation and learning of the teacher network by www.nature.com/scientificreports/ the student network.NST 19 proposed a new KT loss function to minimize the maximum average difference in neuron feature distribution between the teacher model and the student model, significantly improving the performance of the student model.CRD 20 is a knowledge distillation method based on contrastive learning, which preserves mutual information between teachers and students by optimizing the distillation loss function.OFD 21 uses a novel distance function and edge residual function to distill essential information between teacher and student networks.ReviewKD 22 utilizes a multilevel composite knowledge approach to transfer dark knowledge at the feature level, achieving state-of-the-art performance.",
                    "score": 0.6878154477609052,
                    "section_title": "Knowledge distillation",
                    "char_start_offset": 2815,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 194,
                            "end": 356
                        },
                        {
                            "start": 356,
                            "end": 489
                        },
                        {
                            "start": 491,
                            "end": 642
                        },
                        {
                            "start": 642,
                            "end": 783
                        },
                        {
                            "start": 783,
                            "end": 952
                        },
                        {
                            "start": 952,
                            "end": 1248
                        },
                        {
                            "start": 1248,
                            "end": 1579
                        },
                        {
                            "start": 1579,
                            "end": 1780
                        },
                        {
                            "start": 1780,
                            "end": 1981
                        },
                        {
                            "start": 1981,
                            "end": 2204
                        },
                        {
                            "start": 2204,
                            "end": 2387
                        },
                        {
                            "start": 2387,
                            "end": 2522
                        },
                        {
                            "start": 2522,
                            "end": 2673
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 68,
                            "end": 70,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 955,
                            "end": 957,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 1251,
                            "end": 1253,
                            "matchedPaperCorpusId": "53213211"
                        },
                        {
                            "start": 1582,
                            "end": 1584,
                            "matchedPaperCorpusId": "3608236"
                        },
                        {
                            "start": 2208,
                            "end": 2210,
                            "matchedPaperCorpusId": "204838340"
                        },
                        {
                            "start": 2391,
                            "end": 2393,
                            "matchedPaperCorpusId": "102483181"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.783203125
                }
            ],
            "relevance_judgement": 0.783203125,
            "relevance_judgment_input_expanded": "# Title: Multistage feature fusion knowledge distillation\n# Venue: Scientific Reports\n# Authors: Gang Li, Kun Wang, Pengfei Lv, Pan He, Zheng Zhou, Chuanyun Xu\n## Abstract\nGenerally, the recognition performance of lightweight models is often lower than that of large models. Knowledge distillation, by teaching a student model using a teacher model, can further enhance the recognition accuracy of lightweight models. In this paper, we approach knowledge distillation from the perspective of intermediate feature-level knowledge distillation. We combine a cross-stage feature fusion symmetric framework, an attention mechanism to enhance the fused features, and a contrastive loss function for teacher and student models at the same stage to comprehensively implement a multistage feature fusion knowledge distillation method. This approach addresses the problem of significant differences in the intermediate feature distributions between teacher and student models, making it difficult to effectively learn implicit knowledge and thus improving the recognition accuracy of the student model. Compared to existing knowledge distillation methods, our method performs at a superior level. On the CIFAR100 dataset, it boosts the recognition accuracy of ResNet20 from 69.06% to 71.34%, and on the TinyImagenet dataset, it increases the recognition accuracy of ResNet18 from 66.54% to 68.03%, demonstrating the effectiveness and generalizability of our approach. Furthermore, there is room for further optimization of the overall distillation structure and feature extraction methods in this approach, which requires further research and exploration.\n## Knowledge distillation\nKnowledge distillation (KD), as initially proposed by Hinton et al. 10 , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge.This approach has given rise to variations, including intermediate feature layer distillation and multistage soft label distillation.\n\nIn the context of intermediate feature layer distillation, the challenge of inconsistent multistage feature knowledge distribution is a critical issue.FitNet 11 employs squared distance constraints to measure the similarity of intermediate layer features between teacher and student networks.AT 15 uses multi-layer attention maps to extract features between the teacher network and the student network, and builds a knowledge transfer mechanism between the two.CC 16 proposed a correlation congruence method to reduce the correlation consistency distribution between teachers and students across multiple sample instances, and improve the distribution consistency between student models and teacher models in the classification output of multiple instances.AB 17 proposed a method for knowledge transfer by extracting the activation boundaries formed by hidden neurons, which enables students to learn the separation boundaries between different activation regions formed by each neuron in the teacher, thereby reducing the differences between the student network and the teacher network.FT 18 proposed two convolutional modules, the reader and the translator, which are used to extract the feature information of teachers and the translator to extract the feature information of students.Through distillation training, the differences between the two modules are reduced, achieving the imitation and learning of the teacher network by www.nature.com/scientificreports/ the student network.NST 19 proposed a new KT loss function to minimize the maximum average difference in neuron feature distribution between the teacher model and the student model, significantly improving the performance of the student model.CRD 20 is a knowledge distillation method based on contrastive learning, which preserves mutual information between teachers and students by optimizing the distillation loss function.OFD 21 uses a novel distance function and edge residual function to distill essential information between teacher and student networks.ReviewKD 22 utilizes a multilevel composite knowledge approach to transfer dark knowledge at the feature level, achieving state-of-the-art performance.",
            "reference_string": "[270389751 | Li et al. | 2024 | Citations: 1]"
        },
        {
            "title": "T3DNet: Compressing Point Cloud Models for Lightweight 3-D Recognition",
            "venue": "IEEE Transactions on Cybernetics",
            "year": 2024,
            "reference_count": 66,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2402.19264",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.19264, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2289702887",
                    "name": "Zhiyuan Yang"
                },
                {
                    "authorId": "2118117287",
                    "name": "Yunjiao Zhou"
                },
                {
                    "authorId": "2152786430",
                    "name": "Lihua Xie"
                },
                {
                    "authorId": "2249754996",
                    "name": "Jianfei Yang"
                }
            ],
            "abstract": "The 3-D point cloud has been widely used in many mobile application scenarios, including autonomous driving and 3-D sensing on mobile devices. However, existing 3-D point cloud models tend to be large and cumbersome, making them hard to deploy on edged devices due to their high memory requirements and nonreal-time latency. There has been a lack of research on how to compress 3-D point cloud models into lightweight models. In this article, we propose a method called T3DNet (tiny 3-D network with augmentation and distillation) to address this issue. We find that the tiny model after network augmentation is much easier for a teacher to distill. Instead of gradually reducing the parameters through techniques, such as pruning or quantization, we predefine a tiny model and improve its performance through auxiliary supervision from augmented networks and the original model. We evaluate our method on several public datasets, including ModelNet40, ShapeNet, and ScanObjectNN. Our method can achieve high compression rates without significant accuracy sacrifice, achieving state-of-the-art performances on three datasets against existing methods. Amazingly, our T3DNet is <inline-formula> <tex-math notation=\"LaTeX\">$58\\times $ </tex-math></inline-formula> smaller and <inline-formula> <tex-math notation=\"LaTeX\">$54\\times $ </tex-math></inline-formula> faster than the original model yet with only 1.4% accuracy descent on the ModelNet40 dataset. Our code is available at <uri>https://github.com/Zhiyuan002/T3DNet</uri>.",
            "corpus_id": 268063805,
            "sentences": [
                {
                    "corpus_id": "268063805",
                    "title": "T3DNet: Compressing Point Cloud Models for Lightweight 3-D Recognition",
                    "text": "The basic idea of knowledge distillation (KD) [21] is to transfer knowledge from a large, powerful teacher network to a small, lightweight student network by learning the soft class distributions after the SoftMax layer. The student network then mimics the teacher model to improve its performance. Deploying deep learning models on embedded systems or mobile robots can be challenging due to their limited computational capacity and memory. Knowledge distillation offers a solution to this problem by allowing us to carefully allocate resources such as memory and computation. For real-time human-robot interaction applications, knowledge distillation is a promising research focus [51]- [53]. \n\nLogit-based knowledge typically refers to the output of the softmax layer of the teacher model. Early, or \"vanilla,\" knowledge distillation methods used the logits of a deep, complex model as \"dark knowledge\" [21]. Logit-based knowledge distillation is a simple but effective approach for achieving good performance in compression tasks and has gained popularity in solving various tasks and applications. The most popular logit-based knowledge for classification tasks is the soft target [21], [54]. In this approach, the \"dark knowledge\" is usually evaluated using the Kullback-Leibler divergence loss, which minimizes the difference between the student and teacher logits. The vanilla approach to KD involves joint training of the distillation and student losses. The student loss is typically set as the cross-entropy loss between the ground truth and the softmax class prediction of the student model. \n\nOne issue with logit-based distillation is that it can be difficult for very deep networks to learn the logit knowledge, and it is also limited to supervised learning. To address these limitations, the concept of representation learning [55] can be applied to knowledge distillation. In representation learning, multiple levels of features with increasing abstraction are learned, which can be particularly helpful for thinner and deeper networks in distillation. FitNet [27] was the first to introduce the concept of intermediate representation in knowledge distillation, using the hidden layers of the teacher model to supervise the learning of the student model. This type of knowledge, derived from feature activations, is known as \"hint\".",
                    "score": 0.5799756061975982,
                    "section_title": "C. Knowledge Distillation",
                    "char_start_offset": 12981,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 220
                        },
                        {
                            "start": 221,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 441
                        },
                        {
                            "start": 442,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 694
                        },
                        {
                            "start": 697,
                            "end": 792
                        },
                        {
                            "start": 793,
                            "end": 911
                        },
                        {
                            "start": 912,
                            "end": 1102
                        },
                        {
                            "start": 1103,
                            "end": 1197
                        },
                        {
                            "start": 1198,
                            "end": 1372
                        },
                        {
                            "start": 1373,
                            "end": 1463
                        },
                        {
                            "start": 1464,
                            "end": 1603
                        },
                        {
                            "start": 1606,
                            "end": 1773
                        },
                        {
                            "start": 1774,
                            "end": 1889
                        },
                        {
                            "start": 1890,
                            "end": 2069
                        },
                        {
                            "start": 2070,
                            "end": 2271
                        },
                        {
                            "start": 2272,
                            "end": 2349
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 683,
                            "end": 687,
                            "matchedPaperCorpusId": "195893309"
                        },
                        {
                            "start": 689,
                            "end": 693,
                            "matchedPaperCorpusId": "238239298"
                        },
                        {
                            "start": 1192,
                            "end": 1196,
                            "matchedPaperCorpusId": "11536917"
                        },
                        {
                            "start": 1843,
                            "end": 1847,
                            "matchedPaperCorpusId": "393948"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78173828125
                }
            ],
            "relevance_judgement": 0.78173828125,
            "relevance_judgment_input_expanded": "# Title: T3DNet: Compressing Point Cloud Models for Lightweight 3-D Recognition\n# Venue: IEEE Transactions on Cybernetics\n# Authors: Zhiyuan Yang, Yunjiao Zhou, Lihua Xie, Jianfei Yang\n## Abstract\nThe 3-D point cloud has been widely used in many mobile application scenarios, including autonomous driving and 3-D sensing on mobile devices. However, existing 3-D point cloud models tend to be large and cumbersome, making them hard to deploy on edged devices due to their high memory requirements and nonreal-time latency. There has been a lack of research on how to compress 3-D point cloud models into lightweight models. In this article, we propose a method called T3DNet (tiny 3-D network with augmentation and distillation) to address this issue. We find that the tiny model after network augmentation is much easier for a teacher to distill. Instead of gradually reducing the parameters through techniques, such as pruning or quantization, we predefine a tiny model and improve its performance through auxiliary supervision from augmented networks and the original model. We evaluate our method on several public datasets, including ModelNet40, ShapeNet, and ScanObjectNN. Our method can achieve high compression rates without significant accuracy sacrifice, achieving state-of-the-art performances on three datasets against existing methods. Amazingly, our T3DNet is <inline-formula> <tex-math notation=\"LaTeX\">$58\\times $ </tex-math></inline-formula> smaller and <inline-formula> <tex-math notation=\"LaTeX\">$54\\times $ </tex-math></inline-formula> faster than the original model yet with only 1.4% accuracy descent on the ModelNet40 dataset. Our code is available at <uri>https://github.com/Zhiyuan002/T3DNet</uri>.\n## C. Knowledge Distillation\nThe basic idea of knowledge distillation (KD) [21] is to transfer knowledge from a large, powerful teacher network to a small, lightweight student network by learning the soft class distributions after the SoftMax layer. The student network then mimics the teacher model to improve its performance. Deploying deep learning models on embedded systems or mobile robots can be challenging due to their limited computational capacity and memory. Knowledge distillation offers a solution to this problem by allowing us to carefully allocate resources such as memory and computation. For real-time human-robot interaction applications, knowledge distillation is a promising research focus [51]- [53]. \n\nLogit-based knowledge typically refers to the output of the softmax layer of the teacher model. Early, or \"vanilla,\" knowledge distillation methods used the logits of a deep, complex model as \"dark knowledge\" [21]. Logit-based knowledge distillation is a simple but effective approach for achieving good performance in compression tasks and has gained popularity in solving various tasks and applications. The most popular logit-based knowledge for classification tasks is the soft target [21], [54]. In this approach, the \"dark knowledge\" is usually evaluated using the Kullback-Leibler divergence loss, which minimizes the difference between the student and teacher logits. The vanilla approach to KD involves joint training of the distillation and student losses. The student loss is typically set as the cross-entropy loss between the ground truth and the softmax class prediction of the student model. \n\nOne issue with logit-based distillation is that it can be difficult for very deep networks to learn the logit knowledge, and it is also limited to supervised learning. To address these limitations, the concept of representation learning [55] can be applied to knowledge distillation. In representation learning, multiple levels of features with increasing abstraction are learned, which can be particularly helpful for thinner and deeper networks in distillation. FitNet [27] was the first to introduce the concept of intermediate representation in knowledge distillation, using the hidden layers of the teacher model to supervise the learning of the student model. This type of knowledge, derived from feature activations, is known as \"hint\".",
            "reference_string": "[268063805 | Yang et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Effective Online Knowledge Distillation via Attention-Based Model Ensembling",
            "venue": "Mathematics",
            "year": 2022,
            "reference_count": 34,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2227-7390/10/22/4285/pdf?version=1668591858",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/math10224285?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/math10224285, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144583200",
                    "name": "D. Borza"
                },
                {
                    "authorId": "1821352",
                    "name": "A. Darabant"
                },
                {
                    "authorId": "74810447",
                    "name": "Tudor Alexandru Ileni"
                },
                {
                    "authorId": "150020994",
                    "name": "Alexandru-Ion Marinescu"
                }
            ],
            "abstract": "Large-scale deep learning models have achieved impressive results on a variety of tasks; however, their deployment on edge or mobile devices is still a challenge due to the limited available memory and computational capability. Knowledge distillation is an effective model compression technique, which can boost the performance of a lightweight student network by transferring the knowledge from a more complex model or an ensemble of models. Due to its reduced size, this lightweight model is more suitable for deployment on edge devices. In this paper, we introduce an online knowledge distillation framework, which relies on an original attention mechanism to effectively combine the predictions of a cohort of lightweight (student) networks into a powerful ensemble, and use this as a distillation signal. The proposed aggregation strategy uses the predictions of the individual students as well as ground truth data to determine a set of weights needed for ensembling these predictions. This mechanism is solely used during system training. When testing or at inference time, a single, lightweight student is extracted and used. The extensive experiments we performed on several image classification benchmarks, both by training models from scratch (on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets) and using transfer learning (on Oxford Pets and Oxford Flowers datasets), showed that the proposed framework always leads to an improvement in the accuracy of knowledge-distilled students and demonstrates the effectiveness of the proposed solution. Moreover, in the case of ResNet architecture, we observed that the knowledge-distilled model achieves a higher accuracy than a deeper, individually trained ResNet model.",
            "corpus_id": 253655030,
            "sentences": [
                {
                    "corpus_id": "253655030",
                    "title": "Effective Online Knowledge Distillation via Attention-Based Model Ensembling",
                    "text": "Knowledge distillation [3] was proposed as an effective and elegant compression technique to derive a lighter and faster network (student) from a more complex one (teacher), by penalizing the difference between their logits. Later, this mechanism was formalized by [2] to distill the \"dark-knowledge\" from the teacher to the student. The authors noticed that a powerful and confident teacher does not bring more knowledge than ground truth data, as its prediction tends to be a narrow probability distribution with a single peak for the ground truth class. To alleviate this issue, the teacher's logits are \"softened\" by a temperature scaling factor of the softmax activation. In such a manner, the lightweight network can infer what other classes were found similar by the teacher network. More formally, this can be expressed as \n\nwhere \u03c4 is the temperature of the softmax function (equation from [2]). When \u03c4 is greater than 1, the small probabilities of the softmax function are increased and the output is \"softened\". \n\nIn the classical setup, during training, a Kullback-Libeler divergence loss term is employed to ensure that the student network mimics the teacher's softened predictions. Other methods proposed using the root-mean-square error loss [9] or distilling from hardlabels [10].",
                    "score": 0.6547830869863676,
                    "section_title": "Related Work",
                    "char_start_offset": 3724,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 224
                        },
                        {
                            "start": 225,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 556
                        },
                        {
                            "start": 557,
                            "end": 676
                        },
                        {
                            "start": 677,
                            "end": 790
                        },
                        {
                            "start": 791,
                            "end": 830
                        },
                        {
                            "start": 833,
                            "end": 904
                        },
                        {
                            "start": 905,
                            "end": 1022
                        },
                        {
                            "start": 1025,
                            "end": 1195
                        },
                        {
                            "start": 1196,
                            "end": 1296
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 23,
                            "end": 26,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 265,
                            "end": 268,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 899,
                            "end": 902,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1291,
                            "end": 1295,
                            "matchedPaperCorpusId": "229363322"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78076171875
                }
            ],
            "relevance_judgement": 0.78076171875,
            "relevance_judgment_input_expanded": "# Title: Effective Online Knowledge Distillation via Attention-Based Model Ensembling\n# Venue: Mathematics\n# Authors: D. Borza, A. Darabant, Tudor Alexandru Ileni, Alexandru-Ion Marinescu\n## Abstract\nLarge-scale deep learning models have achieved impressive results on a variety of tasks; however, their deployment on edge or mobile devices is still a challenge due to the limited available memory and computational capability. Knowledge distillation is an effective model compression technique, which can boost the performance of a lightweight student network by transferring the knowledge from a more complex model or an ensemble of models. Due to its reduced size, this lightweight model is more suitable for deployment on edge devices. In this paper, we introduce an online knowledge distillation framework, which relies on an original attention mechanism to effectively combine the predictions of a cohort of lightweight (student) networks into a powerful ensemble, and use this as a distillation signal. The proposed aggregation strategy uses the predictions of the individual students as well as ground truth data to determine a set of weights needed for ensembling these predictions. This mechanism is solely used during system training. When testing or at inference time, a single, lightweight student is extracted and used. The extensive experiments we performed on several image classification benchmarks, both by training models from scratch (on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets) and using transfer learning (on Oxford Pets and Oxford Flowers datasets), showed that the proposed framework always leads to an improvement in the accuracy of knowledge-distilled students and demonstrates the effectiveness of the proposed solution. Moreover, in the case of ResNet architecture, we observed that the knowledge-distilled model achieves a higher accuracy than a deeper, individually trained ResNet model.\n## Related Work\nKnowledge distillation [3] was proposed as an effective and elegant compression technique to derive a lighter and faster network (student) from a more complex one (teacher), by penalizing the difference between their logits. Later, this mechanism was formalized by [2] to distill the \"dark-knowledge\" from the teacher to the student. The authors noticed that a powerful and confident teacher does not bring more knowledge than ground truth data, as its prediction tends to be a narrow probability distribution with a single peak for the ground truth class. To alleviate this issue, the teacher's logits are \"softened\" by a temperature scaling factor of the softmax activation. In such a manner, the lightweight network can infer what other classes were found similar by the teacher network. More formally, this can be expressed as \n\nwhere \u03c4 is the temperature of the softmax function (equation from [2]). When \u03c4 is greater than 1, the small probabilities of the softmax function are increased and the output is \"softened\". \n\nIn the classical setup, during training, a Kullback-Libeler divergence loss term is employed to ensure that the student network mimics the teacher's softened predictions. Other methods proposed using the root-mean-square error loss [9] or distilling from hardlabels [10].",
            "reference_string": "[253655030 | Borza et al. | 2022 | Citations: 2]"
        },
        {
            "title": "Maximizing discrimination capability of knowledge distillation with energy function",
            "venue": "Knowledge-Based Systems",
            "year": 2023,
            "reference_count": 57,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2311.14334",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.14334, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268350036",
                    "name": "Seonghak Kim"
                },
                {
                    "authorId": "2156910329",
                    "name": "Gyeongdo Ham"
                },
                {
                    "authorId": "2268370058",
                    "name": "Suin Lee"
                },
                {
                    "authorId": "2268310103",
                    "name": "Donggon Jang"
                },
                {
                    "authorId": "2145154407",
                    "name": "Daeshik Kim"
                }
            ],
            "abstract": null,
            "corpus_id": 267657497,
            "sentences": [
                {
                    "corpus_id": "267657497",
                    "title": "Maximizing discrimination capability of knowledge distillation with energy function",
                    "text": "Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks. Over the years, KD methods have evolved to narrow the performance gap between student and teacher models by utilizing both final predictions, known as logits-based distillation [10,15,16,11,17], and intermediate features, known as features-based distillation [18,19,20,21,22,23,24,12,25,26,27,28]. \n\nPrevious works on logits-based distillations include the following: DML [15] proposed a mutual learning strategy for collaboratively teaching and learning between student and teacher models; TAKD [16] introduced a multi-step method with an intermediate-size network (i.e., assistant network) to bridge the gap between teachers and students; DKD [11] decomposed the soft-label distillation loss into two components: target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD), enabling each part to independently harness its effectiveness; Multi KD [17] proposed multi-level prediction alignment, containing instance, batch, and class levels, and prediction augmentation. While these approaches emphasize effective knowledge transfer, they do not consider dividing entire datasets or provide mechanisms to distinguish and transfer knowledge from specific samples. \n\nFitNet [18] was groundbreaking as it leveraged not only the final outputs but also intermediate representations. Since the introduction of FitNet, various feature-based KD methods have emerged as follows: AT [22] prompted the student to mimic the attention map of the teacher network; PKT [19] employed various kernels to estimate the probability distributions, employing different divergence metrics for distillation; RKD [20] focused on transferring the mutual relations of data samples; CRD [21] framed the objective as contrastive learning for distillation; VID [23] took a different approach by maximizing mutual information; OFD [24] introduced a novel loss function incorporating teacher transform and a new distance function; Review KD [12] introduced a review mechanism that leverages past features for guiding current ones through residual learning.",
                    "score": 0.6679538088471308,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 3764,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 473
                        },
                        {
                            "start": 476,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1372
                        },
                        {
                            "start": 1375,
                            "end": 1487
                        },
                        {
                            "start": 1488,
                            "end": 2234
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 363,
                            "end": 366,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 366,
                            "end": 369,
                            "matchedPaperCorpusId": "260933721"
                        },
                        {
                            "start": 439,
                            "end": 442,
                            "matchedPaperCorpusId": "219169868"
                        },
                        {
                            "start": 442,
                            "end": 445,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 451,
                            "end": 454,
                            "matchedPaperCorpusId": "118649278"
                        },
                        {
                            "start": 454,
                            "end": 457,
                            "matchedPaperCorpusId": "102483181"
                        },
                        {
                            "start": 457,
                            "end": 460,
                            "matchedPaperCorpusId": "233296935"
                        },
                        {
                            "start": 460,
                            "end": 463,
                            "matchedPaperCorpusId": "258298441"
                        },
                        {
                            "start": 463,
                            "end": 466,
                            "matchedPaperCorpusId": "258309453"
                        },
                        {
                            "start": 466,
                            "end": 469,
                            "matchedPaperCorpusId": "269167845"
                        },
                        {
                            "start": 469,
                            "end": 472,
                            "matchedPaperCorpusId": "269206209"
                        },
                        {
                            "start": 821,
                            "end": 825,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 1058,
                            "end": 1062,
                            "matchedPaperCorpusId": "260933721"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77783203125
                }
            ],
            "relevance_judgement": 0.77783203125,
            "relevance_judgment_input_expanded": "# Title: Maximizing discrimination capability of knowledge distillation with energy function\n# Venue: Knowledge-Based Systems\n# Authors: Seonghak Kim, Gyeongdo Ham, Suin Lee, Donggon Jang, Daeshik Kim\n## Abstract\nNone\n## Knowledge Distillation\nKnowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks. Over the years, KD methods have evolved to narrow the performance gap between student and teacher models by utilizing both final predictions, known as logits-based distillation [10,15,16,11,17], and intermediate features, known as features-based distillation [18,19,20,21,22,23,24,12,25,26,27,28]. \n\nPrevious works on logits-based distillations include the following: DML [15] proposed a mutual learning strategy for collaboratively teaching and learning between student and teacher models; TAKD [16] introduced a multi-step method with an intermediate-size network (i.e., assistant network) to bridge the gap between teachers and students; DKD [11] decomposed the soft-label distillation loss into two components: target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD), enabling each part to independently harness its effectiveness; Multi KD [17] proposed multi-level prediction alignment, containing instance, batch, and class levels, and prediction augmentation. While these approaches emphasize effective knowledge transfer, they do not consider dividing entire datasets or provide mechanisms to distinguish and transfer knowledge from specific samples. \n\nFitNet [18] was groundbreaking as it leveraged not only the final outputs but also intermediate representations. Since the introduction of FitNet, various feature-based KD methods have emerged as follows: AT [22] prompted the student to mimic the attention map of the teacher network; PKT [19] employed various kernels to estimate the probability distributions, employing different divergence metrics for distillation; RKD [20] focused on transferring the mutual relations of data samples; CRD [21] framed the objective as contrastive learning for distillation; VID [23] took a different approach by maximizing mutual information; OFD [24] introduced a novel loss function incorporating teacher transform and a new distance function; Review KD [12] introduced a review mechanism that leverages past features for guiding current ones through residual learning.",
            "reference_string": "[267657497 | Kim et al. | 2023 | Citations: 4]"
        },
        {
            "title": "Knowledge Distillation via Instance-level Sequence Learning",
            "venue": "Knowledge-Based Systems",
            "year": 2021,
            "reference_count": 41,
            "citation_count": 24,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2106.10885",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.10885, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50981688",
                    "name": "Haoran Zhao"
                },
                {
                    "authorId": "144326521",
                    "name": "Xin Sun"
                },
                {
                    "authorId": "1964397",
                    "name": "Junyu Dong"
                },
                {
                    "authorId": "2087106420",
                    "name": "Zihe Dong"
                },
                {
                    "authorId": "2108144626",
                    "name": "Qiong Li"
                }
            ],
            "abstract": null,
            "corpus_id": 235489777,
            "sentences": [
                {
                    "corpus_id": "235489777",
                    "title": "Knowledge Distillation via Instance-level Sequence Learning",
                    "text": "The idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels. \n\nLet us denote x, y as the input of the DNN and one-hot labels of our architecture. \u03c6 s and \u03c6 t represent the student network and teacher network with parameters W s and W t respectively. Given an input image x, the teacher network outputs the final predictions as P T which are obtained by applying the softmax function on the un-normalized log proba-bility values a T , i.e. P T = sof tmax(a T ). Similarly, the same image is fed into the student network to obtain the predictions P S = sof tmax(a S ). The standard cross-entropy is denoted as H. In classical supervised learning, the mismatch between the output of the student network softmax and the ground-truth label y is usually penalized using cross-entropy loss: \n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss: \n\nIt contains the relative probabilities of incorrect classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 , the student network will be trained to optimize the following loss function: \n\nwhere \u03bb is a second hyper parameter controlling the trade-off between the two losses. The teacher network is sometimes deeper and wider than the above approaches, but sometimes has the similar size as the student network [26][27] [28]. Snapshot Distillation [28] proposes to finish teacher-student optimization within one generation which acquires teacher information from the previous iterations of the same training process. Inspired by this, we propose to employ the snapshot of student from the previous epochs to design curriculum for efficient knowledge distillation. \n\nInstance-level sequence learning for knowledge distillation.",
                    "score": 0.6725699338374113,
                    "section_title": "B. Formulation",
                    "char_start_offset": 12183,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 361
                        },
                        {
                            "start": 364,
                            "end": 446
                        },
                        {
                            "start": 447,
                            "end": 550
                        },
                        {
                            "start": 551,
                            "end": 739
                        },
                        {
                            "start": 740,
                            "end": 761
                        },
                        {
                            "start": 762,
                            "end": 867
                        },
                        {
                            "start": 868,
                            "end": 911
                        },
                        {
                            "start": 912,
                            "end": 1084
                        },
                        {
                            "start": 1087,
                            "end": 1239
                        },
                        {
                            "start": 1240,
                            "end": 1399
                        },
                        {
                            "start": 1402,
                            "end": 1506
                        },
                        {
                            "start": 1507,
                            "end": 1655
                        },
                        {
                            "start": 1658,
                            "end": 1743
                        },
                        {
                            "start": 1744,
                            "end": 1893
                        },
                        {
                            "start": 1894,
                            "end": 2084
                        },
                        {
                            "start": 2085,
                            "end": 2231
                        },
                        {
                            "start": 2234,
                            "end": 2294
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1101,
                            "end": 1105,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1128,
                            "end": 1132,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 1879,
                            "end": 1883,
                            "matchedPaperCorpusId": "4110009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77490234375
                }
            ],
            "relevance_judgement": 0.77490234375,
            "relevance_judgment_input_expanded": "# Title: Knowledge Distillation via Instance-level Sequence Learning\n# Venue: Knowledge-Based Systems\n# Authors: Haoran Zhao, Xin Sun, Junyu Dong, Zihe Dong, Qiong Li\n## Abstract\nNone\n## B. Formulation\nThe idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels. \n\nLet us denote x, y as the input of the DNN and one-hot labels of our architecture. \u03c6 s and \u03c6 t represent the student network and teacher network with parameters W s and W t respectively. Given an input image x, the teacher network outputs the final predictions as P T which are obtained by applying the softmax function on the un-normalized log proba-bility values a T , i.e. P T = sof tmax(a T ). Similarly, the same image is fed into the student network to obtain the predictions P S = sof tmax(a S ). The standard cross-entropy is denoted as H. In classical supervised learning, the mismatch between the output of the student network softmax and the ground-truth label y is usually penalized using cross-entropy loss: \n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss: \n\nIt contains the relative probabilities of incorrect classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 , the student network will be trained to optimize the following loss function: \n\nwhere \u03bb is a second hyper parameter controlling the trade-off between the two losses. The teacher network is sometimes deeper and wider than the above approaches, but sometimes has the similar size as the student network [26][27] [28]. Snapshot Distillation [28] proposes to finish teacher-student optimization within one generation which acquires teacher information from the previous iterations of the same training process. Inspired by this, we propose to employ the snapshot of student from the previous epochs to design curriculum for efficient knowledge distillation. \n\nInstance-level sequence learning for knowledge distillation.",
            "reference_string": "[235489777 | Zhao et al. | 2021 | Citations: 24]"
        },
        {
            "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification",
            "venue": "International Journal of Advanced Computer Science and Applications",
            "year": 2021,
            "reference_count": 41,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.14569/ijacsa.2021.0120309",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14569/IJACSA.2021.0120309?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14569/IJACSA.2021.0120309, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2139643077",
                    "name": "Bhavesh Jaiswal"
                },
                {
                    "authorId": "32136431",
                    "name": "Nagendra Gajjar"
                }
            ],
            "abstract": "Deep learning frameworks have progressed beyond human recognition capabilities and, now it\u2019s the perfect opportunity to optimize them for implementation on the embedded platforms. The present deep learning architectures support learning capabilities, but they lack flexibility for applying learned knowledge on the tasks in other unfamiliar domains. This work tries to fill this gap with the deep neural network-based solution for object detection in unrelated domains with a focus on the reduced footprint of the developed model. Knowledge distillation provides efficient and effective teacher-student learning for a variety of different visual recognition tasks. A lightweight student network can be easily trained under the guidance of the high-capacity teacher networks. The teacher-student architecture implementation on binary classes shows a 20% improvement in accuracy within the same training iterations using the transfer learning approach. The scalability of the student model is tested with binary, ternary and multiclass and their performance is compared on basis of inference speed. The results show that the inference speed does not depend on the number of classes. For similar recognition accuracy, the inference speed of about 50 frames per second or 20ms per image. Thus, this approach can be generalized as per the application requirement with minimal changes, provided the dataset format compatibility.",
            "corpus_id": 234336288,
            "sentences": [
                {
                    "corpus_id": "234336288",
                    "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification",
                    "text": "Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model. IV. TEACHER-STUDENT LEARNING Knowledge distillation starts with training a larger model, the teacher 'T'. As it is trained on a heavier platform (GPU), it achieves high performance. Then a lightweight model known as student 'S' is deployed to learn from 'T'. Now, 'S' is supposed to give comparable performance as 'T' but with less memory and more speed. \n\nTo improve knowledge transfer from teacher to student various types of methods are researched. Assuming a trained 'T' has already eliminated some label errors contained in the ground truth data, the authors in [29] treated the hard label predicted by 'T' as the underlying knowledge. While in [30], the soft label produced by 'T', i.e., the classification probabilities, are focused to provide more information to transfer. In general, knowledge is transferred from the 'T' to 'S' by minimizing a loss function in which the target is the distribution of class probabilities predicted by 'T'. This probability distribution has the correct class at a very high probability (close to '1') with all other class probabilities very close to '0'. As such, it does not provide much information beyond the ground truth labels already provided in the dataset. For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process. The distillation related work can be categorized as below:",
                    "score": 0.7005902905223443,
                    "section_title": "C. Knowledge Distillation",
                    "char_start_offset": 8373,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 54
                        },
                        {
                            "start": 57,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 153
                        },
                        {
                            "start": 156,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 370
                        },
                        {
                            "start": 371,
                            "end": 411
                        },
                        {
                            "start": 414,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 513
                        },
                        {
                            "start": 514,
                            "end": 615
                        },
                        {
                            "start": 616,
                            "end": 691
                        },
                        {
                            "start": 692,
                            "end": 768
                        },
                        {
                            "start": 769,
                            "end": 864
                        },
                        {
                            "start": 867,
                            "end": 961
                        },
                        {
                            "start": 962,
                            "end": 1150
                        },
                        {
                            "start": 1151,
                            "end": 1290
                        },
                        {
                            "start": 1291,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1606
                        },
                        {
                            "start": 1607,
                            "end": 1716
                        },
                        {
                            "start": 1717,
                            "end": 1788
                        },
                        {
                            "start": 1789,
                            "end": 1968
                        },
                        {
                            "start": 1969,
                            "end": 2069
                        },
                        {
                            "start": 2070,
                            "end": 2128
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1077,
                            "end": 1081,
                            "matchedPaperCorpusId": "11536917"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7685546875
                }
            ],
            "relevance_judgement": 0.7685546875,
            "relevance_judgment_input_expanded": "# Title: Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification\n# Venue: International Journal of Advanced Computer Science and Applications\n# Authors: Bhavesh Jaiswal, Nagendra Gajjar\n## Abstract\nDeep learning frameworks have progressed beyond human recognition capabilities and, now it\u2019s the perfect opportunity to optimize them for implementation on the embedded platforms. The present deep learning architectures support learning capabilities, but they lack flexibility for applying learned knowledge on the tasks in other unfamiliar domains. This work tries to fill this gap with the deep neural network-based solution for object detection in unrelated domains with a focus on the reduced footprint of the developed model. Knowledge distillation provides efficient and effective teacher-student learning for a variety of different visual recognition tasks. A lightweight student network can be easily trained under the guidance of the high-capacity teacher networks. The teacher-student architecture implementation on binary classes shows a 20% improvement in accuracy within the same training iterations using the transfer learning approach. The scalability of the student model is tested with binary, ternary and multiclass and their performance is compared on basis of inference speed. The results show that the inference speed does not depend on the number of classes. For similar recognition accuracy, the inference speed of about 50 frames per second or 20ms per image. Thus, this approach can be generalized as per the application requirement with minimal changes, provided the dataset format compatibility.\n## C. Knowledge Distillation\nKnowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model. IV. TEACHER-STUDENT LEARNING Knowledge distillation starts with training a larger model, the teacher 'T'. As it is trained on a heavier platform (GPU), it achieves high performance. Then a lightweight model known as student 'S' is deployed to learn from 'T'. Now, 'S' is supposed to give comparable performance as 'T' but with less memory and more speed. \n\nTo improve knowledge transfer from teacher to student various types of methods are researched. Assuming a trained 'T' has already eliminated some label errors contained in the ground truth data, the authors in [29] treated the hard label predicted by 'T' as the underlying knowledge. While in [30], the soft label produced by 'T', i.e., the classification probabilities, are focused to provide more information to transfer. In general, knowledge is transferred from the 'T' to 'S' by minimizing a loss function in which the target is the distribution of class probabilities predicted by 'T'. This probability distribution has the correct class at a very high probability (close to '1') with all other class probabilities very close to '0'. As such, it does not provide much information beyond the ground truth labels already provided in the dataset. For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process. The distillation related work can be categorized as below:",
            "reference_string": "[234336288 | Jaiswal et al. | 2021 | Citations: 0]"
        },
        {
            "title": "An Improved Tuna-YOLO Model Based on YOLO v3 for Real-Time Tuna Detection Considering Lightweight Deployment",
            "venue": "Journal of Marine Science and Engineering",
            "year": 2023,
            "reference_count": 52,
            "citation_count": 16,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2077-1312/11/3/542/pdf?version=1679214285",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/jmse11030542?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/jmse11030542, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108996566",
                    "name": "Yuqing Liu"
                },
                {
                    "authorId": "2184621961",
                    "name": "Huiyong Chu"
                },
                {
                    "authorId": "2150597349",
                    "name": "Liming Song"
                },
                {
                    "authorId": "2175962002",
                    "name": "Zhonglin Zhang"
                },
                {
                    "authorId": "2184685448",
                    "name": "Xing Wei"
                },
                {
                    "authorId": "2210716766",
                    "name": "Ming Chen"
                },
                {
                    "authorId": "150162456",
                    "name": "Jieran Shen"
                }
            ],
            "abstract": "A real-time tuna detection network on mobile devices is a common tool for accurate tuna catch statistics. However, most object detection models have multiple parameters, and normal mobile devices have difficulties in satisfying real-time detection. Based on YOLOv3, this paper proposes a Tuna-YOLO, which is a lightweight object detection network for mobile devices. Firstly, following a comparison of the performance of various lightweight backbone networks, the MobileNet v3 was used as a backbone structure to reduce the number of parameters and calculations. Secondly, the SENET module was replaced with a CBAM attention module to further improve the feature extraction ability of tuna. Then, the knowledge distillation was used to make the Tuna-YOLO detect more accurate. We created a small dataset by deframing electronic surveillance video of fishing boats and labeled the data. After data annotation on the dataset, the K-means algorithm was used to get nine better anchor boxes on the basis of label information, which was used to improve the detection precision. In addition, we compared the detection performance of the Tuna-YOLO and three versions of YOLO v5-6.1 s/m/l after image enhancement. The results show that the Tuna-YOLO reduces the parameters of YOLOv3 from 234.74 MB to 88.45 MB, increases detection precision from 93.33% to 95.83%, and increases the calculation speed from 10.12 fps to 15.23 fps. The performance of the Tuna-YOLO is better than three versions of YOLO v5-6.1 s/m/l. Tuna-YOLO provides a basis for subsequent deployment of algorithms to mobile devices and real-time catch statistics.",
            "corpus_id": 257346971,
            "sentences": [
                {
                    "corpus_id": "257346971",
                    "title": "An Improved Tuna-YOLO Model Based on YOLO v3 for Real-Time Tuna Detection Considering Lightweight Deployment",
                    "text": "The calculations and parameter amounts of the network were reduced significantly after adopting the lightweight design, but so was the detection accuracy. To address this problem, knowledge distillation (KD), a joint training method by transferring \"knowledge\", was employed to improve the detection accuracy. The KD structure was shown in Figure 3. KD is the process of imitating the distillation in chemistry, using the softmax function with temperature parameters to \"distill\" the logit output from complex and large networks, so as to generate more information in categories. This part of the in-formation is called \"dark knowledge\". The additional information guides the simple and small network to learn more knowledge, and the two networks are called the teacher network and the student network, respectively. \n\nTo diversify the information distribution output by the teacher network, we used the temperature parameter \u03c4 to get soft prediction output by distilling logits output between the teacher network and student network. The same dataset was used because soft prediction output implied the information of the negative samples. With the help of SoftMax active function, the teacher network's class prediction probability distribution could be regarded as the soft target. Similarly, this method was used to get not only the soft prediction output but also the hard prediction output from the student network. As for the soft prediction output, soft prediction output and soft target were used to calculate loss value by loss function L so f t , which was a part of total loss. The L so f t was defined as: \n\nwhere P T i is the i-th soft target at time T, Q T i is the i-th soft prediction output at time T, N is the total number of samples and N = 27 in this paper. \n\nshown in Figure 3. KD is the process of imitating the distillation in chem softmax function with temperature parameters to \"distill\" the logit outpu and large networks, so as to generate more information in categories. This formation is called \"dark knowledge\". The additional information guides small network to learn more knowledge, and the two networks are called work and the student network, respectively. Figure 3. The KD structure.",
                    "score": 0.6918832022867221,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 17198,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 154
                        },
                        {
                            "start": 155,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 579
                        },
                        {
                            "start": 580,
                            "end": 637
                        },
                        {
                            "start": 638,
                            "end": 816
                        },
                        {
                            "start": 819,
                            "end": 1034
                        },
                        {
                            "start": 1035,
                            "end": 1140
                        },
                        {
                            "start": 1141,
                            "end": 1284
                        },
                        {
                            "start": 1285,
                            "end": 1421
                        },
                        {
                            "start": 1422,
                            "end": 1589
                        },
                        {
                            "start": 1590,
                            "end": 1618
                        },
                        {
                            "start": 1621,
                            "end": 1778
                        },
                        {
                            "start": 1781,
                            "end": 1999
                        },
                        {
                            "start": 2000,
                            "end": 2042
                        },
                        {
                            "start": 2043,
                            "end": 2191
                        },
                        {
                            "start": 2192,
                            "end": 2201
                        },
                        {
                            "start": 2202,
                            "end": 2219
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76806640625
                }
            ],
            "relevance_judgement": 0.76806640625,
            "relevance_judgment_input_expanded": "# Title: An Improved Tuna-YOLO Model Based on YOLO v3 for Real-Time Tuna Detection Considering Lightweight Deployment\n# Venue: Journal of Marine Science and Engineering\n# Authors: Yuqing Liu, Huiyong Chu, Liming Song, Zhonglin Zhang, Xing Wei, Ming Chen, Jieran Shen\n## Abstract\nA real-time tuna detection network on mobile devices is a common tool for accurate tuna catch statistics. However, most object detection models have multiple parameters, and normal mobile devices have difficulties in satisfying real-time detection. Based on YOLOv3, this paper proposes a Tuna-YOLO, which is a lightweight object detection network for mobile devices. Firstly, following a comparison of the performance of various lightweight backbone networks, the MobileNet v3 was used as a backbone structure to reduce the number of parameters and calculations. Secondly, the SENET module was replaced with a CBAM attention module to further improve the feature extraction ability of tuna. Then, the knowledge distillation was used to make the Tuna-YOLO detect more accurate. We created a small dataset by deframing electronic surveillance video of fishing boats and labeled the data. After data annotation on the dataset, the K-means algorithm was used to get nine better anchor boxes on the basis of label information, which was used to improve the detection precision. In addition, we compared the detection performance of the Tuna-YOLO and three versions of YOLO v5-6.1 s/m/l after image enhancement. The results show that the Tuna-YOLO reduces the parameters of YOLOv3 from 234.74 MB to 88.45 MB, increases detection precision from 93.33% to 95.83%, and increases the calculation speed from 10.12 fps to 15.23 fps. The performance of the Tuna-YOLO is better than three versions of YOLO v5-6.1 s/m/l. Tuna-YOLO provides a basis for subsequent deployment of algorithms to mobile devices and real-time catch statistics.\n## Knowledge Distillation\nThe calculations and parameter amounts of the network were reduced significantly after adopting the lightweight design, but so was the detection accuracy. To address this problem, knowledge distillation (KD), a joint training method by transferring \"knowledge\", was employed to improve the detection accuracy. The KD structure was shown in Figure 3. KD is the process of imitating the distillation in chemistry, using the softmax function with temperature parameters to \"distill\" the logit output from complex and large networks, so as to generate more information in categories. This part of the in-formation is called \"dark knowledge\". The additional information guides the simple and small network to learn more knowledge, and the two networks are called the teacher network and the student network, respectively. \n\nTo diversify the information distribution output by the teacher network, we used the temperature parameter \u03c4 to get soft prediction output by distilling logits output between the teacher network and student network. The same dataset was used because soft prediction output implied the information of the negative samples. With the help of SoftMax active function, the teacher network's class prediction probability distribution could be regarded as the soft target. Similarly, this method was used to get not only the soft prediction output but also the hard prediction output from the student network. As for the soft prediction output, soft prediction output and soft target were used to calculate loss value by loss function L so f t , which was a part of total loss. The L so f t was defined as: \n\nwhere P T i is the i-th soft target at time T, Q T i is the i-th soft prediction output at time T, N is the total number of samples and N = 27 in this paper. \n\nshown in Figure 3. KD is the process of imitating the distillation in chem softmax function with temperature parameters to \"distill\" the logit outpu and large networks, so as to generate more information in categories. This formation is called \"dark knowledge\". The additional information guides small network to learn more knowledge, and the two networks are called work and the student network, respectively. Figure 3. The KD structure.",
            "reference_string": "[257346971 | Liu et al. | 2023 | Citations: 16]"
        },
        {
            "title": "Combine-Net: An Improved Filter Pruning Algorithm",
            "venue": "Inf.",
            "year": 2021,
            "reference_count": 25,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2078-2489/12/7/264/pdf?version=1626255483",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/info12070264?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/info12070264, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109643902",
                    "name": "Jinghan Wang"
                },
                {
                    "authorId": "2151302904",
                    "name": "Guangyue Li"
                },
                {
                    "authorId": "2107940856",
                    "name": "Wenzhao Zhang"
                }
            ],
            "abstract": "The powerful performance of deep learning is evident to all. With the deepening of research, neural networks have become more complex and not easily generalized to resource-constrained devices. The emergence of a series of model compression algorithms makes artificial intelligence on edge possible. Among them, structured model pruning is widely utilized because of its versatility. Structured pruning prunes the neural network itself and discards some relatively unimportant structures to compress the model\u2019s size. However, in the previous pruning work, problems such as evaluation errors of networks, empirical determination of pruning rate, and low retraining efficiency remain. Therefore, we propose an accurate, objective, and efficient pruning algorithm\u2014Combine-Net, introducing Adaptive BN to eliminate evaluation errors, the Kneedle algorithm to determine the pruning rate objectively, and knowledge distillation to improve the efficiency of retraining. Results show that, without precision loss, Combine-Net achieves 95% parameter compression and 83% computation compression on VGG16 on CIFAR10, 71% of parameter compression and 41% computation compression on ResNet50 on CIFAR100. Experiments on different datasets and models have proved that Combine-Net can efficiently compress the neural network\u2019s parameters and computation.",
            "corpus_id": 236984375,
            "sentences": [
                {
                    "corpus_id": "236984375",
                    "title": "Combine-Net: An Improved Filter Pruning Algorithm",
                    "text": "Knowledge distillation (Figure 3) is put forward by Hinton et al. [7]. It is a widely used knowledge transfer technology in the deep learning field. First, a well-trained, robust, high-precision teacher network is needed. Its output is softened with temperature T to provide more information entropy, which extracts hidden knowledge behind its output layer. Then, a relatively small student network is trained to imitate the teacher network's probability output distribution, obtaining a better output result. The main idea of knowledge distillation. The label of the input image is cat; the probability is expressed as {0, 1, 0}. After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q'', which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics. \n\nTo improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become: After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q\", which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics. \n\nTo improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become: \n\nwhere Q T s , Q T t are the softmax probability distribution of the student network and teacher network after softening according to temperature T.",
                    "score": 0.7067643886003013,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 15408,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 70
                        },
                        {
                            "start": 71,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 357
                        },
                        {
                            "start": 358,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 550
                        },
                        {
                            "start": 551,
                            "end": 630
                        },
                        {
                            "start": 631,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 940
                        },
                        {
                            "start": 941,
                            "end": 1010
                        },
                        {
                            "start": 1011,
                            "end": 1159
                        },
                        {
                            "start": 1162,
                            "end": 1479
                        },
                        {
                            "start": 1480,
                            "end": 1569
                        },
                        {
                            "start": 1570,
                            "end": 1634
                        },
                        {
                            "start": 1635,
                            "end": 1703
                        },
                        {
                            "start": 1704,
                            "end": 1852
                        },
                        {
                            "start": 1855,
                            "end": 2017
                        },
                        {
                            "start": 2020,
                            "end": 2167
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 66,
                            "end": 69,
                            "matchedPaperCorpusId": "7200347"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.765625
                }
            ],
            "relevance_judgement": 0.765625,
            "relevance_judgment_input_expanded": "# Title: Combine-Net: An Improved Filter Pruning Algorithm\n# Venue: Inf.\n# Authors: Jinghan Wang, Guangyue Li, Wenzhao Zhang\n## Abstract\nThe powerful performance of deep learning is evident to all. With the deepening of research, neural networks have become more complex and not easily generalized to resource-constrained devices. The emergence of a series of model compression algorithms makes artificial intelligence on edge possible. Among them, structured model pruning is widely utilized because of its versatility. Structured pruning prunes the neural network itself and discards some relatively unimportant structures to compress the model\u2019s size. However, in the previous pruning work, problems such as evaluation errors of networks, empirical determination of pruning rate, and low retraining efficiency remain. Therefore, we propose an accurate, objective, and efficient pruning algorithm\u2014Combine-Net, introducing Adaptive BN to eliminate evaluation errors, the Kneedle algorithm to determine the pruning rate objectively, and knowledge distillation to improve the efficiency of retraining. Results show that, without precision loss, Combine-Net achieves 95% parameter compression and 83% computation compression on VGG16 on CIFAR10, 71% of parameter compression and 41% computation compression on ResNet50 on CIFAR100. Experiments on different datasets and models have proved that Combine-Net can efficiently compress the neural network\u2019s parameters and computation.\n## Knowledge Distillation\nKnowledge distillation (Figure 3) is put forward by Hinton et al. [7]. It is a widely used knowledge transfer technology in the deep learning field. First, a well-trained, robust, high-precision teacher network is needed. Its output is softened with temperature T to provide more information entropy, which extracts hidden knowledge behind its output layer. Then, a relatively small student network is trained to imitate the teacher network's probability output distribution, obtaining a better output result. The main idea of knowledge distillation. The label of the input image is cat; the probability is expressed as {0, 1, 0}. After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q'', which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics. \n\nTo improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become: After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q\", which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics. \n\nTo improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become: \n\nwhere Q T s , Q T t are the softmax probability distribution of the student network and teacher network after softening according to temperature T.",
            "reference_string": "[236984375 | Wang et al. | 2021 | Citations: 3]"
        },
        {
            "title": "Multi-level Knowledge Distillation via Knowledge Alignment and Correlation",
            "venue": "",
            "year": 2020,
            "reference_count": 46,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.00573, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2064424445",
                    "name": "Fei Ding"
                },
                {
                    "authorId": "2257087597",
                    "name": "Yin Yang"
                },
                {
                    "authorId": "2256778126",
                    "name": "Hongxin Hu"
                },
                {
                    "authorId": "2095713717",
                    "name": "V. Krovi"
                },
                {
                    "authorId": "2140495064",
                    "name": "Feng Luo"
                }
            ],
            "abstract": "Knowledge distillation (KD) has become an important technique for model compression and knowledge transfer. In this work, we first perform a comprehensive analysis of the knowledge transferred by different KD methods. We demonstrate that traditional KD methods, which minimize the KL divergence of softmax outputs between networks, are related to the knowledge alignment of an individual sample only. Meanwhile, recent contrastive learning-based KD methods mainly transfer relational knowledge between different samples, namely, knowledge correlation. While it is important to transfer the full knowledge from teacher to student, we introduce the Multi-level Knowledge Distillation (MLKD) by effectively considering both knowledge alignment and correlation. MLKD is task-agnostic and model-agnostic, and can easily transfer knowledge from supervised or self-supervised pretrained teachers. We show that MLKD can improve the reliability and transferability of learned representations. Experiments demonstrate that MLKD outperforms other state-of-the-art methods on a large number of experimental settings including different (a) pretraining strategies (b) network architectures (c) datasets (d) tasks.",
            "corpus_id": 263789814,
            "sentences": [
                {
                    "corpus_id": "263789814",
                    "title": "Multi-level Knowledge Distillation via Knowledge Alignment and Correlation",
                    "text": "Knowledge Distillation. Hinton et al. [23] first propose KD to transfer dark knowledge from the teacher to the student. The softmax outputs encode richer knowledge than one-hot labels and can provide extra supervisory signals. SRRL [41] performs knowledge distillation by leveraging the teacher's projection matrix to train the student's representation via L2 loss. However, these works rely on a supervised pretrained teacher (with logits), and they may be not suitable for self-supervised pretrained teachers. SSKD [40] is proposed to combine the self-supervised auxiliary task and KD to transfer richer dark knowledge, but it cannot be trained in an end-to-end training way. Similar to logits matching, intermediate representation [32,43,42,36,22] are widely used for KD. FitNet [32] proposes to match the whole feature maps, which is difficult and may affect the convergence of the student in some cases. Attention transfer [43] utilizes spatial attention maps as the supervisory signal. AB [22] proposes to learn the activation boundaries of the hidden neurons in the teacher. SP [36] focuses on transferring the similar (dissimilar) activations between the teacher and student. However, most of these works depend on certain architectures, such as convolutional networks. Since these distillation methods involve knowledge matching in an individual sample, they are related to knowledge alignment. Our work also includes the knowledge alignment objective, and it doesn't rely on pretraining strategies or network architectures. \n\nKnowledge distillation and self-supervised learning. Self-supervised learning [30,2,8,20,6] focuses on learning low-dimensional representations by the instance discrimination, which usually requires a large number of negative samples. Recently, BYOL [18] and DINO [7] utilize the momentum encoder to avoid collapse without negatives. The momentum encoder can be considered as the mean teacher [34], which is built dynamically during the student training. For KD, the teacher is pretrained and fixed during distillation. Although different views (augmented images) are passed through networks in self-supervised learning, they are from the same original sample, and have the same semantic meaning.",
                    "score": 0.6117491799145311,
                    "section_title": "Related Work",
                    "char_start_offset": 4102,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 23
                        },
                        {
                            "start": 24,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 365
                        },
                        {
                            "start": 366,
                            "end": 511
                        },
                        {
                            "start": 512,
                            "end": 677
                        },
                        {
                            "start": 678,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 908
                        },
                        {
                            "start": 909,
                            "end": 991
                        },
                        {
                            "start": 992,
                            "end": 1081
                        },
                        {
                            "start": 1082,
                            "end": 1183
                        },
                        {
                            "start": 1184,
                            "end": 1277
                        },
                        {
                            "start": 1278,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1533
                        },
                        {
                            "start": 1536,
                            "end": 1588
                        },
                        {
                            "start": 1589,
                            "end": 1770
                        },
                        {
                            "start": 1771,
                            "end": 1869
                        },
                        {
                            "start": 1870,
                            "end": 1990
                        },
                        {
                            "start": 1991,
                            "end": 2055
                        },
                        {
                            "start": 2056,
                            "end": 2232
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 232,
                            "end": 236,
                            "matchedPaperCorpusId": "235613564"
                        },
                        {
                            "start": 741,
                            "end": 744,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 744,
                            "end": 747,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 747,
                            "end": 750,
                            "matchedPaperCorpusId": "53213211"
                        },
                        {
                            "start": 995,
                            "end": 999,
                            "matchedPaperCorpusId": "53213211"
                        },
                        {
                            "start": 1085,
                            "end": 1089,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 1622,
                            "end": 1625,
                            "matchedPaperCorpusId": "207930212"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.763671875
                }
            ],
            "relevance_judgement": 0.763671875,
            "relevance_judgment_input_expanded": "# Title: Multi-level Knowledge Distillation via Knowledge Alignment and Correlation\n# Venue: \n# Authors: Fei Ding, Yin Yang, Hongxin Hu, V. Krovi, Feng Luo\n## Abstract\nKnowledge distillation (KD) has become an important technique for model compression and knowledge transfer. In this work, we first perform a comprehensive analysis of the knowledge transferred by different KD methods. We demonstrate that traditional KD methods, which minimize the KL divergence of softmax outputs between networks, are related to the knowledge alignment of an individual sample only. Meanwhile, recent contrastive learning-based KD methods mainly transfer relational knowledge between different samples, namely, knowledge correlation. While it is important to transfer the full knowledge from teacher to student, we introduce the Multi-level Knowledge Distillation (MLKD) by effectively considering both knowledge alignment and correlation. MLKD is task-agnostic and model-agnostic, and can easily transfer knowledge from supervised or self-supervised pretrained teachers. We show that MLKD can improve the reliability and transferability of learned representations. Experiments demonstrate that MLKD outperforms other state-of-the-art methods on a large number of experimental settings including different (a) pretraining strategies (b) network architectures (c) datasets (d) tasks.\n## Related Work\nKnowledge Distillation. Hinton et al. [23] first propose KD to transfer dark knowledge from the teacher to the student. The softmax outputs encode richer knowledge than one-hot labels and can provide extra supervisory signals. SRRL [41] performs knowledge distillation by leveraging the teacher's projection matrix to train the student's representation via L2 loss. However, these works rely on a supervised pretrained teacher (with logits), and they may be not suitable for self-supervised pretrained teachers. SSKD [40] is proposed to combine the self-supervised auxiliary task and KD to transfer richer dark knowledge, but it cannot be trained in an end-to-end training way. Similar to logits matching, intermediate representation [32,43,42,36,22] are widely used for KD. FitNet [32] proposes to match the whole feature maps, which is difficult and may affect the convergence of the student in some cases. Attention transfer [43] utilizes spatial attention maps as the supervisory signal. AB [22] proposes to learn the activation boundaries of the hidden neurons in the teacher. SP [36] focuses on transferring the similar (dissimilar) activations between the teacher and student. However, most of these works depend on certain architectures, such as convolutional networks. Since these distillation methods involve knowledge matching in an individual sample, they are related to knowledge alignment. Our work also includes the knowledge alignment objective, and it doesn't rely on pretraining strategies or network architectures. \n\nKnowledge distillation and self-supervised learning. Self-supervised learning [30,2,8,20,6] focuses on learning low-dimensional representations by the instance discrimination, which usually requires a large number of negative samples. Recently, BYOL [18] and DINO [7] utilize the momentum encoder to avoid collapse without negatives. The momentum encoder can be considered as the mean teacher [34], which is built dynamically during the student training. For KD, the teacher is pretrained and fixed during distillation. Although different views (augmented images) are passed through networks in self-supervised learning, they are from the same original sample, and have the same semantic meaning.",
            "reference_string": "[263789814 | Ding et al. | 2020 | Citations: 4]"
        },
        {
            "title": "Towards Generalized Multi-stage Clustering: Multi-view Self-distillation",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 85,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.18890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2186275686",
                    "name": "Jiatai Wang"
                },
                {
                    "authorId": "2244022079",
                    "name": "Zhiwei Xu"
                },
                {
                    "authorId": "2262805047",
                    "name": "Xin Wang"
                }
            ],
            "abstract": "Existing multi-stage clustering methods independently learn the salient features from multiple views and then perform the clustering task. Particularly, multi-view clustering (MVC) has attracted a lot of attention in multi-view or multi-modal scenarios. MVC aims at exploring common semantics and pseudo-labels from multiple views and clustering in a self-supervised manner. However, limited by noisy data and inadequate feature learning, such a clustering paradigm generates overconfident pseudo-labels that mis-guide the model to produce inaccurate predictions. Therefore, it is desirable to have a method that can correct this pseudo-label mistraction in multi-stage clustering to avoid the bias accumulation. To alleviate the effect of overconfident pseudo-labels and improve the generalization ability of the model, this paper proposes a novel multi-stage deep MVC framework where multi-view self-distillation (DistilMVC) is introduced to distill dark knowledge of label distribution. Specifically, in the feature subspace at different hierarchies, we explore the common semantics of multiple views through contrastive learning and obtain pseudo-labels by maximizing the mutual information between views. Additionally, a teacher network is responsible for distilling pseudo-labels into dark knowledge, supervising the student network and improving its predictive capabilities to enhance the robustness. Extensive experiments on real-world multi-view datasets show that our method has better clustering performance than state-of-the-art methods.",
            "corpus_id": 264590688,
            "sentences": [
                {
                    "corpus_id": "264590688",
                    "title": "Towards Generalized Multi-stage Clustering: Multi-view Self-distillation",
                    "text": "The specific experimental settings are shown in Table I. The corresponding distillation methods are as follows: 1) FT [30] uses convolutional operations to transfer dark knowledge; 2) DeiT [31] proposes the distillation token and uses its representation with the teacher model's dark knowledge to compute the distillation loss; 3) KD-RP [32] exploits the differences in student and teacher networks to guide dark knowledge distillation; 4) KD [33] provides additional information about semantic similarity to model learning through the use of dark knowledge generated by self-distillation; 5) SD [34] exploits self-distillation to learn effective representations to group point clouds in the target domain. \n\nThe experimental results are shown in Fig. 2, with the corresponding distillation methods highlighted in red. The five tasks can all improve the performance of their backbone networks after exploiting the Knowledge distillation. Compared with pseudo labels, dark knowledge from the teacher contains the similarity information between classes [36]. With the incorporation of self-distillation, a weak teacher with much lower accuracy than students can still significantly improve the clustering accuracy of students. The success of selfdistillation even with a weak teacher is not solely due to the shared similarity information between classes, but rather due to the regularization of the distilled knowledge [36]. This demonstrates that dark knowledge of knowledge distillation plays a positive role in different learning tasks. \n\nIn the next section, we consider this observation and leverage knowledge self-distillation in Multi-stage MVC.",
                    "score": 0.7262620551072124,
                    "section_title": "B. Contrastive Learning",
                    "char_start_offset": 10461,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 56
                        },
                        {
                            "start": 57,
                            "end": 706
                        },
                        {
                            "start": 709,
                            "end": 818
                        },
                        {
                            "start": 819,
                            "end": 937
                        },
                        {
                            "start": 938,
                            "end": 1056
                        },
                        {
                            "start": 1057,
                            "end": 1224
                        },
                        {
                            "start": 1225,
                            "end": 1423
                        },
                        {
                            "start": 1424,
                            "end": 1538
                        },
                        {
                            "start": 1541,
                            "end": 1651
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 118,
                            "end": 122,
                            "matchedPaperCorpusId": "3608236"
                        },
                        {
                            "start": 189,
                            "end": 193,
                            "matchedPaperCorpusId": "229363322"
                        },
                        {
                            "start": 337,
                            "end": 341,
                            "matchedPaperCorpusId": "245006036"
                        },
                        {
                            "start": 596,
                            "end": 600,
                            "matchedPaperCorpusId": "252918735"
                        },
                        {
                            "start": 1051,
                            "end": 1055,
                            "matchedPaperCorpusId": "219962714"
                        },
                        {
                            "start": 1418,
                            "end": 1422,
                            "matchedPaperCorpusId": "219962714"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76220703125
                }
            ],
            "relevance_judgement": 0.76220703125,
            "relevance_judgment_input_expanded": "# Title: Towards Generalized Multi-stage Clustering: Multi-view Self-distillation\n# Venue: arXiv.org\n# Authors: Jiatai Wang, Zhiwei Xu, Xin Wang\n## Abstract\nExisting multi-stage clustering methods independently learn the salient features from multiple views and then perform the clustering task. Particularly, multi-view clustering (MVC) has attracted a lot of attention in multi-view or multi-modal scenarios. MVC aims at exploring common semantics and pseudo-labels from multiple views and clustering in a self-supervised manner. However, limited by noisy data and inadequate feature learning, such a clustering paradigm generates overconfident pseudo-labels that mis-guide the model to produce inaccurate predictions. Therefore, it is desirable to have a method that can correct this pseudo-label mistraction in multi-stage clustering to avoid the bias accumulation. To alleviate the effect of overconfident pseudo-labels and improve the generalization ability of the model, this paper proposes a novel multi-stage deep MVC framework where multi-view self-distillation (DistilMVC) is introduced to distill dark knowledge of label distribution. Specifically, in the feature subspace at different hierarchies, we explore the common semantics of multiple views through contrastive learning and obtain pseudo-labels by maximizing the mutual information between views. Additionally, a teacher network is responsible for distilling pseudo-labels into dark knowledge, supervising the student network and improving its predictive capabilities to enhance the robustness. Extensive experiments on real-world multi-view datasets show that our method has better clustering performance than state-of-the-art methods.\n## B. Contrastive Learning\nThe specific experimental settings are shown in Table I. The corresponding distillation methods are as follows: 1) FT [30] uses convolutional operations to transfer dark knowledge; 2) DeiT [31] proposes the distillation token and uses its representation with the teacher model's dark knowledge to compute the distillation loss; 3) KD-RP [32] exploits the differences in student and teacher networks to guide dark knowledge distillation; 4) KD [33] provides additional information about semantic similarity to model learning through the use of dark knowledge generated by self-distillation; 5) SD [34] exploits self-distillation to learn effective representations to group point clouds in the target domain. \n\nThe experimental results are shown in Fig. 2, with the corresponding distillation methods highlighted in red. The five tasks can all improve the performance of their backbone networks after exploiting the Knowledge distillation. Compared with pseudo labels, dark knowledge from the teacher contains the similarity information between classes [36]. With the incorporation of self-distillation, a weak teacher with much lower accuracy than students can still significantly improve the clustering accuracy of students. The success of selfdistillation even with a weak teacher is not solely due to the shared similarity information between classes, but rather due to the regularization of the distilled knowledge [36]. This demonstrates that dark knowledge of knowledge distillation plays a positive role in different learning tasks. \n\nIn the next section, we consider this observation and leverage knowledge self-distillation in Multi-stage MVC.",
            "reference_string": "[264590688 | Wang et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Training convolutional neural networks with cheap convolutions and online distillation",
            "venue": "arXiv.org",
            "year": 2019,
            "reference_count": 76,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1909.13063, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2212036699",
                    "name": "Jiao Xie"
                },
                {
                    "authorId": "3431378",
                    "name": "Shaohui Lin"
                },
                {
                    "authorId": "2121310989",
                    "name": "Yichen Zhang"
                },
                {
                    "authorId": "39378434",
                    "name": "Linkai Luo"
                }
            ],
            "abstract": "The large memory and computation consumption in convolutional neural networks (CNNs) has been one of the main barriers for deploying them on resource-limited systems. To this end, most cheap convolutions (e.g., group convolution, depth-wise convolution, and shift convolution) have recently been used for memory and computation reduction but with the specific architecture designing. Furthermore, it results in a low discriminability of the compressed networks by directly replacing the standard convolution with these cheap ones. In this paper, we propose to use knowledge distillation to improve the performance of the compact student networks with cheap convolutions. In our case, the teacher is a network with the standard convolution, while the student is a simple transformation of the teacher architecture without complicated redesigning. In particular, we propose a novel online distillation method, which online constructs the teacher network without pre-training and conducts mutual learning between the teacher and student network, to improve the performance of the student model. Extensive experiments demonstrate that the proposed approach achieves superior performance to simultaneously reduce memory and computation overhead of cutting-edge CNNs on different datasets, including CIFAR-10/100 and ImageNet ILSVRC 2012, compared to the state-of-the-art CNN compression and acceleration methods. The codes are publicly available at this https URL.",
            "corpus_id": 203593636,
            "sentences": [
                {
                    "corpus_id": "203593636",
                    "title": "Training convolutional neural networks with cheap convolutions and online distillation",
                    "text": "By replacing the standard convolution with several cheap convolutions, a variety of CNN models can be compressed and accelerated substantially. It is simple and straightforward to train the compressed models with cheap convolutions from scratch to improve the accuracy. By doing this, it however leads to a limited improvement on accuracy, which is due to a limited knowledge used only by the ground-truth labels. \n\nAlternatively, knowledge distillation [30,32] is becoming a promising solution, which aims to transfer more knowledge from a teacher network to a student network to boost the accuracy of the student network. For this paper, we first review two different distillation methods for learning a smaller student network from a large, pre-trained teacher network: dark knowledge (DK) [30] and attention transfer (AT) [32]. In that case, we can select a CNN model with the standard convolution as a teacher, while a model with cheap convolution by keeping the teacher's architecture is regarded as a student. Then, we propose our online distillation (OD) method to replace the pretrained teacher network by constructing online from the multiple student networks with the same architecture, and train both teacher and student networks in a one-shot manner. \n\nDark Knowledge. Let t and s be a teacher network and a student network with the final output features Z L (t) and Z L (s) , respectively. a (t) and a (s) are the logits of teacher and student networks, which can be computed respectively by: \n\nGiven an input image x, the probabilistic class posterior of teacher and student network p(c|x, K (t) ) and p(c|x, K (s) ) over a class c can be computed respectively as: \n\nwhere K (t) and K (s) are the parameters in the teacher and student networks. \n\nTo perform dark knowledge, we train the student network to minimize the following loss function: \n\nwhere \u03c4 is a temperature parameter to soften the distribution of predictions. Different from fine-tuning just by class labels, dark knowledge is to learn the student network from the soft outputs of teacher network and ground-truth labels. \n\nAttention transfer.",
                    "score": 0.6210430143490889,
                    "section_title": "Online Distillation",
                    "char_start_offset": 18581,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 269
                        },
                        {
                            "start": 270,
                            "end": 413
                        },
                        {
                            "start": 416,
                            "end": 623
                        },
                        {
                            "start": 624,
                            "end": 831
                        },
                        {
                            "start": 832,
                            "end": 1016
                        },
                        {
                            "start": 1017,
                            "end": 1263
                        },
                        {
                            "start": 1266,
                            "end": 1281
                        },
                        {
                            "start": 1282,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1506
                        },
                        {
                            "start": 1509,
                            "end": 1679
                        },
                        {
                            "start": 1682,
                            "end": 1759
                        },
                        {
                            "start": 1762,
                            "end": 1858
                        },
                        {
                            "start": 1861,
                            "end": 1938
                        },
                        {
                            "start": 1939,
                            "end": 2100
                        },
                        {
                            "start": 2103,
                            "end": 2122
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 458,
                            "end": 461,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 826,
                            "end": 830,
                            "matchedPaperCorpusId": "829159"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76220703125
                }
            ],
            "relevance_judgement": 0.76220703125,
            "relevance_judgment_input_expanded": "# Title: Training convolutional neural networks with cheap convolutions and online distillation\n# Venue: arXiv.org\n# Authors: Jiao Xie, Shaohui Lin, Yichen Zhang, Linkai Luo\n## Abstract\nThe large memory and computation consumption in convolutional neural networks (CNNs) has been one of the main barriers for deploying them on resource-limited systems. To this end, most cheap convolutions (e.g., group convolution, depth-wise convolution, and shift convolution) have recently been used for memory and computation reduction but with the specific architecture designing. Furthermore, it results in a low discriminability of the compressed networks by directly replacing the standard convolution with these cheap ones. In this paper, we propose to use knowledge distillation to improve the performance of the compact student networks with cheap convolutions. In our case, the teacher is a network with the standard convolution, while the student is a simple transformation of the teacher architecture without complicated redesigning. In particular, we propose a novel online distillation method, which online constructs the teacher network without pre-training and conducts mutual learning between the teacher and student network, to improve the performance of the student model. Extensive experiments demonstrate that the proposed approach achieves superior performance to simultaneously reduce memory and computation overhead of cutting-edge CNNs on different datasets, including CIFAR-10/100 and ImageNet ILSVRC 2012, compared to the state-of-the-art CNN compression and acceleration methods. The codes are publicly available at this https URL.\n## Online Distillation\nBy replacing the standard convolution with several cheap convolutions, a variety of CNN models can be compressed and accelerated substantially. It is simple and straightforward to train the compressed models with cheap convolutions from scratch to improve the accuracy. By doing this, it however leads to a limited improvement on accuracy, which is due to a limited knowledge used only by the ground-truth labels. \n\nAlternatively, knowledge distillation [30,32] is becoming a promising solution, which aims to transfer more knowledge from a teacher network to a student network to boost the accuracy of the student network. For this paper, we first review two different distillation methods for learning a smaller student network from a large, pre-trained teacher network: dark knowledge (DK) [30] and attention transfer (AT) [32]. In that case, we can select a CNN model with the standard convolution as a teacher, while a model with cheap convolution by keeping the teacher's architecture is regarded as a student. Then, we propose our online distillation (OD) method to replace the pretrained teacher network by constructing online from the multiple student networks with the same architecture, and train both teacher and student networks in a one-shot manner. \n\nDark Knowledge. Let t and s be a teacher network and a student network with the final output features Z L (t) and Z L (s) , respectively. a (t) and a (s) are the logits of teacher and student networks, which can be computed respectively by: \n\nGiven an input image x, the probabilistic class posterior of teacher and student network p(c|x, K (t) ) and p(c|x, K (s) ) over a class c can be computed respectively as: \n\nwhere K (t) and K (s) are the parameters in the teacher and student networks. \n\nTo perform dark knowledge, we train the student network to minimize the following loss function: \n\nwhere \u03c4 is a temperature parameter to soften the distribution of predictions. Different from fine-tuning just by class labels, dark knowledge is to learn the student network from the soft outputs of teacher network and ground-truth labels. \n\nAttention transfer.",
            "reference_string": "[203593636 | Xie et al. | 2019 | Citations: 12]"
        },
        {
            "title": "Energy-efficient Knowledge Distillation for Spiking Neural Networks",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 42,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.07172, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109519891",
                    "name": "Dongjin Lee"
                },
                {
                    "authorId": "120425550",
                    "name": "Seongsik Park"
                },
                {
                    "authorId": "2157129258",
                    "name": "Jongwan Kim"
                },
                {
                    "authorId": "2111845679",
                    "name": "Wuhyeong Doh"
                },
                {
                    "authorId": "2999019",
                    "name": "Sungroh Yoon"
                }
            ],
            "abstract": "Spiking neural networks (SNNs) have been gaining interest as energy-efficient alternatives of conventional artificial neural networks (ANNs) due to their event-driven computation. Considering the future deployment of SNN models to constrained neuromorphic devices, many studies have applied techniques originally used for ANN model compression, such as network quantization, pruning, and knowledge distillation, to SNNs. Among them, existing works on knowledge distillation reported accuracy improvements of student SNN model. However, analysis on energy efficiency, which is also an important feature of SNN, was absent. In this paper, we thoroughly analyze the performance of the distilled SNN model in terms of accuracy and energy efficiency. In the process, we observe a substantial increase in the number of spikes, leading to energy inefficiency, when using the conventional knowledge distillation methods. Based on this analysis, to achieve energy efficiency, we propose a novel knowledge distillation method with heterogeneous temperature parameters. We evaluate our method on two different datasets and show that the resulting SNN student satisfies both accuracy improvement and reduction of the number of spikes. On MNIST dataset, our proposed student SNN achieves up to 0.09% higher accuracy and produces 65% less spikes compared to the student SNN trained with conventional knowledge distillation method. We also compare the results with other SNN compression techniques and training methods.",
            "corpus_id": 235421655,
            "sentences": [
                {
                    "corpus_id": "235421655",
                    "title": "Energy-efficient Knowledge Distillation for Spiking Neural Networks",
                    "text": "Efficiently transferring knowledge from large teacher network to small student network has been a traditional topic which has drawn more and more attention in recent years. Adopting this teacherstudent learning paradigm, many distillation methods have been investigated. [2] first proposed model compression that transfers information from a large model or from an ensemble of models to a small model without a significant drop in accuracy. [11] proposed the concept of soft label which is distilled from the teacher model to the student model. Soft labels are the probabilities of an input belonging to each class, and can be estimated by a softmax function. Here, a temperature factor is introduced to control the importance of each soft label, which contains the informative dark knowledge from the teacher model. By increasing the temperature, the logits can contain richer information than one-hot labels. However, if the temperature becomes too large, the probability of irrelevant classes will also be over-emphasized. \n\nExtra information other than the outputs, for example, the intermediate-level supervision from the teacher model, can additionally boost the performance of knowledge distillation. Application of the intermediate representations was first introduced in FitNet [26] to provide hints to improve the training of the student model. Thanks to the additional hint-based training, the trained deep student network showed better accuracy with fewer parameters compared to the original wide teacher network. \n\nTo explore the relationships between different feature maps, [40] proposed a flow of solution process (FSP), which is defined by the Gram matrix between two layers. Many other different algorithms have been proposed to improve the process of distilling knowledge in more complex settings. [3,15,38,41] As mentioned above, knowledge distillation has been extensively studied and effectively used for ANNs in the literature. However, some recent works show how off-the-shelf knowledge distillation commonly used in ANNs can be adapted for SNNs. [16] first proposed a study that applied knowledge distillation to SNNs. In this work, output spike train of the teacher SNN model was involved in the distillation process, but the performance was unsatisfactory.",
                    "score": 0.5782952682047509,
                    "section_title": "Knowledge distillation",
                    "char_start_offset": 7213,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 270
                        },
                        {
                            "start": 271,
                            "end": 440
                        },
                        {
                            "start": 441,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 816
                        },
                        {
                            "start": 817,
                            "end": 910
                        },
                        {
                            "start": 911,
                            "end": 1025
                        },
                        {
                            "start": 1028,
                            "end": 1207
                        },
                        {
                            "start": 1208,
                            "end": 1354
                        },
                        {
                            "start": 1355,
                            "end": 1525
                        },
                        {
                            "start": 1528,
                            "end": 1692
                        },
                        {
                            "start": 1693,
                            "end": 1816
                        },
                        {
                            "start": 1817,
                            "end": 1950
                        },
                        {
                            "start": 1951,
                            "end": 2070
                        },
                        {
                            "start": 2071,
                            "end": 2143
                        },
                        {
                            "start": 2144,
                            "end": 2283
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 271,
                            "end": 274,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 1589,
                            "end": 1593,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 1823,
                            "end": 1826,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 1826,
                            "end": 1829,
                            "matchedPaperCorpusId": "26071966"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76025390625
                }
            ],
            "relevance_judgement": 0.76025390625,
            "relevance_judgment_input_expanded": "# Title: Energy-efficient Knowledge Distillation for Spiking Neural Networks\n# Venue: arXiv.org\n# Authors: Dongjin Lee, Seongsik Park, Jongwan Kim, Wuhyeong Doh, Sungroh Yoon\n## Abstract\nSpiking neural networks (SNNs) have been gaining interest as energy-efficient alternatives of conventional artificial neural networks (ANNs) due to their event-driven computation. Considering the future deployment of SNN models to constrained neuromorphic devices, many studies have applied techniques originally used for ANN model compression, such as network quantization, pruning, and knowledge distillation, to SNNs. Among them, existing works on knowledge distillation reported accuracy improvements of student SNN model. However, analysis on energy efficiency, which is also an important feature of SNN, was absent. In this paper, we thoroughly analyze the performance of the distilled SNN model in terms of accuracy and energy efficiency. In the process, we observe a substantial increase in the number of spikes, leading to energy inefficiency, when using the conventional knowledge distillation methods. Based on this analysis, to achieve energy efficiency, we propose a novel knowledge distillation method with heterogeneous temperature parameters. We evaluate our method on two different datasets and show that the resulting SNN student satisfies both accuracy improvement and reduction of the number of spikes. On MNIST dataset, our proposed student SNN achieves up to 0.09% higher accuracy and produces 65% less spikes compared to the student SNN trained with conventional knowledge distillation method. We also compare the results with other SNN compression techniques and training methods.\n## Knowledge distillation\nEfficiently transferring knowledge from large teacher network to small student network has been a traditional topic which has drawn more and more attention in recent years. Adopting this teacherstudent learning paradigm, many distillation methods have been investigated. [2] first proposed model compression that transfers information from a large model or from an ensemble of models to a small model without a significant drop in accuracy. [11] proposed the concept of soft label which is distilled from the teacher model to the student model. Soft labels are the probabilities of an input belonging to each class, and can be estimated by a softmax function. Here, a temperature factor is introduced to control the importance of each soft label, which contains the informative dark knowledge from the teacher model. By increasing the temperature, the logits can contain richer information than one-hot labels. However, if the temperature becomes too large, the probability of irrelevant classes will also be over-emphasized. \n\nExtra information other than the outputs, for example, the intermediate-level supervision from the teacher model, can additionally boost the performance of knowledge distillation. Application of the intermediate representations was first introduced in FitNet [26] to provide hints to improve the training of the student model. Thanks to the additional hint-based training, the trained deep student network showed better accuracy with fewer parameters compared to the original wide teacher network. \n\nTo explore the relationships between different feature maps, [40] proposed a flow of solution process (FSP), which is defined by the Gram matrix between two layers. Many other different algorithms have been proposed to improve the process of distilling knowledge in more complex settings. [3,15,38,41] As mentioned above, knowledge distillation has been extensively studied and effectively used for ANNs in the literature. However, some recent works show how off-the-shelf knowledge distillation commonly used in ANNs can be adapted for SNNs. [16] first proposed a study that applied knowledge distillation to SNNs. In this work, output spike train of the teacher SNN model was involved in the distillation process, but the performance was unsatisfactory.",
            "reference_string": "[235421655 | Lee et al. | 2021 | Citations: 12]"
        },
        {
            "title": "Dual discriminator adversarial distillation for data-free model compression",
            "venue": "International Journal of Machine Learning and Cybernetics",
            "year": 2021,
            "reference_count": 72,
            "citation_count": 20,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.05382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50981688",
                    "name": "Haoran Zhao"
                },
                {
                    "authorId": "144326521",
                    "name": "Xin Sun"
                },
                {
                    "authorId": "1964397",
                    "name": "Junyu Dong"
                },
                {
                    "authorId": "2185595070",
                    "name": "Milos Manic"
                },
                {
                    "authorId": "46544755",
                    "name": "Huiyu Zhou"
                },
                {
                    "authorId": "145429878",
                    "name": "Hui Yu"
                }
            ],
            "abstract": "Knowledge distillation has been widely used to produce portable and efficient neural networks which can be well applied on edge devices for computer vision tasks. However, almost all top-performing knowledge distillation methods need to access the original training data, which usually has a huge size and is often unavailable. To tackle this problem, we propose a novel data-free approach in this paper, named Dual Discriminator Adversarial Distillation (DDAD) to distill a neural network without the need of any training data or meta-data. To be specific, we use a generator to create samples through dual discriminator adversarial distillation, which mimics the original training data. The generator not only uses the pre-trained teacher\u2019s intrinsic statistics in existing batch normalization layers but also obtains the maximum discrepancy from the student model. Then the generated samples are used to train the compact student network under the supervision of the teacher. The proposed method obtains an efficient student network which closely approximates its teacher network, without using the original training data. Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach on CIFAR, Caltech101 and ImageNet datasets for classification tasks. Moreover, we extend our method to semantic segmentation tasks on several public datasets such as CamVid, NYUv2, Cityscapes and VOC 2012. To the best of our knowledge, this is the first work on generative model based data-free knowledge distillation on large-scale datasets such as ImageNet, Cityscapes and VOC 2012. Experiments show that our method outperforms all baselines for data-free knowledge distillation.",
            "corpus_id": 233210081,
            "sentences": [
                {
                    "corpus_id": "233210081",
                    "title": "Dual discriminator adversarial distillation for data-free model compression",
                    "text": "Caruana et al. [41] first confirm that one ensemble of networks could transfer the knowledge to the single network. Then Ba et al. [42] propose to teach the student network by penalizing the difference of logits between the teacher and student. Later, the concept of Knowledge Distillation (KD) is introduced by Hinton et al. [24] to solve model compression problems. It transfers the dark knowledge from the pre-trained teacher network to the compact student network by mimicking the class probabilities outputs, which are softened by setting a temperature hyperparameter in softmax. Then the performance of compact student can be retained and close to the performance of the teacher. Note that, KD requires original training data to capture the valuable knowledge from the teacher network. However, the knowledge contained in the soft-labels is insufficient when the teacher network goes deeper. To tackle this issue, some improvements have been made, which extend KD by utilizing the intermediate representation as supervision. For example, Fitnets [27] forces the student to learn the similar intermediate features as teacher's which are defined as hints. Zagoruyko et al. [28] define the attention maps from the intermediate features as the knowledge and then obtain a better performance compared to the one using original feature itself. Moreover, FSP [29] designs the flow distillation loss to force the student to mimic flow matrices of teacher among the feature maps between two layers. RKD [30] transfers mutual relations of data examples by the distance-wise and angle-wise distillation losses. SP [31] preserves the pairwise similarities in student's representation space instead to mimic the representation space of the teacher. CTKD [43] combines the knowledge from different teacher models to improve the student's performance in KD. Due to excellent performance, knowledge distillation has been used to solve a variety of complex applications such as object detection [26,44], semantic segmentation [45], lane detection [46], face recognition [47][48][49] and action recognition [50]. Nevertheless, the above traditional data-driven knowledge distillation methods need full of original training data, which are difficult to be obtained in real world. Thus, several few-shot knowledge distillation approaches are proposed to di",
                    "score": 0.6186971049395273,
                    "section_title": "Data-driven knowledge distillation",
                    "char_start_offset": 7759,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 15,
                            "end": 19,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 131,
                            "end": 135,
                            "matchedPaperCorpusId": "11536917"
                        },
                        {
                            "start": 326,
                            "end": 330,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1052,
                            "end": 1056,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 1177,
                            "end": 1181,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 1358,
                            "end": 1362,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 1500,
                            "end": 1504,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 1609,
                            "end": 1613,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 1747,
                            "end": 1751,
                            "matchedPaperCorpusId": "198179767"
                        },
                        {
                            "start": 1988,
                            "end": 1991,
                            "matchedPaperCorpusId": "201666186"
                        },
                        {
                            "start": 2015,
                            "end": 2019,
                            "matchedPaperCorpusId": "73729180"
                        },
                        {
                            "start": 2036,
                            "end": 2040,
                            "matchedPaperCorpusId": "199405591"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.751953125
                }
            ],
            "relevance_judgement": 0.751953125,
            "relevance_judgment_input_expanded": "# Title: Dual discriminator adversarial distillation for data-free model compression\n# Venue: International Journal of Machine Learning and Cybernetics\n# Authors: Haoran Zhao, Xin Sun, Junyu Dong, Milos Manic, Huiyu Zhou, Hui Yu\n## Abstract\nKnowledge distillation has been widely used to produce portable and efficient neural networks which can be well applied on edge devices for computer vision tasks. However, almost all top-performing knowledge distillation methods need to access the original training data, which usually has a huge size and is often unavailable. To tackle this problem, we propose a novel data-free approach in this paper, named Dual Discriminator Adversarial Distillation (DDAD) to distill a neural network without the need of any training data or meta-data. To be specific, we use a generator to create samples through dual discriminator adversarial distillation, which mimics the original training data. The generator not only uses the pre-trained teacher\u2019s intrinsic statistics in existing batch normalization layers but also obtains the maximum discrepancy from the student model. Then the generated samples are used to train the compact student network under the supervision of the teacher. The proposed method obtains an efficient student network which closely approximates its teacher network, without using the original training data. Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach on CIFAR, Caltech101 and ImageNet datasets for classification tasks. Moreover, we extend our method to semantic segmentation tasks on several public datasets such as CamVid, NYUv2, Cityscapes and VOC 2012. To the best of our knowledge, this is the first work on generative model based data-free knowledge distillation on large-scale datasets such as ImageNet, Cityscapes and VOC 2012. Experiments show that our method outperforms all baselines for data-free knowledge distillation.\n## Data-driven knowledge distillation\nCaruana et al. [41] first confirm that one ensemble of networks could transfer the knowledge to the single network. Then Ba et al. [42] propose to teach the student network by penalizing the difference of logits between the teacher and student. Later, the concept of Knowledge Distillation (KD) is introduced by Hinton et al. [24] to solve model compression problems. It transfers the dark knowledge from the pre-trained teacher network to the compact student network by mimicking the class probabilities outputs, which are softened by setting a temperature hyperparameter in softmax. Then the performance of compact student can be retained and close to the performance of the teacher. Note that, KD requires original training data to capture the valuable knowledge from the teacher network. However, the knowledge contained in the soft-labels is insufficient when the teacher network goes deeper. To tackle this issue, some improvements have been made, which extend KD by utilizing the intermediate representation as supervision. For example, Fitnets [27] forces the student to learn the similar intermediate features as teacher's which are defined as hints. Zagoruyko et al. [28] define the attention maps from the intermediate features as the knowledge and then obtain a better performance compared to the one using original feature itself. Moreover, FSP [29] designs the flow distillation loss to force the student to mimic flow matrices of teacher among the feature maps between two layers. RKD [30] transfers mutual relations of data examples by the distance-wise and angle-wise distillation losses. SP [31] preserves the pairwise similarities in student's representation space instead to mimic the representation space of the teacher. CTKD [43] combines the knowledge from different teacher models to improve the student's performance in KD. Due to excellent performance, knowledge distillation has been used to solve a variety of complex applications such as object detection [26,44], semantic segmentation [45], lane detection [46], face recognition [47][48][49] and action recognition [50]. Nevertheless, the above traditional data-driven knowledge distillation methods need full of original training data, which are difficult to be obtained in real world. Thus, several few-shot knowledge distillation approaches are proposed to di",
            "reference_string": "[233210081 | Zhao et al. | 2021 | Citations: 20]"
        },
        {
            "title": "Similarity Transfer for Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 56,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.10047, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50981688",
                    "name": "Haoran Zhao"
                },
                {
                    "authorId": "2058266174",
                    "name": "Kun Gong"
                },
                {
                    "authorId": "144326521",
                    "name": "Xin Sun"
                },
                {
                    "authorId": "1964397",
                    "name": "Junyu Dong"
                },
                {
                    "authorId": "145429878",
                    "name": "Hui Yu"
                }
            ],
            "abstract": "Knowledge distillation is a popular paradigm for learning portable neural networks by transferring the knowledge from a large model into a smaller one. Most existing approaches enhance the student model by utilizing the similarity information between the categories of instance level provided by the teacher model. However, these works ignore the similarity correlation between different instances that plays an important role in confidence prediction. To tackle this issue, we propose a novel method in this paper, called similarity transfer for knowledge distillation (STKD), which aims to fully utilize the similarities between categories of multiple samples. Furthermore, we propose to better capture the similarity correlation between different instances by the mixup technique, which creates virtual samples by a weighted linear interpolation. Note that, our distillation loss can fully utilize the incorrect classes similarities by the mixed labels. The proposed approach promotes the performance of student model as the virtual sample created by multiple images produces a similar probability distribution in the teacher and student networks. Experiments and ablation studies on several public classification datasets including CIFAR-10,CIFAR-100,CINIC-10 and Tiny-ImageNet verify that this light-weight method can effectively boost the performance of the compact student model. It shows that STKD substantially has outperformed the vanilla knowledge distillation and has achieved superior accuracy over the state-of-the-art knowledge distillation methods.",
            "corpus_id": 232269823,
            "sentences": [
                {
                    "corpus_id": "232269823",
                    "title": "Similarity Transfer for Knowledge Distillation",
                    "text": "Different from above methods, knowledge distillation enrich and get the student model by extracting kinds of knowledge from the fixed teacher model. To address the challenge of deploying CNNs in resource-constrained edge devices, Bucilua et al. [38] first propose to transfer the knowledge of an ensemble of models to a small model. Then Caruana et al. [39] propose to train student model by mimicking the teacher model's logits. Later, Hinton et al. [12] popularize the idea of knowledge distillation, which efficiently transfers knowledge from large teacher network to compact student network by mimicking the class probabilities outputs. Note that, the outputs of the teacher network are defined as the dark knowledge in KD, which provides similarity information as extra supervisions compared with one-hot labels. In other words, there are two sources of supervision used to train the student in KD, one from the ground-truth label, and the other from the soft targets of teacher network. Afterward, some recent works [14] [15] extend KD by distilling knowledge from intermediate feature representations instead of soft labels. For example, FitNets [14] propose to train student network by mimicking the intermediate feature maps of teacher network, which are defined as hints. Inspired by this, Zagoruyko et al. [15] propose to match the attention maps between the teacher and the student, which are defined from the original feature maps as knowledge. Wang et al. [40] propose to improve the performance of student network by matching the distributions of spatial neuron activations between the teacher and the student. Recently, Heo et al. [41] introduce the activation boundary of the hidden neuron as knowledge for distilling the compact student network. \n\nHowever, the aforementioned knowledge distillation methods only utilize the knowledge contained in the output of specific layers of the teacher network. More richer knowledge between different layers is explored and utilized for knowledge distillation. For example, Yim et al. [16] propose to use Gram matrix between different feature layers as distilled knowledge, which named flow of solution process (FSP) that reflects the relations of different features maps.",
                    "score": 0.6143214183740953,
                    "section_title": "B. Knowledge Distillation.",
                    "char_start_offset": 9178,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 640
                        },
                        {
                            "start": 641,
                            "end": 817
                        },
                        {
                            "start": 818,
                            "end": 992
                        },
                        {
                            "start": 993,
                            "end": 1131
                        },
                        {
                            "start": 1132,
                            "end": 1281
                        },
                        {
                            "start": 1282,
                            "end": 1457
                        },
                        {
                            "start": 1458,
                            "end": 1625
                        },
                        {
                            "start": 1626,
                            "end": 1763
                        },
                        {
                            "start": 1766,
                            "end": 1918
                        },
                        {
                            "start": 1919,
                            "end": 2018
                        },
                        {
                            "start": 2019,
                            "end": 2230
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 245,
                            "end": 249,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 353,
                            "end": 357,
                            "matchedPaperCorpusId": "11536917"
                        },
                        {
                            "start": 451,
                            "end": 455,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1022,
                            "end": 1026,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 1153,
                            "end": 1157,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 1317,
                            "end": 1321,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 1470,
                            "end": 1474,
                            "matchedPaperCorpusId": "30307744"
                        },
                        {
                            "start": 1647,
                            "end": 1651,
                            "matchedPaperCorpusId": "53213211"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.751953125
                }
            ],
            "relevance_judgement": 0.751953125,
            "relevance_judgment_input_expanded": "# Title: Similarity Transfer for Knowledge Distillation\n# Venue: arXiv.org\n# Authors: Haoran Zhao, Kun Gong, Xin Sun, Junyu Dong, Hui Yu\n## Abstract\nKnowledge distillation is a popular paradigm for learning portable neural networks by transferring the knowledge from a large model into a smaller one. Most existing approaches enhance the student model by utilizing the similarity information between the categories of instance level provided by the teacher model. However, these works ignore the similarity correlation between different instances that plays an important role in confidence prediction. To tackle this issue, we propose a novel method in this paper, called similarity transfer for knowledge distillation (STKD), which aims to fully utilize the similarities between categories of multiple samples. Furthermore, we propose to better capture the similarity correlation between different instances by the mixup technique, which creates virtual samples by a weighted linear interpolation. Note that, our distillation loss can fully utilize the incorrect classes similarities by the mixed labels. The proposed approach promotes the performance of student model as the virtual sample created by multiple images produces a similar probability distribution in the teacher and student networks. Experiments and ablation studies on several public classification datasets including CIFAR-10,CIFAR-100,CINIC-10 and Tiny-ImageNet verify that this light-weight method can effectively boost the performance of the compact student model. It shows that STKD substantially has outperformed the vanilla knowledge distillation and has achieved superior accuracy over the state-of-the-art knowledge distillation methods.\n## B. Knowledge Distillation.\nDifferent from above methods, knowledge distillation enrich and get the student model by extracting kinds of knowledge from the fixed teacher model. To address the challenge of deploying CNNs in resource-constrained edge devices, Bucilua et al. [38] first propose to transfer the knowledge of an ensemble of models to a small model. Then Caruana et al. [39] propose to train student model by mimicking the teacher model's logits. Later, Hinton et al. [12] popularize the idea of knowledge distillation, which efficiently transfers knowledge from large teacher network to compact student network by mimicking the class probabilities outputs. Note that, the outputs of the teacher network are defined as the dark knowledge in KD, which provides similarity information as extra supervisions compared with one-hot labels. In other words, there are two sources of supervision used to train the student in KD, one from the ground-truth label, and the other from the soft targets of teacher network. Afterward, some recent works [14] [15] extend KD by distilling knowledge from intermediate feature representations instead of soft labels. For example, FitNets [14] propose to train student network by mimicking the intermediate feature maps of teacher network, which are defined as hints. Inspired by this, Zagoruyko et al. [15] propose to match the attention maps between the teacher and the student, which are defined from the original feature maps as knowledge. Wang et al. [40] propose to improve the performance of student network by matching the distributions of spatial neuron activations between the teacher and the student. Recently, Heo et al. [41] introduce the activation boundary of the hidden neuron as knowledge for distilling the compact student network. \n\nHowever, the aforementioned knowledge distillation methods only utilize the knowledge contained in the output of specific layers of the teacher network. More richer knowledge between different layers is explored and utilized for knowledge distillation. For example, Yim et al. [16] propose to use Gram matrix between different feature layers as distilled knowledge, which named flow of solution process (FSP) that reflects the relations of different features maps.",
            "reference_string": "[232269823 | Zhao et al. | 2021 | Citations: 3]"
        },
        {
            "title": "Shared Knowledge Distillation Network for Object Detection",
            "venue": "Electronics",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2079-9292/13/8/1595/pdf?version=1713782440",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics13081595?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics13081595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2158003499",
                    "name": "Zhen Guo"
                },
                {
                    "authorId": "2297246012",
                    "name": "Pengzhou Zhang"
                },
                {
                    "authorId": "2297096848",
                    "name": "Peng Liang"
                }
            ],
            "abstract": "Object detection based on Knowledge Distillation can enhance the capabilities and performance of 5G and 6G networks in various domains, such as autonomous vehicles, smart surveillance, and augmented reality. The integration of object detection with Knowledge Distillation techniques is expected to play a pivotal role in realizing the full potential of these networks. This study presents Shared Knowledge Distillation (Shared-KD) as a solution to overcome optimization challenges caused by disparities in cross-layer features between teacher\u2013student networks. The significant gaps in intermediate-level features between teachers and students present a considerable obstacle to the efficacy of distillation. To tackle this issue, we draw inspiration from collaborative learning in real-world education, where teachers work together to prepare lessons and students engage in peer learning. Building upon this concept, our innovative contributions in model construction are highlighted as follows: (1) A teacher knowledge augmentation module: this module is proposed to combine lower-level teacher features, facilitating the knowledge transfer from the teacher to the student. (2) A student mutual learning module is introduced to enable students to learn from each other, mimicking the peer learning concept in collaborative learning. (3) The Teacher Share Module combines lower-level teacher features: the specific functionality of the teacher knowledge augmentation module is described, which involves combining lower-level teacher features. (4) The multi-step transfer process can be easily optimized due to the minimal gap between the features: the proposed approach breaks down the knowledge transfer process into multiple steps, which can be easily optimized due to the minimal gap between the features involved in each step. Shared-KD uses simple feature losses without additional weights in transformation, resulting in an efficient distillation process that can be easily combined with other methods for further improvement. The effectiveness of our approach is validated through experiments on popular tasks such as object detection and instance segmentation.",
            "corpus_id": 269317596,
            "sentences": [
                {
                    "corpus_id": "269317596",
                    "title": "Shared Knowledge Distillation Network for Object Detection",
                    "text": "Firstly, we will recap the Knowledge Distillation method proposed by Hinton et al. [11]. This widely used method involves using category similarity as a guide for student networks. To regularize the network's learning, temperature is introduced to soften the initial categorical information, also referred to as 'dark knowledge'. The output probability of the teacher network and student network can be calculated as Equations ( 1) and (2). \n\nwhere T represents temperature, which can adjust the softening of the output probability. z i z j are the logits input for softmax, p i represents the output probability of each category in the teacher network, and q i represents the output probability of each category in the student network. Cross-entropy between the distilled teacher and student models calculates the soft loss. The hard loss is calculated based on the predicted values of the student model and the true values. \n\nThe ground truth label (also known as the hard target label) for the i-th sample is represented by y i . The teacher and student models' predicted probability are represented by p i and q i , respectively. The total loss function is calculated as below. \n\nThe loss function used in Knowledge Distillation involves two types of losses: soft target loss L soft and hard target loss L hard . The former guides the student to replicate the teacher's probability distribution, whereas the latter reflects the guidance from the actual ground truth labels. The parameter \u03b1 balances the effect of these two losses. During the Knowledge Distillation process, the student receives both the challenging and soft target knowledge. The loss function can be written as follows: \n\nwhere the loss of the cross-entropy is represented by L CE . The softmax function is represented by \u03c3. y represents the ground truth label. The output logits of the student and teacher networks are denoted by z S and z T . The balancing hyperparameter is represented by \u03b1.",
                    "score": 0.7983046972626031,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 11431,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 88
                        },
                        {
                            "start": 89,
                            "end": 180
                        },
                        {
                            "start": 181,
                            "end": 329
                        },
                        {
                            "start": 330,
                            "end": 440
                        },
                        {
                            "start": 443,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 736
                        },
                        {
                            "start": 737,
                            "end": 825
                        },
                        {
                            "start": 826,
                            "end": 925
                        },
                        {
                            "start": 928,
                            "end": 1032
                        },
                        {
                            "start": 1033,
                            "end": 1133
                        },
                        {
                            "start": 1134,
                            "end": 1181
                        },
                        {
                            "start": 1184,
                            "end": 1316
                        },
                        {
                            "start": 1317,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1534
                        },
                        {
                            "start": 1535,
                            "end": 1646
                        },
                        {
                            "start": 1647,
                            "end": 1691
                        },
                        {
                            "start": 1694,
                            "end": 1754
                        },
                        {
                            "start": 1755,
                            "end": 1833
                        },
                        {
                            "start": 1834,
                            "end": 1916
                        },
                        {
                            "start": 1917,
                            "end": 1966
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75048828125
                }
            ],
            "relevance_judgement": 0.75048828125,
            "relevance_judgment_input_expanded": "# Title: Shared Knowledge Distillation Network for Object Detection\n# Venue: Electronics\n# Authors: Zhen Guo, Pengzhou Zhang, Peng Liang\n## Abstract\nObject detection based on Knowledge Distillation can enhance the capabilities and performance of 5G and 6G networks in various domains, such as autonomous vehicles, smart surveillance, and augmented reality. The integration of object detection with Knowledge Distillation techniques is expected to play a pivotal role in realizing the full potential of these networks. This study presents Shared Knowledge Distillation (Shared-KD) as a solution to overcome optimization challenges caused by disparities in cross-layer features between teacher\u2013student networks. The significant gaps in intermediate-level features between teachers and students present a considerable obstacle to the efficacy of distillation. To tackle this issue, we draw inspiration from collaborative learning in real-world education, where teachers work together to prepare lessons and students engage in peer learning. Building upon this concept, our innovative contributions in model construction are highlighted as follows: (1) A teacher knowledge augmentation module: this module is proposed to combine lower-level teacher features, facilitating the knowledge transfer from the teacher to the student. (2) A student mutual learning module is introduced to enable students to learn from each other, mimicking the peer learning concept in collaborative learning. (3) The Teacher Share Module combines lower-level teacher features: the specific functionality of the teacher knowledge augmentation module is described, which involves combining lower-level teacher features. (4) The multi-step transfer process can be easily optimized due to the minimal gap between the features: the proposed approach breaks down the knowledge transfer process into multiple steps, which can be easily optimized due to the minimal gap between the features involved in each step. Shared-KD uses simple feature losses without additional weights in transformation, resulting in an efficient distillation process that can be easily combined with other methods for further improvement. The effectiveness of our approach is validated through experiments on popular tasks such as object detection and instance segmentation.\n## Knowledge Distillation\nFirstly, we will recap the Knowledge Distillation method proposed by Hinton et al. [11]. This widely used method involves using category similarity as a guide for student networks. To regularize the network's learning, temperature is introduced to soften the initial categorical information, also referred to as 'dark knowledge'. The output probability of the teacher network and student network can be calculated as Equations ( 1) and (2). \n\nwhere T represents temperature, which can adjust the softening of the output probability. z i z j are the logits input for softmax, p i represents the output probability of each category in the teacher network, and q i represents the output probability of each category in the student network. Cross-entropy between the distilled teacher and student models calculates the soft loss. The hard loss is calculated based on the predicted values of the student model and the true values. \n\nThe ground truth label (also known as the hard target label) for the i-th sample is represented by y i . The teacher and student models' predicted probability are represented by p i and q i , respectively. The total loss function is calculated as below. \n\nThe loss function used in Knowledge Distillation involves two types of losses: soft target loss L soft and hard target loss L hard . The former guides the student to replicate the teacher's probability distribution, whereas the latter reflects the guidance from the actual ground truth labels. The parameter \u03b1 balances the effect of these two losses. During the Knowledge Distillation process, the student receives both the challenging and soft target knowledge. The loss function can be written as follows: \n\nwhere the loss of the cross-entropy is represented by L CE . The softmax function is represented by \u03c3. y represents the ground truth label. The output logits of the student and teacher networks are denoted by z S and z T . The balancing hyperparameter is represented by \u03b1.",
            "reference_string": "[269317596 | Guo et al. | 2024 | Citations: 3]"
        },
        {
            "title": "A Lightweight Fault Diagnosis Method Based on Knowledge Distillation Under Time-Varying Rotational Speeds",
            "venue": "IEEE Access",
            "year": 2025,
            "reference_count": 23,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3431228",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3431228?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2024.3431228, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2304217576",
                    "name": "Xiwang Yang"
                },
                {
                    "authorId": "2304301205",
                    "name": "Yarong Wang"
                },
                {
                    "authorId": "2289791638",
                    "name": "Lele Gao"
                },
                {
                    "authorId": "2119194967",
                    "name": "Jia Luo"
                },
                {
                    "authorId": "2294094731",
                    "name": "Licheng Jing"
                },
                {
                    "authorId": "2294320327",
                    "name": "Jinying Huang"
                },
                {
                    "authorId": "2312885379",
                    "name": "Guangpu Liu"
                },
                {
                    "authorId": "2312892885",
                    "name": "Chenfeng Yang"
                }
            ],
            "abstract": "Intelligent fault diagnosis models for engineering must meet real-time requirements while satisfying high diagnosis rates. However, collecting a large amount of fault information of different working conditions in the engineering environment is difficult, and the lack of fault data is a problem that has been difficult to solve. This paper presents a lightweight fault diagnosis model for wind turbine gearboxes based on transfer learning and knowledge distillation. The model is designed to be lightweight and can be used under time-varying rotational speeds. First, TL-ResPConv based on transfer learning and partial convolution was designed as a teacher network model to train a lightweight student network model for fault diagnosis based on TL-ResPConv-KD. Then, the student model obtained through knowledge distillation training is used as the distilled fault diagnosis model. Finally, the performance of the different models is validated and compared on a laboratory simulated planetary gearbox dataset. The experiments prove that the proposed model outperforms other models in fault recognition, achieving a balance between lightweight and high diagnostic accuracy. This makes it a practical network with higher diagnostic accuracy for fault diagnosis under time-varying rotational speeds, meeting the demand for lightweight deployment of edge equipment.",
            "corpus_id": 271356843,
            "sentences": [
                {
                    "corpus_id": "271356843",
                    "title": "A Lightweight Fault Diagnosis Method Based on Knowledge Distillation Under Time-Varying Rotational Speeds",
                    "text": "The core concept of knowledge distillation is that the deep knowledge accumulated in the complex pre-trained teacher network model is distilled and injected into the more streamlined student model, which is able to inherit and mimic the predictive performance of the teacher model through knowledge distillation, thus reducing the model complexity while maintaining similar predictive effects. Knowledge distillation aims to optimize the balance between model size and performance through knowledge transfer to achieve effective model compression and knowledge transfer. The principle of knowledge distillation is shown in Figure 2. In this paper, we adopt the knowledge transfer mechanism, which is a framework in which the student network learns by simulating the output of the teacher network. In this framework, the student network is trained under the guidance and supervision of the teacher network, which draws on the a priori knowledge and generalization ability of the pre-trained teacher network, and helps to improve the learning efficiency and effectiveness. \n\nOffline distillation follows the classical ''teacher-student'' paradigm, so that the student network, in the process of imitating the teacher network, not only learns the decision-making behavior of the teacher network on the training samples, but also passes on the deep patterns and abstract knowledge contained in the teacher network, thus realizing the effective transfer and compression of knowledge, and the degree of fit of the student network to the output of the teacher network is quantified by the loss function. The degree of fitting of the teacher network output results by the loss function can be expressed as follows: \n\nwhere L task denotes the learning performance of the student network on the original task, Ls is the output vector of the student network and Lt is the output vector of the teacher network, which L KD is used to quantify the difference between the student network and the teacher network at the output level. In order to integrate the loss of the student network's own learning task with the loss of imitating the teacher network's output, a balance coefficient is introduced, which \u03bb is used to regulate the relative weights of the two in the total loss function.",
                    "score": 0.5831276734859647,
                    "section_title": "C. KNOWLEDGE DISTILLATION",
                    "char_start_offset": 10854,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 393
                        },
                        {
                            "start": 394,
                            "end": 570
                        },
                        {
                            "start": 571,
                            "end": 796
                        },
                        {
                            "start": 797,
                            "end": 1070
                        },
                        {
                            "start": 1073,
                            "end": 1596
                        },
                        {
                            "start": 1597,
                            "end": 1706
                        },
                        {
                            "start": 1709,
                            "end": 2017
                        },
                        {
                            "start": 2018,
                            "end": 2273
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74072265625
                }
            ],
            "relevance_judgement": 0.74072265625,
            "relevance_judgment_input_expanded": "# Title: A Lightweight Fault Diagnosis Method Based on Knowledge Distillation Under Time-Varying Rotational Speeds\n# Venue: IEEE Access\n# Authors: Xiwang Yang, Yarong Wang, Lele Gao, Jia Luo, Licheng Jing, Jinying Huang, Guangpu Liu, Chenfeng Yang\n## Abstract\nIntelligent fault diagnosis models for engineering must meet real-time requirements while satisfying high diagnosis rates. However, collecting a large amount of fault information of different working conditions in the engineering environment is difficult, and the lack of fault data is a problem that has been difficult to solve. This paper presents a lightweight fault diagnosis model for wind turbine gearboxes based on transfer learning and knowledge distillation. The model is designed to be lightweight and can be used under time-varying rotational speeds. First, TL-ResPConv based on transfer learning and partial convolution was designed as a teacher network model to train a lightweight student network model for fault diagnosis based on TL-ResPConv-KD. Then, the student model obtained through knowledge distillation training is used as the distilled fault diagnosis model. Finally, the performance of the different models is validated and compared on a laboratory simulated planetary gearbox dataset. The experiments prove that the proposed model outperforms other models in fault recognition, achieving a balance between lightweight and high diagnostic accuracy. This makes it a practical network with higher diagnostic accuracy for fault diagnosis under time-varying rotational speeds, meeting the demand for lightweight deployment of edge equipment.\n## C. KNOWLEDGE DISTILLATION\nThe core concept of knowledge distillation is that the deep knowledge accumulated in the complex pre-trained teacher network model is distilled and injected into the more streamlined student model, which is able to inherit and mimic the predictive performance of the teacher model through knowledge distillation, thus reducing the model complexity while maintaining similar predictive effects. Knowledge distillation aims to optimize the balance between model size and performance through knowledge transfer to achieve effective model compression and knowledge transfer. The principle of knowledge distillation is shown in Figure 2. In this paper, we adopt the knowledge transfer mechanism, which is a framework in which the student network learns by simulating the output of the teacher network. In this framework, the student network is trained under the guidance and supervision of the teacher network, which draws on the a priori knowledge and generalization ability of the pre-trained teacher network, and helps to improve the learning efficiency and effectiveness. \n\nOffline distillation follows the classical ''teacher-student'' paradigm, so that the student network, in the process of imitating the teacher network, not only learns the decision-making behavior of the teacher network on the training samples, but also passes on the deep patterns and abstract knowledge contained in the teacher network, thus realizing the effective transfer and compression of knowledge, and the degree of fit of the student network to the output of the teacher network is quantified by the loss function. The degree of fitting of the teacher network output results by the loss function can be expressed as follows: \n\nwhere L task denotes the learning performance of the student network on the original task, Ls is the output vector of the student network and Lt is the output vector of the teacher network, which L KD is used to quantify the difference between the student network and the teacher network at the output level. In order to integrate the loss of the student network's own learning task with the loss of imitating the teacher network's output, a balance coefficient is introduced, which \u03bb is used to regulate the relative weights of the two in the total loss function.",
            "reference_string": "[271356843 | Yang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2021,
            "reference_count": 42,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/16910/16717",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2101.08106, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152751276",
                    "name": "Lingyun Feng"
                },
                {
                    "authorId": "2642333",
                    "name": "Minghui Qiu"
                },
                {
                    "authorId": "2110479359",
                    "name": "Yaliang Li"
                },
                {
                    "authorId": "16215052",
                    "name": "Haitao Zheng"
                },
                {
                    "authorId": "2115382645",
                    "name": "Ying Shen"
                }
            ],
            "abstract": "Despite pre-trained language models such as BERT have achieved appealing performance in a wide range of Natural Language Processing (NLP) tasks, they are computationally expensive to be deployed in real-time applications. A typical method is to adopt knowledge distillation to compress these large pre-trained models (teacher models) to small student models. However, for a target domain with scarce training data, the teacher can hardly pass useful knowledge to the student, which yields performance degradation for the student models. To tackle this problem, we propose a method to learn to augment data for BERT Knowledge Distillation in target domains with scarce labeled data, by learning a cross-domain manipulation scheme that automatically augments the target domain with the help of resource-rich source domains. Specifically, the proposed method generates samples acquired from a stationary distribution near the target data and adopts a reinforced controller to automatically refine the augmentation strategy according to the performance of the student. Extensive experiments demonstrate that the proposed method significantly outperforms state-of-the-art baselines on different NLP tasks, and for the data-scarce domains, the compressed student models even perform better than the original large teacher model, with much fewer parameters (only ~13.3%) when only a few labeled examples available.",
            "corpus_id": 231648215,
            "sentences": [
                {
                    "corpus_id": "231648215",
                    "title": "Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation",
                    "text": "Before we dive into our method, we first introduce the process of knowledge distillation. The distillation process aims to transfer the knowledge of a large teacher network to a small student network. The objective is defined as follows:\n\nwhere f s and f t represent the features of student and teacher models respectively. L(\u00b7) is a loss function that evaluates the difference between the teacher and student models. Inspired by the success of transformer networks such as BERT (Sun et al. 2019;Jiao et al. 2019;Wang et al. 2020), our Figure 1: Overview of the proposed Learning to Augment (L2A) method. The generator generates augmented data based on both source and target domain data from a statistic stationary distribution (P s ). The reinforced selector selects useful augmented data to help the task of knowledge distillation and updates its policy according to the student network performance. distillation model is based on the BERT network (Vaswani et al. 2017). We consider three types of distillation strategy: L att based on attention information, L hidden on intermediate hidden representations, and L dark on the prediction outputs or dark knowledge, detailed as follows:\n\nwhere A i represents the attention matrix corresponding to the i-th self-attention head of the last BERT layer and h is the number of attention heads. H s , H t denotes the output of the last layer of student network and teacher network, respectively. W denotes a transformation matrix that transforms the hidden states of the student network into the same space as the teacher network's states. For dark knowledge based distillation, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits as follows:\n\nwhere g s and g t are the logits from the student and teacher respectively. T KD denotes the temperature value which controls the smoothness of the output distribution. Note that for the regression problem, the above loss is reformulated as the mean square error between the student's and the teacher's logits.\n\nWe combined the above three types of loss as our final KD loss, namely: L KD = L att + L hidden + L dark .",
                    "score": 0.7042118689773702,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 7626,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 479,
                            "end": 496,
                            "matchedPaperCorpusId": "201670719"
                        },
                        {
                            "start": 951,
                            "end": 972,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73583984375
                }
            ],
            "relevance_judgement": 0.73583984375,
            "relevance_judgment_input_expanded": "# Title: Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Lingyun Feng, Minghui Qiu, Yaliang Li, Haitao Zheng, Ying Shen\n## Abstract\nDespite pre-trained language models such as BERT have achieved appealing performance in a wide range of Natural Language Processing (NLP) tasks, they are computationally expensive to be deployed in real-time applications. A typical method is to adopt knowledge distillation to compress these large pre-trained models (teacher models) to small student models. However, for a target domain with scarce training data, the teacher can hardly pass useful knowledge to the student, which yields performance degradation for the student models. To tackle this problem, we propose a method to learn to augment data for BERT Knowledge Distillation in target domains with scarce labeled data, by learning a cross-domain manipulation scheme that automatically augments the target domain with the help of resource-rich source domains. Specifically, the proposed method generates samples acquired from a stationary distribution near the target data and adopts a reinforced controller to automatically refine the augmentation strategy according to the performance of the student. Extensive experiments demonstrate that the proposed method significantly outperforms state-of-the-art baselines on different NLP tasks, and for the data-scarce domains, the compressed student models even perform better than the original large teacher model, with much fewer parameters (only ~13.3%) when only a few labeled examples available.\n## Knowledge Distillation\nBefore we dive into our method, we first introduce the process of knowledge distillation. The distillation process aims to transfer the knowledge of a large teacher network to a small student network. The objective is defined as follows:\n\nwhere f s and f t represent the features of student and teacher models respectively. L(\u00b7) is a loss function that evaluates the difference between the teacher and student models. Inspired by the success of transformer networks such as BERT (Sun et al. 2019;Jiao et al. 2019;Wang et al. 2020), our Figure 1: Overview of the proposed Learning to Augment (L2A) method. The generator generates augmented data based on both source and target domain data from a statistic stationary distribution (P s ). The reinforced selector selects useful augmented data to help the task of knowledge distillation and updates its policy according to the student network performance. distillation model is based on the BERT network (Vaswani et al. 2017). We consider three types of distillation strategy: L att based on attention information, L hidden on intermediate hidden representations, and L dark on the prediction outputs or dark knowledge, detailed as follows:\n\nwhere A i represents the attention matrix corresponding to the i-th self-attention head of the last BERT layer and h is the number of attention heads. H s , H t denotes the output of the last layer of student network and teacher network, respectively. W denotes a transformation matrix that transforms the hidden states of the student network into the same space as the teacher network's states. For dark knowledge based distillation, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits as follows:\n\nwhere g s and g t are the logits from the student and teacher respectively. T KD denotes the temperature value which controls the smoothness of the output distribution. Note that for the regression problem, the above loss is reformulated as the mean square error between the student's and the teacher's logits.\n\nWe combined the above three types of loss as our final KD loss, namely: L KD = L att + L hidden + L dark .",
            "reference_string": "[231648215 | Feng et al. | 2021 | Citations: 10]"
        },
        {
            "title": "Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation",
            "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
            "year": 2020,
            "reference_count": 26,
            "citation_count": 19,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2011.09113",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.09113, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "143747407",
                    "name": "Gaurav Kumar Nayak"
                },
                {
                    "authorId": "2217000",
                    "name": "Konda Reddy Mopuri"
                },
                {
                    "authorId": "1429640900",
                    "name": "Anirban Chakraborty"
                }
            ],
            "abstract": "Knowledge Distillation is an effective method to transfer the learning across deep neural networks. Typically, the dataset originally used for training the Teacher model is chosen as the \"Transfer Set\" to conduct the knowledge transfer to the Student. However, this original training data may not always be freely available due to privacy or sensitivity concerns. In such scenarios, existing approaches either iteratively compose a synthetic set representative of the original training dataset, one sample at a time or learn a generative model to compose such a transfer set. However, both these approaches involve complex optimization (GAN training or several backpropagation steps to synthesize one sample) and are often computationally expensive. In this paper, as a simple alternative, we investigate the effectiveness of \"arbitrary transfer sets\" such as random noise, publicly available synthetic, and natural datasets, all of which are completely unrelated to the original training dataset in terms of their visual or semantic contents. Through extensive experiments on multiple benchmark datasets such as MNIST, FMNIST, CIFAR-10 and CIFAR-100, we discover and validate surprising effectiveness of using arbitrary data to conduct knowledge distillation when this dataset is \"target-class balanced\". We believe that this important observation can potentially lead to designing base-lines for the data-free knowledge distillation task.",
            "corpus_id": 227013462,
            "sentences": [
                {
                    "corpus_id": "227013462",
                    "title": "Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation",
                    "text": "Knowledge Distillation (KD) [3,7] is a contemporary technique for transferring learning across neural network models. Typically, knowledge from one or more complex and deep models (called Teachers) is distilled into a relatively lightweight model (called Student). The core idea of Knowledge Distillation, as discussed in the seminal paper by Hinton et al. [7], is to transfer the (input to output) learned mapping function from Teacher to Student via shar-ing the \"dark knowledge\" extracted by the Teacher on the training images. This typically is achieved via matching the soft targets (or soft labels, i.e., output of softmax layer) predicted by the Student to that of the Teacher for the same inputs. This is the distillation mechanism that enables transfer of the better generalization capability (i.e., the \"knowledge\") of the Teacher to the Student. Thus, Knowledge Distillation has established itself as a very useful and practical tool because of its simplicity and potential. \n\nThe samples used for performing distillation constitute the \"Transfer set\", which is typically required to be constructed using the data sampled from the target distribution. Therefore, the most commonly used transfer set is the original training dataset on which the Teacher model was trained. However, this requirement has been identified as a limitation (e.g. [17,15]) since it is common now-a-days that many popular pre-trained models are released without providing access to the training data (e.g. Facebook's Deepface model trained on 4M confidential face images). This is due to one or more practical constraints such as (i) privacy (e.g. models trained on patients' data from hospitals), (ii) property (proprietary data of companies that invest on collection and annotation), and (iii) transience (observations from the training of a reinforcement learning environment do not exist). \n\nTo handle this \"data-free\" (or zero-shot) distillation scenario, most of the approaches broadly follow either of the two ways: (i) compose a synthetic transfer set by directly utilizing the trained Teacher model that acts as a proxy to the target data (e.g.",
                    "score": 0.5968680543051154,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 117
                        },
                        {
                            "start": 118,
                            "end": 264
                        },
                        {
                            "start": 265,
                            "end": 530
                        },
                        {
                            "start": 531,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 985
                        },
                        {
                            "start": 988,
                            "end": 1162
                        },
                        {
                            "start": 1163,
                            "end": 1282
                        },
                        {
                            "start": 1283,
                            "end": 1350
                        },
                        {
                            "start": 1351,
                            "end": 1491
                        },
                        {
                            "start": 1492,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1633
                        },
                        {
                            "start": 1634,
                            "end": 1879
                        },
                        {
                            "start": 1882,
                            "end": 2139
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 28,
                            "end": 31,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 31,
                            "end": 33,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 357,
                            "end": 360,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1351,
                            "end": 1355,
                            "matchedPaperCorpusId": "159041346"
                        },
                        {
                            "start": 1355,
                            "end": 1358,
                            "matchedPaperCorpusId": "162183830"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73193359375
                }
            ],
            "relevance_judgement": 0.73193359375,
            "relevance_judgment_input_expanded": "# Title: Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation\n# Venue: IEEE Workshop/Winter Conference on Applications of Computer Vision\n# Authors: Gaurav Kumar Nayak, Konda Reddy Mopuri, Anirban Chakraborty\n## Abstract\nKnowledge Distillation is an effective method to transfer the learning across deep neural networks. Typically, the dataset originally used for training the Teacher model is chosen as the \"Transfer Set\" to conduct the knowledge transfer to the Student. However, this original training data may not always be freely available due to privacy or sensitivity concerns. In such scenarios, existing approaches either iteratively compose a synthetic set representative of the original training dataset, one sample at a time or learn a generative model to compose such a transfer set. However, both these approaches involve complex optimization (GAN training or several backpropagation steps to synthesize one sample) and are often computationally expensive. In this paper, as a simple alternative, we investigate the effectiveness of \"arbitrary transfer sets\" such as random noise, publicly available synthetic, and natural datasets, all of which are completely unrelated to the original training dataset in terms of their visual or semantic contents. Through extensive experiments on multiple benchmark datasets such as MNIST, FMNIST, CIFAR-10 and CIFAR-100, we discover and validate surprising effectiveness of using arbitrary data to conduct knowledge distillation when this dataset is \"target-class balanced\". We believe that this important observation can potentially lead to designing base-lines for the data-free knowledge distillation task.\n## Introduction\nKnowledge Distillation (KD) [3,7] is a contemporary technique for transferring learning across neural network models. Typically, knowledge from one or more complex and deep models (called Teachers) is distilled into a relatively lightweight model (called Student). The core idea of Knowledge Distillation, as discussed in the seminal paper by Hinton et al. [7], is to transfer the (input to output) learned mapping function from Teacher to Student via shar-ing the \"dark knowledge\" extracted by the Teacher on the training images. This typically is achieved via matching the soft targets (or soft labels, i.e., output of softmax layer) predicted by the Student to that of the Teacher for the same inputs. This is the distillation mechanism that enables transfer of the better generalization capability (i.e., the \"knowledge\") of the Teacher to the Student. Thus, Knowledge Distillation has established itself as a very useful and practical tool because of its simplicity and potential. \n\nThe samples used for performing distillation constitute the \"Transfer set\", which is typically required to be constructed using the data sampled from the target distribution. Therefore, the most commonly used transfer set is the original training dataset on which the Teacher model was trained. However, this requirement has been identified as a limitation (e.g. [17,15]) since it is common now-a-days that many popular pre-trained models are released without providing access to the training data (e.g. Facebook's Deepface model trained on 4M confidential face images). This is due to one or more practical constraints such as (i) privacy (e.g. models trained on patients' data from hospitals), (ii) property (proprietary data of companies that invest on collection and annotation), and (iii) transience (observations from the training of a reinforcement learning environment do not exist). \n\nTo handle this \"data-free\" (or zero-shot) distillation scenario, most of the approaches broadly follow either of the two ways: (i) compose a synthetic transfer set by directly utilizing the trained Teacher model that acts as a proxy to the target data (e.g.",
            "reference_string": "[227013462 | Nayak et al. | 2020 | Citations: 19]"
        },
        {
            "title": "Distillation \u2248 Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network",
            "venue": "arXiv.org",
            "year": 2019,
            "reference_count": 58,
            "citation_count": 41,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.01255, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145496882",
                    "name": "Bin Dong"
                },
                {
                    "authorId": "104162954",
                    "name": "Jikai Hou"
                },
                {
                    "authorId": "48518029",
                    "name": "Yiping Lu"
                },
                {
                    "authorId": "47294286",
                    "name": "Zhihua Zhang"
                }
            ],
            "abstract": "Distillation is a method to transfer knowledge from one model to another and often achieves higher accuracy with the same capacity. In this paper, we aim to provide a theoretical understanding on what mainly helps with the distillation. Our answer is \"early stopping\". Assuming that the teacher network is overparameterized, we argue that the teacher network is essentially harvesting dark knowledge from the data via early stopping. This can be justified by a new concept, {Anisotropic Information Retrieval (AIR)}, which means that the neural network tends to fit the informative information first and the non-informative information (including noise) later. Motivated by the recent development on theoretically analyzing overparameterized neural networks, we can characterize AIR by the eigenspace of the Neural Tangent Kernel(NTK). AIR facilities a new understanding of distillation. With that, we further utilize distillation to refine noisy labels. We propose a self-distillation algorithm to sequentially distill knowledge from the network in the previous training epoch to avoid memorizing the wrong labels. We also demonstrate, both theoretically and empirically, that self-distillation can benefit from more than just early stopping. Theoretically, we prove convergence of the proposed algorithm to the ground truth labels for randomly initialized overparameterized neural networks in terms of $\\ell_2$ distance, while the previous result was on convergence in $0$-$1$ loss. The theoretical result ensures the learned neural network enjoy a margin on the training data which leads to better generalization. Empirically, we achieve better testing accuracy and entirely avoid early stopping which makes the algorithm more user-friendly.",
            "corpus_id": 203642142,
            "sentences": [
                {
                    "corpus_id": "203642142",
                    "title": "Distillation \u2248 Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network",
                    "text": "Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing [24]. Among these tasks, image classification is considered as one of the fundamental tasks since classification networks are commonly used as base networks for other problems. In order to achieve higher accuracy using a network with similar complexity as the base network, distillation has been proposed, which aims to utilize the prediction of one (teacher) network to guide the training of another (student) network. In [17], the authors suggested to generate a soft target by a heavy-duty teacher network to guide the training of a light-weighted student network. More interestingly, [14,5] proposed to train a student network parameterized identically as the teacher network. Surprisingly, the student network significantly outperforms the teacher network. Later, it was suggested by [49,19,9] to transfer knowledge of representations, such as attention maps and gradients of the classifier, to help with the training of the student network. In this work, we focus on the distillation utilizing the network outputs [17,14,45,5,46]. \n\nTo explain the effectiveness of distillation, [17] suggested that instead of the hard labels (i.e one-hot vectors), the soft labels generated by the pre-trained teacher network provide extra information, which is called the \"Dark Knowledge\". The \"Dark knowledge\" is the knowledge encoded by the relative probabilities of the incorrect outputs. In [17,14,45], the authors pointed out that secondary information, i.e the semantic similarity between different classes, is part of the \"Dark Knowledge\", and [5] observed that the \"Dark Knowledge\" can help to refine noisy labels. In this paper, we would like to answer the following question: can we theoretically explain how neural networks learn the Dark Knowledge? Answering this question will help us to understand the regularization effect of distillation. \n\nIn this work, we assume that the teacher network is overparameterized, which means that it can memorize all the labels via gradient descent training [12,11,34,1].",
                    "score": 0.6533921859057485,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 119,
                            "end": 289
                        },
                        {
                            "start": 290,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 793
                        },
                        {
                            "start": 794,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 1059
                        },
                        {
                            "start": 1060,
                            "end": 1149
                        },
                        {
                            "start": 1152,
                            "end": 1393
                        },
                        {
                            "start": 1394,
                            "end": 1495
                        },
                        {
                            "start": 1496,
                            "end": 1726
                        },
                        {
                            "start": 1727,
                            "end": 1864
                        },
                        {
                            "start": 1865,
                            "end": 1958
                        },
                        {
                            "start": 1961,
                            "end": 2123
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 113,
                            "end": 117,
                            "matchedPaperCorpusId": "1779661"
                        },
                        {
                            "start": 909,
                            "end": 911,
                            "matchedPaperCorpusId": "21596346"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73095703125
                }
            ],
            "relevance_judgement": 0.73095703125,
            "relevance_judgment_input_expanded": "# Title: Distillation \u2248 Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network\n# Venue: arXiv.org\n# Authors: Bin Dong, Jikai Hou, Yiping Lu, Zhihua Zhang\n## Abstract\nDistillation is a method to transfer knowledge from one model to another and often achieves higher accuracy with the same capacity. In this paper, we aim to provide a theoretical understanding on what mainly helps with the distillation. Our answer is \"early stopping\". Assuming that the teacher network is overparameterized, we argue that the teacher network is essentially harvesting dark knowledge from the data via early stopping. This can be justified by a new concept, {Anisotropic Information Retrieval (AIR)}, which means that the neural network tends to fit the informative information first and the non-informative information (including noise) later. Motivated by the recent development on theoretically analyzing overparameterized neural networks, we can characterize AIR by the eigenspace of the Neural Tangent Kernel(NTK). AIR facilities a new understanding of distillation. With that, we further utilize distillation to refine noisy labels. We propose a self-distillation algorithm to sequentially distill knowledge from the network in the previous training epoch to avoid memorizing the wrong labels. We also demonstrate, both theoretically and empirically, that self-distillation can benefit from more than just early stopping. Theoretically, we prove convergence of the proposed algorithm to the ground truth labels for randomly initialized overparameterized neural networks in terms of $\\ell_2$ distance, while the previous result was on convergence in $0$-$1$ loss. The theoretical result ensures the learned neural network enjoy a margin on the training data which leads to better generalization. Empirically, we achieve better testing accuracy and entirely avoid early stopping which makes the algorithm more user-friendly.\n## Introduction\nDeep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing [24]. Among these tasks, image classification is considered as one of the fundamental tasks since classification networks are commonly used as base networks for other problems. In order to achieve higher accuracy using a network with similar complexity as the base network, distillation has been proposed, which aims to utilize the prediction of one (teacher) network to guide the training of another (student) network. In [17], the authors suggested to generate a soft target by a heavy-duty teacher network to guide the training of a light-weighted student network. More interestingly, [14,5] proposed to train a student network parameterized identically as the teacher network. Surprisingly, the student network significantly outperforms the teacher network. Later, it was suggested by [49,19,9] to transfer knowledge of representations, such as attention maps and gradients of the classifier, to help with the training of the student network. In this work, we focus on the distillation utilizing the network outputs [17,14,45,5,46]. \n\nTo explain the effectiveness of distillation, [17] suggested that instead of the hard labels (i.e one-hot vectors), the soft labels generated by the pre-trained teacher network provide extra information, which is called the \"Dark Knowledge\". The \"Dark knowledge\" is the knowledge encoded by the relative probabilities of the incorrect outputs. In [17,14,45], the authors pointed out that secondary information, i.e the semantic similarity between different classes, is part of the \"Dark Knowledge\", and [5] observed that the \"Dark Knowledge\" can help to refine noisy labels. In this paper, we would like to answer the following question: can we theoretically explain how neural networks learn the Dark Knowledge? Answering this question will help us to understand the regularization effect of distillation. \n\nIn this work, we assume that the teacher network is overparameterized, which means that it can memorize all the labels via gradient descent training [12,11,34,1].",
            "reference_string": "[203642142 | Dong et al. | 2019 | Citations: 41]"
        },
        {
            "title": "Label Noise Types and Their Effects on Deep Learning",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 44,
            "citation_count": 53,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2003.10471, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1454230053",
                    "name": "G. Algan"
                },
                {
                    "authorId": "152986771",
                    "name": "I. Ulusoy"
                }
            ],
            "abstract": "The recent success of deep learning is mostly due to the availability of big datasets with clean annotations. However, gathering a cleanly annotated dataset is not always feasible due to practical challenges. As a result, label noise is a common problem in datasets, and numerous methods to train deep neural networks in the presence of noisy labels are proposed in the literature. These methods commonly use benchmark datasets with synthetic label noise on the training set. However, there are multiple types of label noise, and each of them has its own characteristic impact on learning. Since each work generates a different kind of label noise, it is problematic to test and compare those algorithms in the literature fairly. In this work, we provide a detailed analysis of the effects of different kinds of label noise on learning. Moreover, we propose a generic framework to generate feature-dependent label noise, which we show to be the most challenging case for learning. Our proposed method aims to emphasize similarities among data instances by sparsely distributing them in the feature domain. By this approach, samples that are more likely to be mislabeled are detected from their softmax probabilities, and their labels are flipped to the corresponding class. The proposed method can be applied to any clean dataset to synthesize feature-dependent noisy labels. For the ease of other researchers to test their algorithms with noisy labels, we share corrupted labels for the most commonly used benchmark datasets. Our code and generated noisy synthetic labels are available online.",
            "corpus_id": 214623014,
            "sentences": [
                {
                    "corpus_id": "214623014",
                    "title": "Label Noise Types and Their Effects on Deep Learning",
                    "text": "Compared to the previous two label noise models, feature dependent noise is harder to implement since all samples should be vectorized in the feature domain, and similarities among samples should be calculated. [37] flips labels for clusters of instances, which results in locally concentrated label noise. However, this approach doesn't utilize the similarities among instances; therefore, different from our proposed method. \n\nOne option is to train a deep network on the dataset first and then use it as the feature extractor. However, since the network extracts the features of data that it is trained on, it is prone to overfitting. Since we are especially interested in similarities among instances in feature space, it is desired that samples are sparsely distributed. On the contrary, in the case of overfitting, samples are gathered in a small region in feature space. \n\nTherefore, in this work we used the idea of knowledge distillation [16]. In the original work, the authors used distillation to transfer knowledge from the big teacher network to a much smaller student network without decreasing the performance. The idea is mainly motivated by learning from soft labels where the similarity of each instance to each class is emphasized by temperature hyperparameter. \n\nClass probabilities on softmax output, beyond the true class probability, are usually very low. But, compared with each other, some classes may have a much higher probability than others, and this carries important information about that data instance, which is also called as dark knowledge. By making probability distribution smoother, this relation is emphasized, as shown in Equation 2. \n\nInstead of being trained on hard labels, the student network is trained on the weighted sum of hard labels and soft labels produced by the teacher network. So, the loss function is defined as follows, \n\nwhere q i represents the soft labels produced by the teacher network using temperature T and y i represents the given label. \n\nWithin the context of this work, we are not interested in compressing the network to a smaller network. However, the idea of learning by emphasizing instance similarities can be used to find instances that have similar features with other classes. For that purpose, our student network is at the same size as the teacher network. Firstly, the teacher network is trained on the dataset. Secondly, soft labels are produced from the softmax output of the teacher for a given temperature T .",
                    "score": 0.631431347108921,
                    "section_title": "C. Feature-dependent noise",
                    "char_start_offset": 9009,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 210
                        },
                        {
                            "start": 211,
                            "end": 306
                        },
                        {
                            "start": 307,
                            "end": 426
                        },
                        {
                            "start": 429,
                            "end": 529
                        },
                        {
                            "start": 530,
                            "end": 637
                        },
                        {
                            "start": 638,
                            "end": 775
                        },
                        {
                            "start": 776,
                            "end": 877
                        },
                        {
                            "start": 880,
                            "end": 952
                        },
                        {
                            "start": 953,
                            "end": 1125
                        },
                        {
                            "start": 1126,
                            "end": 1280
                        },
                        {
                            "start": 1283,
                            "end": 1378
                        },
                        {
                            "start": 1379,
                            "end": 1575
                        },
                        {
                            "start": 1576,
                            "end": 1673
                        },
                        {
                            "start": 1676,
                            "end": 1831
                        },
                        {
                            "start": 1832,
                            "end": 1876
                        },
                        {
                            "start": 1879,
                            "end": 2003
                        },
                        {
                            "start": 2006,
                            "end": 2109
                        },
                        {
                            "start": 2110,
                            "end": 2253
                        },
                        {
                            "start": 2254,
                            "end": 2335
                        },
                        {
                            "start": 2336,
                            "end": 2391
                        },
                        {
                            "start": 2392,
                            "end": 2493
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 211,
                            "end": 215,
                            "matchedPaperCorpusId": "36832256"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73095703125
                }
            ],
            "relevance_judgement": 0.73095703125,
            "relevance_judgment_input_expanded": "# Title: Label Noise Types and Their Effects on Deep Learning\n# Venue: arXiv.org\n# Authors: G. Algan, I. Ulusoy\n## Abstract\nThe recent success of deep learning is mostly due to the availability of big datasets with clean annotations. However, gathering a cleanly annotated dataset is not always feasible due to practical challenges. As a result, label noise is a common problem in datasets, and numerous methods to train deep neural networks in the presence of noisy labels are proposed in the literature. These methods commonly use benchmark datasets with synthetic label noise on the training set. However, there are multiple types of label noise, and each of them has its own characteristic impact on learning. Since each work generates a different kind of label noise, it is problematic to test and compare those algorithms in the literature fairly. In this work, we provide a detailed analysis of the effects of different kinds of label noise on learning. Moreover, we propose a generic framework to generate feature-dependent label noise, which we show to be the most challenging case for learning. Our proposed method aims to emphasize similarities among data instances by sparsely distributing them in the feature domain. By this approach, samples that are more likely to be mislabeled are detected from their softmax probabilities, and their labels are flipped to the corresponding class. The proposed method can be applied to any clean dataset to synthesize feature-dependent noisy labels. For the ease of other researchers to test their algorithms with noisy labels, we share corrupted labels for the most commonly used benchmark datasets. Our code and generated noisy synthetic labels are available online.\n## C. Feature-dependent noise\nCompared to the previous two label noise models, feature dependent noise is harder to implement since all samples should be vectorized in the feature domain, and similarities among samples should be calculated. [37] flips labels for clusters of instances, which results in locally concentrated label noise. However, this approach doesn't utilize the similarities among instances; therefore, different from our proposed method. \n\nOne option is to train a deep network on the dataset first and then use it as the feature extractor. However, since the network extracts the features of data that it is trained on, it is prone to overfitting. Since we are especially interested in similarities among instances in feature space, it is desired that samples are sparsely distributed. On the contrary, in the case of overfitting, samples are gathered in a small region in feature space. \n\nTherefore, in this work we used the idea of knowledge distillation [16]. In the original work, the authors used distillation to transfer knowledge from the big teacher network to a much smaller student network without decreasing the performance. The idea is mainly motivated by learning from soft labels where the similarity of each instance to each class is emphasized by temperature hyperparameter. \n\nClass probabilities on softmax output, beyond the true class probability, are usually very low. But, compared with each other, some classes may have a much higher probability than others, and this carries important information about that data instance, which is also called as dark knowledge. By making probability distribution smoother, this relation is emphasized, as shown in Equation 2. \n\nInstead of being trained on hard labels, the student network is trained on the weighted sum of hard labels and soft labels produced by the teacher network. So, the loss function is defined as follows, \n\nwhere q i represents the soft labels produced by the teacher network using temperature T and y i represents the given label. \n\nWithin the context of this work, we are not interested in compressing the network to a smaller network. However, the idea of learning by emphasizing instance similarities can be used to find instances that have similar features with other classes. For that purpose, our student network is at the same size as the teacher network. Firstly, the teacher network is trained on the dataset. Secondly, soft labels are produced from the softmax output of the teacher for a given temperature T .",
            "reference_string": "[214623014 | Algan et al. | 2020 | Citations: 53]"
        },
        {
            "title": "Self-Distillation Amplifies Regularization in Hilbert Space",
            "venue": "Neural Information Processing Systems",
            "year": 2020,
            "reference_count": 50,
            "citation_count": 235,
            "influential_citation_count": 19,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.05715, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3232655",
                    "name": "H. Mobahi"
                },
                {
                    "authorId": "1682124",
                    "name": "Mehrdad Farajtabar"
                },
                {
                    "authorId": "1745169",
                    "name": "P. Bartlett"
                }
            ],
            "abstract": "Knowledge distillation introduced in the deep learning context is a method to transfer knowledge from one architecture to another. In particular, when the architectures are identical, this is called self-distillation. The idea is to feed in predictions of the trained model as new target values for retraining (and iterate this loop possibly a few times). It has been empirically observed that the self-distilled model often achieves higher accuracy on held out data. Why this happens, however, has been a mystery: the self-distillation dynamics does not receive any new information about the task and solely evolves by looping over training. To the best of our knowledge, there is no rigorous understanding of why this happens. This work provides the first theoretical analysis of self-distillation. We focus on fitting a nonlinear function to training data, where the model space is Hilbert space and fitting is subject to L2 regularization in this function space. We show that self-distillation iterations modify regularization by progressively limiting the number of basis functions that can be used to represent the solution. This implies (as we also verify empirically) that while a few rounds of self-distillation may reduce over-fitting, further rounds may lead to under-fitting and thus worse performance.",
            "corpus_id": 211096976,
            "sentences": [
                {
                    "corpus_id": "211096976",
                    "title": "Self-Distillation Amplifies Regularization in Hilbert Space",
                    "text": "Knowledge distillation, originally introduced in the deep learning setting [Hinton et al., 2015], is a method that transfers knowledge from one architecture (teacher) to another (student), where often the student model is much smaller. The original formulation achieves this by training the student model using the output probability distribution of the teacher model in addition to original labels. The student model benefits from this \"dark knowledge\" (extra information in soft predictions) and often performs better than if it was trained on the actual labels. \n\nVarious extensions of this approach have been recently proposed, where instead of output predictions, the student matches other statistics from the teacher model such as intermediate feature representations [Romero et al., 2014], Jacobian matrices [Srinivas and Fleuret, 2018], distributions [Huang and Wang, 2017], Gram matrices [Yim et al., 2017]. Additional developments on knowledge distillation include its extensions to Bayesian settings [Korattikara Balan et al., 2015, Vadera andMarlin, 2020], uncertainty preservation [Tran et al., 2020], reinforcement learning [Hong et al., 2020, Teh et al., 2017, Ghosh et al., 2018], online distillation [lan et al., 2018], zero-shot learning [Nayak et al., 2019], multistep knowledge distillation [Mirzadeh et al., 2020], tackling catastrophic forgetting [Li and Hoiem, 2016], transfer of relational knowledge [Park et al., 2019], adversarial distillation [Wang et al., 2018]. \n\nThe special case when the teacher and student architectures are identical is called1 self-distillation. The idea is to feed in predictions of the trained model as new target values for retraining (and iterate this loop possibly a few times). It has been consistently observed that the self-distilled model often achieves higher accuracy on held out data [Furlanello et al., 2018, Yang et al., 2019, Ahn et al., 2019].",
                    "score": 0.5853813815468956,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 235
                        },
                        {
                            "start": 236,
                            "end": 399
                        },
                        {
                            "start": 400,
                            "end": 564
                        },
                        {
                            "start": 567,
                            "end": 916
                        },
                        {
                            "start": 917,
                            "end": 1490
                        },
                        {
                            "start": 1493,
                            "end": 1596
                        },
                        {
                            "start": 1597,
                            "end": 1734
                        },
                        {
                            "start": 1735,
                            "end": 1910
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1156,
                            "end": 1174,
                            "matchedPaperCorpusId": "31009408"
                        },
                        {
                            "start": 1256,
                            "end": 1276,
                            "matchedPaperCorpusId": "159041346"
                        },
                        {
                            "start": 1311,
                            "end": 1334,
                            "matchedPaperCorpusId": "62841516"
                        },
                        {
                            "start": 1369,
                            "end": 1389,
                            "matchedPaperCorpusId": "62841516"
                        },
                        {
                            "start": 1424,
                            "end": 1443,
                            "matchedPaperCorpusId": "159041346"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72802734375
                }
            ],
            "relevance_judgement": 0.72802734375,
            "relevance_judgment_input_expanded": "# Title: Self-Distillation Amplifies Regularization in Hilbert Space\n# Venue: Neural Information Processing Systems\n# Authors: H. Mobahi, Mehrdad Farajtabar, P. Bartlett\n## Abstract\nKnowledge distillation introduced in the deep learning context is a method to transfer knowledge from one architecture to another. In particular, when the architectures are identical, this is called self-distillation. The idea is to feed in predictions of the trained model as new target values for retraining (and iterate this loop possibly a few times). It has been empirically observed that the self-distilled model often achieves higher accuracy on held out data. Why this happens, however, has been a mystery: the self-distillation dynamics does not receive any new information about the task and solely evolves by looping over training. To the best of our knowledge, there is no rigorous understanding of why this happens. This work provides the first theoretical analysis of self-distillation. We focus on fitting a nonlinear function to training data, where the model space is Hilbert space and fitting is subject to L2 regularization in this function space. We show that self-distillation iterations modify regularization by progressively limiting the number of basis functions that can be used to represent the solution. This implies (as we also verify empirically) that while a few rounds of self-distillation may reduce over-fitting, further rounds may lead to under-fitting and thus worse performance.\n## Introduction\nKnowledge distillation, originally introduced in the deep learning setting [Hinton et al., 2015], is a method that transfers knowledge from one architecture (teacher) to another (student), where often the student model is much smaller. The original formulation achieves this by training the student model using the output probability distribution of the teacher model in addition to original labels. The student model benefits from this \"dark knowledge\" (extra information in soft predictions) and often performs better than if it was trained on the actual labels. \n\nVarious extensions of this approach have been recently proposed, where instead of output predictions, the student matches other statistics from the teacher model such as intermediate feature representations [Romero et al., 2014], Jacobian matrices [Srinivas and Fleuret, 2018], distributions [Huang and Wang, 2017], Gram matrices [Yim et al., 2017]. Additional developments on knowledge distillation include its extensions to Bayesian settings [Korattikara Balan et al., 2015, Vadera andMarlin, 2020], uncertainty preservation [Tran et al., 2020], reinforcement learning [Hong et al., 2020, Teh et al., 2017, Ghosh et al., 2018], online distillation [lan et al., 2018], zero-shot learning [Nayak et al., 2019], multistep knowledge distillation [Mirzadeh et al., 2020], tackling catastrophic forgetting [Li and Hoiem, 2016], transfer of relational knowledge [Park et al., 2019], adversarial distillation [Wang et al., 2018]. \n\nThe special case when the teacher and student architectures are identical is called1 self-distillation. The idea is to feed in predictions of the trained model as new target values for retraining (and iterate this loop possibly a few times). It has been consistently observed that the self-distilled model often achieves higher accuracy on held out data [Furlanello et al., 2018, Yang et al., 2019, Ahn et al., 2019].",
            "reference_string": "[211096976 | Mobahi et al. | 2020 | Citations: 235]"
        },
        {
            "title": "Robustness-aware 2-bit quantization with real-time performance for neural network",
            "venue": "arXiv.org",
            "year": 2020,
            "reference_count": 29,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.11271, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109348932",
                    "name": "Xiaobin Li"
                },
                {
                    "authorId": "2292508",
                    "name": "Hongxu Jiang"
                },
                {
                    "authorId": "2118018442",
                    "name": "Shuangxi Huang"
                },
                {
                    "authorId": "152978298",
                    "name": "Fangzheng Tian"
                }
            ],
            "abstract": "Quantized neural network (NN) with a reduced bit precision is an effective solution to reduces the computational and memory resource requirements and plays a vital role in machine learning. However, it is still challenging to avoid the significant accuracy degradation due to its numerical approximation and lower redundancy. In this paper, a novel robustness-aware 2-bit quantization scheme is proposed for NN base on binary NN and generative adversarial network(GAN), witch improves the performance by enriching the information of binary NN, efficiently extract the structural information and considering the robustness of the quantized NN. Specifically, using shift addition operation to replace the multiply-accumulate in the quantization process witch can effectively speed the NN. Meanwhile, a structural loss between the original NN and quantized NN is proposed to such that the structural information of data is preserved after quantization. The structural information learned from NN not only plays an important role in improving the performance but also allows for further fine tuning of the quantization network by applying the Lipschitz constraint to the structural loss. In addition, we also for the first time take the robustness of the quantized NN into consideration and propose a non-sensitive perturbation loss function by introducing an extraneous term of spectral norm. The experiments are conducted on CIFAR-10 and ImageNet datasets with popular NN( such as MoblieNetV2, SqueezeNet, ResNet20, etc). The experimental results show that the proposed algorithm is more competitive under 2-bit-precision than the state-of-the-art quantization methods. Meanwhile, the experimental results also demonstrate that the proposed method is robust under the FGSM adversarial samples attack.",
            "corpus_id": 225040647,
            "sentences": [
                {
                    "corpus_id": "225040647",
                    "title": "Robustness-aware 2-bit quantization with real-time performance for neural network",
                    "text": "Knowledge distillation is a technique to transfer knowledge from a complex network (teacher network) to a compact network (student network), including data knowledge distillation and structure knowledge distillation. Wu et al. train the student network by taking the output of the teacher network as the soft target, and achieve the knowledge transfer by replacing the L2 loss with cross entropy loss [14]. Adriana Romero et al. fit the complexity of the teacher network by inputting more no-tag data into the student network. Junho Yim et al. optimized the knowledge transfer by refining knowledge distillation to the layer [15]. For the first time, we use knowledge distillation to extract the structural information of the network and construct new loss function to guide the quantization process of the network.",
                    "score": 0.7034631955377417,
                    "section_title": "Knowledge distillation",
                    "char_start_offset": 6179,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 216
                        },
                        {
                            "start": 217,
                            "end": 406
                        },
                        {
                            "start": 407,
                            "end": 526
                        },
                        {
                            "start": 527,
                            "end": 630
                        },
                        {
                            "start": 631,
                            "end": 815
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 401,
                            "end": 405,
                            "matchedPaperCorpusId": "9183542"
                        },
                        {
                            "start": 625,
                            "end": 629,
                            "matchedPaperCorpusId": "206596723"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7265625
                }
            ],
            "relevance_judgement": 0.7265625,
            "relevance_judgment_input_expanded": "# Title: Robustness-aware 2-bit quantization with real-time performance for neural network\n# Venue: arXiv.org\n# Authors: Xiaobin Li, Hongxu Jiang, Shuangxi Huang, Fangzheng Tian\n## Abstract\nQuantized neural network (NN) with a reduced bit precision is an effective solution to reduces the computational and memory resource requirements and plays a vital role in machine learning. However, it is still challenging to avoid the significant accuracy degradation due to its numerical approximation and lower redundancy. In this paper, a novel robustness-aware 2-bit quantization scheme is proposed for NN base on binary NN and generative adversarial network(GAN), witch improves the performance by enriching the information of binary NN, efficiently extract the structural information and considering the robustness of the quantized NN. Specifically, using shift addition operation to replace the multiply-accumulate in the quantization process witch can effectively speed the NN. Meanwhile, a structural loss between the original NN and quantized NN is proposed to such that the structural information of data is preserved after quantization. The structural information learned from NN not only plays an important role in improving the performance but also allows for further fine tuning of the quantization network by applying the Lipschitz constraint to the structural loss. In addition, we also for the first time take the robustness of the quantized NN into consideration and propose a non-sensitive perturbation loss function by introducing an extraneous term of spectral norm. The experiments are conducted on CIFAR-10 and ImageNet datasets with popular NN( such as MoblieNetV2, SqueezeNet, ResNet20, etc). The experimental results show that the proposed algorithm is more competitive under 2-bit-precision than the state-of-the-art quantization methods. Meanwhile, the experimental results also demonstrate that the proposed method is robust under the FGSM adversarial samples attack.\n## Knowledge distillation\nKnowledge distillation is a technique to transfer knowledge from a complex network (teacher network) to a compact network (student network), including data knowledge distillation and structure knowledge distillation. Wu et al. train the student network by taking the output of the teacher network as the soft target, and achieve the knowledge transfer by replacing the L2 loss with cross entropy loss [14]. Adriana Romero et al. fit the complexity of the teacher network by inputting more no-tag data into the student network. Junho Yim et al. optimized the knowledge transfer by refining knowledge distillation to the layer [15]. For the first time, we use knowledge distillation to extract the structural information of the network and construct new loss function to guide the quantization process of the network.",
            "reference_string": "[225040647 | Li et al. | 2020 | Citations: 1]"
        },
        {
            "title": "Multi-Fidelity Neural Architecture Search With Knowledge Distillation",
            "venue": "IEEE Access",
            "year": 2020,
            "reference_count": 79,
            "citation_count": 15,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10007805.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.08341, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144035647",
                    "name": "I. Trofimov"
                },
                {
                    "authorId": "143888131",
                    "name": "Nikita Klyuchnikov"
                },
                {
                    "authorId": "49614468",
                    "name": "Mikhail Salnikov"
                },
                {
                    "authorId": "8792436",
                    "name": "Alexander N. Filippov"
                },
                {
                    "authorId": "51139941",
                    "name": "Evgeny Burnaev"
                }
            ],
            "abstract": "Neural architecture search (NAS) targets at finding the optimal architecture of a neural network for a problem or a family of problems. Evaluations of neural architectures are very time-consuming. One of the possible ways to mitigate this issue is to use low-fidelity evaluations, namely training on a part of a dataset, fewer epochs, with fewer channels, etc. In this paper, we propose a Bayesian multi-fidelity (MF) method for neural architecture search: MF-KD. The method relies on a new approach to low-fidelity evaluations of neural architectures by training for a few epochs using a knowledge distillation (KD). Knowledge distillation adds to a loss function a term forcing a network to mimic some teacher network. We carry out experiments on CIFAR-10, CIFAR-100, and ImageNet-16-120. We show that training for a few epochs with such a modified loss function leads to a better selection of neural architectures than training for a few epochs with a logistic loss. The proposed method outperforms several state-of-the-art baselines.",
            "corpus_id": 219687782,
            "sentences": [
                {
                    "corpus_id": "219687782",
                    "title": "Multi-Fidelity Neural Architecture Search With Knowledge Distillation",
                    "text": "The knowledge distillation (KD) assumes two models: a teacher and a student. The teacher is typically a large and accurate network or an ensemble. The student is trained to fit the softmax outputs of the teacher together with ground truth labels. The idea is that outputs of the teacher capture not only the information provided by ground truth labels but also the probabilities of other classes -\"dark knowledge\". The knowledge distillation can be summarized as follows. \n\nLet z i be logits (pre-softmax activations) and q i -probabilities of classes as predicted by a neural network. Knowledge distillation smooths z i with the temperature \u03c4 \n\nNeural networks often do very confident predictions (close to 0 or 1) and smoothing helps to provide for student more information during training [11]. The KD loss is a linear combination of the logistic loss and cross-entropy between predictions of the teacher and the student \n\nwhere z T i , z S i are logits of the teacher and the student, H(p, q) = \u2212p log(q) is the cross-entropy function. The factor \u03c4 2 is used for scaling gradients of both parts of the loss function to be the same order. In the rest of the paper, we will refer to this variant of the knowledge distillation as \"original KD\". \n\nOther variants of KD suggest matching feature maps of the student and the teacher with various discrepancy functions [33,45,39,30,2,28,12,38]. For example, the NST loss [12] uses Maximum Mean Discrepancy (MMD): \n\nwhere F T , F S are the feature maps of the teacher and the student, \n\nHere f i\u2022 T ,f j\u2022 S are feature map from the layers i, j of the teacher and the student respectively, k(x, y) is a kernel.",
                    "score": 0.6242708957028862,
                    "section_title": "Knowledge Distillation (KD)",
                    "char_start_offset": 6640,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 76
                        },
                        {
                            "start": 77,
                            "end": 146
                        },
                        {
                            "start": 147,
                            "end": 246
                        },
                        {
                            "start": 247,
                            "end": 414
                        },
                        {
                            "start": 415,
                            "end": 471
                        },
                        {
                            "start": 474,
                            "end": 585
                        },
                        {
                            "start": 586,
                            "end": 643
                        },
                        {
                            "start": 646,
                            "end": 797
                        },
                        {
                            "start": 798,
                            "end": 923
                        },
                        {
                            "start": 926,
                            "end": 1039
                        },
                        {
                            "start": 1040,
                            "end": 1141
                        },
                        {
                            "start": 1142,
                            "end": 1245
                        },
                        {
                            "start": 1248,
                            "end": 1390
                        },
                        {
                            "start": 1391,
                            "end": 1458
                        },
                        {
                            "start": 1461,
                            "end": 1529
                        },
                        {
                            "start": 1532,
                            "end": 1654
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1372,
                            "end": 1375,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 1375,
                            "end": 1378,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 1380,
                            "end": 1383,
                            "matchedPaperCorpusId": "52012952"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7265625
                }
            ],
            "relevance_judgement": 0.7265625,
            "relevance_judgment_input_expanded": "# Title: Multi-Fidelity Neural Architecture Search With Knowledge Distillation\n# Venue: IEEE Access\n# Authors: I. Trofimov, Nikita Klyuchnikov, Mikhail Salnikov, Alexander N. Filippov, Evgeny Burnaev\n## Abstract\nNeural architecture search (NAS) targets at finding the optimal architecture of a neural network for a problem or a family of problems. Evaluations of neural architectures are very time-consuming. One of the possible ways to mitigate this issue is to use low-fidelity evaluations, namely training on a part of a dataset, fewer epochs, with fewer channels, etc. In this paper, we propose a Bayesian multi-fidelity (MF) method for neural architecture search: MF-KD. The method relies on a new approach to low-fidelity evaluations of neural architectures by training for a few epochs using a knowledge distillation (KD). Knowledge distillation adds to a loss function a term forcing a network to mimic some teacher network. We carry out experiments on CIFAR-10, CIFAR-100, and ImageNet-16-120. We show that training for a few epochs with such a modified loss function leads to a better selection of neural architectures than training for a few epochs with a logistic loss. The proposed method outperforms several state-of-the-art baselines.\n## Knowledge Distillation (KD)\nThe knowledge distillation (KD) assumes two models: a teacher and a student. The teacher is typically a large and accurate network or an ensemble. The student is trained to fit the softmax outputs of the teacher together with ground truth labels. The idea is that outputs of the teacher capture not only the information provided by ground truth labels but also the probabilities of other classes -\"dark knowledge\". The knowledge distillation can be summarized as follows. \n\nLet z i be logits (pre-softmax activations) and q i -probabilities of classes as predicted by a neural network. Knowledge distillation smooths z i with the temperature \u03c4 \n\nNeural networks often do very confident predictions (close to 0 or 1) and smoothing helps to provide for student more information during training [11]. The KD loss is a linear combination of the logistic loss and cross-entropy between predictions of the teacher and the student \n\nwhere z T i , z S i are logits of the teacher and the student, H(p, q) = \u2212p log(q) is the cross-entropy function. The factor \u03c4 2 is used for scaling gradients of both parts of the loss function to be the same order. In the rest of the paper, we will refer to this variant of the knowledge distillation as \"original KD\". \n\nOther variants of KD suggest matching feature maps of the student and the teacher with various discrepancy functions [33,45,39,30,2,28,12,38]. For example, the NST loss [12] uses Maximum Mean Discrepancy (MMD): \n\nwhere F T , F S are the feature maps of the teacher and the student, \n\nHere f i\u2022 T ,f j\u2022 S are feature map from the layers i, j of the teacher and the student respectively, k(x, y) is a kernel.",
            "reference_string": "[219687782 | Trofimov et al. | 2020 | Citations: 15]"
        },
        {
            "title": "Decentralized and Incentivized Federated Learning: A Blockchain-Enabled Framework Utilising Compressed Soft-Labels and Peer Consistency",
            "venue": "IEEE Transactions on Services Computing",
            "year": 2024,
            "reference_count": 72,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/4629386/4629387/10330055.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TSC.2023.3336980?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TSC.2023.3336980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115471566",
                    "name": "Leon Witt"
                },
                {
                    "authorId": "2067050909",
                    "name": "Usama Zafar"
                },
                {
                    "authorId": "2115288561",
                    "name": "KuoYeh Shen"
                },
                {
                    "authorId": "46223357",
                    "name": "Felix Sattler"
                },
                {
                    "authorId": "2150385385",
                    "name": "Dan Li"
                },
                {
                    "authorId": "2268713838",
                    "name": "Songtao Wang"
                },
                {
                    "authorId": "2242938198",
                    "name": "Wojciech Samek"
                }
            ],
            "abstract": "Federated Learning (FL) has emerged as a powerful paradigm in Artificial Intelligence, facilitating the parallel training of Artificial Neural Networks on edge devices while safeguarding data privacy. Nonetheless, to encourage widespread adoption, Federated Learning Frameworks (FLFs) must tackle (i) the power imbalance between a central authority and its participants, and (ii) the challenge of equitably measuring and incentivizing contributions. Existing approaches to decentralize and incentivize FL processes are hindered by (i) computational overhead and (ii) uncertainty in contribution assessment (Witt et al. 2023), limiting FL's scalability beyond use cases where trust between participants and the server is established. This work introduces a cutting-edge, blockchain-enabled federated learning framework that incorporates Federated Knowledge Distillation (FD) with compressed 1-bit soft-labels, aggregated through a smart contract. Furthermore, we present the Peer Truth Serum for Federated Distillation (PTSFD), which cultivates an incentive-compatible ecosystem by rewarding honest participation based on an implicit yet effective comparison of worker contributions. The primary innovation stems from its lightweight architecture that simultaneously promotes decentralization and incentivization, addressing critical challenges in contemporary FL approaches.",
            "corpus_id": 265502078,
            "sentences": [
                {
                    "corpus_id": "265502078",
                    "title": "Decentralized and Incentivized Federated Learning: A Blockchain-Enabled Framework Utilising Compressed Soft-Labels and Peer Consistency",
                    "text": "1) Knowledge Distillation: Knowledge Distillation (KD), depicted in Fig. 1, is a technique in deep learning where a smaller NN model (often called the \"student\") is trained to mimic the behavior of a larger, pre-trained model (referred to as the \"teacher\") [27]. This is accomplished not by transferring the model parameters directly, but rather by aligning the output distributions of both models. Traditional training methods involve training a model directly on ground-truth labels, using a cross-entropy loss that measures the discrepancy between the model's predictions and these true labels. In contrast, KD employs a divergence-based loss, such as the Kullback-Leibler (KL) divergence, to measure the difference between the student's predicted probabilities and those of the teacher model. This divergence provides insights into how closely the student is able to mimic the behavior of its teacher. A distinct feature of KD is the use of \"softened\" labels. In traditional classification tasks, hard labels are used, which unequivocally classify a data point into one category. However, the teacher model in KD provides \"soft\" labels in the form of probabilities, indicating the confidence levels across various categories. These probabilities can be further softened using a temperature parameter T to yield a smoother distribution, capturing the nuances of decision boundaries and offering richer guidance to the student model. This process allows the student to inherit not just the overt knowledge from the ground-truth labels but also the implicit, or \"dark\", knowledge embedded in the teacher model's predictions. Since only soft-labels are necessary to perform backpropagation, models with varying architectures can learn from the teacher. The appeal of KD lies in its ability to produce compact models with performance that closely mirrors that of much larger networks. These compact models are advantageous for deployment in resource-constrained environments, such as mobile devices or edge devices, without sacrificing much in terms of accuracy. With the above foundation in KD, we can delve deeper into its application in the FL setup, specifically focusing on Federated Distillation.",
                    "score": 0.5735960177804764,
                    "section_title": "B. Knowledge Distillation and Federated Settings",
                    "char_start_offset": 4984,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 262
                        },
                        {
                            "start": 263,
                            "end": 398
                        },
                        {
                            "start": 399,
                            "end": 597
                        },
                        {
                            "start": 598,
                            "end": 796
                        },
                        {
                            "start": 797,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 963
                        },
                        {
                            "start": 964,
                            "end": 1083
                        },
                        {
                            "start": 1084,
                            "end": 1229
                        },
                        {
                            "start": 1230,
                            "end": 1435
                        },
                        {
                            "start": 1436,
                            "end": 1625
                        },
                        {
                            "start": 1626,
                            "end": 1752
                        },
                        {
                            "start": 1753,
                            "end": 1883
                        },
                        {
                            "start": 1884,
                            "end": 2061
                        },
                        {
                            "start": 2062,
                            "end": 2201
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7255859375
                }
            ],
            "relevance_judgement": 0.7255859375,
            "relevance_judgment_input_expanded": "# Title: Decentralized and Incentivized Federated Learning: A Blockchain-Enabled Framework Utilising Compressed Soft-Labels and Peer Consistency\n# Venue: IEEE Transactions on Services Computing\n# Authors: Leon Witt, Usama Zafar, KuoYeh Shen, Felix Sattler, Dan Li, Songtao Wang, Wojciech Samek\n## Abstract\nFederated Learning (FL) has emerged as a powerful paradigm in Artificial Intelligence, facilitating the parallel training of Artificial Neural Networks on edge devices while safeguarding data privacy. Nonetheless, to encourage widespread adoption, Federated Learning Frameworks (FLFs) must tackle (i) the power imbalance between a central authority and its participants, and (ii) the challenge of equitably measuring and incentivizing contributions. Existing approaches to decentralize and incentivize FL processes are hindered by (i) computational overhead and (ii) uncertainty in contribution assessment (Witt et al. 2023), limiting FL's scalability beyond use cases where trust between participants and the server is established. This work introduces a cutting-edge, blockchain-enabled federated learning framework that incorporates Federated Knowledge Distillation (FD) with compressed 1-bit soft-labels, aggregated through a smart contract. Furthermore, we present the Peer Truth Serum for Federated Distillation (PTSFD), which cultivates an incentive-compatible ecosystem by rewarding honest participation based on an implicit yet effective comparison of worker contributions. The primary innovation stems from its lightweight architecture that simultaneously promotes decentralization and incentivization, addressing critical challenges in contemporary FL approaches.\n## B. Knowledge Distillation and Federated Settings\n1) Knowledge Distillation: Knowledge Distillation (KD), depicted in Fig. 1, is a technique in deep learning where a smaller NN model (often called the \"student\") is trained to mimic the behavior of a larger, pre-trained model (referred to as the \"teacher\") [27]. This is accomplished not by transferring the model parameters directly, but rather by aligning the output distributions of both models. Traditional training methods involve training a model directly on ground-truth labels, using a cross-entropy loss that measures the discrepancy between the model's predictions and these true labels. In contrast, KD employs a divergence-based loss, such as the Kullback-Leibler (KL) divergence, to measure the difference between the student's predicted probabilities and those of the teacher model. This divergence provides insights into how closely the student is able to mimic the behavior of its teacher. A distinct feature of KD is the use of \"softened\" labels. In traditional classification tasks, hard labels are used, which unequivocally classify a data point into one category. However, the teacher model in KD provides \"soft\" labels in the form of probabilities, indicating the confidence levels across various categories. These probabilities can be further softened using a temperature parameter T to yield a smoother distribution, capturing the nuances of decision boundaries and offering richer guidance to the student model. This process allows the student to inherit not just the overt knowledge from the ground-truth labels but also the implicit, or \"dark\", knowledge embedded in the teacher model's predictions. Since only soft-labels are necessary to perform backpropagation, models with varying architectures can learn from the teacher. The appeal of KD lies in its ability to produce compact models with performance that closely mirrors that of much larger networks. These compact models are advantageous for deployment in resource-constrained environments, such as mobile devices or edge devices, without sacrificing much in terms of accuracy. With the above foundation in KD, we can delve deeper into its application in the FL setup, specifically focusing on Federated Distillation.",
            "reference_string": "[265502078 | Witt et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 36,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.18119, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2262406221",
                    "name": "Yeongseo Jung"
                },
                {
                    "authorId": "2262217139",
                    "name": "Eunseo Jung"
                },
                {
                    "authorId": "2262372006",
                    "name": "Lei Chen"
                }
            ],
            "abstract": "In Conversational Recommendation System (CRS), an agent is asked to recommend a set of items to users within natural language conversations. To address the need for both conversational capability and personalized recommendations, prior works have utilized separate recommendation and dialogue modules. However, such approach inevitably results in a discrepancy between recommendation results and generated responses. To bridge the gap, we propose a multi-task learning for a unified CRS, where a single model jointly learns both tasks via Contextualized Knowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate and soft gate. The former selectively gates between two task-specific teachers, while the latter integrates knowledge from both teachers. Our gates are computed on-the-fly in a context-specific manner, facilitating flexible integration of relevant knowledge. Extensive experiments demonstrate that our single model significantly improves recommendation performance while enhancing fluency, and achieves comparable results in terms of diversity.",
            "corpus_id": 264555654,
            "sentences": [
                {
                    "corpus_id": "264555654",
                    "title": "Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation",
                    "text": "The core idea behind Knowledge Distillation (KD) (Hinton et al., 2015) is transferring knowledge of a high-capacity teacher network to a relatively smaller student model. In knowledge distillation, a student network is guided by not only a one-hot encoded ground-truth but also a soft target mapped by the teacher network (probability distribution). This is known to transfer a class-wise relation mapped by a teacher is commonly termed the dark knowledge. Given a data sample from a joint distribution (x, y) \u2208 X \u00d7 Y, a student model is optimized by combining two cross-entropy terms. \n\nwhere |Y | and \u0177k denote the number of classes and a k-th target label (one-hot encoded) respectively. \u03b3, and P denote a balancing parameter, and a probability distribution scaled with a temperature. \u03b8 and \u03d5 are parameters of a student and teacher network respectively.",
                    "score": 0.962439488027186,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 6986,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 349
                        },
                        {
                            "start": 350,
                            "end": 456
                        },
                        {
                            "start": 457,
                            "end": 585
                        },
                        {
                            "start": 588,
                            "end": 690
                        },
                        {
                            "start": 691,
                            "end": 787
                        },
                        {
                            "start": 788,
                            "end": 857
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72412109375
                }
            ],
            "relevance_judgement": 0.72412109375,
            "relevance_judgment_input_expanded": "# Title: Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Yeongseo Jung, Eunseo Jung, Lei Chen\n## Abstract\nIn Conversational Recommendation System (CRS), an agent is asked to recommend a set of items to users within natural language conversations. To address the need for both conversational capability and personalized recommendations, prior works have utilized separate recommendation and dialogue modules. However, such approach inevitably results in a discrepancy between recommendation results and generated responses. To bridge the gap, we propose a multi-task learning for a unified CRS, where a single model jointly learns both tasks via Contextualized Knowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate and soft gate. The former selectively gates between two task-specific teachers, while the latter integrates knowledge from both teachers. Our gates are computed on-the-fly in a context-specific manner, facilitating flexible integration of relevant knowledge. Extensive experiments demonstrate that our single model significantly improves recommendation performance while enhancing fluency, and achieves comparable results in terms of diversity.\n## Knowledge Distillation\nThe core idea behind Knowledge Distillation (KD) (Hinton et al., 2015) is transferring knowledge of a high-capacity teacher network to a relatively smaller student model. In knowledge distillation, a student network is guided by not only a one-hot encoded ground-truth but also a soft target mapped by the teacher network (probability distribution). This is known to transfer a class-wise relation mapped by a teacher is commonly termed the dark knowledge. Given a data sample from a joint distribution (x, y) \u2208 X \u00d7 Y, a student model is optimized by combining two cross-entropy terms. \n\nwhere |Y | and \u0177k denote the number of classes and a k-th target label (one-hot encoded) respectively. \u03b3, and P denote a balancing parameter, and a probability distribution scaled with a temperature. \u03b8 and \u03d5 are parameters of a student and teacher network respectively.",
            "reference_string": "[264555654 | Jung et al. | 2023 | Citations: 5]"
        },
        {
            "title": "Improving the Latent Space of Image Style Transfer",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 31,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2205.12135",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.12135, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48442720",
                    "name": "Yun-Hao Bai"
                },
                {
                    "authorId": "47073960",
                    "name": "Cairong Wang"
                },
                {
                    "authorId": "2117729099",
                    "name": "C. Yuan"
                },
                {
                    "authorId": "2140245719",
                    "name": "Yanbo Fan"
                },
                {
                    "authorId": "2144537318",
                    "name": "Jue Wang"
                }
            ],
            "abstract": "Existing neural style transfer researches have studied to match statistical information between the deep features of content and style images, which were extracted by a pre-trained VGG, and achieved significant improvement in synthesizing artistic images. However, in some cases, the feature statistics from the pre-trained encoder may not be consistent with the visual style we perceived. For example, the style distance between images of different styles is less than that of the same style. In such an inappropriate latent space, the objective function of the existing methods will be optimized in the wrong direction, resulting in bad stylization results. In addition, the lack of content details in the features extracted by the pre-trained encoder also leads to the content leak problem. In order to solve these issues in the latent space used by style transfer, we propose two contrastive training schemes to get a refined encoder that is more suitable for this task. The style contrastive loss pulls the stylized result closer to the same visual style image and pushes it away from the content image. The content contrastive loss enables the encoder to retain more available details. We can directly add our training scheme to some existing style transfer methods and significantly improve their results. Extensive experimental results demonstrate the effectiveness and superiority of our methods.",
            "corpus_id": 249017724,
            "sentences": [
                {
                    "corpus_id": "249017724",
                    "title": "Improving the Latent Space of Image Style Transfer",
                    "text": "Knowledge distillation (KD) [26,27,28] is a model compression method, in which a student network is trained by learning the knowledge from a teacher network. The knowledge is expressed in the form of softened probability [28,29], which can reflect the inherent class similarity structure known as dark knowledge. The distillation objective encourages the output probability distribution over predictions from the student and teacher networks to be similar. With the help of additional information on top of the one-hot labels, the performance of student network can be boosted. This dark knowledge is mainly related to labels, so they are rarely used in low-level vision tasks (e.g., neural style transfer). Wang et al. [10] developed a collaborative knowledge distillation method to learn a much smaller model from pre-trained redundant VGG-19 for ultra-resolution style transfer. In our method, the pre-trained encoder is regarded as a regularizer to guarantee that the features extracted by the new encoder are near a suitable value.",
                    "score": 0.5833185260056334,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 8201,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 157
                        },
                        {
                            "start": 158,
                            "end": 312
                        },
                        {
                            "start": 313,
                            "end": 456
                        },
                        {
                            "start": 457,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 707
                        },
                        {
                            "start": 708,
                            "end": 881
                        },
                        {
                            "start": 882,
                            "end": 1036
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 35,
                            "end": 38,
                            "matchedPaperCorpusId": "102351826"
                        },
                        {
                            "start": 221,
                            "end": 225,
                            "matchedPaperCorpusId": "102351826"
                        },
                        {
                            "start": 225,
                            "end": 228,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 720,
                            "end": 724,
                            "matchedPaperCorpusId": "213178672"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7177734375
                }
            ],
            "relevance_judgement": 0.7177734375,
            "relevance_judgment_input_expanded": "# Title: Improving the Latent Space of Image Style Transfer\n# Venue: arXiv.org\n# Authors: Yun-Hao Bai, Cairong Wang, C. Yuan, Yanbo Fan, Jue Wang\n## Abstract\nExisting neural style transfer researches have studied to match statistical information between the deep features of content and style images, which were extracted by a pre-trained VGG, and achieved significant improvement in synthesizing artistic images. However, in some cases, the feature statistics from the pre-trained encoder may not be consistent with the visual style we perceived. For example, the style distance between images of different styles is less than that of the same style. In such an inappropriate latent space, the objective function of the existing methods will be optimized in the wrong direction, resulting in bad stylization results. In addition, the lack of content details in the features extracted by the pre-trained encoder also leads to the content leak problem. In order to solve these issues in the latent space used by style transfer, we propose two contrastive training schemes to get a refined encoder that is more suitable for this task. The style contrastive loss pulls the stylized result closer to the same visual style image and pushes it away from the content image. The content contrastive loss enables the encoder to retain more available details. We can directly add our training scheme to some existing style transfer methods and significantly improve their results. Extensive experimental results demonstrate the effectiveness and superiority of our methods.\n## Knowledge Distillation\nKnowledge distillation (KD) [26,27,28] is a model compression method, in which a student network is trained by learning the knowledge from a teacher network. The knowledge is expressed in the form of softened probability [28,29], which can reflect the inherent class similarity structure known as dark knowledge. The distillation objective encourages the output probability distribution over predictions from the student and teacher networks to be similar. With the help of additional information on top of the one-hot labels, the performance of student network can be boosted. This dark knowledge is mainly related to labels, so they are rarely used in low-level vision tasks (e.g., neural style transfer). Wang et al. [10] developed a collaborative knowledge distillation method to learn a much smaller model from pre-trained redundant VGG-19 for ultra-resolution style transfer. In our method, the pre-trained encoder is regarded as a regularizer to guarantee that the features extracted by the new encoder are near a suitable value.",
            "reference_string": "[249017724 | Bai et al. | 2022 | Citations: 0]"
        },
        {
            "title": "Learning from Matured Dumb Teacher for Fine Generalization",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 21,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.05776, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "117489847",
                    "name": "Heeseung Jung"
                },
                {
                    "authorId": "2111865928",
                    "name": "Kangil Kim"
                },
                {
                    "authorId": "2125037748",
                    "name": "Hoyong Kim"
                },
                {
                    "authorId": "2633176",
                    "name": "Jong-Hun Shin"
                }
            ],
            "abstract": "The flexibility of decision boundaries in neural networks that are unguided by training data is a well-known problem typically resolved with generalization methods. A surprising result from recent knowledge distillation (KD) literature is that random, untrained, and equally structured teacher networks can also vastly improve generalization performance. It raises the possibility of existence of undiscovered assumptions useful for generalization on an uncertain region. In this paper, we shed light on the assumptions by analyzing decision boundaries and confidence distributions of both simple and KD-based generalization methods. Assuming that a decision boundary exists to represent the most general tendency of distinction on an input sample space (i.e., the simplest hypothesis), we show the various limitations of methods when using the hypothesis. To resolve these limitations, we propose matured dumb teacher based KD, conservatively transferring the hypothesis for generalization of the student without massive destruction of trained information. In practical experiments on feed-forward and convolution neural networks for image classification tasks on MNIST, CIFAR-10, and CIFAR-100 datasets, the proposed method shows stable improvement to the best test performance in the grid search of hyperparameters. The analysis and results imply that the proposed method can provide finer generalization than existing methods.",
            "corpus_id": 236987020,
            "sentences": [
                {
                    "corpus_id": "236987020",
                    "title": "Learning from Matured Dumb Teacher for Fine Generalization",
                    "text": "Knowledge Distillation (KD) is a method of transferring information from a larger and cumbersome teacher model to a new student model, which induces good performance even that student is a small and shallow model [14]- [16]. The main factor of this effect is to transfer dark knowledge of the teacher as a smoothed soft label. [8], [17] To effectively use the dark knowledge, the softmax function is modified as follows: \n\nwhere T is temperature, and N is the number of classes. The new loss function that uses both hard and soft labels is defined as \n\nwhere L CE is the cross-entropy loss, y t and y s are the output probability of the teacher and student, and y gt is the ground truth.",
                    "score": 0.8855752423790144,
                    "section_title": "II. RELATED WORK A. Knowledge Distillation",
                    "char_start_offset": 3835,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 224
                        },
                        {
                            "start": 225,
                            "end": 326
                        },
                        {
                            "start": 327,
                            "end": 420
                        },
                        {
                            "start": 423,
                            "end": 478
                        },
                        {
                            "start": 479,
                            "end": 550
                        },
                        {
                            "start": 553,
                            "end": 687
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 219,
                            "end": 223,
                            "matchedPaperCorpusId": "212908749"
                        },
                        {
                            "start": 332,
                            "end": 336,
                            "matchedPaperCorpusId": "28671436"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71484375
                }
            ],
            "relevance_judgement": 0.71484375,
            "relevance_judgment_input_expanded": "# Title: Learning from Matured Dumb Teacher for Fine Generalization\n# Venue: arXiv.org\n# Authors: Heeseung Jung, Kangil Kim, Hoyong Kim, Jong-Hun Shin\n## Abstract\nThe flexibility of decision boundaries in neural networks that are unguided by training data is a well-known problem typically resolved with generalization methods. A surprising result from recent knowledge distillation (KD) literature is that random, untrained, and equally structured teacher networks can also vastly improve generalization performance. It raises the possibility of existence of undiscovered assumptions useful for generalization on an uncertain region. In this paper, we shed light on the assumptions by analyzing decision boundaries and confidence distributions of both simple and KD-based generalization methods. Assuming that a decision boundary exists to represent the most general tendency of distinction on an input sample space (i.e., the simplest hypothesis), we show the various limitations of methods when using the hypothesis. To resolve these limitations, we propose matured dumb teacher based KD, conservatively transferring the hypothesis for generalization of the student without massive destruction of trained information. In practical experiments on feed-forward and convolution neural networks for image classification tasks on MNIST, CIFAR-10, and CIFAR-100 datasets, the proposed method shows stable improvement to the best test performance in the grid search of hyperparameters. The analysis and results imply that the proposed method can provide finer generalization than existing methods.\n## II. RELATED WORK A. Knowledge Distillation\nKnowledge Distillation (KD) is a method of transferring information from a larger and cumbersome teacher model to a new student model, which induces good performance even that student is a small and shallow model [14]- [16]. The main factor of this effect is to transfer dark knowledge of the teacher as a smoothed soft label. [8], [17] To effectively use the dark knowledge, the softmax function is modified as follows: \n\nwhere T is temperature, and N is the number of classes. The new loss function that uses both hard and soft labels is defined as \n\nwhere L CE is the cross-entropy loss, y t and y s are the output probability of the teacher and student, and y gt is the ground truth.",
            "reference_string": "[236987020 | Jung et al. | 2021 | Citations: 2]"
        },
        {
            "title": "Self-Knowledge Distillation via Progressive Associative Learning",
            "venue": "Electronics",
            "year": 2024,
            "reference_count": 49,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2079-9292/13/11/2062/pdf?version=1716627360",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics13112062?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics13112062, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292247795",
                    "name": "Haoran Zhao"
                },
                {
                    "authorId": "2297863565",
                    "name": "Yanxian Bi"
                },
                {
                    "authorId": "2304028661",
                    "name": "Shuwen Tian"
                },
                {
                    "authorId": "2237771070",
                    "name": "Jian Wang"
                },
                {
                    "authorId": "40075749",
                    "name": "Peiying Zhang"
                },
                {
                    "authorId": "2292213958",
                    "name": "Zhaopeng Deng"
                },
                {
                    "authorId": "2301512529",
                    "name": "Kai Liu"
                }
            ],
            "abstract": "As a specific form of knowledge distillation (KD), self-knowledge distillation enables a student network to progressively distill its own knowledge without relying on a pretrained, complex teacher network; however, recent studies of self-KD have discovered that additional dark knowledge captured by auxiliary architecture or data augmentation could create better soft targets for enhancing the network but at the cost of significantly more computations and/or parameters. Moreover, most existing self-KD methods extract the soft label as a supervisory signal from individual input samples, which overlooks the knowledge of relationships among categories. Inspired by human associative learning, we propose a simple yet effective self-KD method named associative learning for self-distillation (ALSD), which progressively distills richer knowledge regarding the relationships between categories across independent samples. Specifically, in the process of distillation, the propagation of knowledge is weighted based on the intersample relationship between associated samples generated in different minibatches, which are progressively estimated with the current network. In this way, our ALSD framework achieves knowledge ensembling progressively across multiple samples using a single network, resulting in minimal computational and memory overhead compared to existing ensembling methods. Extensive experiments demonstrate that our ALSD method consistently boosts the classification performance of various architectures on multiple datasets. Notably, ALSD pushes forward the self-KD performance to 80.10% on CIFAR-100, which exceeds the standard backpropagation by 4.81%. Furthermore, we observe that the proposed method shows comparable performance with the state-of-the-art knowledge distillation methods without the pretrained teacher network.",
            "corpus_id": 270077283,
            "sentences": [
                {
                    "corpus_id": "270077283",
                    "title": "Self-Knowledge Distillation via Progressive Associative Learning",
                    "text": "In this section, we first briefly introduce the most related works of knowledge distillation.Then we specifically review recent self-distillation works.\n\nKnowledge distillation is a widely used paradigm for model compression, which transfers knowledge from a complex teacher model to a compact student model.To be specific, the teacher network has high accuracy and huge parameters, while the student network is not as accurate as the teacher network but has fewer parameters.Through knowledge distillation, we hope that the student network can approach or exceed the teacher network as much as possible.In this way, we obtain a compact student network with a similar prediction effect as the teacher network.Ba et al. [30] first proposed a method that uses the teacher's logits before the softmax as the regression target to train the student network, which completes the imitation of the teacher network by forcing the student network to mimic the teacher network's logits.Hinton et al. [12] first proposed to use the soft outputs of the pretrained teacher network as dark knowledge to supervise the training of the student network.They introduced a temperature hyperparameter T and formulated the problem as \"knowledge distillation\".The student network is forced to learn the soft targets of the teacher network, which are obtained through using a high temperature T on the softmax inputs.In the process of knowledge transfer, soft targets often contain richer information than one-hot targets.Romero et al. [13] extended the knowledge distillation method proposed by Hinton et al.In their method, the student network can be deeper and narrower than the teacher network and improve the performance by learning the outputs of the teacher network and the features of the middle layer.All the above methods are offline distillation methods [31,32], which need a pretrained teacher network.\n\nIn contrast to these methods, online knowledge distillation trains the student network under the supervision of a teacher from scratch.For example, Zhang et al. [33] proposed a mutual learning method, which uses multiple neural networks.Zhao et al. [9] proposed a collaborative training method, which uses both an expert teacher and a from-scratch teacher to supervise the student.To reduce the computational cost, Zhou et al. [34] proposed to employ two different networks which share some low parameters and train separately.",
                    "score": 0.6571825275582734,
                    "section_title": "Related Work",
                    "char_start_offset": 5661,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 93
                        },
                        {
                            "start": 93,
                            "end": 152
                        },
                        {
                            "start": 154,
                            "end": 308
                        },
                        {
                            "start": 308,
                            "end": 476
                        },
                        {
                            "start": 476,
                            "end": 604
                        },
                        {
                            "start": 604,
                            "end": 709
                        },
                        {
                            "start": 709,
                            "end": 975
                        },
                        {
                            "start": 975,
                            "end": 1134
                        },
                        {
                            "start": 1134,
                            "end": 1236
                        },
                        {
                            "start": 1236,
                            "end": 1392
                        },
                        {
                            "start": 1392,
                            "end": 1497
                        },
                        {
                            "start": 1497,
                            "end": 1584
                        },
                        {
                            "start": 1584,
                            "end": 1785
                        },
                        {
                            "start": 1785,
                            "end": 1889
                        },
                        {
                            "start": 1891,
                            "end": 2026
                        },
                        {
                            "start": 2026,
                            "end": 2128
                        },
                        {
                            "start": 2128,
                            "end": 2272
                        },
                        {
                            "start": 2272,
                            "end": 2418
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 719,
                            "end": 723,
                            "matchedPaperCorpusId": "11536917"
                        },
                        {
                            "start": 989,
                            "end": 993,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1840,
                            "end": 1844,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 1844,
                            "end": 1847,
                            "matchedPaperCorpusId": "258888057"
                        },
                        {
                            "start": 2052,
                            "end": 2056,
                            "matchedPaperCorpusId": "26071966"
                        },
                        {
                            "start": 2140,
                            "end": 2143,
                            "matchedPaperCorpusId": "198179767"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71142578125
                }
            ],
            "relevance_judgement": 0.71142578125,
            "relevance_judgment_input_expanded": "# Title: Self-Knowledge Distillation via Progressive Associative Learning\n# Venue: Electronics\n# Authors: Haoran Zhao, Yanxian Bi, Shuwen Tian, Jian Wang, Peiying Zhang, Zhaopeng Deng, Kai Liu\n## Abstract\nAs a specific form of knowledge distillation (KD), self-knowledge distillation enables a student network to progressively distill its own knowledge without relying on a pretrained, complex teacher network; however, recent studies of self-KD have discovered that additional dark knowledge captured by auxiliary architecture or data augmentation could create better soft targets for enhancing the network but at the cost of significantly more computations and/or parameters. Moreover, most existing self-KD methods extract the soft label as a supervisory signal from individual input samples, which overlooks the knowledge of relationships among categories. Inspired by human associative learning, we propose a simple yet effective self-KD method named associative learning for self-distillation (ALSD), which progressively distills richer knowledge regarding the relationships between categories across independent samples. Specifically, in the process of distillation, the propagation of knowledge is weighted based on the intersample relationship between associated samples generated in different minibatches, which are progressively estimated with the current network. In this way, our ALSD framework achieves knowledge ensembling progressively across multiple samples using a single network, resulting in minimal computational and memory overhead compared to existing ensembling methods. Extensive experiments demonstrate that our ALSD method consistently boosts the classification performance of various architectures on multiple datasets. Notably, ALSD pushes forward the self-KD performance to 80.10% on CIFAR-100, which exceeds the standard backpropagation by 4.81%. Furthermore, we observe that the proposed method shows comparable performance with the state-of-the-art knowledge distillation methods without the pretrained teacher network.\n## Related Work\nIn this section, we first briefly introduce the most related works of knowledge distillation.Then we specifically review recent self-distillation works.\n\nKnowledge distillation is a widely used paradigm for model compression, which transfers knowledge from a complex teacher model to a compact student model.To be specific, the teacher network has high accuracy and huge parameters, while the student network is not as accurate as the teacher network but has fewer parameters.Through knowledge distillation, we hope that the student network can approach or exceed the teacher network as much as possible.In this way, we obtain a compact student network with a similar prediction effect as the teacher network.Ba et al. [30] first proposed a method that uses the teacher's logits before the softmax as the regression target to train the student network, which completes the imitation of the teacher network by forcing the student network to mimic the teacher network's logits.Hinton et al. [12] first proposed to use the soft outputs of the pretrained teacher network as dark knowledge to supervise the training of the student network.They introduced a temperature hyperparameter T and formulated the problem as \"knowledge distillation\".The student network is forced to learn the soft targets of the teacher network, which are obtained through using a high temperature T on the softmax inputs.In the process of knowledge transfer, soft targets often contain richer information than one-hot targets.Romero et al. [13] extended the knowledge distillation method proposed by Hinton et al.In their method, the student network can be deeper and narrower than the teacher network and improve the performance by learning the outputs of the teacher network and the features of the middle layer.All the above methods are offline distillation methods [31,32], which need a pretrained teacher network.\n\nIn contrast to these methods, online knowledge distillation trains the student network under the supervision of a teacher from scratch.For example, Zhang et al. [33] proposed a mutual learning method, which uses multiple neural networks.Zhao et al. [9] proposed a collaborative training method, which uses both an expert teacher and a from-scratch teacher to supervise the student.To reduce the computational cost, Zhou et al. [34] proposed to employ two different networks which share some low parameters and train separately.",
            "reference_string": "[270077283 | Zhao et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching",
            "venue": "IEEE Transactions on Cybernetics",
            "year": 2019,
            "reference_count": 92,
            "citation_count": 59,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1907.09643",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1907.09643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "50981688",
                    "name": "Haoran Zhao"
                },
                {
                    "authorId": "144326521",
                    "name": "Xin Sun"
                },
                {
                    "authorId": "1964397",
                    "name": "Junyu Dong"
                },
                {
                    "authorId": "10944885",
                    "name": "Changrui Chen"
                },
                {
                    "authorId": "2087106420",
                    "name": "Zihe Dong"
                }
            ],
            "abstract": "High storage and computational costs obstruct deep neural networks to be deployed on resource-constrained devices. Knowledge distillation (KD) aims to train a compact student network by transferring knowledge from a larger pretrained teacher model. However, most existing methods on KD ignore the valuable information among the training process associated with training results. In this article, we provide a new collaborative teaching KD (CTKD) strategy which employs two special teachers. Specifically, one teacher trained from scratch (i.e., scratch teacher) assists the student step by step using its temporary outputs. It forces the student to approach the optimal path toward the final logits with high accuracy. The other pretrained teacher (i.e., expert teacher) guides the student to focus on a critical region that is more useful for the task. The combination of the knowledge from two special teachers can significantly improve the performance of the student network in KD. The results of experiments on CIFAR-10, CIFAR-100, SVHN, Tiny ImageNet, and ImageNet datasets verify that the proposed KD method is efficient and achieves state-of-the-art performance.",
            "corpus_id": 198179767,
            "sentences": [
                {
                    "corpus_id": "198179767",
                    "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching",
                    "text": "Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process. \n\nLet us respectively denote x and y as the input of the DNNs and one-hot labels of our architecture. We let P T be the teacher network's softmax output as P T = sof tmax(a T ). Specifically, P T is obtained by applying softmax function on the un-normalized log probability values a T . Similarly, the same image fed to the student network to get the predictions P S = sof tmax(a S ). In the intermediate layers of the DNN, we denote the activation tensor A \u2208 R C\u00d7X\u00d7W with its corresponding layer. The pairs of teacher and student attention maps are denoted as F (A j T ) and F (A j S ) in vectorized form respectively [21]. And the standard cross entropy is denoted as H. Hinton et al. [19] extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 the student network will be trained to optimize the following loss function: \n\nMishra et al. [22] propose a new perspective view to jointly train a teacher network (full-precision) and a student network (low-precision) from scratch using knowledge distillation. The total loss function is as following: L(x; W t , W s ) = \u03b1H(y true , P T )+\u03b2H(y true , P S )+\u03b3H(a T , P S ) \n\n(2) In this case, the teacher and student network both train from scratch.",
                    "score": 0.6887001122979333,
                    "section_title": "B. Formulation",
                    "char_start_offset": 17544,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 59
                        },
                        {
                            "start": 60,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 308
                        },
                        {
                            "start": 309,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 516
                        },
                        {
                            "start": 517,
                            "end": 637
                        },
                        {
                            "start": 640,
                            "end": 739
                        },
                        {
                            "start": 740,
                            "end": 815
                        },
                        {
                            "start": 816,
                            "end": 924
                        },
                        {
                            "start": 925,
                            "end": 1022
                        },
                        {
                            "start": 1023,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1262
                        },
                        {
                            "start": 1263,
                            "end": 1454
                        },
                        {
                            "start": 1455,
                            "end": 1529
                        },
                        {
                            "start": 1530,
                            "end": 1636
                        },
                        {
                            "start": 1637,
                            "end": 1783
                        },
                        {
                            "start": 1786,
                            "end": 1968
                        },
                        {
                            "start": 1969,
                            "end": 2079
                        },
                        {
                            "start": 2082,
                            "end": 2156
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1257,
                            "end": 1261,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 1325,
                            "end": 1329,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1800,
                            "end": 1804,
                            "matchedPaperCorpusId": "3643430"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70361328125
                }
            ],
            "relevance_judgement": 0.70361328125,
            "relevance_judgment_input_expanded": "# Title: Highlight Every Step: Knowledge Distillation via Collaborative Teaching\n# Venue: IEEE Transactions on Cybernetics\n# Authors: Haoran Zhao, Xin Sun, Junyu Dong, Changrui Chen, Zihe Dong\n## Abstract\nHigh storage and computational costs obstruct deep neural networks to be deployed on resource-constrained devices. Knowledge distillation (KD) aims to train a compact student network by transferring knowledge from a larger pretrained teacher model. However, most existing methods on KD ignore the valuable information among the training process associated with training results. In this article, we provide a new collaborative teaching KD (CTKD) strategy which employs two special teachers. Specifically, one teacher trained from scratch (i.e., scratch teacher) assists the student step by step using its temporary outputs. It forces the student to approach the optimal path toward the final logits with high accuracy. The other pretrained teacher (i.e., expert teacher) guides the student to focus on a critical region that is more useful for the task. The combination of the knowledge from two special teachers can significantly improve the performance of the student network in KD. The results of experiments on CIFAR-10, CIFAR-100, SVHN, Tiny ImageNet, and ImageNet datasets verify that the proposed KD method is efficient and achieves state-of-the-art performance.\n## B. Formulation\nDeep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process. \n\nLet us respectively denote x and y as the input of the DNNs and one-hot labels of our architecture. We let P T be the teacher network's softmax output as P T = sof tmax(a T ). Specifically, P T is obtained by applying softmax function on the un-normalized log probability values a T . Similarly, the same image fed to the student network to get the predictions P S = sof tmax(a S ). In the intermediate layers of the DNN, we denote the activation tensor A \u2208 R C\u00d7X\u00d7W with its corresponding layer. The pairs of teacher and student attention maps are denoted as F (A j T ) and F (A j S ) in vectorized form respectively [21]. And the standard cross entropy is denoted as H. Hinton et al. [19] extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 the student network will be trained to optimize the following loss function: \n\nMishra et al. [22] propose a new perspective view to jointly train a teacher network (full-precision) and a student network (low-precision) from scratch using knowledge distillation. The total loss function is as following: L(x; W t , W s ) = \u03b1H(y true , P T )+\u03b2H(y true , P S )+\u03b3H(a T , P S ) \n\n(2) In this case, the teacher and student network both train from scratch.",
            "reference_string": "[198179767 | Zhao et al. | 2019 | Citations: 59]"
        },
        {
            "title": "Detect, Distill and Update: Learned DB Systems Facing Out of Distribution Data",
            "venue": "Proc. ACM Manag. Data",
            "year": 2022,
            "reference_count": 92,
            "citation_count": 18,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://wrap.warwick.ac.uk/172075/1/WRAP-Detect-distill-update-learned-DB-distribution-data-22.pdf",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.05508, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "151499040",
                    "name": "Meghdad Kurmanji"
                },
                {
                    "authorId": "1732298",
                    "name": "P. Triantafillou"
                }
            ],
            "abstract": "Machine Learning (ML) is changing DBs as many DB components are being replaced by ML models. One open problem in this setting is how to update such ML models in the presence of data updates. We start this investigation focusing on data insertions (dominating updates in analytical DBs). We study how to update neural network (NN) models when new data follows a different distribution (a.k.a. it is \"out-of-distribution\" -- OOD), rendering previously-trained NNs inaccurate. A requirement in our problem setting is that learned DB components should ensure high accuracy for tasks on old and new data (e.g., for approximate query processing (AQP), cardinality estimation (CE), synthetic data generation (DG), etc.). This paper proposes a novel updatability framework (DDUp). DDUp can provide updatability for different learned DB system components, even based on different NNs, without the high costs to retrain the NNs from scratch. DDUp entails two components: First, a novel, efficient, and principled statistical-testing approach to detect OOD data. Second, a novel model updating approach, grounded on the principles of transfer learning with knowledge distillation, to update learned models efficiently, while still ensuring high accuracy. We develop and showcase DDUp's applicability for three different learned DB components, AQP, CE, and DG, each employing a different type of NN. Detailed experimental evaluation using real and benchmark datasets for AQP, CE, and DG detail DDUp's performance advantages.",
            "corpus_id": 252816131,
            "sentences": [
                {
                    "corpus_id": "252816131",
                    "title": "Detect, Distill and Update: Learned DB Systems Facing Out of Distribution Data",
                    "text": "KD was first introduced in [22] for   by transferring knowledge from an accurate and \"cumbersome\" model, called teacher, to a smaller model called student. In its basic form, instead of fitting the student model directly to the actual data labels, one would use the class probability distribution learned by the teacher to fit the student model. Hinton et al. [22] argued that small probabilities in \"wrong\" label logits, known as \"soft labels\", include extra information called \"dark knowledge\" that result in better learning than actual \"hard labels\". Distillation has since been extensively studied. Figure 2 shows a general view of the principles of a distillation process. A small dataset referred to as transfer-set is fed into a pre-trained model (teacher) and a new model (student) to be trained. A   is calculated using the predictions of the pre-trained model instead of the actual labels. This loss and a typical loss using actual labels will be used to train the new model. To formulate knowledge distillation, consider a model with parameters \u0398, representing a function   ( for teacher) which has been trained via Eq. 2. We would like to transfer knowledge from this teacher model to a student model with parameter \u0398 \u2032 , representing a function   . This new model could be trained as follows: \n\nfor weight , distillation loss \u2112  , and transfer-set  . \n\n4.2 DDUp: Updating By Knowledge Distillation [12,66] showed that, for classification tasks, if instead of having a compact student model, one uses the same architecture of the teacher, and repeat distillation sequentially for several generations, the student models in the later generations could outperform the teacher model. This approach is called sequential self-distillation. Inspired by this and anticipating that this will be valid for our learning tasks, DDUp also employs a sequential self-distillation approach. \n\nTo update a model using KD, a copy of the previously trained model becomes the new student. Then, the student is updated using a distillation loss (to be defined soon). After updating, the previous teacher is replaced with the new updated model. This cycle repeats with every new insertion batch.",
                    "score": 0.6325162826798058,
                    "section_title": "General Knowledge Distillation (KD)",
                    "char_start_offset": 22591,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 155
                        },
                        {
                            "start": 156,
                            "end": 345
                        },
                        {
                            "start": 346,
                            "end": 553
                        },
                        {
                            "start": 554,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 677
                        },
                        {
                            "start": 678,
                            "end": 804
                        },
                        {
                            "start": 805,
                            "end": 899
                        },
                        {
                            "start": 900,
                            "end": 985
                        },
                        {
                            "start": 986,
                            "end": 1261
                        },
                        {
                            "start": 1262,
                            "end": 1305
                        },
                        {
                            "start": 1308,
                            "end": 1363
                        },
                        {
                            "start": 1366,
                            "end": 1692
                        },
                        {
                            "start": 1693,
                            "end": 1746
                        },
                        {
                            "start": 1747,
                            "end": 1887
                        },
                        {
                            "start": 1890,
                            "end": 1981
                        },
                        {
                            "start": 1982,
                            "end": 2058
                        },
                        {
                            "start": 2059,
                            "end": 2135
                        },
                        {
                            "start": 2136,
                            "end": 2186
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1411,
                            "end": 1415,
                            "matchedPaperCorpusId": "4110009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.703125
                }
            ],
            "relevance_judgement": 0.703125,
            "relevance_judgment_input_expanded": "# Title: Detect, Distill and Update: Learned DB Systems Facing Out of Distribution Data\n# Venue: Proc. ACM Manag. Data\n# Authors: Meghdad Kurmanji, P. Triantafillou\n## Abstract\nMachine Learning (ML) is changing DBs as many DB components are being replaced by ML models. One open problem in this setting is how to update such ML models in the presence of data updates. We start this investigation focusing on data insertions (dominating updates in analytical DBs). We study how to update neural network (NN) models when new data follows a different distribution (a.k.a. it is \"out-of-distribution\" -- OOD), rendering previously-trained NNs inaccurate. A requirement in our problem setting is that learned DB components should ensure high accuracy for tasks on old and new data (e.g., for approximate query processing (AQP), cardinality estimation (CE), synthetic data generation (DG), etc.). This paper proposes a novel updatability framework (DDUp). DDUp can provide updatability for different learned DB system components, even based on different NNs, without the high costs to retrain the NNs from scratch. DDUp entails two components: First, a novel, efficient, and principled statistical-testing approach to detect OOD data. Second, a novel model updating approach, grounded on the principles of transfer learning with knowledge distillation, to update learned models efficiently, while still ensuring high accuracy. We develop and showcase DDUp's applicability for three different learned DB components, AQP, CE, and DG, each employing a different type of NN. Detailed experimental evaluation using real and benchmark datasets for AQP, CE, and DG detail DDUp's performance advantages.\n## General Knowledge Distillation (KD)\nKD was first introduced in [22] for   by transferring knowledge from an accurate and \"cumbersome\" model, called teacher, to a smaller model called student. In its basic form, instead of fitting the student model directly to the actual data labels, one would use the class probability distribution learned by the teacher to fit the student model. Hinton et al. [22] argued that small probabilities in \"wrong\" label logits, known as \"soft labels\", include extra information called \"dark knowledge\" that result in better learning than actual \"hard labels\". Distillation has since been extensively studied. Figure 2 shows a general view of the principles of a distillation process. A small dataset referred to as transfer-set is fed into a pre-trained model (teacher) and a new model (student) to be trained. A   is calculated using the predictions of the pre-trained model instead of the actual labels. This loss and a typical loss using actual labels will be used to train the new model. To formulate knowledge distillation, consider a model with parameters \u0398, representing a function   ( for teacher) which has been trained via Eq. 2. We would like to transfer knowledge from this teacher model to a student model with parameter \u0398 \u2032 , representing a function   . This new model could be trained as follows: \n\nfor weight , distillation loss \u2112  , and transfer-set  . \n\n4.2 DDUp: Updating By Knowledge Distillation [12,66] showed that, for classification tasks, if instead of having a compact student model, one uses the same architecture of the teacher, and repeat distillation sequentially for several generations, the student models in the later generations could outperform the teacher model. This approach is called sequential self-distillation. Inspired by this and anticipating that this will be valid for our learning tasks, DDUp also employs a sequential self-distillation approach. \n\nTo update a model using KD, a copy of the previously trained model becomes the new student. Then, the student is updated using a distillation loss (to be defined soon). After updating, the previous teacher is replaced with the new updated model. This cycle repeats with every new insertion batch.",
            "reference_string": "[252816131 | Kurmanji et al. | 2022 | Citations: 18]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "264555654",
            "title": "Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation",
            "text": "The core idea behind Knowledge Distillation (KD) (Hinton et al., 2015) is transferring knowledge of a high-capacity teacher network to a relatively smaller student model. In knowledge distillation, a student network is guided by not only a one-hot encoded ground-truth but also a soft target mapped by the teacher network (probability distribution). This is known to transfer a class-wise relation mapped by a teacher is commonly termed the dark knowledge. Given a data sample from a joint distribution (x, y) \u2208 X \u00d7 Y, a student model is optimized by combining two cross-entropy terms. \n\nwhere |Y | and \u0177k denote the number of classes and a k-th target label (one-hot encoded) respectively. \u03b3, and P denote a balancing parameter, and a probability distribution scaled with a temperature. \u03b8 and \u03d5 are parameters of a student and teacher network respectively.",
            "score": 0.962439488027186,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6986,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 857
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72412109375
        },
        {
            "corpus_id": "251066725",
            "title": "Efficient One Pass Self-distillation with Zipf's Label Smoothing",
            "text": "Instead of imposing a fixed prior distribution, knowledge distillation was first proposed by Hinton in [12] to provide sample-level non-uniform soft labels. They demonstrated that the \"dark knowledge\" lies in the output distributions from a large capacity teacher network and benefits the student's representation learning. Recent works mainly explored to better transfer the \"dark knowledge\" and improve the efficiency from various aspects, such as reducing the difference between the teacher and student [3,5,18,34], designing student-friendly architecture [16,20], improving the distillation efficiency [7,14,27,29] and explaining the distillation's working mechanism [1,23]. \n\nIn this work, we focus on how to transfer the \"dark knowledge\" in an almost free manner. Furlanello et al. [7] proposed to improve the performance of the student network by distilling a teacher network with the same architecture. However, it is still a two-stage approach, which first trains the teacher and then distills knowledge to the student. To reduce the training time, many selfdistillation methods were proposed. They gain soft label supervision on the fly without the pretraining step.",
            "score": 0.9397752214692279,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5454,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 678
                },
                {
                    "start": 681,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1176
                }
            ],
            "ref_mentions": [
                {
                    "start": 509,
                    "end": 511,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 511,
                    "end": 514,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 514,
                    "end": 517,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 563,
                    "end": 566,
                    "matchedPaperCorpusId": "231925118"
                },
                {
                    "start": 606,
                    "end": 609,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 609,
                    "end": 612,
                    "matchedPaperCorpusId": "233714221"
                },
                {
                    "start": 612,
                    "end": 615,
                    "matchedPaperCorpusId": "70335318"
                },
                {
                    "start": 615,
                    "end": 618,
                    "matchedPaperCorpusId": "214727822"
                },
                {
                    "start": 674,
                    "end": 677,
                    "matchedPaperCorpusId": "235390933"
                },
                {
                    "start": 788,
                    "end": 791,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "247628251",
            "title": "Multitask Emotion Recognition Model with Knowledge Distillation and Task Discriminator",
            "text": "The knowledge distillation was proposed by hinton et al [9]. The output of the pretrained teacher model is scaled by softmax function with temperature and used as a soft label to train the student model. The student model learns inter-class similarity(dark knowledge) from the soft label of the teacher model and achieves performance similar to the teacher model despite being shallower than the teacher model. Zhang et al. [24] also improved performance through a self-distillation technique that configured the structure of the teacher model and the student model equally. Many teams used the knowledge distillation model at the 2021 ABAW and were also on the leaderboard. In particular, Deng et al. [7] made it possible to train deeper dark knowledge using the knowledge distillation technique, the ensemble technique, and the generation technique in which a trained student model becomes a teacher model and trains a new student model.",
            "score": 0.8914164894430114,
            "section_title": "B. Knowledge distillation",
            "char_start_offset": 3996,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 939
                }
            ],
            "ref_mentions": [
                {
                    "start": 424,
                    "end": 428,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62109375
        },
        {
            "corpus_id": "236987020",
            "title": "Learning from Matured Dumb Teacher for Fine Generalization",
            "text": "Knowledge Distillation (KD) is a method of transferring information from a larger and cumbersome teacher model to a new student model, which induces good performance even that student is a small and shallow model [14]- [16]. The main factor of this effect is to transfer dark knowledge of the teacher as a smoothed soft label. [8], [17] To effectively use the dark knowledge, the softmax function is modified as follows: \n\nwhere T is temperature, and N is the number of classes. The new loss function that uses both hard and soft labels is defined as \n\nwhere L CE is the cross-entropy loss, y t and y s are the output probability of the teacher and student, and y gt is the ground truth.",
            "score": 0.8855752423790144,
            "section_title": "II. RELATED WORK A. Knowledge Distillation",
            "char_start_offset": 3835,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 420
                },
                {
                    "start": 423,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 550
                },
                {
                    "start": 553,
                    "end": 687
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 223,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 332,
                    "end": 336,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71484375
        },
        {
            "corpus_id": "265384964",
            "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
            "text": "Knowledge distillation [34] involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements. In the process of knowledge distillation, logits are used as a basis for comparing the outputs of the student model with those of the teacher model. By examining logits, the model can measure the certainty or confidence of its predictions before applying the softmax function to obtain the final probability. The standard cross-entropy loss depends on the predicted probabilities and ground truth labels. Additionally, the loss function is extended to include the standard cross-entropy loss between the predictions of the student model and the ground truth labels, as well as an additional loss term that measures the discrepancy between the softened probabilities (obtained through a higher temperature softmax) of the teacher model's predictions and the corresponding predictions of the student model. This additional term ensures that the student model not only learns to predict the correct labels but also aims to replicate the softened outputs of the teacher model, effectively transferring its knowledge to the student model. By jointly minimizing these two loss terms, the student model can learn to generalize better and imitate the behavior of the more complex teacher model. The calculation of the probability for the class is as follows: \n\nHere, T represents the \"temperature\" of knowledge distillation. When T = 1, it corresponds to a normalized exponential function. With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\"",
            "score": 0.8160733512936827,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9617,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2255
                },
                {
                    "start": 2256,
                    "end": 2403
                },
                {
                    "start": 2404,
                    "end": 2463
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88232421875
        },
        {
            "corpus_id": "268820185",
            "title": "A Comprehensive Review of Knowledge Distillation in Computer Vision",
            "text": "A powerful machine learning approach called knowledge distillation enables the transfer of knowledge from a large and complex model to a more compact and computationally efficient model.The idea of knowledge distillation was first introduced by Bucila et al. [12] in 2006, and since then it has gained significant attention from researchers and practitioners in the field of machine learning.\n\nTraining a smaller model, known as a student model, to do the same tasks as the bigger model, known as a teacher model, is the fundamental aim of knowledge distillation.To achieve cutting-edge performance, the teacher model is often trained on a specific task and dataset.On the other hand, the student model is smaller and more computationally effective, making it suitable for deployment on devices with constrained resources.Transferring knowledge from the teacher model to the student model allows us to improve the performance of the student model without appreciably raising its complexity.The student model is trained using the same task and dataset as the teacher model throughout the knowledge distillation process (figure 2).The student model is trained according to the teacher model's predictions, which provide the student model additional knowledge and help it perform better.Knowledge transfer techniques include soft target training, attention transfer, and feature mimicry [52].\n\nFig 2 : The generic teacher-student framework for knowledge distillation [50] In the distillation loss, the lightweight student model duplicates the output produced by the teacher model by using the loss function.The weighted total contains the cross-entropy loss between the student's output and the real labels, as well as the cross-entropy loss between the student's output and the teacher's output (both after temperature scaling).\n\nwhere   is the student model output,  \u210e is the teacher model output, t is the true labels,  is a hyperparameter that regulates the weight given to each term, and T is the temperature parameter used for temperature scaling.The use of temperature scaling to soften soft logits produced by the softmax function is another important feature of knowledge distillation.This is accomplished by dividing the logits by a temperature parameter T, followed by the softmax function.The temperature parameter governs the probability distribution's \"softness,\" with higher temperatures resulting in softer distributions.",
            "score": 0.8127975981384403,
            "section_title": "INTRODUCTION TO KNOWLEDGE DISTILLATION",
            "char_start_offset": 12659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 392
                },
                {
                    "start": 394,
                    "end": 563
                },
                {
                    "start": 563,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1129
                },
                {
                    "start": 1129,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1389
                },
                {
                    "start": 1391,
                    "end": 1604
                },
                {
                    "start": 1604,
                    "end": 1826
                },
                {
                    "start": 1828,
                    "end": 2050
                },
                {
                    "start": 2050,
                    "end": 2191
                },
                {
                    "start": 2191,
                    "end": 2298
                },
                {
                    "start": 2298,
                    "end": 2434
                }
            ],
            "ref_mentions": [
                {
                    "start": 1384,
                    "end": 1388,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28173828125
        },
        {
            "corpus_id": "235186864",
            "title": "KnowSR: Knowledge Sharing among Homogeneous Agents in Multi-agent Reinforcement Learning",
            "text": "As mentioned earlier, KD has achieved great success in the field of computer vision though extracting dark knowledge. The key to extracting dark knowledge is to get the soft probability output of trained teachers with distillation. Here, we let a t be the input log of the final softmax layer of the teacher network, where a t = [a 1 , a 2 , ...., a j ]. The logits are converted into probabilities q t = [q 1 , q 2 , ...., q j ] using the following softmax function: q i = e a i \u03a3je a j where i represents the i-th neuron. [3] proposed softening the teacher probabilities with temperature T to extract more knowledge: \n\nCompared to true labels from datasets, the soft output of the teacher provides more information. Based on the same input x, the teacher and student networks produce probabilities q t (x) and q s (x) with Equation ( 4) respectively. The gap between q t (x) and q s (x) is usually penalised by the Kullback-Leibler (KL) divergence (Equation ( 6)): \n\n(5) \n\nwhere P(x) and Q(x) are two probability distributions of the random variable x. KD inspires us to believe that minimizing the gap between agent models through the skill of distillation is the essence of knowledge sharing. We then draw upon the KD thought in our MARL research to share knowledge and verify the feasibility of KnowSR in Section 3.3.",
            "score": 0.8086347574680224,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8906,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 972
                },
                {
                    "start": 975,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1322
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.405517578125
        },
        {
            "corpus_id": "269317596",
            "title": "Shared Knowledge Distillation Network for Object Detection",
            "text": "Firstly, we will recap the Knowledge Distillation method proposed by Hinton et al. [11]. This widely used method involves using category similarity as a guide for student networks. To regularize the network's learning, temperature is introduced to soften the initial categorical information, also referred to as 'dark knowledge'. The output probability of the teacher network and student network can be calculated as Equations ( 1) and (2). \n\nwhere T represents temperature, which can adjust the softening of the output probability. z i z j are the logits input for softmax, p i represents the output probability of each category in the teacher network, and q i represents the output probability of each category in the student network. Cross-entropy between the distilled teacher and student models calculates the soft loss. The hard loss is calculated based on the predicted values of the student model and the true values. \n\nThe ground truth label (also known as the hard target label) for the i-th sample is represented by y i . The teacher and student models' predicted probability are represented by p i and q i , respectively. The total loss function is calculated as below. \n\nThe loss function used in Knowledge Distillation involves two types of losses: soft target loss L soft and hard target loss L hard . The former guides the student to replicate the teacher's probability distribution, whereas the latter reflects the guidance from the actual ground truth labels. The parameter \u03b1 balances the effect of these two losses. During the Knowledge Distillation process, the student receives both the challenging and soft target knowledge. The loss function can be written as follows: \n\nwhere the loss of the cross-entropy is represented by L CE . The softmax function is represented by \u03c3. y represents the ground truth label. The output logits of the student and teacher networks are denoted by z S and z T . The balancing hyperparameter is represented by \u03b1.",
            "score": 0.7983046972626031,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 11431,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 1966
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75048828125
        },
        {
            "corpus_id": "256900863",
            "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK",
            "text": "Knowledge distillation [14] transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence [14], the mean squared error [37] and the Jensen-Shannon divergence [38], etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. [32] pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence. Furlanello et al. [33] demonstrated that the decoupled dark knowledge of teacher model can guide student model to have stronger generalization ability than that of teacher model. In this paper, we attempt to distill fuzzy dark knowledge from High-order TSK fuzzy classifier, and propose a novel born-again TSK fuzzy classifier endowed with the powerful classification performance as well as high interpretability.",
            "score": 0.7568996490814457,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 8754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1583
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 730,
                    "end": 734,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 759,
                    "end": 763,
                    "matchedPaperCorpusId": "233033712"
                },
                {
                    "start": 798,
                    "end": 802,
                    "matchedPaperCorpusId": "209405263"
                },
                {
                    "start": 959,
                    "end": 963,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1188,
                    "end": 1192,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "232404302",
            "title": "KnowRU: Knowledge Reuse via Knowledge Distillation in Multi-Agent Reinforcement Learning",
            "text": "As mentioned above, because the teacher model provides more useful information for the student model, KD has achieved success in the field of computer vision. The soft probabilities output of trained teachers is the key of distillation. Let a t be the input logits of final softmax layer of teacher network where a t = [a 1 , a 2 , ...., a j ]. The logits are converted into probabilities q t = [q 1 , q 2 , ...., q j ] with softmax function: \n\n\u03a3j e a j . In order to extract more information compared with true labels, [7] proposes to soften the teacher probabilities with temperature T : \n\nIn KD, such dark knowledge from soft output of teacher provides more information than true labels. Based on the same image input x, the teacher network and student network produce probability q t (x) and q s (x) with Equation (6). The gap between q t (x) and q s (x) is usually penalized by Kullback-Leibler divergence (Equation ( 8)) or cross-entropy loss : \n\nwhere P(x) and Q(x) are two probability distributions on random variable x. \n\nTemperature T in Equation ( 7) also aims to soften the output of the teacher network. Then the student network could reuse knowledge the teacher network by the back propagation of L KD . Knowledge distillation inspires us that minimizing the gap between previous agents and current agents with the skill of distillation is the essence of knowledge reusing. We then draw upon such KD thought in our MARL research to reuse knowledge and verify the feasibility of KnowRU in section 3.3.",
            "score": 0.7552242303934251,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1514
                }
            ],
            "ref_mentions": [
                {
                    "start": 818,
                    "end": 821,
                    "matchedPaperCorpusId": "33870153"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.515625
        },
        {
            "corpus_id": "229340414",
            "title": "Knowledge Transfer Based Fine-Grained Visual Classification",
            "text": "Knowledge distillation Learning has been widely studied and applied in machine learning. It is a technology to distill the knowledge of one model to another. Hinton et al. [16] firstly propose the concept of dark knowledge based on teacherstudent network, where teacher network is often a more complex network with desirable performance and fine generalization. With teacher's help the simpler student model with less parameter also has similar performance with teacher. Remero et al. [17] convey knowledge by feature maps, guiding student to acquire the ability to extract features. Zaforuyko et al. [18] propose the method that the student model is trained with the guidance of the teacher's attention map. \n\nAll aforementioned methods aim to make the student model has similar performance and fine generalization to the teacher model. However, in this paper, we guide the student model learn complementary knowledge with the teacher. We replace the KL divergence with the proposed OR-Loss and take spatial attention map as knowledge carrier.",
            "score": 0.7487647672855434,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6662,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1044
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.332275390625
        },
        {
            "corpus_id": "212855595",
            "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
            "text": "This subsection illustrates the objective functions of teacher and student networks, we introduce two additional loss functions to help our model efficiently transfer dark knowledge (i.e., the global type constraints and the relevance among relations) from teacher to student. \n\nThe Teacher Network In vanilla knowledge distillation, the teacher network is trained to fit one-hot labels. However, the ultimate goal of teacher is to provide better guidances for student, rather than achieve high accuracy simply. Inspired by Yang et al. (2019), we introduce a top score difference (TSD) loss to make the teacher's distribution softer. More concretely, we first pick up K classes with the highest confidence scores from the teacher's output, and then compute the gap between the confidence scores of the primary class and other K-1 classes: \n\nwhere \u03c1 k refers to the value of k-th largest element in the output distribution of teacher. Based on the global statistics, K is set to 3 empirically. We add the penalty term to standard ground-truth loss L T GT when training the teacher, facilitating it to distribute confidence to a few secondary relations: \n\nThe Student Network Typically, knowledge distillation transfers dark knowledge from the final output of teacher. Chen et al. (2017) demonstrate that using the intermediate representation of teacher as hint can stabilize the training process and improve the final performance of student. Here, we utilize the Kullback-Leibler divergence to measure the differences of corresponding branches between the teacher and student networks as hint learning loss: \n\nwhere L T L and L T G are the local and global logits of the teacher network respectively, L S L and L S A are the local and auxiliary logits of the student network respectively. Intuitively, it encourages the results of MAA to be similar with the scaled global logits of the teacher network. The loss of knowledge distillation is calculated as the sum of L HT and L KD with a weight factor \u03bb ht . As a result, the updated loss of student network is defined as follows:",
            "score": 0.7444051655534826,
            "section_title": "Objective Functions",
            "char_start_offset": 15526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 279,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1606
                },
                {
                    "start": 1609,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 524,
                    "end": 542,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 1267,
                    "end": 1285,
                    "matchedPaperCorpusId": "29308926"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "212855595",
            "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
            "text": "knowledge distillation is an effective framework to transfer knowledge from a neural network to another, which typically consists of two branches: a teacher T , which is usually a complex model or accompanied by some extra knowledge, and a student S, which is a small network that learns from the teacher (Hinton, Vinyals, and Dean 2015). In standard knowledge distillation model, the teacher network T is trained with ground-truth (i.e., hard labels) and outputs soft labels PT = softmax( ZT /\u03c4 ), in which ZT is the pre-softmax logits and \u03c4 is the temperature parameter that is normally set to 1. Similarly, one can define PS = softmax( ZS /\u03c4 ) for the student network S. In the training stage, S is required to match not only the ground-truth one-hot labels, but also the probability outputs of the teacher model: \n\nwhere L S GT is the ground-truth loss using one-hot labels, L KD is the knowledge distillation loss using teacher's soft labels and \u03bb is the coefficient to trade off such two terms. Typically, L GT is often the cross entropy loss in classification problems, and L KD is the Kullback-Leibler divergence to quantify the difference of output distribution from student to teacher: \n\nHenebry was 86 and lived in St Martinville. [0, \u2026, 0.69, \u2026, 0.17, ..., 0.14 where G is the one-hot distribution of ground-truth and G(i) is the i-th element of G. The soft distribution PT outputted by teacher model is more smooth by assigning non-zero probabilities to more than one class and yields smaller variance in gradients, which contain hidden information (also known as dark knowledge) about the relationship between different classes. By learning from soft labels, the student network inherits such dark knowledge and often has a faster convergence speed (Chen et al. 2017).",
            "score": 0.7416181802368398,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1782
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 337,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1763,
                    "end": 1780,
                    "matchedPaperCorpusId": "29308926"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69775390625
        },
        {
            "corpus_id": "227228204",
            "title": "A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models",
            "text": "Despite the recent advances of knowledge distillation technique, a clear understanding of where knowledge resides in a deep neural network and an optimal method for capturing knowledge from teacher and transferring it to student remains an open question. \n\nIn recent advances of knowledge distillation, many forms of knowledge have been defined (Jiaxi Tang et al., 2020 [18]) based on the teacher-student learning paradigm and have shown dramatic success and were analyzed empirically: \n\n\u2022 Layer activation [7] \u2022 Auxiliary information [23] \u2022 Jacobian matrix of the model parameters [72], [73] \u2022 Gram matrix derived from pairs of layers [74] \u2022 Activation boundary [34] Distillation loss for knowledge distillation training is a key factor which is used to penalize the student to transfer this Knowledge from the Teacher to the Student. \n\nFahad Sarfraz et al. [75] presented broad categorization of a diverse set of knowledge distillation methods which differ from each other with respect to how knowledge is defined and transferred from the teacher. Borrowing from their categorization, we cite two groups below to demonstrate how to capture the knowledge from teacher. \n\na) Response Distillation uses only the outputs of a Teacher to train the student to mimic it. C. Bucilu\u01ce et al. [4] proposed to use the logits of a teacher network as target for the student and to minimize the squared difference. Hinton et al. [6] proposed to minimize the KL divergence between the smoother output probabilities. In the original formulation, Hinton et al. [6] introduced a knowledge distillation compression framework and proposed mimicking the softened softmax output of the teacher using a temperature parameter. It raised the temperature of the final softmax function and minimize the Kullback-Leibler (KL) divergence between the smoother output probabilities. This softened output transfers more important information which is called dark knowledge compared to the hard output. When the soft targets have high entropy, they provide much more information per training case than hard targets and much less variance in the gradient between training cases, so the small model can often be trained on much less data than the original cumbersome model while using a much higher learning rate.",
            "score": 0.7343703194857405,
            "section_title": "A. Distilled Knowledge and Loss",
            "char_start_offset": 4459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 257,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2279
                }
            ],
            "ref_mentions": [
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "12874183"
                },
                {
                    "start": 582,
                    "end": 586,
                    "matchedPaperCorpusId": "21596346"
                },
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 663,
                    "end": 667,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1284,
                    "end": 1287,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1416,
                    "end": 1419,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1545,
                    "end": 1548,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5634765625
        },
        {
            "corpus_id": "203593636",
            "title": "Training convolutional neural networks with cheap convolutions and online distillation",
            "text": "After that, Hinton et al. [30] proposed dark knowledge (DK) to transfer knowledge from a large pre-trained model to a small student network, which uses an extra supervision provided by the soft final outputs of the pre-trained teacher. The extra supervision extracted from a pre-trained teacher model is often in form of class posterior probabilities [30], feature representations [31,32,40,4], distribution of intermediate activations [41,42], or inter-layer flow [43]). However, these distillation methods require at least two-stage training, including pre-training the teacher network and training the student network with an extra supervision, which is computation expensive during training. The more recently proposed deep mutual learning [44] overcomes this limitation by conducting an online distillation in one stage training between two peer student networks. Anil et al. [45] further extended this idea to accelerate the training of large-scale distributed neural networks. However, these online distillation methods use each student as the opposing teacher, which is not powerful and even limits the efficacy of knowledge discovery. \n\nActually, the above knowledge distillation methods redesign the new student network, which is thinner or more shallow than the teacher network. Different from the previous knowledge distillation schemes, we produce a student network by directly replacing the standard convolutions with different cheap convolutions without a complicated redesign. Moreover, to take the place of peer student as a teacher, we design a new online distillation method to online construct the strong teacher network, in which mutual learning between the teacher and the student model is used to further improve their performance. \n\nOther orthogonal methods. There are other four kinds of CNN compression methods, i.e., network architecture search [46,47,48,49,50,51], parameter quantization [14,15,13,52,53], network pruning [9,10,54,11,12,55,56,57,58] and low-rank decomposition [59,4,7,6,5,8]. Our method can be integrated with these orthogonal methods to further improve the performance, which are however orthogonal to the core contribution of this paper.",
            "score": 0.7266744819211212,
            "section_title": "Related Work",
            "char_start_offset": 9451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1754
                },
                {
                    "start": 1757,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 385,
                    "end": 388,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 388,
                    "end": 391,
                    "matchedPaperCorpusId": "172133986"
                },
                {
                    "start": 391,
                    "end": 393,
                    "matchedPaperCorpusId": "52915624"
                },
                {
                    "start": 436,
                    "end": 440,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 440,
                    "end": 443,
                    "matchedPaperCorpusId": "149834243"
                },
                {
                    "start": 465,
                    "end": 469,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 744,
                    "end": 748,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 881,
                    "end": 885,
                    "matchedPaperCorpusId": "2331610"
                },
                {
                    "start": 1872,
                    "end": 1876,
                    "matchedPaperCorpusId": "12713052"
                },
                {
                    "start": 1876,
                    "end": 1879,
                    "matchedPaperCorpusId": "12227989"
                },
                {
                    "start": 1879,
                    "end": 1882,
                    "matchedPaperCorpusId": "1740355"
                },
                {
                    "start": 1882,
                    "end": 1885,
                    "matchedPaperCorpusId": "743641"
                },
                {
                    "start": 1885,
                    "end": 1888,
                    "matchedPaperCorpusId": "206770867"
                },
                {
                    "start": 1929,
                    "end": 1932,
                    "matchedPaperCorpusId": "39867659"
                },
                {
                    "start": 1953,
                    "end": 1956,
                    "matchedPaperCorpusId": "2134321"
                },
                {
                    "start": 1956,
                    "end": 1959,
                    "matchedPaperCorpusId": "14089312"
                },
                {
                    "start": 1962,
                    "end": 1965,
                    "matchedPaperCorpusId": "20157893"
                },
                {
                    "start": 1968,
                    "end": 1971,
                    "matchedPaperCorpusId": "85459412"
                },
                {
                    "start": 1971,
                    "end": 1974,
                    "matchedPaperCorpusId": "51607445"
                },
                {
                    "start": 1974,
                    "end": 1977,
                    "matchedPaperCorpusId": "51608028"
                },
                {
                    "start": 2005,
                    "end": 2009,
                    "matchedPaperCorpusId": "7340116"
                },
                {
                    "start": 2009,
                    "end": 2011,
                    "matchedPaperCorpusId": "52915624"
                },
                {
                    "start": 2011,
                    "end": 2013,
                    "matchedPaperCorpusId": "1437449"
                },
                {
                    "start": 2013,
                    "end": 2015,
                    "matchedPaperCorpusId": "15602035"
                },
                {
                    "start": 2017,
                    "end": 2019,
                    "matchedPaperCorpusId": "5748557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5517578125
        },
        {
            "corpus_id": "264590688",
            "title": "Towards Generalized Multi-stage Clustering: Multi-view Self-distillation",
            "text": "The specific experimental settings are shown in Table I. The corresponding distillation methods are as follows: 1) FT [30] uses convolutional operations to transfer dark knowledge; 2) DeiT [31] proposes the distillation token and uses its representation with the teacher model's dark knowledge to compute the distillation loss; 3) KD-RP [32] exploits the differences in student and teacher networks to guide dark knowledge distillation; 4) KD [33] provides additional information about semantic similarity to model learning through the use of dark knowledge generated by self-distillation; 5) SD [34] exploits self-distillation to learn effective representations to group point clouds in the target domain. \n\nThe experimental results are shown in Fig. 2, with the corresponding distillation methods highlighted in red. The five tasks can all improve the performance of their backbone networks after exploiting the Knowledge distillation. Compared with pseudo labels, dark knowledge from the teacher contains the similarity information between classes [36]. With the incorporation of self-distillation, a weak teacher with much lower accuracy than students can still significantly improve the clustering accuracy of students. The success of selfdistillation even with a weak teacher is not solely due to the shared similarity information between classes, but rather due to the regularization of the distilled knowledge [36]. This demonstrates that dark knowledge of knowledge distillation plays a positive role in different learning tasks. \n\nIn the next section, we consider this observation and leverage knowledge self-distillation in Multi-stage MVC.",
            "score": 0.7262620551072124,
            "section_title": "B. Contrastive Learning",
            "char_start_offset": 10461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 706
                },
                {
                    "start": 709,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1538
                },
                {
                    "start": 1541,
                    "end": 1651
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 122,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 189,
                    "end": 193,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 337,
                    "end": 341,
                    "matchedPaperCorpusId": "245006036"
                },
                {
                    "start": 596,
                    "end": 600,
                    "matchedPaperCorpusId": "252918735"
                },
                {
                    "start": 1051,
                    "end": 1055,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 1418,
                    "end": 1422,
                    "matchedPaperCorpusId": "219962714"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76220703125
        },
        {
            "corpus_id": "272945756",
            "title": "Fast reconstruction of milling temperature field based on CNN-GRU machine learning models",
            "text": "Knowledge distillation is an instructor-student training structure that typically utilizes a student model with a simpler network structure to learn the knowledge provided by an instructor model that has been trained with a more complex network structure; this approach trades a slight performance loss for faster computation and smaller model parameters. Knowledge distillation works by training the student model with both the predictions of the teacher model (soft labeling) and the real data (hard labeling), and calculating the weighted total loss of the student model on both the soft and hard labels, essentially \"migrating\" the knowledge learned by the teacher model to the student model. The structure of the knowledge distillation strategy used in this paper is shown in Figure 6. \n\nThe specific knowledge distillation strategy process is as follows: \n\n(1) The raw data that has been preprocessed is input to both the teacher model and the student model, the teacher model is the CNN-GRU model constructed in the previous section, and the student model is a small model with a single CNN layer and a single GRU layer. ( 2  (5) The distillation loss and the student loss are weighted to obtain the total loss, and the gradient of each parameter is updated in the backpropagation process. \n\nThe following are the calculation formulas involved in the knowledge distillation operation process: \n\nKnowledge distillation soft labeling calculation formula as Equation ( 5): \n\nwhere T is the distillation temperature coefficient, used to control the \"hardness\" of the soft label. When T is larger, the soft label distribution area is uniform, more softened, when T is smaller, the soft label distribution closer to the hard label. \n\nDistillation loss of the loss function LOSSsoft formula is as Equation ( 6): \n\nwhere k is the total number of samples, p i (u i ,T) is the ith output of the teacher model at temperature coefficient T, and p i (z i ,T) is the ith output of the student model at temperature coefficient T. \n\nThe loss function LOSShard for student loss is formulated as Equation ( 7): \n\nwhere y i 7is a vector of hard labels representing the class i output of the unsoftened student model. \n\nThe total loss of knowledge distillation can be expressed as Equation ( 8): \n\n( )",
            "score": 0.722433617811018,
            "section_title": "Temperature boundary condition estimation model based on knowledge distillation with gated convolutional recurrent networks",
            "char_start_offset": 29942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 790
                },
                {
                    "start": 793,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1811
                },
                {
                    "start": 1814,
                    "end": 2021
                },
                {
                    "start": 2024,
                    "end": 2099
                },
                {
                    "start": 2102,
                    "end": 2204
                },
                {
                    "start": 2207,
                    "end": 2282
                },
                {
                    "start": 2285,
                    "end": 2288
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63623046875
        },
        {
            "corpus_id": "231925118",
            "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
            "text": "Since Hinton et al. [1] introduce the basic concept of knowledge distillation, where the dark knowledge in teacher models is given by the temperature-scaled representations of the softmax function, various kinds of information have been employed as the sources of knowledge for distillation from teachers to students. FitNets [18] distills intermediate features of a teacher network, where the student network transforms the intermediate features using guided layers and then calculates the difference between the guided layers and the intermediate features of teacher network. The position of distillation is shifted to the layers before the ReLU operations in [19], which also proposes the novel activation function and the partial L 2 loss function for effective knowledge transfer. Zagoruyko and Komodakis [20] argue importance of attention and propose an attention transfer (AT) method from teachers to students while Kim et al. [21] compute the factor information of the teacher representations using an autoencoder, which is decoded by students for knowledge transfer. Relational knowledge distillation (RKD) [22] introduces a technique to transfer relational information such as distances and angles of features. \n\nCRD [23] maximizes mutual information between a teacher and a student via contrastive learning. \n\nThere exist a couple of methods to perform knowledge distillation without teacher models. For example, ONE [24] distills knowledge from an ensemble of multiple students while BYOT [25] transfers knowledge from deeper layers to shallower ones. Besides, SSKD [26] distills self-supervised features of teachers to students for transferring richer knowledge.",
            "score": 0.7089397198980211,
            "section_title": "What to distill",
            "char_start_offset": 4237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1220
                },
                {
                    "start": 1223,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1675
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 23,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 662,
                    "end": 666,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1227,
                    "end": 1231,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1428,
                    "end": 1432,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1578,
                    "end": 1582,
                    "matchedPaperCorpusId": "219636179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6787109375
        },
        {
            "corpus_id": "236984375",
            "title": "Combine-Net: An Improved Filter Pruning Algorithm",
            "text": "Knowledge distillation (Figure 3) is put forward by Hinton et al. [7]. It is a widely used knowledge transfer technology in the deep learning field. First, a well-trained, robust, high-precision teacher network is needed. Its output is softened with temperature T to provide more information entropy, which extracts hidden knowledge behind its output layer. Then, a relatively small student network is trained to imitate the teacher network's probability output distribution, obtaining a better output result. The main idea of knowledge distillation. The label of the input image is cat; the probability is expressed as {0, 1, 0}. After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q'', which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics. \n\nTo improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become: After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q\", which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics. \n\nTo improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become: \n\nwhere Q T s , Q T t are the softmax probability distribution of the student network and teacher network after softening according to temperature T.",
            "score": 0.7067643886003013,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 15408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 2017
                },
                {
                    "start": 2020,
                    "end": 2167
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 69,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.765625
        },
        {
            "corpus_id": "212908749",
            "title": "Improved Knowledge Distillation via Teacher Assistant",
            "text": "Despite its huge popularity, there are few systematic and theoretical studies on how and why knowledge distillation improves neural network training. The so-called dark knowledge transferred in the process helps the student learn the finer structure of teacher network. Hinton, Vinyals, and Dean (2015) argues that the success of knowledge distillation is attributed to the logit distribution of the incorrect outputs, which provides information on the similarity between output categories. Furlanello et al. (2018) investigated the success of knowledge distillation via gradients of the loss where the soft-target part acts as an importance sampling weight based on the teachers confidence in its maximum value. Zhang et al. (2017) analyzed knowledge distillation from the posterior entropy viewpoint claiming that soft-targets bring robustness by regularizing a much more informed choice of alternatives than blind entropy regularization. Last but not least, Lopez-Paz et al. (2015) studied the effectiveness of knowledge distillation from the perspective of learning theory (Vapnik 1998) by studying the estimation error in empirical risk minimization framework. \n\nIn this paper, we take this last approach to support our claim on the effectiveness of introducing an intermediate network between student and teacher. Moreover, we empirically analyze it via visualizing the loss function.",
            "score": 0.7062366165635148,
            "section_title": "Related Work",
            "char_start_offset": 6977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1390
                }
            ],
            "ref_mentions": [
                {
                    "start": 270,
                    "end": 302,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 713,
                    "end": 732,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 961,
                    "end": 984,
                    "matchedPaperCorpusId": "3323727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.440673828125
        },
        {
            "corpus_id": "231648215",
            "title": "Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation",
            "text": "Before we dive into our method, we first introduce the process of knowledge distillation. The distillation process aims to transfer the knowledge of a large teacher network to a small student network. The objective is defined as follows:\n\nwhere f s and f t represent the features of student and teacher models respectively. L(\u00b7) is a loss function that evaluates the difference between the teacher and student models. Inspired by the success of transformer networks such as BERT (Sun et al. 2019;Jiao et al. 2019;Wang et al. 2020), our Figure 1: Overview of the proposed Learning to Augment (L2A) method. The generator generates augmented data based on both source and target domain data from a statistic stationary distribution (P s ). The reinforced selector selects useful augmented data to help the task of knowledge distillation and updates its policy according to the student network performance. distillation model is based on the BERT network (Vaswani et al. 2017). We consider three types of distillation strategy: L att based on attention information, L hidden on intermediate hidden representations, and L dark on the prediction outputs or dark knowledge, detailed as follows:\n\nwhere A i represents the attention matrix corresponding to the i-th self-attention head of the last BERT layer and h is the number of attention heads. H s , H t denotes the output of the last layer of student network and teacher network, respectively. W denotes a transformation matrix that transforms the hidden states of the student network into the same space as the teacher network's states. For dark knowledge based distillation, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits as follows:\n\nwhere g s and g t are the logits from the student and teacher respectively. T KD denotes the temperature value which controls the smoothness of the output distribution. Note that for the regression problem, the above loss is reformulated as the mean square error between the student's and the teacher's logits.\n\nWe combined the above three types of loss as our final KD loss, namely: L KD = L att + L hidden + L dark .",
            "score": 0.7042118689773702,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7626,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 479,
                    "end": 496,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 951,
                    "end": 972,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73583984375
        },
        {
            "corpus_id": "225040647",
            "title": "Robustness-aware 2-bit quantization with real-time performance for neural network",
            "text": "Knowledge distillation is a technique to transfer knowledge from a complex network (teacher network) to a compact network (student network), including data knowledge distillation and structure knowledge distillation. Wu et al. train the student network by taking the output of the teacher network as the soft target, and achieve the knowledge transfer by replacing the L2 loss with cross entropy loss [14]. Adriana Romero et al. fit the complexity of the teacher network by inputting more no-tag data into the student network. Junho Yim et al. optimized the knowledge transfer by refining knowledge distillation to the layer [15]. For the first time, we use knowledge distillation to extract the structural information of the network and construct new loss function to guide the quantization process of the network.",
            "score": 0.7034631955377417,
            "section_title": "Knowledge distillation",
            "char_start_offset": 6179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 815
                }
            ],
            "ref_mentions": [
                {
                    "start": 401,
                    "end": 405,
                    "matchedPaperCorpusId": "9183542"
                },
                {
                    "start": 625,
                    "end": 629,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7265625
        },
        {
            "corpus_id": "256901005",
            "title": "Learning From Biased Soft Labels",
            "text": "Recently, knowledge distillation Bucilu\u01ce et al. (2006); Ba and Caruana (2014); Hinton et al. (2015) has engendered remarkable achievements in a wide range of applications. Although it was firstly proposed for model compression by distilling knowledge from the big model (teacher) to the small model (student), considerable efforts have been devoted to figuring out the dark knowledge hidden in the outputs of the teacher model. The dark knowledge is compatibly utilized for transfer learning Vapnik et al. (2015); Zagoruyko and Komodakis (2016a); Noroozi et al. (2018). \n\nIn practice, the student loss is defined as the tradeoff between imitating the ground-truth label and imitating the output of the teacher model. Many studies Rusu et al. (2015); Furlanello et al. (2018) have demonstrated that learning from the teacher model can be more effective than the ground-truth labels. This seems counterintuitive since it challenges the correctness of the groundtruth labels. Apart from knowledge distillation, label smoothing Szegedy et al. (2016); Zoph et al. (2018) also softens the labels by incorporating uniform noise, which is a useful trick to improve generalization. Knowledge distillation and label smoothing are often analyzed together, and Yuan et al. (2020) elucidates that they can be unified as learning from soft labels. The essence of both is why the soft labels are effective. \n\nIn this paper, we mainly focus on the effectiveness of these soft labels. To be clarified, this paper investigates when the soft labels are effective, rather than when the soft labels are superior to ground-truth labels. It is apparent that, when the soft labels are close to the ground-truth labels, the student model will have an adequate performance. A straightforward question is, \"whether the large-bias soft labels are still effective?\". 2020) empirically demonstrates the poorly-trained teacher model can also improve the student model. However, it sets the tradeoff \u03b1 = 0.9 and temperature \u03c4 = 20, which means the defective soft labels are still close to the ground-truth label.",
            "score": 0.7022155958342475,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2080
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 54,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 56,
                    "end": 77,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 492,
                    "end": 512,
                    "matchedPaperCorpusId": "12874183"
                },
                {
                    "start": 547,
                    "end": 568,
                    "matchedPaperCorpusId": "4525685"
                },
                {
                    "start": 750,
                    "end": 774,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1024,
                    "end": 1045,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 1047,
                    "end": 1065,
                    "matchedPaperCorpusId": "12227989"
                },
                {
                    "start": 1249,
                    "end": 1267,
                    "matchedPaperCorpusId": "219962714"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50634765625
        },
        {
            "corpus_id": "233378598",
            "title": "Undistillable: Making A Nasty Teacher That CANNOT teach students",
            "text": "Knowledge distillation (Hinton et al., 2015) helps the training process of \"student\" networks by distilling knowledge from one or multiple well-trained \"teacher\" networks. The key idea is to leverage soft probabilities output of teacher networks, of which incorrect-class assignments reveal the way how teacher networks generalize from previous training. By mimicking probabilities output, student networks are able to imbibe the knowledge that teacher networks have discovered before, and the performance of student networks is usually better than those being trained with labels only. In what follows, we formally formulate the learning process of knowledge distillation. \n\nGiven a pre-trained teacher network f \u03b8 T (\u2022) and a student network f \u03b8 S (\u2022), where \u03b8 T and \u03b8 S denote the network parameters, the goal of knowledge distillation is to force the output probabilities of \n\n, the student network f \u03b8 S could be learned by the following: \n\nwhere KL(\u2022, \u2022) and X E(\u2022, \u2022) are Kullback-Leibler divergence (K-L divergence) and cross-entropy loss, respectively. The introduced \"softmax temperature\" function \u03c3 \u03c4s (\u2022) (Hinton et al., 2015) produces soft probabilities output when a large temperature \u03c4 s (usually greater than 1) is picked, and it decays to normal softmax function \u03c3(\u2022) when \u03c4 s equals 1. Another hyper-parameter \u03b1 is also introduced to balance between knowledge distillation and cost minimization.",
            "score": 0.7015320440410845,
            "section_title": "REVISITING KNOWLEDGE DISTILLATION",
            "char_start_offset": 8694,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1413
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54052734375
        },
        {
            "corpus_id": "234336288",
            "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification",
            "text": "Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model. IV. TEACHER-STUDENT LEARNING Knowledge distillation starts with training a larger model, the teacher 'T'. As it is trained on a heavier platform (GPU), it achieves high performance. Then a lightweight model known as student 'S' is deployed to learn from 'T'. Now, 'S' is supposed to give comparable performance as 'T' but with less memory and more speed. \n\nTo improve knowledge transfer from teacher to student various types of methods are researched. Assuming a trained 'T' has already eliminated some label errors contained in the ground truth data, the authors in [29] treated the hard label predicted by 'T' as the underlying knowledge. While in [30], the soft label produced by 'T', i.e., the classification probabilities, are focused to provide more information to transfer. In general, knowledge is transferred from the 'T' to 'S' by minimizing a loss function in which the target is the distribution of class probabilities predicted by 'T'. This probability distribution has the correct class at a very high probability (close to '1') with all other class probabilities very close to '0'. As such, it does not provide much information beyond the ground truth labels already provided in the dataset. For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process. The distillation related work can be categorized as below:",
            "score": 0.7005902905223443,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 8373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 57,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 411
                },
                {
                    "start": 414,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 1077,
                    "end": 1081,
                    "matchedPaperCorpusId": "11536917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7685546875
        },
        {
            "corpus_id": "236272577",
            "title": "Modeling microscopic and macroscopic information diffusion for rumor detection",
            "text": "To further improve the model performance on rumor detection task, inspired by knowledge distillation technique 43 -which involves capturing the \"dark knowledge\" from a teacher model to guide the learning of a student network, has emerged as an essential technique for model improving. We first train a teacher model via Algorithm 2, and then transfer the knowledge from the teacher model to a student model, here in our work, the student model has the same model architecture as the teacher model (self-distillation 44,45 ). Before introducing the concrete training procedure of MMRD with knowledge distillation, we first give the definition of the softmax with temperature: \n\nwhere \u03c4 is a temperature that is normally set to 1, using a higher value for temperature \u03c4 to produce a softer probability distribution over the class, which brings the advantage that the information carried by the negative label will be relatively amplified, and the model training will pay more attention to the negative label. \n\nThe concrete training procedure of the knowledge distillation is listed in Algorithm 4.6, and Figure 1C gives a visualization of Algorithm 4.6. The objective function of the knowledge distillation is a weighted average of two different objective functions. The first loss function is the cross-entropy with the soft targets and it is computed using the same high temperature \u03c4 t = in the softmax of the student model as was used for generating the soft targets from the teacher model. is soft output from student model. The second loss function is the cross-entropy with the ground truth. This is computed using exactly the same logits in softmax of the student model but at a temperature of 1. \n\nwhere y i is the ground truth and y \u03c4 H \u02c6= softmax(FC( ), = 1) \n\nis the hard output of student model. Finally, the objective function of knowledge distillation is: \n\nwhere \u03b2 is the balance weight, which always been a considerably lower value since the am- plitude of the gradients produced by the scale of the soft output as \u03c4 1 2 \u2215 . This ensures that the relative contributions of the hard and soft targets remain roughly unchanged. 43 gorithm 3. Training procedure of MMRD with knowledge distillation. \n\n\uf047 \uf050 , the max-order number K , temperature \u03c4 . \n\nInput: Student-optimized parameters \u0398. We now present the findings from our experimental evaluations.",
            "score": 0.7001076776037378,
            "section_title": "| Rumor detection with knowledge distilling",
            "char_start_offset": 20002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1869
                },
                {
                    "start": 1872,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2210
                },
                {
                    "start": 2213,
                    "end": 2259
                },
                {
                    "start": 2262,
                    "end": 2300
                },
                {
                    "start": 2301,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 113,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "199405591"
                },
                {
                    "start": 519,
                    "end": 521,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 2141,
                    "end": 2143,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.685546875
        },
        {
            "corpus_id": "233189631",
            "title": "Annealing Knowledge Distillation",
            "text": "In the original Knowledge distillation method by , which is referred to as KD in this paper, the student network is trained based on two guiding signals: first, the training dataset or hard labels, and second, the teacher network predictions, which is known as soft labels. Therefore, KD is trained based on a linear combination of two loss functions: the regular cross entropy loss function between the student outputs and hard labels, and the KD loss function to minimize the distance between the output predictions of the teacher and student networks at a particular temperature, T , on training samples:\n\nwhere H CE (.) and KL(.) are representing the cross entropy and KL divergence respectively, z s (x) and z t (x) are the output logits from the student and teacher networks, T is the temperature parameter, \u03c3(.) is the softmax function and \u03bb is a coefficient between [0,1] to control the contribution of the two loss functions. The above loss function minimizes the distance between the student model and both the underlying function and the teacher model assuming the teacher is a good approximation of the underlying function of the data. A particular problem with KD, that we would like to address in this paper, is that the larger the gap between the teacher and the student networks, the more difficult is their training using knowledge distillation (Lopez-Paz et al., 2015;Mirzadeh et al., 2019).",
            "score": 0.6969812732966992,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 4435,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.578125
        },
        {
            "corpus_id": "273811396",
            "title": "Decoupling Dark Knowledge via Block-Wise Logit Distillation for Feature-Level Alignment",
            "text": "The concept of knowledge distillation [16] was first proposed as a learning strategy that employs a larger teacher network to steer the training process of a smaller student network for various tasks [27,28]. The working mechanism of vanilla KD is often comprehended via the process of \"teaching\" [51], whereby the transfer of \"dark knowledge\" occurs through the use of soft labels, known as logits, provided by the teacher to the student. Recent studies examine how to select, express, and transfer the \"dark knowledge\" more practically and effectively, which can be categorized into two main types: logit-and feature-based distillations. \n\nLogits Distillation. In the wake of the earliest KD method based on temperature-regulated distillation [16], previous logit-based methods have concentrated mainly on effective regularization and optimization methods. DML [64] proposes a mutual learning method to train students and teachers simultaneously. TAKD [34] proposes using intermediary \"teacher assistants\" to transmit knowledge in a step-by-step manner in order to narrow the performance disparity between teachers and students. Additionally, several works [6,39] focus on interpreting the principles underlying KD. Recently, DKD [65] introduces an improved logit-based objective by decoupling the classical KD loss, which re-explores the potential of logitbased methods with comparable performance gains. \n\nFeature Distillation. To further enhance knowledge distillation, feature distillation is proposed to perform alignments on intermediate features as well as logit outputs, which can directly transfer teacher representations [5,14,15,41] or the correlation [36,37,47,48] from the teacher to the student. Feature methods are more likely to obtain high performance with extensive information from the teacher; however, tight feature alignment frequently relies on prior empirical observation and meticulous adjustment of hyperparameters.",
            "score": 0.6954746289714078,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 5421,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1407
                },
                {
                    "start": 1410,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 204,
                    "matchedPaperCorpusId": "267213"
                },
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "236912875"
                },
                {
                    "start": 297,
                    "end": 301,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 863,
                    "end": 867,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 954,
                    "end": 958,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1159,
                    "end": 1162,
                    "matchedPaperCorpusId": "212633769"
                },
                {
                    "start": 1162,
                    "end": 1165,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 1232,
                    "end": 1236,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1633,
                    "end": 1636,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 1636,
                    "end": 1639,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1665,
                    "end": 1669,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1669,
                    "end": 1672,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1675,
                    "end": 1678,
                    "matchedPaperCorpusId": "198179476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8291015625
        },
        {
            "corpus_id": "260704230",
            "title": "Teacher-Student Architecture for Knowledge Distillation: A Survey",
            "text": "Currently, most teacher-student architectures are employed on classification tasks, where intermediate feature embeddings and soft logits can be commonly represented as dark knowledge transferred to student networks. Moreover, more advanced training schemes and architecture designs are introduced to improve the efficiency of the distillation process. Although several works [193,194,195,196] focus on regression tasks, one promising research direction can be investigated in the theoretical studies of regression-based knowledge learning, such as the representation of dark knowledge on regression problems. The final predictions of teacher networks are represented as knowledge to be transferred to student networks [204,205], and student networks also aim to mimic the extracted feature representations from teacher networks [206,207]. With deeper theoretical studies on regression-based knowledge learning, teacherstudent architectures will be further effectively employed in practical applications.",
            "score": 0.6945528740339264,
            "section_title": "Theoretical Understandings of Regression-Based Knowledge Learning",
            "char_start_offset": 72996,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1004
                }
            ],
            "ref_mentions": [
                {
                    "start": 376,
                    "end": 381,
                    "matchedPaperCorpusId": "211572450"
                },
                {
                    "start": 381,
                    "end": 385,
                    "matchedPaperCorpusId": "233582109"
                },
                {
                    "start": 389,
                    "end": 393,
                    "matchedPaperCorpusId": "243791573"
                },
                {
                    "start": 724,
                    "end": 728,
                    "matchedPaperCorpusId": "233582109"
                },
                {
                    "start": 829,
                    "end": 834,
                    "matchedPaperCorpusId": "243791573"
                },
                {
                    "start": 834,
                    "end": 838,
                    "matchedPaperCorpusId": "199405345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6484375
        },
        {
            "corpus_id": "266671498",
            "title": "Insulator Defect Detection Based on ML-YOLOv5 Algorithm",
            "text": "Knowledge distillation is a technique that leverages knowledge transfer to train a lightweight student network from a larger teacher network. In this study, we apply the knowledge distillation method proposed by Mehta et al. [34] to distill the improved model. The knowledge distillation process is illustrated in Figure 7. As shown in the figure above, the output values of the Softmax function, with an added variable T, are used as soft targets to prevent minimal contributions to the loss function when the probability distribution has low entropy. The Softmax function is defined by the following equation: \n\nIn the equation, q i represents the true probability, z is a vector, and z i and z j are elements of the vector. T represents the temperature, where higher T values result in a larger entropy in the Softmax distribution, increasing the focus on training negative labels. \n\nThe overall loss consists of algorithmic and knowledge distillation losses, with \u03bb D used to balance these two types of losses. The total loss function is expressed as shown in Equation (2). \n\nIn this study, we made improvements to the YOLOv5s, a lightweight network model, and YOLOv5m, a deeper network model, as follows: \n\n(1) Integrated depthwise separable convolutions into the backbone. \n\n(2) Replaced the C3 module in the neck with an enhanced C2f_DG. \n\n(3) Replaced the feature pyramid network (FPN) with the modified feature pyramid network (MFPN). \n\nThe improved models based on YOLOv5s and YOLOv5m are, respectively, named M-YOLOv5s and M-YOLOv5m. We employed M-YOLOv5m as a teacher network to train the student model M-YOLOv5s. Through knowledge distillation, we improved model accuracy without adding extra parameters.",
            "score": 0.692607695804819,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 20496,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 884
                },
                {
                    "start": 887,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1077
                },
                {
                    "start": 1080,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1278
                },
                {
                    "start": 1281,
                    "end": 1344
                },
                {
                    "start": 1347,
                    "end": 1443
                },
                {
                    "start": 1446,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1717
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "21713986"
                },
                {
                    "start": 1073,
                    "end": 1076,
                    "matchedPaperCorpusId": "26856801"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49462890625
        },
        {
            "corpus_id": "257346971",
            "title": "An Improved Tuna-YOLO Model Based on YOLO v3 for Real-Time Tuna Detection Considering Lightweight Deployment",
            "text": "The calculations and parameter amounts of the network were reduced significantly after adopting the lightweight design, but so was the detection accuracy. To address this problem, knowledge distillation (KD), a joint training method by transferring \"knowledge\", was employed to improve the detection accuracy. The KD structure was shown in Figure 3. KD is the process of imitating the distillation in chemistry, using the softmax function with temperature parameters to \"distill\" the logit output from complex and large networks, so as to generate more information in categories. This part of the in-formation is called \"dark knowledge\". The additional information guides the simple and small network to learn more knowledge, and the two networks are called the teacher network and the student network, respectively. \n\nTo diversify the information distribution output by the teacher network, we used the temperature parameter \u03c4 to get soft prediction output by distilling logits output between the teacher network and student network. The same dataset was used because soft prediction output implied the information of the negative samples. With the help of SoftMax active function, the teacher network's class prediction probability distribution could be regarded as the soft target. Similarly, this method was used to get not only the soft prediction output but also the hard prediction output from the student network. As for the soft prediction output, soft prediction output and soft target were used to calculate loss value by loss function L so f t , which was a part of total loss. The L so f t was defined as: \n\nwhere P T i is the i-th soft target at time T, Q T i is the i-th soft prediction output at time T, N is the total number of samples and N = 27 in this paper. \n\nshown in Figure 3. KD is the process of imitating the distillation in chem softmax function with temperature parameters to \"distill\" the logit outpu and large networks, so as to generate more information in categories. This formation is called \"dark knowledge\". The additional information guides small network to learn more knowledge, and the two networks are called work and the student network, respectively. Figure 3. The KD structure.",
            "score": 0.6918832022867221,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 17198,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1618
                },
                {
                    "start": 1621,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2191
                },
                {
                    "start": 2192,
                    "end": 2201
                },
                {
                    "start": 2202,
                    "end": 2219
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76806640625
        },
        {
            "corpus_id": "278327499",
            "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law",
            "text": "Knowledge Distillation is a technique to transfer knowledge from a large, complex model (the teacher) to a smaller, efficient model (the student) [132]. The goal is to compress the teacher's expertise into the student while retaining performance. Two primary approaches exist: Model Distillation and Data Distillation. \n\nModel Distillation. Model Distillation directly transfers knowledge via the teacher's outputs. The student learns by mimicking the teacher's softened probability distribution (soft labels) over classes, which contains richer information than hard labels (one-hot vectors). A temperature-scaled softmax is used to smooth the outputs, and the student's loss combines both the teacher's guidance and ground-truth labels \n\nwhere   and   are the teacher's and student's softened probabilities for class ,   and   are logits from the teacher and student, and  is the temperature. \n\nThen, the loss function is computed as: \n\nwhere KL( \u2225 ) is the Kullback-Leibler divergence between teacher and student distributions; CE(,  \u2032 ) is cross-entropy loss between student predictions  \u2032 = softmax(  ) and true labels ;  is the weight balancing distillation and cross-entropy losses;  2 compensates for gradient scaling caused by temperature. Moreover,   ,   is logits from teacher and student;  is the emperature ( > 1 smooths probabilities,  = 1 recovers standard softmax);  is the distillation weight (typically 0.5); ,  is the teacher/student softened probabilities;  is the ground-truth labels. \n\nData Distillation. Data Distillation generates synthetic data that encapsulates the teacher's knowledge for student training. Synthetic data  syn ,  syn is generated such that training the student on it mimics training on the original dataset with the teacher. Methods include gradient matching or leveraging the teacher to label synthetic samples. \n\nMinimize the difference between student gradients on synthetic data and teacher gradients on real data: \n\nwhere L syn is the loss on synthetic data ( syn ,  syn ); L real is loss on real data (,  );  is the model parameters. Moreover,  syn ,  syn is the synthetic inputs and labels (learned via optimization);  is the parameters of the student model.",
            "score": 0.6898708244483016,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 16249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 318
                },
                {
                    "start": 321,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1505
                },
                {
                    "start": 1508,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1856
                },
                {
                    "start": 1859,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2209
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.298828125
        },
        {
            "corpus_id": "251460030",
            "title": "Improving Deep Mutual Learning via Knowledge Distillation",
            "text": "Knowledge distillation in [8] is one of the most popular knowledge transfer methods today, and it uses a teacher-student framework. The basic idea of this method is that a pretrained teacher network (i.e., a cumbersome network or the biggest network) using certain hyperparameters are then used to train an untrained student network (i.e., small network) for the purpose of transferring knowledge. This process uses a distillation knowledge equation where a temperature (T) is involved and can be varied to obtain a soft probability output from a class C image which can be calculated as: \n\nSuppose the teacher network is marked as G t and the student network is G s , then the distillation loss can be defined as: \n\nAs a result, the student loss function contained in Figure 2 is minimized during the training process based on ( 6) and (7) as: \n\nwhere \u03bb is a balancing value between the two losses. The main purpose of the teacherstudent framework is to force the student output probability to imitate or match the pre-trained teacher network's probability output.",
            "score": 0.6891439249060775,
            "section_title": "DML and KD",
            "char_start_offset": 10613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1065
                }
            ],
            "ref_mentions": [
                {
                    "start": 837,
                    "end": 840,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.546875
        },
        {
            "corpus_id": "198179767",
            "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching",
            "text": "Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process. \n\nLet us respectively denote x and y as the input of the DNNs and one-hot labels of our architecture. We let P T be the teacher network's softmax output as P T = sof tmax(a T ). Specifically, P T is obtained by applying softmax function on the un-normalized log probability values a T . Similarly, the same image fed to the student network to get the predictions P S = sof tmax(a S ). In the intermediate layers of the DNN, we denote the activation tensor A \u2208 R C\u00d7X\u00d7W with its corresponding layer. The pairs of teacher and student attention maps are denoted as F (A j T ) and F (A j S ) in vectorized form respectively [21]. And the standard cross entropy is denoted as H. Hinton et al. [19] extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 the student network will be trained to optimize the following loss function: \n\nMishra et al. [22] propose a new perspective view to jointly train a teacher network (full-precision) and a student network (low-precision) from scratch using knowledge distillation. The total loss function is as following: L(x; W t , W s ) = \u03b1H(y true , P T )+\u03b2H(y true , P S )+\u03b3H(a T , P S ) \n\n(2) In this case, the teacher and student network both train from scratch.",
            "score": 0.6887001122979333,
            "section_title": "B. Formulation",
            "char_start_offset": 17544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1783
                },
                {
                    "start": 1786,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2079
                },
                {
                    "start": 2082,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 1257,
                    "end": 1261,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1325,
                    "end": 1329,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1800,
                    "end": 1804,
                    "matchedPaperCorpusId": "3643430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70361328125
        },
        {
            "corpus_id": "270389751",
            "title": "Multistage feature fusion knowledge distillation",
            "text": "Knowledge distillation (KD), as initially proposed by Hinton et al. 10 , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge.This approach has given rise to variations, including intermediate feature layer distillation and multistage soft label distillation.\n\nIn the context of intermediate feature layer distillation, the challenge of inconsistent multistage feature knowledge distribution is a critical issue.FitNet 11 employs squared distance constraints to measure the similarity of intermediate layer features between teacher and student networks.AT 15 uses multi-layer attention maps to extract features between the teacher network and the student network, and builds a knowledge transfer mechanism between the two.CC 16 proposed a correlation congruence method to reduce the correlation consistency distribution between teachers and students across multiple sample instances, and improve the distribution consistency between student models and teacher models in the classification output of multiple instances.AB 17 proposed a method for knowledge transfer by extracting the activation boundaries formed by hidden neurons, which enables students to learn the separation boundaries between different activation regions formed by each neuron in the teacher, thereby reducing the differences between the student network and the teacher network.FT 18 proposed two convolutional modules, the reader and the translator, which are used to extract the feature information of teachers and the translator to extract the feature information of students.Through distillation training, the differences between the two modules are reduced, achieving the imitation and learning of the teacher network by www.nature.com/scientificreports/ the student network.NST 19 proposed a new KT loss function to minimize the maximum average difference in neuron feature distribution between the teacher model and the student model, significantly improving the performance of the student model.CRD 20 is a knowledge distillation method based on contrastive learning, which preserves mutual information between teachers and students by optimizing the distillation loss function.OFD 21 uses a novel distance function and edge residual function to distill essential information between teacher and student networks.ReviewKD 22 utilizes a multilevel composite knowledge approach to transfer dark knowledge at the feature level, achieving state-of-the-art performance.",
            "score": 0.6878154477609052,
            "section_title": "Knowledge distillation",
            "char_start_offset": 2815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 194,
                    "end": 356
                },
                {
                    "start": 356,
                    "end": 489
                },
                {
                    "start": 491,
                    "end": 642
                },
                {
                    "start": 642,
                    "end": 783
                },
                {
                    "start": 783,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 1248
                },
                {
                    "start": 1248,
                    "end": 1579
                },
                {
                    "start": 1579,
                    "end": 1780
                },
                {
                    "start": 1780,
                    "end": 1981
                },
                {
                    "start": 1981,
                    "end": 2204
                },
                {
                    "start": 2204,
                    "end": 2387
                },
                {
                    "start": 2387,
                    "end": 2522
                },
                {
                    "start": 2522,
                    "end": 2673
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 70,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 955,
                    "end": 957,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1251,
                    "end": 1253,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1582,
                    "end": 1584,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 2208,
                    "end": 2210,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 2391,
                    "end": 2393,
                    "matchedPaperCorpusId": "102483181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.783203125
        },
        {
            "corpus_id": "264516404",
            "title": "Attention and feature transfer based knowledge distillation",
            "text": "The concept of knowledge distillation (KD) was proposed by Hinton et al. 21 , which forced the student network to extract knowledge from the soft labels and ground truth labels provided by teachers. In order to make full use of the \"dark knowledge\" contained in soft labels, the concept of temperature was introduced. The existing KD methods can be mainly divided into three types: logic-based 20,21,[31][32][33][34] , feature-based 18,[22][23][24][25][26][27][28][29]35 , and attention maps-based 19,30 . \n\nLogic distillation transfers the knowledge implicit in the output logic of the teacher model to the student network. BAN 32 obtained superior performance to the teacher model by directing the same parameterized network as the teacher. DKD 20 reformulates KD loss into target-class knowledge distillation (TCKD) and non-target-class knowledge distillation (NCKD), revealing that KD's coupling formula limits the effectiveness and flexibility of knowledge transfer. CrossKD 34 passes intermediate features of the student network to the teacher's detection head, resulting in cross predictions, which are then forced to mimic the teacher's predictions. In addition, there are several articles on logical distillation methods 21,33,34 . \n\nFeature-based KD methods tend to have better performance, forcing students to extract valid content from intermediate features of the teacher network at the cost of requiring more computation than logical distillation. RKD 25 can transform the relationship of data examples to punish differences in teacher and student relevance, similar to the transfer of sample relevance studies from teacher and student networks 26,27 . PKT 35 models the teacher's knowledge as a probability distribution and uses KL divergence to measure distance. RKD 25 uses multicase relationships to guide students' learning. CRD 22 combines comparative learning with knowledge distillation, and uses comparative objectives to carry out knowledge transfer. ReviewKD 18 uses cross-layer connection paths to integrate the knowledge implied by features at different levels. \n\nKD method based on attention diagram instructs students what information the network should pay attention to in reasoning.",
            "score": 0.6844057453339191,
            "section_title": "Related work",
            "char_start_offset": 2593,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1240
                },
                {
                    "start": 1243,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2088
                },
                {
                    "start": 2091,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 397,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 400,
                    "end": 404,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 404,
                    "end": 408,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 433,
                    "end": 436,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 440,
                    "end": 444,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "232232777"
                },
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "229220499"
                },
                {
                    "start": 498,
                    "end": 501,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 629,
                    "end": 631,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 747,
                    "end": 749,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1233,
                    "end": 1236,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1466,
                    "end": 1468,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1659,
                    "end": 1662,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1662,
                    "end": 1664,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1783,
                    "end": 1785,
                    "matchedPaperCorpusId": "131765296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6806640625
        },
        {
            "corpus_id": "159041346",
            "title": "Zero-Shot Knowledge Distillation in Deep Networks",
            "text": "Knowledge Distillation (Hinton et al., 2015) enables to transfer the complex mapping functions learned by cumbersome models to relatively simpler models. The cumbersome model can be an ensemble of multiple large models or a single model with large capacity and strong regualrizers such as Dropout (Srivastava et al., 2014), BatchNorm (Ioffe & Szegedy, 2015), etc. Typically the complex and small models are referred to as Teacher (T) and Student (S) models respectively. Generally the Teacher models deliver excellent performance, but they can be huge and computationally expensive. Hence, these models can not be deployed in The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability. For performing the knowledge distillation, one can use the training data from the target distribution or an arbitrary data. Typically, the data used to perform the distillation is called 'Transfer set'. In order to maximize the information provided per sample, we can make the soft targets to have a high entropy (non-peaky). This is generally achieved by using a high temperature at the softmax layer (Hinton et al., 2015). Also, because of non-peaky soft-labels, the training gradients computed on the loss will have less variance and enable to use higher learning rates leading to quick convergence. \n\nThe existing approaches use natural data either from the target data distribution or a different transfer set to perform the distillation. It is found by (Hinton et al., 2015) that using original training data performs relatively better. They also suggest to have an additional term in the objective for the Student to predict correct labels on the training data along with matching the soft-labels from the Teacher (as shown in eq. ( 1)). However, accessing the samples over which the Teacher had been trained may not always be feasible.",
            "score": 0.6831973876440309,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1731
                },
                {
                    "start": 1734,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2272
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 322,
                    "matchedPaperCorpusId": "6844431"
                },
                {
                    "start": 334,
                    "end": 357,
                    "matchedPaperCorpusId": "5808102"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82470703125
        },
        {
            "corpus_id": "257504799",
            "title": "MetaMixer: A Regularization Strategy for Online Knowledge Distillation",
            "text": "The idea of transferring dark knowledge from the highcapacity teacher model to the compact student model was first proposed in [1]. However, it did not gain significant attention from researchers until the work by Hinton et al. [13], where the Kullback-Leibler (KL) divergence loss is used to minimize the difference between the probability distribution generated by a student network and the soft targets generated by a pre-trained teacher network. Besides the vanilla KD, researchers have explored more methods to extract more salient dark knowledge from the pre-trained teacher network to the student [28,15,26,27,31,30]. For example, Fitnets [28] proposed to use intermediate feature representation as a hint to guide the training of the student network. Tian et al. [30] first used the contrastive learning mechanism to extract the structural knowledge of the teacher network to supervise the training of the student.",
            "score": 0.6830659596590322,
            "section_title": "Conventional Knowledge Distillation",
            "char_start_offset": 3200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 922
                }
            ],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 130,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 611,
                    "end": 614,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 614,
                    "end": 617,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 617,
                    "end": 620,
                    "matchedPaperCorpusId": "198179476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67822265625
        },
        {
            "corpus_id": "255546282",
            "title": "Designing an improved deep learning-based model for COVID-19 recognition in chest X-ray images: a knowledge distillation approach",
            "text": "To distill knowledge from the teacher to the student, a weighted average (mean) is necessary. The initial objective function is Cross-Entropy with soft targets, which is calculated through the softmax function in the smaller network using a higher temperature. Soft targets are generated by a larger architecture or network. The second objective function is Cross-Entropy with valid labels, which utilizes the softmax output from the student model with a temperature of zero. \n\nThe teacher network and the student network start receiving training data in parallel. The teacher model incorporates a softmax function with temperature in its output. In contrast, the student model generates two distinct outputs. The first output is softmax with temperature, while the second output consists of the standard softmax. The purpose of the student model is to produce softened probabilities, which correspond to the output of the teacher model. The loss of knowledge distillation is calculated using the following formula: 2 ( . ) \n\n(1 ) ( . ) \n\nHere , p and q denote probabilities generated by the student and teacher networks in a specific temperature (T), respectively, and KL denotes the Kullback-Leibler divergence, that measures the level of distinction between two probabilistic distributions. The Cross-Entropy of the student model with T=1 is (LWs.x). \n\nAccording to [38], \uf061 and T are hyperparameters where the greater the value of \uf061 , the better the learning experience for the student model. \n\nDuring the distillation phase, back-propagation should only be performed in the student network, as the teacher network has already fine-tuned its parameters. The transfer of knowledge from the teacher to the student model takes place throughout the distillation procedure. It is worth noting that the student model can be trained at a faster rate compared to the teacher model. For further details on the distillation procedure, please refer to [39]. The knowledge distillation procedure is illustrated in Figure 2.",
            "score": 0.6830484202262305,
            "section_title": "Hinton et al. pioneered Knowledge Distillation in",
            "char_start_offset": 18494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 1944,
                    "end": 1948,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41455078125
        },
        {
            "corpus_id": "233407431",
            "title": "Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification",
            "text": "Knowledge distillation [21] can be considered as regularizing the training of the student network using soft targets that carry the \"dark knowledge\" of the teacher network. \n\nThe student network generally consists of a backbone encoder F and a classifier C to perform classification. For each training sample x, its logit vector is encoded as z = C(F (x)). The predictive probability vector p \u03c4 can be obtained via a softmax function on the logits, i.e., the probability of class k can be formulated as \n\nwhere \u03c4 is a temperature hyper-parameter, and K is the number of total classes. Let y \u2208 {1, . . . , K} denotes the ground truth label and q \u03c4 is the soft target produced by the teacher network. The cross-entropy loss and the KL divergence between the predictions and soft targets are minimized jointly to train the student via \n\nwhere p(y) denotes the probability normalized without a temperature, and \u03bb weights the two terms. \n\nRecent works [14,28,40,47] found that ensembling diverse \"dark knowledge\" from multiple teachers or students can form better soft targets, leading to better final performance (see Figure 2 (a)&(b) for details). However, this strategy would increase much more computational and memory overhead to enable multiple networks or branches training. To tackle the challenge, we introduce batch knowledge ensembling in a single network via self-distillation, as illustrated in Figure 2 (c).",
            "score": 0.6820748678385891,
            "section_title": "Revisit of Knowledge Distillation",
            "char_start_offset": 8697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 175,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1416
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 951,
                    "end": 954,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 954,
                    "end": 957,
                    "matchedPaperCorpusId": "54447578"
                },
                {
                    "start": 957,
                    "end": 960,
                    "matchedPaperCorpusId": "204838340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.794921875
        },
        {
            "corpus_id": "257308816",
            "title": "A Survey on Deep-Learning-Based Real-Time SAR Ship Detection",
            "text": "Hinton put forward the concept of knowledge distillation for the first time in distilling the knowledge in a neural network. And he introduced the soft targets of teacher to induce the training of students' network. The knowledge distillation is classified into three categories, they are logits transfer, teacher assistant, and domain adaptation [109]. \n\nGenerally speaking, the teacher model has strong ability and performance, while the student model is compact. The knowledge distillation methods transfer the generalization ability of the teacher model to the compact student model to improve its performance with less complexity. The basic idea of knowledge distillation is to transfer the dark knowledge in the complex teacher model to the simple student model. These methods match or outperform the teacher's performance, while requiring notably fewer parameters and multiplications [110], [111], [112], as shown in Fig. 7. \n\nThe parameter T represents temperature. Generally, T is 1. When T is larger, a softer probability distribution will be obtained. There are two loss functions. The first loss function requires that the student model and the teacher model use the same T when calculating the softmax layer. The second loss function requires the student model T to be taken as 1, and the loss function is the weighted average of the two objective functions. Soft prediction carries more and more useful information than hard prediction. The knowledge distillation can get a lightweight CNN model with high accuracy [113]. \n\nThe softmax function is formulated as follows:",
            "score": 0.6801878708864355,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 14030,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 353
                },
                {
                    "start": 356,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1584
                }
            ],
            "ref_mentions": [
                {
                    "start": 891,
                    "end": 896,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 898,
                    "end": 903,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 905,
                    "end": 910,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1529,
                    "end": 1534,
                    "matchedPaperCorpusId": "102483463"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5244140625
        },
        {
            "corpus_id": "240544883",
            "title": "Deep Neural Network Compression for Plant Disease Recognition",
            "text": "In the traditional three-stage iterative pruning process, after each pruning step, the model is retrained to restore the accuracy loss incurred by pruning. In this paper, knowledge distillation [37] is used instead of retraining to obtain a better precision recovery effect, as shown in Figure 5. In knowledge distillation, the teacher model uses output soft labels to enable the student model to learn dark knowledge; this process can be defined as: \n\nwhere the temperature parameter T is used to control the smoothness of the output to preferably transfer the knowledge of the teacher network. L KL is the Kullback-Liebler (KL) divergence between the soft labels of the two network models. The soft label output S stu of the student network can be given by: \n\nwhere z S is the output of the student network without a softmax layer. Equation ( 5) controls the degree of smoothness of the output, and it is the softmax function when T = 1. S Tea can also be calculated in this way. Under the effect of knowledge distillation, the total loss function of the student network model becomes: \n\nwhere y True denotes the ground-truth label of the output, and the latter term is the classic cross-entropy loss. \u03b1 and \u03b2 are weight hyperparameters.",
            "score": 0.6797861386070734,
            "section_title": "Retraining",
            "char_start_offset": 18151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1239
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61962890625
        },
        {
            "corpus_id": "238223320",
            "title": "Determining Top Fully Connected Layer\u2019s Hidden Neuron Count for Transfer Learning, Using Knowledge Distillation: a Case Study on Chest X-Ray Classification of Pneumonia and COVID-19",
            "text": "Neural networks use distributed representation of feature map of input to interpret and processing in hidden layers [26]. These representations are hard to understand and represent in general form. Knowledge distillation (KD) was first applied on deep neural networks by Hinton et al. [27]. KD was first proposed by Bucila et al. [28]. \n\nKnowledge distillation is the process to transfer the representations and abilities learned by a large network (teacher network) to any smaller network (student network). Main drawback of knowledge distillation is that it can only be applied for classification tasks, not for regression tasks [29]. KD uses the \"dark knowledge\" (softened logit output of the bottom output layer of teacher network) that is transferred to student network. This dark knowledge is more than interlabel correlations and one-hot encoding of labels. In case of regression, the deep network interprets the continuous values, which has a tendency of unknown error distribution; as a result, there is no dark knowledge for deep networks trained for regression. But later [30] present their knowledge distillation process for pose regression. Multiple papers try to propose a framework to train the teacher and student network in parallel. Yim et al. [31] and You et al. [32] proposed a shared layer-representation for it. Czarnecki et al. [33] narrowed down the variance between teacher and student derivatives of the loss shared with the discrepancy from teacher predictions. Tarvainen and Valpola [34] proposed an averaging procedure for model weights for the same purpose. Urban et al. [35] trained a network of convolutional neural networks and teach shallow multilayer perceptron as student networks. Sau and Balasubramanian [36] injected noise into teacher logits for making the student network more robust. Employing several teacher networks is always a way to decrease the accuracy difference between teacher and student network. Zhang et al. [37] proposed deep mutual learning so that teacher and student networks can learn side by side during training. \n\nHinton et al. [27] claim that the success of knowledge distillation is credited to the logit distribution of the incorrect outputs.",
            "score": 0.6794195560652748,
            "section_title": "Distilling Neural Networks",
            "char_start_offset": 5608,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 335
                },
                {
                    "start": 338,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2074
                },
                {
                    "start": 2077,
                    "end": 2208
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 120,
                    "matchedPaperCorpusId": "1779661"
                },
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "32588614"
                },
                {
                    "start": 1262,
                    "end": 1266,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1282,
                    "end": 1286,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1351,
                    "end": 1355,
                    "matchedPaperCorpusId": "21596346"
                },
                {
                    "start": 1511,
                    "end": 1515,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 1601,
                    "end": 1605,
                    "matchedPaperCorpusId": "16550689"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6611328125
        },
        {
            "corpus_id": "274269913",
            "title": "A Precise and Scalable Indoor Positioning System Using Cross-Modal Knowledge Distillation",
            "text": "Knowledge distillation (KD) is a powerful technique for transferring knowledge from a larger, more complex model (the teacher) to a smaller, more efficient model (the student). The goal is to retain the performance benefits of the larger model while reducing the computational complexity, enabling deployment in resource-constrained environments. \n\nIn the knowledge distillation framework, let x \u2208 R d denote the input data, and let y \u2208 {1, 2, . . . , C} represent the corresponding true labels, where C is the number of classes. The teacher model is a pre-trained model with parameters \u03b8 T , and it outputs a set of logits z T = f T (x; \u03b8 T ). These logits are then passed through a softmax function to obtain the predicted probability distribution: \n\nSimilarly, the student model, with parameters \u03b8 S , produces its own set of logits z S = f S (x; \u03b8 S ) and corresponding probability distribution: \n\nThe central idea in knowledge distillation is to train the student model such that its output distribution p S closely matches the teacher's output distribution p T , while also ensuring that the student performs well on the actual classification task. \n\nA key component of knowledge distillation is the use of softened probability distributions, achieved by introducing a temperature parameter T > 1 in the softmax function. The logits from both the teacher and the student are softened as follows: \n\nThe temperature T controls the softness of the output distribution. When T = 1, the distribution is the same as the original softmax output. When T > 1, the distribution becomes softer, spreading the probabilities more evenly across the classes. This softened distribution contains richer information about the inter-class relationships, which the student model can learn from. \n\nThe loss function used to train the student model is a weighted combination of two components: the supervised loss and the distillation loss. The supervised cross-entropy loss ensures that the student model performs well on the actual classification task by minimizing the cross-entropy between the true labels, denoted as y, and the student's predictions, p S . This loss is calculated as follows: \n\nwhere y i represents the one-hot encoded true label. In addition to this, the distillation loss encourages the student model to mimic the softened output distribution of the teacher model.",
            "score": 0.6789877460197542,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 14052,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 750
                },
                {
                    "start": 753,
                    "end": 899
                },
                {
                    "start": 902,
                    "end": 1154
                },
                {
                    "start": 1157,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1401
                },
                {
                    "start": 1404,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1781
                },
                {
                    "start": 1784,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2182
                },
                {
                    "start": 2185,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2373
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.477783203125
        },
        {
            "corpus_id": "209515527",
            "title": "Modeling Teacher-Student Techniques in Deep Neural Networks for Knowledge Distillation",
            "text": "Assume training data tuples of inputs and labels (. ) \u2208  which D is a set of training data. Let T be Teacher network with parameters   and S be Student network with parameters   . \n\nThe distillation equation can be written as Eq. ( 1) to minimize . \n\nIn Eq. ( 1),   is cross-entropy which is computed on the labels  \ufffd  that are predicted by the student and ground truth labels y with temperature =1.   is distillation loss, which is crossentropy computed on softmax output of teacher and student with temperature  . Hence, (.   . )  (.   . ) represent of softmax output of the student and the teacher, respectively.  is hyper-parameters to balance the influence of each loss [7][1]. \n\nHow the knowledge is transferred between the teacher(s) and student(s) is specified in this stage. As illustrated in Fig. 5, there are three steps in knowledge distillation playing an essential role in having an efficient TS model. These steps are including the determination of knowledge types, location of distillation, and methods of knowledge transfer. \n\nIn the base TS models, soft-labels (also known as logits) are considered as distilled knowledge. However, the knowledge can be distilled from each location of the teacher model, including the end of the model and between layers. In [14], knowledge is transferred between blocks of the teacher to student. Distillation-loss is realized through a cross-entropy function that is applied to the output of the student and softlabels of the teacher. Different knowledge types are considered in previous studies, such as soft labels, hard labels, etc. In [9], knowledge type in the form of the mutual information between intermediate layers is maximized and several functions are used to minimize the loss of intermediate layers. In [7], knowledge  types are in the form of a similarity matrix between teacher and student models. Transfer Methods is another step in the knowledge distillation stage. In [15] a multilevel approach for knowledge transfer is presented in which an information mask is provided by the teacher, and the student is trained with both of the information masks and teacher's ground truth. Also, in [16] and [17], there is more than one level of distillation.",
            "score": 0.677795055810382,
            "section_title": "V. KNOWLEDGE DISTILLATION",
            "char_start_offset": 10817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 1592,
                    "end": 1595,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1940,
                    "end": 1944,
                    "matchedPaperCorpusId": "182952755"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.338134765625
        },
        {
            "corpus_id": "231986112",
            "title": "Exploring Knowledge Distillation of a Deep Neural Network for Multi-Script identification",
            "text": "Earlier papers used ensemble methods for model compression [8,9]. Distillation of knowledge from a teacher network and transferring it to a student network to mimic the teacher network is a basic fundamental concept of knowledge distillation. The first proposed concept of knowledge distillation [7] introduces the concept of compressing the knowledge of a more in-depth or larger model to a single computational efficient neural network. It has introduced the concept of dark knowledge transfer from a deep teacher network to a smaller student network by taking the softmax of the results of the teacher network with a specific temperature value and calculating loss between it and the predicted outputs of the student network. They validated their findings by running on MNIST dataset and, JFT dataset by google and other speech recognition tasks. Since then, knowledge distillation has progressed a lot, and adversarial methods [17,18] also have utilized for modelling knowledge transfer between teacher and student. After this study, extensive research has conducted on knowledge distillation. In the paper [11] has introduced the transfer of a hidden activation output and other has proposed transferring attention information as knowledge [20]. \n\nArticle [19] has briefly described the advantages and efficiency of the knowledge distillation. It has described importance of knowledge transfer from teacher to student model using distilled knowledge. They have compared two student deep neural networks trained with teacher network and without teacher model with same size.They have proposed a method of transferring the distilled knowledge between two layers to shows three important points. The student model is more efficient than the original model and it also outperform the original model which is trained from scratch. The student network understand the flow of solving the problem and it start learning with good initial weights. It can learnt and optimized faster than original or normal deep neural network.This paper proves that, the student model reports better efficiency than a normal network without a teacher model.They have compared various knowledge transfer techniques with a normal network without any teacher model for knowledge transfer.They have learned their model with two main condition. First, the teacher model must pretrained with some different dataset and second condition is the teacher model is shallower or deeper than the student model. Their approach contains two step training.",
            "score": 0.67747658140902,
            "section_title": "Related Study",
            "char_start_offset": 4015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2318
                },
                {
                    "start": 2319,
                    "end": 2476
                },
                {
                    "start": 2477,
                    "end": 2519
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 62,
                    "matchedPaperCorpusId": "9433631"
                },
                {
                    "start": 62,
                    "end": 64,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 931,
                    "end": 935,
                    "matchedPaperCorpusId": "53976534"
                },
                {
                    "start": 1261,
                    "end": 1265,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4765625
        },
        {
            "corpus_id": "268297332",
            "title": "Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples",
            "text": "In the context of knowledge distillation, the objective is to train a student neural network, which is denoted as f \u03b8 S (\u2022), to mimic the behavior of a pre-trained teacher network, denoted as f \u03b8 T (\u2022). Both networks have their respective parameter sets: \u03b8 T for the teacher network and \u03b8 S for the student network. The training dataset contains pairs of samples and their corresponding labels, denoted as (x (i) , y (i) ). The logit response of a sample x (i) from the network f \u03b8 (\u2022) is indicated by p f \u03b8 (x (i) ). The softmax temperature function, \u03c3 \u03c4s (\u2022), which is proposed by Hinton et al. in 2015, transforms logits into soft probabilities when a large temperature \u03c4 s , which is usually greater than 1, is applied; and it behaves like the standard softmax function \u03c3(\u2022), when \u03c4 S is set to 1 [1]. To achieve knowledge distillation, a combined loss function is utilized for training the student network, as shown in Eqn. (1). \n\nThis combined loss function consists of two terms. The first term measures the KL divergence between the softened logits of the teacher network, \u03c3 \u03c4 S (p f \u03b8 T (x (i) )), and the student network, \u03c3 \u03c4 S (p f \u03b8 S (x (i) )). This divergence quantifies the difference in the distributions of soft probabilities generated by the two networks. The second term is the cross-entropy loss between the softened probabilities of the student network, \u03c3(p f \u03b8 S (x (i) )), and the ground truth label, y (i) . This term ensures that the student network is learning to predict the correct labels directly. \n\nTo control the balance between knowledge distillation and conventional cost minimization, a hyperparameter \u03b1 is usually used. By adjusting the value of \u03b1, the relative importance of knowledge transfer from the teacher network and the direct supervision of the student network can be controlled during the training process [1].",
            "score": 0.676694855599945,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1526
                },
                {
                    "start": 1529,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 801,
                    "end": 804,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 929,
                    "end": 932,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1851,
                    "end": 1854,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58349609375
        },
        {
            "corpus_id": "268681594",
            "title": "Attention is all you need for boosting graph convolutional neural network",
            "text": "Knowledge distillation is a technique for enhancing the performance of student models through the transfer of advanced and complex knowledge from large and high-performance teacher models.By feeding the output of the teacher model as a supervised signal to the student model, the student model can leverage the experience and knowledge of the teacher model to enhance its ability to learn the distinguishing features and patterns of the task, ultimately improving overall performance.The knowledge distillation first proposed in [3] by having the student output label probabilities to match the teacher output soft label probabilities, with a specific loss function formula of Eq. ( 5):\n\nwhere H denotes the cross-entropy loss or KL divergence, and T 1 denotes the distillation temperature, which allows for smoother labels for distillation.z T and z S are the labeling probabilities of teachers and students, respectively.",
            "score": 0.6765167551591786,
            "section_title": "Knowledge distillation",
            "char_start_offset": 19270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 188,
                    "end": 484
                },
                {
                    "start": 484,
                    "end": 686
                },
                {
                    "start": 688,
                    "end": 841
                },
                {
                    "start": 841,
                    "end": 923
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.419677734375
        },
        {
            "corpus_id": "272969060",
            "title": "Harmonizing knowledge Transfer in Neural Network with Unified Distillation",
            "text": "Logits-based KD was first proposed in [9], where the student learns the logits knowledge from the teacher through KL divergence. In contrast to direct supervised learning through ground truth, logits-based KD using soft labels can elicit more 'dark knowledge' [9], thereby enhancing the performance of the student network without altering its architecture. The logits distillation losses are represented as follows: \n\nP j denotes the class probability derived from the logits z \u2208 R C after undergoing Softmax. \u03c4 is the temperature scaling hyper-parameter which enables the production of different probability distributions: \n\nFeature-based KD transfers the knowledge of intermediate layers features from the teacher to the student. Generally, we can formulate such distillation methods as: \n\nwhere F t i and F s i are the corresponding features of the teacher and student at i-th layer. w(\u2022) is a mapping function that aligns the dimensions of features from the student to the teacher. \n\nFrom the above distillation process, it can be seen that logits-based knowledge distillation focuses on constraining the overall knowledge distribution. In contrast, feature-based knowledge distillation imposes pixel-level constraints on the student network. These two methods have distinct optimization objectives during the distillation process, and directly integrating them can lead to suboptimal solutions. In this study, we aim to achieve comprehensive knowledge transfer by conducting knowledge distillation on the intermediate layer features and the final layer logits. To address this issue, we introduce UniKD, a meticulously designed framework that adjusts the intermediate layer features for a thorough and consistent knowledge transfer, using the same constraints employed for the final layer's logits.",
            "score": 0.6727024900126358,
            "section_title": "Preliminaries",
            "char_start_offset": 8573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 789
                },
                {
                    "start": 792,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1803
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8115234375
        },
        {
            "corpus_id": "235489777",
            "title": "Knowledge Distillation via Instance-level Sequence Learning",
            "text": "The idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels. \n\nLet us denote x, y as the input of the DNN and one-hot labels of our architecture. \u03c6 s and \u03c6 t represent the student network and teacher network with parameters W s and W t respectively. Given an input image x, the teacher network outputs the final predictions as P T which are obtained by applying the softmax function on the un-normalized log proba-bility values a T , i.e. P T = sof tmax(a T ). Similarly, the same image is fed into the student network to obtain the predictions P S = sof tmax(a S ). The standard cross-entropy is denoted as H. In classical supervised learning, the mismatch between the output of the student network softmax and the ground-truth label y is usually penalized using cross-entropy loss: \n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss: \n\nIt contains the relative probabilities of incorrect classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 , the student network will be trained to optimize the following loss function: \n\nwhere \u03bb is a second hyper parameter controlling the trade-off between the two losses. The teacher network is sometimes deeper and wider than the above approaches, but sometimes has the similar size as the student network [26][27] [28]. Snapshot Distillation [28] proposes to finish teacher-student optimization within one generation which acquires teacher information from the previous iterations of the same training process. Inspired by this, we propose to employ the snapshot of student from the previous epochs to design curriculum for efficient knowledge distillation. \n\nInstance-level sequence learning for knowledge distillation.",
            "score": 0.6725699338374113,
            "section_title": "B. Formulation",
            "char_start_offset": 12183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 361
                },
                {
                    "start": 364,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1655
                },
                {
                    "start": 1658,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2231
                },
                {
                    "start": 2234,
                    "end": 2294
                }
            ],
            "ref_mentions": [
                {
                    "start": 1101,
                    "end": 1105,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1128,
                    "end": 1132,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1879,
                    "end": 1883,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77490234375
        },
        {
            "corpus_id": "265351966",
            "title": "Leveraging different learning styles for improved knowledge distillation in biomedical imaging",
            "text": "Knowledge Distillation [17] is an approach introduced to transfer the knowledge in terms of probability outputs, p i , from a complex, highly parameterized pre-trained teacher network f (X, \u03d5) to a simple and compact student network g(X, \u03b8) to achieve model compression while retaining the high performance of the teacher.Given a training set with N samples X = {x i } N i=1 with corresponding labels Y = {y i } N i=1 , the teacher network f (X, \u03d5), is trained on the ground truth labels.The probabilistic output of a teacher network for a sample x i is defined as p i given by the extended softmax as:\n\nwhere z c corresponds to the logits, C is the number of classes, and T is the temperature parameter to get a smoother output\n\nPredictions, Predictions Features, Features Features, Predictions T \u2192 S 2 , S 1 \u2192 S 2 Predictions, Predictions Features, Features Predictions, Features probability distribution of the classes.Generally, the objective function for the teacher network is the standard Cross-Entropy (CE) error defined as:\n\nNow, the student networks are trained on the combined loss of Cross-Entropy (CE), and Knowledge Distillation (KD), where the CE helps the student networks to adhere to the ground truth labels and KD assists them to align their learning with that of the teacher.Here, Kullback Leibler (KL) divergence [28] is used for L KD p to measure the correspondence between the teacher and student predictions p i and s i respectively as:\n\nFinally, the loss function for the student network is the weighted (\u03b1) summation of the cross entropy (L CE ) and knowledge distillation (L KD p ) terms:\n\nwhere hyperparameter \u03b1 is used to balance the contributions of the hard target loss (L CE ) and soft target loss (L KD p ) during the distillation process for each student.The knowledge can be transferred in an online or offline manner from the teacher to the student networks.",
            "score": 0.6707797020481139,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 322
                },
                {
                    "start": 322,
                    "end": 488
                },
                {
                    "start": 488,
                    "end": 602
                },
                {
                    "start": 604,
                    "end": 728
                },
                {
                    "start": 730,
                    "end": 922
                },
                {
                    "start": 922,
                    "end": 1032
                },
                {
                    "start": 1034,
                    "end": 1295
                },
                {
                    "start": 1295,
                    "end": 1460
                },
                {
                    "start": 1462,
                    "end": 1615
                },
                {
                    "start": 1617,
                    "end": 1789
                },
                {
                    "start": 1789,
                    "end": 1894
                }
            ],
            "ref_mentions": [
                {
                    "start": 1334,
                    "end": 1338,
                    "matchedPaperCorpusId": "116908168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58154296875
        },
        {
            "corpus_id": "267657497",
            "title": "Maximizing discrimination capability of knowledge distillation with energy function",
            "text": "Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks. Over the years, KD methods have evolved to narrow the performance gap between student and teacher models by utilizing both final predictions, known as logits-based distillation [10,15,16,11,17], and intermediate features, known as features-based distillation [18,19,20,21,22,23,24,12,25,26,27,28]. \n\nPrevious works on logits-based distillations include the following: DML [15] proposed a mutual learning strategy for collaboratively teaching and learning between student and teacher models; TAKD [16] introduced a multi-step method with an intermediate-size network (i.e., assistant network) to bridge the gap between teachers and students; DKD [11] decomposed the soft-label distillation loss into two components: target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD), enabling each part to independently harness its effectiveness; Multi KD [17] proposed multi-level prediction alignment, containing instance, batch, and class levels, and prediction augmentation. While these approaches emphasize effective knowledge transfer, they do not consider dividing entire datasets or provide mechanisms to distinguish and transfer knowledge from specific samples. \n\nFitNet [18] was groundbreaking as it leveraged not only the final outputs but also intermediate representations. Since the introduction of FitNet, various feature-based KD methods have emerged as follows: AT [22] prompted the student to mimic the attention map of the teacher network; PKT [19] employed various kernels to estimate the probability distributions, employing different divergence metrics for distillation; RKD [20] focused on transferring the mutual relations of data samples; CRD [21] framed the objective as contrastive learning for distillation; VID [23] took a different approach by maximizing mutual information; OFD [24] introduced a novel loss function incorporating teacher transform and a new distance function; Review KD [12] introduced a review mechanism that leverages past features for guiding current ones through residual learning.",
            "score": 0.6679538088471308,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 3764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 366,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 366,
                    "end": 369,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 439,
                    "end": 442,
                    "matchedPaperCorpusId": "219169868"
                },
                {
                    "start": 442,
                    "end": 445,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 451,
                    "end": 454,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 454,
                    "end": 457,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 457,
                    "end": 460,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 460,
                    "end": 463,
                    "matchedPaperCorpusId": "258298441"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 466,
                    "end": 469,
                    "matchedPaperCorpusId": "269167845"
                },
                {
                    "start": 469,
                    "end": 472,
                    "matchedPaperCorpusId": "269206209"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1058,
                    "end": 1062,
                    "matchedPaperCorpusId": "260933721"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77783203125
        },
        {
            "corpus_id": "256900863",
            "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK",
            "text": "Distillation (LLM-DKD) 1) Teacher Logits: LLM-DKD takes the Negative Euclidean distance [39] between teacher model output and each class as teacher logits: \n\nwhere \u0177t is the label of the t-th class, z M is teacher logits as shown in Fig. 2, \n\n2) Target Class Knowledge and Non-target Class Knowledge: For a given data from the t-th class, the soft labels can be denoted as u = [u 1 , u 2 , . . . , u t , . . . , u C ] \u2208 R 1\u00d7C , where u i is the soft label of the i-th class. Each element in u can be obtained by the softmax function with temperature \u03c4 : \n\nwhere z i represents the logit of the i-th class. LLM-DKD decouples fuzzy dark knowledge into target class knowledge and non-target class knowledge by: \n\nwhere u t represents soft labels of the target class, containing knowledge about the \"difficulty\" of data. u \\t represents soft labels of non-target class, containing knowledge making knowledge distillation work [32]. \n\nto independently model probabilities among non-target classes: \n\n3) Fuzzy Dark Knowledge Decoupling Process of LLM-DKD: Widely used KL divergence [14] is employed for decoupling fuzzy dark knowledge, which can be expressed as: \n\nAlgorithm 1 Teacher model and Student model. \n\nthe number of fuzzy rules K, the regularization parameter L, the maximum iteration epoch \u03b8, the threshold parameter \u03be, the learning rate \u03b7. \n\nOutput: the outputs of teacher model and student model. Procedure: \n\nStep 1: Randomly select the center v k i of Gaussian membership function in (2b) from five fixed fuzzy partition {0, 0.25, 0.5, 0.75, 1}, set the width \u03b4 k i to be a positive value, and compute the normalized fuzzy membership degree by (2a)-(2b). \n\nStep 2: Calculate antecedent parameter matrixs of teacher model and student model by (3a)-(3c). \n\nStep 3: The consequent parameter of teacher model q g can be determined by using",
            "score": 0.6666609004790914,
            "section_title": "B. Least Learning Machine based Decoupling Knowledge",
            "char_start_offset": 13868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 553
                },
                {
                    "start": 556,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 927
                },
                {
                    "start": 930,
                    "end": 992
                },
                {
                    "start": 995,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1414
                },
                {
                    "start": 1417,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 922,
                    "end": 926,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1076,
                    "end": 1080,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.314697265625
        },
        {
            "corpus_id": "233777216",
            "title": "On-Device Learning Systems for Edge Intelligence: A Software and Hardware Synergy Perspective",
            "text": "However, the key difference is that knowledge distillation requires a pre-defined student model and needs to modify its loss function. \n\n1) Combination of Loss Functions: Although the basic idea is to extract knowledge from the teacher model, we cannot simply employ its vanilla loss function. As the teach model is well trained with high prediction accuracy, the output class corresponding to the ground-truth label dominates in the classification distribution, while other classes are approximately to zero after the softmax block. In this case, even employing the teacher's loss function cannot bring extra useful information to the student, over the vanilla loss function based on the groundtruth labels. An effective method to solve this problem is to employ the temperature-based softmax function [82] to adjust the original logits from FC layers and balance the unscaled log probabilities of shape. The temperature-based softmax can be described as: \n\nwhere p i is the classification probability of each class i and z is corresponding logits inside the softmax function. Besides, T is the hyper-parameter called temperature to control the probability distribution of the output. Note that we will get the standard softmax function when T is 1 and the probability distribution will become softer when setting a higher value of T, so as to make the teacher model provide more effective information for the student model's learning. The key that a teacher can successfully transfer the knowledge to the student is to get the prediction experience from the teacher's loss function, which is called the dark knowledge [83]. The entire distillation procedure is actually based on the transferring of this dark knowledge, from the teacher to the student. Therefore, the core modification of the student's loss function (i.e., overall loss) can be formulated as: \n\nwhere x, y, w and \u03c3 represent the input data, ground-truth label, model parameters and the softmax block, respectively. Besides, z s and z d are the logits of the student and teacher, respectively. Note that the overall loss function L contains two parts: the student loss L s and distillation loss the L d , under the control of the coefficient hyper-parameters \u03b1 and \u03b2. We use Fig. 4 to better illustrate the workflow of knowledge distillation with these two kinds of loss functions.",
            "score": 0.6658232738163063,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 29410,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 137,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 956
                },
                {
                    "start": 959,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2235
                },
                {
                    "start": 2236,
                    "end": 2349
                }
            ],
            "ref_mentions": [
                {
                    "start": 1620,
                    "end": 1624,
                    "matchedPaperCorpusId": "198179476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8310546875
        },
        {
            "corpus_id": "260447668",
            "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback",
            "text": "In this case, the form of knowledge is converted to a simpler one while the knowledge itself is kept the same. This approach can help avoid using cumbersome networks in actual deployment. \n\nCompared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part. \n\n3) Loss Function for KD. The proposed KD-based neural network lightweight method is depicted in Fig. 3. After the teacher network is trained, the student network is trained using a combination of distillation loss and ordinary MSE loss.",
            "score": 0.6649727084486374,
            "section_title": "B. Key Idea and Training Framework",
            "char_start_offset": 18765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 187
                },
                {
                    "start": 190,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1205
                },
                {
                    "start": 1208,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1543
                },
                {
                    "start": 1546,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2175
                },
                {
                    "start": 2178,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2281
                },
                {
                    "start": 2282,
                    "end": 2414
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83837890625
        },
        {
            "corpus_id": "251040920",
            "title": "HIRE: Distilling High-order Relational Knowledge From Heterogeneous Graph Neural Networks",
            "text": "Aggregation Function RGCN [13] h \n\nnodes are imprecise. The node-level knowledge distillation is applied to transfer soft labels (p \u03c4 t i ) predicted by the teacher network to the student network, allowing students to learn smoother knowledge [17]. The soft labels can be obtained using softmax with temperature scaling, providing more inter-class and intraclass information. The distillation loss for soft logits can be denoted as: \n\nwhere D (\u2022) is the distance function (e.g., Kullback-Leibler divergence) to minimize the difference between the logits produced by the teacher model and those produced by the student model. z i is the corresponding logit of the i-th class. \u03c4 is a temperature hyperparameter intended to smooth outputs from teacher models. As shown in Figure 2, the teacher and student models use the softmax function with \u03c4 after their respective MLP layer to obtain p \u03c4 t i and p \u03c4 s i . A larger \u03c4 leads to a smoother probability distribution over classes, promoting the student model to learn more smoothing information about which classes are more similar to the predicted class. . Besides the teacher's soft labels, it can further boost the training performance with existing ground truth labels, as stated by Hinton et al. [17], which can significantly improve the performance of the student model. Accordingly, the NKD loss can be written as a weighted average of the cross-entropy loss and the distillation loss: \n\nwhere L CE = \u2212 N i y i log p s i is the basic cross-entropy loss, i denotes the i-th node, \u03b1 is the hyperparamemter for balancing the cross-entropy loss and the distillation loss. \n\nNevertheless, NKD cannot address the semantic relation modeling issue because it ignores the crucial correlation between different types of nodes in heterogeneous graphs, which can be solved in the next section.",
            "score": 0.6648491334377048,
            "section_title": "Models",
            "char_start_offset": 13265,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 35,
                    "end": 55
                },
                {
                    "start": 56,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1438
                },
                {
                    "start": 1441,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1834
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 30,
                    "matchedPaperCorpusId": "5458500"
                },
                {
                    "start": 243,
                    "end": 247,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1247,
                    "end": 1251,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54296875
        },
        {
            "corpus_id": "273549238",
            "title": "AI Readiness in Healthcare through Storytelling XAI",
            "text": "Knowledge distillation refers to the process of transferring knowledge from a comparatively large model to a smaller model without compromising on performance. The complex large model is called teacher and the smaller model is called student. The rationale behind model distillation is to train complex large models also called teacher for a specific task and then transfer the knowledge to a smaller model also called student using distillation loss (Equation: 1). The loss function employed in knowledge distillation to instruct the student model to imitate the teacher model's behavior is called the distillation loss. By using a \"temperature\" scaling function in the softmax, the logits are softened, so smoothing down the probability distribution and exposing the teacher's taught inter-class correlations. The probability p i of class i from the logits z is calculated as: \n\nwhere  is the parameter of temperature and when  = 1 then we get the original softmax function. The probability distribution produced by the softmax function softens with increasing T, giving more insight into which classes the teacher thought were more like the ones that were expected. The discrepancy between the instructor model's soft targets and the student model's predictions is quantified by this loss function. The student model learns the internal representation from the teacher model [13]. The student model learns not only the target outputs but also the internal representations and similarity information from the teacher model. This allows the student model to capture the same high-level concepts and reasoning as the teacher but in a more transparent and interpretable form [15,16].",
            "score": 0.664655555711736,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1682
                }
            ],
            "ref_mentions": [
                {
                    "start": 1678,
                    "end": 1681,
                    "matchedPaperCorpusId": "57189239"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2237548828125
        },
        {
            "corpus_id": "248683566",
            "title": "Teacher-student collaborative knowledge distillation for image classification",
            "text": "Knowledge distillation (KD), an important method of model compression [3][4][5], is effective in transferring \"dark knowledge\" from a larger model to a smaller model, allowing the smaller model to approximate the performance level achieved by the larger model [6][7][8]. This concept was first proposed in [9], but then was not explicitly explained. In 2014, [10] proposed an approach that enables a student network to learn the soft targets output by a teacher network and defined the method as knowledge distillation. However, conventional knowledge distillation methods only learn the output of the teacher network, which leads to the loss of intermediate layer knowledge. Later approaches attempted to exploit the information contained in middle model layers by designing different knowledge representations rather than just using the output information [11][12][13][14][15][16][17]. For example, [11] proposed an approach in which the student network simulates not only the output of the teacher network but also the hidden layer characteristics of the teacher network. [12] used attention transfer mechanisms to significantly improve its performance by forcing the student network to mimic the attention map of the powerful teacher network. Although the above algorithms utilized knowledge from the teacher network, they only consider the output of a specific layer of the teacher network. The relational knowledge distillation (RKD) approach proposed by [15] can transfer the structured relationships associated with the output results obtained by the teacher network to the student network, which alleviates the above problem. The correlations among different categories of probabilities may contain useful information to regularize a learning problem, and [16] found that the generation gap between teacher and student representation of mutual information can be minimized through contrastive representation distillation. Based on an adversarial-based learning strategy as a supervisor to guide and optimize lightweight student networks and recover knowledge from teacher networks, [18] recently proposed a knowledge distillation method for one-stage object detection . [19] constructed a compressed model to learn low-dimensional spatial information from potential representations of teacher networks. Most studies have focused on the representation of feature knowledge or methods of maximizing the transfer of teacher network feature knowledge while ignoring the potential capabilities of student networks.",
            "score": 0.6640724503083473,
            "section_title": "Knowledge distillation",
            "char_start_offset": 4620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2311
                },
                {
                    "start": 2312,
                    "end": 2518
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 73,
                    "matchedPaperCorpusId": "167217261"
                },
                {
                    "start": 73,
                    "end": 76,
                    "matchedPaperCorpusId": "32588614"
                },
                {
                    "start": 76,
                    "end": 79,
                    "matchedPaperCorpusId": "222310537"
                },
                {
                    "start": 260,
                    "end": 263,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 263,
                    "end": 266,
                    "matchedPaperCorpusId": "8451212"
                },
                {
                    "start": 266,
                    "end": 269,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 306,
                    "end": 309,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 862,
                    "end": 866,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 866,
                    "end": 870,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 870,
                    "end": 874,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 874,
                    "end": 878,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 878,
                    "end": 882,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 882,
                    "end": 886,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1075,
                    "end": 1079,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1461,
                    "end": 1465,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1765,
                    "end": 1769,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 2091,
                    "end": 2095,
                    "matchedPaperCorpusId": "237734820"
                },
                {
                    "start": 2179,
                    "end": 2183,
                    "matchedPaperCorpusId": "225257208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.658203125
        },
        {
            "corpus_id": "252510341",
            "title": "Automatic Modulation Classification with Neural Networks via Knowledge Distillation",
            "text": "The main idea behind knowledge distillation is that to achieve superior performance, the student model must imitate the teacher model. Knowledge types, distillation techniques and teacher-student learning architecture all play critical roles in student learning. [19]. A general teacher-student framework for knowledge distillation is shown in Figure 4. The teacher-student framework is the most fundamental framework for knowledge distillation. The teacher network in this paper is the pre-trained Inception-ResNet, and the student networks are a CNN with three convolutional layers and a Mini-Inception-ResNet. The output of the teacher network is distilled to form a soft and hard target to calculate the student network's loss function.  Neural networks typically produce class probabilities by using a \"softmax\" output layer that converts the logit z i , computed for each class into a probability q i , by comparing z i with the other logits [18].\n\nwhere T is a temperature that is normally set to 1. Using a higher value for T produces a softer probability distribution over the classes. To understand the knowledge distillation, a benchmark model, which is the distillation combined with student losses, is given in Figure 5. The basic process of knowledge extraction is as follows: The same input enters the teacher and student networks; the temperature of the softmax layer of the teacher network is set to t; the output produced is soft targets; the temperature of the softmax layer of the student network is set to t; and the soft targets produced together with those produced by the teacher network produce a distillation loss; the temperature of the student network's softmax layer is set to 1; and the resulting soft targets and true labels of the data generate the both the student and distillation losses as well as a combined loss, which comprises the overall loss.  When calculating losses in KD, the weighted average of two different objective functions is used. The first is cross entropy with the soft targets, which is computed using the same high temperature in the softmax of the distilled model that was used to generate the soft targets from the cumbersome model. The second is cross entropy with the correct labels. Since the magnitudes of the gradients produced by the soft target scale as 1/T 2 , it is important",
            "score": 0.6629546164688002,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10977,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6064453125
        },
        {
            "corpus_id": "260234949",
            "title": "Lightweight Tunnel Defect Detection Algorithm Based on Knowledge Distillation",
            "text": "Figure 5. The multidimensional knowledge distillation process, where the leftmost component is the teacher model learning process, is connected to the middle student model through the distillation parameter \"T\" to form Soft loss, and the rightmost component is the real label with parameter \"T\" of 1, connected to The rightmost component is the real label with parameter \"T\" of 1, connected to the student model to form Hard loss, and the two losses are combined to form the full multidimensional knowledge distillation loss. \n\nTo help train the student network in depth, the outputs of the intermediate layers of the teacher network were introduced, and then each intermediate and output layer parameter was optimized by a loss function as in Equation ( 9): \n\nOf these, the f t (x) and f s (x) are the feature maps of the middle layer of the teacher and student models respectively. When the feature maps of the teacher and student models are not in the same shape, it is common to apply the transformation functions \u03c6 t ( f t (x)) and \u03c6 s ( f s (x)). L F ( ) denotes the similarity function used to match the feature maps of the teacher and student models, and in this study L F ( ) denotes the cross-entropy loss function. \n\nThe training process is distilled by distilling the output with the high-temperature match to obtain a hard loss L hard defined as the cross-entropy loss between the ground truth label and the student model as in Equation ( 10): \n\nSoft loss L soft is defined as the cross-entropy between the teacher model and the student model as in Equation ( 11): \n\nwhere v i denotes the output of the teacher network, z i denotes the output of the student network and p T i , and q T i denotes the value of the softmax output of the teacher network and the student network at temperature T for class i, respectively. c i denotes the value of ground truth at class i. c j \u2208 {0, 1} N denotes the total number of samples. The total loss in the output layer is obtained through Equation ( 8) as Equation ( 12):",
            "score": 0.6628241073287204,
            "section_title": "Multidimensional Knowledge Distillation",
            "char_start_offset": 21009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 525
                },
                {
                    "start": 528,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1577
                },
                {
                    "start": 1580,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2021
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37646484375
        },
        {
            "corpus_id": "245974615",
            "title": "Knowledge Distillation via Weighted Ensemble of Teaching Assistants",
            "text": "Originally proposed by Bucila, Caruana, and Niculescu-Mizil (2006) [6] and popularized by Hinton, Vinyals, and Dean (2015) [2] knowledge distillation compress the knowledge of a large and computational expensive model (often an ensemble of neural networks) to a single computational efficient neural network. The idea of knowledge distillation is to train the small model, the student, on a transfer set with soft targets provided by the large model, the teacher. Knowledge distillation has been commonly used in a number of learning tasks since then. Modeling knowledge transfer between teacher and student has also been done using adversarial methods. Using several teachers was still a good way to improve robustness. Some studies also proposed deep mutual learning which allows an ensemble of student models to learn collaboratively and teach each other during training. \n\nThe main idea of using knowledge distillation is that student network (S) to be trained not only using the true labels information but also observation of how the teacher (T) works with the unseen data provided. Because the teacher model has more more generalization power, the idea is to train the student model is such way that it can mimic the behaviour of that generalization. The teacher network is complex in size being deeper and wider. \n\nLet a t and a s be the logits (the inputs to the final softmax) of the teacher and student network, respectively. In classic supervised learning, the mismatch between output of student network softmax(a s ) and the ground-truth label y r is usually penalized using cross-entropy loss and is given as, \n\nIn knowledge distillation one also tries to match the softened outputs of teacher y t =softmax(a t ) and student y s =softmax(a s ) via Kullback-Leibler divergence loss, \n\nby using a temperature parameter \u03c4 which has an additional control on softening of signal arising from the output of the teacher network. The student network is then trained under the following loss equation which used KD loss and cross-entropy loss, \n\nwhere \u03bb is the parameter used to trade-off between these two losses. This method is used for knowledge distillation from teacher and student models.",
            "score": 0.6626955565280832,
            "section_title": "B. Knowledge distillation",
            "char_start_offset": 6434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1795
                },
                {
                    "start": 1798,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2048
                },
                {
                    "start": 2051,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2199
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5927734375
        },
        {
            "corpus_id": "227228186",
            "title": "Bringing AI To Edge: From Deep Learning's Perspective",
            "text": "Knowledge distillation is another technique to conduct model compression, where a more compact student model can learn the knowledge from a complicated and powerful teacher model. Bucila et al. [148] first propose the concept of knowledge distillation, and Hinton et al. [140] generalize knowledge distillation and apply it to DNNs. \n\nThe core idea of knowledge distillation is to train a compact model (student) with the assistant of a complicated, pre-trained model (teacher). During training, the student model exploit the conventional method to train the model and obtain a loss according to the one-hot class distribution, e.g., [0, 0, 1, 0], namely hard targets and at the same time the knowledge from the teacher model is distilled and transferred to the student model by calculating a new loss in which the target is the probability distribution of predicted class P from the teacher model, e.g., [0.1, 0.21, 0.6, 0.09], namely soft target. Nevertheless, the probability of the correct class dominates the probability distribution generated by the teacher network (e.g., [0.97, 0.1, 0.0, 0.2]), which significantly limits the knowledge transferring capability. To alleviate this issue, Hinton et al. [140] propose softmax temperature in which temperature T is to soften the generated probability distribution. Intuitively, a larger T leads to a 'softer' probability distribution (e.g., [0.4, 0.2, 0.2, 0.2]). Hence, we are able to formulate the softmax with temperature as follows: \n\nwhen T is set to 1, it is the original softmax function. Please note that we refer the softmax function with temperature T as \u03b4 T for simplicity. Therefore, we can formulate the overall loss function as: \n\nwhere F denotes the cross-entropy function. \u03b1 and \u03b2 are two balancing factors. z s and z t represent the output logits from student model and teacher model, respectively. y is the ground truth. An illustration about how knowledge distillation works is shown in Fig. 7. \n\nAfter [140], many efforts are made towards improving the performance of knowledge distillation.",
            "score": 0.6626690594780716,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 46895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1695
                },
                {
                    "start": 1698,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 1966
                },
                {
                    "start": 1969,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 199,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.505859375
        },
        {
            "corpus_id": "274436184",
            "title": "Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation",
            "text": "Knowledge distillation seeks to transfer knowledge from a cumbersome, complicated teacher model to a more lightweight student model, enabling the student to achieve performance comparable to the teacher. In this work, we emphasize the transfer of fair and rich knowledge during the distillation process, rather than solely focusing on model compression. \n\nTo facilitate knowledge transfer, the knowledge distillation approach was designed by Hinton et al. (Hinton et al., 2015) to align the softened outputs generated by both the teacher and student models. Two supervisory signals are utilized in the training of the student model: (1) Hard Labels: The true labels from the training dataset. (2) Soft Labels: The label predictions of the teacher model. Formally, let Z tea represent the output logits from the teacher model, and Z stu denote the output logits from the student model, the loss to be minimized is defined as: \n\nwhere L CE denotes the cross-entropy loss and the hyperparameter \u03b1 regulates the relative contribution of the two loss functions. The knowledge distillation loss, L KD , is given by: \n\nwhere H KL is the KL-divergence, and \u03c4 is a temperature hyperparameter that regulates the smoothness of the two logit distributions by appropriately scaling them. A higher temperature makes the distribution flatter, generating softer predictions; a lower temperature enlarges the difference between the two distributions, generating harder predictions.",
            "score": 0.662480639320447,
            "section_title": "Knowledge distillation",
            "char_start_offset": 7851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 353
                },
                {
                    "start": 356,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1109
                },
                {
                    "start": 1112,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1464
                }
            ],
            "ref_mentions": [
                {
                    "start": 456,
                    "end": 477,
                    "matchedPaperCorpusId": "3144218"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69189453125
        },
        {
            "corpus_id": "221340727",
            "title": "MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down Distillation",
            "text": "Knowledge Distillation (KD), proposed by Hinton et al. [10], aims at transferring knowledge from a deep teacher network, denoting as T, to a shallow student network, denoting as S. In order to transfer knowledge from teacher to student, the loss for training student network is modified by adding KL divergence between the teacher's and the student's output probability distributions. Formally, given labeled dataset D of N samples D = {(x 1 , y 1 ) , . . . , (x N , y N )}, we can write the loss function of student network as following, \n\nwhere \u03b1 is the hyper-parameter to control the relative importance of the two terms, \u03c4 is the temperature hyper-parameter, \u03b8 T and \u03b8 S are the parameters of teacher T and student S respectively. L CE refers to the cross entropy loss and KL refers to the KL divergence which measures how one probability distribution is different from another. \n\nBy minimizing this modified loss function, the student network will try to match the one-hot ground-truth distribution (the first term) while lowering its discrepancy with output probability distribution from teacher T (x i ; \u03b8 T ) to transfer the knowledge of negative classes learned by teacher network. The probability distribution over classes can be obtained by employing softmax function, where the probability of sample s i belonging to class j can be expressed as, \n\nwhere C is number of classes, z i is the output logits of sample x i , \u03c4 is the same temperature hyper-parameter as (1). The role of \u03c4 is to control the smoothness of the output probability distribution. The higher the temperature, the higher the entropy of distribution, i.e. the smoother distribution.",
            "score": 0.6615453129278538,
            "section_title": "Background: Knowledge Distillation",
            "char_start_offset": 10547,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1663
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53271484375
        },
        {
            "corpus_id": "270077283",
            "title": "Self-Knowledge Distillation via Progressive Associative Learning",
            "text": "In this section, we first briefly introduce the most related works of knowledge distillation.Then we specifically review recent self-distillation works.\n\nKnowledge distillation is a widely used paradigm for model compression, which transfers knowledge from a complex teacher model to a compact student model.To be specific, the teacher network has high accuracy and huge parameters, while the student network is not as accurate as the teacher network but has fewer parameters.Through knowledge distillation, we hope that the student network can approach or exceed the teacher network as much as possible.In this way, we obtain a compact student network with a similar prediction effect as the teacher network.Ba et al. [30] first proposed a method that uses the teacher's logits before the softmax as the regression target to train the student network, which completes the imitation of the teacher network by forcing the student network to mimic the teacher network's logits.Hinton et al. [12] first proposed to use the soft outputs of the pretrained teacher network as dark knowledge to supervise the training of the student network.They introduced a temperature hyperparameter T and formulated the problem as \"knowledge distillation\".The student network is forced to learn the soft targets of the teacher network, which are obtained through using a high temperature T on the softmax inputs.In the process of knowledge transfer, soft targets often contain richer information than one-hot targets.Romero et al. [13] extended the knowledge distillation method proposed by Hinton et al.In their method, the student network can be deeper and narrower than the teacher network and improve the performance by learning the outputs of the teacher network and the features of the middle layer.All the above methods are offline distillation methods [31,32], which need a pretrained teacher network.\n\nIn contrast to these methods, online knowledge distillation trains the student network under the supervision of a teacher from scratch.For example, Zhang et al. [33] proposed a mutual learning method, which uses multiple neural networks.Zhao et al. [9] proposed a collaborative training method, which uses both an expert teacher and a from-scratch teacher to supervise the student.To reduce the computational cost, Zhou et al. [34] proposed to employ two different networks which share some low parameters and train separately.",
            "score": 0.6571825275582734,
            "section_title": "Related Work",
            "char_start_offset": 5661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 152
                },
                {
                    "start": 154,
                    "end": 308
                },
                {
                    "start": 308,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 975
                },
                {
                    "start": 975,
                    "end": 1134
                },
                {
                    "start": 1134,
                    "end": 1236
                },
                {
                    "start": 1236,
                    "end": 1392
                },
                {
                    "start": 1392,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1584
                },
                {
                    "start": 1584,
                    "end": 1785
                },
                {
                    "start": 1785,
                    "end": 1889
                },
                {
                    "start": 1891,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2128
                },
                {
                    "start": 2128,
                    "end": 2272
                },
                {
                    "start": 2272,
                    "end": 2418
                }
            ],
            "ref_mentions": [
                {
                    "start": 719,
                    "end": 723,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 989,
                    "end": 993,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1840,
                    "end": 1844,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1844,
                    "end": 1847,
                    "matchedPaperCorpusId": "258888057"
                },
                {
                    "start": 2052,
                    "end": 2056,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 2140,
                    "end": 2143,
                    "matchedPaperCorpusId": "198179767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71142578125
        },
        {
            "corpus_id": "246634659",
            "title": "Locally Differentially Private Distributed Deep Learning via Knowledge Distillation",
            "text": "Knowledge Distillation (KD) [14], [24], [25] was originally designed for deep neural network (DNN) compression and knowledge transfer. KD usually considers a \"teacherstudent\" paradigm, where the teacher model is a DNN (or an ensemble of a set of DNNs) that performs well on a given dataset, and the student model is another neural network that may or may not have the same architecture as the teacher model, but aims to mimic the performance of the teacher model(s) using another public dataset. Hinton, et al. [14] proposes an end-to-end knowledge distillation framework with a loss function, namely Distillation Loss, where the output of the teacher model is used as the soft target (i.e., soft label) for the student model, and the overall loss function is presented as below: \n\nwhere y is the true label of data x, z s is the output of the student model, z t is the output of the teacher model, \u03c3(z s ; T = 1)) is the softened label of z s at temperature T = 1, and \u03c3(\u2022; T = \u03c4 )) is the softened label at temperature T = \u03c4 , and usually \u03c4 > 1.",
            "score": 0.6564889968096781,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8307,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 779
                },
                {
                    "start": 782,
                    "end": 1047
                }
            ],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 38,
                    "matchedPaperCorpusId": "11536917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42236328125
        },
        {
            "corpus_id": "240544883",
            "title": "Deep Neural Network Compression for Plant Disease Recognition",
            "text": "Knowledge distillation, also known as teacher-student training, was proposed in [37]. An experiment demonstrated that a well-trained network uses its output soft labels to guide a simple, small student network through training, and dark knowledge can be easily transferred to the student network without changing its structure. In [38], authors proposed an ensemble of teacher networks to improve the generalization ability of a student network. \n\nIn [39], the authors used the attention maps of the middle layers of a teacher network to guide the training of the student network, aiming at enabling the student network to learn the feature maps of the teacher more effectively. Researchers in [40] found that if the given teacher and student networks are similar in structure, the student network more easily learns the knowledge of the teacher network, thus saving time when training a complex model.",
            "score": 0.6555152214812767,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9061,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 445
                },
                {
                    "start": 448,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 902
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 331,
                    "end": 335,
                    "matchedPaperCorpusId": "30258763"
                },
                {
                    "start": 694,
                    "end": 698,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5908203125
        },
        {
            "corpus_id": "253655030",
            "title": "Effective Online Knowledge Distillation via Attention-Based Model Ensembling",
            "text": "Knowledge distillation [3] was proposed as an effective and elegant compression technique to derive a lighter and faster network (student) from a more complex one (teacher), by penalizing the difference between their logits. Later, this mechanism was formalized by [2] to distill the \"dark-knowledge\" from the teacher to the student. The authors noticed that a powerful and confident teacher does not bring more knowledge than ground truth data, as its prediction tends to be a narrow probability distribution with a single peak for the ground truth class. To alleviate this issue, the teacher's logits are \"softened\" by a temperature scaling factor of the softmax activation. In such a manner, the lightweight network can infer what other classes were found similar by the teacher network. More formally, this can be expressed as \n\nwhere \u03c4 is the temperature of the softmax function (equation from [2]). When \u03c4 is greater than 1, the small probabilities of the softmax function are increased and the output is \"softened\". \n\nIn the classical setup, during training, a Kullback-Libeler divergence loss term is employed to ensure that the student network mimics the teacher's softened predictions. Other methods proposed using the root-mean-square error loss [9] or distilling from hardlabels [10].",
            "score": 0.6547830869863676,
            "section_title": "Related Work",
            "char_start_offset": 3724,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1296
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 26,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 265,
                    "end": 268,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 899,
                    "end": 902,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1291,
                    "end": 1295,
                    "matchedPaperCorpusId": "229363322"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "259228825",
            "title": "Research on Safety Helmet Detection Algorithm Based on Improved YOLOv5s",
            "text": "Knowledge distillation is a technique utilized to extract the knowledge of a large teacher model and condense it into a small student model. It can be understood as a large teacher neural network teaching his knowledge to a small student network [35][36][37]. \n\nThe process is transferred from the teacher network to the student network. The teacher network is generally bloated; therefore, the teacher network provides knowledge to the student network. The student network is a relatively small network and can thus obtain a lightweight network model. Knowledge distillation adopts the teacher-student mode. In this mode, the teacher is the output party of \"knowledge\", and the student is the receiver of \"knowledge\" [38]. \n\nThe teacher has a strong learning ability and can transfer the learned knowledge to the student model with a lower learning ability, so as to improve the generalization ability of the student model. The complicated and cumbersome but easy-to-use teacher model has no upper limit; it is purely a tutor, and in reality, a simple and flexible student model is deployed. The knowledge distillation process is shown in Figure 9   First, distill a deeper teacher network with a better extraction ability to obtain a logit, and distill it at T temperature. Then, use the classification prediction probability distribution in the Softmax layer to obtain soft targets. At the same temperature T, the logits in the student network are distilled, and then the category prediction probability distribution in Softmax is used to obtain the loss function L soft . Its expression is: \n\nwhere C j is the true label value of the j-th class. Finally, L hard and L soft are weighted and summed to obtain the final loss function L. This loss function can prevent the wrong information from the teacher network from being transmitted to the student network by comparing it with the real label. In this study, the improved YOLOv5s model was used as the teacher network, and the YOLOv5s model with the large target detection layer removed by structural pruning was used as the student model for knowledge distillation to obtain the final model and reduce the amount of calculation and parameters of the improved network model.",
            "score": 0.6540387232792229,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 38607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 259
                },
                {
                    "start": 262,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1594
                },
                {
                    "start": 1597,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 250,
                    "matchedPaperCorpusId": "250957345"
                },
                {
                    "start": 250,
                    "end": 254,
                    "matchedPaperCorpusId": "231969372"
                },
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "244283694"
                },
                {
                    "start": 718,
                    "end": 722,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43115234375
        },
        {
            "corpus_id": "203642142",
            "title": "Distillation \u2248 Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network",
            "text": "Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing [24]. Among these tasks, image classification is considered as one of the fundamental tasks since classification networks are commonly used as base networks for other problems. In order to achieve higher accuracy using a network with similar complexity as the base network, distillation has been proposed, which aims to utilize the prediction of one (teacher) network to guide the training of another (student) network. In [17], the authors suggested to generate a soft target by a heavy-duty teacher network to guide the training of a light-weighted student network. More interestingly, [14,5] proposed to train a student network parameterized identically as the teacher network. Surprisingly, the student network significantly outperforms the teacher network. Later, it was suggested by [49,19,9] to transfer knowledge of representations, such as attention maps and gradients of the classifier, to help with the training of the student network. In this work, we focus on the distillation utilizing the network outputs [17,14,45,5,46]. \n\nTo explain the effectiveness of distillation, [17] suggested that instead of the hard labels (i.e one-hot vectors), the soft labels generated by the pre-trained teacher network provide extra information, which is called the \"Dark Knowledge\". The \"Dark knowledge\" is the knowledge encoded by the relative probabilities of the incorrect outputs. In [17,14,45], the authors pointed out that secondary information, i.e the semantic similarity between different classes, is part of the \"Dark Knowledge\", and [5] observed that the \"Dark Knowledge\" can help to refine noisy labels. In this paper, we would like to answer the following question: can we theoretically explain how neural networks learn the Dark Knowledge? Answering this question will help us to understand the regularization effect of distillation. \n\nIn this work, we assume that the teacher network is overparameterized, which means that it can memorize all the labels via gradient descent training [12,11,34,1].",
            "score": 0.6533921859057485,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1958
                },
                {
                    "start": 1961,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 117,
                    "matchedPaperCorpusId": "1779661"
                },
                {
                    "start": 909,
                    "end": 911,
                    "matchedPaperCorpusId": "21596346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73095703125
        },
        {
            "corpus_id": "257771365",
            "title": "SELF-VS: Self-supervised Encoding Learning For Video Summarization",
            "text": "Knowledge distillation, first proposed by (Hinton et al. 2015) is a technique that allows for the transfer of knowledge from a pre-trained model, known as the teacher, to a smaller model, known as the student. The basic idea behind knowledge distillation is that the student model will be able to improve its performance by mimicking the behavior of the teacher model. This method is mostly utilized for model compression and learning from multiple teacher. However, recently in (Touvron et al. 2021) authors demonstrated that distillation training can not only encapsulate knowledge in smaller models but also aid in situations where there is a shortage of proper amounts of data. This is known as \"softdistillation\" and is formulated by the following equation: \n\nwhere Z t is the logits of the teacher model and Z s the logits of the student model. \u03c4 is denoted as the temperature for the distillation, \u03bb as a coefficient balancing the Kullback-Leibler divergence loss (KL) and the cross-entropy (L CE ) on ground truth labels y, and \u03c8 is the softmax function.",
            "score": 0.6498720183481104,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9287,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1062
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.412353515625
        },
        {
            "corpus_id": "189762324",
            "title": "Linear Distillation Learning",
            "text": "Knowledge distillation (KD) is a method of transferring \"knowledge\" from one machine learning model called teacher to another one called student. The idea behind KD is that a teacher network is a high-capacity model with desired high performance and a student network is a more lightweight model [1,19,27]. A student cannot match the teacher, but the distillation process brings the student closer to the predictive power of the teacher. Distillation idea was brought to the neural network community by Hinton et. al. [7]. \n\nIn distillation learning, knowledge is transferred by training a student model, using a soft target distribution for comparison with the output layer. This distribution is produced by a cumbersome model with a high temperature in its output softmax \n\nwith z i are logits and T is temperature. Another scenario of knowledge distillation training is transferring knowledge from an ensemble of highly regularized models to a smaller model [7]. \n\nDistillation can also be applied for adversarial permutation [15], born-again neural networks [6] and Global Additive Explanations [26]. Furthermore, Sau and Balasubramanian [20] proposed to add random perturbations into soft labels for simulating learning from multiple teachers. \n\nSurprisingly, a distillation method often allows smaller student network to be trained to mimic the larger and deeper models very accurately, while the student trained on the one-hot hard targets cannot achieve the same results. The clear reason for this awaits to be discovered.",
            "score": 0.6489767707952903,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 4344,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1248
                },
                {
                    "start": 1251,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1530
                }
            ],
            "ref_mentions": [
                {
                    "start": 296,
                    "end": 299,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 299,
                    "end": 302,
                    "matchedPaperCorpusId": "1923568"
                },
                {
                    "start": 302,
                    "end": 305,
                    "matchedPaperCorpusId": "16550689"
                },
                {
                    "start": 1029,
                    "end": 1033,
                    "matchedPaperCorpusId": "2672720"
                },
                {
                    "start": 1062,
                    "end": 1065,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1099,
                    "end": 1103,
                    "matchedPaperCorpusId": "53531803"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43408203125
        },
        {
            "corpus_id": "258564700",
            "title": "Do Not Blindly Imitate the Teacher: Using Perturbed Loss for Knowledge Distillation",
            "text": "Knowledge Distillation. Knowledge distillation was initially proposed in Bucilu\u01ce et al. [2006] to compress the large models to smaller, faster models without a significant performance drop. Hinton et al. [2015] generalized this technique by introducing a temperature parameter to smooth the teacher model prediction and Tian et al. [2019] employed contrastive learning to train the student model. Later, Yuan et al. [2020] explored the connection between KD and label smoothing while a review mechanism is developed to learn one student layer from multiple teacher layers [Chen et al., 2021]. Zhao et al. [2022] decoupled the classical loss to target classes and non-target classes for KD efficiency and flexibility. Ren et al. [2022] investigated supervisory signals and proposed to average teacher outputs for KD stability. \n\nDistillation Theory. Concurrent with the empirical success of knowledge distillation, numerous works aim to understand its mechanisms. Hinton et al. [2015] suggest that teacher's soft labels offer \"dark knowledge\" through weights on incorrect labels. Menon et al. [2021] present a statistical view, observing that a good teacher model should be Bayesian to reduce the student objective variance. Stanton et al. [2021] highlight discrepancies between teacher and student output distributions and emphasize the optimization challenge in distillation. While more recent studies [Ji and Zhu, 2020, Zhou et al., 2021, Hsu et al., 2021, Allen-Zhu and Li, 2023] explore distillation from several various angles, a gap remains between the theoretical analysis and the improved distillation techniques. \n\nLoss Function Design. Our work also relates to loss function design and learning. Lin et al. [2017] propose reshaping the cross-entropy loss to concentrate on hard examples and address the data imbalance issue. Leng et al. [2022] expand cross-entropy loss and focal loss into a linear combination of polynomial functions, primarily studying Poly-1 formulation on computer vision tasks while avoiding issues with high-order polynomial hyper-parameter searches.",
            "score": 0.6478582048250323,
            "section_title": "Related Work",
            "char_start_offset": 26010,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 94,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 404,
                    "end": 422,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 572,
                    "end": 591,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 593,
                    "end": 611,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1403,
                    "end": 1420,
                    "matchedPaperCorpusId": "224802975"
                },
                {
                    "start": 1706,
                    "end": 1723,
                    "matchedPaperCorpusId": "47252984"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56689453125
        },
        {
            "corpus_id": "232147231",
            "title": "Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation",
            "text": "After obtaining two pre-trained domain-mixed teachers, we employ knowledge distillation (KD), a technique to distilling knowledge by minimizing the KL-divergence between outputs of these two models. Here we adapt it to extract \"dark knowledge\" from these two complementary teachers. The pipeline of multi-teacher KD is shown in Fig. 1 (b), including two pre-trained domain-mixed teachers and one student with the same network architecture as teacher. The outputs of two teachers are ensembled as a stronger guidance to supervise the training of the student model on unlabeled target data. Besides, the student model is also supervised by the labels on the small amount of labeled target data. The objective function of learning student M S is defined as below. \n\nwhere \u03bb kl and \u03bb ce are the weights of KL-divergence loss and cross entropy loss respectively, E denotes the ensemble operation of two models. In experiments, the ensemble operation is implemented by averaging the outputs of two complementary teachers. By integrating knowledge from two views and making full use of unlabeled data, we can obtain one student with even superior performance than any one of its teachers.",
            "score": 0.6462930549294406,
            "section_title": "Multi-teacher Knowledge Distillation",
            "char_start_offset": 13820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1181
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78564453125
        },
        {
            "corpus_id": "253498852",
            "title": "Acoustic Pornography Recognition Using Convolutional Neural Networks and Bag of Refinements",
            "text": "Knowledge distillation [7] improves the ability of the student model by transferring the knowledge from the teacher model.\n\nAssuming z t and z s are the last fully connected layer outputs of the teacher model and student model respectively, and g represents the ground truth label. The traditional cross-entropybased classification loss function is as follows:\n\nThe knowledge distillation loss is as follows:\n\nwhere T is the temperature. The final training loss function of the student network is as follows:\n\nwhere \u03bb is a hyper-parameter to balance the two loss functions.",
            "score": 0.6460788995624918,
            "section_title": "F. Knowledge distillation",
            "char_start_offset": 8224,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.315673828125
        },
        {
            "corpus_id": "268230792",
            "title": "Differentially Private Knowledge Distillation via Synthetic Text Generation",
            "text": "Knowledge Distillation (KD) aims at transferring the knowledge of a teacher LLM T to a smaller student LLM S. The standard approach is to train the student to mimic the output distribution of the teacher and the true labels.Let f T (x) and f S (x) be the logit outputs of the teacher and student networks evaluated on an input data x, respectively.P T and P S be the softmax outputs of the teacher and student networks, respectively, Z T and Z S be the logit outputs of the teacher and student networks, respectively, and y be the true labels.The student is trained to minimize the linear combination of the supervised learning loss and the distillation loss:\n\nwhere \u03bb is a weight hyperparameter, L CE is the cross-entropy loss, L KL is the Kullback-Leibler divergence, \u03c3 is the softmax function, and the second term is the distillation loss which follows the softmax-temperature using a temperature parameter t (Hinton et al., 2015).At inference time, t is set to 1 to recover the standard softmax.y is known as the hard label because of its one-hot encoding while \u03c3(f T (x)/t) is the soft label.",
            "score": 0.6449540691542581,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 224,
                    "end": 348
                },
                {
                    "start": 348,
                    "end": 543
                },
                {
                    "start": 543,
                    "end": 659
                },
                {
                    "start": 661,
                    "end": 934
                },
                {
                    "start": 934,
                    "end": 999
                },
                {
                    "start": 999,
                    "end": 1097
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4609375
        },
        {
            "corpus_id": "274981678",
            "title": "CBNN: 3-Party Secure Framework for Customized Binary Neural Networks Inference",
            "text": "The standard approach in Knowledge Distillation (KD) [14] employs a teacherstudent paradigm, where a large and complex teacher network imparts knowledge to a smaller student network trained on the identical dataset. Figure 1 illustrates the entire KD algorithm's workflow. Distillation: DNNs typically convert the layer output, known as logit z i , into a class probability q T i for each class using the softmax function. \n\nThe concept of temperature T , introduced by Hinton [14], determines the softness of the probability distribution across classes. \n\nTeacher-Student Network: During training, as depicted in Figure 1, the computation of the student model's loss function follows the standard protocol. The dataset comprises inputs and labels, with the label of the ground truth referred to as the hard label. The student loss is determined using the crossentropy loss function H. \n\nThe term class denotes the accurate outcome of the classification process. Depending on the hard label y i , if y i = 1 when i corresponds to the correct class, or y i = 0 when i does not match the correct class, the student loss can be simplified as follows: \n\nAs Figure 1 illustrates, the teacher model employs the softmax function under a specific temperature T to generate predictions q T i , referred to as soft labels. Given the identical temperature T and inputs, the student model computes predictions p T i , known as soft predictions. Following the formula 3, the teacher loss is defined as: \n\nDuring the training process, the overall loss function L is computed as a combination of the student loss and the teacher loss. \n\nThe essence of knowledge distillation lies in the utilization of a higher temperature T to generate soft labels in the teacher model, facilitating the transfer of knowledge. The weighting factor \u03bb serves to prioritize the output of a specific loss function relative to others. Additionally, y denotes the hard label corresponding to the soft label p T . By training the student model with both soft and hard labels, it can achieve comparable accuracy to the more complex teacher model.",
            "score": 0.644637940873568,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 3427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 885
                },
                {
                    "start": 888,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1147
                },
                {
                    "start": 1150,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1619
                },
                {
                    "start": 1622,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2107
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.497802734375
        },
        {
            "corpus_id": "246680405",
            "title": "Conditional generative data-free knowledge distillation",
            "text": "Knowledge distillation is a general model compression technique. In KD, we can transfer knowledge by minimizing network logit difference. However, teacher logits usually have a high probability in the correct class and close to zero in other classes. It cannot provide more information than the one-hot label. To this end, Hinton et al. [9] introduced SoftMax temperature to soften model output and extract more relationship information between different classes. Furthermore, Furlanello et al. [26] trained and integrated a set of student networks as the final student model. Mirzadeh et al. [27] introduced a middle-sized model (teacher assistant) as a bridge to help the student network training. \n\nBesides distilling the logits, transferring knowledge to the student can be achieved by utilizing the feature information in hidden layers. Feature knowledge contains precise and efficient cues of the training process. In this case, Romero et al. [10] trained student networks using both logits and intermediate features. Zagoruyko et al. [11] transformed latent knowledge into attention maps and forced student networks to mimic the attention maps of the teacher. Yim et al. [28] introduced the FSP (Flow of Solution Procedure) matrix to learn the relationship between inputs and outputs. Park et al. [29] proposed a relational knowledge distillation method. They transferred the mutual relations of data according to the distance-wise loss and angle-wise loss. Bai et al. [30] designed a novel layer-wise cross distillation method and realize few-shot distillation. Li et al. [31] trained a portable objective detection model based on the supervision from proposal features. Liu et al. [32] presented a dense prediction distillation method based on holistic knowledge and pairwise similarity. Some recent works [33,34] also explored the structure effect to model distillation. Moreover, combining the knowledge distillation with other techniques (e.g., NAS [34], RL [35], or GNN [36]) can further compress and accelerate the heavy models. For other related works, [37] reviewed and looked at the development of knowledge distillation.",
            "score": 0.6441389313220858,
            "section_title": "Data-driven Knowledge Distillation",
            "char_start_offset": 5786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 495,
                    "end": 499,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 593,
                    "end": 597,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1041,
                    "end": 1045,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1178,
                    "end": 1182,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1304,
                    "end": 1308,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1476,
                    "end": 1480,
                    "matchedPaperCorpusId": "208201983"
                },
                {
                    "start": 1580,
                    "end": 1584,
                    "matchedPaperCorpusId": "267213"
                },
                {
                    "start": 1690,
                    "end": 1694,
                    "matchedPaperCorpusId": "208109903"
                },
                {
                    "start": 1815,
                    "end": 1819,
                    "matchedPaperCorpusId": "211004051"
                },
                {
                    "start": 1819,
                    "end": 1822,
                    "matchedPaperCorpusId": "235212488"
                },
                {
                    "start": 1961,
                    "end": 1965,
                    "matchedPaperCorpusId": "235212488"
                },
                {
                    "start": 1970,
                    "end": 1974,
                    "matchedPaperCorpusId": "13352766"
                },
                {
                    "start": 1983,
                    "end": 1987,
                    "matchedPaperCorpusId": "207930254"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.457763671875
        },
        {
            "corpus_id": "236782270",
            "title": "Adaptive Teacher Finetune: Towards high-performance knowledge distillation through adaptive fine-tuning",
            "text": "Knowledge distillation [1] is a well-known technique to learn compact deep neural network models with competitive accuracy, where a smaller network (student) is trained to simulate the representations of a larger one (teacher) with higher accuracy. The popularity of knowledge distillation is mainly due to its simplicity and generality; it is straightforward to learn a student model based on a teacher and there is no restriction about the network architectures of both models. The main goal of most approaches is how to transfer dark knowledge to student models effectively, given predefifined or pretrained teacher networks. Parameter quantification or character recognition, pruning, and knowledge distillation (KD) are some well-known methods in this subject [1]. KD defines the class probability of the teacher system as the aim that the tiny contactors strives to replicate by transferring the knowledge of big pre-training networks (or integration of tiny networks) as a teacher network. Students can enhance their performance by matching their predictions with the teacher's predictions. There is no unique teacher-student function in provided to users refinement. Starting with the initial training, all systems learn at the same time by teaching each other. It uses ground truth labels to train traditional cross-entropy loss and imitation loss to learn from peers. The result obtained by the network trained in this online distillation method is not only better than the network trained only by cross-entropy loss, but it performs better than a network taught in the typical offline distillation method through an also before the teacher network Hinton et al. developed many prospective knowledge distillation approaches to promote the optimization process of distilling by using diverse \"information,\" such as intermediate representation, in [2], inter-layer flow [3], pay attention to figure [5], structure Relationship [6] and activation similarity [7]. While these 's system well in compressed deployment models, they often use a two-stage training approach, which involves before the a highly capable previous teachers before transferring knowledge to a compact student model, which takes more time and money.",
            "score": 0.6431553104083524,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2227
                }
            ],
            "ref_mentions": [
                {
                    "start": 1907,
                    "end": 1910,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1935,
                    "end": 1938,
                    "matchedPaperCorpusId": "198185886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5478515625
        },
        {
            "corpus_id": "269921267",
            "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
            "text": "This process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.\n\nThe core idea of knowledge distillation is that while large teacher models may have high predictive accuracy due to the large number of parameters, these advantages are not always directly applicable to resource-constrained environments.Knowledge distillation therefore aims to extract the \"dark knowledge\" of the teacher model -not just the final predictions, but also its decision-making processes and uncertainty estimates for the input data -and then infuse this knowledge into a more concise student model:  The knowledge distillation technique adopts a typical teacher-student training framework, as shown in Figure 2. In this framework, the \"teacher\" model represents a highly complex deep neural network.In contrast, the Student model is designed to be more streamlined and aims to achieve a similar level of performance with fewer parameters.The process begins with the pre-training of the Teacher model, a step that allows it to accumulate broad and deep knowledge on the specified data set.Instead of directly replicating the weights of the \"teacher\" model, the subsequent knowledge transfer phase uses its output -particularly soft probability distributions (rather than hard classification labels).This process combines the rich experience of the \"teacher\" model with direct guidance from real-world data to form a dual oversight mechanism designed to efficiently extract and transfer deep learning capabilities, while reducing resource burdens through model compression techniques and enhancing the feasibility and flexibility of the model in practical applications.\n\nKnowledge distillation is generally categorized into two main types: output-based and feature-based distillation.In this approach, the student model acquires similar predictive abilities by assimilating the output probability distribution from the teacher model, essentially grasping and transforming high-level abstract concepts.This process is achieved by designing specialized distillation loss functions that ensure that the student model can capture and mimic the high-level transformation logic of the teacher model to the input information, thus maintaining consistent or close performance to the teacher model on the prediction task.\n\nFeature-based distillation emphasizes the assimilation of intermediate layer features from the teacher model.",
            "score": 0.6426790735200186,
            "section_title": "B. Knowledge distillation",
            "char_start_offset": 14355,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 182,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 894
                },
                {
                    "start": 894,
                    "end": 1033
                },
                {
                    "start": 1033,
                    "end": 1183
                },
                {
                    "start": 1183,
                    "end": 1393
                },
                {
                    "start": 1393,
                    "end": 1762
                },
                {
                    "start": 1764,
                    "end": 1877
                },
                {
                    "start": 1877,
                    "end": 2405
                },
                {
                    "start": 2407,
                    "end": 2516
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8330078125
        },
        {
            "corpus_id": "258108057",
            "title": "Knowledge-Distilled Graph Neural Networks for Personalized Epileptic Seizure Detection",
            "text": "Knowledge Distillation Knowledge distillation (KD) [11] refers to transferring knowledge from a large/sophisticated pre-trained neural network (known as the teacher network) to a smaller network (known as the student network). The student represents a light-weight model derived from the teacher while enforcing the performance to be similar to that of the teacher. A distillation loss is used during training to guide the student to replicate the teacher's behavior as closely as possible. Different types of knowledge can be transferred, but the most straightforward one is response-based KD, which refers to the response of the output layer of the teacher. A widely used example of this is the class probability called as soft targets defined using a softmax function as \n\nwhere p i is the probability of belonging to class i, z is the vector of logits (outputs of the last layer of the teacher to a given input). The temperature T controls the contribution of each soft target to the knowledge. When T is equal to 1, we get the standard softmax function, but as T increases, the probability distribution is softened. The distillation loss can be seen as comparing the class probabilities obtained from the teacher and the student. It enforces the distribution of the outputs produced by the student to be close to that of the teacher. The Kullback-Leibler (KL) divergence is therefore often used as the distillation loss function, and minimizing this loss during training makes the logits of the student get closer to the logits of the teacher [10]. Let z t and z s denote the representation produced by the teacher and student models, respectively, for the same input. Then, the final loss function used to train the student is a weighted average of the two terms and is defined as \n\nwhere L D is the distillation loss function, p(z t , T ) are the teacher soft targets, p(z s , T ) are the student soft targets, L CE is the cross entropy loss function, y are the ground truth labels, and \u03b1 is the weighting factor. The parameter \u03b4 represents the relative weight given to the teacher's knowledge over the new training data corresponding to the student training \u2212 the higher \u03b4, the lesser the model relies on the teacher for the training of the student.",
            "score": 0.6409755188521797,
            "section_title": "Preliminaries",
            "char_start_offset": 7650,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1786
                },
                {
                    "start": 1789,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 1548,
                    "end": 1552,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65869140625
        },
        {
            "corpus_id": "270257779",
            "title": "Robust Knowledge Distillation Based on Feature Variance Against Backdoored Teacher Model",
            "text": "acquired by the teacher model to a smaller student model.Typically, teacher models are complex and powerful deep models, and student models can learn from both the output logits of the teacher model and the ground truth for knowledge transfer.While most neural networks [57,58] typically use the \"softmax\" output layer to generate category probabilities, the purpose of knowledge distillation is to make the softmax outputs of the student model and the teacher model similar enough.To achieve this, knowledge distillation introduces a softmax function with a temperature parameter, which can be defined as:\n\nwhere z i is the logits of the model, and T is the temperature factor.When T =1, q i is the standard softmax function.In this situation, the results output by the softmax layer will be more distributed and more information between and within classes will be retained with the increase of the temperature factor.\n\nAccording to the above properties, when knowledge distillation is implemented, the logits output by the teacher model and the student model will be processed with a higher temperature factor to obtain a soft target.Let p T j and q T j denote the output soft target of the teacher model and the student model after being \"softened\" under the temperature T , N is the total number of labels, and let L denote a standard cross entropy loss which is used to measure the direct distribution difference between p T j and q T j .The loss function of the soft target is as follows:\n\nSince the teacher model also has a certain error rate, the use of ground truth can effectively reduce the possibility of errors being transmitted to the student model.c j is defined as the value of ground truth in the j-th class.The positive label takes '1' and the negative label takes '0'.The loss function of the hard target is as follows:\n\nCombining the loss of soft and hard target, the total object function of knowledge distillation:\n\nwhere \u03b1 is a hyperparameter balancing the two terms.",
            "score": 0.6403741067597054,
            "section_title": "Knowledge distillation is a compression technique designed to transfer knowledge or information",
            "char_start_offset": 15843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 57,
                    "end": 243
                },
                {
                    "start": 243,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 606
                },
                {
                    "start": 608,
                    "end": 678
                },
                {
                    "start": 678,
                    "end": 726
                },
                {
                    "start": 726,
                    "end": 919
                },
                {
                    "start": 921,
                    "end": 1136
                },
                {
                    "start": 1136,
                    "end": 1443
                },
                {
                    "start": 1443,
                    "end": 1494
                },
                {
                    "start": 1496,
                    "end": 1663
                },
                {
                    "start": 1663,
                    "end": 1725
                },
                {
                    "start": 1725,
                    "end": 1787
                },
                {
                    "start": 1787,
                    "end": 1838
                },
                {
                    "start": 1840,
                    "end": 1936
                },
                {
                    "start": 1938,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 274,
                    "end": 277,
                    "matchedPaperCorpusId": "216263993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.469970703125
        },
        {
            "corpus_id": "262046518",
            "title": "Distilling Hubert with LSTMs via Decoupled Knowledge Distillation",
            "text": "The method of knowledge distillation proposed in [4] consists of transferring knowledge from a potentially large and cumber-arXiv:2309.09920v1 [eess.AS] 18 Sep 2023 some classification model to a distilled one using a soft target distribution for each sample in a transfer dataset, calculated for each of C classes as \n\nwhere l c represents the logit of class c, and \u03c4 is a temperature scaling. The authors argue that soft targets provide much more information than hard labels and show that they are an effective way of communicating patterns discovered by a teacher model, being leveraged as guidance for training smaller models even with less data than the teacher. The knowledge distillation loss [4,5] is defined as \n\nwhere KL(\u2022||\u2022) is the Kullback-Leibler divergence, and p T and p S are the softmax-probability scores output by the teacher and the student model, respectively. The final loss is a linear combination of L KD and the cross-entropy loss between the student's class probabilities and the ground-truth hard labels. More recently, Knowledge Distillation (DKD) [5] was proposed. The authors first show that the KD loss can be split into two parts: Target Class Knowledge Distillation (TCKD) and Non-Target Class Knowledge Distillation (NCKD). The first one is argued to transfer the knowledge about the \"difficulty\" of the training samples, while the latter contains valuable knowledge about the non-target classes. The reformulated loss reads as \n\nwhere b \u2208 R 2 are the binary probabilities of the target class and all the other non-target classes together, and p \u2208 R C\u22121 represents the probabilities among non-target classes, discarding the target one. The NCKD term is shown to be coupled and inversely tied to the teacher's confidence on the target output, p T . This means that a good teacher would have a high p T , suppressing the NCKD component. As a consequence, the effectiveness of distillation of valuable information about the non-target classes would be limited for well-predicted samples.",
            "score": 0.6399991570480461,
            "section_title": "KNOWLEDGE DISTILLATION",
            "char_start_offset": 3750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1463
                },
                {
                    "start": 1466,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 52,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 701,
                    "end": 704,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 704,
                    "end": 706,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1078,
                    "end": 1081,
                    "matchedPaperCorpusId": "247476179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265380859375
        },
        {
            "corpus_id": "274436184",
            "title": "Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation",
            "text": "In this subsection, we introduce the concept of learning node-specific temperatures to enhance knowledge transfer from teacher to student models. While dual-teacher distillation and graphlevel distillation ensure that teacher models encode fair and information-rich representations, a critical remaining challenge is ensuring effective transfer of this knowledge to the student model, thereby improving performance. \n\nLearning node-specific temperatures addresses the limitations inherent in the temperature mechanism used in traditional distillation methods. In conventional knowledge distillation, a predefined temperature parameter is used to generate soft targets. The teacher model's informative dark knowledge is embedded in the soft targets, which infer the probability that a node belongs to a given class. The softmax function with a fixed temperature is defined as follows: \n\nwhere the temperature parameter \u03c4 is used to control the softness of each prediction. The distribution of predictions becomes smoother as the temperature rises, and sharper as the temperature falls. Thus, the temperature is responsible for regulating the balance between the true label knowledge and the dark knowledge. However, using a fixed temperature treats the softening of all teacher logits equally, ignoring variations in node representations. This uniform approach fails to provide precise guidance for individual nodes, which can hinder knowledge transfer. Moreover, existing research has shown that temperature scaling can improve model performance (Z. Zhang and Sabuncu, 2020). \n\nTo confront these issues, we propose an entropy-based approach to learn node-specific temperatures. This approach assigns a unique temperature \u03c4i to each node, controlling the degree of softening individually. The softmax function with node-specific temperatures is defined as: \n\nwhere the temperature parameter \u03c4i is the specific temperature of the i-th node. Obviously, \u03c4i determines the softening direction of each node. The temperature of the node is related to the teacher's confidence for each respective node (Zhichun Guo et al., 2023). Therefore, we can judge the teacher's confidence for each node through the entropy of the teacher's logits. The lower the entropy value, the higher the teacher's confidence for this node. Specifically, we calculate the node-specific temperature through the probability distribution of the teacher and the confidence of the teacher, defined as follows:",
            "score": 0.639935571852821,
            "section_title": "Learning node-specific temperatures",
            "char_start_offset": 17591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1575
                },
                {
                    "start": 1578,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1855
                },
                {
                    "start": 1858,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2229
                },
                {
                    "start": 2230,
                    "end": 2309
                },
                {
                    "start": 2310,
                    "end": 2473
                }
            ],
            "ref_mentions": [
                {
                    "start": 1550,
                    "end": 1574,
                    "matchedPaperCorpusId": "219558831"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80810546875
        },
        {
            "corpus_id": "232076059",
            "title": "PURSUhInT: In Search of Informative Hint Points Based on Layer Clustering for Knowledge Distillation",
            "text": "Knowledge distillation (KD) is the process of transferring knowledge between networks, where one usually aims to transfer the knowledge of a big network (teacher) to a smaller/more compact network (student). KD is mostly known due to Hinton's work [16], while it was first proposed by [3]. The most well known form of KD uses the combination of soft targets produced by the teacher model and labels as the target in its objective function. The methods that uses soft targets without hints, are known as output (logit) distillation [14]. \n\nSoft targets are calculated as in (1), by passing training batches through teacher model and using softmax output layer with a higher temperature value (T > 1): \n\nwhere p, z and T show the softened class probability, logit and temperature values, respectively. For instance, i, j \u2208 {0, ..., 99} for a classification task with 100 classes, such as CIFAR-100. The same temperature value is used for generating both softened logits in teacher and student models, and higher T results in a softer class probability distribution. The loss function to train the student network is evaluated in (2) as follows: \n\nwhere \u03bb is the trade-off between logit distillation loss (L logit ) and classification loss (L cls ), y represents the label, p S and p T are the softened logits of student and teacher networks, respectively. Temperature is set to 1 in L cls . Cross-entropy and Kullback-Leibler losses are generaly used for L cls and L logit , respectively. A recent study [10] improves KD, where it introduces Spherical Knowledge Distillation (SKD) that projects all logits of the teacher and the student on a sphere by normalization. In SKD, logits are scaled into a unit vector and multiplied with the average teacher norm in order to recover its norm to the original level. [69] introduces a novel distillation approach by reformulating the classical logit distillation. Furthermore, [47] proposes a logit distillation approach in order to train a transformer student using lightweight teacher models with different architectures.",
            "score": 0.6395090410819151,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 536
                },
                {
                    "start": 539,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2063
                }
            ],
            "ref_mentions": [
                {
                    "start": 285,
                    "end": 288,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 531,
                    "end": 535,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 573,
                    "end": 576,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1807,
                    "end": 1811,
                    "matchedPaperCorpusId": "247476179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.441650390625
        },
        {
            "corpus_id": "245974615",
            "title": "Knowledge Distillation via Weighted Ensemble of Teaching Assistants",
            "text": "Neural networks typically produce class probabilities by using a \"softmax\" output layer that converts the logit, z i , computed for each class into a probability, q i , by comparing z i with the other logits. \n\nwhere T is a temperature that is normally set to 1. As T is set to a higher value, the probability distribution over classes becomes softer. In the most basic form of distillation, knowledge is transferred to the distilled model by training it on a transfer set and using a soft target distribution for each case in the transfer set, which is produced by using a cumbersome model with a high softmax temperature. There is theoretical study on how the dark knowledge helps the student to learn finer structure from the big model, the teacher model. The low temperature models are usually good at hard predictions and we lose the dark knowledge. That's why, the main objective of distillation is to transfer the dark knowledge from teacher to student.",
            "score": 0.6373323622812805,
            "section_title": "A. Distillation Theory",
            "char_start_offset": 13849,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 211,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 960
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68310546875
        },
        {
            "corpus_id": "248848394",
            "title": "Bearing Faulty Prediction Method Based on Federated Transfer Learning and Knowledge Distillation",
            "text": "The concept of knowledge distillation (KD) was first introduced by Hinton and Dean [19] as a model compression framework which releases the parameter size of the deep learning model by constructing a teacher-student paradigm where the student network is trained to capture the information contained not only in the hard version of the true label, but also in the softer version of the teacher's output. Different from the ordinary transfer learning, the knowledge distillation accomplishes the knowledge transference tasks by altering the loss function of the student model to follow the output of the teacher model [20]. The traditional KD framework compresses one or several cumbersome networks (teachers) into a student network with a shallow structure. The framework of the conventional knowledge distillation can be categorized into two types: single teacherbased knowledge distillation and multi-teacher-based knowledge distillation [19,[21][22][23][24].",
            "score": 0.6366230218648042,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 11190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 960
                }
            ],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 620,
                    "matchedPaperCorpusId": "220632998"
                },
                {
                    "start": 955,
                    "end": 959,
                    "matchedPaperCorpusId": "209078813"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.296630859375
        },
        {
            "corpus_id": "247521335",
            "title": "Generalized Knowledge Distillation via Relationship Matching",
            "text": "Knowledge Distillation (KD). Rich supervision plays a crucial role in building a machine learning or visual recognition system, where taking advantage of the learning experience from related pre-trained models becomes a shortcut to facilitate the model training in the current task [29]. Different from fine-tuning [30] or weights matching [31], [32], [33], [34], [35], [36] that regularize the model from the \"parameters\" perspective, we can reuse the dark knowledge/privileged information [37], [38], [39] to explain or assist the training process of the model from the \"data\" aspect [40], [3]. Denote a fixed well-trained model from a related task and the model in the current task as the \"teacher\" and the \"student\", respectively. KD matches the behaviors of two models on the current task's data [41], [42], [19]. The teacher could be a highcapacity deep neural network trained on the same task [13], [9], [43] or a previous generation of the model along the training progress [44], [7], [8]. The dark knowledge in KD can be implemented as the soft label, i.e., the posterior probability of an instance [3], [45], [17], hidden layer activation [5], [46], [47], [48], parameter flows [4], transformations [49], and pathwise statistics [50]. Distilling the knowledge from one model to another has been investigated for model compression [2], [11], [12], [51] and incremental learning [52], [9], [43]. Cross-task knowledge transfer. In practical applications, the knowledge from teachers of other tasks, i.e., teachers trained on non-overlapping sets of labels, also assists the training of a target task student. Heterogeneous transfer learning updates both student and teacher on the current and related domains (resp. tasks) to close distribution (resp. label) divergence [22], [53].",
            "score": 0.636073233395664,
            "section_title": "RELATED WORK",
            "char_start_offset": 6355,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1788
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 286,
                    "matchedPaperCorpusId": "3444548"
                },
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "13740328"
                },
                {
                    "start": 340,
                    "end": 344,
                    "matchedPaperCorpusId": "8628903"
                },
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "3603048"
                },
                {
                    "start": 358,
                    "end": 362,
                    "matchedPaperCorpusId": "3603145"
                },
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "46972534"
                },
                {
                    "start": 370,
                    "end": 374,
                    "matchedPaperCorpusId": "219489919"
                },
                {
                    "start": 491,
                    "end": 495,
                    "matchedPaperCorpusId": "12342641"
                },
                {
                    "start": 497,
                    "end": 501,
                    "matchedPaperCorpusId": "12874183"
                },
                {
                    "start": 503,
                    "end": 507,
                    "matchedPaperCorpusId": "43829067"
                },
                {
                    "start": 592,
                    "end": 595,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 801,
                    "end": 805,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 813,
                    "end": 817,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 900,
                    "end": 904,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 906,
                    "end": 909,
                    "matchedPaperCorpusId": "226841742"
                },
                {
                    "start": 988,
                    "end": 991,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 993,
                    "end": 996,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 1108,
                    "end": 1111,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1113,
                    "end": 1117,
                    "matchedPaperCorpusId": "16391544"
                },
                {
                    "start": 1119,
                    "end": 1123,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1160,
                    "end": 1164,
                    "matchedPaperCorpusId": "21596346"
                },
                {
                    "start": 1166,
                    "end": 1170,
                    "matchedPaperCorpusId": "172133986"
                },
                {
                    "start": 1188,
                    "end": 1191,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1209,
                    "end": 1213,
                    "matchedPaperCorpusId": "49869692"
                },
                {
                    "start": 1239,
                    "end": 1243,
                    "matchedPaperCorpusId": "222178995"
                },
                {
                    "start": 1340,
                    "end": 1343,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1387,
                    "end": 1391,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 1393,
                    "end": 1396,
                    "matchedPaperCorpusId": "226841742"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60888671875
        },
        {
            "corpus_id": "212644537",
            "title": "Multitask Emotion Recognition with Incomplete Labels",
            "text": "Hinton et al. [16] proposed Knowledge Distillation for model compression. The knowledge of a larger network is transferred to a relatively smaller network using a modified cross entropy loss function. They introduce a new hyperparameter called temperature T into the softmax function, and suggest that setting T > 1 can increase the weight of smaller logit values, thus providing dark knowledge. In other words, the relative probabilities can reveal more information about inter-class relations than the one-hot labels. Knowledge Distillation has been proved effective in model compression, continual learning [27] and domain adaptation [1]. However, its application to multitask learning with missing labels is under-researched. \n\nKnowledge distillation for regression is not as common as for classification. Some work in face alignment [26], [32] has used the L1 or L2 distance as the distillation loss function. \n\nTo enable the use of knowledge distillation using the hyperparameter temperature for valence-arousal estimation, we transform the regression task to a classification task by discretizing the continuous values. Then we can use temperature to control the smoothness of the soft labels.",
            "score": 0.6335910436097685,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 6487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 914
                },
                {
                    "start": 917,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1200
                }
            ],
            "ref_mentions": [
                {
                    "start": 610,
                    "end": 614,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 637,
                    "end": 640,
                    "matchedPaperCorpusId": "206742843"
                },
                {
                    "start": 838,
                    "end": 842,
                    "matchedPaperCorpusId": "8911820"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5234375
        },
        {
            "corpus_id": "276538080",
            "title": "Tiny Language Models for Automation and Control: Overview, Potential Applications, and Future Research Directions",
            "text": "Knowledge distillation is a technique that transfers knowledge from a larger, pretrained teacher model to a smaller student model [49][50][51][52]. The student model is trained to mimic the teacher's behavior, matching its predictions to achieve similar performance with fewer parameters. \n\nThe distillation loss combines task-specific loss L task and the knowledge distillation loss L distill , which measures the KL-divergence between the teacher's and student's softened probability distributions. The total loss is given by: \n\nwhere P t = softmax(z t /T) and P s = softmax(z s /T) are the probability distributions from the teacher and student models, respectively, T is the temperature parameter that controls the softness of the output logits, and \u03b1 is a balancing factor between the task and distillation loss. Algorithm 1 demonstrates the process of knowledge distillation, which involves training the student model by combining the task-specific loss and the distillation loss. The algorithm begins with the teacher and student models' forward passes to calculate logits and softened probabilities. It then computes the total loss using both components and iteratively updates the student model's weights to minimize the combined loss. This iterative process ensures that the student model captures the teacher's knowledge while maintaining computational efficiency. Forward pass M t and M s to obtain logits z t and z s 4:",
            "score": 0.6334389223611828,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7887,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1432
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 142,
                    "matchedPaperCorpusId": "259164722"
                },
                {
                    "start": 142,
                    "end": 146,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.352294921875
        },
        {
            "corpus_id": "257102399",
            "title": "Knowledge Distillation-based Information Sharing for Online Process Monitoring in Decentralized Manufacturing System",
            "text": "Knowledge distillation is a model compression technique proposed by Hinton et al. in 2015 [9], which could effectively train a smaller student model by learning from a larger teacher model. As shown in Figure 2, a general knowledge distillation framework consists of three major components: teacher-student network architecture, knowledge, as well as distillation algorithm. The main idea of knowledge distillation is that the student model mimics the teacher model in order to achieve a comparable or even a superior performance [10]. Teacher network usually has more complicated network architecture and learns from the data first. After the training of teacher network, the useful knowledge could be distilled from the teacher network. Hereafter, student network learns the knowledge distilled from teacher network to mimic teacher's behavior together with the training data. The core of knowledge distillation is how to simplify model while maintaining relatively good performance. \n\nOne promising and efficient way to distill knowledge is to mimic the logit output of last layer in the teacher model (i.e., teacher's prediction). Soft target, i.e., the probability that the input data   belongs to each class, is proposed as the logit of teacher network. For a -class classification problem, the soft target of teacher network \u0398  could be calculated by the softmax function as: \n\nWhere   denotes the -th training sample,  is the number of classes,    is logit of last layer for the th class and  denotes temperature factor which could control importance of each target. For example, a higher  produces a softer probability distribution among different classes (assign probability with less difference to each class). Soft target contains informative dark knowledge of teacher network and could enhance performance of student network. Empirically, when the student model is very small compared to the teacher model, lower temperatures work better [9]. This is because a very small model might not be able to capture all the information when we raise the temperature. Therefore, soft target could be seen as knowledge extracted from teacher network and it could be transferred to student network by matching output of two networks. To measure the output similarity between teacher network \u0398  and student network \u0398  , the Kullback Leibler (KL) divergence is often employed, which could be formulated in Eq. ( 2),",
            "score": 0.6326175772314484,
            "section_title": "Knowledge distillation",
            "char_start_offset": 15159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1382
                },
                {
                    "start": 1385,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2234
                },
                {
                    "start": 2235,
                    "end": 2414
                }
            ],
            "ref_mentions": [
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60205078125
        },
        {
            "corpus_id": "269282659",
            "title": "Dynamic Temperature Knowledge Distillation",
            "text": "Knowledge distillation [Hinton et al., 2015] is a very effective model compression technology that aims to transfer knowledge from a more bloated and complex teacher model to a more compact and lightweight student model so that the model can be deployed on devices with more restricted resources.Typically the output of the teacher is too sharp (confident) and makes it difficult for the student model to learn the subtle differences between the incorrect classes.As a result, soft labels regulated by \"temperature\" are often used to improve the KD performance.In practice, the loss function of knowledge distillation usually consists of two parts: cross-entropy (CE) loss with hard labels, and KL-divergence loss with soft labels.Temperature has a great impact on the smoothness of the soft labels.Larger temperatures make the soft label smoother, and smaller temperatures make the soft label sharper.A few research studies have observed that a fixed temperature might hinder the KD process, and therefore devoted to exploring the dynamic regulation of temperature during the KD process.For instance, Annealing KD [Jafari et al., 2021] believed that tuning the temperature can bridge the capacity gap between the teacher and the student networks, and proposed to control the temperature through simulated annealing.CTKD [Li et al., 2023] showed that the task difficulty level can be controlled during the student's learning career through a dynamic and learnable temperature.These methods overlooked the differences in smoothness between the output distributions of the student and teacher networks, and they all apply the same temperature to both the teacher and the student.Furthermore, all the above methods need to add new modules or increase training costs, which greatly reduces the flexibility of their usage.As a result, empirically fixed temperatures remain in most recently developed KD algorithms (for example, DKD [Zhao et al., 2022]).NormKD [Chi et al., 2023] originates from a perspective akin to ours, employing normalization of the output logits to establish a consistent degree of smoothness across the distribution.However, their approach does not account for the collaborative dynamics be-tween teacher and student models, resulting in students with markedly low confidence levels that render them less practical for real-world deployment.",
            "score": 0.6325180087742013,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 296,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 561
                },
                {
                    "start": 561,
                    "end": 731
                },
                {
                    "start": 731,
                    "end": 799
                },
                {
                    "start": 799,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 1088
                },
                {
                    "start": 1088,
                    "end": 1316
                },
                {
                    "start": 1316,
                    "end": 1476
                },
                {
                    "start": 1476,
                    "end": 1677
                },
                {
                    "start": 1677,
                    "end": 1817
                },
                {
                    "start": 1817,
                    "end": 1948
                },
                {
                    "start": 1948,
                    "end": 2134
                },
                {
                    "start": 2134,
                    "end": 2359
                }
            ],
            "ref_mentions": [
                {
                    "start": 1321,
                    "end": 1338,
                    "matchedPaperCorpusId": "254069919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.388671875
        },
        {
            "corpus_id": "252816131",
            "title": "Detect, Distill and Update: Learned DB Systems Facing Out of Distribution Data",
            "text": "KD was first introduced in [22] for   by transferring knowledge from an accurate and \"cumbersome\" model, called teacher, to a smaller model called student. In its basic form, instead of fitting the student model directly to the actual data labels, one would use the class probability distribution learned by the teacher to fit the student model. Hinton et al. [22] argued that small probabilities in \"wrong\" label logits, known as \"soft labels\", include extra information called \"dark knowledge\" that result in better learning than actual \"hard labels\". Distillation has since been extensively studied. Figure 2 shows a general view of the principles of a distillation process. A small dataset referred to as transfer-set is fed into a pre-trained model (teacher) and a new model (student) to be trained. A   is calculated using the predictions of the pre-trained model instead of the actual labels. This loss and a typical loss using actual labels will be used to train the new model. To formulate knowledge distillation, consider a model with parameters \u0398, representing a function   ( for teacher) which has been trained via Eq. 2. We would like to transfer knowledge from this teacher model to a student model with parameter \u0398 \u2032 , representing a function   . This new model could be trained as follows: \n\nfor weight , distillation loss \u2112  , and transfer-set  . \n\n4.2 DDUp: Updating By Knowledge Distillation [12,66] showed that, for classification tasks, if instead of having a compact student model, one uses the same architecture of the teacher, and repeat distillation sequentially for several generations, the student models in the later generations could outperform the teacher model. This approach is called sequential self-distillation. Inspired by this and anticipating that this will be valid for our learning tasks, DDUp also employs a sequential self-distillation approach. \n\nTo update a model using KD, a copy of the previously trained model becomes the new student. Then, the student is updated using a distillation loss (to be defined soon). After updating, the previous teacher is replaced with the new updated model. This cycle repeats with every new insertion batch.",
            "score": 0.6325162826798058,
            "section_title": "General Knowledge Distillation (KD)",
            "char_start_offset": 22591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1305
                },
                {
                    "start": 1308,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1887
                },
                {
                    "start": 1890,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2186
                }
            ],
            "ref_mentions": [
                {
                    "start": 1411,
                    "end": 1415,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.703125
        },
        {
            "corpus_id": "214623014",
            "title": "Label Noise Types and Their Effects on Deep Learning",
            "text": "Compared to the previous two label noise models, feature dependent noise is harder to implement since all samples should be vectorized in the feature domain, and similarities among samples should be calculated. [37] flips labels for clusters of instances, which results in locally concentrated label noise. However, this approach doesn't utilize the similarities among instances; therefore, different from our proposed method. \n\nOne option is to train a deep network on the dataset first and then use it as the feature extractor. However, since the network extracts the features of data that it is trained on, it is prone to overfitting. Since we are especially interested in similarities among instances in feature space, it is desired that samples are sparsely distributed. On the contrary, in the case of overfitting, samples are gathered in a small region in feature space. \n\nTherefore, in this work we used the idea of knowledge distillation [16]. In the original work, the authors used distillation to transfer knowledge from the big teacher network to a much smaller student network without decreasing the performance. The idea is mainly motivated by learning from soft labels where the similarity of each instance to each class is emphasized by temperature hyperparameter. \n\nClass probabilities on softmax output, beyond the true class probability, are usually very low. But, compared with each other, some classes may have a much higher probability than others, and this carries important information about that data instance, which is also called as dark knowledge. By making probability distribution smoother, this relation is emphasized, as shown in Equation 2. \n\nInstead of being trained on hard labels, the student network is trained on the weighted sum of hard labels and soft labels produced by the teacher network. So, the loss function is defined as follows, \n\nwhere q i represents the soft labels produced by the teacher network using temperature T and y i represents the given label. \n\nWithin the context of this work, we are not interested in compressing the network to a smaller network. However, the idea of learning by emphasizing instance similarities can be used to find instances that have similar features with other classes. For that purpose, our student network is at the same size as the teacher network. Firstly, the teacher network is trained on the dataset. Secondly, soft labels are produced from the softmax output of the teacher for a given temperature T .",
            "score": 0.631431347108921,
            "section_title": "C. Feature-dependent noise",
            "char_start_offset": 9009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1280
                },
                {
                    "start": 1283,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1673
                },
                {
                    "start": 1676,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1876
                },
                {
                    "start": 1879,
                    "end": 2003
                },
                {
                    "start": 2006,
                    "end": 2109
                },
                {
                    "start": 2110,
                    "end": 2253
                },
                {
                    "start": 2254,
                    "end": 2335
                },
                {
                    "start": 2336,
                    "end": 2391
                },
                {
                    "start": 2392,
                    "end": 2493
                }
            ],
            "ref_mentions": [
                {
                    "start": 211,
                    "end": 215,
                    "matchedPaperCorpusId": "36832256"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73095703125
        },
        {
            "corpus_id": "265608780",
            "title": "EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything",
            "text": "Knowledge distillation (KD) is a technique to improve the performance of deep learning models without changing their architectures. [27] is a pioneering work to distill the dark knowledge from a larger teacher model to a smaller student model. The learning of a student model is supervised by the hard labels and the soft labels from a teacher model. This practice is followed by multiple works which aim to make better use of soft labels to transfer more knowledge. In [64], the distillation method decouples representation learning and classification. Decoupled knowledge distillation [70] separates",
            "score": 0.6314106387235068,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6933,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 601
                }
            ],
            "ref_mentions": [
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "235613564"
                },
                {
                    "start": 587,
                    "end": 591,
                    "matchedPaperCorpusId": "247476179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61279296875
        },
        {
            "corpus_id": "257102363",
            "title": "Detecting software vulnerabilities using Language Models",
            "text": "The technique of teacher-presented knowledge distillation involves training a student model using both soft and hard labels provided by the teacher. The soft labels, or targets, are calculated by applying the softmax function to the teacher's logits with a temperature parameter T. A higher T results in softer targets. In this study, we follow the suggestion of Lan, Zhu, and Gong ( 2018) and set T to 3 for all methods. To transfer knowledge from the teacher model to the student model, the predicted distribution q of the student model is aligned with the target distribution t, both calculated with a temperature of 3. The distillation loss function, which measures the difference between the two distributions, is represented by the Kullback-Leibler (KL) divergence equation (3). The total loss for training the model with both hard and soft labels is given by equation ( 4), where the distillation loss Ldis is multiplied by T2 before combination to maintain its contribution roughly unchanged. It's important to note that the probabilities predicted by the student model q are computed from the logits with a temperature of 1 when aligning with hard labels, and with a higher temperature when aligning with soft targets. Throughout the paper, the notation q represents the T=1 version and q represents the high-temperature version.",
            "score": 0.6298277046140688,
            "section_title": "IV. DISTILLATION WITH A TEACHER MODEL",
            "char_start_offset": 6230,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1338
                }
            ],
            "ref_mentions": [
                {
                    "start": 780,
                    "end": 783,
                    "matchedPaperCorpusId": "249062897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.345703125
        },
        {
            "corpus_id": "222096654",
            "title": "Class-Incremental Learning of Convolutional Neural Networks Based on Double Consolidation Mechanism",
            "text": "Knowledge distillation is not originally proposed for incremental learning, but it has been widely used in this domain. Knowledge distillation is a simple and effective method for model compression and model training, which was proposed by Hinton et al. [15] in 2015. In the context of deep learning, to achieve better prediction, over-parameterized deep neural networks are often constructed. Such kind of network has very strong learning ability but requires a large amount of memory and computing resources, which is very unfavorable for deployment. However, it is empirically difficult to train a small model from scratch to achieve the equivalent performance of a large model. Fortunately, by using the knowledge distillation method, knowledge can be migrated from a large network or network cluster to a smaller network for more efficient deployment. \n\nThe small model need to be trained is called the student network, and the large model that has been trained is called the teacher network. Our goal is to make the output of the student network sufficiently close to that of the teacher network, but there is a problem with doing this directly: the Softmax output of the teacher network is similar to a one-hot vector, with one value large and the others small. In this case, the output of the teacher network provides very limited supervision information. Compared with the ''hard'' output which is almost one-hot, we hope the output can be more ''soft,'' that is, the probability distribution can be more moderate. Knowledge distillation presents an effective method. Let's consider a generalized Softmax function: \n\nwhere z i is the output value of the model's ith output node before Softmax function performed, and T is the temperature parameter. It is easy to prove that q i converges to a one-hot vector as temperature T approaches 0, and becomes softer as T gets higher. \n\nThe knowledge distillation method assists the training of the student network by introducing the soft-target related to the teacher network as a part of the total loss, to realize knowledge transfer. The whole process of knowledge distillation is shown in Fig. 2. In Fig. 2, the predicted output of the left teacher network is transformed by generalized Softmax to obtain the soft probability distribution. The value is between 0 and 1, and the value distribution is relatively moderate.",
            "score": 0.6279198795043208,
            "section_title": "B. KNOWLEDGE DISTILLATION",
            "char_start_offset": 9329,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 856
                },
                {
                    "start": 859,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2293
                },
                {
                    "start": 2294,
                    "end": 2374
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53759765625
        },
        {
            "corpus_id": "214623002",
            "title": "Dynamic Hierarchical Mimicking Towards Consistent Optimization Objectives",
            "text": "Our knowledge matching loss is partially inspired by the line of Knowledge Transfer (KT) research but we shift its primary focus away from model compression in the conventional KT methods. The representative Dark Knowledge Distillation [9] requires a large teacher model to aid the optimization process of a small student model via offering informative hint in the form of probabilistic prediction output as the soft label. In this framework, aiming at easing the optimization difficulty of small networks, an available strong model is required beforehand. In contrast, we concentrate on developing deeply supervised training scheme and further boosting the optimization process of state-ofthe-art CNNs instead of compact models. Moreover, unlike the teacher and student in the distillation procedure which are optimized sequentially without straightforward association during their separate training process, our training strategy drives all auxiliary branch classifiers together with the original classifier to be optimized simultaneously with a knowledge matching loss among them computed in an on-the-fly manner. Knowledge transfer process occurs in a more compact way within our proposed mechanism, which enables knowledge sharing across hierarchical layers in one single network, without the demand of an extra teacher model. Thus our knowledge integration learning scheme is ready to be deployed in the optimization process of any convolutional neural networks, both lightweight networks and heavy ones.",
            "score": 0.6273377801768842,
            "section_title": "G. Comparison to Knowledge Transfer Research",
            "char_start_offset": 40604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1510
                }
            ],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6806640625
        },
        {
            "corpus_id": "257233096",
            "title": "Graph-based Knowledge Distillation: A survey and experimental evaluation",
            "text": "Knowledge distillation [12] was originally proposed for model compression. Unlike pruning and quantification in model compression, knowledge distillation (KD) uses the T-S framework to pre-train a large teacher model to distill to obtain a lightweight student model, enhancing the generalization ability of the student model and achieving better performance and higher precision. Through distillation, \"knowledge\" (soft label supervised information) in the teacher model is transferred to the student model. In this way, student models can reduce the complexity of time and space, which can also learn soft label information (containing inter-category information) that is not available in the one-hot label without losing the quality of the prediction. Generally, KD can be divided into two technical routes in accordance with the different ways of knowledge transfer. \n\nThe first is response-based distillation, which is closely related to label smoothing [111], using the output probability of the teacher model as smoothing labels to train students. [12] is the pioneering work of knowledge distillation, proposed by Hinton in 2015, which was first proposed to transfer the output probability by the softmax layer of the teacher model to the student model as a \"softtarget\" to improve the performance of the student model. To learn the feedback information in the student network, DML [112] proposes the strategy of deep mutual learning, allowing a group of students to train simultaneously on the network and realize mutual learning and progress through the supervision of real labels and the learning experience of peer network output results. BAN [113] uses an integrated approach to train the student model so that its network structure is the same as that of the teacher model, which significantly outperforms the teacher model in computer vision and language modeling downstream tasks. \n\nAnother type of knowledge distillation is the featurebased distillation method, in which the semantic information contained in the middle layer feature representation in the teacher network structure as knowledge transfer to the student model. FitNet [114] is the first classical work to adopt this method, leveraging the output of the teacher network and the feature embedding of the middle layer as supervision information to extend KD and realize the problem of deep model network compression.",
            "score": 0.6262242377292537,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 11291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 869
                },
                {
                    "start": 872,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1895
                },
                {
                    "start": 1898,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2394
                }
            ],
            "ref_mentions": [
                {
                    "start": 958,
                    "end": 963,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 1389,
                    "end": 1394,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1654,
                    "end": 1659,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67919921875
        },
        {
            "corpus_id": "247447425",
            "title": "CEKD:Cross ensemble knowledge distillation for augmented fine-grained data",
            "text": "Knowledge distillation is to optimize student network under the guidance of teacher network. The loss function uses the soften output of the teacher network and the student network to calculate the KL divergence. The formulation is as follows:\n\nwhere, n is the batch size, T is the temperature, p and q represent for the soften probability distribution given by teacher and student network. Note the output of teacher and student network as logits t and logits s. Then the soft target p = sof tmax (t/T ) and q = sof tmax (s/T ).",
            "score": 0.6253209032400113,
            "section_title": "Knowledge distillation",
            "char_start_offset": 10394,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2197265625
        },
        {
            "corpus_id": "249712477",
            "title": "Multi scale Feature Extraction and Fusion for Online Knowledge Distillation",
            "text": "The key idea of knowledge distillation is that soft targets contain the dark knowledge which can be used as a supervisor to transfer knowledge to the student model. Given a labeled dataset D{x i , y i } N i=1 , with N samples, x i is the ith input sample and y i \u2208 {1, 2, ..., M } is the corresponding ground-truth label. M is the total number of classes in the dataset. Consider n student models {S j } n j=1 , the logit produced by the last fully connected layer of the student S j is denoted as z j = {z 1 j , z 2 j , ..., z M j }. Then the probability of the jth student for the sample x i over the mth class p m j (x i ) can be estimated by a softmax function, where T is the temperature which produces a more softened probability distribution as it increases. Specifically, when T = 1, it is defined as the original softmax output, we consider writing it as p m j (x i ); otherwise it is rewritten as pm j (x i ). For multi-class classification, the objective is to minimize the cross-entropy loss between the softmax outputs and the ground-truth labels, \n\nwhere l i = 1 if y i = m, and 0 otherwise. Knowledge transfer is facilitated by matching the softened probability of the student model pm j (x i ) and the teacher model pm t (x i ). We introduce the distillation loss of j \u2212 th student model in the form of Kullback-Leibler Divergence \n\n. \n\n(3)",
            "score": 0.6250724919761025,
            "section_title": "Problem Definition",
            "char_start_offset": 8018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1060
                },
                {
                    "start": 1063,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1356
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44580078125
        },
        {
            "corpus_id": "275993890",
            "title": "Mini-ResEmoteNet: Leveraging Knowledge Distillation for Human-Centered Design",
            "text": "Employing ResEmoteNet as a teacher model within the framework of knowledge distillation, the methodology prioritizes the extraction of hard labels and soft predictions from the teacher model, culminating in a composite of distillation loss functions. The input data is subsequently propagated through the student model for processing. Fig. 2 illustrates the proposed method. \n\nIn the mechanism of Soft Target Distillation, temperature scaling is implemented via the temperature hyper-parameter T to smooth the probability distributions, thereby enhancing the efficacy of knowledge distillation. This smoothing is achieved by dividing the logits by T before the application of the softmax function. The teacher model's softened logits convey comprehensive information regarding class relationships, thereby simplifying the learning process to a certain degree. The use of the KL divergence loss facilitates the transfer of knowledge between the teacher and student soft predictions, seemingly enabling accelerated training. Where  , is the \"softened\" probability distribution of the teacher model and  , is the \"softened\" probability distribution of the student model. In the context of Hard Label Learning, the Cross-entropy loss is employed to guarantee that the student model sustains its accuracy concerning the primary task while preserving direct supervision through ground truth labels; this loss can be calculated between the predictions of the student model and the ground truth labels. \n\nWhere C is the number of classes,  , is the ground truth label for class c (1 for the correct class, 0 otherwise), ` , is the predicted probability (softmax output) for class c for sample i. Then the total loss function is a weighted combination of two components:  =  +  \u00d7  2 \u00d7  (6) Where \u03b1 is the weight for hard label loss, \u03b2 is the weight for distillation loss, T is the temperature parameter, CrossEntropyLoss is Cross-entropy loss with hard labels and KLDivLoss is KL divergence loss between softened predictions.",
            "score": 0.6246741532172957,
            "section_title": "B. Our Knowledge Distillation Method:",
            "char_start_offset": 6372,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1494
                },
                {
                    "start": 1497,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 2016
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6611328125
        },
        {
            "corpus_id": "219687782",
            "title": "Multi-Fidelity Neural Architecture Search With Knowledge Distillation",
            "text": "The knowledge distillation (KD) assumes two models: a teacher and a student. The teacher is typically a large and accurate network or an ensemble. The student is trained to fit the softmax outputs of the teacher together with ground truth labels. The idea is that outputs of the teacher capture not only the information provided by ground truth labels but also the probabilities of other classes -\"dark knowledge\". The knowledge distillation can be summarized as follows. \n\nLet z i be logits (pre-softmax activations) and q i -probabilities of classes as predicted by a neural network. Knowledge distillation smooths z i with the temperature \u03c4 \n\nNeural networks often do very confident predictions (close to 0 or 1) and smoothing helps to provide for student more information during training [11]. The KD loss is a linear combination of the logistic loss and cross-entropy between predictions of the teacher and the student \n\nwhere z T i , z S i are logits of the teacher and the student, H(p, q) = \u2212p log(q) is the cross-entropy function. The factor \u03c4 2 is used for scaling gradients of both parts of the loss function to be the same order. In the rest of the paper, we will refer to this variant of the knowledge distillation as \"original KD\". \n\nOther variants of KD suggest matching feature maps of the student and the teacher with various discrepancy functions [33,45,39,30,2,28,12,38]. For example, the NST loss [12] uses Maximum Mean Discrepancy (MMD): \n\nwhere F T , F S are the feature maps of the teacher and the student, \n\nHere f i\u2022 T ,f j\u2022 S are feature map from the layers i, j of the teacher and the student respectively, k(x, y) is a kernel.",
            "score": 0.6242708957028862,
            "section_title": "Knowledge Distillation (KD)",
            "char_start_offset": 6640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1245
                },
                {
                    "start": 1248,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1529
                },
                {
                    "start": 1532,
                    "end": 1654
                }
            ],
            "ref_mentions": [
                {
                    "start": 1372,
                    "end": 1375,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1375,
                    "end": 1378,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1380,
                    "end": 1383,
                    "matchedPaperCorpusId": "52012952"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7265625
        },
        {
            "corpus_id": "260866171",
            "title": "Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features",
            "text": "Knowledge distillation [29] is a method for transferring knowledge from a teacher model to a student model. The student uses the predicted probabilities from the teacher's output as a soft target in place of the usual hard target (ground truth labels) when calculating the loss between predicted and target outputs. Knowledge is distilled by the student model through gradient descent backpropagation of the distillation loss between the predictions and soft teacher targets. The student thus learns a mapping from its inputs to all the probable label outputs as learned by the teacher, enabling it to quickly form a rich representation of the teacher's knowledge. The distillation loss is a modified cross-entropy loss whereby the teacher model's prediction output is modified by a temperature T to attain the soft prediction output y so f t given by ( 1), where k is the number of classes and z \u2208 R k is a vector of logits (the output prior to the softmax activation layer). As seen in Figure 2, higher temperatures smooth the probability distribution of the soft outputs, giving increased weight to smaller probabilities and decreased weight to the highest probability, whilst retaining the probabilities' relative ranking order. \n\n(2) demonstrates the categorical cross-entropy loss L used during standard model training, whilst (3) is the distillation loss L dist used for knowledge distillation, whereby y is the true label (one-hot encoded), \u0177 is the prediction output, and \u0177t\u2212so f t and \u0177so f t are the soft teacher prediction and soft student prediction outputs, respectively. \n\nKnowledge distillation can reduce the effects of catastrophic forgetting in continual learning by reinforcing a model's knowledge of known classes when learning new classes. [13] uses knowledge distillation with continual learning for FER through an indicator loss function which is the weighted sum of the distillation loss and hard loss. An indicator function allows the model to treat each new training example differently depending on whether it is reinforcing prior knowledge or gaining new knowledge. For new classes, the hard loss is weighted more and for old classes the distillation loss is weighted more. FER accuracy with this method is reduced significantly less with each new class learned, compared to other methods. [13,28,34].",
            "score": 0.6238717935454294,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1232
                },
                {
                    "start": 1235,
                    "end": 1585
                },
                {
                    "start": 1588,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2318
                },
                {
                    "start": 2319,
                    "end": 2330
                }
            ],
            "ref_mentions": [
                {
                    "start": 1762,
                    "end": 1766,
                    "matchedPaperCorpusId": "244334333"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52490234375
        },
        {
            "corpus_id": "232232785",
            "title": "BreakingBED - Breaking Binary and Efficient Deep Neural Networks by Adversarial Attacks",
            "text": "Knowledge distillation (KD) is the transfer of knowledge from a teacher to a student network [40,21]. The student can be a smaller DNN, which is trained on the soft labels of the larger teacher network, achieving an improvement in an accuracy-efficiency trade-off. The student represents a compressed version of the teacher, condensing its knowledge. This paper focuses on KD training, using Kullback-Leibler (KL) divergence between the teacher and the student output distribution formulated as the loss function in Eq. ( 1). Here, \u03c3(f t (I)) and \u03c3(f s (I)) represent the softmax output logits of the teacher and student network respectively, computed for a sample image I in a mini batch of N samples. \n\nDuring the knowledge transfer using the teacher's logits, a softmax temperature T 1 is used. During the evaluation, we use T = 1 to obtain softmax-cross entropy loss.",
            "score": 0.6237322836074897,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 2441,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 871
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 97,
                    "matchedPaperCorpusId": "204838340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47119140625
        },
        {
            "corpus_id": "270562086",
            "title": "Mutual Learning for Finetuning Click-Through Rate Prediction Models",
            "text": "Knowledge distillation framework consists of training a complex teacher network and distilling the knowledge learned by the teacher model into the light student network.In the Figure 1, the training scheme for knowledge distillation is showed.Firstly, heavy teacher model trained on the CTR dataset and the training loss is defined between the model prediction and ground-truth labels.After that, the student model is trained with two loss functions that are distillation loss resulted from the soft label predictions of the teacher and student models and also the student loss between the student model predictions and hard labels.",
            "score": 0.623402532854897,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5584,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 243
                },
                {
                    "start": 243,
                    "end": 385
                },
                {
                    "start": 385,
                    "end": 632
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.366943359375
        },
        {
            "corpus_id": "233777216",
            "title": "On-Device Learning Systems for Edge Intelligence: A Software and Hardware Synergy Perspective",
            "text": "We use Fig. 4 to better illustrate the workflow of knowledge distillation with these two kinds of loss functions. The student loss captures the distance between predicted class and the groundtruth labels marked in the dataset, under the configuration of T = 1. Meanwhile, the distillation loss measures the difference between student's loss and the teacher's loss, which can be used to reflect the dark knowledge learned from the teacher and the temperature inside is T = t. \n\n2) Tuning of Hyper-parameters: It is worth noting that the configuration of the hyper-parameters of \u03b1, \u03b2 and t is an errorand-trial procedure, which significantly impacts the distillation efficiency. In the implementation of existing methods [84,85], the temperature value t is often set as t \u2208 [1,20] . Empirically, a lower temperature makes sense when the student model is much smaller over the size of teacher model. The core reason of this setting is that a higher value of the temperature will bring too much information from the distillation loss, which may easily exceed the learning capacity of the student model [86]. However, as the true learning capacity of a student model is hard to measure in prior, the setting of temperature t still requires future research to obtain a more intelligent and efficient controlling. Meanwhile, the values of \u03b1 and \u03b2 also require careful setting because these hyper-parameters reflect the learning weight between the student loss from the ground-truth labels and the distillation loss from the teacher's experience. A general setting is to make \u03b1 + \u03b2 = 1 and \u03b1 is often smaller than \u03b2 [28]. Some recent researchers also focus on how to adjust these hyper-parameters in a more flexible manner [87]. \n\n3) Usage of Model Training: On top of the tuning of loss function and coefficient hyper-parameters, we can easily generate a light-weight student model that holds similar performance of the complex teacher model. More precisely, at the beginning of each iteration, the data samples will input to both student and teacher models. After the forwarding propagation to calculate the output prediction, the student model can adjust its loss function by introducing the teacher's loss. This pattern ensures that the student can learn from ground-truth labels and teacher's experience.",
            "score": 0.6222927843735151,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 31646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 474
                },
                {
                    "start": 477,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2301
                }
            ],
            "ref_mentions": [
                {
                    "start": 719,
                    "end": 723,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 772,
                    "end": 775,
                    "matchedPaperCorpusId": "206594738"
                },
                {
                    "start": 775,
                    "end": 778,
                    "matchedPaperCorpusId": "52920837"
                },
                {
                    "start": 1098,
                    "end": 1102,
                    "matchedPaperCorpusId": "829159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6123046875
        },
        {
            "corpus_id": "203593636",
            "title": "Training convolutional neural networks with cheap convolutions and online distillation",
            "text": "By replacing the standard convolution with several cheap convolutions, a variety of CNN models can be compressed and accelerated substantially. It is simple and straightforward to train the compressed models with cheap convolutions from scratch to improve the accuracy. By doing this, it however leads to a limited improvement on accuracy, which is due to a limited knowledge used only by the ground-truth labels. \n\nAlternatively, knowledge distillation [30,32] is becoming a promising solution, which aims to transfer more knowledge from a teacher network to a student network to boost the accuracy of the student network. For this paper, we first review two different distillation methods for learning a smaller student network from a large, pre-trained teacher network: dark knowledge (DK) [30] and attention transfer (AT) [32]. In that case, we can select a CNN model with the standard convolution as a teacher, while a model with cheap convolution by keeping the teacher's architecture is regarded as a student. Then, we propose our online distillation (OD) method to replace the pretrained teacher network by constructing online from the multiple student networks with the same architecture, and train both teacher and student networks in a one-shot manner. \n\nDark Knowledge. Let t and s be a teacher network and a student network with the final output features Z L (t) and Z L (s) , respectively. a (t) and a (s) are the logits of teacher and student networks, which can be computed respectively by: \n\nGiven an input image x, the probabilistic class posterior of teacher and student network p(c|x, K (t) ) and p(c|x, K (s) ) over a class c can be computed respectively as: \n\nwhere K (t) and K (s) are the parameters in the teacher and student networks. \n\nTo perform dark knowledge, we train the student network to minimize the following loss function: \n\nwhere \u03c4 is a temperature parameter to soften the distribution of predictions. Different from fine-tuning just by class labels, dark knowledge is to learn the student network from the soft outputs of teacher network and ground-truth labels. \n\nAttention transfer.",
            "score": 0.6210430143490889,
            "section_title": "Online Distillation",
            "char_start_offset": 18581,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1679
                },
                {
                    "start": 1682,
                    "end": 1759
                },
                {
                    "start": 1762,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2100
                },
                {
                    "start": 2103,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 458,
                    "end": 461,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 826,
                    "end": 830,
                    "matchedPaperCorpusId": "829159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76220703125
        },
        {
            "corpus_id": "247476179",
            "title": "Decoupled Knowledge Distillation",
            "text": "The concept of knowledge distillation (KD) was firstly proposed by Hinton et al. in [12]. KD defines a learning manner where a bigger teacher network is employed to guide the training of a smaller student network for many tasks [12,17,18]. The \"dark knowledge\" is transferred to students via soft labels from teachers. For raising the attention on negative logits, the hyper-parameter temperature was introduced. The following works can be divided into two types, distillation from logits [3,6,22,40,44] and intermediate features [10,11,14,15,23,25,28,33,34,41,43].\n\nPrevious works of logit distillation mainly focus on proposing effective regularization and optimization methods rather than novel methods. DML [44] proposes a mutual learning manner to train students and teachers simultaneously. TAKD [22] introduces an intermediate-sized network named \"teacher assistant\" to bridge the gap between teachers and students. Besides, several works also focus on interpreting the classical KD method [2,26].\n\nState-of-the-art methods are mainly based on intermediate features, which can directly transfer representations from the teacher to the student [10,11,28] or transfer the correlation between samples captured in the teacher to the student [23,33,34]. Most of the feature-based methods could achieve preferable performances (significant higher than logits-based methods), yet involving considerably high computational and storage costs.\n\nThis paper focuses on analyzing what limits the potential of logits-based methods and revitalizing logit distillation.",
            "score": 0.6210342223708385,
            "section_title": "Related work",
            "char_start_offset": 6277,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 235,
                    "matchedPaperCorpusId": "267213"
                },
                {
                    "start": 235,
                    "end": 238,
                    "matchedPaperCorpusId": "236912875"
                },
                {
                    "start": 489,
                    "end": 492,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 492,
                    "end": 494,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 494,
                    "end": 497,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 497,
                    "end": 500,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 500,
                    "end": 503,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 534,
                    "end": 537,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 540,
                    "end": 543,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 543,
                    "end": 546,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 546,
                    "end": 549,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 552,
                    "end": 555,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 555,
                    "end": 558,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 558,
                    "end": 561,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 711,
                    "end": 715,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 802,
                    "end": 806,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 997,
                    "end": 1000,
                    "matchedPaperCorpusId": "212633769"
                },
                {
                    "start": 1000,
                    "end": 1003,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 1150,
                    "end": 1154,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1154,
                    "end": 1157,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1244,
                    "end": 1248,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1248,
                    "end": 1251,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1251,
                    "end": 1254,
                    "matchedPaperCorpusId": "198179476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6513671875
        },
        {
            "corpus_id": "245537111",
            "title": "Online Adversarial Knowledge Distillation for Graph Neural Networks",
            "text": "In this subsection, we review the concepts about knowledge distillation briefly. Knowledge distillation is generally composed by two neural networks with distinct architectures: A teacher network with larger parameters and a student network with fewer parameters. The goal of knowledge distillation is to transfer meaningful knowledge of teacher network to enhance to generalization performance of student network. To achieve knowledge transfer, Classic knowledge distillation [14] encouraged the alignment of softened outputs of teacher and student. Specifically, besides standard cross entropy loss between predictions p s and ground truth label y, the student network is also trained to minimize the Kullback-Leibler (KL) divergence between student predictions p s and teacher predictions p t after softened transformation: \n\nwhere T is a hyper-parameter known as temperature. The higher T leads to more significant softened effect. Besides output logits, the teacher intermediate layers also encoded highly representative knowledge to the given dataset, which may be potential to boost the student performance further. Many works has been proposed to match the raw or transformed hidden layers. For example, Fitnet [25] randomly selects one hidden layer from both teacher network and student network and minimize their mean square error (MSE) loss: \n\nwhere r(\u2022) is a dimension transformation operator such as Multi-Layer Perception.",
            "score": 0.6207925813291071,
            "section_title": "A. Knowledge Distillation",
            "char_start_offset": 10158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1436
                }
            ],
            "ref_mentions": [
                {
                    "start": 1219,
                    "end": 1223,
                    "matchedPaperCorpusId": "2723173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.389892578125
        },
        {
            "corpus_id": "273186993",
            "title": "Gap Preserving Distillation by Building Bidirectional Mappings with A Dynamic Teacher",
            "text": "Knowledge distillation. Knowledge distillation (KD) (Hinton et al., 2015) transfers knowledge from a teacher to a smaller student model. Methods improve this by focusing on logits or intermediate features (Sun et al., 2024;Jin et al., 2023;Zhao et al., 2022a;Li et al., 2023;Passalis et al., 2021;Tian et al., 2019;Zagoruyko & Komodakis, 2017a;Heo et al., 2019a;Chen et al., 2021b;Heo et al., 2019b;Kim et al., 2018). Standard methods prioritize fully converged teachers with high performance, yet the performance gap can hinder knowledge transfer (Wang et al., 2022;Gao et al., 2020;Cho & Hariharan, 2019;Yuan et al., 2019). Strategies to address this include using intermediatestage teachers (Cho & Hariharan, 2019;Zhao et al., 2022b), pre-training student-friendly teacher model (Yang et al., 2019a;Park et al., 2021;Dong et al., 2024), introducing intermediate-sized assistant teachers (Mirzadeh et al., 2020;Son et al., 2021) or introducing auxiliary networks (Gao et al., 2021). These methods often rely on specially designed and pre-trained intermediate models. Feature-based methods like DTSKD (Li et al., 2024) and DiffKD (Huang et al., 2023) focus on bridging semantic gaps or denoising features. SCKD (Zhu & Wang, 2021) optimizes transfer using gradient similarity. Recent works refine soft labels (Yuan et al., 2024;Rao et al., 2023) or student's output entropy (Zhu et al., 2024a) to enhance knowledge transfer. In contrast, our GPD constructs a trainable dynamic teacher based on the student model, maintaining an appropriate accuracy gap throughout distillation for effective knowledge transfer. Reparameterization.",
            "score": 0.6190840383786941,
            "section_title": "RELATED WORK",
            "char_start_offset": 6141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1630
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 223,
                    "matchedPaperCorpusId": "268247468"
                },
                {
                    "start": 223,
                    "end": 240,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 240,
                    "end": 259,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 259,
                    "end": 275,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 275,
                    "end": 297,
                    "matchedPaperCorpusId": "219169868"
                },
                {
                    "start": 315,
                    "end": 344,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 344,
                    "end": 362,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 362,
                    "end": 381,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 381,
                    "end": 399,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 399,
                    "end": 416,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 548,
                    "end": 567,
                    "matchedPaperCorpusId": "252846591"
                },
                {
                    "start": 584,
                    "end": 606,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 694,
                    "end": 717,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 717,
                    "end": 736,
                    "matchedPaperCorpusId": "198179767"
                },
                {
                    "start": 782,
                    "end": 802,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 802,
                    "end": 820,
                    "matchedPaperCorpusId": "231925118"
                },
                {
                    "start": 820,
                    "end": 838,
                    "matchedPaperCorpusId": "249642077"
                },
                {
                    "start": 890,
                    "end": 913,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 913,
                    "end": 930,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 965,
                    "end": 983,
                    "matchedPaperCorpusId": "229400079"
                },
                {
                    "start": 1102,
                    "end": 1119,
                    "matchedPaperCorpusId": "268439162"
                },
                {
                    "start": 1131,
                    "end": 1151,
                    "matchedPaperCorpusId": "258888057"
                },
                {
                    "start": 1212,
                    "end": 1230,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 1328,
                    "end": 1345,
                    "matchedPaperCorpusId": "249209818"
                },
                {
                    "start": 1374,
                    "end": 1393,
                    "matchedPaperCorpusId": "258564799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.278564453125
        },
        {
            "corpus_id": "267413204",
            "title": "Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate",
            "text": "Knowledge distillation (KD) is a technique that transfers knowledge from a teacher model to a student model by training the student to imitate the teacher's outputs [36]. This approach is widely applied in areas such as model compression, transparency, and interpretability [8,10,27,36,60,91]. Model compression is often motivated by resource constraints. Pioneering works include Bucilu\u01ce et al. [10], which compresses ensemble neural networks into a single network, and Ba and Caruana [3], which improves shallow neural network accuracy by mimicking deep networks. KD is also applied in various domains, including deep reinforcement learning [80], continual learning [28,59,85], and learning privileged information theory [62,76]. The dark knowledge method [36] further develops KD, where a student model aims to fully match the output distribution of the teacher. Intuitively, distillation is effective because the teacher's output distribution over classes provides a more informative training signal than a one-hot label. Additionally, in born-again networks (BAN) [29], the teacher and student have identical neural architecture and model sizes, but the student can surprisingly surpass the teacher's accuracy.",
            "score": 0.619014364989352,
            "section_title": "B.2.1 Knowledge Distillation",
            "char_start_offset": 29576,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1215
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 277,
                    "end": 280,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 280,
                    "end": 283,
                    "matchedPaperCorpusId": "3976789"
                },
                {
                    "start": 283,
                    "end": 286,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "21713934"
                },
                {
                    "start": 396,
                    "end": 400,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 672,
                    "end": 675,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 727,
                    "end": 730,
                    "matchedPaperCorpusId": "41866457"
                },
                {
                    "start": 758,
                    "end": 762,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1069,
                    "end": 1073,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5556640625
        },
        {
            "corpus_id": "269664643",
            "title": "Few-Shot Federated Learning: A Federated Learning Model for Small-Sample Scenarios",
            "text": "The concept of knowledge distillation was first introduced by Hinton et al. in [22], comprising a system of teacher and student models [23][24][25][26]. The so-called teacher model refers to a large, structurally complex, and redundantly trained network that acts as the \"teacher\" during training, instructing a simpler, under-trained \"student\" model to achieve a performance as close as possible to that of the teacher model. This process is realized through the use of soft and hard labels along with corresponding loss functions. Specifically, it begins by calculating the soft output of the teacher model, which is the raw prediction values, or logits, from the last fully connected layer of the neural network-values that have not been activated by the softmax function. Using the probability distribution after applying the softmax function would limit the knowledge transferred to the student model and might even have a negative effect by amplifying errors. These prediction values represent the teacher model's confidence level for each class. \n\nHere, z T i denotes the logit output of the teacher model for the ith class, and T is the temperature parameter that controls the smoothness of the output probability distribution. It is evident that a smaller T value, which brings the function closer to the softmax function, makes q i more similar to the probability distribution after activation. Conversely, a larger T value results in a smoother probability distribution curve from the softmax output, allowing the student model to acquire knowledge with richer information entropy. \n\nAfter obtaining the logits from the teacher model, the student model's logits are calculated in a similar manner. Here, z S i represents the logit output of the student model for the ith class. \n\nSubsequently, a loss function is employed to enable the student model to absorb and compress the knowledge from the teacher model. \n\nHere, Q i and P i represent the probability distributions of the teacher and student models, respectively. The difference between these probability distributions is calculated using the Kullback-Leibler divergence, with the temperature parameter used to adjust it, ensuring that the gradients do not become too small as the temperature value increases. \n\nThroughout the knowledge distillation process, in addition to computing the soft target loss, it is also necessary to calculate the hard target loss.",
            "score": 0.6187907735251861,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9401,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1592
                },
                {
                    "start": 1595,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1788
                },
                {
                    "start": 1791,
                    "end": 1921
                },
                {
                    "start": 1924,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2276
                },
                {
                    "start": 2279,
                    "end": 2428
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 83,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 135,
                    "end": 139,
                    "matchedPaperCorpusId": "235694435"
                },
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "247779043"
                },
                {
                    "start": 147,
                    "end": 151,
                    "matchedPaperCorpusId": "266620794"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68310546875
        },
        {
            "corpus_id": "267740612",
            "title": "FedD2S: Personalized Data-Free Federated Knowledge Distillation",
            "text": "KD is referred to any method of transferring knowledge from one or multiple teacher models into a student model [24]. This process levearges a public dataset, denoted by D p , to align the mapping functions of teacher and student models. . Specifically, the logit outputs of the teacher model, when applied to the public dataset, are passed through a softmax function to generate soft labels. Along with the ground-truth outputs, which are used to train the student model in a conventional way, soft labels are utilized as regularizers to constrain the loss of the student model. Typically, a Kullback-Leibler divergence function is employed to minimize the discrepancy between the soft labels of the teacher model and the predictions made by the student model, as follows [24]: \n\nwhere F s (\u2022; \u03b8 s ) and F t (\u2022; \u03b8 t ) are the student and teacher models, parameterized by \u03b8 s and \u03b8 t , respectively. In addition, L KL is the per-sample Kullback-Leibler loss function, and \u03c4 is the so-called temperature hyper-parameter used to soften generated logits. Notably, throughout this paper, we assume that a soft-max function with temperature \u03c4 is employed in the output layer of models. \n\nIn FKD, depending on the method, both clients and the server can play the role of either the teacher or the student. In [25], clients first update their local models using their respective local datasets, then, each client generates a set of soft labels by making predictions on a shared public dataset. On the server side, these local soft labels are averaged to create global soft labels, which represent global knowledge. Finally, global knowledge is utilized in Eq. ( 5) to execute knowledge distillation. This allows for the transfer of knowledge from other clients to each individual client. \n\nThe performance of knowledge sharing in FKD methods depends significantly on the distribution of the public dataset. However, in real-world scenarios, accessing a public dataset that accurately represents the entire distribution of local datasets is often impractical. Therefore, it is necessary to develop methods that facilitate the exchange of knowledge between clients and the server without the need for a shared dataset.",
            "score": 0.6187654021057187,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 9430,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1780
                },
                {
                    "start": 1783,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2209
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.492431640625
        },
        {
            "corpus_id": "233210081",
            "title": "Dual discriminator adversarial distillation for data-free model compression",
            "text": "Caruana et al. [41] first confirm that one ensemble of networks could transfer the knowledge to the single network. Then Ba et al. [42] propose to teach the student network by penalizing the difference of logits between the teacher and student. Later, the concept of Knowledge Distillation (KD) is introduced by Hinton et al. [24] to solve model compression problems. It transfers the dark knowledge from the pre-trained teacher network to the compact student network by mimicking the class probabilities outputs, which are softened by setting a temperature hyperparameter in softmax. Then the performance of compact student can be retained and close to the performance of the teacher. Note that, KD requires original training data to capture the valuable knowledge from the teacher network. However, the knowledge contained in the soft-labels is insufficient when the teacher network goes deeper. To tackle this issue, some improvements have been made, which extend KD by utilizing the intermediate representation as supervision. For example, Fitnets [27] forces the student to learn the similar intermediate features as teacher's which are defined as hints. Zagoruyko et al. [28] define the attention maps from the intermediate features as the knowledge and then obtain a better performance compared to the one using original feature itself. Moreover, FSP [29] designs the flow distillation loss to force the student to mimic flow matrices of teacher among the feature maps between two layers. RKD [30] transfers mutual relations of data examples by the distance-wise and angle-wise distillation losses. SP [31] preserves the pairwise similarities in student's representation space instead to mimic the representation space of the teacher. CTKD [43] combines the knowledge from different teacher models to improve the student's performance in KD. Due to excellent performance, knowledge distillation has been used to solve a variety of complex applications such as object detection [26,44], semantic segmentation [45], lane detection [46], face recognition [47][48][49] and action recognition [50]. Nevertheless, the above traditional data-driven knowledge distillation methods need full of original training data, which are difficult to be obtained in real world. Thus, several few-shot knowledge distillation approaches are proposed to di",
            "score": 0.6186971049395273,
            "section_title": "Data-driven knowledge distillation",
            "char_start_offset": 7759,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 19,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 131,
                    "end": 135,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 326,
                    "end": 330,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1052,
                    "end": 1056,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1177,
                    "end": 1181,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1358,
                    "end": 1362,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1500,
                    "end": 1504,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1609,
                    "end": 1613,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1747,
                    "end": 1751,
                    "matchedPaperCorpusId": "198179767"
                },
                {
                    "start": 1988,
                    "end": 1991,
                    "matchedPaperCorpusId": "201666186"
                },
                {
                    "start": 2015,
                    "end": 2019,
                    "matchedPaperCorpusId": "73729180"
                },
                {
                    "start": 2036,
                    "end": 2040,
                    "matchedPaperCorpusId": "199405591"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.751953125
        },
        {
            "corpus_id": "260334695",
            "title": "BearingPGA-Net: A Lightweight and Deployable Bearing Fault Diagnosis Network via Decoupled Knowledge Distillation and FPGA Acceleration",
            "text": "I. 2) Decoupled knowledge distillation. Despite numerous forms of knowledge distillation, our method adopts the response-based KD, which utilizes the teacher model's logits for knowledge transfer. This is because the limited hardware resource in a low-cost FPGA cannot hold two or more convolutional layers. Then, there are no hidden features for featurebased KD to distill. \n\nIn classical knowledge distillation, soft labels, which are the logits produced by the teacher, are deemed as distilled knowledge [27]. Soft labels are obtained by the softmax function converting the logits z i of a neural network into the probability p i of the i-th class: \n\nwhere C represents the number of classes, and T \u2208 R + serves as the temperature factor. When the temperature T is higher, the probability distribution over classes becomes smoother. A lower value of T (where T < 1) sharpens the output, increasing the disparity in probability values of different classes. \n\nFor classification, the cross-entropy (CE) loss is adopted to measure the difference between the probability of the predicted label p and the ground truth y: \n\nand KL-Divergence measures the similarity between the probability labels of the teacher p T and the student p S , \n\nKD combines two loss functions: \n\n) where \u03b1 is the scale factor to reconcile the weights of two loss functions, and T 2 keeps two loss functions at the same level of magnitude. Combining L KL and L KD helps the student get the guidance of the teacher and the feedback from the ground truth. This ensures that the student model learns effectively and reduces the risk of being misled. \n\nHowever, L KL implies that all logits from the teacher are transferred to the student on an equal booting. Intuitively, student models should have the ability to filter the knowledge they receive. In other words, knowledge relevant to the current target category should be reinforced, while knowledge far from the target category should be attenuated. The coupling in classical KD harms the effectiveness and flexibility across various tasks. To address this, decoupled knowledge distillation, which factorizes L KL into a weighted sum of two terms (the target class and non-target class), was proposed [31].",
            "score": 0.6186589829893254,
            "section_title": "III. METHOD",
            "char_start_offset": 11609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1118
                },
                {
                    "start": 1121,
                    "end": 1234
                },
                {
                    "start": 1237,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2231
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.453369140625
        },
        {
            "corpus_id": "237091534",
            "title": "Online Multi-Granularity Distillation for GAN Compression",
            "text": "Knowledge Distillation (KD) [19] is a fundamental compression technique, where a smaller student model is optimized under the effective information transfer and supervision of a larger teacher model or ensembles [6]. Hinton [19] performed knowledge distillation via minimizing the distance between the output distribution statistics between student and teacher network. In this way, the student network attempts to learn dark knowledge [19] that contains the similarities between different classes, which can not be provided by the ground truth labels. Romero et al. [41] further took advantage of the concepts of feature maps from the intermediate layers to enhance the performance of the student network. Zhou et al. [65] presented that each channel of the feature map corresponds to a visual pattern, so they focus on transferring attention concepts [53,54,55] of feature map from each channel in intermediate layers. \n\nMoreover, You et al. [59] revealed that multiple teacher networks can provide more comprehensive knowledge for learning a more effective student network. MEAL [44] compressed large and complex trained ensembles into a sin-gle network, which employs an adversarial-based learning strategy to guide the pre-defined student network to transfer knowledge from teacher models. Offline knowledge distillation requires a pre-trained teacher model in the stage of optimizing, while online KD simultaneously optimizes the teacher and student network or just a group of student peers [51]. Anil et al. [2] trained two networks with the identical architecture parallelly and these two networks play the role of student and teacher iteratively. In this paper, we employ the multi-granularity based online distillation scheme, which aims to learn an effective student model from complementary structure of the teacher generators and the knowledge from diverse layers.",
            "score": 0.6183796252797836,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1877
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 224,
                    "end": 228,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 436,
                    "end": 440,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 853,
                    "end": 857,
                    "matchedPaperCorpusId": "56594619"
                },
                {
                    "start": 857,
                    "end": 860,
                    "matchedPaperCorpusId": "201809759"
                },
                {
                    "start": 860,
                    "end": 863,
                    "matchedPaperCorpusId": "39802142"
                },
                {
                    "start": 944,
                    "end": 948,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1082,
                    "end": 1086,
                    "matchedPaperCorpusId": "54447578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65478515625
        },
        {
            "corpus_id": "257637125",
            "title": "Out of Thin Air: Exploring Data-Free Adversarial Robustness Distillation",
            "text": "To explore the impact of missing original training data on existing ARD tasks, we start with the effectiveness of knowledge transfer in the distillation process. By analyzing the lipschitzness of the robust model and the properties of the generated data, we theoretically demonstrate why DFARD is more challenging than KD and ARD tasks. DFARD has a lower information upper limit than KD and ARD in the knowledge transfer process. This conclusion implies that for the DFARD task, more knowledge is needed. Based on this, we try to improve the efficiency of knowledge transfer and ensure higher data information to meet this challenge. Inspired by Human Education and Curriculum Learning (Bengio et al. 2009;Pentina, Sharmanska, and Lampert 2015), we try to look at the knowledge transfer process from the perspective of 1) the knowledge from the teacher and 2) the knowledge from the data. Detailed discussions are as follows: \n\nDistillation Temperature Lower Higher The Knowledge from the Teacher. In the process of human education, teachers always teach students simple knowledge in the beginning. With improving students' abilities, more difficult knowledge is gradually covered. This easy-to-hard training process improves the efficiency of knowledge transfer. Inspired by this, we analogize the DFARD task to the complex challenge of the human learning process. The key lies in how to build an easy-to-hard process. In fact, distillation temperature enables the teacher network to provide suitable soft labels to transfer knowledge from the cumbersome model to a small model (Hinton et al. 2015;Romero et al. 2014). The temperature controls the discrepancy between two distributions and represents learning objectives of varying degrees of difficulty (M\u00fcller, Kornblith, and Hinton 2019;Li et al. 2022a;Zi et al. 2021;Li et al. 2022b) as shown in Figure 3. Most existing methods ignore the usefulness of the distillation temperature itself, regard it as a fixed hyperparameter, and inefficiently search for optimum. On this basis, they spend several times on computational costs.",
            "score": 0.6175271553957995,
            "section_title": "The Challenges of DFARD",
            "char_start_offset": 5225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 686,
                    "end": 706,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 706,
                    "end": 744,
                    "matchedPaperCorpusId": "8502955"
                },
                {
                    "start": 1791,
                    "end": 1807,
                    "matchedPaperCorpusId": "252780669"
                },
                {
                    "start": 1807,
                    "end": 1822,
                    "matchedPaperCorpusId": "237194985"
                },
                {
                    "start": 1822,
                    "end": 1837,
                    "matchedPaperCorpusId": "260378701"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1405029296875
        },
        {
            "corpus_id": "264323617",
            "title": "Deep Learning-Based Eye Gaze Estimation for Automotive Applications Using Knowledge Distillation",
            "text": "In this stage, the knowledge distillation algorithm is used. Forward propagation is performed for the teacher and student networks, while backpropagation is applied only to the student network. The main loss function is defined using two different loss functions: student loss and distillation loss. \n\nKnowledge from a model is categorized into three different types: Response-based knowledge, Feature-based knowledge, and Relation-based knowledge [32]. Response-based knowledge focuses on the final output layer, where the student model will learn the predictions of the teacher model. Feature-based knowledge leverages the knowledge of the data from teacher intermediate layers to train a student model. Relation-based knowledge focuses on the correlation between feature maps, graphs, similarity matrix, feature embeddings, or probabilistic distributions. In this paper, response-based knowledge is used because it showed the best results in different tasks and applications. \n\nThe block diagram for the knowledge distillation process is shown in Fig. 1. \n\nThe knowledge transfer from teacher to student is performed by minimizing the main loss function with the target to yield the same probabilities as using the teacher model. Basically, it refers to the output of the softmax function applied to non-normalized predictions. These predictions of the teacher model are usually found under the name of logits. Most often, the correct class of the probability distribution has a higher level compared to the other class probabilities that are close to zero. Therefore, the outcome provided by this probability distribution is very similar to the ground truth labels of the dataset. Regarding this behavior, Hinton et al. [15] introduced the softmax parameter temperature. By denoting this parameter with T , the probability p i of class i from the logit z i is computed as in (1). \n\nWhen the parameter T is set to 1, the equation becomes a standard softmax function. Setting a value higher than 1 for T , the probability distribution will provide more information about the classes where the teacher model reported a prediction close to the correct class. This is the knowledge of the teacher model that is transferred to the student model using the distillation algorithm. When the distillation loss function is calculated using the soft labels, the same value of T is used to calculate the softmax on the student's logits.",
            "score": 0.6173658196405387,
            "section_title": "3) TRAINING THE STUDENT NETWORK WITH KNOWLEDGE TRANSFER FROM THE TEACHER NETWORK",
            "char_start_offset": 23846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 299
                },
                {
                    "start": 302,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1057
                },
                {
                    "start": 1060,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1883
                },
                {
                    "start": 1886,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2276
                },
                {
                    "start": 2277,
                    "end": 2427
                }
            ],
            "ref_mentions": [
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1879,
                    "end": 1882,
                    "matchedPaperCorpusId": "28182923"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6796875
        },
        {
            "corpus_id": "257346971",
            "title": "An Improved Tuna-YOLO Model Based on YOLO v3 for Real-Time Tuna Detection Considering Lightweight Deployment",
            "text": "The calculations and parameter amounts of the network were reduced significantly after adopting the lightweight design, but so was the detection accuracy. To address this problem, knowledge distillation (KD), a joint training method by transferring \"knowledge\", was employed to improve the detection accuracy. The KD structure was shown in Figure 3. KD is the process of imitating the distillation in chemistry, using the softmax function with temperature parameters to \"distill\" the logit output from complex and large networks, so as to generate more information in categories. This part of the information is called \"dark knowledge\". The additional information guides the simple and small network to learn more knowledge, and the two networks are called the teacher network and the student network, respectively. In total, 9 sizes were obtained from clustering, e.g., (16,23), (32,45), (34,26), (39, 68), (74, 48), (82, 123), (136, 98), (187, 231) and (386, 334).",
            "score": 0.6164594458010976,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 16205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 966
                }
            ],
            "ref_mentions": [
                {
                    "start": 871,
                    "end": 875,
                    "matchedPaperCorpusId": "195908774"
                },
                {
                    "start": 884,
                    "end": 887,
                    "matchedPaperCorpusId": "237825585"
                },
                {
                    "start": 893,
                    "end": 896,
                    "matchedPaperCorpusId": "29535948"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.499755859375
        },
        {
            "corpus_id": "258714078",
            "title": "Mitigating carbon footprint for knowledge distillation based deep learning model compression",
            "text": "The Kullback-Liebler loss provides additional gradient information to the distillation loss that transfers a teacher's knowledge to a student. Properly softening the hyper-parameter Temperature, \u03c4, is required to produce the softening effect that introduces dark knowledge for the",
            "score": 0.6161231453203881,
            "section_title": "Issues with knowledge distillation",
            "char_start_offset": 14882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 280
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2193603515625
        },
        {
            "corpus_id": "273850157",
            "title": "Towards Personalized Federated Learning via Comprehensive Knowledge Distillation",
            "text": "Knowledge distillation (KD) transfers knowledge from a trained teacher model to an untrained student model [15]. Let p t and p s denote the outputs of the teacher and student models, respectively. To smooth the output distribution, a temperature parameter \u03c4 is used in the softmax function p \u03c4 = exp(zi/\u03c4 ) j exp(zj /\u03c4 ) , where z i is the i-th output element. KD aims to minimize the discrepancy between the teacher and student models. The loss function is defined as follows: \n\nwhere L CE is the Cross-Entropy loss between the soft labels p s and the hard labels y, L KL is the Kullback-Leibler divergence loss between the soft labels p \u03c4 s and the soft labels p \u03c4 t , \u03bb is the weight factor. \n\nRecently, KD has become a critical technique in FL [16], offering innovative solutions to tackle the challenge of catastrophic forgetting, exemplified by methods such as pFedSD [7]. This method facilitates the transfer of knowledge from the historical model to the local model for each client. However, it places excessive emphasis on utilizing personalized knowledge from the historical model while overlooking generalized knowledge of the global model. \n\nAs shown in Fig. 2, We propose a comprehensive knowledge distillation method using multi-teacher knowledge distillation. In our method, we retained the local model from previous training, referred to as the historical model, to maintain personalized knowledge. The global model, in contrast, embodies generalized knowledge. We employ both the global and historical models as teachers, with the local model serving as the student, to facilitate a thorough knowledge transfer that balances generalization and personalization. The loss function is defined as follows: \n\nwhere p \u03c4 k represents the soft labels of the local model w k , p \u03c4 g represents the soft labels of the global model w g , and p \u03c4 h represents the soft labels of the historical model w h . \n\nTo facilitate effective knowledge transfer, we implement an annealing mechanism. Initially, the student model primarily learns from the teacher model's soft labels. As training advances, \u03bb undergoes annealing, gradually increasing its emphasis on the hard labels.",
            "score": 0.6159159670989,
            "section_title": "B. Comprehensive Knowledge Distillation",
            "char_start_offset": 7182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1718
                },
                {
                    "start": 1721,
                    "end": 1910
                },
                {
                    "start": 1913,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2176
                }
            ],
            "ref_mentions": [
                {
                    "start": 874,
                    "end": 877,
                    "matchedPaperCorpusId": "254089825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44921875
        },
        {
            "corpus_id": "265680820",
            "title": "Learn by Yourself: A Feature-Augmented Self-Distillation Convolutional Neural Network for Remote Sensing Scene Image Classification",
            "text": "In the process of knowledge distillation, a large teacher model is usually use training first, and then the teacher model is adopted to generate soft labels, which are together with the output of the student model to train the student model. During the ing process, different weights can be used to balance the relative importance of har soft objects. Choosing an appropriate teacher model is crucial to the effect of know distillation. In general, the teacher model should be complex and accurate enough to vide high-quality soft targets. A commonly used teacher model is a pre-trained deep ral network model. The student model is usually more lightweight and simplified the teacher model for deployment where computing resources are constrained. common student model design strategies include using shallow network structure reducing the number of network parameters. With the deepening of research, man proved and extended knowledge distillation methods have emerged. For exampl FitNets method [39] introduced the concept of intermediate layer alignment to alig intermediate layer outputs of the teacher model and the student model. The atte transfer method [40] learned knowledge from the teacher network by having the stu network imitate the attention map of the teacher network. The relational knowledg tillation method [41] exploited relational modeling to improve knowledge distillati comprehensive overhaul of the feature distillation method [42] adopted the feature lation, designed a new distillation loss, distilled features before the ReLU function retained negative values before distillation. Ahn et al. [43] proposed a variational mation distillation framework, which transfers the knowledge learned by Soft labels are the probability distributions output by the teacher model, which can provide richer information to help the student model learn. To transfer knowledge effectively, an appropriate loss function needs to be defined to measure the difference between the output of the student model and the output of the teacher model. Commonly used loss functions include the mean squared error [37], cross-entropy loss [37], and KL divergence [38]. The mean square error loss function measures the numerical difference of the output, while the cross-entropy loss function measures the difference in the probability distribution of the output. KL divergence (Kullback-Leibler divergence), also known as relative entropy, is an indicator used to measure the difference between two probability distributions.",
            "score": 0.6156741810231703,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 16621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2359
                },
                {
                    "start": 2360,
                    "end": 2522
                }
            ],
            "ref_mentions": [
                {
                    "start": 1326,
                    "end": 1330,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1451,
                    "end": 1455,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1619,
                    "end": 1623,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 2160,
                    "end": 2164,
                    "matchedPaperCorpusId": "116908168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52685546875
        },
        {
            "corpus_id": "265444951",
            "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
            "text": "KD aims to transfer the dark knowledge from a complex teacher model to a lightweight student model by aligning their logits that are softened with a temperature hyperparameter. This enables the student to approximate the teacher's performance in a compact form. \n\nLogit and Feature Distillation Logit distillation methods aim to align the teacher and student output logits, valued for their simplicity and wide applicability. In contrast, feature distillation focuses on minimizing divergence in intermediate feature representations, offering enhanced learning but at the cost of increased computational demands. \n\nBoth pathways have demonstrated state-of-the-art performance across various tasks and domains. While most of these methods follow a static training approach, recent techniques have further refined the distillation process through dynamic approaches. \n\nAdaptive Distillation Diverging from the static approaches, adaptive distillation methods pave the way for more dynamic and tailored knowledge transfer [3,13,18,20,31,37,38]. These methods either dynamically provide prior knowledge [38], modulate hyperparameters [13,18,20], or adjust distillation strategies [3,31,37] based on the teacher-student performance gap. Within this landscape, the Curriculum Temperature for Knowledge Distillation (CTKD) [13] emerges as a simple yet effective approach. This approach introduces curriculum training and adversarial temperature learning to KD, progressively exposes the student to complexities, and pushes it to address harder challenges through instance-wise temperature modulation. Nonetheless, CTKD relies on temperature modulation which dictates the overall softness of labels. In contrast, our ER-KD reweights the KD loss with the entropy of the teacher's predictions, precisely emphasizing challenging samples while reducing the focus on simpler ones.",
            "score": 0.6155290522506045,
            "section_title": "Related work",
            "char_start_offset": 5957,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 612
                },
                {
                    "start": 615,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1867
                }
            ],
            "ref_mentions": [
                {
                    "start": 1019,
                    "end": 1022,
                    "matchedPaperCorpusId": "239024317"
                },
                {
                    "start": 1022,
                    "end": 1025,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 1028,
                    "end": 1031,
                    "matchedPaperCorpusId": "244119770"
                },
                {
                    "start": 1034,
                    "end": 1037,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 1037,
                    "end": 1040,
                    "matchedPaperCorpusId": "249626454"
                },
                {
                    "start": 1099,
                    "end": 1103,
                    "matchedPaperCorpusId": "249626454"
                },
                {
                    "start": 1130,
                    "end": 1134,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 1137,
                    "end": 1140,
                    "matchedPaperCorpusId": "244119770"
                },
                {
                    "start": 1176,
                    "end": 1179,
                    "matchedPaperCorpusId": "239024317"
                },
                {
                    "start": 1182,
                    "end": 1185,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 1316,
                    "end": 1320,
                    "matchedPaperCorpusId": "254069919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79052734375
        },
        {
            "corpus_id": "259274976",
            "title": "Mitigating Accuracy-Robustness Trade-Off via Balanced Multi-Teacher Adversarial Distillation",
            "text": "Knowledge distillation can transfer the performance of other models to the target model [20]. Extensive research has been widely studied in recent years [23], [25], [52]. \n\nKnowledge distillation can briefly be formulated as the following optimization: \n\nwhere S represents the student model with weight \u03b8 S , T represents the teacher model. KL is Kullback-Leibler divergence loss, CE represents the cross-entropy loss. \u03c4 is a temperature constant combined with softmax operation, \u03b1 is a weight hyper-parameter. \n\nThe original knowledge distillation [20] attempts to improve the student's performance on clean examples, and pre-define a high temperature until the teacher produces suitably soft labels, while the same high temperature is applied to train the student to match these soft labels, which allows the student to better acquire knowledge from a single type of teacher. Our method (with our Entropy-Based Balance Algorithm) is to balance the student's performance between accuracy and robustness, and we adaptively adjust the temperature for different types of teachers to balance their knowledge scales, which allows the student to equally acquire knowledge from different types of teachers. \n\nThe temperature \u03c4 is artificially adjusted as a hyperparameter in previous work. Recently, [26], [28] propose to adjust \u03c4 automatically based on the designed learnable sub-network. These methods need to train the sub-network according to the feedback of the student model, which adds additional training overhead. Our method automatically adjusts the temperature only based on multiple teachers' knowledge scales and has almost no computational cost. Some studies also exist on multi-teacher knowledge distillation, and are designed from different views: including response-based knowledge [23], [52], feature-based knowledge [29], [57], and relation-based knowledge [48], [51]",
            "score": 0.6153234143975145,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 12272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 170
                },
                {
                    "start": 173,
                    "end": 252
                },
                {
                    "start": 255,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1201
                },
                {
                    "start": 1204,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1881
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 157,
                    "matchedPaperCorpusId": "216265175"
                },
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "228376532"
                },
                {
                    "start": 1794,
                    "end": 1798,
                    "matchedPaperCorpusId": "216265175"
                },
                {
                    "start": 1800,
                    "end": 1804,
                    "matchedPaperCorpusId": "228376532"
                },
                {
                    "start": 1830,
                    "end": 1834,
                    "matchedPaperCorpusId": "224818016"
                },
                {
                    "start": 1836,
                    "end": 1840,
                    "matchedPaperCorpusId": "198179767"
                },
                {
                    "start": 1877,
                    "end": 1881,
                    "matchedPaperCorpusId": "26021416"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.431884765625
        },
        {
            "corpus_id": "271592942",
            "title": "A Lightweight and High-Precision Passion Fruit YOLO Detection Model for Deployment in Embedded Devices",
            "text": "Knowledge distillation (KD) is an effective method for further improving a model's detection accuracy [43]. Distillation is not yet widely used in the YOLO series of network improvements, especially for small single-target models. We conducted a special study for G-YOLOv5-N and used the distillation technique to achieve a final further improvement to the detection effect of the G-YOLOv5-N model. First, the teacher network model was chosen rationally. We chose the YOLOv5 series of models in order to ensure that the student and teacher models use the same scale in the output layer. Next, the YOLOv5x model was selected as the teacher model based on its high accuracy during training on the passion fruit dataset. \n\nIn general, the implementation of distillation involves parameter initialization, training a teacher network, then using the rich knowledge learned by the teacher network to train a student network. A flowchart detailing the use of the knowledge distillation algorithm is shown in Figure 8. Teacher networks can be used for predictive learning in student networks. To enhance the information exchange between them, the predictions of the teacher network are used as soft labels. The teacher network then trains the student network using these soft labels, allowing the student network to learn from the teacher's knowledge. Additionally, the student network helps to prevent the teacher network from making mistakes through learning from hard labels. By incorporating soft labels, the student network can acquire more nuanced and hidden knowledge. This hidden knowledge is usually expressed as a categorical output y i \u2032 , while y i represents the input knowledge or information, as shown in Equation (3): \n\nFrom Equation (3), it can be determined that the model does not facilitate learning of the dark knowledge in the passion fruit images. Thus, a warming process is needed, as detailed in Equation ( 4), where T represents the temperature coefficient: \n\nThe cumulative loss function utilized in the knowledge distillation algorithm comprises the original network model's loss and the distillation loss. The distillation loss is composed of the classification, bounding box, and localization losses.",
            "score": 0.6153090822660867,
            "section_title": "Knowledge Distillation Enhancement",
            "char_start_offset": 16789,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1975
                },
                {
                    "start": 1978,
                    "end": 2126
                },
                {
                    "start": 2127,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 106,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.533203125
        },
        {
            "corpus_id": "232269823",
            "title": "Similarity Transfer for Knowledge Distillation",
            "text": "Different from above methods, knowledge distillation enrich and get the student model by extracting kinds of knowledge from the fixed teacher model. To address the challenge of deploying CNNs in resource-constrained edge devices, Bucilua et al. [38] first propose to transfer the knowledge of an ensemble of models to a small model. Then Caruana et al. [39] propose to train student model by mimicking the teacher model's logits. Later, Hinton et al. [12] popularize the idea of knowledge distillation, which efficiently transfers knowledge from large teacher network to compact student network by mimicking the class probabilities outputs. Note that, the outputs of the teacher network are defined as the dark knowledge in KD, which provides similarity information as extra supervisions compared with one-hot labels. In other words, there are two sources of supervision used to train the student in KD, one from the ground-truth label, and the other from the soft targets of teacher network. Afterward, some recent works [14] [15] extend KD by distilling knowledge from intermediate feature representations instead of soft labels. For example, FitNets [14] propose to train student network by mimicking the intermediate feature maps of teacher network, which are defined as hints. Inspired by this, Zagoruyko et al. [15] propose to match the attention maps between the teacher and the student, which are defined from the original feature maps as knowledge. Wang et al. [40] propose to improve the performance of student network by matching the distributions of spatial neuron activations between the teacher and the student. Recently, Heo et al. [41] introduce the activation boundary of the hidden neuron as knowledge for distilling the compact student network. \n\nHowever, the aforementioned knowledge distillation methods only utilize the knowledge contained in the output of specific layers of the teacher network. More richer knowledge between different layers is explored and utilized for knowledge distillation. For example, Yim et al. [16] propose to use Gram matrix between different feature layers as distilled knowledge, which named flow of solution process (FSP) that reflects the relations of different features maps.",
            "score": 0.6143214183740953,
            "section_title": "B. Knowledge Distillation.",
            "char_start_offset": 9178,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1763
                },
                {
                    "start": 1766,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2230
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 249,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 353,
                    "end": 357,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 451,
                    "end": 455,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1022,
                    "end": 1026,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1153,
                    "end": 1157,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1317,
                    "end": 1321,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1470,
                    "end": 1474,
                    "matchedPaperCorpusId": "30307744"
                },
                {
                    "start": 1647,
                    "end": 1651,
                    "matchedPaperCorpusId": "53213211"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.751953125
        },
        {
            "corpus_id": "202889259",
            "title": "Revisit Knowledge Distillation: a Teacher-free Framework",
            "text": "To examine the common belief on dark knowledge in KD, we conduct two exploratory experiments: \n\n1) The standard knowledge distillation is to adopt a teacher to teach a weaker student. What if we reverse the operation? Based on the common belief, the teacher should not be improved significantly because the student is too weak to transfer effective knowledge. 2) If we use a poorly-trained teacher which has much worse performance than the student to teach the student, it is assumed to bring no improvement to the latter. For example, if a poorly-trained teacher with only 10% accuracy is adopted in an image classification task, the student would learn from its soft targets with 90% error, thus the student should not be improved or even suffer worse performance. We name the \"student teach teacher\" as Reversed Knowledge Distillation (Re-KD), and the \"poorlytrained teacher teach student\" as Defective Knowledge Distillation (De-KD) (Fig. 1). We conduct Re-KD and De-KD experiments on CIFAR10, CIFAR100 and Tiny-ImageNet datasets with a variety of neural networks. For fair comparisons, all experiments are conducted with the same settings. Detailed implementation and experiment settings are given in Appendix A.1.",
            "score": 0.6136458345921026,
            "section_title": "EXPLORATORY EXPERIMENTS AND COUNTERINTUITIVE OBSERVATIONS",
            "char_start_offset": 5433,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 96,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1219
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10394287109375
        },
        {
            "corpus_id": "251719303",
            "title": "Rethinking Knowledge Distillation via Cross-Entropy",
            "text": "Knowledge distillation (KD) is a method to improve the model while keeping the network structure unchanged. It was first proposed by Hinton et al. (Hinton et al., 2015), where the student is supervised by the hard labels and the soft labels from the teacher's output. Many following works focus on making better use of soft labels to transfer more knowledge. WSLD (Zhou et al., 2020) analyzes soft labels and distributes different weights for them from a perspective of bias-variance trade-off. DKD (Zhao et al., 2022) divides the classical KD according to the teacher's prediction and modifies the formulation of KD, achieving state-of-the-art performances. SRRL (Yang et al., 2020) forces the output logits of teacher's and student's features after the teacher's linear layer to be the same. \n\nBesides distillation on logits, some works aim at transferring knowledge from intermediate features. \n\nFitNet (Romero et al., 2014) distills the semantic information from intermediate feature directly. OFD (Heo et al., 2019) designs the margin ReLU and modifies the measurement for the distance between students and teachers. RKD (Park et al., 2019) extracts the relation from the feature map. CRD (Tian et al., 2019) applies contrastive learning to distillation successfully. KR (Chen et al., 2021) transfers knowledge from multi-level features for distillation. SRRL (Yang et al., 2020) utilizes the teacher's classifier to train the student's feature. MGD (Yang et al., 2022) proposes a new distillation method that makes the student generate the teacher's feature instead of mimicking.",
            "score": 0.613608145216557,
            "section_title": "RELATED WORK",
            "char_start_offset": 5290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 896
                },
                {
                    "start": 899,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1585
                }
            ],
            "ref_mentions": [
                {
                    "start": 364,
                    "end": 383,
                    "matchedPaperCorpusId": "231740588"
                },
                {
                    "start": 664,
                    "end": 683,
                    "matchedPaperCorpusId": "235613564"
                },
                {
                    "start": 1002,
                    "end": 1020,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1126,
                    "end": 1145,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1194,
                    "end": 1213,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1276,
                    "end": 1295,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 1365,
                    "end": 1384,
                    "matchedPaperCorpusId": "235613564"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.425537109375
        },
        {
            "corpus_id": "237492204",
            "title": "On the Efficiency of Subclass Knowledge Distillation in Classification Tasks",
            "text": "Knowledge Distillation in Classification Tasks. Transferring knowledge from one model to another is a research topic that has obtained noteworthy attention during recent years. Ba and Caruana (Ba and Caruana 2014) trained a single and small neural network to imitate the logits of a large and complex neural network. Then, Hinton et al. (Hinton, Vinyals, and Dean 2015) introduced KD and dark knowledge to claim that the deeper teacher model can successfully distill its knowledge into the smaller student neural network by matching their soft targets (softmax distributions). Nowadays, a lot of successive papers have been written to propose different techniques to KD for model compression purposes. Romero et al. (Romero et al. 2015) distilled the feature representations of the teacher's intermediate layers to the student for improving the training stage of the student network. Transferring the attention maps (Zagoruyko and Komodakis 2017;Huang and Wang 2017;Tarvainen and Valpola 2017), the inner products of intermediate activation maps (Yim et al. 2017), and relational knowledge between training samples (Park et al. 2019;Tung and Mori 2019;Peng et al. 2019;Liu et al. 2019) are some other methods to promote the distillation process from one model to another. However, these approaches ignored the possibility of available subclass knowledge within the classes and therefore did not take advantage of hidden subclass knowledge to improve student performance. By contrast, in this study, we use subclass knowledge to enhance the generalization ability of the teacher network. \n\nSubclass Knowledge Distillation. The distillation of knowledge can be improved by increasing the amount of information that the teacher can transfer to the student. M\u00fcller et al. (M\u00fcller, Kornblith, and Hinton 2020) compelled the teacher to create semantically meaningful subclasses for each class during its training phase with auxiliary contrastive loss. The student is then trained to mimic the invented teacher's subclasses predictions (probabilities). When the number of training samples per class is the same, they measured the number of bits of label information about how the teacher generalizes through subclass distillation in binary classification tasks.",
            "score": 0.6134337958215706,
            "section_title": "Related Work",
            "char_start_offset": 4680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1586
                },
                {
                    "start": 1589,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2254
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 213,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 337,
                    "end": 369,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 916,
                    "end": 946,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 966,
                    "end": 993,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 1046,
                    "end": 1063,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1115,
                    "end": 1133,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1133,
                    "end": 1152,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1152,
                    "end": 1169,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1169,
                    "end": 1185,
                    "matchedPaperCorpusId": "198185886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494873046875
        },
        {
            "corpus_id": "232046107",
            "title": "Risk factor identification for incident heart failure using neural network distillation and variable selection",
            "text": "Model distillation includes two components: a teacher network and a student network. The teacher network is a well-trained established deep learning model (BEHRT in our case), and the student network is a simpler model or a target model that we intend to use it to distil knowledge from the teacher network (BDLD in our case). Tang et al. [35] developed a strategy to train a student model with partial assistance from teacher models to help the student network achieve a competitive performance, and Korattikara et al. [20] proposed a Bayesian deep learning-based knowledge distillation method to transfer knowledge between probabilistic models. In this work, we combine both methods to use the probabilistic teacher network BEHRT to partially supervise the training of the probabilistic student network BDLD. As shown in Figure 2, both the teacher network and the student network use the same input information. Afterwards, the prediction from the teacher network will be used as a soft label to partially supervise the training of the student model. At the same time, the student model is also trying to predict the real label. Therefore, the loss function can be summarized as below: \n\nwhere  represents a pre-set weight to balance the training objective for mimicking the teacher and leaning for the classification task. \n\n  represents the loss for evidence lower bound, and   is the loss for knowledge distillation. We used cross entropy for negative log-likelihood in evidence lower bound. Additionally, the loss for knowledge distillation is shown as below: \n\nwhere  (|, ) represents the outputs from the student network,  (|,   ) represents the outputs from the teacher network,   are the parameters for the teacher network, and  is the weights for student network. The distillation loss can also be considered as a reframing of the standard cross entropy loss.",
            "score": 0.6125706494391487,
            "section_title": "Model distillation",
            "char_start_offset": 11160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1325
                },
                {
                    "start": 1328,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1565
                },
                {
                    "start": 1568,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1870
                }
            ],
            "ref_mentions": [
                {
                    "start": 520,
                    "end": 524,
                    "matchedPaperCorpusId": "84176918"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51953125
        },
        {
            "corpus_id": "263789814",
            "title": "Multi-level Knowledge Distillation via Knowledge Alignment and Correlation",
            "text": "Knowledge Distillation. Hinton et al. [23] first propose KD to transfer dark knowledge from the teacher to the student. The softmax outputs encode richer knowledge than one-hot labels and can provide extra supervisory signals. SRRL [41] performs knowledge distillation by leveraging the teacher's projection matrix to train the student's representation via L2 loss. However, these works rely on a supervised pretrained teacher (with logits), and they may be not suitable for self-supervised pretrained teachers. SSKD [40] is proposed to combine the self-supervised auxiliary task and KD to transfer richer dark knowledge, but it cannot be trained in an end-to-end training way. Similar to logits matching, intermediate representation [32,43,42,36,22] are widely used for KD. FitNet [32] proposes to match the whole feature maps, which is difficult and may affect the convergence of the student in some cases. Attention transfer [43] utilizes spatial attention maps as the supervisory signal. AB [22] proposes to learn the activation boundaries of the hidden neurons in the teacher. SP [36] focuses on transferring the similar (dissimilar) activations between the teacher and student. However, most of these works depend on certain architectures, such as convolutional networks. Since these distillation methods involve knowledge matching in an individual sample, they are related to knowledge alignment. Our work also includes the knowledge alignment objective, and it doesn't rely on pretraining strategies or network architectures. \n\nKnowledge distillation and self-supervised learning. Self-supervised learning [30,2,8,20,6] focuses on learning low-dimensional representations by the instance discrimination, which usually requires a large number of negative samples. Recently, BYOL [18] and DINO [7] utilize the momentum encoder to avoid collapse without negatives. The momentum encoder can be considered as the mean teacher [34], which is built dynamically during the student training. For KD, the teacher is pretrained and fixed during distillation. Although different views (augmented images) are passed through networks in self-supervised learning, they are from the same original sample, and have the same semantic meaning.",
            "score": 0.6117491799145311,
            "section_title": "Related Work",
            "char_start_offset": 4102,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1533
                },
                {
                    "start": 1536,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2232
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 236,
                    "matchedPaperCorpusId": "235613564"
                },
                {
                    "start": 741,
                    "end": 744,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 744,
                    "end": 747,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 747,
                    "end": 750,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 995,
                    "end": 999,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1085,
                    "end": 1089,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1622,
                    "end": 1625,
                    "matchedPaperCorpusId": "207930212"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.763671875
        },
        {
            "corpus_id": "271244914",
            "title": "Relational Representation Distillation",
            "text": "The concept of Knowledge Distillation (KD) was first introduced by Hinton et al. [20]. It involves extracting \"dark knowledge\" from accurate teacher models to guide the learning process of student models. This is achieved by utilizing the Kullback-Leibler (KL) loss to regularize the output probabilities of student models, aligning them with those of their teacher models when given the same inputs. This simple yet effective approach significantly improves the generalization ability of smaller models and finds extensive applications in various domains. Since the initial success of KD [20], several advanced methods, including logit distillation [21,35,52] and feature distillation [42,47,53,55], have been introduced. \n\nLogit distillation. Earlier methods on logit-based distillation primarily focused on improving student learning by directly mimicking the teacher's output probabilities. Examples included hierarchical supervision using intermediary teacher networks [52], multi-step student training to 2 Average relative improvement is calculated as: \n\n, where Acc i RRD , Acc i KD , and Acc i van represent the accuracies of RRD, KD, and vanilla training of the i-th student model, respectively [47]. \n\nenhance compatibility [35], collaborative learning among multiple students to improve generalization [58] and mechanisms that separately handle different types of logit information [59]. Recent advancements have sought to refine the quality of knowledge transfer. Some methods modify the distillation target: label decoupling [61] separately processes hard and soft labels, while instance-specific label smoothing [54] adapts the smoothing factor per example. Additional approaches focus on refining probability distributions, including probability reweighting to emphasize important outputs [37] and logit normalization to mitigate overconfidence [46]. Other methods include dynamic temperature scaling to adjust teacher-student similarity [25], specialized transformations to align teacher-student logits more effectively [60], and approaches that adapt teacher logits to better fit weaker students [21]. \n\nFeature distillation. Earlier methods on feature-based distillation emphasized utilizing intermediate feature representations to facilitate learning.",
            "score": 0.6116685122573274,
            "section_title": "Related Work",
            "char_start_offset": 4321,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2119
                },
                {
                    "start": 2122,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2271
                }
            ],
            "ref_mentions": [
                {
                    "start": 686,
                    "end": 690,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 693,
                    "end": 696,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "829159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82177734375
        },
        {
            "corpus_id": "256843895",
            "title": "End-to-end emotional speech recognition using acoustic model adaptation based on knowledge distillation",
            "text": "y \u00f0 \u00de\u00fe\u03b1C soft x; q \u00f0 \u00de \u00f04\u00de\n\nThe total loss in the network is the sum of the distillation and student losses [15]. The distillation loss is contributed by the soft label of the teacher model and the soft prediction of the student model, and the student loss is contributed by the hard prediction of the student model and the hard label of target domain data. The soft label from the teacher model and two types of predictions from the student model are obtained according to softmax with different temperatures. The above equations are mathematical descriptions of knowledge distillation. In (2) and (3), p i (x) is the output probability of the i-th class of the student model, and q i is a soft target of the input feature x. Therefore, C hard (x, y) is a one-hot vector output by the softmax layer, and C soft (x, y) is a softly distributed one-hot vector of softmax probabilities. In (4), \u03b1 is the weight of the hard or soft label in the cross-entropy loss.  As described above, knowledge distillation transfers the excellent performance knowledge of teacher models to shallow student models. After training on the extensive teacher knowledge, the student model delivers its best performance. Thus, knowledge distillation has been widely applied for model compression in DNN-based speech recognition tasks [7].",
            "score": 0.6112550169004474,
            "section_title": "Knowledge distillation",
            "char_start_offset": 7696,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 591,
                    "end": 594,
                    "matchedPaperCorpusId": "220632998"
                },
                {
                    "start": 1309,
                    "end": 1312,
                    "matchedPaperCorpusId": "18195425"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32373046875
        },
        {
            "corpus_id": "251371662",
            "title": "PGX: A Multi-level GNN Explanation Framework Based on Separate Knowledge Distillation Processes",
            "text": "Knowledge Distillation, first proposed in [9], is originally used for model compression, wherein a small model (student) tries to mimic the large model (teacher)'s decisions. Gou et al., 2021 [8] presented a complete survey of knowledge distillation, including various techniques and approaches. Here, let us briefly summarize the original method [9]. \n\nIn Fig. 1, given the same set of input features, both models first output so-called soft labels/predictions, which are results of softmax function with a temperature term  shown in Eq. (1). Then, the student model computes the distillation loss, usually a soft crossentropy loss function. After that, it also generates hard predictions to compare with the ground-truth labels via the cross-entropy loss function. Finally, we have the student loss function as follows: \n\nwhere   ,   , and  are teacher/student outputs and labels, respectively.  is a hyper-parameter that controls the magnitude of L  . [9]. The student's loss is regularized by the knowledge distillation loss, which tries to bring the student's predictive distributions close to the teacher's.",
            "score": 0.6110471442371328,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 351
                },
                {
                    "start": 354,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1113
                }
            ],
            "ref_mentions": [
                {
                    "start": 539,
                    "end": 542,
                    "matchedPaperCorpusId": "244117482"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4267578125
        },
        {
            "corpus_id": "265680820",
            "title": "Learn by Yourself: A Feature-Augmented Self-Distillation Convolutional Neural Network for Remote Sensing Scene Image Classification",
            "text": "The idea of knowledge distillation (Original Knowledge Distillation) is to guide the training of the student model by using the soft targets of the teacher model. The teacher network usually produces class probabilities by using a \"softmax\" output layer with a temperature hyperparameter applied, which is used to convert the logits generated by each class calculation, that is, z i , into a probability q i , and the calculation process can be represented as \n\nwhere T is the temperature, usually set to 1. Using a higher T value produces a softer distribution on the output of the classification. As shown in Figure 2, an image is input into the network, the resulting output is fed into Softmax with temperature hyperparameters, and then a soft-label output is obtained. In the output soft label, we can see that for an input image labeled as a forest, in addition to the probability of the forest category, there will also be a certain probability of other categories in the output. Soft labels are the probability distributions output by the teacher model, whic provide richer information to help the student model learn. To transfer knowledge tively, an appropriate loss function needs to be defined to measure the difference bet the output of the student model and the output of the teacher model. Commonly loss functions include the mean squared error [37], cross-entropy loss [37], and KL d gence [38]. The mean square error loss function measures the numerical difference output, while the cross-entropy loss function measures the difference in the proba distribution of the output. KL divergence (Kullback-Leibler divergence), also know relative entropy, is an indicator used to measure the difference between two proba distributions. The calculation process can be represented as \n\nrepresents the distribution predicted by the teacher network, and represents the distribution predicted by the student network. \n\nKL divergence measures the loss of information from the true distribution t model distribution. The real distribution is simulated using the output of the teache work. In the process of knowledge distillation, a large teacher model is usually use training first, and then the teacher model is adopted to generate soft labels, which are together with the output of the student model to train the student model.",
            "score": 0.61068499245629,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 14529,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1791
                },
                {
                    "start": 1794,
                    "end": 1921
                },
                {
                    "start": 1924,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2333
                }
            ],
            "ref_mentions": [
                {
                    "start": 1406,
                    "end": 1410,
                    "matchedPaperCorpusId": "116908168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60107421875
        },
        {
            "corpus_id": "264426474",
            "title": "Data-Free Knowledge Distillation Using Adversarially Perturbed OpenGL Shader Images",
            "text": "In recent years, many works have been proposed in the areas of KD, DFKD, ML privacy, and adversarial examples. Knowledge Distillation. The transferring of knowledge from a large network to a smaller network was introduced in [5] and was further refined and coined as \"knowledge distillation\" in [27]. The general KD framework outlined in [27] trains a student network to match the temperaturescaled [23] soft outputs from a larger teacher network using entropy-based loss functions. Since then, several works have investigated what properties influence the success of KD [3,10,57] and others have proposed structural improvements to the seminal approach [45,49,53,54]. Notably, [3] argued that KD can be viewed as \"function matching\" and showed that applying mixup [66] to the input distillation images results in improved student performance. They also observed that some knowledge can be transferred to a student network using out-of-domain data, albeit at a significant performance loss compared to employing in-domain data. Furthermore, KD has been coupled with other model compression techniques [39] and applied to other areas such as semi-supervised learning [62], multi-exit architectures [42], and more. Rather than distilling from a large cumbersome network to a smaller network, self-distillation repeatedly transfers knowledge to a student that is the same architecture as the teacher [19]. \n\nData-Free Knowledge Distillation. In standard KD, it is assumed that the original training dataset is available. However this is not always the case, which has motivated a series of data-free KD approaches that attempt to transfer knowledge when no data is available. This line of work can be separated into two categories based on whether they utilize a generator network [4,9,11,18,65] or inherent teacher network statistics [40,60,64] to synthesize examples that may be beneficial for KD. The first truly data-free approach was proposed in [40], where they modelled Dirichlet distributions at the output of the teacher and synthesized examples to match these distributions using backpropagation.",
            "score": 0.6103427126753204,
            "section_title": "Related Work",
            "char_start_offset": 5155,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1402
                },
                {
                    "start": 1405,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2103
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 228,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 399,
                    "end": 403,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 571,
                    "end": 574,
                    "matchedPaperCorpusId": "235376877"
                },
                {
                    "start": 574,
                    "end": 577,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 577,
                    "end": 580,
                    "matchedPaperCorpusId": "16550689"
                },
                {
                    "start": 654,
                    "end": 658,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 658,
                    "end": 661,
                    "matchedPaperCorpusId": "221761421"
                },
                {
                    "start": 661,
                    "end": 664,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 664,
                    "end": 667,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 678,
                    "end": 681,
                    "matchedPaperCorpusId": "235376877"
                },
                {
                    "start": 765,
                    "end": 769,
                    "matchedPaperCorpusId": "3162051"
                },
                {
                    "start": 1101,
                    "end": 1105,
                    "matchedPaperCorpusId": "3643430"
                },
                {
                    "start": 1166,
                    "end": 1170,
                    "matchedPaperCorpusId": "207853355"
                },
                {
                    "start": 1197,
                    "end": 1201,
                    "matchedPaperCorpusId": "207994757"
                },
                {
                    "start": 1397,
                    "end": 1401,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1778,
                    "end": 1781,
                    "matchedPaperCorpusId": "245837482"
                },
                {
                    "start": 1781,
                    "end": 1783,
                    "matchedPaperCorpusId": "91183944"
                },
                {
                    "start": 1783,
                    "end": 1786,
                    "matchedPaperCorpusId": "218571034"
                },
                {
                    "start": 1786,
                    "end": 1789,
                    "matchedPaperCorpusId": "237101258"
                },
                {
                    "start": 1789,
                    "end": 1792,
                    "matchedPaperCorpusId": "202774028"
                },
                {
                    "start": 1832,
                    "end": 1836,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 1836,
                    "end": 1839,
                    "matchedPaperCorpusId": "233210322"
                },
                {
                    "start": 1839,
                    "end": 1842,
                    "matchedPaperCorpusId": "209405263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4033203125
        },
        {
            "corpus_id": "237420393",
            "title": "Complementary Calibration: Boosting General Continual Learning With Collaborative Distillation and Self-Supervision",
            "text": "Knowledge distillation [33] refers to the approach that the training process of a student model is supported under the supervision of a teacher model with dark knowledge (soft targets). Dark knowledge contains richer similarity relationship among all classes. In addition, the student model could distill knowledge by itself, which is called self-distillation [34]. Knowledge distillation is widely applied in continual learning to address the catastrophic forgetting problem. LWF [35] is the earliest work to explore it in continual learning, which aims at leveraging new samples of the old model's outputs to constrain the new model's outputs. Afterward, FDR [36] stores samples as well as the dark knowledge of the old model, and constrains the \u2113 2 norm of the difference between the new model's outputs and dark knowledge. Unlike the label distillation, LUCIR [10] directly limits the normalized features extracted by the new model as consistent as those by the old model, while PODNet [11] constrains the evolution of each layer's output. Further, GeoDL [37] distills the geodesic path between the old and new models' features, while DDE [12] introduces causal inference to distill the casual effect between the old and new data. Different from them, our proposed collaborative distillation explores ensemble dark knowledge from old and new models, which contains more informative similarity relationships than that from a single model.",
            "score": 0.6100874971187534,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 9830,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 661,
                    "end": 665,
                    "matchedPaperCorpusId": "46898034"
                },
                {
                    "start": 864,
                    "end": 868,
                    "matchedPaperCorpusId": "195453293"
                },
                {
                    "start": 990,
                    "end": 994,
                    "matchedPaperCorpusId": "220665451"
                },
                {
                    "start": 1059,
                    "end": 1063,
                    "matchedPaperCorpusId": "233296374"
                },
                {
                    "start": 1143,
                    "end": 1147,
                    "matchedPaperCorpusId": "232092679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.541015625
        },
        {
            "corpus_id": "244119160",
            "title": "Combining Curriculum Learning and Knowledge Distillation for Dialogue Generation",
            "text": "Knowledge distillation describes a class of methods for the knowledge transfer from teacher network to student network. In our model, the student network S \u03b8 is trained over the same architecture but different parameters as teacher model T \u03b8 . The teacher has previously been trained, and we freeze its parameters when training the student network. \n\nWe transfer the knowledge from teacher to student by minimizing the similarity distance between the output of student network and the soft label generated by the teacher network. We use crossentropy loss to measure the two logits as (Romero et al., 2015). To further improve the sequence-tosequence student model, hard-assigned labels are also utilized. The final student network is trained to optimize the following compound objective: \n\nwhere H refers to the cross-entropy and V is a parameter to indicate the temperature of distillation. Later, we will use the method of the model level curriculum learning to process \u03bb in section 2.5. Note that the first term in Equation ( 3) corresponds to the traditional cross-entropy between the softmax layer's output of a (student) network and word distribution in response Y , whereas the second term is to learn from the softened output of the teacher network to strengthen its supervision for the student. \n\nIn the teacher model, we train it by using all the dataset with original order, while in the student model, the training starts from the step that consists of examples with the lowest difficulty. After that, data in the next step is aggregated to the current training dataset.",
            "score": 0.60972203258073,
            "section_title": "Output Knowledge Distillation",
            "char_start_offset": 9984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1303
                },
                {
                    "start": 1306,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1582
                }
            ],
            "ref_mentions": [
                {
                    "start": 584,
                    "end": 605,
                    "matchedPaperCorpusId": "2723173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64453125
        },
        {
            "corpus_id": "267046068",
            "title": "Object Detection of Remote Sensing Image Based on Multi-Scale Feature Fusion and Attention Mechanism",
            "text": "In knowledge distillation, Soft target is used as the label of the student network, and the training process is to fit the probability of the whole model output. \n\nSoft target contains the category probability distribution of the target and the location information of the detection box, which has more knowledge and information than the Hard target. In order to adjust the contribution degree of positive and negative samples, the distillation temperature coefficient T is introduced into Sigmoid activation function, as shown in (11). \n\n8626 VOLUME 12, 2024 Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. \n\nwhere z i is the output value of the last fully connection layer, and p i is the probability of the input belonging to category i. The larger the temperature coefficient T , the smoother the output probability distribution of the function, and the information of negative samples will be relatively enlarged. Specifically, when T \u2192 \u221e, all categories have the same probability, and when T \u21920, the Soft target reverts to Hard target. \n\nIn the knowledge distillation, another loss function needs to be defined because of the error between the large model and the small model considered. The loss function of knowledge distillation is weighted by distill loss in teacher network (corresponding to Soft target) and student loss (corresponding to Hard target). The mathematical expression is shown as (12): \n\nwhere \u03b1 and \u03b2 are used to adjust the contributions of teacher loss and student loss. \n\nThe loss function of the knowledge distillation architecture of the joint teacher network based on feature layer consists of three parts: the distillation loss of knowledge transfer in the middle feature layer L feature , the distillation loss of the teacher network L Soft and the student network L Hard . \n\nL Total loss = \u03d5L feature + \u03b1L soft + \u03b2L hard (13) The loss of the middle feature layer L feature uses KL divergence to measure the difference between the student and teacher models. Specifically, assuming that the middle layer feature of teacher model and student model be T (x) and S(x) respectively, then the KL divergence between them can be defined as:",
            "score": 0.6095535731527714,
            "section_title": "C. LOSS DESIGN OF JOINT TEACHER KNOWLEDGE DISTILLATION BASED ON FEATURE LAYER",
            "char_start_offset": 30013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 164,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 536
                },
                {
                    "start": 539,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 671
                },
                {
                    "start": 674,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1474
                },
                {
                    "start": 1477,
                    "end": 1561
                },
                {
                    "start": 1564,
                    "end": 1870
                },
                {
                    "start": 1873,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2230
                }
            ],
            "ref_mentions": [
                {
                    "start": 531,
                    "end": 535,
                    "matchedPaperCorpusId": "215966761"
                },
                {
                    "start": 1469,
                    "end": 1473,
                    "matchedPaperCorpusId": "228819545"
                },
                {
                    "start": 1919,
                    "end": 1923,
                    "matchedPaperCorpusId": "14542261"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.243408203125
        },
        {
            "corpus_id": "267374660",
            "title": "Unleashing the Potential of Knowledge Distillation for IoT Traffic Classification",
            "text": "Knowledge distillation is a technique of ML that involves transferring knowledge from one model to another. The idea is to train a smaller, more lightweight model to mimic the behavior of a larger, more complex model by learning from the teacher's outputs [14], [15]. More specifically, the basic idea of knowledge distillation is to train the student model to predict the same outputs as the teacher model, but with fewer computational resources [16]. This is achieved by using the teacher model's outputs as ''soft targets'', also known as soft labels, during training, rather than the ''hard labels'' used in standard supervised learning. Soft targets are probability distributions over the possible outputs, rather than the actual output values themselves. By using these soft targets, the student model is encouraged to learn a more general and robust representation of the data. \n\nKnowledge distillation has been used successfully in a variety of applications, including image classification [17], object detection [18], and natural language processing [19]. It is particularly useful in scenarios where computational resources are limited, such as on mobile devices or embedded systems. Additionally, it has been shown that knowledge distillation can improve the performance of the student model, even when the teacher model is not perfect, by helping the student model learn from the teacher's mistakes [20]. \n\nThe basic mathematics of knowledge distillation can be understood through the following steps: \n\n\u2022 Softmax function: The softmax activation function is used to convert the output of a neural network (logits or scores) into probabilities. It is defined as follows: \n\nwhere z i is the input to the i th neuron, K is the total number of neurons, and e is the base of the natural logarithm. The softmax function ensures that the output probabilities sum up to one. \n\n\u2022 Temperature parameter: In knowledge distillation, the teacher model's output is modified using a temperature parameter T . The modified output probabilities are given by: \n\nwhere z i is the input to the i th neuron of the teacher model and K is the total number of neurons. The temperature parameter T controls the ''softness'' of the probabilities.",
            "score": 0.6090598766363347,
            "section_title": "A. KNOWLEDGE DISTILLATION",
            "char_start_offset": 7577,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 884
                },
                {
                    "start": 887,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1682
                },
                {
                    "start": 1685,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1879
                },
                {
                    "start": 1882,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2054
                },
                {
                    "start": 2057,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2233
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 260,
                    "matchedPaperCorpusId": "212633769"
                },
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 447,
                    "end": 451,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 998,
                    "end": 1002,
                    "matchedPaperCorpusId": "52290108"
                },
                {
                    "start": 1021,
                    "end": 1025,
                    "matchedPaperCorpusId": "195218687"
                },
                {
                    "start": 1411,
                    "end": 1415,
                    "matchedPaperCorpusId": "233296935"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.457763671875
        },
        {
            "corpus_id": "233407431",
            "title": "Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification",
            "text": "Knowledge distillation. Knowledge distillation [21] aims to transfer the \"dark\" knowledge learned from a highcapacity teacher network to a student network via soft labels. The soft labels can be the class probabilities [6,14,56] or the feature representations [34,48] output by the teacher, containing more complete structured information than the one-hot ground-truth labels. The distillation process can be formed by a \"teacher-student\" framework [21,40], a \"peer-teaching\" framework [14,28,59], or a self-distillation framework [6,25,52,56,58]. Our BAKE is mostly related to the self-distillation methods, i.e., teaching a single network using its own knowledge. However, most of them [6,25,58] only considered the knowledge of individual instances, resulting in sub-optimal learning targets. Recent works introduced to preserve the predictive consistency between intraimage (original v.s. perturbed) [52] or intra-class (images out of the same class) [56] samples. However, they only focused on pairwise images, carrying limited information compared to the ensembled batch knowledge of BAKE. More importantly, they simply defined the positive pairs using constant instance or class IDs, which may incur false supervisions as their visual features might be actually dissimilar, especially after the random crop augmentation [43]. \n\nKnowledge ensembling. It is well-known that an ensemble of multiple networks generally yields better predictions than a single network in the ensemble. The ensembling technologies aim to generate robust supervision signals via aggregating models [13,15,46] or predictions [14,27,28,40,41,47]. Several attempts leveraged the spirit of knowledge ensembling in distillation tasks, dubbed \"ensemble distillation\" methods. For example, CRD [47] and MEAL [40] proposed to enhance the soft targets by en-sembling the knowledge of multiple pre-trained teacher networks. KDCL [14] introduced to aggregate the information from multiple independent students, which are collaboratively training.",
            "score": 0.6089868475791747,
            "section_title": "Related Works",
            "char_start_offset": 4865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1332
                },
                {
                    "start": 1335,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 51,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 222,
                    "end": 225,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 225,
                    "end": 228,
                    "matchedPaperCorpusId": "214727822"
                },
                {
                    "start": 260,
                    "end": 264,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 449,
                    "end": 453,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 453,
                    "end": 456,
                    "matchedPaperCorpusId": "54447578"
                },
                {
                    "start": 486,
                    "end": 490,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 490,
                    "end": 493,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 493,
                    "end": 496,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 534,
                    "end": 537,
                    "matchedPaperCorpusId": "233714221"
                },
                {
                    "start": 537,
                    "end": 540,
                    "matchedPaperCorpusId": "70335318"
                },
                {
                    "start": 540,
                    "end": 543,
                    "matchedPaperCorpusId": "214727822"
                },
                {
                    "start": 543,
                    "end": 546,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 691,
                    "end": 694,
                    "matchedPaperCorpusId": "233714221"
                },
                {
                    "start": 694,
                    "end": 697,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 904,
                    "end": 908,
                    "matchedPaperCorpusId": "70335318"
                },
                {
                    "start": 955,
                    "end": 959,
                    "matchedPaperCorpusId": "214727822"
                },
                {
                    "start": 1327,
                    "end": 1331,
                    "matchedPaperCorpusId": "206592484"
                },
                {
                    "start": 1581,
                    "end": 1585,
                    "matchedPaperCorpusId": "219687798"
                },
                {
                    "start": 1585,
                    "end": 1588,
                    "matchedPaperCorpusId": "207930212"
                },
                {
                    "start": 1588,
                    "end": 1591,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 1607,
                    "end": 1611,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1614,
                    "end": 1617,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1617,
                    "end": 1620,
                    "matchedPaperCorpusId": "54447578"
                },
                {
                    "start": 1620,
                    "end": 1623,
                    "matchedPaperCorpusId": "221802641"
                },
                {
                    "start": 1623,
                    "end": 1626,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1770,
                    "end": 1774,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1784,
                    "end": 1788,
                    "matchedPaperCorpusId": "54447578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6611328125
        },
        {
            "corpus_id": "258811961",
            "title": "Dynamic Gesture Recognition Based on Three-Stream Coordinate Attention Network and Knowledge Distillation",
            "text": "To reduce computational complexity while improving realtime performance, we use the complex I3D network as a teacher model, and the ResNet18 network as a student model to conduct knowledge distillation training to compress the model to obtain a simple and efficient network model. As shown in Fig. 1, the input and feature fusion methods of the ResNet18 network are the same as those of the teacher network I3D network. \n\nThe distillation training process is divided into two steps: \n\n(1) Use training data to train a teacher model and save the parameters of the model; \n\n(2) The basic idea of knowledge distillation is to approximate the student network to the teacher network by minimizing the difference in the predicted distribution between the teacher network and the student network. The neural network usually converts the calculated logits of each category into classification probabilities by using the Softmax output layer, as shown in Equation ( 1): \n\nwhere, z i is the ith component of Logits, and T is the temperature parameter. The higher the temperature, the softer the inter-class classification probability will be. \n\nThe knowledge distillation loss consists of two parts: one is the cross-entropy between the classification probabilities, where the student network and the teacher network use the same temperature T, and the other is the cross-entropy loss between the classification prediction of the student network and the real label, where the temperature is 1, as shown in Equation ( 2): \n\nwhere N is the size of a small batch, and L CE represents crossentropy. \u03c3 (\u2022) represents the Softmax function, where T is the distillation temperature, y is the ground truth label of sample i, and z S \u2208 R C and T \u2208 R are the output from the network and teacher network for class C classification tasks, respectively. \n\nAlthough the teacher network is more accurate than the student network at the beginning of training, teachers still have some prediction errors. When the teacher network predicts errors, knowledge will also be transferred to the student network, which will affect the performance of the student network. Therefore, the traditional knowledge distillation loss is improved, ignoring the erroneous prediction distribution of the teacher network, and only passing the correct prediction distribution to the student network.",
            "score": 0.6083466383348042,
            "section_title": "C. KNOWLEDGE DISTILLATION OPTIMIZATION",
            "char_start_offset": 27717,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 419
                },
                {
                    "start": 422,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1132
                },
                {
                    "start": 1135,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1829
                },
                {
                    "start": 1832,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2351
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41259765625
        },
        {
            "corpus_id": "237002008",
            "title": "In-Network Flow Classification With Knowledge Distillation",
            "text": "During distillation, knowledge is transferred from the teacher model to the student by minimizing the distillation loss. However, in conventional neural networks, the well-trained model usually gives a very high probability to the correct class, while making the probabilities of all the other classes close to 0. In this case, the soft label output by the teacher model would be very similar to the ground-truth hard label, as a result providing little information beyond the ground truth label. To resolve this issue, knowledge distillation usually incorporates with another technique, called softmax temperature [1], which transforms the logit z j (input of the softmax layer) to the following probability p j of class j: \n\nwhere T is the temperature parameter. When T = 1, the distribution is the standard softmax function. As T grows, the probability distribution becomes softer (i.e., the gap between the correct class and the others becoming smaller). Such softer distribution then provides more information for the student model to distill the knowledge from the teacher model. \n\nTo enable knowledge distillation with temperature, the temperature of the soft label output by the teacher model should be the same with the temperature of the student model. Hence, the student model should now have two outputs, one using the standard softmax function (T = 1) for calculating the student loss and the other using the softmax function with the temperature T = \u03c4 . Then, the overall loss function can be expressed by \n\nwhere x is the input, W is the vector of the student model parameters, y is the ground truth label, H is the cross-entropy loss function and \u03c3 is the softmax function with temperature T . The coefficient \u03bb is used to determine the weight of the student loss and the distillation loss. By setting a large value of \u03bb, we emphasize more the knowledge transformation from the teacher model to the student model. The hyper-parameter T can be fine-tuned to adapt the learning efficiency from the teacher model.",
            "score": 0.6082057639438123,
            "section_title": "2) SOFTMAX TEMPERATURE",
            "char_start_offset": 15303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1519
                },
                {
                    "start": 1522,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68603515625
        },
        {
            "corpus_id": "246634659",
            "title": "Locally Differentially Private Distributed Deep Learning via Knowledge Distillation",
            "text": "While the data user receiving all the query results from the data owners, our proposed framework uses the knowledge distillation technique to transfer the knowledge learned from the queried teacher models to the student model. Our usage of knowledge distillation is slightly different from the it convectional usage (as described in Section 2.2), where (i) we only focus on the knowledge transfer perspective of KD, but not the model compression, thus the student model and all the teacher models could use different and arbitrary DNN architectures; and (ii) in our case, the public dataset does not have the true label information, thus cannot directly use equation (3). Hence, in our framework, the student model is trained to minimize the gap between its own predicted soft label and the aggregated soft label from the teacher models, i.e., the knowledge distillation loss: \n\nwhere z s is the soft label predicted by the student model, z t is the aggregated soft label from the teacher models, T is the temperature parameter, and \u03c3(z; T ) = sof tmax(z/T ). \n\nThe temperature parameter is usually set to 1. While T > 1, the probabilities of the classes whose normal values are near zero would be increased. To better distill the knowledge to the student, two temperature values are adopted in our KD loss (i.e., T = 1 and T = \u03c4 > 1).",
            "score": 0.6081042775404228,
            "section_title": "Build Student Model via Knowledge Transfer",
            "char_start_offset": 18896,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1335
                }
            ],
            "ref_mentions": [
                {
                    "start": 667,
                    "end": 670,
                    "matchedPaperCorpusId": "67855604"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.498779296875
        },
        {
            "corpus_id": "248848394",
            "title": "Bearing Faulty Prediction Method Based on Federated Transfer Learning and Knowledge Distillation",
            "text": "The concept of knowledge distillation (KD) was first introduced by Hinton and Dean [19] as a model compression framework which releases the parameter size of the deep learning model by constructing a teacher-student paradigm where the student network is trained to capture the information contained not only in the hard version of the true label, but also in the softer version of the teacher's output. Different from the ordinary transfer learning, the knowledge distillation accomplishes the knowledge transference tasks by altering the loss function of the student model to follow the output of the teacher model [20]. The traditional KD framework compresses one or several cumbersome networks (teachers) into a student network with a shallow structure. The framework of the conventional knowledge distillation can be categorized into two types: single teacher-based knowledge distillation and multi-teacher-based knowledge distillation [19,[21][22][23][24].",
            "score": 0.6079667124070023,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 12177,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 961
                }
            ],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 620,
                    "matchedPaperCorpusId": "220632998"
                },
                {
                    "start": 956,
                    "end": 960,
                    "matchedPaperCorpusId": "209078813"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302001953125
        },
        {
            "corpus_id": "267335043",
            "title": "Scavenging Hyena: Distilling Transformers into Long Convolution Models",
            "text": "Knowledge distillation in neural networks (Hinton et al., 2015) involves transferring information from a larger, more complex model to a smaller one while minimizing information loss. This method extends to both compressing a single larger model and consolidating insights from multiple models (ensemble) into a singular one. \n\nDistillation, a knowledge transfer method in neural networks, leverages temperature-adjusted softmax probabilities. Initially, the cumbersome model generates soft targets by applying a higher temperature in its softmax, aiding the training of a smaller distilled model. Besides mimicking soft targets, optimizing the distilled model with correct labels further enhances learning. \n\nThe training involves a weighted average of two objective functions: the first part is the Kullback-Leibler divergence with the soft targets (at higher temperature). The second part is the cross entropy loss with correct labels (at temperature 1). \n\nThis methodology allows the distilled model to effectively learn from both the nuanced information present in the soft targets generated by the larger model and the precise ground truth labels, resulting in a more compact yet knowledgeable model. \n\nOne notable example of distillation in LLMs is the Distil-BERT model: DistilBERT is 40% smaller than its parent model BERT, 60% faster than its parent model, and yet retains 97% of BERT's language capabilities. (Sanh et al., 2020) 2.4. Progressive Knowledge Transfer. \n\nWhen distillation is implemented on large models, there is a risk that knowledge transfer is not optimally passed on from the teacher model to the student model due to differences between the architectures of the teacher and student models. One approach to maximize knowledge transfer is progressive knowledge transfer: the student model is first trained only on the inputs and outputs of the first encoder block, and the student model then subsequently trains the output of the next encoder block while freezing the previous trained blocks. (Sun et al., 2020) In our case, encoder blocks are replaced by decoders as the architecture is autoregressive. (Fig. 2) \n\n3. Methods",
            "score": 0.6078135805864124,
            "section_title": "Distillation",
            "char_start_offset": 6794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2140
                },
                {
                    "start": 2143,
                    "end": 2153
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57958984375
        },
        {
            "corpus_id": "249209818",
            "title": "Parameter-Efficient and Student-Friendly Knowledge Distillation",
            "text": "3.1 Knowledge Distillation with Fine-Tuned Teacher Motivation. Compared with label smoothing, knowledge distillation can improve the training of the network for the following two reasons: First, teachers can understand the nuances of different classes, and such inter-class information brings more information than label smoothing and helps students generalize some unseen data. Second, the soft distribution of teachers constrains students' learning directions such that they will not be mistakenly overconfident. For a student network, the information it learns is obtained through this soft label. Previous works [29; 52; 20; 27] pointed out that the mismatch of teacher and student network capacity can cause the knowledge transfer difficulty of such soft label [11]. One of the solutions to reduce transfer difficulty is smoothing the teacher's output by adjusting the temperature [12]. M\u00fcller et al. [28] indicates that the label smoothness of the target provided by the teacher exerts a great influence on the student network, and the difference information between classes determines whether the student's performance can be improved. But manual conditioning of the label smoothness by the temperature is quite difficult and may cause the loss of inter-class information when the temperature is too high. In our preliminary experiments, we found that if the teacher network continues to fine-tune through the ground truth labels with the rest of the settings as the valina KD [17] , teacher's labels will be smoother, and the accuracy of the distilled student network is better, where we leave this competitive setting as the strong benchmark, namely \"KD w/ fine-tuned teacher\", in the main experiments (Section 5). This phenomenon urges us to explore the relationship between the teacher network and the student network in Section 4. \n\nTraining Objectives. As shown in Figure 1(b), different from vanilla KD [17], our distillation method requires fine-tuning the parameters of the teacher network. The teacher network needs to output soft labels to supervise the student network, it also requires ground-truth labels to training itself. Take classification task as instance, the corresponding teacher's loss is: \n\nwhere c is a class label and C denotes the set of class labels.",
            "score": 0.6074054593845899,
            "section_title": "Methodology",
            "char_start_offset": 6475,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2220
                },
                {
                    "start": 2223,
                    "end": 2286
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.525390625
        },
        {
            "corpus_id": "263135545",
            "title": "An Enhanced Low-Resolution Image Recognition Method for Traffic Environments",
            "text": "Knowledge distillation [12] is a model compression method that transforms a structurally complex and highly accurate teacher network into a structurally simplified student network through \"distillation,\" reducing model size and improving inference speed. As previously discussed, a model's expressive capacity is positively correlated with its complexity. With increased depth and channel count, the model acquires more knowledge from the dataset and performs better. However, in limited datasets, as the model's complexity continues to grow, improvements in recognition accuracy tend to plateau. In other words, overly complex models carry redundant parameters. To mitigate this redundancy, knowledge distillation employs a teacher network with a large parameter count and complex structure to fit a massive dataset during training, enhancing recognition accuracy. During deployment, a student network with a reduced parameter count and computational complexity is used for accelerated inference. The knowledge distillation process is depicted in fig. 5 and consists of the following steps: \n\nStep 1: Train the teacher network using the dataset (employing cross-entropy loss). \n\nStep 2: Keep the teacher network parameters fixed and use the teacher network's outputs as soft targets to guide the training of the student network. The student network's loss function comprises two parts: \n\n(1) Soft targets are obtained by dividing the feature vectors output by the teacher network's fully connected layer by a temperature parameter T and then passing them through a softmax layer. These soft targets and the predicted probability distribution from the student network's fully connected layer, also divided by the same temperature parameter T and passed through a softmax layer, constitute part of the loss function. \n\n(2) The other part of the loss function is the cross-entropy between the directly output predictions of the student network and the labels (also known as hard targets). \n\nStep 3: Infer with the trained student network. The key to knowledge distillation is introducing soft targets to guide the training of the student network. Soft targets are probability distributions obtained from the outputs of the trained teacher network and contain more critical information than one-hot encoded labels. The temperature parameter T is introduced to soften the probability distribution. A larger T leads to more uniform probabilities among different classes.",
            "score": 0.6070850264132395,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 15814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1091
                },
                {
                    "start": 1094,
                    "end": 1177
                },
                {
                    "start": 1180,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1815
                },
                {
                    "start": 1818,
                    "end": 1986
                },
                {
                    "start": 1989,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2311
                },
                {
                    "start": 2312,
                    "end": 2393
                },
                {
                    "start": 2394,
                    "end": 2465
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6044921875
        },
        {
            "corpus_id": "273821996",
            "title": "Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning",
            "text": "Knowledge distillation leverages additional supervisory signals from a pre-trained teacher network to aid in training a student network [15]. There are generally two categories of knowledge distillation methods: distillation from intermediate features [13,14,19,33,35,39,45,43,65,73] and distillation from logits [6,10,30,52,75]. Many studies [5,17,38,21,47,51] utilize knowledge distillation for MSA tasks with missing modalities. These approaches aim to transfer dark knowledge from teacher networks trained on complete modalities to student networks trained by missing modalities. The teacher network typically provides richer and more comprehensive feature representations than the student network. For instance, KD-Net [17] utilizes a teacher network with complete modalities to supervise the unimodal student network at both the feature and logits levels. Despite their promising results, these methods neglect precise supervision of representations, resulting in low-quality knowledge transfer. To this end, we implement hierarchical semantic and distributional alignment of the multi-scale representations of both networks to transfer knowledge effectively.",
            "score": 0.6070329867686757,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1165
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 256,
                    "end": 259,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 259,
                    "end": 262,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 262,
                    "end": 265,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 265,
                    "end": 268,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 271,
                    "end": 274,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 277,
                    "end": 280,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 313,
                    "end": 316,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 316,
                    "end": 319,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 319,
                    "end": 322,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 322,
                    "end": 325,
                    "matchedPaperCorpusId": "54436113"
                },
                {
                    "start": 325,
                    "end": 328,
                    "matchedPaperCorpusId": "5299559"
                },
                {
                    "start": 343,
                    "end": 346,
                    "matchedPaperCorpusId": "233219684"
                },
                {
                    "start": 346,
                    "end": 349,
                    "matchedPaperCorpusId": "221543802"
                },
                {
                    "start": 349,
                    "end": 352,
                    "matchedPaperCorpusId": "245445463"
                },
                {
                    "start": 355,
                    "end": 358,
                    "matchedPaperCorpusId": "263605398"
                },
                {
                    "start": 724,
                    "end": 728,
                    "matchedPaperCorpusId": "221543802"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6728515625
        },
        {
            "corpus_id": "231977039",
            "title": "A Lightweight Action Recognition Method for Unmanned-Aerial-Vehicle Video",
            "text": "Knowledge distillation [16] refers to the process of transferring the learned knowledge of the teacher network to the student network. Generally speaking, teacher networks refers to large cumbersome model, even ensemble of several models, while student network is smaller. Teacher model can learn better knowledge representation of videos, however, it can hardly deploy in real applications because their expensive computation cost. The output probability encodes the learned knowledge representation of teacher network: when model correctly predicts a class, it also assigns a smaller probability to a similar class. For example, from the output probability it can be inferred that the similarity between 'squat' and 'bend' is higher than 'squat' and 'walk'. While the true label is one-hot encoded, the correlations among classes is ignored. Therefore, to transfer knowledge from teacher network to student network, we train the student network by letting it match both the soft output of teacher network and the ground truth label. \n\nThe process of knowledge distillation is shown in Figure 2. Teacher network with larger parameters is trained at first. Here I3D model is selected as the teacher network, which is pre-trained on the Kinetics dataset, and then trained on the action recognition dataset. To further improve the performance of the teacher network, the teacher network has two streams: spatial and temporal stream, which are trained separately and then fused. Then student network is trained to fit both the soft output of teacher network and the ground truth label. Let p and q denote the output of the student network and teacher network respectively, y denotes the one-hot encoding of the true label, and the loss function is: Here \u03bb is a hyper-parameter, which controls the weight of the loss corresponding to the soft target q and the real label y. However, directly matching the output of teacher network q is not a good choice. As the probability of correct class is too high, and the probabilities of other classes are too small. As a result, it is difficult to distill from the teacher network. softmax-T is introduced here to \"soften\" the original output probability. The formula is as follows: \n\nin which z is the logit before softmax function. If T = 1, softmax-T is equivalent to standard softmax function.",
            "score": 0.6061355551460123,
            "section_title": "D. Knowledge Distillation",
            "char_start_offset": 9265,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2193
                },
                {
                    "start": 2194,
                    "end": 2220
                },
                {
                    "start": 2223,
                    "end": 2271
                },
                {
                    "start": 2272,
                    "end": 2335
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.498779296875
        },
        {
            "corpus_id": "255942245",
            "title": "Dataset Distillation: A Comprehensive Review",
            "text": "Knowledge distillation (KD) [37], [38], [39], [40] aims to transfer knowledge from a large teacher network to a smaller student network, such that the student network can preserve the performance of the teacher with reduced computational overhead. The seminal work by Hinton et al. [37] leads the student to mimic the outputs of the teacher, which can represent knowledge acquired by the teacher network. Afterward, improvements of KD have focused on four aspects: representations of knowledge, teacherstudent architectures, distillation algorithms, and distillation schemes. First, knowledge can be represented by model response/output [37], [41], features [38], [42], [43], and relation [44], [45], [46]. Second, teacher-student architectures refer to the network architectures of teacher and student models, which determines the quality of knowledge acquisition and distillation from teacher to student [40]. Third, distillation algorithms determine the ways of knowledge transfer. A simple and typical way is to match the knowledge captured by the teacher and student models directly [37], [38]. Beyond that, many different algorithms are proposed to handle more complex settings, such as adversarial distillation [47], attention-based distillation [39], and data-free distillation [48], [49]. Finally, distillation schemes control training configurations of teacher and student, and there are offline- [37], [38], online- [50], and self-distillation [51]. As for application, KD is widely used in ensemble learning [52] and model compression [38], [53], [54]. \n\nThe concept of DD is inspired by KD [18]. Specifically, DD aims at a lightweight dataset, while KD aims at a lightweight model. In this view, DD and KD are only conceptually related but technically orthogonal. It is worth noting that, similar to DD, recent data-free KD methods [48], [49], [55] are also concerned with the generation of synthetic training samples since original training datasets are unavailable. Their differences are two-fold.",
            "score": 0.6058529522298672,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1564
                },
                {
                    "start": 1567,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2012
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 50,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 643,
                    "end": 647,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 664,
                    "end": 668,
                    "matchedPaperCorpusId": "221559239"
                },
                {
                    "start": 670,
                    "end": 674,
                    "matchedPaperCorpusId": "227232038"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 695,
                    "end": 699,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 701,
                    "end": 705,
                    "matchedPaperCorpusId": "198160865"
                },
                {
                    "start": 906,
                    "end": 910,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1218,
                    "end": 1222,
                    "matchedPaperCorpusId": "53976534"
                },
                {
                    "start": 1292,
                    "end": 1296,
                    "matchedPaperCorpusId": "261514306"
                },
                {
                    "start": 1427,
                    "end": 1431,
                    "matchedPaperCorpusId": "24982157"
                },
                {
                    "start": 1455,
                    "end": 1459,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1520,
                    "end": 1524,
                    "matchedPaperCorpusId": "7350432"
                },
                {
                    "start": 1553,
                    "end": 1557,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1851,
                    "end": 1855,
                    "matchedPaperCorpusId": "261514306"
                },
                {
                    "start": 1857,
                    "end": 1861,
                    "matchedPaperCorpusId": "159041346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52294921875
        },
        {
            "corpus_id": "248243663",
            "title": "Optimizing Performance of Federated Person Re-identification: Benchmarking and Analysis",
            "text": "Besides client clustering, we adopt knowledge distillation (KD) to elevate performance and improve the convergence of distillation is a method proposed by Hinton et, al. [14] to transfer knowledge from a teacher model to a student model, where the teacher model contains more knowledge than the student model. We adopt knowledge distillation to better transfer knowledge from local models to the global model, regarding clients as teachers and the server as the student. \n\nAfter clients finish local training and upload models, we apply knowledge distillation with a public shared dataset D \u210e in the server. Figure 3b illustrates the additional steps required from knowledge distillation: (1) The server uses each trained model   of client  to generate soft labels3 \u2113  using samples of D \u210e . These soft labels represent the knowledge of clients' models. (2) Apart from model aggregation, the server aggregates these soft labels with \n\nThe server fine-tunes the global model with D \u210e and corresponding labels \u2113 to learn the distilled knowledge.",
            "score": 0.6054335419086189,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 18063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 932
                },
                {
                    "start": 935,
                    "end": 1043
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 174,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.389892578125
        },
        {
            "corpus_id": "271488200",
            "title": "A Review of Recent Hardware and Software Advances in GPU-Accelerated Edge-Computing Single-Board Computers (SBCs) for Computer Vision",
            "text": "Knowledge distillation involves transferring knowledge from a large, cumbersome model (the teacher) to a smaller, more efficient model (the student). Techniques such as Hinton's Knowledge Distillation [84] and Born-Again Networks [85] aid in this knowledge transfer. Recent advancements incorporate attention mechanisms and multi-stage distillation to enhance the student model performance and accuracy. Knowledge distillation is pivotal for transferring knowledge from large, complex models to smaller, efficient ones, facilitating deployment on resource-constrained edge-computing devices. Various approaches have evolved over time, enabling the compression of complex models without significant performance loss. Early techniques focused on transferring knowledge from a large, well-trained teacher model to a smaller student model, typically involving mimicking the teacher's behaviour through soft labels or intermediate representations. Notable early approaches include the following: Recent advancements in knowledge distillation have focused on integrating selfdistillation techniques and exploring the synergy between different distillation approaches. These developments aim to enhance the scalability and adaptability of knowledge distillation methods, enabling the efficient deployment of compact and accurate CV models in diverse edge-computing applications.",
            "score": 0.6048303391214584,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 43009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1371
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "220364494"
                },
                {
                    "start": 230,
                    "end": 234,
                    "matchedPaperCorpusId": "238203535"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3203125
        },
        {
            "corpus_id": "250096410",
            "title": "An Industrial-Grade Solution for Crop Disease Image Detection Tasks",
            "text": "After model pruning, the accuracy of the model generally decreases. Even if the pruned model is fine-tuned again, the accuracy may still have a large gap with the model before pruning. Therefore, this article can solve this problem by minimizing the accuracy loss by Knowledge Distillation (KD). Knowledge distillation uses transfer learning to supplement specific parameters missing in the small model to achieve the recognition accuracy of the large model as much as possible. Knowledge distillation can be regarded as a model compression method, where the large model is the teacher and the miniature model is the student. \n\nUsually, the traditional training process finds the excellent likelihood for the ground truth under Hard Label. In contrast, the training process of KD uses the category probabilities of the teacher model as soft targets (Labels With Probabilities) to guide the training of the student model. The knowledge describing the similarity of different categories of information can be transferred from these soft targets (Hinton et al., 2015) to improve the performance of the student model. \n\nFigure 4 shows the primary technical process of knowledge distillation. The teacher model is the original model with high training accuracy in the knowledge extraction process. The pruned original model is the student model, with a small number of parameters and a relatively simple model structure. The teacher model uses a series of hyperparameters to converge to the optimal state according to the established principles. Then, the same hyperparameters of the teacher model are used to train the student model for knowledge distillation. The distillation loss is corrected by coefficients \u03b2 for the distillation loss of the teacher model and the student model where the Hard Label (Ground Truth) can effectively reduce the possibility of errors being propagated to the student model. Measuring the similarity of student and teacher models can be expressed in Equation ( 6), L R is a function that can measure the similarity, expressed explicitly in softmax. In general, when the entropy value of the probability distribution output from softmax \n\nis relatively small, the value of negative labels is very close to 0, which contributes very little to the loss function, which leads to a reduction in the attention of the student model for negative labels during distillation, which is addressed by the temperature coefficient T in Equation ( 7).",
            "score": 0.6045892329463587,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 18368,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1113
                },
                {
                    "start": 1116,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2163
                },
                {
                    "start": 2166,
                    "end": 2463
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3193359375
        },
        {
            "corpus_id": "238253065",
            "title": "Student Helping Teacher: Teacher Evolution via Self-Knowledge Distillation",
            "text": "Knowledge distillation (Hinton, Vinyals, and Dean 2015), as one of the main network compression techniques (Wu et al. 2016;Molchanov et al. 2016), has been widely used in many tasks (Li, Jin, and Yan 2017;Liu et al. 2019;Zhang, Zhu, and Ye 2019). The traditional two-stage distillation process usually starts with a pre-trained cumbersome teacher network. Then a compact student network will be trained under the supervision of the teacher network in the form of soft predictions or intermediate representations (Romero et al. 2014;Yim et al. 2017). After the distillation, the student can master the expertise of the teacher and thus is used for efficient deployment. Such a learning process can be typically viewed as a teacher-teaching-student paradigm. Given the labeled classification dataset D = {(x i , y i )} n i=1 , the Kullback-Leibler (KL) divergence loss is used to minimize the discrepancy between the soften output probabilities of the student network and teacher network: \n\nwhere T is the temperature parameter to scale the smoothness of distribution, q(s) and q(t) denotes the soften probability produced by the student and the teacher, respectively. The predicted probabilities are calculated with a softmax layer built on logits t i , i.e., q i = sof tmax(t i /T ). A larger temperature T will make the probability distribution softer. \n\nTo train a multi-class classification network, we also minimize the traditional Cross-Entropy (CE) loss between the predicted probabilities q(s) i and the ground-truth one-hot label y i of each training sample: \n\nWith both hard labels and soft labels, the final loss function of the conventional knowledge distillation is written with the balancing parameter \u03bb as follows:",
            "score": 0.6039343414026372,
            "section_title": "Background and Notations",
            "char_start_offset": 9249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1566
                },
                {
                    "start": 1569,
                    "end": 1728
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 123,
                    "matchedPaperCorpusId": "9183542"
                },
                {
                    "start": 123,
                    "end": 145,
                    "matchedPaperCorpusId": "17240902"
                },
                {
                    "start": 182,
                    "end": 205,
                    "matchedPaperCorpusId": "267213"
                },
                {
                    "start": 205,
                    "end": 221,
                    "matchedPaperCorpusId": "73729180"
                },
                {
                    "start": 221,
                    "end": 245,
                    "matchedPaperCorpusId": "53292120"
                },
                {
                    "start": 532,
                    "end": 547,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3828125
        },
        {
            "corpus_id": "236772780",
            "title": "Semi-Supervising Learning, Transfer Learning, and Knowledge Distillation with SimCLR",
            "text": "While transfer learning focuses on transferring the weights of a big network to another big network, knowledge distillation aims to transfer the representational learning from a large and deep neural network (the teacher model) to a smaller network (the student model). If knowledge distillation is widely successful, neural networks could potentially be applied in many industry applications. \n\nThe key idea of knowledge distillation is to use soft probabilities of the teacher network ( teacher's logits ) to supervise the training of the student network in addition to available class labels [3]. Intuitively, we may think that these soft probabilities reveal more information that the teacher has discovered than just the class labels alone. Similarly, a student produces a softened class probability distribution. The Loss function then is the linear combination of the knowledge distillation loss (compared to teacher's logits), and the usual cross entropy loss (compared to ground truth label) [10]. \n\nIn SimCLR as well as in our project, however, the student network is trained on unlabelled data. So it is relying entirely on teacher's logits. The loss function is just the knowledge distillation loss.",
            "score": 0.6037885676110981,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1211
                }
            ],
            "ref_mentions": [
                {
                    "start": 1001,
                    "end": 1005,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69921875
        },
        {
            "corpus_id": "227228204",
            "title": "A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models",
            "text": "Knowledge distillation is an effective model compression technique in which a compact model (student) is trained under the supervision of a larger pre-trained model or an ensemble of models (teacher). \n\nKnowledge distillation aims to improve the performance of the student network by providing additional supervision from a teacher network. To the best of our knowledge, exploiting knowledge transfer to compress model was first proposed in C. Bucilu\u01ce et al. [4]. They trained a compressed/ensemble model of strong classifiers with pseudo-labeled data, and reproduced the output of the original larger network. However, the work is limited to shallow models. The idea has been adopted in [5] as knowledge distillation to compress deep and wide networks into shallower ones, where the compressed model mimicked the function learned by the complex model. Hinton et al. [6] popularized the concept of Knowledge Distillation to be extended to more practical uses. The work in [6] proposed knowledge distillation as a more general case of C. Bucilu\u01ce et al. [4] by adopting the concept of temperature parameter at the output of teacher. The student was trained to predict the output and the classification labels. \n\nThe main idea of knowledge distillation approach is to shift knowledge from a large teacher model into a small one by learning the class distributions output via softmax [6]. It has even been observed that the student learns much faster and more reliably if trained using outputs of teacher as soft labels, instead of one-hot-encoded labels. \n\nSince then, a number of knowledge distillation methods have been proposed, each trying to capture and transfer some characteristics of the teacher such as the representation space, decision boundary or intra-data relationship. Despite its simplicity, knowledge distillation demonstrates promising results in various image classification tasks. \n\nKnowledge distillation has proven empirically to be an effective technique for training a compact model [7], [11], [17].",
            "score": 0.6036880477022303,
            "section_title": "II. KNOWLEDGE DISTILLATION",
            "char_start_offset": 2403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 203,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1551
                },
                {
                    "start": 1554,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1897
                },
                {
                    "start": 1900,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 459,
                    "end": 462,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 688,
                    "end": 691,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 867,
                    "end": 870,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 972,
                    "end": 975,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1052,
                    "end": 1055,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1380,
                    "end": 1383,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4931640625
        },
        {
            "corpus_id": "270878760",
            "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application",
            "text": "As implied by its name, logic-based KD [43] is a distillation paradigm that employs logic within teacher models for knowledge transfer.We can formulate the general knowledge distillation loss function as follows:\n\nwhere  s ,  t \u2208 R  denote the logits output of the student and teacher network, respectively. is a temperature parameter that adjusts the smoothness of the logits. represents the number of classes.The Kullback-Leibler divergence (KLD) [43] loss can also be replaced with other functions, such as Reverse Kullback-Leibler (RKL) [20,53,65,96] distillation, Jenson-Shannon (JS) [129] distillation, etc.",
            "score": 0.6030635719004384,
            "section_title": "Logits-based KD",
            "char_start_offset": 3560,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 135,
                    "end": 212
                },
                {
                    "start": 214,
                    "end": 307
                },
                {
                    "start": 307,
                    "end": 377
                },
                {
                    "start": 377,
                    "end": 411
                },
                {
                    "start": 411,
                    "end": 613
                }
            ],
            "ref_mentions": [
                {
                    "start": 541,
                    "end": 545,
                    "matchedPaperCorpusId": "3826036"
                },
                {
                    "start": 548,
                    "end": 551,
                    "matchedPaperCorpusId": "251493147"
                },
                {
                    "start": 551,
                    "end": 554,
                    "matchedPaperCorpusId": "107645"
                },
                {
                    "start": 589,
                    "end": 594,
                    "matchedPaperCorpusId": "233168978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09722900390625
        },
        {
            "corpus_id": "236428604",
            "title": "Federated Action Recognition on Heterogeneous Embedded Devices",
            "text": "Knowledge distillation leverages information present in a larger \"teacher\" model to train a \"smaller\" student model. The soft probabilities (or logits) output from the teacher convey additional information to the student [27]: rather than just using training labels, students can learn more by understanding the relation between different classes. The additional information from the soft probabilities can help the student network identify the decision boundaries better if multiple classes have high soft probabilities. Concretely, given an input x, the teacher model computes a vector of scores (or logits) z z z t (x) = [z t 1 (x), z t 2 (x) . . . z t K (x)] for K classes. We define the knowledge distillation loss L KD as the Mean Squared Error between the logits from the teacher model z z z t and the student model z z z s , i.e., L KD = z z z t (x) \u2212 z z z s (x) \n\n2 . \n\nThe overall loss function is a combination of two loss functions, L = \u03b1L cls +(1\u2212\u03b1)L KD where L cls is the conventional crossentropy loss which is computed for the predictions made by the student and the ground truth corresponding to the input x. \n\nThe teacher model cannot effectively transfer its knowledge to the student if the size gap between them is large [28]. To alleviate this, the knowledge distillation is done in steps and an intermediate-sized model, the teaching assistant (TA), is introduced. Thus, in the first round of a teacher-TA-student knowledge distillation algorithm, the TA distils knowledge from the teacher model, with the loss function \n\nSubsequently, the student distils the knowledge that is gained by the TA and is trained with the loss function \n\nHere z z z T A refers to the vector of logits computed by the TA, \n\n2 , and L T A cls and L S cls are the crossentropy losses for the TA and student model respectively, and these are calculated considering the ground truth to be the output of the teacher for the input x.",
            "score": 0.6025195872894169,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 12088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 871
                },
                {
                    "start": 874,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1542
                },
                {
                    "start": 1545,
                    "end": 1655
                },
                {
                    "start": 1658,
                    "end": 1723
                },
                {
                    "start": 1726,
                    "end": 1929
                }
            ],
            "ref_mentions": [
                {
                    "start": 221,
                    "end": 225,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1242,
                    "end": 1246,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5263671875
        },
        {
            "corpus_id": "266555759",
            "title": "PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models",
            "text": "Knowledge Distillation aims to transfer knowledge from a large, complex model (referred to as the teacher model) to a smaller, more lightweight model (referred to as the student model) [15]. The student model is trained to mimic the behavior and predictions of the teacher model by learning the embedded information in the teacher's logits. This process  has been shown to be effective in improving the performance of compact models in hopes of achieving comparable performance to their larger counterparts. The problem definition of using KD for model compression is as below. Problem Definition. Given a trained large teacher network f (x; \u03b8), where x is the input to the network and \u03b8 is the parameters. The goal of knowledge distillation is to learn a new set of parameters \u03b8 \u2032 for a shallower student network f (x; \u03b8 \u2032 ), such that the student network achieves similar performance to the teacher, with much lower computational cost. \n\nThe original knowledge distillation work primarily focuses on single-label classification. In this context, a custom loss function L is designed that considers both the hard labels from ground truth and soft labels from a teacher model. Importantly, the soft labels are obtained using a T-temperature softmax activation function. \n\nThe softmax activation function returns probabilities P (z i , T ) is defined as: \n\nTherefore, the complete loss L can be represented as: \n\nL = \u03b1L soft + \u03b2L hard (6) where \u03b1 and \u03b2 are hyperparameters, t i are the logits from the teacher model, s i are the logits from the student model, c i is the ground truth for the i-th instance, N is the total number of instances, and T is the temperature parameter for softmax. \n\nIn this work, we extend the original KD approach by training pattern-specific teacher models and designing a softsigmoid activation function for multi-label knowledge distillation (Section III-D1). By implementing the proposed approach in memory access prediction models, achieving efficient compression becomes possible by transferring the expertise of a large and intricate teacher model to a smaller and more streamlined student model. This approach allows for significant memory savings without compromising on performance and accuracy, as the student model can closely match the capabilities of the teacher model [25].",
            "score": 0.6024818950671004,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 8704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 937
                },
                {
                    "start": 940,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1269
                },
                {
                    "start": 1272,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1409
                },
                {
                    "start": 1412,
                    "end": 1689
                },
                {
                    "start": 1692,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2315
                }
            ],
            "ref_mentions": [
                {
                    "start": 1434,
                    "end": 1437,
                    "matchedPaperCorpusId": "3759373"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79345703125
        },
        {
            "corpus_id": "269157594",
            "title": "Comprehensive Survey of Model Compression and Speed up for Vision Transformers",
            "text": "Knowledge distillation techniques, such as soft and hard distillation, facilitate the transfer of knowledge from a complex 'teacher' model to a simpler 'student' model.Soft distillation focuses on minimizing the Kullback-Leibler (KL) divergence between the softened logits (or outputs) of both the teacher and student models.This is formally captured by the distillation objective:\n\nwhere LCE denotes the cross-entropy loss, \u03c8 represents the softmax function, Zt and Zs are the logits from the teacher and student models, respectively, \u03c4 is the temperature parameter enhancing softness of distributions, and \u03bb balances the contributions of the KL divergence and the cross-entropy loss.Conversely, hard distillation uses the teacher's predictions as definitive labels for training the student, simplifying the process by directly comparing the student's predictions against these labels:\n\nwhere yt = argmaxcZt(c) represents the hard label decision by the teacher model.\n\nThe DeiT [27] method introduces a novel approach specific to Transformers, incorporating a 'distillation token' into the architecture that functions analogously to the class token but focuses on mimicking the teacher's predictions.This mechanism allows for a direct interaction between the distillation token and other components through the self-attention layers, demonstrating superior performance in distillation.Our experimental setup involves applying the DeiT framework for knowledge distillation on the CIFAR dataset, adjusting for computational resource constraints.",
            "score": 0.6020487263612545,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 381
                },
                {
                    "start": 383,
                    "end": 685
                },
                {
                    "start": 685,
                    "end": 886
                },
                {
                    "start": 888,
                    "end": 968
                },
                {
                    "start": 970,
                    "end": 1201
                },
                {
                    "start": 1201,
                    "end": 1386
                },
                {
                    "start": 1386,
                    "end": 1544
                }
            ],
            "ref_mentions": [
                {
                    "start": 979,
                    "end": 983,
                    "matchedPaperCorpusId": "229363322"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34423828125
        },
        {
            "corpus_id": "269214409",
            "title": "KDk: A Defense Mechanism Against Label Inference Attacks in Vertical Federated Learning",
            "text": "Knowledge Distillation (KD, for short) is an ML model compression technique, in which the knowledge from a complex model, or \"teacher\" model, is transferred to a smaller and more efficient model, known as the \"student\" model without a significant drop in accuracy [9].The general idea was first presented by Bucilua et al. in 2006 [2] and modeled in its current known form in 2014 by Hinton et al. [10] who found it easier to train a classifier using the outputs of another classifier as target values than using actual ground-truth labels.The teacher network outputs are represented by the so-called soft probabilities that contain more information about a data point than just the class label (or hard predictions) and are the input of the student network.\n\nIn practice, given an input  the teacher network produces a vector of scores\n\nthat are converted into probabilities:\n\nHinton et al. [10] proposed to modify these probabilities in soft probabilities as following:\n\nwhere  is a hyperparameter.A student network will produce a softened class probability distribution, p ().The loss for the student network is a linear combination of the cross entropy loss, namely L  and a knowledge distillation loss L  : Figure 3 shows the generic architecture of the KD using the teacher-student model.Thanks to the distillation algorithm the student mimics the teacher network learning the relationship between different classes discovered by the teacher model that contains information beyond the ground truth labels.",
            "score": 0.6020410158331874,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 13636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 540
                },
                {
                    "start": 540,
                    "end": 758
                },
                {
                    "start": 760,
                    "end": 836
                },
                {
                    "start": 838,
                    "end": 876
                },
                {
                    "start": 878,
                    "end": 971
                },
                {
                    "start": 973,
                    "end": 1000
                },
                {
                    "start": 1000,
                    "end": 1079
                },
                {
                    "start": 1079,
                    "end": 1294
                },
                {
                    "start": 1294,
                    "end": 1511
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 331,
                    "end": 334,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.368408203125
        },
        {
            "corpus_id": "251741056",
            "title": "Design Automation for Fast, Lightweight, and Effective Deep Learning Models: A Survey",
            "text": "Knowledge distillation (KD) is extended from knowledge transfer (KT) [143] by Ba and Caruana [144] to compress a cumbersome network (teacher) into a smaller and simpler network (student). This is done by making the student model mimic the function learned by the teacher model in order to achieve a competitive accuracy. It is later formally popularized by Hinton et al. [21] as a student-teacher paradigm, where the knowledge is transferred from the teacher to the student by minimizing the difference between the logits (features before the final softmax) of the teacher and student. In many situations, the performance of the teacher is almost perfect with a very high classification probability for the correct class and flat probabilities for the other classes. Therefore, the teacher is not able to provide much more information than the ground truth labels. Hinton et al. [21] introduce the concept of softmax temperature to transfer knowledge, which can better deliver the information of which classes the teacher find similar to the correct class. Formally, given the logits of the teacher model, the classification probability p i of the class i is: \n\nwhere \u03c4 is the temperature parameter. It controls how soft the labels from the teacher are. The soft labels together with the ground truth labels are used to supervise a compact student model. \n\nVanilla knowledge distillation mostly focuses on transferring knowledge to a student model with a fixed small architecture, which is manually designed in advance. However, different teachers and tasks favour different student architectures, and hand-crafted architectures are prone to be suboptimal. Considering these limitations, there is a growing trend to automate the architecture design of a student model [145], [146], [147], [148], [149], [150], [151]. The ground-truth labels are combined with the distillation labels to guide the automatic design process. AKDNet [145] proposes to search optimal student architectures for distilling a given teacher by RL-based NAS. It adopts the efficient search space of [48] and designs a KD-guided reward with a teacher network.",
            "score": 0.6012195559884274,
            "section_title": "B. Automated Knowledge Distillation",
            "char_start_offset": 84556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 74,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 93,
                    "end": 98,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 371,
                    "end": 375,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 879,
                    "end": 883,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1768,
                    "end": 1773,
                    "matchedPaperCorpusId": "208175624"
                },
                {
                    "start": 1782,
                    "end": 1787,
                    "matchedPaperCorpusId": "235428017"
                },
                {
                    "start": 1810,
                    "end": 1815,
                    "matchedPaperCorpusId": "233877900"
                },
                {
                    "start": 1929,
                    "end": 1934,
                    "matchedPaperCorpusId": "208175624"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.475341796875
        },
        {
            "corpus_id": "221246388",
            "title": "Rectified Decision Trees: Exploring the Landscape of Interpretable and Effective Machine Learning",
            "text": "Knowledge distillation (KD) is widely adopted in model compression, which compresses a big model (e.g., DNNs) into a smaller one, where the compressed model preserves the function learned by the complex model. KD can be considered to be motivated by the transfer learning, which transfers knowledge learned by a large teacher model into a smaller student model by learning the class distribution [18]. \n\nThe first work was proposed by Hinton et al., where a teacher-student learning paradigm was adopted [18]. Specifically, softened logits obtained from the teacher DNN based on the temperature function is used to teach a small student DNN. After that, the focus of past work has been either on (1) improving the performance or (2) finding new applications for the idea. In general, to improve the performance, prior works usually introduced additional loss terms on intermediate feature maps of the student to bring them closer to those of the teacher [21], [22]. Recently, KD was also adopted in the study of interpretability. In [20], they used a trained DNN to create a more explainable model in the form of soft decision trees. However, soft decision trees [23] is more like a treeshape DNN and its decision path is probabilistic rather than deterministic, therefore its interpretability is much weaker than that of standard decision trees. Besides, the proposed KD method in [20] requires the backpropagation of the student model, therefore it can not be adopted directly to the standard decision trees.",
            "score": 0.6009909525622247,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 6399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1510
                }
            ],
            "ref_mentions": [
                {
                    "start": 396,
                    "end": 400,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 504,
                    "end": 508,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 954,
                    "end": 958,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 960,
                    "end": 964,
                    "matchedPaperCorpusId": "73729180"
                },
                {
                    "start": 1033,
                    "end": 1037,
                    "matchedPaperCorpusId": "3976789"
                },
                {
                    "start": 1163,
                    "end": 1167,
                    "matchedPaperCorpusId": "9691881"
                },
                {
                    "start": 1382,
                    "end": 1386,
                    "matchedPaperCorpusId": "3976789"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30517578125
        },
        {
            "corpus_id": "252683187",
            "title": "To Improve Is to Change: Towards Improving Mood Prediction by Learning Changes in Emotion",
            "text": "In addition to model fusion and end-to-end optimisation, we employ knowledge distillation [12] to transfer knowledge from a larger teacher network to a smaller student network. Fig. 4 (right) presents the Teacher-Student Network, where the teacher and student networks are as described in Sec. 3.2 and 3.1, respectively. The pre-trained teacher network utilizing both mood and \u0394 labels (2-CNN+MLP) distills knowledge, while training the student network (1-CNN) only requires mood labels. As inference is again based on the student network, \u0394 labels are not required during test time. \n\nThe student model's SoftMax layer involves a hyper-parameter called the temperature  , which controls the smoothness of the output probabilities. Setting  > 1 increases the weight of smaller logit 1 values, thus revealing more information about inter-class relations than the one-hot labels [12]. The Kullback-Leibler (KL) divergence is used to compute the distillation loss, while sparse categorical cross-entropy is used as the student loss function. The overall loss of the teacher-student model    is the weighted sum of the student loss  stu and distillation loss  dis : \n\nwhere  is a training hyper-parameter. The fine-tuned hyperparameters include a batch size \u2208 {16, 64, 128},  \u2208 {3, 5, 7} and  \u2208 {0.05, 0.1, 0.15, 0.2, 0.25, 0.3}.",
            "score": 0.5995353367111093,
            "section_title": "Teacher-Student Network",
            "char_start_offset": 11693,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1325
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52587890625
        },
        {
            "corpus_id": "252968120",
            "title": "On effects of Knowledge Distillation on Transfer Learning",
            "text": "Knowledge distillation is a way to transfer knowledge from large ensembles of strong models to smaller/faster networks. Knowledge distillation consists of two different networks, called the teacher network and the student network. Teacher networks are generally large ensembles of networks or a single neural network larger than student networks. The teacher network, generally pre-trained in vanilla form, takes in data and outputs logits, which the student network tries to mimic. Logits are the values we get as an output of the neural networks. Logits are generally passed through normalization layers such as Softmax. The softmax layer converts the real values to a distribution of probabilities for K classes that add up to 1. The loss function compares the output of both teacher and student models and provides a loss value used to iterate and train the Student through Backpropagation [37]. The architecture of vanilla knowledge distillation (or the teacher-student framework) is shown in Figure 1 The student's output is compared with the teacher's output instead of the ground truth that generally occurs during transfer learning. The student network is then optimized and trained with the signal from the loss function. The teacher network is frozen and is not trained during this process. This figure is adapted from [18].\n\nDuring training, a batch of images(x) is passed through the network to produce a vector of raw outputs(z(x)). If there are K classes from which the model has to choose, the dimension of the raw output is also K. To give the probabilities for each class, the raw outputs are passed through normalization layers like Softmax. For an input x and the raw output vector z(x) = [z 1 (x), z 2 (x)...z K (x)] where K is the number of classes, i th output from the Softmax function is:\n\nwhere j ranges from 0 to K and i is an instance of j.\n\nFurthermore, as proposed in a seminal paper [14], we can use the temperature scaling [14] in Softmax to obtain softer probability predictions(p i (x : T)) that result in a better distillation of knowledge compared to the hard labels of\n\nwhere T refers to the temperature parameter.\n\nThe temperature parameter T in Equation 1.2 controls the softness of",
            "score": 0.5977335075657191,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 24585,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 894,
                    "end": 898,
                    "matchedPaperCorpusId": "121072881"
                },
                {
                    "start": 1330,
                    "end": 1334,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1914,
                    "end": 1918,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6298828125
        },
        {
            "corpus_id": "270094801",
            "title": "Aligning in a Compact Space: Contrastive Knowledge Distillation between Heterogeneous Architectures",
            "text": "The recent methods for knowledge distillation can be roughly categorized into two aspects, i.e., logits-based methods and feature-based methods.\n\nLogits-based knowledge distillation [18,19,20] enhances a smaller student model by transferring soft targets from a larger teacher model.Hinton et al. [1] introduce a method to soften logits in the softmax function, guiding the students to acquire knowledge from the teacher's prediction.Zhao et al. [2] decomposes the logit-based distillation loss into the target domain and the non-target domain, so as to learn the dark knowledge in the non-target domain.Recognizing the challenge for students to learn from stronger teachers, Huang et al. [3] suggest a relation-based loss to mitigate the strict match requirement of KL divergence.Sun et al. [21] highlight issues with a shared temperature in KD and propose logit normalization to allow students to focus more on the teacher's logits internal relations.Hao et al. [4] present a KD framework for heterogeneous architectures, employing projectors to align student features with teacher outputs.\n\nFeature-based knowledge distillation [22,23,24] involves transferring knowledge within the feature space.FitNet [5] introduces a method that utilizes features from intermediate layers of a teacher model to guide the learning process of the student model.Unlike conventional point-to-point knowledge transfer methods, RKD [7] proposes learning the relations between output feature maps.CRD [8] employs a contrastive learning approach to maximize the lower bound of mutual information between teacher and student.FSPNet [25] contends that the variations in features between each layer represent learnable knowledge.CC [6], also based on relational knowledge distillation, proposes a generalized kernel method based on Taylor series expansion to better capture relationships between features.CAT-KD [26] propose a highly interpretable knowledge distillation method based on class activation map.Yang et al. [27] believes that directly mimicking the teacher is not necessary to enhance the expressive power of student features.Instead, they propose MGD which initially masks student features randomly and then employs a simple block to generate complete teacher features from the masked features.",
            "score": 0.597275713127942,
            "section_title": "Knowledge distillation",
            "char_start_offset": 4251,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 146,
                    "end": 283
                },
                {
                    "start": 283,
                    "end": 434
                },
                {
                    "start": 434,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 781
                },
                {
                    "start": 781,
                    "end": 953
                },
                {
                    "start": 953,
                    "end": 1092
                },
                {
                    "start": 1094,
                    "end": 1199
                },
                {
                    "start": 1199,
                    "end": 1348
                },
                {
                    "start": 1348,
                    "end": 1479
                },
                {
                    "start": 1479,
                    "end": 1605
                },
                {
                    "start": 1605,
                    "end": 1707
                },
                {
                    "start": 1707,
                    "end": 1883
                },
                {
                    "start": 1883,
                    "end": 1986
                },
                {
                    "start": 1986,
                    "end": 2117
                },
                {
                    "start": 2117,
                    "end": 2286
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 186,
                    "matchedPaperCorpusId": "245006036"
                },
                {
                    "start": 189,
                    "end": 192,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 446,
                    "end": 449,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 689,
                    "end": 692,
                    "matchedPaperCorpusId": "248986690"
                },
                {
                    "start": 792,
                    "end": 796,
                    "matchedPaperCorpusId": "268247468"
                },
                {
                    "start": 964,
                    "end": 967,
                    "matchedPaperCorpusId": "265871679"
                },
                {
                    "start": 1131,
                    "end": 1135,
                    "matchedPaperCorpusId": "253244027"
                },
                {
                    "start": 1138,
                    "end": 1141,
                    "matchedPaperCorpusId": "248986384"
                },
                {
                    "start": 1206,
                    "end": 1209,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1415,
                    "end": 1418,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1483,
                    "end": 1486,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1612,
                    "end": 1616,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1710,
                    "end": 1713,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1890,
                    "end": 1894,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 1998,
                    "end": 2002,
                    "matchedPaperCorpusId": "248506080"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26904296875
        },
        {
            "corpus_id": "258486778",
            "title": "A First Look at the Impact of Distillation Hyper-Parameters in Federated Knowledge Distillation",
            "text": "Self distillation student/ teacher where T is the \"temperature\" parameter that is usually set to 1 in the regular softmax, whereas in the distillation setting, it is used to control the softness of the probability distribution over classes. \n\nThere are several variants of distillation, using different types of loss functions and different options for what dataset is used to distill the knowledge to the student model (called transfer set or proxy dataset). For example, the transfer set could be a large public unlabeled dataset, a public labeled dataset, or even the teacher's original training data. If the transfer set is labeled, the student can be trained using a linear combination of two loss functions: \n\nwhere  and (1\u2212) are the weights assigned to the Kullback-Leibler (KL) divergence (i.e., distillation loss) and the cross entropy (CE) loss, respectively,  is the true label hot encoded,   and   are the class probabilities (soft targets) predicted by the student model and the teacher model, respectively. Figure 1 illustrates the knowledge distillation pipeline. Distillation approaches. The distillation literature has investigated three learning approaches of knowledge distillation: offline distillation, online distillation, and self-distillation. Figure 2 summarizes the distillation approaches. We provide a brief description below; for details, we refer the reader to a recent survey [8]. \n\nThe offline distillation is a traditional way commonly used for model compression. A pre-trained teacher is used to train an untrained student using a loss term that encourages the student's predictions to match the predictions of the teacher model [10]. Most previous knowledge distillation works use the offline method [8]. In the online distillation, both the teacher and the student are untrained, and they learn collaboratively and teach each other throughout the training process [1,27]. In self-distillation, the same networks are used for the teacher and the student models, and the knowledge is transferred in the same model, from the deeper layers to the shallow layers [26].",
            "score": 0.5970229679916012,
            "section_title": "Untrained student",
            "char_start_offset": 4704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2099
                }
            ],
            "ref_mentions": [
                {
                    "start": 1407,
                    "end": 1410,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1735,
                    "end": 1738,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1903,
                    "end": 1906,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.457763671875
        },
        {
            "corpus_id": "125985701",
            "title": "Knowledge Distillation via Route Constrained Optimization",
            "text": "of models could be transferred to the other single model. Then Hinton et al. [10] further claimed that knowledge distillation (KD) could transfer distilled knowledge to student network efficiently. By increasing the temperature, the logits (the inputs to the final softmax) contain richer information than one-hot labels. Afterward, [14] proposed to learn the curriculum from data by a network called Mentor-Net. [18] adopted a method to learn from noisy labels.\n\nLearning Representation from Hint. Hint-based learning is often used for open-set classification such as face recognition and person Re-identification. FitNet [24] firstly introduced more supervision by exploiting intermediatelevel feature maps from the hidden layers of teacher to guide training process of student. Afterward, Zagoruyko et al. [30] proposed the method to transfer attention maps from teacher to student. Yim et al. [29] defined the distilled knowledge from teacher network as the flow of the solution process (FSP), which is calculated by the inner product between feature maps from two selected layers.\n\nPrevious knowledge transfer methods only supervise student with converged teacher, thus fail to capture the knowledge during teacher's training process. Our work differs from existing approaches in that we supervise student with the knowledge transferred from teacher's training trajectory.",
            "score": 0.5968781421032636,
            "section_title": "Related Work",
            "char_start_offset": 5767,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 333,
                    "end": 337,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 413,
                    "end": 417,
                    "matchedPaperCorpusId": "14659675"
                },
                {
                    "start": 897,
                    "end": 901,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50830078125
        },
        {
            "corpus_id": "227013462",
            "title": "Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation",
            "text": "Knowledge Distillation (KD) [3,7] is a contemporary technique for transferring learning across neural network models. Typically, knowledge from one or more complex and deep models (called Teachers) is distilled into a relatively lightweight model (called Student). The core idea of Knowledge Distillation, as discussed in the seminal paper by Hinton et al. [7], is to transfer the (input to output) learned mapping function from Teacher to Student via shar-ing the \"dark knowledge\" extracted by the Teacher on the training images. This typically is achieved via matching the soft targets (or soft labels, i.e., output of softmax layer) predicted by the Student to that of the Teacher for the same inputs. This is the distillation mechanism that enables transfer of the better generalization capability (i.e., the \"knowledge\") of the Teacher to the Student. Thus, Knowledge Distillation has established itself as a very useful and practical tool because of its simplicity and potential. \n\nThe samples used for performing distillation constitute the \"Transfer set\", which is typically required to be constructed using the data sampled from the target distribution. Therefore, the most commonly used transfer set is the original training dataset on which the Teacher model was trained. However, this requirement has been identified as a limitation (e.g. [17,15]) since it is common now-a-days that many popular pre-trained models are released without providing access to the training data (e.g. Facebook's Deepface model trained on 4M confidential face images). This is due to one or more practical constraints such as (i) privacy (e.g. models trained on patients' data from hospitals), (ii) property (proprietary data of companies that invest on collection and annotation), and (iii) transience (observations from the training of a reinforcement learning environment do not exist). \n\nTo handle this \"data-free\" (or zero-shot) distillation scenario, most of the approaches broadly follow either of the two ways: (i) compose a synthetic transfer set by directly utilizing the trained Teacher model that acts as a proxy to the target data (e.g.",
            "score": 0.5968680543051154,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1879
                },
                {
                    "start": 1882,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 31,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 31,
                    "end": 33,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 357,
                    "end": 360,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1351,
                    "end": 1355,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 1355,
                    "end": 1358,
                    "matchedPaperCorpusId": "162183830"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73193359375
        },
        {
            "corpus_id": "270377282",
            "title": "Research on temperature detection method of liquor distilling pot feeding operation based on a compressed algorithm",
            "text": "However, negative labels also contain a great deal of information. Some negative labels may correspond to higher probabilities than others, and even contain more information than positive labels. In order to obtain more information from negative labels, knowledge distillation introduces a temperature variable, denoted by T. By adjusting the value of T, the entropy of the Softmax output probability distribution can be increased, thus amplifying the information carried by negative labels. \n\nwhere q i is the \"softened\" probability vector, obtained by exponential operation and normalization; Z i indicates the logit value for the current category; j denotes the number of output nodes (number of categories); Z j represents the logit value for each category output by the full connectivity layer; T is the temperature parameter, and when T = 1, the function is the original Softmax function. The knowledge distillation process was shown in the knowledge distillation model in Fig. 1 22,23 . First, the teacher network model is trained and the logits output of the teacher network is divided by the T after doing Softmax calculation to get the soft label value. Then, the same training as for the teacher network is performed to get the logits output. Next, a two-step calculation is performed. The first step is to perform a Softmax calculation by dividing the logits output of the student network by the same T as the teacher model to obtain the soft prediction. Soft predictions were compared to soft labels, and the difference between the two probability distributions was measured using the distillation loss function. The second step is to perform Softmax computation on the logits output of the student network to get the hard predicted values. The hard predicted values were compared to the actual labels and the difference between them was measured using the student loss function. The two parts of the loss function are added together to get the total loss function, which is calculated as where V loss is the value of total loss function; V loss-SL is the value of the student loss function; V loss-KD is the value of distillation loss function; \u03b1 is the scaling factor, which is used to adjust the weights of the two loss functions. When \u03b1 is equal to zero, this corresponds to the network not being distilled and trained using only the student loss function.",
            "score": 0.5966608025173458,
            "section_title": "Fermented grains surface temperature detection based on YOLO v5n",
            "char_start_offset": 12832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2246
                },
                {
                    "start": 2247,
                    "end": 2373
                }
            ],
            "ref_mentions": [
                {
                    "start": 989,
                    "end": 991,
                    "matchedPaperCorpusId": "264462123"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39501953125
        },
        {
            "corpus_id": "215416219",
            "title": "LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression",
            "text": "Knowledge distillation is first proposed by (Hinton et al., 2015), which trains a compact or smaller model to approximate the function learned by a large and complex model. A preliminary step of knowledge distillation is to train a deep network (the teacher model) that automatically generates soft labels for training instances. This \"synthetic\" label is then used to train a smaller network (the student model), which assimilates the function that is learned by the teacher model. Chen et al. (2017) successfully applied knowledge distillation to object detection tasks by introducing several modifications, including a weighted cross-entropy loss, a teacher bounded loss, and adaptation layers to model intermediate teacher distributions. Li et al. (2017) developed a framework to learn from noisy labels, where the knowledge learned from a clean dataset and semantic knowledge graph were leveraged to correct the wrong labels. Anil et al. (2018) proposed online distillation, a variant of knowledge distillation which enabled extra parallelism for training large-scale data. In addition, knowledge distillation is also useful for aggregating model ensembles into a single model by treating the ensemble model as a teacher.",
            "score": 0.5959760151895949,
            "section_title": "Knowledge distillation",
            "char_start_offset": 8235,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1226
                }
            ],
            "ref_mentions": [
                {
                    "start": 483,
                    "end": 501,
                    "matchedPaperCorpusId": "29308926"
                },
                {
                    "start": 742,
                    "end": 758,
                    "matchedPaperCorpusId": "14659675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60986328125
        },
        {
            "corpus_id": "219531383",
            "title": "An Overview of Neural Network Compression",
            "text": "Knowledge distillation (also known as knowledge distillation) involves learning a smaller network from a large network using supervision from the larger network and minimizing the entropy, distance or divergence between their probabilistic estimates.\n\nTo our knowledge, Bucilu\u01ce et al. [16] first explored the idea of reducing model size by learning a student network from an ensemble of models. They use a teacher network to label a large amount of unlabeled data and train a student network using supervision from the pseudo labels provided by the teacher. They find performance is close to the original ensemble with 1000 times smaller network.\n\nHinton et al. [72] a neural network knowledge distillation approach where a relatively small model (2-hidden layer with 800 hidden units and ReLU activations) is trained using supervision (class probability outputs) for the original \"teacher\" model (2-hidden layer, 1200 hidden units). They showed that learning from the larger network outperformed the smaller network learning from scratch in the standard supervised classification setup. In the case of learning from ensemble, the average class probability is used as the target.\n\nThe cross entropy loss is used between the class probability outputs of the student output y S and one-hot target y and a second term is used to ensure that the student representation z s is similar to the teacher output z T . This is expressed in terms of KL divergence as,\n\nwhere \u03c1 is the temperature, \u03b1 balances between both terms, and \u03c6 represents the softmax function. The\n\nand a constant entropy H \u03c6( z T \u03c1 ) . The idea of training a student network on the logit outputs (i.e log of the predicted probabilities) of the teacher to gain more information from the teacher network can be attributed to the work of Ba and Caruana [7]. By using logits, as opposed to a softmax normalization across class probabilities for example, the student network better learns the relationship between each class on a log-scale which is more forgiving than the softmax when the differences in probabilities are large.",
            "score": 0.595970259315111,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 65279,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1812,
                    "end": 1815,
                    "matchedPaperCorpusId": "11536917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326904296875
        },
        {
            "corpus_id": "238227092",
            "title": "Prune Your Model Before Distill It",
            "text": "Knowledge distillation (KD) [20] transfers the knowledge from the strong teacher network to a smaller student network. The student network is trained with soft targets provided by the teacher network and some intermediate features [40,53,57]. There are variations of KD such as KD using GAN [52], Jacobian matching KD [5,45], distillation of activation boundaries [19], contrastive distillation [49], and distillation from graph neural networks [23,53]. \n\nRecently, many works have reported that the large gap between student and teacher causes degradation in student network performance [36]. Cho and Hariharan showed that the less-trained network transfers better knowledge to a small network [4]. Park et al. [37] proposed a student-aware teacher learning to transfer the teacher's knowledge effectively. In this paper, we provide an extremely simple way to generate a student-friendly teacher using unstructured pruning.",
            "score": 0.595707350157268,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 4528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 924
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 291,
                    "end": 295,
                    "matchedPaperCorpusId": "4916078"
                },
                {
                    "start": 318,
                    "end": 321,
                    "matchedPaperCorpusId": "21596346"
                },
                {
                    "start": 321,
                    "end": 324,
                    "matchedPaperCorpusId": "3603145"
                },
                {
                    "start": 395,
                    "end": 399,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 695,
                    "end": 698,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 712,
                    "end": 716,
                    "matchedPaperCorpusId": "231925118"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360595703125
        },
        {
            "corpus_id": "229923739",
            "title": "Towards Zero-Shot Knowledge Distillation for Natural Language Processing",
            "text": "Knowledge Distillation KD (Hinton et al., 2015) is a well-known deep learning technique to transfer the knowledge from an already trained large teacher model to a smaller student network. KD adds a new loss function to the student's regular training loss over the training labels. This new loss function aims at matching the smoothened output probabilities of the student with those of the teacher. More specifically, the training data is fed into the teacher model and the teacher logits are obtained. These are fed, typically, into a softmax function and the temperature parameter is adjusted to smoothen the resulting label distribution. The training loss function for the KD algorithm is as following:\n\nwhere H can be the cross-entropy or any other valid loss function and z s and z t are the student and teacher logits respectively. We will use the Kullback-Leibler (KL) divergence in our algorithm between the teacher and student logits. \u03c3 is the softmax function, and \u03c4 and \u03b1 are training parameters.",
            "score": 0.5950780920547074,
            "section_title": "Related Work",
            "char_start_offset": 4141,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4931640625
        },
        {
            "corpus_id": "227231666",
            "title": "Knowledge Base Embedding By Cooperative Knowledge Distillation",
            "text": "Following objective O2 (see Section 3.1), the knowledge distillation is cooperatively conducted on the set of n KBs. At each learning step, each KB model M i takes turns in the student-teacher process. As a teacher, the model distills its knowledge background through class prediction estimates L i s which are used as soft labels by the other student KBs to compute their mimicry loss function L j KD , j = 1 . . . n, j = i. Mutually, as a student, M i model uses in its own mimicry loss L i KD soft labels distilled from the other KB teachers through L j s , j = 1 . . . n, j = i. From the perspective of KB i , the distillation loss function L i KD is formalized as the sum of two losses related to relation distillation L ij KDr and entity distillation L ij KDe from teacher network j to student network i as follows:\n\nFollowing the relation (resp. entity) distillation principles, the distillation function L ij KDr (resp. L ij KDe ) quantifies the match of each student network relation (resp. entity) prediction outputs using soft labels provided by the teacher networks with respect to the plausibility of the embeddings estimate given by the corresponding supervised classification functions f j on ground truth labels. Relation and entity confidence scores used by the student and the teachers for prediction are obtained by converting the triple plausibility scores f i and f j on triples involving seed aligned relations and seed aligned entities as detailed in the following. Relation distillation. A relation distillation favors the student model M i to mimic the teacher model M j on the relation prediction outputs over the set of aligned relations r \u2208 I r (i, j) such as triples (e j 1 , r, e j 2 ) and (e i 1 , r, e i 2 ) have close plausibility scores. Thus, L ij KDr is computed as follows:\n\nwhere D is the distillation function which can be defined in several ways (Sau and Balasubramanian, 2016) such as the L2 loss (Ba and Caruana, 2014) or Kullback-",
            "score": 0.5947068435874976,
            "section_title": "Cooperative knowledge distillation loss",
            "char_start_offset": 16619,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5751953125
        },
        {
            "corpus_id": "267897636",
            "title": "Practical Insights into Knowledge Distillation for Pre-Trained Models",
            "text": "A. Knowledge distillation KD is a technique initially proposed to transfer knowledge from a large, complex model (termed the teacher) to a smaller model (termed the student), with the goal of maintaining comparable performance [1]- [3]. The central concept is that the student model can learn more effectively by emulating the soft targets, which are the output probabilities computed using the softmax function parameterized by temperature , as shown: \n\nHere,   ,, and   , , are logits for classes  and  from the teacher model for a specific data sample . The temperature  is used to control the smoothness of the probability distribution; in KD contexts, it is often set to a value greater than 1 to produce softer probability distributions [1]. \n\nVarious KD methodologies utilize different loss functions and datasets, often referred to as a transfer set or proxy dataset, which can include public labeled or unlabeled datasets, or the teacher's training data. When labeled data is available, the student model's training combines the cross-entropy loss with the teacher's soft targets and a distillation loss as: \n\nThe weight  balances the cross-entropy loss L  and the Kullback-Leibler divergence loss L   ;  represents the true labels;   and   denote the predicted probabilities (cf. Eq. ( 1)) by the student and teacher models, respectively. Fig. 1 illustrates the KD pipeline.",
            "score": 0.5946936029590346,
            "section_title": "II. PRELIMINARIES AND RELATED WORK",
            "char_start_offset": 3288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1116
                },
                {
                    "start": 1119,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1384
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33544921875
        },
        {
            "corpus_id": "276634364",
            "title": "A Malware Classification Method Based on Knowledge Distillation and Feature Fusion",
            "text": "Knowledge distillation is a widely used model compression technique that leverages the output supervisory information from a powerful teacher model, which typically has a large number of parameters, to aid in training a lightweight student model. This process aims to enhance the performance and accuracy of the student model. The core concept is for the student model to learn from both the ''hard labels'' of the dataset and the ''soft labels'' produced by the teacher model. \n\nTo achieve this goal, MC-KDFF employs a two-branch network architecture comprising both the teacher and student models, as illustrated in Figure 5. In the first branch, the teacher model performs global feature extraction, learns from these features, and conducts classification. In the second branch, the feature information from the teacher model is distilled into the parameters and structure of the student model for classification purposes. The entire process involves jointly optimizing the outputs of both the teacher and student models through loss computation and optimization. This joint training ultimately results in a lightweight, yet highly effective, student model. Additionally, the introduction of a Softmax temperature function helps smooth the predicted probability distribution, further enriching the class-specific feature information provided by the teacher model. During the training process of knowledge distillation, the loss function Lkd of the student 51272 VOLUME 13, 2025 Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. \n\nmodel contains two key components, Lsoft and Lhard, which correspond to ''hard labeling'' and ''soft labeling'' learning.Lsoft uses KL dispersion to calculate the difference between the results predicted by the teacher model and the results predicted by the student model, i.e., the cross-entropy loss between the Softmax output of the student model and the soft labeling for the same conditions of temperature Thard uses the cross-entropy loss function to calculate the difference between the results predicted by the student model and the true label. Finally the sum of these two components of loss is optimized as the final loss of the model. Lsoft loss function formula is given below:",
            "score": 0.5946643244989699,
            "section_title": "F. KNOWLEDGE DISTILLATION",
            "char_start_offset": 12193,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1592
                },
                {
                    "start": 1595,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2240
                },
                {
                    "start": 2241,
                    "end": 2284
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.301513671875
        },
        {
            "corpus_id": "214623002",
            "title": "Dynamic Hierarchical Mimicking Towards Consistent Optimization Objectives",
            "text": "Knowledge Transfer. Our method also has a connection with the research field of Knowledge Transfer (KT). Top-performing deep CNN models suffering from intensive computational demands are hindered from being embedded into resource-aware applications. To narrow the gap between theoretical performance and real-world feasibility, Dark Knowledge Distillation [9] takes the probabilistic distribution prediction from a powerful but resourcehungry teacher model or an ensemble of teacher models as the soft target, to jointly regularize the optimization objective when training a smaller student model with given image samples and the corresponding one-hot labels. Intermediate feature maps are demonstrated to be effective hints to further advance the knowledge distillation process [32,42,45]. Extending the concept of knowledge distillation and its variants, Deep Mutual Learning [50] shows that the teacher model would benefit from the knowledge of the student model in turn, in contrast to the prevailing one-way teaching-learning mode. The newly established idea was soon used in person re-identification tasks [50,49]. Different from the method above in focus and formulation, our motivation is to solve the inherent deficiency hidden in the deeply-supervised training procedure, utilizing representation mimicking as a tool. Our proposed Dynamic Hierarchical Mimicking can be viewed as an internal knowledge transfer process limited in one single neural network among hierarchical auxiliary classifiers, which has never attracted enough attention from the research community. We also include a more comprehensive analysis of the differences between our method and KT in the supplementary materials.",
            "score": 0.5944579450551789,
            "section_title": "Related Work",
            "char_start_offset": 8302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1701
                }
            ],
            "ref_mentions": [
                {
                    "start": 356,
                    "end": 359,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 783,
                    "end": 786,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 878,
                    "end": 882,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1112,
                    "end": 1116,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4921875
        },
        {
            "corpus_id": "245502619",
            "title": "Extracting knowledge from features with multilevel abstraction",
            "text": "Knowledge distillation(KD): KD [5] is a technique used to compress a teacher network to a student network while maintaining the performance. The student model learns from ground true labels of the dataset and soft labels from the teacher model; using a higher temperature to shift the soft probability distribution enables a more useful soft label, and this a one method of KD from logits. And then, Hinton [6] proposed to perform knowledge distillation by minimizing the distance between student and teacher network output distribution statistics. Furthermore, Fitnets first introduced hints, extracted feature results from the middle layer of the teacher network to train student networks [13]. Further utilizing Fintnets, the flow of solution procedure (FSP) that fits the relationship between layers of the large model is also used to transfer knowledge between networks [19]. And Zhou et al [23]. proposed each channel of feature maps corresponds with a visual model, so they focus on transferring attention concepts of feature map [16,17] from each channel of intermediate layers However, some exiting approaches do not make better of different knowledge from different layers. Form each intermediate layer, the level of knowledge is different. The intermediate knowledge has large potential uses. In our paper, we will make more effective use of the dark matter in the middle layer so that the network can also use the dark matter that the model may not use. \n\nSelf-distillation: Self-distillation means that the model distils itself, and we can say that the teacher model is an integrated version of the student model. There some approaches that introduce an informative thought in self-distillation. BYOT [21] suggested that the last block of networks is teacher and the rest of the shallow blocks are students, where feature maps and soft labels generated from the teacher are used to train the student. Based on the training teacher network, it uses KD to train student networks. Furthermore the paper, the paper [8] posed the current model as a student and the previous model as teacher perform KD to train . Selfdistillation greatly increases the accuracy of a model after training and shorten the training time of the Figure 1: Overview of our method, LFMA. Given a input image, the blocks output various level abstraction feature maps.",
            "score": 0.5940299294094058,
            "section_title": "Related Work",
            "char_start_offset": 3982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1465
                },
                {
                    "start": 1468,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2271
                },
                {
                    "start": 2272,
                    "end": 2350
                }
            ],
            "ref_mentions": [
                {
                    "start": 407,
                    "end": 410,
                    "matchedPaperCorpusId": "199405591"
                },
                {
                    "start": 875,
                    "end": 879,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1037,
                    "end": 1041,
                    "matchedPaperCorpusId": "56594619"
                },
                {
                    "start": 1041,
                    "end": 1044,
                    "matchedPaperCorpusId": "201809759"
                },
                {
                    "start": 1714,
                    "end": 1718,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58544921875
        },
        {
            "corpus_id": "233476572",
            "title": "Spirit Distillation: A Model Compression Method with Multi-domain Knowledge Transfer",
            "text": "Knowledge distillation researches on the technical means of training compact student network with the prompt of cumbersome teacher network. Previous works can be mainly classified into logit-based distillation [15,21,28] and feature-based distillation [14,18,19,24], which transfer the knowledge from different stages of the teacher network to improve the performance of the student network. Pioneering works on knowledge distillation bases on logits tranfer [15], which adopt a weighted average of soft and hard labels as supervisory information for student network training. Subsequent works begin to focus on transferring intermediate representation of the teacher network, like FitNet stage-wise training [24], knowledge adaptation [14], and structured knowledge distillation [18], hoping that the student network learns an effective representation based on the front part of the teacher with much fewer FLOPs.",
            "score": 0.5934426697607931,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 2969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 914
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 217,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "75137175"
                },
                {
                    "start": 256,
                    "end": 259,
                    "matchedPaperCorpusId": "73729180"
                },
                {
                    "start": 259,
                    "end": 262,
                    "matchedPaperCorpusId": "208109903"
                },
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "75137175"
                },
                {
                    "start": 780,
                    "end": 784,
                    "matchedPaperCorpusId": "73729180"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44384765625
        },
        {
            "corpus_id": "250426284",
            "title": "FairDistillation: Mitigating Stereotyping in Language Models",
            "text": "Knowledge distillation is a method to transfer learnt knowledge from one model-originally proposed as an ensemble of models-to another, usually smaller model [7,17]. Bucilu\u01ce et al. [7] introduced this technique as model compression with an ensemble of models that are used to label a dataset. This was later adapted for neural networks [17]. The teacher outputs a label probability distribution zi where some labels have higher probabilities, for example names or pronouns are more likely than verbs in the sentence \"<mask> is a doctor.\". To incorporate this information, a variation of the softmax function (Equation 1) can be used with a temperature T to tune the importance of these labels. \n\nSanh et al. [31] focus on the distillation of the MLM task from pre-trained LMs. Their models, DistilBERT and DistilRoBERTa, are trained on a linear combination of a distillation loss Lce with the softening function from Equation 1, the original MLM loss L mlm , and additionally a cosine loss Lcos for the last hidden states, following L = \u03b1ceLce + \u03b1 mlm L mlm + \u03b1cosLcos. TinyBERT [19] takes the same approach but also proposes a set of loss functions that perform distillation on (i) the embeddings layer, (ii) each of the transformer layers, and (iii) the prediction layer for specific tasks. These different loss functions make TinyBERT perform slightly better than DistilBERT, but these functions require additional transformations to be learnt. In addition, if the student and teacher have a different number of layers, a mapping function is also required to transfer the knowledge between both.",
            "score": 0.5931689593818839,
            "section_title": "Knowledge distillation",
            "char_start_offset": 6420,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1598
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 161,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 708,
                    "end": 712,
                    "matchedPaperCorpusId": "203626972"
                },
                {
                    "start": 1079,
                    "end": 1083,
                    "matchedPaperCorpusId": "202719327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339599609375
        },
        {
            "corpus_id": "233363076",
            "title": "Continual Learning for Named Entity Recognition",
            "text": "Knowledge Distillation (Hinton, Vinyals, and Dean 2015) is a technique to transfer the knowledge between models with a process called \"distillation\". KD was proposed to transfer knowledge from a cumbersome model into a smaller and more efficient one. This technique has been used to compress huge language models, such as BERT (Devlin et al. 2019), into smaller ones, e.g., DistilBERT (Sanh et al. 2019). \n\nKD works by first training a teacher model on some task. After training, the teacher will be able to assign a probability distribution p T over the categories of the task. p T is typically computed by the softmax over the logits z j of the last layer of a neural network architecture, for each category j. The softmax is defined as sof tmax(z j ) = exp(zj /Tm) l exp(z l /Tm) , where T m is a temperature hyper-parameter1 , which is typically set to 1. The teacher is trained by minimizing the crossentropy between the one-hot distribution and its predicted output distribution. Then, a student model is trained by imitating the teacher output distribution over a transfer dataset. That is, the student is trained by computing the KL divergence between p T (also referred to as the \"soft targets\") and the student output probability distribution p S . KD has been used for CL in the image domain (Shmelkov, Schmid, and Alahari 2017; Li and Hoiem 2018; Rannen et al. 2017) -for example, by updating an image classifier to support new categories while preventing catastrophic forgetting (Li and Hoiem 2018).",
            "score": 0.5929616856523665,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1512
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 55,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 327,
                    "end": 347,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 385,
                    "end": 403,
                    "matchedPaperCorpusId": "203626972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275634765625
        },
        {
            "corpus_id": "237453494",
            "title": "Self Supervision to Distillation for Long-Tailed Visual Recognition",
            "text": "Knowledge distillation [16] is first introduced for transferring knowledge from high-capability networks (teacher models) to small networks (student models) via soft labels. Our SSD method is inspired by knowledge distillation, yet exhibits essential difference with it. In our SSD method, student models are identical to teacher models, but are learned under different sampling strategies. Also, in particular for long-tailed recognition, the dark knowledge in soft labels can be helpful by transferring knowledge from head classes to tail classes. Due to the complementary properties of soft and hard labels, we propose a customized design by applying two separate classifiers supervised by hard and soft labels, respectively.\n\nMore formally, we denote x a training image with its hard label y and soft label y. We aim to learning an embedding function F that encodes x into feature vector f = F(x; \u03b8), as well as two classifiers G hard and G sof t . The feature vector f will be sent to two linear classifiers G hard and G sof t to get output logits z hard = G hard (f ) and z sof t = G sof t (f ). Let z denote the output logits of teacher model, then the soft label is given by:\n\nwhere i is the category index and T is the temperature which is set to 2 by default. Then, the knowledge distillation loss is written as:\n\n(4) For hard label supervision, we utilize the standard cross entropy loss L ce . Thus, the final loss is the combination of these two losses:\n\nwhere both of \u03bb 1 and \u03bb 2 are the weight of each loss and set to 1 in our experiments.",
            "score": 0.5919342381285473,
            "section_title": "Long-tailed recognition via self-distillation",
            "char_start_offset": 16788,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82470703125
        },
        {
            "corpus_id": "269982135",
            "title": "Exploring Dark Knowledge under Various Teacher Capacities and Addressing Capacity Mismatch",
            "text": "D EPLOYING large-scale neural networks on portable de- vices with limited computation and storage resources is challenging [1], and efficient architectures such as Mo-bileNets [2], [3] and ShuffleNets [4], [5] have been designed for lightweight deployment.However, the performances of these lightweight networks are usually not comparable to the larger ones.Commonly, second learning [6], [7] or knowledge distillation (KD) [8]- [10] could be utilized to transfer the knowledge of a more complex and well-performed network (i.e., the teacher) to the smaller ones (i.e., the student).The dark knowledge in KD is still a mystery that has attracted lots of studies [9], [11]- [13], and their goal is to answer the following question: what's the knowledge that the teacher provides and why they are effective in KD?\n\nIn the original KD method [9], the student aims to mimic the teacher's behavior by minimizing the Kullback-Leibler (KL) divergence between their output probabilities.That is, the logits and softened probabilities, i.e., the inputs to the final softmax operator and the corresponding outputs, are the specific knowledge transferred in KD.With the development of KD methods, the output-level knowledge has been extended to various types [14], including the intermediate features [15]- [20], the sample relationships [21]- [27], the parameters [28], [29], and the collaborative or online knowledge [10], [30] etc.However, the outputs of neural networks are much easier to visualize, analyze, and understand.Therefore, we focus on the original KD [9] and aim to understand the dark knowledge (i.e., the logits and softened probabilities) provided by the teachers.Unlike previous studies, we majorly study the output-level dark knowledge provided by teachers with various capacities, which receives little attention in previous studies.",
            "score": 0.5918455057515419,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 256,
                    "end": 358
                },
                {
                    "start": 358,
                    "end": 583
                },
                {
                    "start": 583,
                    "end": 811
                },
                {
                    "start": 813,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1150
                },
                {
                    "start": 1150,
                    "end": 1423
                },
                {
                    "start": 1423,
                    "end": 1517
                },
                {
                    "start": 1517,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 126,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 176,
                    "end": 179,
                    "matchedPaperCorpusId": "12670695"
                },
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "4555207"
                },
                {
                    "start": 201,
                    "end": 204,
                    "matchedPaperCorpusId": "24982157"
                },
                {
                    "start": 206,
                    "end": 209,
                    "matchedPaperCorpusId": "51880435"
                },
                {
                    "start": 384,
                    "end": 387,
                    "matchedPaperCorpusId": "3039694"
                },
                {
                    "start": 389,
                    "end": 392,
                    "matchedPaperCorpusId": "1024861"
                },
                {
                    "start": 424,
                    "end": 427,
                    "matchedPaperCorpusId": "16550689"
                },
                {
                    "start": 429,
                    "end": 433,
                    "matchedPaperCorpusId": "253020769"
                },
                {
                    "start": 662,
                    "end": 665,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 667,
                    "end": 671,
                    "matchedPaperCorpusId": "8125776"
                },
                {
                    "start": 673,
                    "end": 677,
                    "matchedPaperCorpusId": "235826057"
                },
                {
                    "start": 839,
                    "end": 842,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1248,
                    "end": 1252,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1290,
                    "end": 1294,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1296,
                    "end": 1300,
                    "matchedPaperCorpusId": "243860797"
                },
                {
                    "start": 1327,
                    "end": 1331,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1333,
                    "end": 1337,
                    "matchedPaperCorpusId": "219965004"
                },
                {
                    "start": 1354,
                    "end": 1358,
                    "matchedPaperCorpusId": "198160865"
                },
                {
                    "start": 1360,
                    "end": 1364,
                    "matchedPaperCorpusId": "3603048"
                },
                {
                    "start": 1408,
                    "end": 1412,
                    "matchedPaperCorpusId": "253020769"
                },
                {
                    "start": 1414,
                    "end": 1418,
                    "matchedPaperCorpusId": "232380330"
                },
                {
                    "start": 1556,
                    "end": 1559,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.487548828125
        },
        {
            "corpus_id": "229307924",
            "title": "Unsupervised Anomaly Detection Using Style Distillation",
            "text": "Knowledge distillation is a method of transferring knowledge from a teacher network to a student network. Its applications mainly entail network compression. Hinton et al. [15] employed the predictions of a teacher network as soft labels and trained a smaller student network with these labels for a classification task. Chen et al. [16] and Fukuda et al. [17] applied knowledge distillation for object detection and speech recognition tasks, respectively. \n\nThe aforementioned applications use knowledge distillation to distill the knowledge of heavy ensemble models, achieving state-of-the-art performance with a lighter, faster network. To minimize the loss of accuracy, as much knowledge as possible should be transferred. \n\nUnlike the conventional knowledge distillation methods, the objective of knowledge distillation for the proposed method is not compression but style mimicking. Thus, instead of transferring all knowledge from the teacher network, the proposed method aims to extract and distill only a small portion of knowledge that corresponds to style translation.",
            "score": 0.5915387426669485,
            "section_title": "C. KNOWLEDGE DISTILLATION",
            "char_start_offset": 5252,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 726
                },
                {
                    "start": 729,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1079
                }
            ],
            "ref_mentions": [
                {
                    "start": 172,
                    "end": 176,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 333,
                    "end": 337,
                    "matchedPaperCorpusId": "29308926"
                },
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "30258763"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.515625
        },
        {
            "corpus_id": "226282114",
            "title": "Knowledge Distillation for Singing Voice Detection",
            "text": "The basic idea behind knowledge distillation is to have the student network trained with not only the information provided by true labels (also called hard targets) but also by using soft targets produced by the teacher network (also referred to as cumbersome model) as a regularizer. This helps the student network learn to mimic the teacher's behaviour. In accordance with [10], a scaling hyperparameter referred to as temperature is used to regulate the softness of the targets from the teacher network. \n\nThe generalized formula for softmax is given by: \n\nwhere si and pi are the logit produced for the i th class and the corresponding class probability respectively and \u03c4 is the softness regulating temperature. \n\nA KL-divergence loss is taken between the softmax probabilities of the teacher and the student, represented as q and p respectively, raised to temperature \u03c4 (between 2-20 in our experiments) and is given by: \n\nThe combined loss is given by:. \n\nwhere LCE is the cross-entropy loss between the student's softmax probabilities (at \u03c4 = 1) and the correct labels (hard targets) and \u03bb is a second hyperparameter controlling the trade-off between the 2 losses. Generally, better results are obtained by keeping \u03bb close to 1, i.e., weight on LKD higher.",
            "score": 0.5911997117055812,
            "section_title": "Basics of Knowledge Distillation",
            "char_start_offset": 2244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 557
                },
                {
                    "start": 560,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1264
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4716796875
        },
        {
            "corpus_id": "208175624",
            "title": "Search to Distill: Pearls Are Everywhere but Not the Eyes",
            "text": "Interest in KD increased following Hinton et al. [8], who demonstrated a method called dark knowledge distillation, in which a student model trained with the objective of matching full softmax distribution of the teacher model. Commonly, the teacher is a high-capacity model with formidable performance, while the student network is compact. By transferring knowledge, one hopes to benefit from both the student's compactness and the teacher's capacity. While this strategy has been widely adopted, especially for edge devices that require low memory or fast execution, there are few systematic and theoretical studies on how and why knowledge distillation improves neural network training. [8] suggest that the success of KD depends on the distribution of logits of the wrong responses, that carry information on the similarity between output categories. [3] argue that soft-target distribution acts as an importance sampling weight based on the teacher's confidence in its maximum value. [42] analyzed knowledge distillation from the posterior entropy viewpoint claiming that soft-targets bring robustness by regularizing a much more informed choice of alternatives than blind entropy regularization. Nevertheless, to our knowledge, no previous works attempt to explain from the perspective of network inherent architecture, as previous efforts investigate mainly from learning theory and distillation methods.",
            "score": 0.5910437176541317,
            "section_title": "Na\u00efve distillation",
            "char_start_offset": 5422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1412
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.331298828125
        },
        {
            "corpus_id": "264052001",
            "title": "Decentralized Heterogeneous Federal Distillation Learning Based on Blockchain",
            "text": "Knowledge distillation is a technique that extracts valuable insights from complex models and condenses them into a singular, streamlined model, thereby enabling its deployment in real-world applications. Knowledge distillation [15] is a knowledge transfer and model compression algorithm proposed by Geoffrey Hinton et al. in 2015. For a specific character, through the use of a knowledge distillation algorithm, the information of an ideally trained teacher network containing more knowledge can be transferred to a smaller untrained student network. \n\nIn this paper, the loss function L student of the student network can be defined as: \n\nLCE is the cross entropy loss function, LKL is the Kullback Leibler (KL) divergence, p student and p teacher are the outputs of the network after the softmax activation function, z is the output logits of the neural network, and T is the temperature, which is generally set as 1. The primary purpose of temperature is to reduce the loss of knowledge contained in the small probability results caused by excessive probability differences. KL divergence can measure the difference between the two models. The larger the KL divergence, the more significant the distribution difference between the models, and the smaller the KL divergence, the smaller the distribution difference between the two models. The formula of KL divergence is: \n\nwhere P(x) and Q(x) respectively represent the output of different networks after the softmax activation function.",
            "score": 0.5909750525681515,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1375
                },
                {
                    "start": 1378,
                    "end": 1492
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 232,
                    "matchedPaperCorpusId": "233296935"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28466796875
        },
        {
            "corpus_id": "237250417",
            "title": "BERT Learns to Teach: Knowledge Distillation with Meta Learning",
            "text": "Knowledge Distillation Recently, many attempts have been made to accelerate large neural networks (Xu et al., , 2021bXu & McAuley, 2022). Knowledge distillation is a prominent method for training compact networks to achieve comparable performance to a deep network. Hinton et al. (2015b) first introduced the idea of knowledge distillation to exploit the \"dark knowledge\" (i.e., soft label distribution) from a large teacher model as additional supervision for training a smaller student model. Since its introduction, several works (Romero et al., 2015;Zagoruyko & Komodakis, 2017;Tung & Mori, 2019;Park et al., 2019;Sun et al., 2019;Jiao et al., 2019) have investigated methods that align different latent representations between the student and teacher models for better knowledge transfer. In the context of knowledge distillation, MetaDistil shares some common ideas with the line of work that utilizes a sequence of intermediate teacher models to make the teacher network better adapt to the capacity of the student model throughout the training process, including teacher assistant knowledge distillation (TAKD) (Mirzadeh et al., 2020) and route constraint optimization (RCO) . However, the intermediate teachers are heuristically selected independently of the training process and the evolution of the teacher network is discrete. In contrast, MetaDistil employs meta learning to make the teacher model adapt to the current state of the student model and provide a continuously evolving meta-teacher that can better teach the student. Concurrently, Park et al. (2021) and Shi et al. (2021) propose to update the teacher model jointly with the student model with task specific objectives (e.g., cross-entropy loss) during the KD process and add constraints to keep student and teacher similar to each other. Their approaches makes the teacher model aware of the student model by constraining the teacher model's capacity. However, the teacher models in their methods are still not optimized for knowledge transfer. In addition, Zhang et al. (2018) introduced deep mutual learning where multiple models learn collaboratively and teach each other throughout the training process. While it is focused on a different setting where different models have approximately the same capacity and are learned from scratch, it also",
            "score": 0.5909032483153791,
            "section_title": "Related Work",
            "char_start_offset": 9555,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 582,
                    "end": 600,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 600,
                    "end": 618,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 618,
                    "end": 635,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 1119,
                    "end": 1142,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.560546875
        },
        {
            "corpus_id": "236352816",
            "title": "Revisiting Label Smoothing Regularization with Knowledge Distillation",
            "text": "As Figure 1a shows, a large teacher network is usually trained beforehand in the traditional knowledge distillation method. Then, to transfer the knowledge from the pretrained teacher model to the student, the Kullback-Leibler (KL)-divergence between their output probabilities is penalized: \n\nwhere p t (\u03c4) and p(\u03c4) are the soft teacher and student distribution obtained from their corresponding model output with Equation (1). The temperature \u03c4 (>1) is a hyperparameter that needs to be tuned. During training, the KD method calculates the sum of the two losses above with a hyperparameter \u03b1: \n\nwhere \u03c4 2 is a factor in ensuring that the relative contribution of the ground-truth label and teacher output distribution remains roughly unchanged [13].",
            "score": 0.5904633328747653,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 12505,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 751
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30322265625
        },
        {
            "corpus_id": "259243927",
            "title": "Efficient Online Processing with Deep Neural Networks",
            "text": "Originally proposed for compressing the knowledge of an ensemble of models into a single model [85], knowledge distillation (KD) is a widely applied model compression technique [18] for transferring the knowledge of a cumbersome teacher model to an efficient student model. Specifically, a student model is supervised by the soft target distributions produced by a teacher model using a high temperature coefficient for the softmax operation. If ground truth labels (hard targets) are available, a weighted sum of objectives for the soft and hard targets is employed.\n\nThere are multiple variants of KD, including deep mutual learning [227], where an ensemble of students teach each other concurrently during the training process; assistant teaching [129], where models of intermediary size (teacher assistants) are employed to bridge the supervision from teacher models with too large capacity to effectively teach low-capacity student networks; lifelong learning [224] uses KD to",
            "score": 0.5902246620680545,
            "section_title": "Knowledge distillation",
            "char_start_offset": 23778,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 181,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 635,
                    "end": 640,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 750,
                    "end": 755,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.493408203125
        },
        {
            "corpus_id": "272564024",
            "title": "A Comprehensive Review of Hardware Acceleration Techniques and Convolutional Neural Networks for EEG Signals",
            "text": "Knowledge distillation is a technique that transfers knowledge from large, complex CNNs (teachers) to smaller, more efficient CNNs (students). The student network is trained to mimic the behavior of the teacher, resulting in a compact model with comparable performance. This concept can be traced back to the pioneering work in [101] and has since been extended to the context of deep learning [102]. The core challenge of knowledge distillation revolves around the techniques used to transfer knowledge from the teacher model to the student model, which involves three fundamental components-knowledge, distillation algorithms-and the architecture defining the relationship between the teacher and student models. In this context, knowledge manifests in various forms, including logits, activations, or features extracted from intermediate layers of the teacher model. The distillation algorithms can be categorized as offline, online, or self-distillation. \n\nOffline distillation [101,[103][104][105] extracts knowledge from a pre-trained teacher model and uses the soft label output of the teacher model to train the student network. The authors of [103] used data augmentation to exploit the output distributions of multiple teacher networks. The authors of [101] introduced a tailored distillation approach for quantized models, demonstrating that quantized student networks can closely match the accuracy of full-precision teacher networks while achieving high compression rates and inference acceleration. In contrast, [104] pioneered a data-free technique, training the student network using synthetic data responses from the complex teacher network. Online distillation [106][107][108] occurs during the simultaneous training of both the teacher and student models. It employs online knowledge distillation using the soft-label outputs of the teacher network. Ref. [106] proposed training the student model at different checkpoints of the teacher model until convergence is achieved. Meanwhile, Collaborative Learning Knowledge Distillation (KDCL) [107] dynamically generates high-quality soft targets through an ensemble approach for one-stage online training. Furthermore, as observed in [105,108], knowledge distillation extends its influence to the intermediate layers of the teacher network.",
            "score": 0.590099649985955,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 46568,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2170
                },
                {
                    "start": 2171,
                    "end": 2305
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 399,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 987,
                    "end": 992,
                    "matchedPaperCorpusId": "30258763"
                },
                {
                    "start": 992,
                    "end": 997,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 997,
                    "end": 1002,
                    "matchedPaperCorpusId": "182183296"
                },
                {
                    "start": 1152,
                    "end": 1157,
                    "matchedPaperCorpusId": "30258763"
                },
                {
                    "start": 1526,
                    "end": 1531,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 1679,
                    "end": 1684,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 1684,
                    "end": 1689,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1689,
                    "end": 1694,
                    "matchedPaperCorpusId": "226841849"
                },
                {
                    "start": 1874,
                    "end": 1879,
                    "matchedPaperCorpusId": "125985701"
                },
                {
                    "start": 2057,
                    "end": 2062,
                    "matchedPaperCorpusId": "219965421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6904296875
        },
        {
            "corpus_id": "234067330",
            "title": "Optimising Hardware Accelerated Neural Networks with Quantisation and a Knowledge Distillation Evolutionary Algorithm",
            "text": "Distillation loss: cross entropy of the teacher's high temperature (T = \u03c4) output with the students high temperature output. \n\nThe loss function for knowledge distillation (from [30]) is: \n\nwhere x is an input, W are the parameters of the student network, y the ground truth vector and \u03c3(z; T = \u03c4) is the softmax function applied to logit vector z and temperature T = \u03c4. The student and teacher logit vectors are s and t, and hyper-parameters \u03b1 and \u03b2 are arbitrary constants. \n\nIn the NEMOKD methodology (Section 3.4), student models in the initial CNN architecture population are partially trained using knowledge distillation.",
            "score": 0.5897912736763856,
            "section_title": "2.",
            "char_start_offset": 12417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 127,
                    "end": 187
                },
                {
                    "start": 190,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 628
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339111328125
        },
        {
            "corpus_id": "247447455",
            "title": "Communication-Efficient Federated Distillation with Active Data Sampling",
            "text": "Knowledge Distillation (KD) is the process of distilling knowledge from a large and well-trained teacher model to a small student model. For a classification problem with N c classes, the logit of data sample x i is denoted as t(x i ) and it is the vector of the class probabilities which is obtained by using a softmax function on the neural network output. That is,\n\nwhere \u03b8(x i ) \u2208 R Nc denotes the model output of input data sample x i , and \u03b8(\u00b7) is the neural network function parameterized by model parameters \u03b8. Speficically, for the\n\nwhere T is the distillation temperature with a higher temperature producing a smoother probability distribution over classes.\n\nThe distillation loss of the trainset D is the cross-entropy loss for the teacher logit t t and the student logit t s , which is\n\nIn the distillation process, the student's objective function is an average of the distillation loss L distll and the cross entropy loss with the groudtruth labels.",
            "score": 0.5895843873970295,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 6409,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.325439453125
        },
        {
            "corpus_id": "277632659",
            "title": "Crowd counting at the edge using weighted knowledge distillation",
            "text": "Knowledge Distillation (KD) refers to the mechanism in which the knowledge learned by a cumbersome and sophisticated model (teacher) is transferred to a shallow model (student) to improve the student model's generalization performance. Using KD, the student can capture the finer details learned by the teacher instead of learning from the labels. The student model is the model that will be deployed for the application. \n\nIn its simpler form, the student model can mimic the performance of the teacher model by using the class prediction probabilities of the teacher called \"soft targets\". The soft targets are calculated using the \"temperature\" parameter in the softmax function. \n\nwhere T is the \"temperature\" parameter used to soften the probabilities. \n\nKnowledge can be distilled in three different ways. First, response-based KD 5 in which the student model aims to mimic the teacher's predictions, with the help of a distillation loss function that measures the difference between their logits. As the loss decreases over time, the student becomes more accurate in making predictions similar to the teacher. Second, feature-based distillation 6 in which the teacher model, distills the data knowledge in its intermediate layers into the student models. The student model thus learns the same feature activations as the teacher. This is achieved by minimizing the difference between the feature activations of both models. Third, relation-based distillation 7 in which connections between different layers or data samples are used employing a Flow of Solution Process (FSP) based on Gram matrices between layers to explore relationships among feature maps. The FSP matrix summarizes these relationships through inner product calculations between feature maps. Singular value decomposition is used to distill knowledge, with correlations between feature maps representing the distilled knowledge. This approach encapsulates relationships through feature maps, graphs, similarity matrices, feature embeddings, and probabilistic distributions, enhancing our understanding of knowledge distillation. Figure 2 illustrates the three types of KD in neural networks 8 . investigates KD and observed that a student model trained via KD can exhibit higher intra-class variance than the teacher model, and thus act as a better teacher 9 .",
            "score": 0.5895306128744333,
            "section_title": "Knowledge distillation",
            "char_start_offset": 1414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 421
                },
                {
                    "start": 424,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2169
                },
                {
                    "start": 2170,
                    "end": 2335
                }
            ],
            "ref_mentions": [
                {
                    "start": 837,
                    "end": 838,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1466,
                    "end": 1467,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 2166,
                    "end": 2167,
                    "matchedPaperCorpusId": "264467753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.453369140625
        },
        {
            "corpus_id": "212747830",
            "title": "Pre-trained models for natural language processing: A survey",
            "text": "Knowledge distillation (KD) [146] is a compression technique in which a small model calledstudent model is trained to reproduce the behaviors of a large model calledteacher model. Here the teacher model can be an ensemble of many models and usually well pre-trained. Different to model compression, distillation techniques learn a small student model from a fixed teacher model through some optimization objectives, while compression techniques aiming at searching a sparser architecture.\n\nGenerally, distillation mechanisms can be divided into three types: (1) distillation from soft target probabilities, (2) distillation from other knowledge, and (3) distillation to other structures.\n\n(1) Distillation from soft target probabilities. Ref. [143] showed that making the student approximate the teacher model can transfer knowledge from teacher to student. A common method is approximating the logits of the teacher model. DistilBERT [91] trained the student model with a distillation loss over the soft target probabilities of the teacher as\n\nwhere t i and s i are the probabilities estimated by the teacher model and the student, respectively. Distillation from soft target probabilities can also be used in task-specific models, such as information retrieval [155], and sequence labeling [156].\n\n(2) Distillation from other knowledge. Distillation from soft target probabilities regards the teacher model as a black box and only focus on its outputs. Moreover, decomposing the teacher model and distilling more knowledge can bring improvement to the student model. a) The desing of this table is borrowed from [94,151].\n\nb) The averaged score on 8 tasks (without WNLI) of GLUE benchmark (see Sect. 7.1). Here MNLI-m and MNLI-mm are regarded as two different tasks. \"dev\" indicates the result is on dev set. \"ensemble\" indicates the result is from the ensemble model. c) \"L MLM \", \"L NSP \", and \"L SOP \" indicate pre-training objective (see Sect. 3.1 and Table 1).\"L Task \" means task-specific loss. \"HAWQ\", \"GWQ\", \"DQ\", and \"QAT",
            "score": 0.5895125460405721,
            "section_title": "Knowledge distillation",
            "char_start_offset": 50542,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 743,
                    "end": 748,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1292,
                    "end": 1297,
                    "matchedPaperCorpusId": "202122780"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15869140625
        },
        {
            "corpus_id": "233189631",
            "title": "Annealing Knowledge Distillation",
            "text": "Significant memory and computational requirements of large deep neural networks restricts their application on edge devices. Knowledge distillation (KD) is a prominent model compression technique for deep neural networks in which the knowledge of a trained large teacher model is transferred to a smaller student model. The success of knowledge distillation is mainly attributed to its training objective function, which exploits the soft-target information (also known as \u201cdark knowledge\u201d) besides the given regular hard labels in a training set. However, it is shown in the literature that the larger the gap between the teacher and the student networks, the more difficult is their training using knowledge distillation. To address this shortcoming, we propose an improved knowledge distillation method (called Annealing-KD) by feeding the rich information provided by teacher\u2019s soft-targets incrementally and more efficiently. Our Annealing-KD technique is based on a gradual transition over annealed soft-targets generated by the teacher at different temperatures in an iterative process; and therefore, the student is trained to follow the annealed teacher output in a step-by-step manner. This paper includes theoretical and empirical evidence as well as practical experiments to support the effectiveness of our Annealing-KD method. We did a comprehensive set of experiments on different tasks such as image classification (CIFAR-10 and 100) and NLP language inference with BERT-based models on the GLUE benchmark and consistently got superior results.",
            "score": 0.5887075868561957,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8955078125
        },
        {
            "corpus_id": "267036722",
            "title": "Research on Real-Time Detection of Maize Seedling Navigation Line Based on Improved YOLOv5s Lightweighting Technology",
            "text": "The technique of knowledge distillation is a widely adopted approach for compressing models, which differs from pruning and quantization in model compression. Essentially, knowledge distillation involves training a compact network model to emulate the knowledge extracted from a pre-trained larger network. This training methodology is commonly referred to as \"teacher-student\", where the larger network is the \"teacher network\", while the smaller network is the \"student network\". The goal of knowledge distillation is to enable the student network to achieve comparable, if not better, accuracy than the larger network while having fewer parameters and a smaller scale. By distilling the model, the problem of slow speed and high memory consumption is resolved, while also enhancing model accuracy. The distillation process can be observed in Figure 11. \n\nAgriculture 2024, 14, x FOR PEER REVIEW 11 of 28 \n\nThe technique of knowledge distillation is a widely adopted approach for compressing models, which differs from pruning and quantization in model compression. Essentially, knowledge distillation involves training a compact network model to emulate the knowledge extracted from a pre-trained larger network. This training methodology is commonly referred to as \"teacher-student\", where the larger network is the \"teacher network\", while the smaller network is the \"student network\". The goal of knowledge distillation is to enable the student network to achieve comparable, if not better, accuracy than the larger network while having fewer parameters and a smaller scale. By distilling the model, the problem of slow speed and high memory consumption is resolved, while also enhancing model accuracy. The distillation process can be observed in Figure 11. This article utilizes the YOLOv5m model as the teacher model. Firstly, a deeper and more capable teacher network is trained using the data to extract features. Then, the teacher network outputs the logits function, which is distilled at a temperature of T. The class probability distribution obtained by applying the softmax layer is used as soft targets. At the same time, the student network outputs logits that are distilled at the same temperature T, and knowledge distillation is performed. This is a commonly used method for model compression, which differs from pruning and quantization.",
            "score": 0.588084713644056,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 32175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 906
                },
                {
                    "start": 909,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2260
                },
                {
                    "start": 2261,
                    "end": 2359
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21484375
        },
        {
            "corpus_id": "246738423",
            "title": "Application of Knowledge Distillation Based on Transfer Learning of ERNIE Model in Intelligent Dialogue Intention Recognition",
            "text": "The knowledge distillation is teaching a smaller model that is more suitable for reasoning by using an already trained network; the process by which is also known as model compression. Knowledge distillation introduces the soft target of the \"teacher\" model with more complex reasoning ability to teach the \"student\" network with small parameters and weak learning ability, as shown in Figure 3. The soft label is the probability vector output for large-scale samples, and the probability vector has more information compared with the one-hot label for text classification task. By passing the soft label information of the large model to the student model, the learning ability of the student model can be improved.",
            "score": 0.5877718418709841,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 12791,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 716
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.258056640625
        },
        {
            "corpus_id": "257102399",
            "title": "Knowledge Distillation-based Information Sharing for Online Process Monitoring in Decentralized Manufacturing System",
            "text": "An early-stop distillation framework is proposed to make knowledge distillated from teacher network easier for student to learn by making teacher model less powerful [44]. A new loss function and a multi-exit architecture for ensemble KD was proposed by Phuong et al. to ensure the diversity of distribution [45]. \n\nYang et al. kept using soft labels in the distillation process and added several constrains to optimize the model in successive generations [46]. A strategy to use a more noisy dataset is proposed to improve performance of student network by focusing on the data issue [47]. \n\nCurrently, there are some studies investigating how to incorporate knowledge distillation framework in manufacturing to simplify the model [48][49][50] while utilizing knowledge distillation for information sharing to enhance monitoring performance needs to be explored [51]. Meanwhile, the teacher and student model are trained with the same dataset following same distribution [9], which needs to be adjusted in the decentralized manufacturing system because the data distribution at different units may not be identical. In addition, the architecture of teacher and student model does not have to be different in decentralized manufacturing system. Therefore, an information sharing framework is developed based on knowledge distillation in Sec. 3, which could narrow the gap of the existing solutions.",
            "score": 0.587491506360741,
            "section_title": "Knowledge distillation (KD) is a model compression technique proposed by Hinton et al. in 2015",
            "char_start_offset": 12437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 313
                },
                {
                    "start": 316,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1398
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 170,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 308,
                    "end": 312,
                    "matchedPaperCorpusId": "207994757"
                },
                {
                    "start": 456,
                    "end": 460,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 585,
                    "end": 589,
                    "matchedPaperCorpusId": "207853355"
                },
                {
                    "start": 732,
                    "end": 736,
                    "matchedPaperCorpusId": "233965427"
                },
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "238745215"
                },
                {
                    "start": 863,
                    "end": 867,
                    "matchedPaperCorpusId": "256621756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11358642578125
        },
        {
            "corpus_id": "258426697",
            "title": "Emotions Beyond Words: Non-Speech Audio Emotion Recognition With Edge Computing",
            "text": "The process of knowledge distillation as the name suggests is the method of transferring knowledge from a larger computationally expensive model to a relatively smaller model. The larger and smaller models are called the teacher and student models respectively. Thus, knowledge distillation consists of three principal components: (1) knowledge; (2) distillation algorithm; and (3) teacherstudent architecture. While there are now multiple methods of distillation algorithms we selected the response-based algorithm. As shown in Figure 3, the hypothesis is that the student model will learn to mimic the predictions of the teacher model. This can be achieved by using a loss function, termed the distillation loss, that captures the difference between the logits of the student and the teacher model respectively. As this loss minimizes overtraining, the student model will improve at making the same predictions as the teacher. In the offline training scheme, the teacher model is first trained and the weights are then frozen. Next, we train the student model using the distillation loss and the logits from the teacher model as targets. Following is the equation of the distillation loss. \n\nwhere: L d : the loss function for knowledge distillation \u03b1: a hyperparameter that controls the trade-off between the classification loss and the distillation loss T : the temperature hyperparameter used to soften the logits (outputs of the last layer before softmax) of the teacher and student models KL: the Kullback-Leibler divergence, a measure of how different two probability distributions are softmax: a function that converts the logits to probabilities f (T, x): the logits of the teacher model for input x g(T, x): the logits of the student model for input x 2) Teacher Model: Generally, for the teacher model a larger and deeper network is chosen so that it performs well on the task at hand. We chose ResNet18 [38] as our teacher model. ResNet18 contains 18 residual blocks stacked together which alleviates the degradation and vanishing gradient problem. Figure 4 shows a single layer, where the outputs of the previous layer are added to the outputs of the next layer, and Figure 5 depicts the complete architecture of ResNet18 used for the teacher model.",
            "score": 0.5870566954985896,
            "section_title": "1) Knowledge Distillation:",
            "char_start_offset": 16396,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2263
                }
            ],
            "ref_mentions": [
                {
                    "start": 1916,
                    "end": 1920,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5400390625
        },
        {
            "corpus_id": "265384964",
            "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
            "text": "This kind of loss is frequently called \"distillation loss.\" Therefore, with the increase in T, we are better able to impart the knowledge of the teacher model to the student model, aiding the latter in learning and generalization. \n\nIn 2015, Hinton et al. discovered the advantages of training the distilled model to not only produce the soft labels from the teacher but also the correct labels relying on the ground truth labels. Consequently, we compute the \"standard\" loss by comparing the predicted class probabilities of the student with the ground truth labels. This loss is named the \"student loss\". When calculating the class probabilities for the student loss, we employ T = 1. The comprehensive loss function, which integrates both the distillation and student losses, is determined as follows: \n\nHere, x represents the input, W represents the student model's parameters, y denotes the ground truth labels, H signifies the cross-entropy loss function, \u03c3 represents the softmax function characterized by the \"temperature\" T, and \u03b1 and \u03b2 are constants. The logits of the student and teacher are denoted as z s and z t , respectively. The general structure of knowledge distillation is depicted in Figure 2. \n\nHere,  represents the input,  represents the student model's parameters,  denotes the ground truth labels,  signifies the cross-entropy loss function,  represents the softmax function characterized by the \"temperature\" , and  and  are constants. The logits of the student and teacher are denoted as  and  , respectively. The general structure of knowledge distillation is depicted in Figure 2.",
            "score": 0.586720489176679,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 12021,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 804
                },
                {
                    "start": 807,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1610
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4150390625
        },
        {
            "corpus_id": "258187531",
            "title": "Deep Collective Knowledge Distillation",
            "text": "Knowledge distillation [8] is an effective method for compressing a heavy teacher model to a lighter student model. The main idea of knowledge distillation is that a teacher model with higher capacity and better performance distills the softened output distribution into a student model as knowledge. Hinton et al. [8] explained that softened outputs with higher entropy than hard labels provide much richer information. In previous studies [1,8,17], assigning the probabilities to other classes, which leads to increased entropy, was effective in generalizing a network. These probabilities provided valuable information about the correlation between other classes and Dubey et al. [4] showed that maximum entropy training with these correlation information is effective. Many studies [7,10,[14][15][16]18,21,24] on knowledge distillation have focused on efficiently transferring teacher's knowledge to students. We took a step further by adopting a different approach to knowledge enrichment. If a student model learns from only a teacher's soft targets, then the student will only imitate the teacher. However, if the knowledge of peer students is additionally given, it can help a student model outperform the student model learned only from a teacher. Our work focuses on creating a student model that can have rich representation by training not only with the knowledge provided by the teacher but also with additional knowledge of various correlations between classes provided by the peer students. As a method for generating additional knowledge, we propose gathering knowledge from multiple students. \n\nMutual learning methods [2,5,12,26] aim to train powerful student models using ensembled knowledge of multiple untrained students without a pretrained teacher model. These methods train multistudent models with knowledge generated on-the-fly from students' logits, and this online knowledge is often generated to better represent the ground truth or soft targets of the teacher model. However, we propose an approach for generating additional online knowledge containing diverse correlation information from multistudent models, not similar to the ground truth or the teacher's soft targets. Since the teacher model learned with the supervised learning manner becomes overconfident [20], it may overlook other correlation information. Thus, we believe that the additional knowledge, including correlational information between classes, can assist the teacher model.",
            "score": 0.5860284974040882,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2203
                },
                {
                    "start": 2204,
                    "end": 2346
                },
                {
                    "start": 2347,
                    "end": 2477
                }
            ],
            "ref_mentions": [
                {
                    "start": 441,
                    "end": 444,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 446,
                    "end": 449,
                    "matchedPaperCorpusId": "9545399"
                },
                {
                    "start": 789,
                    "end": 792,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 792,
                    "end": 796,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 796,
                    "end": 800,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 800,
                    "end": 804,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 804,
                    "end": 807,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 807,
                    "end": 810,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 810,
                    "end": 813,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1636,
                    "end": 1639,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 1639,
                    "end": 1641,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 1641,
                    "end": 1644,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1644,
                    "end": 1647,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.406982421875
        },
        {
            "corpus_id": "257756916",
            "title": "Low Rank Optimization for Efficient Deep Learning: Making A Balance between Compact Architecture and Fast Training",
            "text": "Knowledge distillation [52] is a promising solution, which aims to feed some extra knowledge learned from teacher networks (one or more complex networks) into a student network (much simpler network). With the help of a teacher, the student can achieve comparable accuracy but with much lower memory and computation costs compared with the teacher. Let q s and q t denote the softmax outputs of the student network and teacher network, respectively. The student network will be trained via aligning q s and q t . But in the case that q t is close to the one-hot code of true labels, the information contained in small values cannot be transferred to the student. Hence, a trick named temperature [52] is utilized to soften the distribution of both q s and q t . Networks compressed by low rank approximation is also a simpler network that can learn knowledge from the uncompressed version. In general, the decomposed networks are recovered by simply fine-tuning to minimize the cross-entropy function. However, the fine-tuning process always converges slowly and cannot recover the original accuracy well. Hence, this underlines the need for training the compressed network with information from the corresponding pre-training network. \n\nHowever, it was demonstrated in [39] that it is difficult to train a student network deeper than the teacher network with knowledge distillation due to the undesirable phenomenon of vanishing gradient. Hence, a novel knowledge transfer (KT) was proposed in [85], which aligns both outputs and intermediate responses from a teacher (original) network to its student (compressed) network. Experiments show that it surpasses the common fine-tuning and knowledge distillation, particularly with a high compression ratio. \n\nHowever, the KT method is still time-consuming and has a demand for a fully annotated large-scale training set, which may be infeasible in practice. Li et al. [80] proposed a revised knowledge distillation that only requires a few labelfree samples. It adds a 1\u00d71 Conv layer at the end of each block of the student network, and aligns block-level outputs of teacher and student by estimating the 1\u00d71 Conv layer's parameters using least-squared regression.",
            "score": 0.5857188225064213,
            "section_title": "4) Integration with Knowledge Distillation",
            "char_start_offset": 54069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1235
                },
                {
                    "start": 1238,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1754
                },
                {
                    "start": 1757,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2212
                }
            ],
            "ref_mentions": [
                {
                    "start": 1270,
                    "end": 1274,
                    "matchedPaperCorpusId": "5575601"
                },
                {
                    "start": 1495,
                    "end": 1499,
                    "matchedPaperCorpusId": "52915624"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60595703125
        },
        {
            "corpus_id": "211096976",
            "title": "Self-Distillation Amplifies Regularization in Hilbert Space",
            "text": "Knowledge distillation, originally introduced in the deep learning setting [Hinton et al., 2015], is a method that transfers knowledge from one architecture (teacher) to another (student), where often the student model is much smaller. The original formulation achieves this by training the student model using the output probability distribution of the teacher model in addition to original labels. The student model benefits from this \"dark knowledge\" (extra information in soft predictions) and often performs better than if it was trained on the actual labels. \n\nVarious extensions of this approach have been recently proposed, where instead of output predictions, the student matches other statistics from the teacher model such as intermediate feature representations [Romero et al., 2014], Jacobian matrices [Srinivas and Fleuret, 2018], distributions [Huang and Wang, 2017], Gram matrices [Yim et al., 2017]. Additional developments on knowledge distillation include its extensions to Bayesian settings [Korattikara Balan et al., 2015, Vadera andMarlin, 2020], uncertainty preservation [Tran et al., 2020], reinforcement learning [Hong et al., 2020, Teh et al., 2017, Ghosh et al., 2018], online distillation [lan et al., 2018], zero-shot learning [Nayak et al., 2019], multistep knowledge distillation [Mirzadeh et al., 2020], tackling catastrophic forgetting [Li and Hoiem, 2016], transfer of relational knowledge [Park et al., 2019], adversarial distillation [Wang et al., 2018]. \n\nThe special case when the teacher and student architectures are identical is called1 self-distillation. The idea is to feed in predictions of the trained model as new target values for retraining (and iterate this loop possibly a few times). It has been consistently observed that the self-distilled model often achieves higher accuracy on held out data [Furlanello et al., 2018, Yang et al., 2019, Ahn et al., 2019].",
            "score": 0.5853813815468956,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1490
                },
                {
                    "start": 1493,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1910
                }
            ],
            "ref_mentions": [
                {
                    "start": 1156,
                    "end": 1174,
                    "matchedPaperCorpusId": "31009408"
                },
                {
                    "start": 1256,
                    "end": 1276,
                    "matchedPaperCorpusId": "159041346"
                },
                {
                    "start": 1311,
                    "end": 1334,
                    "matchedPaperCorpusId": "62841516"
                },
                {
                    "start": 1369,
                    "end": 1389,
                    "matchedPaperCorpusId": "62841516"
                },
                {
                    "start": 1424,
                    "end": 1443,
                    "matchedPaperCorpusId": "159041346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72802734375
        },
        {
            "corpus_id": "242069370",
            "title": "Boost Precision Agriculture with Unmanned Aerial Vehicle Remote Sensing and Edge Intelligence: A Survey",
            "text": "The main objective of knowledge distillation is to train a student network from the teacher network while maintaining its generalization capability [204]. The student network is lighter, i.e., having a smaller model size and less computation, but with the same or comparable performance as the larger network. \n\nGreat efforts have been done to improve the supervision of student network by different knowledge transferred. Romero et al. [205] proposed a FitNets model which teaches the student network to imitate the hints from both middle layers and output layer of the teach network. Instead of hard labels that are used, the work in [206] utilizes soft labels as the representation from teacher network. Kim et al. [207] proposed a paraphrasing based knowledge transfer method which uses convolution operations to paraphrase the teacher model knowledge and translate it to a student model. From the point of teacher networks, student networks can also learn knowledge from multiple teachers [208]. \n\nIn the field of UAV based deep model inference, knowledge distillation is a promising direction. In [209], YOLO + MobileNet model acts as the teacher network, while the pruned model functions as the student network, and knowledge distillation algorithm is used to improve the detection accuracy of the pruned model. Qiu et al. [210] propose to distill knowledge to a lighter distilled network through soft labels from trained teacher network MobileNet. Similar applications using knowledge distillation for model compression can be found in [211,212].",
            "score": 0.5852260766873238,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 49981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 309
                },
                {
                    "start": 312,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1554
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 153,
                    "matchedPaperCorpusId": "211062209"
                },
                {
                    "start": 636,
                    "end": 641,
                    "matchedPaperCorpusId": "84176918"
                },
                {
                    "start": 718,
                    "end": 723,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 994,
                    "end": 999,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1103,
                    "end": 1108,
                    "matchedPaperCorpusId": "226730806"
                },
                {
                    "start": 1544,
                    "end": 1549,
                    "matchedPaperCorpusId": "231977039"
                },
                {
                    "start": 1549,
                    "end": 1553,
                    "matchedPaperCorpusId": "233196782"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.449951171875
        },
        {
            "corpus_id": "270123704",
            "title": "Estimating Human Poses Across Datasets: A Unified Skeleton and Multi-Teacher Distillation Approach",
            "text": "Knowledge distillation [13] is a technique originally developed to compress the knowledge of a large, complex model (teacher) into a smaller, more efficient model (student) with minimal performance loss.Logit-based knowledge distillation [3,13,17,24,34,56,57,57] utilizes the soft output probabilities from the teacher model to guide the training of the student model.By employing a loss function that combines temperature-scaled cross-entropy with Kullback-Leibler divergence [19], the student model is encouraged not only to mimic the teacher's output distribution but also to achieve correct label predictions on its own.This method effectively transfers the intricate decision boundaries learned by the teacher to the student, enhancing its performance beyond what could be achieved through direct training alone.Feature-based distillation [1,4,5,11,12,25,27,30,42,48,54] and relation-based distillaion [14,21,40,41,49,52] have also been proposed to improve the knowledge transfer between teacher and student.Recent advancements, such as the introduction of a teacher assistant model [35], have further refined this process by staging the knowledge transfer through an intermediate-sized model, which bridges the gap between the high-capacity teacher and the smaller student.Knowledge distillation has also been used to enhance the performance of human pose estimation methods [26,47,53,58], including some works [47,58] that use multiple teacher networks.",
            "score": 0.5852122877271274,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5951,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 624
                },
                {
                    "start": 624,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 1013
                },
                {
                    "start": 1013,
                    "end": 1279
                },
                {
                    "start": 1279,
                    "end": 1460
                }
            ],
            "ref_mentions": [
                {
                    "start": 238,
                    "end": 241,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 244,
                    "end": 247,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "222124879"
                },
                {
                    "start": 250,
                    "end": 253,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 256,
                    "end": 259,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 259,
                    "end": 262,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 844,
                    "end": 847,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 847,
                    "end": 849,
                    "matchedPaperCorpusId": "247762862"
                },
                {
                    "start": 849,
                    "end": 851,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 851,
                    "end": 854,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 854,
                    "end": 857,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 857,
                    "end": 860,
                    "matchedPaperCorpusId": "236912875"
                },
                {
                    "start": 860,
                    "end": 863,
                    "matchedPaperCorpusId": "248986384"
                },
                {
                    "start": 863,
                    "end": 866,
                    "matchedPaperCorpusId": "73729180"
                },
                {
                    "start": 869,
                    "end": 872,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 872,
                    "end": 875,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 907,
                    "end": 911,
                    "matchedPaperCorpusId": "248986690"
                },
                {
                    "start": 911,
                    "end": 914,
                    "matchedPaperCorpusId": "245006036"
                },
                {
                    "start": 914,
                    "end": 917,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 917,
                    "end": 920,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 920,
                    "end": 923,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 923,
                    "end": 926,
                    "matchedPaperCorpusId": "235613564"
                },
                {
                    "start": 1088,
                    "end": 1092,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 1381,
                    "end": 1385,
                    "matchedPaperCorpusId": "236912875"
                },
                {
                    "start": 1385,
                    "end": 1388,
                    "matchedPaperCorpusId": "263159006"
                },
                {
                    "start": 1388,
                    "end": 1391,
                    "matchedPaperCorpusId": "260334254"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "263159006"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48046875
        },
        {
            "corpus_id": "229478912",
            "title": "Deep learning based question answering system in Bengali",
            "text": "Knowledge Distillation (Hinton et al., 2015) is a compression technique where a smaller compact model is trained to reproduce the behaviour of a larger model or an ensemble of models. The larger model is often called the teacher and the smaller model is called the student. In knowledge distillation the student network minimizes a loss function where the target is the distribution of class probabilities predicted by the teacher. This probability distribution generally has the correct class at a high probability while the other classes have near zero probability. Knowledge Distillation can be thought of the teacher network teaching the student how to produce outputs like itself. Both of the networks are fed same input. While the target of the teacher network are the actual labels, the student network is rewarded for mimicking the behaviour of the teacher network. \n\nThe student is trained with a distillation loss over the soft target probabilities of the teacher. Following softmax temperature is also used where t controls the smoothness of the output distribution. It is shown in Equation (2). \n\n(2)",
            "score": 0.5846056058729768,
            "section_title": "Knowledge distillation",
            "char_start_offset": 23088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1112
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3193359375
        },
        {
            "corpus_id": "269921267",
            "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
            "text": "Knowledge distillation, a core strategy in modern machine learning, focuses on solving the problem of balance between model size and computational efficiency.The core idea is to effectively transfer the deep knowledge and experience accumulated in large-scale, complex models (often referred to as \"teacher models\") to \"student models\" with smaller numbers of participants and lower computational requirements.\n\nSince then, this technology has rapidly attracted widespread attention from academia and industry, and has been expanded and deepened in several ways: Early stage (2015-2018) : Initial research has focused on simplifying network structure, reducing model volume and computational requirements, while maintaining the predictive performance of the model.In this period, the basic framework of knowledge distillation and the design of loss function were established.Technical deepening (2019-2021) : Researchers began to explore more refined distillation methods, including multi-teacher distillation, feature-stage distillation, relational distillation, etc.\n\nIn the field of deep learning, model knowledge lies in the configuration of parameters it learns through training, which guides the model on how best to extract features from input data and make predictions.Large networks, thanks to their large number of parameters and complex structure design, can capture deeper feature associations on large-scale data sets, showing superior learning ability and generalization performance.However, this advantage is often difficult to play directly in resource-constrained real-world application scenarios, because they have high requirements for computing resources and storage space.Knowledge distillation technology is born to solve this contradiction, it focuses not only on the final classification or regression results of the model output, but also on the teaching of the teacher model's confidence distribution (i.e., soft label) that each sample belongs to various categories.By designing a specific training mechanism, the student model tries to imitate the soft decision-making process while learning the real label, so that it can also \"inherit\" the decision logic and deep understanding of the data of the teacher model under limited parameters.This process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.",
            "score": 0.5844731578692366,
            "section_title": "B. Knowledge distillation",
            "char_start_offset": 12089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 410
                },
                {
                    "start": 412,
                    "end": 764
                },
                {
                    "start": 764,
                    "end": 875
                },
                {
                    "start": 875,
                    "end": 1068
                },
                {
                    "start": 1070,
                    "end": 1277
                },
                {
                    "start": 1277,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1693
                },
                {
                    "start": 1693,
                    "end": 1993
                },
                {
                    "start": 1993,
                    "end": 2266
                },
                {
                    "start": 2266,
                    "end": 2446
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57568359375
        },
        {
            "corpus_id": "271050394",
            "title": "Federated Knowledge Transfer Fine-tuning Large Server Model with Resource-Constrained IoT Clients",
            "text": "Hinton et al. have first introduced knowledge distillation [Hinton et al., 2015].Their work employs a weighted sum of the hard and soft loss as the complete loss.The soft loss is the loss between the soft outputs of the student model and the soft labels generated by the teacher model, and the hard loss is the loss between the hard outputs of the student model and the real labels.Adriana Romero et al. [Adriana et al., 2015] introduce knowledge distillation based on hidden layer knowledge features (hints).Zhang et al. [Zhang et al., 2018] propose mutual distillation, enabling different models to mutually distill knowledge from one another.",
            "score": 0.5844039633453204,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6213,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 81,
                    "end": 162
                },
                {
                    "start": 162,
                    "end": 382
                },
                {
                    "start": 382,
                    "end": 509
                },
                {
                    "start": 509,
                    "end": 645
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 80,
                    "matchedPaperCorpusId": "268553763"
                },
                {
                    "start": 404,
                    "end": 426,
                    "matchedPaperCorpusId": "2723173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12420654296875
        },
        {
            "corpus_id": "252262956",
            "title": "A Novel Knowledge Distillation Method for Self-Supervised Hyperspectral Image Classification",
            "text": "As the number of network layers deepens, current deep-learning models are becoming more and more complex, while the computational resources required to consume them become increasingly large. To alleviate this problem, Hinton et al. proposed the knowledge distillation method [53]. Traditional knowledge distillation methods train a teacher model on a known dataset and then supervise the training of a student model using the soft labels of the teacher model as well as the real labels. In general, the higher the training accuracy of the teacher model compared to the student model, the more effective the distillation effect is [36,54]. According to the present traditional method, a series of novel distillation models have been proposed [55][56][57][58]. Traditional knowledge distillation between models often suffers from inefficient knowledge transfer and requires a lot of experimentation to find the optimal teacher model. For this reason, a novel approach to knowledge distillation is proposed. This is called self-distillation, where the network itself acts as both a teacher model and a student model. Knowledge distillation usually takes place between different layers of the network. In [59], a self-distillation strategy is proposed that achieves improved computational efficiency by designing in a new network structure for knowledge distillation at each layer of the network. Additionally, in [60], the simultaneous use of soft labels and feature maps to achieve knowledge distillation is proposed. Since knowledge distillation enables the knowledge contained in the teacher model to be transferred to the student model, the trained student model can be utilized to achieve good classification results using only a small number of labeled samples. Given the advantages of knowledge distillation on a limited sample dataset, we added it to the training. This is different from traditional methods, which use teacher models to generate soft labels. In summary, we are implementing hierarchical prediction by adding a fully connected layer to each layer of the network and combining it with soft labels to achieve knowledge distillation. Since no teacher model is used, it can be said that this is a self-distilling approach.",
            "score": 0.58419692379614,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10630,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2240
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 280,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 635,
                    "end": 638,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 742,
                    "end": 746,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 746,
                    "end": 750,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 750,
                    "end": 754,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 1202,
                    "end": 1206,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40625
        },
        {
            "corpus_id": "265444951",
            "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
            "text": "The goal of KD [8] is to transfer the dark knowledge encapsulated in the soft probability output of the teacher model to the student model. In classification tasks, the softened probabilities are computed via the temperature-scaled softmax function, given by \n\nwhere p i (T ) is the probability output for class i softened by the temperature hyperparameter T , y i represents the logit for class i, and C is the total number of classes. Typically, T is set to greater than 1 in KD. The higher value of T produces softer probabilities, which are crucial for unveiling the dark knowledge hidden in the inter-class relationships captured by the teacher. The core idea of KD lies in minimizing the KL divergence loss function to align the soft logits of the teacher and student. The loss of KD L KD is defined as \n\nwhere p T and p S are the output logits of the teacher T and the student S, respectively. A notable limitation of KD is its uniform treatment of all samples, regardless of their inherent difficulty. In practice, different samples may present varying levels of challenge, a one-size-fits-all approach might not optimally transfer the insights of the teacher. This paper aims to address this limitation by introducing a sample-wise reweighting scheme. The original KD method computes the distillation loss using the logits from both teacher and student. ER-KD introduces a novel step by reweighting the loss with the entropy of the teacher's predictions at the instance level. This entropy serves as an indicator of sample difficulty, guiding the student to focus more on challenging samples. By ensuring a balanced knowledge transfer, ER-KD reduces the student's overconfidence prediction and aligns it more closely with the teacher's.",
            "score": 0.5841359109301532,
            "section_title": "Preliminaries",
            "char_start_offset": 9275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 258
                },
                {
                    "start": 261,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 808
                },
                {
                    "start": 811,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1745
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8193359375
        },
        {
            "corpus_id": "249017724",
            "title": "Improving the Latent Space of Image Style Transfer",
            "text": "Knowledge distillation (KD) [26,27,28] is a model compression method, in which a student network is trained by learning the knowledge from a teacher network. The knowledge is expressed in the form of softened probability [28,29], which can reflect the inherent class similarity structure known as dark knowledge. The distillation objective encourages the output probability distribution over predictions from the student and teacher networks to be similar. With the help of additional information on top of the one-hot labels, the performance of student network can be boosted. This dark knowledge is mainly related to labels, so they are rarely used in low-level vision tasks (e.g., neural style transfer). Wang et al. [10] developed a collaborative knowledge distillation method to learn a much smaller model from pre-trained redundant VGG-19 for ultra-resolution style transfer. In our method, the pre-trained encoder is regarded as a regularizer to guarantee that the features extracted by the new encoder are near a suitable value.",
            "score": 0.5833185260056334,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1036
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 38,
                    "matchedPaperCorpusId": "102351826"
                },
                {
                    "start": 221,
                    "end": 225,
                    "matchedPaperCorpusId": "102351826"
                },
                {
                    "start": 225,
                    "end": 228,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 720,
                    "end": 724,
                    "matchedPaperCorpusId": "213178672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7177734375
        },
        {
            "corpus_id": "271356843",
            "title": "A Lightweight Fault Diagnosis Method Based on Knowledge Distillation Under Time-Varying Rotational Speeds",
            "text": "The core concept of knowledge distillation is that the deep knowledge accumulated in the complex pre-trained teacher network model is distilled and injected into the more streamlined student model, which is able to inherit and mimic the predictive performance of the teacher model through knowledge distillation, thus reducing the model complexity while maintaining similar predictive effects. Knowledge distillation aims to optimize the balance between model size and performance through knowledge transfer to achieve effective model compression and knowledge transfer. The principle of knowledge distillation is shown in Figure 2. In this paper, we adopt the knowledge transfer mechanism, which is a framework in which the student network learns by simulating the output of the teacher network. In this framework, the student network is trained under the guidance and supervision of the teacher network, which draws on the a priori knowledge and generalization ability of the pre-trained teacher network, and helps to improve the learning efficiency and effectiveness. \n\nOffline distillation follows the classical ''teacher-student'' paradigm, so that the student network, in the process of imitating the teacher network, not only learns the decision-making behavior of the teacher network on the training samples, but also passes on the deep patterns and abstract knowledge contained in the teacher network, thus realizing the effective transfer and compression of knowledge, and the degree of fit of the student network to the output of the teacher network is quantified by the loss function. The degree of fitting of the teacher network output results by the loss function can be expressed as follows: \n\nwhere L task denotes the learning performance of the student network on the original task, Ls is the output vector of the student network and Lt is the output vector of the teacher network, which L KD is used to quantify the difference between the student network and the teacher network at the output level. In order to integrate the loss of the student network's own learning task with the loss of imitating the teacher network's output, a balance coefficient is introduced, which \u03bb is used to regulate the relative weights of the two in the total loss function.",
            "score": 0.5831276734859647,
            "section_title": "C. KNOWLEDGE DISTILLATION",
            "char_start_offset": 10854,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2273
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "264590688",
            "title": "Towards Generalized Multi-stage Clustering: Multi-view Self-distillation",
            "text": "To combat the overconfidence of pseudo-labels, we designed a self-distillation algorithm. Specifically, the teacher network outputs k-dimensional features and converts one-dimensional pseudo-labels into k-dimensional dark knowledge by adjusting the temperature and adding a Softmax activation function. The dark knowledge obtained by the final distillation is used as the ground truth, and the KL divergence is used to measure its similarity to the output of the student network.",
            "score": 0.5830927271888844,
            "section_title": "A. Framework Overview",
            "char_start_offset": 16083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 479
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68603515625
        },
        {
            "corpus_id": "237492204",
            "title": "On the Efficiency of Subclass Knowledge Distillation in Classification Tasks",
            "text": "In many real-world classification problems, each labeled class has a number of available semantically meaningful subclasses. For example, in the cancer diagnosis task, which involves the detection of benign and abnormal lesions, the abnormal class may have multiple subclasses in which each of them can express different types or organs of cancer disease (Oakden- Rayner et al. 2020;Mlynarski et al. 2019). Models trained exclusively on class labels often ignore the fine-grained knowledge of subclasses, which can have an effect on model training, particularly for clinical tasks such as cancer detection (Sohoni et al. 2020;Oakden-Rayner et al. 2020). We can take advantage of this subclass knowledge by forcing the teacher model to train the subclass labels. Then the knowledge can be transferred from the teacher to the student network. \n\nThe relative probabilities of incorrect class prediction (i.e., dark knowledge) can reveal a lot about the teacher generalization tendencies of the teacher. Soft targets probability of the teacher can extract and use the dark knowledge in conventional Knowledge Distillation (KD) (Hinton, Vinyals, and Dean 2015). As long as we distill the teacher's knowledge using soft logits at a high temperature, the amount of information how the teacher generalizes is linear in the number of classes (M\u00fcller, Kornblith, and Hinton 2020). When datasets contain many classes, knowledge transfer from teacher to student is typically successful, as arXiv:2109.05587v3 [cs.LG] 5 Jul 2022 \n\nthe teacher has more relevant information about the function being taught (M\u00fcller, Kornblith, and Hinton 2020). Meanwhile, in classification tasks with a few classes or binary detection problems, the amount of information available to the student about the teacher's generalizability is restricted, thus limiting the utility of the KD approach. To address this problem, we can leverage hidden subclass knowledge, the knowledge of available subclasses that is not captured in the teacher's class logits.",
            "score": 0.582912881458068,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1515
                },
                {
                    "start": 1518,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 364,
                    "end": 383,
                    "matchedPaperCorpusId": "203591636"
                },
                {
                    "start": 626,
                    "end": 652,
                    "matchedPaperCorpusId": "203591636"
                },
                {
                    "start": 1123,
                    "end": 1155,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58984375
        },
        {
            "corpus_id": "246867020",
            "title": "Meta Knowledge Distillation",
            "text": "In the original formulation of knowledge distillation (KD) (Hinton et al., 2015), a student model is trained to match the targets provided by a teacher model, which tries to provide more information than the ground truth one-hot labels. The targets are usually softened by a fixed temperature in the softmax function and the entropy increases as the temperature increases. \n\nFormally, we consider a student model S with parameters \u03b8 s for distilling the teacher. We denote the output logits from the teacher and the student as z s and z t , respectively. The student is trained to minimize the cross-entropy (CE) loss between its predicted probability p s and the teacher's output probability p t , where p s and p t are softened by their corresponding temperatures \u03c4 s and \u03c4 t respectively. \n\nwhere p s = softmax(z s /\u03c4 s ), p t = softmax(z t /\u03c4 t ). \n\nIn prior arts, \u03c4 s and \u03c4 t are manually chosen as the same constant greater than 1. However, as we discussed in Sec. 2, the temperatures play a key role in distillation, and the optimal temperatures vary in different distillation setups, which are difficult to tune manually. Besides, the temperatures control the degree of softness of the targets provided by the teacher. Using a fixed temperature for different distillation setups, including teach/student architectures, augmentation types, datasets, can be sub-optimal. Intuitively, the temperatures are expected to be adjusted according to according to the different training setups. \n\nWe take an explicit approach to tackle this problem: we aim at meta-learning the temperatures of both the teacher and student to dynamically adjust the softness of the learning targets according to the student's on-the-fly performance on the validation set, \n\nwhere L t and L v denote the training and validation losses, and \u03c6 = {\u03c4 s , \u03c4 t } are meta-parameters (temperatures) to be optimized on the meta level.",
            "score": 0.5815594447762774,
            "section_title": "Problem Formulation",
            "char_start_offset": 9833,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 372
                },
                {
                    "start": 375,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 791
                },
                {
                    "start": 794,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1491
                },
                {
                    "start": 1494,
                    "end": 1751
                },
                {
                    "start": 1754,
                    "end": 1905
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.475830078125
        },
        {
            "corpus_id": "271356843",
            "title": "A Lightweight Fault Diagnosis Method Based on Knowledge Distillation Under Time-Varying Rotational Speeds",
            "text": "In order to integrate the loss of the student network's own learning task with the loss of imitating the teacher network's output, a balance coefficient is introduced, which \u03bb is used to regulate the relative weights of the two in the total loss function. In addition, in order to optimize the student network's learning of the teacher network's output distribution, a temperature parameter T is employed to make the output probability distribution of the teacher network smoother and more balanced, thus prompting the student network to capture and learn more comprehensively the decision boundaries and intrinsic knowledge structure of the teacher network. By minimizing the loss function, the student network can be made to fit the teacher network as closely as possible, making the learning process more efficient. Usually, the prediction result of the model represents the probability prediction of each category in the classification task after the Softmax classification layer, and the probability of each category obtained in this way often does not contain the similarity information between the categories, which will weaken the feature information learned by the model to a certain extent. Therefore, the teacher model introduces a temperature factor T in Softmax to obtain the similarity information between categories in the classification task, as shown in equation (6). \n\nDuring the training process of the student model, knowledge distillation introduces the prediction result of the teacher model as an additional target while learning the error between the input data and the real sample label. The cross-entropy loss function is chosen as the calculation function of the loss between the model test probability values and the real labels, then the loss after distillation of the teacher model and the student model L soft as well as the loss between the predicted values of the student model and the real labels L hard are shown in Eqs. ( 7) and ( 8), respectively: \n\nwhere p j and q j denote the predicted probability distributions of the teacher and student models after distillation of the temperature factor T, respectively, c j for the real label of the classification task, and q j denote the predicted probability distribution of the student model when the temperature factor T is 1, which is also referred to as the hard-labeled prediction of the student model. The loss of the whole knowledge distillation model is L soft and L hard two parts, as shown in equation ( 9):",
            "score": 0.581010256506011,
            "section_title": "C. KNOWLEDGE DISTILLATION",
            "char_start_offset": 12872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1384
                },
                {
                    "start": 1387,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1984
                },
                {
                    "start": 1987,
                    "end": 2388
                },
                {
                    "start": 2389,
                    "end": 2498
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4873046875
        },
        {
            "corpus_id": "275920765",
            "title": "The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model",
            "text": "Knowledge distillation (KD) Hinton et al. [2015] is a technique in machine learning that transfers the learned information from a complex model (often referred to as the teacher) to a simpler model (the student). This method attracted attention for achieving model compression with minimal performance loss, and has been applied across various domains, including image classification Liu et al. [2018], Xu et al. [2020], object detection Chen et al. [2017], and natural language processing Calderon et al. [2023], Gu et al. [2024]. \n\nAmong the various forms of KD, self-distillation (SD), originally termed born again neural network Furlanello et al. [2018] is particularly intriguing. In SD, the teacher and student models share Preprint. Under review. arXiv:2501.16226v3 [stat.ML] 17 May 2025 identical architectures. This means that SD does not attempt the model compression; rather, it retrains the student model using the teacher's output. SD presents a intriguing paradox: despite training an identical model on the same dataset, the student model can outperform the teacher Furlanello et al. [2018], Hahn and Choi [2019], Clark et al. [2019]. \n\nTwo main hypotheses have been proposed to explain such seemingly puzzling performance gains. The first suggests that the soft labels generated by the teacher provide dark knowledge Hinton et al. [2015]. \n\nHere, dark knowledge refers to the information implicitly embedded in the prediction probability distribution of the teacher model's output, which is absent in hard labels. It provides the student with additional information that captures subtle relationships within the data. The second hypothesis attributes the improvement to a denoising effect Das and Sanghavi [2023], Das et al. [2024] where the teacher model reduces the influence of the incorrect noisy labels in the training data, enabling the student model to learn a more reliable representation of the underlying patterns Pareek et al. [2024]. \n\nAlthough these hypotheses offer plausible explanations, the optimal behavior of SD, achieved through hyperparameter optimization and repeated iterations Pareek et al. [2024], remains poorly understood.",
            "score": 0.5809332839159029,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 531
                },
                {
                    "start": 534,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1961
                },
                {
                    "start": 1964,
                    "end": 2165
                }
            ],
            "ref_mentions": [
                {
                    "start": 384,
                    "end": 401,
                    "matchedPaperCorpusId": "52290108"
                },
                {
                    "start": 403,
                    "end": 419,
                    "matchedPaperCorpusId": "221559239"
                },
                {
                    "start": 438,
                    "end": 456,
                    "matchedPaperCorpusId": "29308926"
                },
                {
                    "start": 633,
                    "end": 657,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1081,
                    "end": 1105,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1705,
                    "end": 1728,
                    "matchedPaperCorpusId": "256416199"
                },
                {
                    "start": 1940,
                    "end": 1960,
                    "matchedPaperCorpusId": "271039191"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69140625
        },
        {
            "corpus_id": "250526328",
            "title": "Large\u2010scale knowledge distillation with elastic heterogeneous computing resources",
            "text": "In this section, we present the background of elastic deep learning distillation, i.e., knowledge distillation. Then, we introduce the methods for distributed and decentralized training and elastic computing resources. \n\nModel compression based on knowledge transferring was first proposed in 7 in order to compress an ensemble of models or a large model to a compact model. The ensemble of models or the large model take much storage space and require a long time for inference while the compact model requires relatively small storage space and a short time for inference. Knowledge distillation is based on the popular machine learning Softmax function and a temperature 8 , which is defined in Formula 1. \n\n, where is the output from the output layer of a teacher neural network; is a temperature, which indicates the impact of the output from the teacher model. The Softmax output layer computes the probability that the input data corresponds to each class with the corresponding computed logit. The probability is related to a temperature T, i.e., which represents the impact of the distilled knowledge of the teacher model on the student model. A higher temperature corresponds to a weaker impact. As shown in Figure 1, during the training of knowledge distillation, two neural networks are used: teacher model and student model. The student model is trained based on the combination of two loss values. The first loss value is calculated from a soft prediction, which contains the probability of each class calculated from the teacher model. The soft prediction is calculated using Formula 1. The other loss value corresponds to a hard prediction, which is the ground truth label from the training data. \n\nTwo types of work are also related to this paper, i.e., mutual knowledge distillation and model pruning. When there is no available teacher model, the teacher and student can be trained at the same time, i.e., an ensemble of students can learn collaboratively and teach each other throughout the training process 16,17 , which is the mutual knowledge distillation 9 . Note that the mutual knowledge distillation is also denoted \"online knowledge distillation\" in 9 . However, we use the \"online knowledge distillation\" to represent the knowledge distillation with the teacher model and the student model deployed in the same GPU card during the training process of knowledge distillation in this paper.",
            "score": 0.5808538761117696,
            "section_title": "RELATED WORK",
            "char_start_offset": 3556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 218
                },
                {
                    "start": 221,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2417
                }
            ],
            "ref_mentions": [
                {
                    "start": 293,
                    "end": 294,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 674,
                    "end": 675,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 2028,
                    "end": 2031,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 2031,
                    "end": 2033,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 2079,
                    "end": 2080,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 2178,
                    "end": 2179,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41015625
        },
        {
            "corpus_id": "3643430",
            "title": "Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy",
            "text": "We introduce the concept of knowledge distillation in this section. Bucilu\u01ce et al. (2006), Hinton et al. (2015) and Urban et al. (2016) analyze this topic in great detail. \n\nwhere, W T and W A are the parameters of the teacher and the student (apprentice) network, respectively, y is the ground truth, H(\u2022) denotes a loss function and, \u03b1, \u03b2 and \u03b3 are weighting factors to prioritize the output of a certain loss function over the other. In equation 1, lowering the first term of the cost function gives a better teacher network and lowering the second term gives a better student network. The third term is the knowledge distillation term whereby the student network attempts to mimic the knowledge in the teacher network. In Hinton et al. (2015), the logits of the teacher network are divided by a temperature factor \u03c4 . Using a higher value for \u03c4 produces a softer probability distribution when taking the Softmax of the logits. In our studies, we use cross-entropy function for H(\u2022), set \u03b1 = 1, \u03b2 = 0.5 and \u03b3 = 0.5 and, perform the transfer learning process using the logits (inputs to the Softmax function) of the teacher network. In our experiments we study the effect of varying the depth of the teacher and the student network, and the precision of the neurons in the student network.",
            "score": 0.5807605607723337,
            "section_title": "KNOWLEDGE DISTILLATION",
            "char_start_offset": 12530,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 171
                },
                {
                    "start": 174,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1291
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 89,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4599609375
        },
        {
            "corpus_id": "252596141",
            "title": "Label driven Knowledge Distillation for Federated Learning with non-IID Data",
            "text": "Knowledge Distillation (KD) Hinton et al. (2015) employs a technique to transfer the learned knowledge from a pre-trained teacher model to another model with less or similar complexity (student model). The model transfer process is implemented in two steps. In the first step, a surrogate output probability function, namely a temperature-softmax function, is utilized. By adding a temperature scaling variable T , the conventional softmax function then becomes: \n\nwhere z j is the output set corresponding to class j of the given DNN with batch of input data X. \n\nThe subscript l denotes the index of softmax output which corresponds to the prediction on class l of the DNN. The intention of adding the variable T is to adjust the slope of the softmax function in the classifier as shown in equation 16. As we can see, when we increase the value of T , the slope of the softmax function will decrease significantly. With large temperature scale values, over the same output range of data, the range of values represented by input z j is larger. Then, the output value tuple created by the teacher and student carries considerably more information. Therefore, the learning process between teacher and student is more effective. \n\nIn the second step, to help implement the transfer of knowledge from teacher to student, the authors in Hinton et al. (2015) presented a new loss function, called distillation loss function (also known as soft-loss function). This loss function comprises two terms: the intrinsic entropy function of the teacher and the cross-entropy function between the teacher and the student's outputs, which can be expressed as follows: \n\nThe purpose of this function is to compare the output distribution between teacher p(X) and student model q(X). In terms of information theory, this measurement shows the under-performance of the distribution set created by student, when the output distribution set of the teacher is taken as the sample distribution set, with precision of 100%. By minimizing this function, we reduce the functional difference between the two deep networks. As a result, the student model tends to make it's output distribution become more identical to the teacher model's behavior.",
            "score": 0.5806646816246094,
            "section_title": "A.2 KNOWLEDGE DISTILLATION",
            "char_start_offset": 25127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 462
                },
                {
                    "start": 465,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1227
                },
                {
                    "start": 1230,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1654
                },
                {
                    "start": 1657,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2223
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.791015625
        },
        {
            "corpus_id": "212634266",
            "title": "Knowledge distillation via adaptive instance normalization",
            "text": "Knowledge distillation with neural networks, pioneered by Hinton et al. [5] and Bucilu et al. [16], is a transfer learning method that enhances the accuracy of a lightweight student network based on the \"knowledge\" provided by a powerful teacher network. In [5], knowledge is defined as the teacher's soft outputs after the final softmax layer. The soft outputs carry richer information than onehot labels because they provide extra supervision signals in terms of the interclass similarities learned by the teacher. This idea is further explored by other works. Zhou et al. [17] proposed a novel method which introduces a booster net which is trained together with the student network and a logit matching loss for supervising the student's prediction. Another work that also trains the student and teacher simultaneously is [18], in which each student is trained with a conventional loss for supervised learning, and with a mimicry loss that aligns each students class posterior with the class probabilities of a teacher. Recently, [19] found that very accurate networks are \"too good\" to be good teachers and proposed to mitigate this with early stopping of the teacher's training. \n\nIn a similar fashion to [5], intermediate representations extracted from the teacher such as feature tensors [12] or attention maps [6] have been used to define loss functions used to facilitate the optimization of the student. Trying to match the whole feature tensor, as in FitNets [12], is hard and, in certain circumstances, such an approach may adversely affect the performance and convergence of the student. To relax the assumption of FitNet, Attention Transfer (AT) was proposed in [6] where knowledge takes the form of attention maps which are summaries of the energies of the feature tensors over the channel dimension. An extension of [6] using Maximum Mean Discrepancy of the network activations as a loss term for distillation was proposed in [20]. \n\nIn [21], the authors advocate the use of the activation boundary of neurons (rather than their exact output values), and propose a knowledge transfer method via distillation of activation boundaries formed by hidden neurons.",
            "score": 0.5803052095946734,
            "section_title": "Related Work",
            "char_start_offset": 5329,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1948
                },
                {
                    "start": 1951,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 98,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.595703125
        },
        {
            "corpus_id": "268063805",
            "title": "T3DNet: Compressing Point Cloud Models for Lightweight 3-D Recognition",
            "text": "The basic idea of knowledge distillation (KD) [21] is to transfer knowledge from a large, powerful teacher network to a small, lightweight student network by learning the soft class distributions after the SoftMax layer. The student network then mimics the teacher model to improve its performance. Deploying deep learning models on embedded systems or mobile robots can be challenging due to their limited computational capacity and memory. Knowledge distillation offers a solution to this problem by allowing us to carefully allocate resources such as memory and computation. For real-time human-robot interaction applications, knowledge distillation is a promising research focus [51]- [53]. \n\nLogit-based knowledge typically refers to the output of the softmax layer of the teacher model. Early, or \"vanilla,\" knowledge distillation methods used the logits of a deep, complex model as \"dark knowledge\" [21]. Logit-based knowledge distillation is a simple but effective approach for achieving good performance in compression tasks and has gained popularity in solving various tasks and applications. The most popular logit-based knowledge for classification tasks is the soft target [21], [54]. In this approach, the \"dark knowledge\" is usually evaluated using the Kullback-Leibler divergence loss, which minimizes the difference between the student and teacher logits. The vanilla approach to KD involves joint training of the distillation and student losses. The student loss is typically set as the cross-entropy loss between the ground truth and the softmax class prediction of the student model. \n\nOne issue with logit-based distillation is that it can be difficult for very deep networks to learn the logit knowledge, and it is also limited to supervised learning. To address these limitations, the concept of representation learning [55] can be applied to knowledge distillation. In representation learning, multiple levels of features with increasing abstraction are learned, which can be particularly helpful for thinner and deeper networks in distillation. FitNet [27] was the first to introduce the concept of intermediate representation in knowledge distillation, using the hidden layers of the teacher model to supervise the learning of the student model. This type of knowledge, derived from feature activations, is known as \"hint\".",
            "score": 0.5799756061975982,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 12981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2271
                },
                {
                    "start": 2272,
                    "end": 2349
                }
            ],
            "ref_mentions": [
                {
                    "start": 683,
                    "end": 687,
                    "matchedPaperCorpusId": "195893309"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "238239298"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 1843,
                    "end": 1847,
                    "matchedPaperCorpusId": "393948"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78173828125
        },
        {
            "corpus_id": "258079331",
            "title": "Do we need Label Regularization to Fine-tune Pre-trained Language Models?",
            "text": "This classical method of transferring knowledge gained traction after the paper (Hinton et al., 2015). Assume that we have a trained network (a teacher) and a network we want to train (a student). Let p t (x) and p s (x) be teacher's and student's predictions respectively. One wants to transfer the knowledge from the teacher to the student. For that, one can formulate a total loss for KD as:\n\nwhere H(q, p) is the cross-entropy loss and L KD = D KL (p t \u03c4 , p s \u03c4 ) is a KL divergence between the teacher's and the student's outputs scaled with the temperature \u03c4 , i.e., p \u03c4 (k) = sof tmax(z k /\u03c4 ), where z k is the output logits of the model. When \u03c4 = 1, KD training is equivalent to cross-entropy training with the new labels \"smoothed\" by the teacher:\n\n(2)",
            "score": 0.5798487241608281,
            "section_title": "Knowledge distillation (KD)",
            "char_start_offset": 5995,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.171630859375
        },
        {
            "corpus_id": "227335305",
            "title": "Model Compression Using Optimal Transport",
            "text": "The earliest methods of knowledge distillation (KD) were initially developed by Bucilu\u01ce et al. [5] and Hinton et al. [19] and has proven to be an effective method for transferring the knowledge learned by a complex teacher neural network into a simpler student network. Here, the basic idea is to use the softmax outputs of the last layer of the teacher network (i.e., soft labels) in the loss to train the student network, along with the usual cross-entropy loss to map to the one-hot vector given by the ground-truth label. Kullback-Liebler (KL) divergence, or a closely related variant, is used as the additional loss function between student outputs and the teacher's outputs, as both are probabilty distributions over the class labels and share support. In this paper, following recent works, we will refer to this loss as the KD loss. However, KD loss takes into account only the final outputs. Subsequent papers have looked at ways of exploiting intermediate layer outputs of the teacher network (i.e. from internal network layers) to further guide the training of the student network as in FitNets [35], attention transfer [47], relational knowledge distillation (RKD) [29], probabilistic knowledge transfer [30] etc. When combined with KD, all these methods result in significant improvements over KD alone. FitNets tries to match intermediate feature maps of teachers and students exactly, which may be too rigid of a constraint. On the other hand, RKD first computes pairwise relationships among teacher and student features in a batch and matches these geometric relationships between intermediate feature maps, rather than the feature maps themselves, which may be too loose of a constraint. Attention transfer (AT) [47] matches intermediate attention maps which are derived from intermediate feature maps. \n\nIn contrast to these methods, we propose a new loss for knowledge distillation based on optimal transport, to complement the standard KD loss. The central idea is to use optimal transport (OT) distance to measure the distance between the distributions of teacher and student feature maps in intermediate layers of the network. The OT loss function can be seen as a compromise between FitNets and RKD.",
            "score": 0.5798024231615303,
            "section_title": "Knowledge distillation methods",
            "char_start_offset": 3107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1819
                },
                {
                    "start": 1822,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2148
                },
                {
                    "start": 2149,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 95,
                    "end": 98,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1177,
                    "end": 1181,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1216,
                    "end": 1220,
                    "matchedPaperCorpusId": "52012952"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6376953125
        },
        {
            "corpus_id": "233714221",
            "title": "Self-Knowledge Distillation with Progressive Refinement of Targets",
            "text": "KD is a technique to transfer knowledge from one model (i.e., a teacher) to another (i.e., a student), usually from a larger model to a smaller one. The student learns from more informative sources, the predictive probabilities from the teacher, besides one-hot labels. Hence, it can attain a similar performance compared to the teacher although it is usually a much smaller model, and show comparable or even better performance when the student has the same capacity as the teacher [8,48].\n\nFor an input x and a K-dimensional one-hot target y, a model produces the logit vector z(x) = [z 1 (x), \u00b7 \u00b7 \u00b7 , z K (x)], and then outputs the predicted probabilities P (x) = [p 1 (x), \u00b7 \u00b7 \u00b7 , p K (x)] by a softmax function. Hinton et al. [17] suggest to utilize temperature scaling to soften these probabilities for better distillation:\n\nwhere \u03c4 denotes a temperature parameter. By scaling the softmax output P T (x) of the teacher as well as P S (x) of the student, the student is trained with the loss function L KD , given by:\n\nwhere H is a cross-entropy loss and \u03b1 is a hyperparameter. Note that when the temperature \u03c4 is set to 1, Eq. (2) is equivalent to the cross-entropy of P S (x) to the soft target, a linear combination of y and P T (x):\n\nTherefore, the existing methods that use the soft targets for regularization can be interpreted within the framework of knowledge distillation. For example, LS [25] is equivalent to distilling the knowledge from the teacher which produces uniformly distributed probabilities on any inputs.",
            "score": 0.5794042505164527,
            "section_title": "Knowledge Distillation as Softening Targets",
            "char_start_offset": 8351,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 483,
                    "end": 486,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 486,
                    "end": 489,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 731,
                    "end": 735,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1403,
                    "end": 1407,
                    "matchedPaperCorpusId": "174802983"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.441650390625
        },
        {
            "corpus_id": "270123240",
            "title": "Federated and Transfer Learning for Cancer Detection Based on Image Analysis",
            "text": "Knowledge distillation transfers knowledge from a complex model (teacher) to a simpler model (student), aiming to retain much of the teacher's performance.\n\nwhere L hard is the traditional loss (e.g., cross-entropy against true labels), L soft is the distillation loss (e.g., KL divergence between the teacher's and student's predictions), \u03b1 is a hyperparameter balancing the two losses, and T is the temperature scaling the logits before applying softmax.\n\nOverall, TL provides a broad framework for leveraging knowledge from related tasks while Fine-Tuning focuses on adjustments to a pre-trained model for a new task.On the other hand, domain adaptation addresses the discrepancy between source and target domain distributions.Moving on, knowledge distillation focuses on transferring knowledge from a complex model to a simpler one.",
            "score": 0.57895438462899,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 20876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 157,
                    "end": 456
                },
                {
                    "start": 458,
                    "end": 620
                },
                {
                    "start": 620,
                    "end": 730
                },
                {
                    "start": 730,
                    "end": 836
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3193359375
        },
        {
            "corpus_id": "258714078",
            "title": "Mitigating carbon footprint for knowledge distillation based deep learning model compression",
            "text": "KD is a well-known tool to compress a possibly larger (teacher) model, F m \u00f0X; W m \u00de to a smaller (student) model F s \u00f0X; W s \u00de. Here, the input space X belongs to label space y 2 R n , where n represents input the number of classes available in the dataset. We assume a supervised classification setting where model outputs are a m \u00bc F m \u00f0X; W m \u00de; a s \u00bc F s \u00f0X; W s \u00de; fa m ; a s 2 R n g before activation such that a m and a s represent the un-activated output of teacher and student models, respectively. The student model outputs are activated using the softmax function [66] to produce a hard embedding h s such that h s (i) > > h s (j), {1 <= i, j <= n, i 6 \u00bc j} where the logits are probability distributions P n i\u00bc1 h s \u00f0i\u00de \u00bc 1; h s \u00f0i\u00de 2 R \u00fe . However, transfer of knowledge is done through proper softening of the output of the teacher a m and student model a s , which is done by a modified softmax activation containing a hyper-parameter \u03c4: \n\nThe acquisition of soft labels helps utilize the dark knowledge stored in the teacher model for proper knowledge transfer to the student model. The outputs h s , o s , o m and label y are used to provide gradient information for the student to learn better using the distillation loss which is back-propagated to update model parameters. The distillation loss (L DL ) consists of the weighted union of the softmax Cross Entropy (L CE ) [67] and the Kullback-Liebler Divergence (L KLD ) [68] loss with an added weight. \n\nKnowledge distillation varies from a traditional classification because of L KLD , which facilitates the student to learn. The hyper-parameter \u03c4 usually gets a value within the range \u03c4 = 2 to 20. \u03c4 contributes to softmax distributions with different levels of softness. Proper softening of the logits enables better distribution of information to the student, S. As a result, the hyperparameter temperature is essential in knowledge distillation for proper knowledge transfer.",
            "score": 0.5789476580303717,
            "section_title": "Solution using Knowledge distillation (KD) [26]",
            "char_start_offset": 12373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1952
                }
            ],
            "ref_mentions": [
                {
                    "start": 1392,
                    "end": 1396,
                    "matchedPaperCorpusId": "110510"
                },
                {
                    "start": 1442,
                    "end": 1446,
                    "matchedPaperCorpusId": "37718089"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4794921875
        },
        {
            "corpus_id": "235428017",
            "title": "KDAS-ReID: Architecture Search for Person Re-Identification via Distilled Knowledge with Dynamic Temperature",
            "text": "Transferring knowledge from a huge, cumbersome model to a smaller and simpler model without losing too much generalization ability has always been one of the classic problems studied by scholars in recent years. Knowledge distillation using neural networks was originally proposed by Hinton et al. [5], which aims at improving the training of student models by applying knowledge acquired from a powerful teacher model. Ba and Caruana et al. [24] demonstrated that shallow feed-forward nets can learn the complex functions that were previously learned by deep nets and achieve accuracies that were only achievable with deep models. Romero et al. [25] guided the students' training process by introducing intermediate hints from the teacher's hidden layer, compressing wide and deep networks into thin and deep networks. In [26], the author proposes to use activation-based and gradient-based spatial attention maps to transfer attention as a method of knowledge transfer from one network to another. Yim et al. [27] used the extracted two-layer feature map to generate the flow of the solution procedure (FSP) matrix and trained the student DNN to make the FSP matrix similar to the teacher DNN. Sau and Balasubramanian [28] recommended to use a noise-based regularizer when training the student model to learn from the teacher model, which can significantly enhance the performance of the student network. Since knowledge distillation is characterized by transferring complex models to simple ones, the NAS algorithm can be applied in Re-ID task by knowledge distillation. At the same time, due to the characteristics of NAS, it is considered that the distillation loss at dynamic temperature breaks the constraint of the teacher model. A searching and training is started at a high temperature, and the temperature will gradually drop to 1 in order to control the importance of teacher model knowledge.",
            "score": 0.5789022486365547,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1904
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 301,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1011,
                    "end": 1015,
                    "matchedPaperCorpusId": "206596723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.279052734375
        },
        {
            "corpus_id": "224801149",
            "title": "Multi-teacher Knowledge Distillation for Knowledge Graph Completion",
            "text": "Knowledge Distillation (KD) aims to transfer 'knowledge' from one machine learning model (i.e., the teacher) to another one (i.e., the student). Hinton et al. [10] introduce the first KD framework, which applies the classification probabilities of a trained model as 'soft labels' and defines a parameter called 'Temperature' to control the 'soft' degree of those labels. Inspired by this, several KD-based approaches are proposed in different research domains. Furlanello et al. [9] propose Born-Again Networks in which the student is parameterized identically to their teachers. Yang et al. [28] added an additional loss term to facilitate a few secondary classes to emerge and complement to the primary class. Li et al. [13] distill human knowledge from a teacher model to enhance pedestrian attribute recognition task. To the best of our knowledge, our work is the first to apply the KD technologies into the link prediction of KGs.",
            "score": 0.578814095315449,
            "section_title": "RELATED WORK",
            "char_start_offset": 35130,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 936
                }
            ],
            "ref_mentions": [
                {
                    "start": 480,
                    "end": 483,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 593,
                    "end": 597,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 723,
                    "end": 727,
                    "matchedPaperCorpusId": "199465690"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44873046875
        },
        {
            "corpus_id": "235421655",
            "title": "Energy-efficient Knowledge Distillation for Spiking Neural Networks",
            "text": "Efficiently transferring knowledge from large teacher network to small student network has been a traditional topic which has drawn more and more attention in recent years. Adopting this teacherstudent learning paradigm, many distillation methods have been investigated. [2] first proposed model compression that transfers information from a large model or from an ensemble of models to a small model without a significant drop in accuracy. [11] proposed the concept of soft label which is distilled from the teacher model to the student model. Soft labels are the probabilities of an input belonging to each class, and can be estimated by a softmax function. Here, a temperature factor is introduced to control the importance of each soft label, which contains the informative dark knowledge from the teacher model. By increasing the temperature, the logits can contain richer information than one-hot labels. However, if the temperature becomes too large, the probability of irrelevant classes will also be over-emphasized. \n\nExtra information other than the outputs, for example, the intermediate-level supervision from the teacher model, can additionally boost the performance of knowledge distillation. Application of the intermediate representations was first introduced in FitNet [26] to provide hints to improve the training of the student model. Thanks to the additional hint-based training, the trained deep student network showed better accuracy with fewer parameters compared to the original wide teacher network. \n\nTo explore the relationships between different feature maps, [40] proposed a flow of solution process (FSP), which is defined by the Gram matrix between two layers. Many other different algorithms have been proposed to improve the process of distilling knowledge in more complex settings. [3,15,38,41] As mentioned above, knowledge distillation has been extensively studied and effectively used for ANNs in the literature. However, some recent works show how off-the-shelf knowledge distillation commonly used in ANNs can be adapted for SNNs. [16] first proposed a study that applied knowledge distillation to SNNs. In this work, output spike train of the teacher SNN model was involved in the distillation process, but the performance was unsatisfactory.",
            "score": 0.5782952682047509,
            "section_title": "Knowledge distillation",
            "char_start_offset": 7213,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1525
                },
                {
                    "start": 1528,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2283
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 274,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1589,
                    "end": 1593,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1823,
                    "end": 1826,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1826,
                    "end": 1829,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76025390625
        },
        {
            "corpus_id": "275194698",
            "title": "DM-KD: Decoupling Mixed-Images for Efficient Knowledge Distillation",
            "text": "Knowledge distillation (KD) is a technique for transferring knowledge from a teacher to a student. KD can be largely divided into three types according to the type of information transmitted; logit distillation, feature distillation, and relation distillation. Logit distillation was originally suggested in Hinton et al. They hypothesized that the soft output of model contains its information, which is called 'dark knowledge', and student can acquire the knowledge of the teacher by using its output as another ground truth. Kim et al. [12] found that the spatial information of the image is lost due to the global average pooling of the model. So they distill not only global logits but also local logits to resolve the problem. Zhao et al. [30] divided the KD loss into the target class part and the non-target class part. They found that those two parts have different effect in distillation and suggested decoupling KD loss. There are some approaches that exploit multiple teachers for KD. Zhang et al. [29] showed that multiple teacher models learn simultaneously while transferring knowledge to each other, which helps improve generalization performance. Amirkhani et al. [1] proposed using multiple teacher models trained on various datasets. Each teacher model provides supervision from its respective dataset to the student model, enabling more robust and accurate learning. In the perspective of using multiple datasets for KD, Banitalebi-Dehkordi et al. [2] proposed energy-based knowledge distillation with multiple datasets with different domains to address a domain shift problem. \n\nIn [2], a teacher model is trained on the source domain and then inferred to the target domain dataset. The resulting logit uses a pseudo-label, and then they create a mixed domain dataset. So they train a student model on the mixed domain dataset, showing better performance. In the another direction, feature distillation aims to matching teachers and students features. Romero et al. [18] proposed that guide intermediate layer of student by matching intermediate features of student and teacher. Zagoruyko et al. [26] introduced the concept of attention to distillation.",
            "score": 0.5781479084594978,
            "section_title": "II. RELATED WORK A. KNOWLEDGE DISTILLATION",
            "char_start_offset": 6171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2099
                },
                {
                    "start": 2100,
                    "end": 2174
                }
            ],
            "ref_mentions": [
                {
                    "start": 539,
                    "end": 543,
                    "matchedPaperCorpusId": "244074368"
                },
                {
                    "start": 745,
                    "end": 749,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1010,
                    "end": 1014,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1181,
                    "end": 1184,
                    "matchedPaperCorpusId": "237374428"
                },
                {
                    "start": 1468,
                    "end": 1471,
                    "matchedPaperCorpusId": "260149056"
                },
                {
                    "start": 1603,
                    "end": 1606,
                    "matchedPaperCorpusId": "260149056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55224609375
        },
        {
            "corpus_id": "257079238",
            "title": "Distilling Calibrated Student from an Uncalibrated Teacher",
            "text": "The primary goal of KD is to distill the dark knowledge from the teacher network into the student network by comparing the temperature-scaled softmax logits of the teacher and student. The loss term generally consists of two losses, KL divergence loss (L div ) and a task-specific loss (L task ). \n\nwhere N is the total number of samples in the dataset. KL loss is the Kullback-Leibler Divergence score which describes the difference between the probability distribution of soft labels of teacher T and student S. In conventional KD [10], the KL-divergence loss and the task-specific loss are linearly combined as follows: \n\nwhere \u03b1 is a balancing factor. Generally, the value of \u03b1 is kept high to ensure high knowledge transfer from the teacher to the student, however, doing so makes the student network highly uncalibrated as shown in Fig. 1. Therefore, we scale the KL-divergence loss by taking mean across the dimension of the logits (i.e. dividing it by the logit size, which is same as number of classes). Using this scaled KLdivergence loss we observe a improvement in the calibration and a drop in accuracy, as shown in Fig. 2. We encounter a trade-off between accuracy and calibration error in the framework of KD1 . Our approach leverages the distillation framework to take care of the accuracy while improving the calibration with the help of data augmentation techniques.",
            "score": 0.5779000571249753,
            "section_title": "Knowledge distillation",
            "char_start_offset": 8738,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 296
                },
                {
                    "start": 299,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1384
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61328125
        },
        {
            "corpus_id": "254564348",
            "title": "Vertical Layering of Quantized Neural Networks for Heterogeneous Inference",
            "text": "Knowledge distillation transfers the knowledge learned by large models (i.e., teacher) to a compact one (i.e., student) via a weighted combination of original loss using ground-truth labels and distillation loss calculated with the soft output of a teacher model [47]. There is a broad consensus that knowledge distillation can help improve the performance of low-bit-width quantized networks by transferring the knowledge of a full precision network to quantized ones [48], [49], [50]. QKD [49] proposes a quantization-aware knowledge distillation scheme by co-training a low-bit-width student and a larger teacher model. SPEQ [50] introduces a novel self-training paradigm where the high-precision teacher and the low-bit-width student from the same full precision source model are trained collaboratively. In our training process, where different bit-width networks are enclosed, we design a self-knowledge transfer that utilizes a relatively higher precision network's output as soft labels for the cooperative training of the lower precision ones.",
            "score": 0.5775051053558382,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 14280,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 1052
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "222066738"
                },
                {
                    "start": 628,
                    "end": 632,
                    "matchedPaperCorpusId": "222066738"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.544921875
        },
        {
            "corpus_id": "277632659",
            "title": "Crowd counting at the edge using weighted knowledge distillation",
            "text": "Knowledge distillation (KD) in neural networks was proposed by Hinton et al. in 5 which is inspired by the work 48 demonstrating that \"knowledge learned by a large ensemble of models can be transferred to a single model\". Hinton et al. 5 introduced the \"temperature\" parameter T in the softmax function at the output layer used to distill the knowledge from the teacher model to the student model by training the student model using the same dataset as used for training the teacher model with a high value of T. Once the student model is trained, the value of T is set to T = 1 (standard softmax function) during the inference. The student model's accuracy can be highly improved by using two objective (loss) functions i.e., a cross-entropy (CE) function with the soft targets (produced by the teacher) and a CE with the actual ground truth labels. The authors suggest using a high value of T in the first case (same T in student and teacher) and T = 1 for the second case. However, the method was limited to supervised learning. To extend the KD concept to any type of deep learning model, Romero et al. 6 proposed feature-based KD to learn the intermediate representation (activation maps). They proposed to match the intermediate layer outputs of the teacher and student models to guide the student model. Several works extended this model to propose different methods to match feature activation maps such as using attention maps 49 , neuron selectivity 50 , and factors 51 . Similar to the response-based KD in which the knowledge is distilled from the output layer, in feature-based KD the knowledge is distilled from a single intermediate layer. \n\nIn contrast, 7 proposed relations-based KD in which a Gram matrix is calculated as an inner product between the feature maps of two layers. The model has been further extended by 52 to learn from more than one teacher. \n\nAuthors in 53 highlighted that the transfer of knowledge is impacted by the difference in the capacity of the teacher and student models. Knowledge distillation from multiple teacher networks is proposed in 54 .",
            "score": 0.5772447631407023,
            "section_title": "Related work",
            "char_start_offset": 9297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1654
                },
                {
                    "start": 1657,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1875
                },
                {
                    "start": 1878,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2089
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 81,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 112,
                    "end": 114,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 236,
                    "end": 237,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1477,
                    "end": 1479,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 1670,
                    "end": 1671,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1836,
                    "end": 1838,
                    "matchedPaperCorpusId": "13745134"
                },
                {
                    "start": 1889,
                    "end": 1891,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51220703125
        },
        {
            "corpus_id": "244130170",
            "title": "Synthetic Unknown Class Learning for Learning Unknowns",
            "text": "Following the conventional DNN classifier training scheme, the teacher network (T ) is first trained with the aim of minimizing the cross-entropy loss function. DNN classifiers are usually overgeneralized, producing very high confidence scores even for unknowns during testing. Thus, we introduce an additional network, the student network, and apply a knowledge distillation technique [7] based on the pretrained teacher network to relax the posterior probabilities of the student network for known classes. Knowledge distillation transfers softened versions of the target probabilities provided by the pretrained teacher network. The soft target probabilities are defined by introducing a temperature  \u03c4 in the softmax function as follows: \n\nand \n\nwhere l ik is the logit of sample x i for the k-th known class and l u ik is the logit for the k-th synthetic unknown class. The same temperature scaling technique is applied to the student network when it learns known training samples, yielding temperature-scaled posterior probabilities p \u03c4 ic and p u\u03c4 ic . The student network is then trained to minimize the following knowledge distillation loss function: \n\nwhere D is a known training sample set, |D| is the size of D, H is the cross-entropy function, and Q \u03c4 i and P \u03c4 i are temperature-scaled probability vectors for the teacher and student, respectively. \n\nUnlike original knowledge distillation [7], we do not consider the cross-entropy between the true labels and the nonscaled probability vector of the student because our purpose is to minimize overgeneralization while leaving room for learning unknowns. In this way, the student can have a chance to produce high probabilities for the synthetic unknown classes when evaluating an unfamiliar sample by reducing the risk of overfitting for known classes. This chance is further amplified via the synthetic unknown class learning approach described in the next subsection.",
            "score": 0.5770623491794616,
            "section_title": "Knowledge distillation",
            "char_start_offset": 12967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1362
                },
                {
                    "start": 1365,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1933
                }
            ],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1404,
                    "end": 1407,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58447265625
        },
        {
            "corpus_id": "218685026",
            "title": "Learning from a Lightweight Teacher for Efficient Knowledge Distillation",
            "text": "The general idea of knowledge distillation is to let a student network to mimic the soft target generated by a teacher, as shown in Figure 1(a). This is realized in the KD learning framework by adding another loss function to complement the standard cross-entropy loss measuring the gap between ground-truth and predictions. The added loss constrains the soft output of the student to be similar with the soft output of the teacher, which could be regarded as knowledge learned by the teacher. Specifically, the loss is defined as follows: \n\nwhere KL(\u2022) denotes the KL-divergence measure. |X t r | is assumed to be the number of instances in the training set X t r . p s \u03c4 and p t \u03c4 are the soft output of the student and soft target generated by the teacher, respectively. The soft outputs are computed based on the logits, gotten from last layers of neural networks before feeding to the So f tmax function. Taking p t \u03c4 as an example, it is computed by: \n\nwhere z t k is the logit from the teacher and K is the total number of classes. \u03c4 is a hyperparameter (referred as temperature in [10]) to control the scale of logits. For the teacher-free KD framework, the major difference is that p t \u03c4 (k) is not calculated based on a welltrained teacher, but is obtained through manually-crafted class distributions. In what follows, we mathematically specify the details of LW-KD. We use T (x; \u03b8 t ) to represent the teacher network with trainable parameters \u03b8 t and S(x; \u03b8 s ) to denote the student network with learnable parameters \u03b8 s .",
            "score": 0.5766448569283165,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 12319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 539
                },
                {
                    "start": 542,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 956
                },
                {
                    "start": 959,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1536
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4873046875
        },
        {
            "corpus_id": "268248015",
            "title": "PromptKD: Unsupervised Prompt Distillation for Vision-Language Models",
            "text": "Originally proposed by Hinton et al. [15], knowledge distillation aims to transfer the knowledge of a pretrained heavy teacher model to a lightweight student model. After the distillation, the student can master the expertise of the teacher and be used for final deployment. Specifically, the Kullback-Leibler (KL) divergence loss is utilized to match the output distribution of two models, which can be formulated as follows: \n\nL kd (q t , q s , \u03c4 ) = \u03c4 2 KL(\u03c3(q t /\u03c4 ), \u03c3(q s /\u03c4 )). \n\n( \n\nwhere q t and q s denote the logits predicted by the teacher and student. \u03c3(\u2022) is the softmax function and \u03c4 is the temperature [15,31] which controls the softness of distribution.",
            "score": 0.5765403246863345,
            "section_title": "Background",
            "char_start_offset": 12288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 484
                },
                {
                    "start": 487,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 671
                }
            ],
            "ref_mentions": [
                {
                    "start": 623,
                    "end": 626,
                    "matchedPaperCorpusId": "254069919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.366943359375
        },
        {
            "corpus_id": "270737904",
            "title": "On Reducing Activity with Distillation and Regularization for Energy Efficient Spiking Neural Networks",
            "text": "In this paper, we only focus on Response-based KD but we explore various techniques listed and explained below :\n\n-Mean Squared Error (MSE) Knowledge Distillation: this approach involves employing MSE in the KD loss function.The objective is to minimize the squared differences between the logits generated by the teacher and student networks.The MSE loss function is defined as follows:\n\nHere, logits t and logits s represent the predicted logits (presoftmax activations) of the teacher and student models, respectively, for the x i input.\n\n-Soft Targets (ST) Knowledge Distillation: the teacher's outputs are used as new labels (they are named Soft Targets or soft labels).In this method, a temperature \u03c4 is applied to the Softmax function of both the student and the teacher.The temperature parameter controls the smoothness of the probability distribution and therefore impacts the degree of knowledge transfered from the teacher to the student.\n\nWhere p(x i , \u03c4 ) is the probability distribution of the x i input and \u03c4 is the temperature parameter.Accordingly, the loss term for ST KD is defined as follows:\n\nWhere p t and p s represent the probability distributions of the teacher and student models, respectively, for the x i input while D KL denotes the Kullback-Leibler divergence.\n\n-Soft Targets Knowledge Distillation with Heterogeneous Temperature (ST-HET KD): this technique proposes varying the temperature between the teacher and the student.The idea is to leverage the potential benefits of temperature heterogeneity in optimizing neural activity while preserving the performance.For ST-HET KD, the distillation loss is defined as:\n\nIn addition to Eq. ( 4), \u03c4 t and \u03c4 s represent the temperature of the teacher and student models, respectively.The distillation loss, whether MSE or ST, is added to the total loss function during training.The total loss shown in Eq. 1 becomes a weighted combination of the standard loss and the distillation loss (Eq.L KD can be either Eq. 2, or Eq. 4 or Eq. 5).",
            "score": 0.5765402431906013,
            "section_title": "III. METHODOLOGY A. Response-based Knowledge Distillation",
            "char_start_offset": 7946,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 114,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 387
                },
                {
                    "start": 389,
                    "end": 540
                },
                {
                    "start": 542,
                    "end": 675
                },
                {
                    "start": 675,
                    "end": 778
                },
                {
                    "start": 778,
                    "end": 949
                },
                {
                    "start": 951,
                    "end": 1053
                },
                {
                    "start": 1053,
                    "end": 1112
                },
                {
                    "start": 1114,
                    "end": 1290
                },
                {
                    "start": 1292,
                    "end": 1457
                },
                {
                    "start": 1457,
                    "end": 1596
                },
                {
                    "start": 1596,
                    "end": 1647
                },
                {
                    "start": 1649,
                    "end": 1760
                },
                {
                    "start": 1760,
                    "end": 1854
                },
                {
                    "start": 1854,
                    "end": 1966
                },
                {
                    "start": 1966,
                    "end": 2011
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5185546875
        },
        {
            "corpus_id": "268060109",
            "title": "Advanced hybrid LSTM-transformer architecture for real-time multi-task prediction in engineering systems",
            "text": "The softmax loss, also known as the cross-entropy loss between the teacher's softened outputs and the student's predictions, plays a pivotal role in knowledge distillation. We handle this loss calculation as follows: \n\nThe softmax function with temperature scaling is applied to both the teacher's and student's logits, generating softened probability distributions. \n\nThe cross-entropy loss is then computed between these two distributions. This loss quantifies the difference between the teacher's guidance and the student's current understanding. \n\n(5) \n\nThis cross-entropy loss is combined with the traditional loss function to form the final loss function for training the student model. This combined loss function guides the student to not only mimic the teacher's output distribution but also to align correctly with the ground truth. \n\nThe choice of temperature T in the softmax function is crucial. A higher T produces softer probability distributions, encouraging the student model to learn the nuanced relationships captured by the teacher. However, too high a value of T can lead to an overly smoothed distribution, which might be less informative. Therefore, we empirically determine the optimal value of T through a series of experiments, aiming to find the right balance for effective knowledge transfer. \n\nEquations: Softmax function with temperature scaling: \n\nwhere z i is the logit for class i and T is the temperature.",
            "score": 0.5764823992258153,
            "section_title": "Softmax loss calculation:",
            "char_start_offset": 23189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 216
                },
                {
                    "start": 219,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1376
                },
                {
                    "start": 1379,
                    "end": 1439
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6484375
        },
        {
            "corpus_id": "239616535",
            "title": "MSD: Saliency-aware Knowledge Distillation for Multimodal Understanding",
            "text": "In knowledge distillation (Hinton et al., 2015), a student is trained to minimize a weighted sum of two different losses: (a) cross entropy with hard labels (one-hot encodings on correct labels) using a standard softmax function, (b) cross entropy with soft labels (probability distribution of labels) produced by a teacher with a temperature higher than 1 in the softmax of both models. The temperature controls the softness of the probability distributions. Thus, the loss for the student is defined as:\n\nwhere L CE is a standard cross-entropy loss on hard labels, L KD is a distillation loss, which is a crossentropy loss on soft labels, and \u03bb \u2208 [0, 1] controls the balance between hard and soft targets. To be specific, knowledge distillation (Hinton et al., 2015) minimizes the Kullback-Leibler divergence between soft targets from a teacher and probabilities from a student. The soft targets (or soft labels) are defined as softmax on outputs of f T with temperature \u03c4 . The distillation loss is defined as follows:\n\n, \u03c3 is a softmax function. The temperature parameter \u03c4 controls the entropy of the output distribution (higher temperature \u03c4 means higher entropy in the soft labels). Following Hinton et al. (2015), we scale the loss by \u03c4 2 in order to keep gradient magnitudes approximately constant when changing the temperature. We omit \u03c4 for brevity.\n\nLimitations. This KD can be applied to multimodal setups and student models in this distillation are directly trained to mimic a teacher's outputs. As a result, the student and teacher models may significantly differ in outputs with a single-modality input, i.e., modality-specific outputs, which may lead to inefficient distillation (Fig. 1). To better mimic the teacher's behaviors, we introduce a multimodal KD approach, modality-specific distillation, in the next section.",
            "score": 0.5759801240369065,
            "section_title": "Conventional Knowledge Distillation",
            "char_start_offset": 4824,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42138671875
        },
        {
            "corpus_id": "267740249",
            "title": "Rethinking Self-Distillation: Label Averaging and Enhanced Soft Label Refinement with Partial Labels",
            "text": "Knowledge distillation, initially presented in Hinton et al. (2015), is a technique in machine learning where a smaller, more efficient model (the student) is trained to replicate the behavior and predictions of a larger, complex model (the teacher). The student model achieves this by being trained using the teacher model's output probability distribution, often in the form of soft labels. This method is typically described as a teacher-student dynamic, where the teacher model delivers \"dark knowledge\" to the student via these soft labels Chen et al. (2021); Miles et al. (2021); Zhao et al. (2022); Wang et al. (2023b). Knowledge distillation (KD) has been successfully applied in diverse areas such as vision, speech and natural language processing Chebotar & Waters (2016); Cui et al. (2017); Asif et al. (2019); Liu et al. (2019). \n\nThe specific scenario in knowledge distillation where both the teacher and student networks share the same architecture is known as self-distillation (SD). Surprisingly, SD has also been observed to enhance performance Furlanello et al. (2018); Zhang et al. (2019). This phenomenon cannot be easily explained by the conventional theory that attributes KD's improvements to the student mimicking a more complex teacher with higher capacity. Recent studies have aimed to theoretically analyze the effects of self-distillation in analytically simpler settings. For instance, Mobahi et al. (2020); Dong et al. (2019) explored the advantages of SD in classical regression problems using mean squared error. Phuong & Lampert (2019) considered binary classification with cross-entropy (CE) loss in linear networks, examining how distillation-trained classifiers improve generalization and convergence rates. Das & Sanghavi (2023) investigated SD in the presence of label noise in binary classification with CE loss, identifying the corruption regime where the student outperforms the teacher. However, fully understanding and quantifying the benefits of SD, especially in multi-class classification with CE loss-a widely used classification framework-remains a significant, unresolved challenge.",
            "score": 0.5758973078891344,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 545,
                    "end": 563,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 783,
                    "end": 800,
                    "matchedPaperCorpusId": "1174931"
                },
                {
                    "start": 1062,
                    "end": 1086,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1415,
                    "end": 1435,
                    "matchedPaperCorpusId": "211096976"
                },
                {
                    "start": 1744,
                    "end": 1765,
                    "matchedPaperCorpusId": "256416199"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5048828125
        },
        {
            "corpus_id": "255338121",
            "title": "MAC-ResNet: Knowledge Distillation Based Lightweight Multiscale-Attention-Crop-ResNet for Eyelid Tumors Detection and Classification",
            "text": "Therefore, in the knowledge distillation process, the teacher network is usually a network with a complex structure, slow inference process, high consumption of computer resources, and good model performance, while the student network is usually a network with a simpler structure, fewer parameters, and poorer model performance. The process of using knowledge distillation is as follows: first, we train the complex and well-performing teacher network (MAC-ResNet), then guide the training of the student network (ResNet50) using the trained teacher network, and finally use the trained student network to classify the dataset. The teacher network guides the training of the student network by providing the student network with soft labels, or the probabilities of each class predicted by the teacher network, instead of hard labels (as shown in Figure 3), which is the one-hot encoded labels of each class. For soft labeling, the predicted output of the network is divided by the temperature coefficient T and then the softmax operation is performed, which makes the result values between 0 and 1 with a more moderate distribution of values, while hard labeling means that the predicted output of the network is directly softmaxed without dividing by T [43]. This helps the student network learn from the rich information provided by the teacher network. The softmax process can be denote as: \n\nThe loss of the MAC-ResNet network consists of two parts, which are the loss between the predicted value and the label of the first original input picture and the loss between the predicted value and the label of the network model after the attention-guided cropping to generate AttentionPicture into the network, and the weighted sum between them is the final loss. The proposed loss function of the whole training process after using MAC-ResNet as the teacher network and ResNet50 as the student network is shown in (4) and (5). \n\nwhere S SP refers to the output of the hard label by the student network, S SP refers to the output of the soft label by the student network, T SL refers to the soft labels generated by the teacher network for the original picture prediction, and T AHP refers to the hard labels predicted by the teacher network based on the AttentionPicture (the labels are softened only for the results of the original picture prediction).",
            "score": 0.575511083287185,
            "section_title": "Network Optimization Based On Knowledge Distillation",
            "char_start_offset": 19301,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1928
                },
                {
                    "start": 1931,
                    "end": 2355
                }
            ],
            "ref_mentions": [
                {
                    "start": 1256,
                    "end": 1260,
                    "matchedPaperCorpusId": "18865663"
                },
                {
                    "start": 1924,
                    "end": 1927,
                    "matchedPaperCorpusId": "102951"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50927734375
        },
        {
            "corpus_id": "235899214",
            "title": "Confidence Conditioned Knowledge Distillation",
            "text": "In conventional knowledge distillation, the soft targets computed from the teacher model at a temperature \u03c4 are used. Given a teacher network T parametrized by \u03b8 T and a student network S parametrized by \u03b8 S , distillation minimizes the following objective over all samples (x, y) in the transfer set D: \n\nwhere, L KD is the distillation loss which is minimized at a temperature \u03c4 . It can be the cross entropy loss for classification or the L 2 loss for regression. L CE is the cross entropy loss which is minimized at a temperature of 1. \u0177S is the prediction of the student network on the sample x at temperature 1, and \u03bb is a hyperparameter to balance the two losses. It is important to note that the conventional KD process assigns a fixed relative importance \u03bb to the loss incurred on the ground truths y.",
            "score": 0.5753303891665087,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 16561,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 303
                },
                {
                    "start": 306,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 810
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22265625
        },
        {
            "corpus_id": "259749217",
            "title": "Model Protection Scheme Against Distillation Attack in Internet of Vehicles",
            "text": "Knowledge distillation is a widely used method for model compression and optimisation in deep learning. It is based on the concept of a \"teacher-student model\" for training and is highly regarded for its simplicity and effectiveness. Knowledge Distillation facilitates the training of student models by extracting \"knowledge\" from one or more pretrained teacher models using the soft-label probabilistic output of the teacher models. This soft-label output is a mapping from input vectors to output vectors that captures specific knowledge from instantiated objects, with incorrect classification predictions providing insight into how the teacher model generalizes. The student model can improve its performance by mimicking the probabilistic output of the teacher model, and can incorporate the knowledge that the teacher model has already acquired. The process of knowledge distillation is illustrated in Fig. 1. Teacher networks can transfer their model capabilities to student networks through knowledge distillation. \n\nAs shown in Eq. ( 1), neural networks typically generate class probabilities by using a \"softmax\" output layer that compares the output i z of each class with other logits, converting the logit z i calculated for each class into a probability i q in a standardized way. In addition, where T represents temperature in knowledge distillation, using a larger value than 1 for it produces a softer class probability distribution that allows better transfer of knowledge to the model to be distilled. \n\nGiven a pre-trained teacher model ()",
            "score": 0.5752082580660569,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1559
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2744140625
        },
        {
            "corpus_id": "248377257",
            "title": "Proto2Proto: Can you recognize the car, the way I do?",
            "text": "A lot of works focus on the design and development of small models applicable in a resource-constraint deployment. Knowledge Distillation is one such model compression method to improve the performance of small student models using a large teacher model. It distils the dark knowledge by mimicking the logits [3], or soft labels [14] from teacher to student. Survey papers on knowledge distillation covering distillation strategies, student-teacher architectures and the recent findings can be viewed to get the background on this setup [10,50]. Even though a plethora of literature for knowledge distillation is present, very few works have been proposed to focus on the interpretability aspect of knowledge distillation. \n\nInterpretability in knowledge distillation is commonly achieved by transferring the teacher's dark knowledge into interpretable tree-based models in one form or the other. Post-hoc interpretations were obtained for the dark knowledge by varying the inputs to trees such as matching logits [30], soft targets or using different types of trees architecture viz soft decision trees [17], vanilla decision trees , adaptive neural trees [45], Neural backed decision trees [49], Gradient boosting trees [5] and generalized additive models [31]. Instead of using representations from the deep neural networks, Tree-Network-Tree architecture [27] attempts to learn a tree-based model in the input space to extract the decision path and form an embedding representation. This is further used to learn a neural network whose soft labels are used to distill knowledge into another treebased model. This three-step process helps in making the model interpretable due to the extractable decision paths from the distilled tree. Most of the attempts above focus on relieving the tension between accuracy and interpretability. Due to the constraints on input/weight space, the knowledge could not be completely distilled into the student, or the distilled models could not be used to their full capability. \n\nAnother line of thought is to use visualization methods for interpretation. DarkSight [52] uses a simple interpretable classifier such as Naive Bayes' as a student model to mimic the dark knowledge. Applying low dimensional representation to data and jointly optimizing on model compression objective, the visualizations obtained on the network's predictions provides interpretability.",
            "score": 0.5749524387791457,
            "section_title": "Interpretable Knowledge Distillation",
            "char_start_offset": 8710,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 2015
                },
                {
                    "start": 2018,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2216
                },
                {
                    "start": 2217,
                    "end": 2403
                }
            ],
            "ref_mentions": [
                {
                    "start": 537,
                    "end": 541,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 541,
                    "end": 544,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 1014,
                    "end": 1018,
                    "matchedPaperCorpusId": "57189239"
                },
                {
                    "start": 1104,
                    "end": 1108,
                    "matchedPaperCorpusId": "9691881"
                },
                {
                    "start": 1258,
                    "end": 1262,
                    "matchedPaperCorpusId": "7715182"
                },
                {
                    "start": 1359,
                    "end": 1363,
                    "matchedPaperCorpusId": "226348474"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.420166015625
        },
        {
            "corpus_id": "271744835",
            "title": "Real-Time Event Recognition of Long-Distance Distributed Vibration Sensing With Knowledge Distillation and Hardware Acceleration",
            "text": "It is generally believed that large models, that is, models with more complex and efficient structural design and huge number of parameters, tend to be able to extract higher-level features, so that it can achieve better results in various tasks. In other words, large models often have better generalizability, and can better handle signals with various noise. On the other hand, the model inference process can be considered as finishing the specified task based on the 'knowledge' contained in model structure and its parameters. The idea of knowledge distillation is to extract the knowledge from large models (teacher models), and transfer the knowledge to small models (student model), so that the performance and generalizability of the later one can be improved. In the knowledge distillation method based on logits [19], the output prediction distribution of the model is considered to contain valuable knowledge. This distribution represents the probabilities of classifying the input sample into each class. The distribution can be obtained through introducing a temperature parameter T to the softmax function at output layer. The function can be expressed as: \n\nwhere x = [x 1 , x 2 , . . . , x n ] is the input vector, which is the output of the network in this case. The knowledge is then transferred by guiding the student model to imitate the behavior of the teacher model. The process is to minimizing a loss function that measures the difference between the outputs of the teacher network and the student network, and also the difference between the outputs of the student network and the correct results, as shown in Fig. 2. The loss function usually consists of a standard classification loss, like cross-entropy, and a distillation loss that measures the difference between the softened probabilities generated by the teacher and student networks. Formally, the loss function L can be expressed as: where CE denotes the cross-entropy loss, KL represents the Kullback-Leibler divergence, y true is the true label, \u03b1 represents wegiht between two kind of losses, y t and y s denote the outputs of the teacher and student networks respectively. Through this optimization process, knowledge distillation enables the creation of compact yet powerful neural networks capable of retaining the performance of their larger counterparts.",
            "score": 0.5748361715722792,
            "section_title": "II. METHOD A. Knowledge Distillation based on logits",
            "char_start_offset": 7123,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2163
                },
                {
                    "start": 2164,
                    "end": 2349
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54638671875
        },
        {
            "corpus_id": "237478571",
            "title": "BERTtoCNN: Similarity-preserving enhanced knowledge distillation for stance detection",
            "text": "Given the teacher model and the student model, we will introduce the process of knowledge distillation in this section. \n\nIn order to learn knowledge from teacher model, now we only need to match the softmax distribution of the student model with the teacher under the given input, instead of matching the softmax distribution of the student model with the real label distribution. The teacher model provides the probability logits and estimated labels for the samples, and the student network learns from the teacher's outputs. \n\nClassic distillation loss. Usually, knowledge distillation is carried out at the output layer. The student model tries to imitate the behavior of teacher model given any data point. Inspired by Ba and Caruana [26], we use logits of teacher model as labels to train the student model, which contain more information than hard label. \n\nGiven the logits z of the output layer in a teacher model, z i is the logits for the i-th class, the discrete probability output p i corresponding to an input can be estimated by a softmax function: \n\nThe essence of adopting feature matching strategy in softmax layer is to use the output of softmax as supervision. In order to make the score vector softer, distillation temperature T is added in softmax layer to control the importance of each soft label as a divisor: By increasing T, the original probability distribution can be smoothed, and the similarity between data can be revealed better. While the knowledge distillation is not executed, the input to student model becomes the one-hot vector from the ground-truth transformation, which is called \"hard target\". The soft label and ground truth label of teacher model are important to improve the performance of student model, and are used to extract distill loss and student loss, respectively. \n\nTherefore, the classic distillation loss can be defined as follows: \n\nwhere H(.) is the cross entropy loss function, \u03c3(.) is the softmax function, T is the temperature parameter, y is the one-hot vector indicating the ground truth class, z S and z T are the output logits of the student and teacher networks, respectively. In addition, \u03b1 is the weighted hyperparameter to balance the student loss and distill loss. Similarity-preserving loss.",
            "score": 0.5745147833670498,
            "section_title": "Knowledge distillation",
            "char_start_offset": 9868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 122,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 1888
                },
                {
                    "start": 1891,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2235
                },
                {
                    "start": 2236,
                    "end": 2263
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54296875
        },
        {
            "corpus_id": "265502078",
            "title": "Decentralized and Incentivized Federated Learning: A Blockchain-Enabled Framework Utilising Compressed Soft-Labels and Peer Consistency",
            "text": "1) Knowledge Distillation: Knowledge Distillation (KD), depicted in Fig. 1, is a technique in deep learning where a smaller NN model (often called the \"student\") is trained to mimic the behavior of a larger, pre-trained model (referred to as the \"teacher\") [27]. This is accomplished not by transferring the model parameters directly, but rather by aligning the output distributions of both models. Traditional training methods involve training a model directly on ground-truth labels, using a cross-entropy loss that measures the discrepancy between the model's predictions and these true labels. In contrast, KD employs a divergence-based loss, such as the Kullback-Leibler (KL) divergence, to measure the difference between the student's predicted probabilities and those of the teacher model. This divergence provides insights into how closely the student is able to mimic the behavior of its teacher. A distinct feature of KD is the use of \"softened\" labels. In traditional classification tasks, hard labels are used, which unequivocally classify a data point into one category. However, the teacher model in KD provides \"soft\" labels in the form of probabilities, indicating the confidence levels across various categories. These probabilities can be further softened using a temperature parameter T to yield a smoother distribution, capturing the nuances of decision boundaries and offering richer guidance to the student model. This process allows the student to inherit not just the overt knowledge from the ground-truth labels but also the implicit, or \"dark\", knowledge embedded in the teacher model's predictions. Since only soft-labels are necessary to perform backpropagation, models with varying architectures can learn from the teacher. The appeal of KD lies in its ability to produce compact models with performance that closely mirrors that of much larger networks. These compact models are advantageous for deployment in resource-constrained environments, such as mobile devices or edge devices, without sacrificing much in terms of accuracy. With the above foundation in KD, we can delve deeper into its application in the FL setup, specifically focusing on Federated Distillation.",
            "score": 0.5735960177804764,
            "section_title": "B. Knowledge Distillation and Federated Settings",
            "char_start_offset": 4984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2201
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7255859375
        },
        {
            "corpus_id": "264406238",
            "title": "Boosting Generalization with Adaptive Style Techniques for Fingerprint Liveness Detection",
            "text": "Knowledge Distillation. We train SE-ResNeXt101 models to transfer the knowledge to SE-ResNeXt50 models for further improvement. We use temperature=5 to generate the soft prediction. The student network is trained by cross entropy loss and Kullback-Leibler (KL) divergence loss between soft predictions of teacher and student models [7]. Consequently, our proposed model demonstrates remarkable generalization capabilities while maintaining low inference latency.",
            "score": 0.5735651212356612,
            "section_title": "Efficient Representation Extraction",
            "char_start_offset": 6295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 462
                }
            ],
            "ref_mentions": [
                {
                    "start": 332,
                    "end": 335,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38525390625
        },
        {
            "corpus_id": "246285330",
            "title": "One Student Knows All Experts Know: From Sparse to Dense",
            "text": "To mine the knowledge from noise, we adopt soft knowledge distillation [9] to fine-tune the dense student. Soft distillation minimizes the Kullback-Leibler divergence between the output of the teacher and the student. The corresponding distillation loss can be written as: \n\nwhere \u03c9 is the softmax function, L KL is Kullback-Leibler divergence loss, z s and z t are the logits of student and teacher, respectively, and T is the softmax temperature. We also considered hard-label distillation [24] and compared its performance with soft distillation. Please see Appendix C for details.",
            "score": 0.5733541623144385,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 14349,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 272
                },
                {
                    "start": 275,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 584
                }
            ],
            "ref_mentions": [
                {
                    "start": 492,
                    "end": 496,
                    "matchedPaperCorpusId": "229363322"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21337890625
        },
        {
            "corpus_id": "246904396",
            "title": "General Cyclical Training of Neural Networks",
            "text": "Knowledge distillation was described by Hinton, et al. [17] using two networks for model compression: the teacher and student networks. Usually, the student network is smaller than the teacher network and the goal is network compression, in which the smaller student network achieves similar performance as the teacher. \n\nIn student-teacher model compression, it is desirable that the student networks should learn as much as possible from the teacher [38], with the goal of maximizing the mutual information between the teacher and student. For this reason, the student loss function often contains not simply a cross-entropy softmax for the labels, but also loss terms for the student to match the teacher's features (the features from the teacher's hidden layers are also called \"dark information\"). This includes deep supervision, in which features at several layers are matched and not just the final hidden layer's features [28,43]. \n\nAlong these lines, it is helpful when training the student to diversify the training characteristics. This can include using cyclical data augmentation so the student is exposed to strong augmentation during the middle epochs (in addition to weak augmentation for the early and final epochs). It also can include using cyclical softmax temperature, where the higher temperatures expose the student to what the teacher considers as other close classes in addition to training the student to classify the target class. \n\nWe note that there exists work on teacher-student models in which the teacher chooses what the student should learn [18,26]. The teacher, in these cases, is guided by curriculum learning in the choice of data or task for the student to learn; these techniques also can be modified to be cyclical.",
            "score": 0.5733502527321946,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 26985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 319
                },
                {
                    "start": 322,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1457
                },
                {
                    "start": 1460,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1756
                }
            ],
            "ref_mentions": [
                {
                    "start": 452,
                    "end": 456,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 934,
                    "end": 937,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1576,
                    "end": 1580,
                    "matchedPaperCorpusId": "51876228"
                },
                {
                    "start": 1580,
                    "end": 1583,
                    "matchedPaperCorpusId": "204734196"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63671875
        },
        {
            "corpus_id": "215416219",
            "title": "LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression",
            "text": "Knowledge distillation (KD) has been widely used to transfer knowledge from a large teacher model to a smaller student model. In other words, the student model mimics the behavior of the teacher model by minimize the knowledge distillation loss functions. Various types of knowledge distillation can be employed at different sub-layers. Generally, all types of knowledge distillation can be modeled as minimizing the following loss function: \n\nWhere x indicates a sample input and X is the training dataset. f (s) (x) and f (t) (x) represent intermediate outputs or weight matrices for the student model and teacher model correspondingly. L(\u2022) represents for a loss function which can be carefully defined for different types of knowledge distillation. We follow the recent technique proposed by TinyBERT (Jiao et al., 2019), which applies knowledge distillation constraints upon embedding, self-attention, hidden representation and prediction levels. Concretely, there are four types of knowledge distillation constraints as follows: \n\n1. Embedding-layer distillation is performed upon the embedding layer. f (x) \u2208 R n\u00d7d represents for the word embedding output for input x, where n is the input word length and d is the dimension of word embedding. Mean Squared Error (MSE) is adopted as the loss function L(\u2022). \n\n2. Attention-layer distillation is performed upon the self-attention sub-layer. f (x) = {a ij } \u2208 R n\u00d7n represents the attention output for each self-attention sub-layer, and L(\u2022) denotes MSE loss function. \n\n3. Hidden-layer Distillation is performed at each fully-connected sub-layer in the Transformer architectures. f (x) denotes the output representation of the corresponding sub-layer, and L(\u2022) also adopts MSE loss function. \n\n4. Prediction-layer distillation makes the student model to learns the predictions from a teacher model directly. \n\nIt is identical to the vanilla form of knowledge distillation (Hinton et al., 2015). It takes the soft cross-entropy loss function, which is formulated as:",
            "score": 0.5726488935773392,
            "section_title": "Knowledge distillation",
            "char_start_offset": 15739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1313
                },
                {
                    "start": 1316,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1522
                },
                {
                    "start": 1525,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1746
                },
                {
                    "start": 1749,
                    "end": 1862
                },
                {
                    "start": 1865,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2020
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53173828125
        }
    ],
    "quotes": {
        "cost": 0.322302,
        "quotes": [
            {
                "idx": 0,
                "key": "[159041346 | Nayak et al. | 2019 | Citations: 245]",
                "snippets": "The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability. For performing the knowledge distillation, one can use the training data from the target distribution or an arbitrary data. Typically, the data used to perform the distillation is called 'Transfer set'. In order to maximize the information provided per sample, we can make the soft targets to have a high entropy (non-peaky). This is generally achieved by using a high temperature at the softmax layer (Hinton et al., 2015). Also, because of non-peaky soft-labels, the training gradients computed on the loss will have less variance and enable to use higher learning rates leading to quick convergence.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 626,
                        "end": 1731,
                        "sentence_offsets": [
                            {
                                "start": 583,
                                "end": 819
                            },
                            {
                                "start": 820,
                                "end": 1021
                            },
                            {
                                "start": 1022,
                                "end": 1128
                            },
                            {
                                "start": 1129,
                                "end": 1252
                            },
                            {
                                "start": 1253,
                                "end": 1331
                            },
                            {
                                "start": 1332,
                                "end": 1454
                            },
                            {
                                "start": 1455,
                                "end": 1553
                            },
                            {
                                "start": 1554,
                                "end": 1731
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability. For performing the knowledge distillation, one can use the training data from the target distribution or an arbitrary data. Typically, the data used to perform the distillation is called 'Transfer set'. In order to maximize the information provided per sample, we can make the soft targets to have a high entropy (non-peaky). This is generally achieved by using a high temperature at the softmax layer (Hinton et al., 2015). Also, because of non-peaky soft-labels, the training gradients computed on the loss will have less variance and enable to use higher learning rates leading to quick convergence."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[198179767 | Zhao et al. | 2019 | Citations: 59]",
                "snippets": "Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process.\n\nLet us respectively denote x and y as the input of the DNNs and one-hot labels of our architecture. We let P T be the teacher network's softmax output as P T = sof tmax(a T ). Specifically, P T is obtained by applying softmax function on the un-normalized log probability values a T . Similarly, the same image fed to the student network to get the predictions P S = sof tmax(a S ). In the intermediate layers of the DNN, we denote the activation tensor A \u2208 R C\u00d7X\u00d7W with its corresponding layer. The pairs of teacher and student attention maps are denoted as F (A j T ) and F (A j S ) in vectorized form respectively [21]. And the standard cross entropy is denoted as H. Hinton et al. [19] extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 the student network will be trained to optimize the following loss function:",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
                    "[829159 | Zagoruyko et al. | 2016 | Citations: 2586]": "Attention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artificial neural networks to a variety of tasks from fields such as computer vision and NLP. In this work we show that, by properly defining attention for convolutional neural networks, we can actually use this type of information in order to significantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network. To that end, we propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures. Code and models for our experiments are available at this https URL"
                },
                "metadata": [
                    {
                        "section_title": "B. Formulation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1782,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 59
                            },
                            {
                                "start": 60,
                                "end": 214
                            },
                            {
                                "start": 215,
                                "end": 308
                            },
                            {
                                "start": 309,
                                "end": 410
                            },
                            {
                                "start": 411,
                                "end": 516
                            },
                            {
                                "start": 517,
                                "end": 637
                            },
                            {
                                "start": 640,
                                "end": 739
                            },
                            {
                                "start": 740,
                                "end": 815
                            },
                            {
                                "start": 816,
                                "end": 924
                            },
                            {
                                "start": 925,
                                "end": 1022
                            },
                            {
                                "start": 1023,
                                "end": 1135
                            },
                            {
                                "start": 1136,
                                "end": 1262
                            },
                            {
                                "start": 1263,
                                "end": 1454
                            },
                            {
                                "start": 1455,
                                "end": 1529
                            },
                            {
                                "start": 1530,
                                "end": 1636
                            },
                            {
                                "start": 1637,
                                "end": 1783
                            }
                        ],
                        "ref_mentions": [
                            "829159",
                            "7200347"
                        ],
                        "quote": "Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process.\n\nLet us respectively denote x and y as the input of the DNNs and one-hot labels of our architecture. We let P T be the teacher network's softmax output as P T = sof tmax(a T ). Specifically, P T is obtained by applying softmax function on the un-normalized log probability values a T . Similarly, the same image fed to the student network to get the predictions P S = sof tmax(a S ). In the intermediate layers of the DNN, we denote the activation tensor A \u2208 R C\u00d7X\u00d7W with its corresponding layer. The pairs of teacher and student attention maps are denoted as F (A j T ) and F (A j S ) in vectorized form respectively [21]. And the standard cross entropy is denoted as H. Hinton et al. [19] extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 the student network will be trained to optimize the following loss function:"
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[203593636 | Xie et al. | 2019 | Citations: 12]",
                "snippets": "To perform dark knowledge, we train the student network to minimize the following loss function: \n\nwhere \u03c4 is a temperature parameter to soften the distribution of predictions. Different from fine-tuning just by class labels, dark knowledge is to learn the student network from the soft outputs of teacher network and ground-truth labels.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Online Distillation",
                        "pdf_hash": "",
                        "start": 1762,
                        "end": 2100,
                        "sentence_offsets": [
                            {
                                "start": 1762,
                                "end": 1858
                            },
                            {
                                "start": 1861,
                                "end": 1938
                            },
                            {
                                "start": 1939,
                                "end": 2100
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To perform dark knowledge, we train the student network to minimize the following loss function: \n\nwhere \u03c4 is a temperature parameter to soften the distribution of predictions. Different from fine-tuning just by class labels, dark knowledge is to learn the student network from the soft outputs of teacher network and ground-truth labels."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[211096976 | Mobahi et al. | 2020 | Citations: 235]",
                "snippets": "The original formulation achieves this by training the student model using the output probability distribution of the teacher model in addition to original labels. The student model benefits from this \"dark knowledge\" (extra information in soft predictions) and often performs better than if it was trained on the actual labels.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 236,
                        "end": 564,
                        "sentence_offsets": [
                            {
                                "start": 236,
                                "end": 399
                            },
                            {
                                "start": 400,
                                "end": 564
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The original formulation achieves this by training the student model using the output probability distribution of the teacher model in addition to original labels. The student model benefits from this \"dark knowledge\" (extra information in soft predictions) and often performs better than if it was trained on the actual labels."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[212855595 | Zhang et al. | 2020 | Citations: 35]",
                "snippets": "This subsection illustrates the objective functions of teacher and student networks, we introduce two additional loss functions to help our model efficiently transfer dark knowledge (i.e., the global type constraints and the relevance among relations) from teacher to student. \n\nThe Teacher Network In vanilla knowledge distillation, the teacher network is trained to fit one-hot labels. However, the ultimate goal of teacher is to provide better guidances for student, rather than achieve high accuracy simply. Inspired by (Yang et al., 2018), we introduce a top score difference (TSD) loss to make the teacher's distribution softer. More concretely, we first pick up K classes with the highest confidence scores from the teacher's output, and then compute the gap between the confidence scores of the primary class and other K-1 classes...Based on the global statistics, K is set to 3 empirically. We add the penalty term to standard ground-truth loss L T GT when training the teacher, facilitating it to distribute confidence to a few secondary relations...The Student Network Typically, knowledge distillation transfers dark knowledge from the final output of teacher. (Chen et al., 2017) demonstrate that using the intermediate representation of teacher as hint can stabilize the training process and improve the final performance of student. Here, we utilize the Kullback-Leibler divergence to measure the differences of corresponding branches between the teacher and student networks as hint learning loss...Intuitively, it encourages the results of MAA to be similar with the scaled global logits of the teacher network. The loss of knowledge distillation is calculated as the sum of L HT and L KD with a weight factor \u03bb ht . As a result, the updated loss of student network is defined as follows:",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[54986302 | Yang et al. | 2018 | Citations: 136]": "We focus on the problem of training a deep neural network in generations. The flowchart is that, in order to optimize the target network (student), another network (teacher) with the same architecture is first trained, and used to provide part of supervision signals in the next stage. While this strategy leads to a higher accuracy, many aspects (e.g., why teacher-student optimization helps) still need further explorations.This paper studies this problem from a perspective of controlling the strictness in training the teacher network. Existing approaches mostly used a hard distribution (e.g., one-hot vectors) in training, leading to a strict teacher which itself has a high accuracy, but we argue that the teacher needs to be more tolerant, although this often implies a lower accuracy. The implementation is very easy, with merely an extra loss term added to the teacher network, facilitating a few secondary classes to emerge and complement to the primary class. Consequently, the teacher provides a milder supervision signal (a less peaked distribution), and makes it possible for the student to learn from inter-class similarity and potentially lower the risk of over-fitting. Experiments are performed on standard image classification tasks (CIFAR100 and ILSVRC2012). Although the teacher network behaves less powerful, the students show a persistent ability growth and eventually achieve higher classification accuracies than other competitors. Model ensemble and transfer feature extraction also verify the effectiveness of our approach."
                },
                "metadata": [
                    {
                        "section_title": "Objective Functions",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 837,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 276
                            },
                            {
                                "start": 279,
                                "end": 387
                            },
                            {
                                "start": 388,
                                "end": 511
                            },
                            {
                                "start": 512,
                                "end": 633
                            },
                            {
                                "start": 634,
                                "end": 838
                            }
                        ],
                        "ref_mentions": [
                            "54986302"
                        ],
                        "quote": "This subsection illustrates the objective functions of teacher and student networks, we introduce two additional loss functions to help our model efficiently transfer dark knowledge (i.e., the global type constraints and the relevance among relations) from teacher to student. \n\nThe Teacher Network In vanilla knowledge distillation, the teacher network is trained to fit one-hot labels. However, the ultimate goal of teacher is to provide better guidances for student, rather than achieve high accuracy simply. Inspired by (Yang et al., 2018), we introduce a top score difference (TSD) loss to make the teacher's distribution softer. More concretely, we first pick up K classes with the highest confidence scores from the teacher's output, and then compute the gap between the confidence scores of the primary class and other K-1 classes"
                    },
                    {
                        "section_title": "Objective Functions",
                        "pdf_hash": "",
                        "start": 934,
                        "end": 1151,
                        "sentence_offsets": [
                            {
                                "start": 934,
                                "end": 992
                            },
                            {
                                "start": 993,
                                "end": 1151
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Based on the global statistics, K is set to 3 empirically. We add the penalty term to standard ground-truth loss L T GT when training the teacher, facilitating it to distribute confidence to a few secondary relations"
                    },
                    {
                        "section_title": "Objective Functions",
                        "pdf_hash": "",
                        "start": 1154,
                        "end": 1608,
                        "sentence_offsets": [
                            {
                                "start": 1154,
                                "end": 1266
                            },
                            {
                                "start": 1267,
                                "end": 1440
                            },
                            {
                                "start": 1441,
                                "end": 1606
                            }
                        ],
                        "ref_mentions": [
                            "29308926"
                        ],
                        "quote": "The Student Network Typically, knowledge distillation transfers dark knowledge from the final output of teacher. (Chen et al., 2017) demonstrate that using the intermediate representation of teacher as hint can stabilize the training process and improve the final performance of student. Here, we utilize the Kullback-Leibler divergence to measure the differences of corresponding branches between the teacher and student networks as hint learning loss"
                    },
                    {
                        "section_title": "Objective Functions",
                        "pdf_hash": "",
                        "start": 1788,
                        "end": 2079,
                        "sentence_offsets": [
                            {
                                "start": 1788,
                                "end": 1901
                            },
                            {
                                "start": 1902,
                                "end": 2006
                            },
                            {
                                "start": 2007,
                                "end": 2078
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Intuitively, it encourages the results of MAA to be similar with the scaled global logits of the teacher network. The loss of knowledge distillation is calculated as the sum of L HT and L KD with a weight factor \u03bb ht . As a result, the updated loss of student network is defined as follows:"
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[214623014 | Algan et al. | 2020 | Citations: 53]",
                "snippets": "In the original work, the authors used distillation to transfer knowledge from the big teacher network to a much smaller student network without decreasing the performance. The idea is mainly motivated by learning from soft labels where the similarity of each instance to each class is emphasized by temperature hyperparameter.\n\nClass probabilities on softmax output, beyond the true class probability, are usually very low. But, compared with each other, some classes may have a much higher probability than others, and this carries important information about that data instance, which is also called as dark knowledge. By making probability distribution smoother, this relation is emphasized, as shown in Equation 2.\n\nInstead of being trained on hard labels, the student network is trained on the weighted sum of hard labels and soft labels produced by the teacher network. So, the loss function is defined as follows,\n\nwhere q i represents the soft labels produced by the teacher network using temperature T and y i represents the given label.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C. Feature-dependent noise",
                        "pdf_hash": "",
                        "start": 770,
                        "end": 1817,
                        "sentence_offsets": [
                            {
                                "start": 638,
                                "end": 775
                            },
                            {
                                "start": 776,
                                "end": 877
                            },
                            {
                                "start": 880,
                                "end": 952
                            },
                            {
                                "start": 953,
                                "end": 1125
                            },
                            {
                                "start": 1126,
                                "end": 1280
                            },
                            {
                                "start": 1283,
                                "end": 1378
                            },
                            {
                                "start": 1379,
                                "end": 1575
                            },
                            {
                                "start": 1576,
                                "end": 1673
                            },
                            {
                                "start": 1676,
                                "end": 1831
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the original work, the authors used distillation to transfer knowledge from the big teacher network to a much smaller student network without decreasing the performance. The idea is mainly motivated by learning from soft labels where the similarity of each instance to each class is emphasized by temperature hyperparameter.\n\nClass probabilities on softmax output, beyond the true class probability, are usually very low. But, compared with each other, some classes may have a much higher probability than others, and this carries important information about that data instance, which is also called as dark knowledge. By making probability distribution smoother, this relation is emphasized, as shown in Equation 2.\n\nInstead of being trained on hard labels, the student network is trained on the weighted sum of hard labels and soft labels produced by the teacher network. So, the loss function is defined as follows,\n\nwhere q i represents the soft labels produced by the teacher network using temperature T and y i represents the given label."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[219687782 | Trofimov et al. | 2020 | Citations: 15]",
                "snippets": "The knowledge distillation (KD) assumes two models: a teacher and a student. The teacher is typically a large and accurate network or an ensemble. The student is trained to fit the softmax outputs of the teacher together with ground truth labels. The idea is that outputs of the teacher capture not only the information provided by ground truth labels but also the probabilities of other classes -\"dark knowledge\". The knowledge distillation can be summarized as follows. \n\nLet z i be logits (pre-softmax activations) and q i -probabilities of classes as predicted by a neural network. Knowledge distillation smooths z i with the temperature \u03c4 \n\nNeural networks often do very confident predictions (close to 0 or 1) and smoothing helps to provide for student more information during training [11]. The KD loss is a linear combination of the logistic loss and cross-entropy between predictions of the teacher and the student \n\nwhere z T i , z S i are logits of the teacher and the student, H(p, q) = \u2212p log(q) is the cross-entropy function. The factor \u03c4 2 is used for scaling gradients of both parts of the loss function to be the same order.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation (KD)",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1141,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 76
                            },
                            {
                                "start": 77,
                                "end": 146
                            },
                            {
                                "start": 147,
                                "end": 246
                            },
                            {
                                "start": 247,
                                "end": 414
                            },
                            {
                                "start": 415,
                                "end": 471
                            },
                            {
                                "start": 474,
                                "end": 585
                            },
                            {
                                "start": 586,
                                "end": 643
                            },
                            {
                                "start": 646,
                                "end": 797
                            },
                            {
                                "start": 798,
                                "end": 923
                            },
                            {
                                "start": 926,
                                "end": 1039
                            },
                            {
                                "start": 1040,
                                "end": 1141
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The knowledge distillation (KD) assumes two models: a teacher and a student. The teacher is typically a large and accurate network or an ensemble. The student is trained to fit the softmax outputs of the teacher together with ground truth labels. The idea is that outputs of the teacher capture not only the information provided by ground truth labels but also the probabilities of other classes -\"dark knowledge\". The knowledge distillation can be summarized as follows. \n\nLet z i be logits (pre-softmax activations) and q i -probabilities of classes as predicted by a neural network. Knowledge distillation smooths z i with the temperature \u03c4 \n\nNeural networks often do very confident predictions (close to 0 or 1) and smoothing helps to provide for student more information during training [11]. The KD loss is a linear combination of the logistic loss and cross-entropy between predictions of the teacher and the student \n\nwhere z T i , z S i are logits of the teacher and the student, H(p, q) = \u2212p log(q) is the cross-entropy function. The factor \u03c4 2 is used for scaling gradients of both parts of the loss function to be the same order."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[225040647 | Li et al. | 2020 | Citations: 1]",
                "snippets": "Wu et al. train the student network by taking the output of the teacher network as the soft target, and achieve the knowledge transfer by replacing the L2 loss with cross entropy loss (Wu et al., 2015). Adriana Romero et al. fit the complexity of the teacher network by inputting more no-tag data into the student network. Junho Yim et al. optimized the knowledge transfer by refining knowledge distillation to the layer (Yim et al., 2017).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[9183542 | Wu et al. | 2015 | Citations: 1167]": "Recently, convolutional neural networks (CNN) have demonstrated impressive performance in various computer vision tasks. However, high performance hardware is typically indispensable for the application of CNN models due to the high computation complexity, which prohibits their further extensions. In this paper, we propose an efficient framework, namely Quantized CNN, to simultaneously speed-up the computation and reduce the storage and memory overhead of CNN models. Both filter kernels in convolutional layers and weighting matrices in fully-connected layers are quantized, aiming at minimizing the estimation error of each layer's response. Extensive experiments on the ILSVRC-12 benchmark demonstrate 4 ~ 6\u00d7 speed-up and 15 ~ 20\u00d7 compression with merely one percentage loss of classification accuracy. With our quantized CNN model, even mobile devices can accurately classify images within one second."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 217,
                        "end": 630,
                        "sentence_offsets": [
                            {
                                "start": 217,
                                "end": 406
                            },
                            {
                                "start": 407,
                                "end": 526
                            },
                            {
                                "start": 527,
                                "end": 630
                            }
                        ],
                        "ref_mentions": [
                            "9183542",
                            "206596723"
                        ],
                        "quote": "Wu et al. train the student network by taking the output of the teacher network as the soft target, and achieve the knowledge transfer by replacing the L2 loss with cross entropy loss (Wu et al., 2015). Adriana Romero et al. fit the complexity of the teacher network by inputting more no-tag data into the student network. Junho Yim et al. optimized the knowledge transfer by refining knowledge distillation to the layer (Yim et al., 2017)."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[227013462 | Nayak et al. | 2020 | Citations: 19]",
                "snippets": "The core idea of Knowledge Distillation, as discussed in the seminal paper by Hinton et al. [7], is to transfer the (input to output) learned mapping function from Teacher to Student via sharing the \"dark knowledge\" extracted by the Teacher on the training images. This typically is achieved via matching the soft targets (or soft labels, i.e., output of softmax layer) predicted by the Student to that of the Teacher for the same inputs. This is the distillation mechanism that enables transfer of the better generalization capability (i.e., the \"knowledge\") of the Teacher to the Student.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 215,
                        "end": 805,
                        "sentence_offsets": [
                            {
                                "start": 118,
                                "end": 264
                            },
                            {
                                "start": 265,
                                "end": 530
                            },
                            {
                                "start": 531,
                                "end": 704
                            },
                            {
                                "start": 705,
                                "end": 856
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "The core idea of Knowledge Distillation, as discussed in the seminal paper by Hinton et al. [7], is to transfer the (input to output) learned mapping function from Teacher to Student via sharing the \"dark knowledge\" extracted by the Teacher on the training images. This typically is achieved via matching the soft targets (or soft labels, i.e., output of softmax layer) predicted by the Student to that of the Teacher for the same inputs. This is the distillation mechanism that enables transfer of the better generalization capability (i.e., the \"knowledge\") of the Teacher to the Student."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[231648215 | Feng et al. | 2021 | Citations: 10]",
                "snippets": "For dark knowledge based distillation, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits as follows:\n\nwhere g s and g t are the logits from the student and teacher respectively. T KD denotes the temperature value which controls the smoothness of the output distribution. Note that for the regression problem, the above loss is reformulated as the mean square error between the student's and the teacher's logits.\n\nWe combined the above three types of loss as our final KD loss, namely: L KD = L att + L hidden + L dark .",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1585,
                        "end": 2161,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "For dark knowledge based distillation, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits as follows:\n\nwhere g s and g t are the logits from the student and teacher respectively. T KD denotes the temperature value which controls the smoothness of the output distribution. Note that for the regression problem, the above loss is reformulated as the mean square error between the student's and the teacher's logits.\n\nWe combined the above three types of loss as our final KD loss, namely: L KD = L att + L hidden + L dark ."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[232147231 | Chen et al. | 2021 | Citations: 64]",
                "snippets": "After obtaining two pre-trained domain-mixed teachers, we employ knowledge distillation (KD), a technique to distilling knowledge by minimizing the KL-divergence between outputs of these two models. Here we adapt it to extract \"dark knowledge\" from these two complementary teachers. The pipeline of multi-teacher KD is shown in Fig. 1 (b), including two pre-trained domain-mixed teachers and one student with the same network architecture as teacher. The outputs of two teachers are ensembled as a stronger guidance to supervise the training of the student model on unlabeled target data. Besides, the student model is also supervised by the labels on the small amount of labeled target data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Multi-teacher Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 692,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 198
                            },
                            {
                                "start": 199,
                                "end": 282
                            },
                            {
                                "start": 283,
                                "end": 450
                            },
                            {
                                "start": 451,
                                "end": 588
                            },
                            {
                                "start": 589,
                                "end": 692
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "After obtaining two pre-trained domain-mixed teachers, we employ knowledge distillation (KD), a technique to distilling knowledge by minimizing the KL-divergence between outputs of these two models. Here we adapt it to extract \"dark knowledge\" from these two complementary teachers. The pipeline of multi-teacher KD is shown in Fig. 1 (b), including two pre-trained domain-mixed teachers and one student with the same network architecture as teacher. The outputs of two teachers are ensembled as a stronger guidance to supervise the training of the student model on unlabeled target data. Besides, the student model is also supervised by the labels on the small amount of labeled target data."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[232269823 | Zhao et al. | 2021 | Citations: 3]",
                "snippets": "Later, Hinton et al. (Hinton et al., 2015) popularize the idea of knowledge distillation, which efficiently transfers knowledge from large teacher network to compact student network by mimicking the class probabilities outputs. Note that, the outputs of the teacher network are defined as the dark knowledge in KD, which provides similarity information as extra supervisions compared with one-hot labels. In other words, there are two sources of supervision used to train the student in KD, one from the ground-truth label, and the other from the soft targets of teacher network.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "B. Knowledge Distillation.",
                        "pdf_hash": "",
                        "start": 430,
                        "end": 992,
                        "sentence_offsets": [
                            {
                                "start": 430,
                                "end": 640
                            },
                            {
                                "start": 641,
                                "end": 817
                            },
                            {
                                "start": 818,
                                "end": 992
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Later, Hinton et al. (Hinton et al., 2015) popularize the idea of knowledge distillation, which efficiently transfers knowledge from large teacher network to compact student network by mimicking the class probabilities outputs. Note that, the outputs of the teacher network are defined as the dark knowledge in KD, which provides similarity information as extra supervisions compared with one-hot labels. In other words, there are two sources of supervision used to train the student in KD, one from the ground-truth label, and the other from the soft targets of teacher network."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[233210081 | Zhao et al. | 2021 | Citations: 20]",
                "snippets": "Knowledge Distillation (KD) is introduced by Hinton et al. (Hinton et al., 2015) to solve model compression problems. It transfers the dark knowledge from the pre-trained teacher network to the compact student network by mimicking the class probabilities outputs, which are softened by setting a temperature hyperparameter in softmax. Then the performance of compact student can be retained and close to the performance of the teacher.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "Data-driven knowledge distillation",
                        "pdf_hash": "",
                        "start": 267,
                        "end": 685,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Knowledge Distillation (KD) is introduced by Hinton et al. (Hinton et al., 2015) to solve model compression problems. It transfers the dark knowledge from the pre-trained teacher network to the compact student network by mimicking the class probabilities outputs, which are softened by setting a temperature hyperparameter in softmax. Then the performance of compact student can be retained and close to the performance of the teacher."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[233407431 | Ge et al. | 2021 | Citations: 26]",
                "snippets": "Knowledge distillation [21] can be considered as regularizing the training of the student network using soft targets that carry the \"dark knowledge\" of the teacher network.\n\nFor each training sample x, its logit vector is encoded as z = C(F (x)). The predictive probability vector p \u03c4 can be obtained via a softmax function on the logits, i.e., the probability of class k can be formulated as \n\nwhere \u03c4 is a temperature hyper-parameter, and K is the number of total classes. Let y \u2208 {1, . . . , K} denotes the ground truth label and q \u03c4 is the soft target produced by the teacher network. The cross-entropy loss and the KL divergence between the predictions and soft targets are minimized jointly to train the student via \n\nwhere p(y) denotes the probability normalized without a temperature, and \u03bb weights the two terms.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Knowledge distillation [21] can be considered as regularizing the training of the student network using soft targets that carry the \"dark knowledge\" of the teacher network.\n\nFor each training sample x, its logit vector is encoded as z = C(F (x)). The predictive probability vector p \u03c4 can be obtained via a softmax function on the logits, i.e., the probability of class k can be formulated as \n\nwhere \u03c4 is a temperature hyper-parameter, and K is the number of total classes. Let y \u2208 {1, . . . , K} denotes the ground truth label and q \u03c4 is the soft target produced by the teacher network. The cross-entropy loss and the KL divergence between the predictions and soft targets are minimized jointly to train the student via \n\nwhere p(y) denotes the probability normalized without a temperature, and \u03bb weights the two terms.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[233777216 | Zhou et al. | 2021 | Citations: 52]",
                "snippets": "The key that a teacher can successfully transfer the knowledge to the student is to get the prediction experience from the teacher's loss function, which is called the dark knowledge (Tung et al., 2019). The entire distillation procedure is actually based on the transferring of this dark knowledge, from the teacher to the student. Therefore, the core modification of the student's loss function (i.e., overall loss) can be formulated as: \n\nwhere x, y, w and \u03c3 represent the input data, ground-truth label, model parameters and the softmax block, respectively. Besides, z s and z d are the logits of the student and teacher, respectively. Note that the overall loss function L contains two parts: the student loss L s and distillation loss the L d , under the control of the coefficient hyper-parameters \u03b1 and \u03b2.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[198179476 | Tung et al. | 2019 | Citations: 981]": "Knowledge distillation is a widely applicable technique for training a student neural network under the guidance of a trained teacher network. For example, in neural network compression, a high-capacity teacher is distilled to train a compact student; in privileged learning, a teacher trained with privileged data is distilled to train a student without access to that data. The distillation loss determines how a teacher's knowledge is captured and transferred to the student. In this paper, we propose a new form of knowledge distillation loss that is inspired by the observation that semantically similar inputs tend to elicit similar activation patterns in a trained network. Similarity-preserving knowledge distillation guides the training of a student network such that input pairs that produce similar (dissimilar) activations in the teacher network produce similar (dissimilar) activations in the student network. In contrast to previous distillation methods, the student is not required to mimic the representation space of the teacher, but rather to preserve the pairwise similarities in its own representation space. Experiments on three public datasets demonstrate the potential of our approach."
                },
                "metadata": [
                    {
                        "section_title": "B. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1437,
                        "end": 2235,
                        "sentence_offsets": [
                            {
                                "start": 1437,
                                "end": 1625
                            },
                            {
                                "start": 1626,
                                "end": 1754
                            },
                            {
                                "start": 1755,
                                "end": 1861
                            },
                            {
                                "start": 1864,
                                "end": 1983
                            },
                            {
                                "start": 1984,
                                "end": 2061
                            },
                            {
                                "start": 2062,
                                "end": 2235
                            }
                        ],
                        "ref_mentions": [
                            "198179476"
                        ],
                        "quote": "The key that a teacher can successfully transfer the knowledge to the student is to get the prediction experience from the teacher's loss function, which is called the dark knowledge (Tung et al., 2019). The entire distillation procedure is actually based on the transferring of this dark knowledge, from the teacher to the student. Therefore, the core modification of the student's loss function (i.e., overall loss) can be formulated as: \n\nwhere x, y, w and \u03c3 represent the input data, ground-truth label, model parameters and the softmax block, respectively. Besides, z s and z d are the logits of the student and teacher, respectively. Note that the overall loss function L contains two parts: the student loss L s and distillation loss the L d , under the control of the coefficient hyper-parameters \u03b1 and \u03b2."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[234336288 | Jaiswal et al. | 2021 | Citations: 0]",
                "snippets": "Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model....In general, knowledge is transferred from the 'T' to 'S' by minimizing a loss function in which the target is the distribution of class probabilities predicted by 'T'. This probability distribution has the correct class at a very high probability (close to '1') with all other class probabilities very close to '0'. As such, it does not provide much information beyond the ground truth labels already provided in the dataset. For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 511,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 54
                            },
                            {
                                "start": 57,
                                "end": 119
                            },
                            {
                                "start": 120,
                                "end": 153
                            },
                            {
                                "start": 156,
                                "end": 234
                            },
                            {
                                "start": 235,
                                "end": 370
                            },
                            {
                                "start": 371,
                                "end": 411
                            },
                            {
                                "start": 414,
                                "end": 509
                            },
                            {
                                "start": 510,
                                "end": 513
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model"
                    },
                    {
                        "section_title": "C. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1291,
                        "end": 2071,
                        "sentence_offsets": [
                            {
                                "start": 1291,
                                "end": 1458
                            },
                            {
                                "start": 1459,
                                "end": 1606
                            },
                            {
                                "start": 1607,
                                "end": 1716
                            },
                            {
                                "start": 1717,
                                "end": 1788
                            },
                            {
                                "start": 1789,
                                "end": 1968
                            },
                            {
                                "start": 1969,
                                "end": 2069
                            },
                            {
                                "start": 2070,
                                "end": 2128
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".In general, knowledge is transferred from the 'T' to 'S' by minimizing a loss function in which the target is the distribution of class probabilities predicted by 'T'. This probability distribution has the correct class at a very high probability (close to '1') with all other class probabilities very close to '0'. As such, it does not provide much information beyond the ground truth labels already provided in the dataset. For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[235421655 | Lee et al. | 2021 | Citations: 12]",
                "snippets": "[11] proposed the concept of soft label which is distilled from the teacher model to the student model. Soft labels are the probabilities of an input belonging to each class, and can be estimated by a softmax function. Here, a temperature factor is introduced to control the importance of each soft label, which contains the informative dark knowledge from the teacher model. By increasing the temperature, the logits can contain richer information than one-hot labels. However, if the temperature becomes too large, the probability of irrelevant classes will also be over-emphasized.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 441,
                        "end": 1025,
                        "sentence_offsets": [
                            {
                                "start": 441,
                                "end": 544
                            },
                            {
                                "start": 545,
                                "end": 659
                            },
                            {
                                "start": 660,
                                "end": 816
                            },
                            {
                                "start": 817,
                                "end": 910
                            },
                            {
                                "start": 911,
                                "end": 1025
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "[11] proposed the concept of soft label which is distilled from the teacher model to the student model. Soft labels are the probabilities of an input belonging to each class, and can be estimated by a softmax function. Here, a temperature factor is introduced to control the importance of each soft label, which contains the informative dark knowledge from the teacher model. By increasing the temperature, the logits can contain richer information than one-hot labels. However, if the temperature becomes too large, the probability of irrelevant classes will also be over-emphasized."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[235489777 | Zhao et al. | 2021 | Citations: 24]",
                "snippets": "The idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels.\n\nLet us denote x, y as the input of the DNN and one-hot labels of our architecture. \u03c6 s and \u03c6 t represent the student network and teacher network with parameters W s and W t respectively. Given an input image x, the teacher network outputs the final predictions as P T which are obtained by applying the softmax function on the un-normalized log proba-bility values a T , i.e. P T = sof tmax(a T ). Similarly, the same image is fed into the student network to obtain the predictions P S = sof tmax(a S ).\n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss:\n\nIt contains the relative probabilities of incorrect classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4, the student network will be trained to optimize the following loss function:\n\nwhere \u03bb is a second hyper parameter controlling the trade-off between the two losses.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels.\n\nLet us denote x, y as the input of the DNN and one-hot labels of our architecture. \u03c6 s and \u03c6 t represent the student network and teacher network with parameters W s and W t respectively. Given an input image x, the teacher network outputs the final predictions as P T which are obtained by applying the softmax function on the un-normalized log proba-bility values a T , i.e. P T = sof tmax(a T ). Similarly, the same image is fed into the student network to obtain the predictions P S = sof tmax(a S ).\n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss:\n\nIt contains the relative probabilities of incorrect classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4, the student network will be trained to optimize the following loss function:\n\nwhere \u03bb is a second hyper parameter controlling the trade-off between the two losses.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[236984375 | Wang et al. | 2021 | Citations: 3]",
                "snippets": "Knowledge distillation (Figure 3) is put forward by Hinton et al. (Hinton et al., 2015). It is a widely used knowledge transfer technology in the deep learning field. First, a well-trained, robust, high-precision teacher network is needed. Its output is softened with temperature T to provide more information entropy, which extracts hidden knowledge behind its output layer. Then, a relatively small student network is trained to imitate the teacher network's probability output distribution, obtaining a better output result. The main idea of knowledge distillation. The label of the input image is cat; the probability is expressed as {0, 1, 0}. After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q'', which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics...To improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become...where Q T s , Q T t are the softmax probability distribution of the student network and teacher network after softening according to temperature T.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1158,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 70
                            },
                            {
                                "start": 71,
                                "end": 148
                            },
                            {
                                "start": 149,
                                "end": 221
                            },
                            {
                                "start": 222,
                                "end": 357
                            },
                            {
                                "start": 358,
                                "end": 509
                            },
                            {
                                "start": 510,
                                "end": 550
                            },
                            {
                                "start": 551,
                                "end": 630
                            },
                            {
                                "start": 631,
                                "end": 785
                            },
                            {
                                "start": 786,
                                "end": 875
                            },
                            {
                                "start": 876,
                                "end": 940
                            },
                            {
                                "start": 941,
                                "end": 1010
                            },
                            {
                                "start": 1011,
                                "end": 1159
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Knowledge distillation (Figure 3) is put forward by Hinton et al. (Hinton et al., 2015). It is a widely used knowledge transfer technology in the deep learning field. First, a well-trained, robust, high-precision teacher network is needed. Its output is softened with temperature T to provide more information entropy, which extracts hidden knowledge behind its output layer. Then, a relatively small student network is trained to imitate the teacher network's probability output distribution, obtaining a better output result. The main idea of knowledge distillation. The label of the input image is cat; the probability is expressed as {0, 1, 0}. After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q'', which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics"
                    },
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1162,
                        "end": 1324,
                        "sentence_offsets": [
                            {
                                "start": 1162,
                                "end": 1479
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become"
                    },
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 2020,
                        "end": 2168,
                        "sentence_offsets": [
                            {
                                "start": 2020,
                                "end": 2167
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "where Q T s , Q T t are the softmax probability distribution of the student network and teacher network after softening according to temperature T."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[236987020 | Jung et al. | 2021 | Citations: 2]",
                "snippets": "Knowledge Distillation (KD) is a method of transferring information from a larger and cumbersome teacher model to a new student model, which induces good performance even that student is a small and shallow model [14]- (Mirzadeh et al., 2019). The main factor of this effect is to transfer dark knowledge of the teacher as a smoothed soft label. [8], (Guo et al., 2017) To effectively use the dark knowledge, the softmax function is modified as follows: \n\nwhere T is temperature, and N is the number of classes. The new loss function that uses both hard and soft labels is defined as \n\nwhere L CE is the cross-entropy loss, y t and y s are the output probability of the teacher and student, and y gt is the ground truth.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[212908749 | Mirzadeh et al. | 2019 | Citations: 1081]": "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.",
                    "[28671436 | Guo et al. | 2017 | Citations: 5869]": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."
                },
                "metadata": [
                    {
                        "section_title": "II. RELATED WORK A. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 687,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 224
                            },
                            {
                                "start": 225,
                                "end": 326
                            },
                            {
                                "start": 327,
                                "end": 420
                            },
                            {
                                "start": 423,
                                "end": 478
                            },
                            {
                                "start": 479,
                                "end": 550
                            },
                            {
                                "start": 553,
                                "end": 687
                            }
                        ],
                        "ref_mentions": [
                            "212908749",
                            "28671436"
                        ],
                        "quote": "Knowledge Distillation (KD) is a method of transferring information from a larger and cumbersome teacher model to a new student model, which induces good performance even that student is a small and shallow model [14]- (Mirzadeh et al., 2019). The main factor of this effect is to transfer dark knowledge of the teacher as a smoothed soft label. [8], (Guo et al., 2017) To effectively use the dark knowledge, the softmax function is modified as follows: \n\nwhere T is temperature, and N is the number of classes. The new loss function that uses both hard and soft labels is defined as \n\nwhere L CE is the cross-entropy loss, y t and y s are the output probability of the teacher and student, and y gt is the ground truth."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[237453494 | Li et al. | 2021 | Citations: 103]",
                "snippets": "Knowledge distillation (Hinton et al., 2015) is first introduced for transferring knowledge from high-capability networks (teacher models) to small networks (student models) via soft labels...the dark knowledge in soft labels can be helpful by transferring knowledge from head classes to tail classes...We denote x a training image with its hard label y and soft label y...The feature vector f will be sent to two linear classifiers G hard and G sof t to get output logits z hard = G hard (f ) and z sof t = G sof t (f ). Let z denote the output logits of teacher model, then the soft label is given by:\n\nwhere i is the category index and T is the temperature which is set to 2 by default. Then, the knowledge distillation loss is written as:\n\n(4) For hard label supervision, we utilize the standard cross entropy loss L ce . Thus, the final loss is the combination of these two losses:\n\nwhere both of \u03bb 1 and \u03bb 2 are the weight of each loss and set to 1 in our experiments.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "Long-tailed recognition via self-distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 172,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Knowledge distillation (Hinton et al., 2015) is first introduced for transferring knowledge from high-capability networks (teacher models) to small networks (student models) via soft labels"
                    },
                    {
                        "section_title": "Long-tailed recognition via self-distillation",
                        "pdf_hash": "",
                        "start": 440,
                        "end": 549,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "the dark knowledge in soft labels can be helpful by transferring knowledge from head classes to tail classes"
                    },
                    {
                        "section_title": "Long-tailed recognition via self-distillation",
                        "pdf_hash": "",
                        "start": 745,
                        "end": 813,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We denote x a training image with its hard label y and soft label y"
                    },
                    {
                        "section_title": "Long-tailed recognition via self-distillation",
                        "pdf_hash": "",
                        "start": 953,
                        "end": 1555,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The feature vector f will be sent to two linear classifiers G hard and G sof t to get output logits z hard = G hard (f ) and z sof t = G sof t (f ). Let z denote the output logits of teacher model, then the soft label is given by:\n\nwhere i is the category index and T is the temperature which is set to 2 by default. Then, the knowledge distillation loss is written as:\n\n(4) For hard label supervision, we utilize the standard cross entropy loss L ce . Thus, the final loss is the combination of these two losses:\n\nwhere both of \u03bb 1 and \u03bb 2 are the weight of each loss and set to 1 in our experiments."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[249017724 | Bai et al. | 2022 | Citations: 0]",
                "snippets": "Knowledge distillation (KD) [26,27](Yu et al., 2019) is a model compression method, in which a student network is trained by learning the knowledge from a teacher network. The knowledge is expressed in the form of softened probability (Yu et al., 2019)(Peng et al., 2019), which can reflect the inherent class similarity structure known as dark knowledge. The distillation objective encourages the output probability distribution over predictions from the student and teacher networks to be similar. With the help of additional information on top of the one-hot labels, the performance of student network can be boosted.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[102351826 | Yu et al. | 2019 | Citations: 109]": "Metric learning networks are used to compute image embeddings, which are widely used in many applications such as image retrieval and face recognition. In this paper, we propose to use network distillation to efficiently compute image embeddings with small networks. Network distillation has been successfully applied to improve image classification, but has hardly been explored for metric learning. To do so, we propose two new loss functions that model the communication of a deep teacher network to a small student network. We evaluate our system in several datasets, including CUB-200-2011, Cars-196, Stanford Online Products and show that embeddings computed using small student networks perform significantly better than those computed using standard networks of similar size. Results on a very compact network (MobileNet-0.25), which can be used on mobile devices, show that the proposed method can greatly improve Recall@1 results from 27.5\\% to 44.6\\%. Furthermore, we investigate various aspects of distillation for embeddings, including hint and attention layers, semi-supervised learning and cross quality distillation. (Code is available at https://github.com/yulu0724/EmbeddingDistillation).",
                    "[102483463 | Peng et al. | 2019 | Citations: 513]": "Most teacher-student frameworks based on knowledge distillation (KD) depend on a strong congruent constraint on instance level. However, they usually ignore the correlation between multiple instances, which is also valuable for knowledge transfer. In this work, we propose a new framework named correlation congruence for knowledge distillation (CCKD), which transfers not only the instance-level information but also the correlation between instances. Furthermore, a generalized kernel method based on Taylor series expansion is proposed to better capture the correlation between instances. Empirical experiments and ablation studies on image classification tasks (including CIFAR-100, ImageNet-1K) and metric learning tasks (including ReID and Face Recognition) show that the proposed CCKD substantially outperforms the original KD and other SOTA KD-based methods. The CCKD can be easily deployed in the majority of the teacher-student framework such as KD and hint-based learning methods."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 577,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 157
                            },
                            {
                                "start": 158,
                                "end": 312
                            },
                            {
                                "start": 313,
                                "end": 456
                            },
                            {
                                "start": 457,
                                "end": 577
                            }
                        ],
                        "ref_mentions": [
                            "102351826",
                            "102351826",
                            "102483463"
                        ],
                        "quote": "Knowledge distillation (KD) [26,27](Yu et al., 2019) is a model compression method, in which a student network is trained by learning the knowledge from a teacher network. The knowledge is expressed in the form of softened probability (Yu et al., 2019)(Peng et al., 2019), which can reflect the inherent class similarity structure known as dark knowledge. The distillation objective encourages the output probability distribution over predictions from the student and teacher networks to be similar. With the help of additional information on top of the one-hot labels, the performance of student network can be boosted."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[251066725 | Liang et al. | 2022 | Citations: 19]",
                "snippets": "Instead of imposing a fixed prior distribution, knowledge distillation was first proposed by Hinton in [12] to provide sample-level non-uniform soft labels. They demonstrated that the \"dark knowledge\" lies in the output distributions from a large capacity teacher network and benefits the student's representation learning. Recent works mainly explored to better transfer the \"dark knowledge\" and improve the efficiency from various aspects, such as reducing the difference between the teacher and student [3,(Cho et al., 2019)(Mirzadeh et al., 2019)(Zhu et al., 2021), designing student-friendly architecture [16,(Park et al., 2021), improving the distillation efficiency (Furlanello et al., 2018)(Kim et al., 2020)(Xu et al., 2019)(Yun et al., 2020) and explaining the distillation's working mechanism [1](Stanton et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[203642130 | Cho et al. | 2019 | Citations: 619]": "In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don\u2019t make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher\u2019s training early. Our results generalize across datasets and models.",
                    "[212908749 | Mirzadeh et al. | 2019 | Citations: 1081]": "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach.",
                    "[214727822 | Yun et al. | 2020 | Citations: 281]": "Deep neural networks with millions of parameters may suffer from poor generalization due to overfitting. To mitigate the issue, we propose a new regularization method that penalizes the predictive distribution between similar samples. In particular, we distill the predictive distribution between different samples of the same label during training. This results in regularizing the dark knowledge (i.e., the knowledge on wrong predictions) of a single network (i.e., a self-knowledge distillation) by forcing it to produce more meaningful and consistent predictions in a class-wise manner. Consequently, it mitigates overconfident predictions and reduces intra-class variations. Our experimental results on various image classification tasks demonstrate that the simple yet powerful method can significantly improve not only the generalization ability but also the calibration performance of modern convolutional neural networks.",
                    "[231925118 | Park et al. | 2021 | Citations: 101]": "We propose a novel knowledge distillation approach to facilitate the transfer of dark knowledge from a teacher to a student. Contrary to most of the existing methods that rely on effective training of student models given pretrained teachers, we aim to learn the teacher models that are friendly to students and, consequently, more appropriate for knowledge transfer. In other words, at the time of optimizing a teacher model, the proposed algorithm learns the student branches jointly to obtain student-friendly representations. Since the main goal of our approach lies in training teacher models and the subsequent knowledge distillation procedure is straightforward, most of the existing knowledge distillation methods can adopt this technique to improve the performance of diverse student models in terms of accuracy and convergence speed. The proposed algorithm demonstrates outstanding accuracy in several well-known knowledge distillation techniques with various combinations of teacher and student models even in the case that their architectures are heterogeneous and there is no prior knowledge about student models at the time of training teacher networks.",
                    "[233714221 | Kim et al. | 2020 | Citations: 182]": "The generalization capability of deep neural networks has been substantially improved by applying a wide spectrum of regularization methods, e.g., restricting function space, injecting randomness during training, augmenting data, etc. In this work, we propose a simple yet effective regularization method named progressive self-knowledge distillation (PS-KD), which progressively distills a model\u2019s own knowledge to soften hard targets (i.e., one-hot vectors) during training. Hence, it can be interpreted within a framework of knowledge distillation as a student becomes a teacher itself. Specifically, targets are adjusted adaptively by combining the ground-truth and past predictions from the model itself. We show that PS-KD provides an effect of hard example mining by rescaling gradients according to difficulty in classifying examples. The proposed method is applicable to any supervised learning tasks with hard targets and can be easily combined with existing regularization methods to further enhance the generalization performance. Furthermore, it is confirmed that PS-KD achieves not only better accuracy, but also provides high quality of confidence estimates in terms of calibration as well as ordinal ranking. Extensive experimental results on three different tasks, image classification, object detection, and machine translation, demonstrate that our method consistently improves the performance of the state-of-the-art baselines. The code is available at https://github.com/lgcnsai/PS-KD-Pytorch.",
                    "[235390933 | Stanton et al. | 2021 | Citations: 222]": "Knowledge distillation is a popular technique for training a small student network to emulate a larger teacher model, such as an ensemble of networks. We show that while knowledge distillation can improve student generalization, it does not typically work as it is commonly understood: there often remains a surprisingly large discrepancy between the predictive distributions of the teacher and the student, even in cases when the student has the capacity to perfectly match the teacher. We identify difficulties in optimization as a key reason for why the student is unable to match the teacher. We also show how the details of the dataset used for distillation play a role in how closely the student matches the teacher -- and that more closely matching the teacher paradoxically does not always lead to better student generalization.",
                    "[244680427 | Zhu et al. | 2021 | Citations: 83]": "Knowledge distillation (KD) transfers the dark knowledge from cumbersome networks (teacher) to lightweight (student) networks and expects the student to achieve more promising performance than training without the teacher\u2019s knowledge. However, a counter-intuitive argument is that better teachers do not make better students due to the capacity mismatch. To this end, we present a novel adaptive knowledge distillation method to complement traditional approaches. The proposed method, named as Student Customized Knowledge Distillation (SCKD), examines the capacity mismatch between teacher and student from the perspective of gradient similarity. We formulate the knowledge distillation as a multi-task learning problem so that the teacher transfers knowledge to the student only if the student can benefit from learning such knowledge. We validate our methods on multiple datasets with various teacher-student configurations on image classification, object detection, and semantic segmentation.",
                    "[4110009 | Furlanello et al. | 2018 | Citations: 1034]": "Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction.",
                    "[70335318 | Xu et al. | 2019 | Citations: 175]": "Knowledge distillation is an effective technique that has been widely used for transferring knowledge from a network to another network. Despite its effective improvement of network performance, the dependence of accompanying assistive models complicates the training process of single network in the need of large memory and time cost. In this paper, we design a more elegant self-distillation mechanism to transfer knowledge between different distorted versions of same training data without the reliance on accompanying models. Specifically, the potential capacity of single network is excavated by learning consistent global feature distributions and posterior distributions (class probabilities) across these distorted versions of data. Extensive experiments on multiple datasets (i.e., CIFAR-10/100 and ImageNet) demonstrate that the proposed method can effectively improve the generalization performance of various network architectures (such as AlexNet, ResNet, Wide ResNet, and DenseNet), outperform existing distillation methods with little extra training efforts."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 678,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 156
                            },
                            {
                                "start": 157,
                                "end": 323
                            },
                            {
                                "start": 324,
                                "end": 678
                            }
                        ],
                        "ref_mentions": [
                            "203642130",
                            "212908749",
                            "244680427",
                            "231925118",
                            "4110009",
                            "233714221",
                            "70335318",
                            "214727822",
                            "235390933"
                        ],
                        "quote": "Instead of imposing a fixed prior distribution, knowledge distillation was first proposed by Hinton in [12] to provide sample-level non-uniform soft labels. They demonstrated that the \"dark knowledge\" lies in the output distributions from a large capacity teacher network and benefits the student's representation learning. Recent works mainly explored to better transfer the \"dark knowledge\" and improve the efficiency from various aspects, such as reducing the difference between the teacher and student [3,(Cho et al., 2019)(Mirzadeh et al., 2019)(Zhu et al., 2021), designing student-friendly architecture [16,(Park et al., 2021), improving the distillation efficiency (Furlanello et al., 2018)(Kim et al., 2020)(Xu et al., 2019)(Yun et al., 2020) and explaining the distillation's working mechanism [1](Stanton et al., 2021)."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[252596141 | Nguyen et al. | 2022 | Citations: 2]",
                "snippets": "Knowledge Distillation (KD) Hinton et al. (2015) employs a technique to transfer the learned knowledge from a pre-trained teacher model to another model with less or similar complexity (student model). The model transfer process is implemented in two steps. In the first step, a surrogate output probability function, namely a temperature-softmax function, is utilized. By adding a temperature scaling variable T, the conventional softmax function then becomes: \n\nwhere zj is the output set corresponding to class j of the given DNN with batch of input data X. \n\nThe subscript l denotes the index of softmax output which corresponds to the prediction on class l of the DNN. The intention of adding the variable T is to adjust the slope of the softmax function in the classifier as shown in equation 16. As we can see, when we increase the value of T, the slope of the softmax function will decrease significantly. With large temperature scale values, over the same output range of data, the range of values represented by input zj is larger. Then, the output value tuple created by the teacher and student carries considerably more information. Therefore, the learning process between teacher and student is more effective. \n\nIn the second step, to help implement the transfer of knowledge from teacher to student, the authors in Hinton et al. (2015) presented a new loss function, called distillation loss function (also known as soft-loss function). This loss function comprises two terms: the intrinsic entropy function of the teacher and the cross-entropy function between the teacher and the student's outputs, which can be expressed as follows: \n\nThe purpose of this function is to compare the output distribution between teacher p(X) and student model q(X). In terms of information theory, this measurement shows the under-performance of the distribution set created by student, when the output distribution set of the teacher is taken as the sample distribution set, with precision of 100%. By minimizing this function, we reduce the functional difference between the two deep networks. As a result, the student model tends to make it's output distribution become more identical to the teacher model's behavior.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A.2 KNOWLEDGE DISTILLATION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2219,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 201
                            },
                            {
                                "start": 202,
                                "end": 257
                            },
                            {
                                "start": 258,
                                "end": 369
                            },
                            {
                                "start": 370,
                                "end": 462
                            },
                            {
                                "start": 465,
                                "end": 562
                            },
                            {
                                "start": 565,
                                "end": 675
                            },
                            {
                                "start": 676,
                                "end": 804
                            },
                            {
                                "start": 805,
                                "end": 916
                            },
                            {
                                "start": 917,
                                "end": 1045
                            },
                            {
                                "start": 1046,
                                "end": 1148
                            },
                            {
                                "start": 1149,
                                "end": 1227
                            },
                            {
                                "start": 1230,
                                "end": 1455
                            },
                            {
                                "start": 1456,
                                "end": 1654
                            },
                            {
                                "start": 1657,
                                "end": 1768
                            },
                            {
                                "start": 1769,
                                "end": 2002
                            },
                            {
                                "start": 2003,
                                "end": 2098
                            },
                            {
                                "start": 2099,
                                "end": 2223
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge Distillation (KD) Hinton et al. (2015) employs a technique to transfer the learned knowledge from a pre-trained teacher model to another model with less or similar complexity (student model). The model transfer process is implemented in two steps. In the first step, a surrogate output probability function, namely a temperature-softmax function, is utilized. By adding a temperature scaling variable T, the conventional softmax function then becomes: \n\nwhere zj is the output set corresponding to class j of the given DNN with batch of input data X. \n\nThe subscript l denotes the index of softmax output which corresponds to the prediction on class l of the DNN. The intention of adding the variable T is to adjust the slope of the softmax function in the classifier as shown in equation 16. As we can see, when we increase the value of T, the slope of the softmax function will decrease significantly. With large temperature scale values, over the same output range of data, the range of values represented by input zj is larger. Then, the output value tuple created by the teacher and student carries considerably more information. Therefore, the learning process between teacher and student is more effective. \n\nIn the second step, to help implement the transfer of knowledge from teacher to student, the authors in Hinton et al. (2015) presented a new loss function, called distillation loss function (also known as soft-loss function). This loss function comprises two terms: the intrinsic entropy function of the teacher and the cross-entropy function between the teacher and the student's outputs, which can be expressed as follows: \n\nThe purpose of this function is to compare the output distribution between teacher p(X) and student model q(X). In terms of information theory, this measurement shows the under-performance of the distribution set created by student, when the output distribution set of the teacher is taken as the sample distribution set, with precision of 100%. By minimizing this function, we reduce the functional difference between the two deep networks. As a result, the student model tends to make it's output distribution become more identical to the teacher model's behavior."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[252816131 | Kurmanji et al. | 2022 | Citations: 18]",
                "snippets": "KD was first introduced in [22] for   by transferring knowledge from an accurate and \"cumbersome\" model, called teacher, to a smaller model called student. In its basic form, instead of fitting the student model directly to the actual data labels, one would use the class probability distribution learned by the teacher to fit the student model. Hinton et al. [22] argued that small probabilities in \"wrong\" label logits, known as \"soft labels\", include extra information called \"dark knowledge\" that result in better learning than actual \"hard labels\". Distillation has since been extensively studied. Figure 2 shows a general view of the principles of a distillation process. A small dataset referred to as transfer-set is fed into a pre-trained model (teacher) and a new model (student) to be trained. A   is calculated using the predictions of the pre-trained model instead of the actual labels. This loss and a typical loss using actual labels will be used to train the new model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "General Knowledge Distillation (KD)",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 985,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 155
                            },
                            {
                                "start": 156,
                                "end": 345
                            },
                            {
                                "start": 346,
                                "end": 553
                            },
                            {
                                "start": 554,
                                "end": 602
                            },
                            {
                                "start": 603,
                                "end": 677
                            },
                            {
                                "start": 678,
                                "end": 804
                            },
                            {
                                "start": 805,
                                "end": 899
                            },
                            {
                                "start": 900,
                                "end": 985
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "KD was first introduced in [22] for   by transferring knowledge from an accurate and \"cumbersome\" model, called teacher, to a smaller model called student. In its basic form, instead of fitting the student model directly to the actual data labels, one would use the class probability distribution learned by the teacher to fit the student model. Hinton et al. [22] argued that small probabilities in \"wrong\" label logits, known as \"soft labels\", include extra information called \"dark knowledge\" that result in better learning than actual \"hard labels\". Distillation has since been extensively studied. Figure 2 shows a general view of the principles of a distillation process. A small dataset referred to as transfer-set is fed into a pre-trained model (teacher) and a new model (student) to be trained. A   is calculated using the predictions of the pre-trained model instead of the actual labels. This loss and a typical loss using actual labels will be used to train the new model."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[253655030 | Borza et al. | 2022 | Citations: 2]",
                "snippets": "Knowledge distillation (Bucila et al., 2006) was proposed as an effective and elegant compression technique to derive a lighter and faster network (student) from a more complex one (teacher), by penalizing the difference between their logits. Later, this mechanism was formalized by (Hinton et al., 2015) to distill the \"dark-knowledge\" from the teacher to the student. The authors noticed that a powerful and confident teacher does not bring more knowledge than ground truth data, as its prediction tends to be a narrow probability distribution with a single peak for the ground truth class. To alleviate this issue, the teacher's logits are \"softened\" by a temperature scaling factor of the softmax activation. In such a manner, the lightweight network can infer what other classes were found similar by the teacher network. More formally, this can be expressed as \n\nwhere \u03c4 is the temperature of the softmax function (equation from (Hinton et al., 2015)). When \u03c4 is greater than 1, the small probabilities of the softmax function are increased and the output is \"softened\". \n\nIn the classical setup, during training, a Kullback-Libeler divergence loss term is employed to ensure that the student network mimics the teacher's softened predictions. Other methods proposed using the root-mean-square error loss [9] or distilling from hardlabels (Touvron et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[229363322 | Touvron et al. | 2020 | Citations: 6805]": "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1296,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 224
                            },
                            {
                                "start": 225,
                                "end": 333
                            },
                            {
                                "start": 334,
                                "end": 556
                            },
                            {
                                "start": 557,
                                "end": 676
                            },
                            {
                                "start": 677,
                                "end": 790
                            },
                            {
                                "start": 791,
                                "end": 830
                            },
                            {
                                "start": 833,
                                "end": 904
                            },
                            {
                                "start": 905,
                                "end": 1022
                            },
                            {
                                "start": 1025,
                                "end": 1195
                            },
                            {
                                "start": 1196,
                                "end": 1296
                            }
                        ],
                        "ref_mentions": [
                            "11253972",
                            "7200347",
                            "7200347",
                            "229363322"
                        ],
                        "quote": "Knowledge distillation (Bucila et al., 2006) was proposed as an effective and elegant compression technique to derive a lighter and faster network (student) from a more complex one (teacher), by penalizing the difference between their logits. Later, this mechanism was formalized by (Hinton et al., 2015) to distill the \"dark-knowledge\" from the teacher to the student. The authors noticed that a powerful and confident teacher does not bring more knowledge than ground truth data, as its prediction tends to be a narrow probability distribution with a single peak for the ground truth class. To alleviate this issue, the teacher's logits are \"softened\" by a temperature scaling factor of the softmax activation. In such a manner, the lightweight network can infer what other classes were found similar by the teacher network. More formally, this can be expressed as \n\nwhere \u03c4 is the temperature of the softmax function (equation from (Hinton et al., 2015)). When \u03c4 is greater than 1, the small probabilities of the softmax function are increased and the output is \"softened\". \n\nIn the classical setup, during training, a Kullback-Libeler divergence loss term is employed to ensure that the student network mimics the teacher's softened predictions. Other methods proposed using the root-mean-square error loss [9] or distilling from hardlabels (Touvron et al., 2020)."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[256900863 | Zhang et al. | 2023 | Citations: 1]",
                "snippets": "Knowledge distillation (Hinton et al., 2015) transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence (Hinton et al., 2015), the mean squared error (Fang et al., 2021) and the Jensen-Shannon divergence (Yin et al., 2019), etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. (Zhao et al., 2022) pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[209405263 | Yin et al. | 2019 | Citations: 569]": "We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We ``invert'' a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance - (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning.",
                    "[233033712 | Fang et al. | 2021 | Citations: 99]": "Despite exciting progress in pre-training for visual-linguistic (VL) representations, very few aspire to a small VL model. In this paper, we study knowledge distillation (KD) to effectively compress a transformer based large VL model into a small VL model. The major challenge arises from the inconsistent regional visual tokens extracted from different detectors of Teacher and Student, resulting in the misalignment of hidden representations and attention distributions. To address the problem, we retrain and adapt the Teacher by using the same region proposals from Student\u2019s detector while the features are from Teacher\u2019s own object detector. With aligned network inputs, the adapted Teacher is capable of transferring the knowledge through the intermediate representations. Specifically, we use the mean square error loss to mimic the attention distribution inside the transformer block, and present a token-wise noise contrastive loss to align the hidden state by contrasting with negative representations stored in a sample queue. To this end, we show that our proposed distillation significantly improves the performance of small VL models on image captioning and visual question answering tasks. It reaches 120.8 in CIDEr score on COCO captioning, an improvement of 5.1 over its non-distilled counterpart; and an accuracy of 69.8 on VQA 2.0, a 0.8 gain from the baseline. Our extensive experiments and ablations confirm the effectiveness of VL distillation in both pre-training and fine-tuning stages.",
                    "[247476179 | Zhao et al. | 2022 | Citations: 549]": "State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we re-formulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the \u201cdifficulty\u201d of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megviiresearch/mdistiller.",
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "B. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1169,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 209
                            },
                            {
                                "start": 210,
                                "end": 617
                            },
                            {
                                "start": 620,
                                "end": 946
                            },
                            {
                                "start": 947,
                                "end": 1169
                            }
                        ],
                        "ref_mentions": [
                            "7200347",
                            "7200347",
                            "233033712",
                            "209405263",
                            "247476179"
                        ],
                        "quote": "Knowledge distillation (Hinton et al., 2015) transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence (Hinton et al., 2015), the mean squared error (Fang et al., 2021) and the Jensen-Shannon divergence (Yin et al., 2019), etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. (Zhao et al., 2022) pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[257346971 | Liu et al. | 2023 | Citations: 16]",
                "snippets": "The calculations and parameter amounts of the network were reduced significantly after adopting the lightweight design, but so was the detection accuracy. To address this problem, knowledge distillation (KD), a joint training method by transferring \"knowledge\", was employed to improve the detection accuracy. The KD structure was shown in Figure 3. KD is the process of imitating the distillation in chemistry, using the softmax function with temperature parameters to \"distill\" the logit output from complex and large networks, so as to generate more information in categories. This part of the in-formation is called \"dark knowledge\". The additional information guides the simple and small network to learn more knowledge, and the two networks are called the teacher network and the student network, respectively.\n\nTo diversify the information distribution output by the teacher network, we used the temperature parameter \u03c4 to get soft prediction output by distilling logits output between the teacher network and student network. The same dataset was used because soft prediction output implied the information of the negative samples. With the help of SoftMax active function, the teacher network's class prediction probability distribution could be regarded as the soft target. Similarly, this method was used to get not only the soft prediction output but also the hard prediction output from the student network. As for the soft prediction output, soft prediction output and soft target were used to calculate loss value by loss function L so f t , which was a part of total loss.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1588,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 154
                            },
                            {
                                "start": 155,
                                "end": 309
                            },
                            {
                                "start": 310,
                                "end": 579
                            },
                            {
                                "start": 580,
                                "end": 637
                            },
                            {
                                "start": 638,
                                "end": 816
                            },
                            {
                                "start": 819,
                                "end": 1034
                            },
                            {
                                "start": 1035,
                                "end": 1140
                            },
                            {
                                "start": 1141,
                                "end": 1284
                            },
                            {
                                "start": 1285,
                                "end": 1421
                            },
                            {
                                "start": 1422,
                                "end": 1589
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The calculations and parameter amounts of the network were reduced significantly after adopting the lightweight design, but so was the detection accuracy. To address this problem, knowledge distillation (KD), a joint training method by transferring \"knowledge\", was employed to improve the detection accuracy. The KD structure was shown in Figure 3. KD is the process of imitating the distillation in chemistry, using the softmax function with temperature parameters to \"distill\" the logit output from complex and large networks, so as to generate more information in categories. This part of the in-formation is called \"dark knowledge\". The additional information guides the simple and small network to learn more knowledge, and the two networks are called the teacher network and the student network, respectively.\n\nTo diversify the information distribution output by the teacher network, we used the temperature parameter \u03c4 to get soft prediction output by distilling logits output between the teacher network and student network. The same dataset was used because soft prediction output implied the information of the negative samples. With the help of SoftMax active function, the teacher network's class prediction probability distribution could be regarded as the soft target. Similarly, this method was used to get not only the soft prediction output but also the hard prediction output from the student network. As for the soft prediction output, soft prediction output and soft target were used to calculate loss value by loss function L so f t , which was a part of total loss."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[260447668 | Cui et al. | 2022 | Citations: 3]",
                "snippets": "Compared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part. \n\n3) Loss Function for KD. The proposed KD-based neural network lightweight method is depicted in Fig. 3. After the teacher network is trained, the student network is trained using a combination of distillation loss and ordinary MSE loss.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Key Idea and Training Framework",
                        "pdf_hash": "",
                        "start": 190,
                        "end": 2414,
                        "sentence_offsets": [
                            {
                                "start": 190,
                                "end": 402
                            },
                            {
                                "start": 403,
                                "end": 462
                            },
                            {
                                "start": 463,
                                "end": 524
                            },
                            {
                                "start": 525,
                                "end": 643
                            },
                            {
                                "start": 644,
                                "end": 897
                            },
                            {
                                "start": 898,
                                "end": 1079
                            },
                            {
                                "start": 1080,
                                "end": 1205
                            },
                            {
                                "start": 1208,
                                "end": 1382
                            },
                            {
                                "start": 1383,
                                "end": 1468
                            },
                            {
                                "start": 1469,
                                "end": 1543
                            },
                            {
                                "start": 1546,
                                "end": 1711
                            },
                            {
                                "start": 1712,
                                "end": 1912
                            },
                            {
                                "start": 1913,
                                "end": 2061
                            },
                            {
                                "start": 2062,
                                "end": 2175
                            },
                            {
                                "start": 2178,
                                "end": 2202
                            },
                            {
                                "start": 2203,
                                "end": 2281
                            },
                            {
                                "start": 2282,
                                "end": 2414
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Compared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part. \n\n3) Loss Function for KD. The proposed KD-based neural network lightweight method is depicted in Fig. 3. After the teacher network is trained, the student network is trained using a combination of distillation loss and ordinary MSE loss."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[263789814 | Ding et al. | 2020 | Citations: 4]",
                "snippets": "Hinton et al. [23] first propose KD to transfer dark knowledge from the teacher to the student. The softmax outputs encode richer knowledge than one-hot labels and can provide extra supervisory signals.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 24,
                        "end": 226,
                        "sentence_offsets": [
                            {
                                "start": 24,
                                "end": 119
                            },
                            {
                                "start": 120,
                                "end": 226
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Hinton et al. [23] first propose KD to transfer dark knowledge from the teacher to the student. The softmax outputs encode richer knowledge than one-hot labels and can provide extra supervisory signals."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[264555654 | Jung et al. | 2023 | Citations: 5]",
                "snippets": "The core idea behind Knowledge Distillation (KD) (Hinton et al., 2015) is transferring knowledge of a high-capacity teacher network to a relatively smaller student model. In knowledge distillation, a student network is guided by not only a one-hot encoded ground-truth but also a soft target mapped by the teacher network (probability distribution). This is known to transfer a class-wise relation mapped by a teacher is commonly termed the dark knowledge. Given a data sample from a joint distribution (x, y) \u2208 X \u00d7 Y, a student model is optimized by combining two cross-entropy terms. \n\nwhere |Y | and \u0177k denote the number of classes and a k-th target label (one-hot encoded) respectively. \u03b3, and P denote a balancing parameter, and a probability distribution scaled with a temperature. \u03b8 and \u03d5 are parameters of a student and teacher network respectively.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 857,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 170
                            },
                            {
                                "start": 171,
                                "end": 349
                            },
                            {
                                "start": 350,
                                "end": 456
                            },
                            {
                                "start": 457,
                                "end": 585
                            },
                            {
                                "start": 588,
                                "end": 690
                            },
                            {
                                "start": 691,
                                "end": 787
                            },
                            {
                                "start": 788,
                                "end": 857
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The core idea behind Knowledge Distillation (KD) (Hinton et al., 2015) is transferring knowledge of a high-capacity teacher network to a relatively smaller student model. In knowledge distillation, a student network is guided by not only a one-hot encoded ground-truth but also a soft target mapped by the teacher network (probability distribution). This is known to transfer a class-wise relation mapped by a teacher is commonly termed the dark knowledge. Given a data sample from a joint distribution (x, y) \u2208 X \u00d7 Y, a student model is optimized by combining two cross-entropy terms. \n\nwhere |Y | and \u0177k denote the number of classes and a k-th target label (one-hot encoded) respectively. \u03b3, and P denote a balancing parameter, and a probability distribution scaled with a temperature. \u03b8 and \u03d5 are parameters of a student and teacher network respectively."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[264590688 | Wang et al. | 2023 | Citations: 1]",
                "snippets": "KD [33] provides additional information about semantic similarity to model learning through the use of dark knowledge generated by self-distillation...Compared with pseudo labels, dark knowledge from the teacher contains the similarity information between classes (Yuan et al., 2020). With the incorporation of self-distillation, a weak teacher with much lower accuracy than students can still significantly improve the clustering accuracy of students. The success of selfdistillation even with a weak teacher is not solely due to the shared similarity information between classes, but rather due to the regularization of the distilled knowledge (Yuan et al., 2020).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[219962714 | Yuan et al. | 2020 | Citations: 527]": "Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome teacher model into a lightweight student model. Its success is generally attributed to the privileged information on similarities among categories provided by the teacher model, and in this sense, only strong teacher models are deployed to teach weaker students in practice. In this work, we challenge this common belief by following experimental observations: 1) beyond the acknowledgment that the teacher can improve the student, the student can also enhance the teacher significantly by reversing the KD procedure; 2) a poorly-trained teacher with much lower accuracy than the student can still improve the latter significantly. To explain these observations, we provide a theoretical analysis of the relationships between KD and label smoothing regularization. We prove that 1) KD is a type of learned label smoothing regularization and 2) label smoothing regularization provides a virtual teacher model for KD. From these results, we argue that the success of KD is not fully due to the similarity information between categories from teachers, but also to the regularization of soft targets, which is equally or even more important. Based on these analyses, we further propose a novel Teacher-free Knowledge Distillation (Tf-KD) framework, where a student model learns from itself or manually-designed regularization distribution. The Tf-KD achieves comparable performance with normal KD from a superior teacher, which is well applied when a stronger teacher model is unavailable. Meanwhile, Tf-KD is generic and can be directly deployed for training deep neural networks. Without any extra computation cost, Tf-KD achieves up to 0.65\\% improvement on ImageNet over well-established baseline models, which is superior to label smoothing regularization."
                },
                "metadata": [
                    {
                        "section_title": "B. Contrastive Learning",
                        "pdf_hash": "",
                        "start": 440,
                        "end": 588,
                        "sentence_offsets": [
                            {
                                "start": 57,
                                "end": 706
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "KD [33] provides additional information about semantic similarity to model learning through the use of dark knowledge generated by self-distillation"
                    },
                    {
                        "section_title": "B. Contrastive Learning",
                        "pdf_hash": "",
                        "start": 938,
                        "end": 1424,
                        "sentence_offsets": [
                            {
                                "start": 938,
                                "end": 1056
                            },
                            {
                                "start": 1057,
                                "end": 1224
                            },
                            {
                                "start": 1225,
                                "end": 1423
                            }
                        ],
                        "ref_mentions": [
                            "219962714",
                            "219962714"
                        ],
                        "quote": "Compared with pseudo labels, dark knowledge from the teacher contains the similarity information between classes (Yuan et al., 2020). With the incorporation of self-distillation, a weak teacher with much lower accuracy than students can still significantly improve the clustering accuracy of students. The success of selfdistillation even with a weak teacher is not solely due to the shared similarity information between classes, but rather due to the regularization of the distilled knowledge (Yuan et al., 2020)."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[265384964 | Xie et al. | 2023 | Citations: 6]",
                "snippets": "Knowledge distillation (Bucila et al., 2006) involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements. In the process of knowledge distillation, logits are used as a basis for comparing the outputs of the student model with those of the teacher model. By examining logits, the model can measure the certainty or confidence of its predictions before applying the softmax function to obtain the final probability. The standard cross-entropy loss depends on the predicted probabilities and ground truth labels. Additionally, the loss function is extended to include the standard cross-entropy loss between the predictions of the student model and the ground truth labels, as well as an additional loss term that measures the discrepancy between the softened probabilities (obtained through a higher temperature softmax) of the teacher model's predictions and the corresponding predictions of the student model. This additional term ensures that the student model not only learns to predict the correct labels but also aims to replicate the softened outputs of the teacher model, effectively transferring its knowledge to the student model. By jointly minimizing these two loss terms, the student model can learn to generalize better and imitate the behavior of the more complex teacher model. The calculation of the probability for the class is as follows: \n\nHere, T represents the \"temperature\" of knowledge distillation. When T = 1, it corresponds to a normalized exponential function. With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\"",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2463,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 179
                            },
                            {
                                "start": 180,
                                "end": 327
                            },
                            {
                                "start": 328,
                                "end": 510
                            },
                            {
                                "start": 511,
                                "end": 659
                            },
                            {
                                "start": 660,
                                "end": 819
                            },
                            {
                                "start": 820,
                                "end": 915
                            },
                            {
                                "start": 916,
                                "end": 1315
                            },
                            {
                                "start": 1316,
                                "end": 1544
                            },
                            {
                                "start": 1545,
                                "end": 1697
                            },
                            {
                                "start": 1698,
                                "end": 1761
                            },
                            {
                                "start": 1764,
                                "end": 1827
                            },
                            {
                                "start": 1828,
                                "end": 1892
                            },
                            {
                                "start": 1893,
                                "end": 2131
                            },
                            {
                                "start": 2132,
                                "end": 2255
                            },
                            {
                                "start": 2256,
                                "end": 2403
                            },
                            {
                                "start": 2404,
                                "end": 2463
                            }
                        ],
                        "ref_mentions": [
                            "11253972"
                        ],
                        "quote": "Knowledge distillation (Bucila et al., 2006) involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements. In the process of knowledge distillation, logits are used as a basis for comparing the outputs of the student model with those of the teacher model. By examining logits, the model can measure the certainty or confidence of its predictions before applying the softmax function to obtain the final probability. The standard cross-entropy loss depends on the predicted probabilities and ground truth labels. Additionally, the loss function is extended to include the standard cross-entropy loss between the predictions of the student model and the ground truth labels, as well as an additional loss term that measures the discrepancy between the softened probabilities (obtained through a higher temperature softmax) of the teacher model's predictions and the corresponding predictions of the student model. This additional term ensures that the student model not only learns to predict the correct labels but also aims to replicate the softened outputs of the teacher model, effectively transferring its knowledge to the student model. By jointly minimizing these two loss terms, the student model can learn to generalize better and imitate the behavior of the more complex teacher model. The calculation of the probability for the class is as follows: \n\nHere, T represents the \"temperature\" of knowledge distillation. When T = 1, it corresponds to a normalized exponential function. With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\""
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[265444951 | Su et al. | 2023 | Citations: 2]",
                "snippets": "The goal of KD [8] is to transfer the dark knowledge encapsulated in the soft probability output of the teacher model to the student model. In classification tasks, the softened probabilities are computed via the temperature-scaled softmax function, given by \n\nwhere p i (T ) is the probability output for class i softened by the temperature hyperparameter T , y i represents the logit for class i, and C is the total number of classes. Typically, T is set to greater than 1 in KD. The higher value of T produces softer probabilities, which are crucial for unveiling the dark knowledge hidden in the inter-class relationships captured by the teacher. The core idea of KD lies in minimizing the KL divergence loss function to align the soft logits of the teacher and student. The loss of KD L KD is defined as \n\nwhere p T and p S are the output logits of the teacher T and the student S, respectively.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Preliminaries",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 900,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 139
                            },
                            {
                                "start": 140,
                                "end": 258
                            },
                            {
                                "start": 261,
                                "end": 436
                            },
                            {
                                "start": 437,
                                "end": 481
                            },
                            {
                                "start": 482,
                                "end": 650
                            },
                            {
                                "start": 651,
                                "end": 774
                            },
                            {
                                "start": 775,
                                "end": 808
                            },
                            {
                                "start": 811,
                                "end": 900
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The goal of KD [8] is to transfer the dark knowledge encapsulated in the soft probability output of the teacher model to the student model. In classification tasks, the softened probabilities are computed via the temperature-scaled softmax function, given by \n\nwhere p i (T ) is the probability output for class i softened by the temperature hyperparameter T , y i represents the logit for class i, and C is the total number of classes. Typically, T is set to greater than 1 in KD. The higher value of T produces softer probabilities, which are crucial for unveiling the dark knowledge hidden in the inter-class relationships captured by the teacher. The core idea of KD lies in minimizing the KL divergence loss function to align the soft logits of the teacher and student. The loss of KD L KD is defined as \n\nwhere p T and p S are the output logits of the teacher T and the student S, respectively."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[265502078 | Witt et al. | 2024 | Citations: 4]",
                "snippets": "Knowledge Distillation (KD), depicted in Fig. 1, is a technique in deep learning where a smaller NN model (often called the \"student\") is trained to mimic the behavior of a larger, pre-trained model (referred to as the \"teacher\") [27]. This is accomplished not by transferring the model parameters directly, but rather by aligning the output distributions of both models. Traditional training methods involve training a model directly on ground-truth labels, using a cross-entropy loss that measures the discrepancy between the model's predictions and these true labels. In contrast, KD employs a divergence-based loss, such as the Kullback-Leibler (KL) divergence, to measure the difference between the student's predicted probabilities and those of the teacher model. This divergence provides insights into how closely the student is able to mimic the behavior of its teacher. A distinct feature of KD is the use of \"softened\" labels. In traditional classification tasks, hard labels are used, which unequivocally classify a data point into one category. However, the teacher model in KD provides \"soft\" labels in the form of probabilities, indicating the confidence levels across various categories. These probabilities can be further softened using a temperature parameter T to yield a smoother distribution, capturing the nuances of decision boundaries and offering richer guidance to the student model. This process allows the student to inherit not just the overt knowledge from the ground-truth labels but also the implicit, or \"dark\", knowledge embedded in the teacher model's predictions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Knowledge Distillation and Federated Settings",
                        "pdf_hash": "",
                        "start": 27,
                        "end": 1625,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 262
                            },
                            {
                                "start": 263,
                                "end": 398
                            },
                            {
                                "start": 399,
                                "end": 597
                            },
                            {
                                "start": 598,
                                "end": 796
                            },
                            {
                                "start": 797,
                                "end": 905
                            },
                            {
                                "start": 906,
                                "end": 963
                            },
                            {
                                "start": 964,
                                "end": 1083
                            },
                            {
                                "start": 1084,
                                "end": 1229
                            },
                            {
                                "start": 1230,
                                "end": 1435
                            },
                            {
                                "start": 1436,
                                "end": 1625
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge Distillation (KD), depicted in Fig. 1, is a technique in deep learning where a smaller NN model (often called the \"student\") is trained to mimic the behavior of a larger, pre-trained model (referred to as the \"teacher\") [27]. This is accomplished not by transferring the model parameters directly, but rather by aligning the output distributions of both models. Traditional training methods involve training a model directly on ground-truth labels, using a cross-entropy loss that measures the discrepancy between the model's predictions and these true labels. In contrast, KD employs a divergence-based loss, such as the Kullback-Leibler (KL) divergence, to measure the difference between the student's predicted probabilities and those of the teacher model. This divergence provides insights into how closely the student is able to mimic the behavior of its teacher. A distinct feature of KD is the use of \"softened\" labels. In traditional classification tasks, hard labels are used, which unequivocally classify a data point into one category. However, the teacher model in KD provides \"soft\" labels in the form of probabilities, indicating the confidence levels across various categories. These probabilities can be further softened using a temperature parameter T to yield a smoother distribution, capturing the nuances of decision boundaries and offering richer guidance to the student model. This process allows the student to inherit not just the overt knowledge from the ground-truth labels but also the implicit, or \"dark\", knowledge embedded in the teacher model's predictions."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[266555759 | Gupta et al. | 2023 | Citations: 4]",
                "snippets": "The original knowledge distillation work primarily focuses on single-label classification. In this context, a custom loss function L is designed that considers both the hard labels from ground truth and soft labels from a teacher model. Importantly, the soft labels are obtained using a T-temperature softmax activation function. \n\nThe softmax activation function returns probabilities P (z i , T ) is defined as: \n\nTherefore, the complete loss L can be represented as: \n\nL = \u03b1L soft + \u03b2L hard (Hashemi et al., 2018) where \u03b1 and \u03b2 are hyperparameters, t i are the logits from the teacher model, s i are the logits from the student model, c i is the ground truth for the i-th instance, N is the total number of instances, and T is the temperature parameter for softmax.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[3759373 | Hashemi et al. | 2018 | Citations: 205]": "The explosion in workload complexity and the recent slow-down in Moore's law scaling call for new approaches towards efficient computing. Researchers are now beginning to use recent advances in machine learning in software optimizations, augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efficient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the first step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research."
                },
                "metadata": [
                    {
                        "section_title": "C. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 940,
                        "end": 1689,
                        "sentence_offsets": [
                            {
                                "start": 940,
                                "end": 1030
                            },
                            {
                                "start": 1031,
                                "end": 1176
                            },
                            {
                                "start": 1177,
                                "end": 1269
                            },
                            {
                                "start": 1272,
                                "end": 1353
                            },
                            {
                                "start": 1356,
                                "end": 1409
                            },
                            {
                                "start": 1412,
                                "end": 1689
                            }
                        ],
                        "ref_mentions": [
                            "3759373"
                        ],
                        "quote": "The original knowledge distillation work primarily focuses on single-label classification. In this context, a custom loss function L is designed that considers both the hard labels from ground truth and soft labels from a teacher model. Importantly, the soft labels are obtained using a T-temperature softmax activation function. \n\nThe softmax activation function returns probabilities P (z i , T ) is defined as: \n\nTherefore, the complete loss L can be represented as: \n\nL = \u03b1L soft + \u03b2L hard (Hashemi et al., 2018) where \u03b1 and \u03b2 are hyperparameters, t i are the logits from the teacher model, s i are the logits from the student model, c i is the ground truth for the i-th instance, N is the total number of instances, and T is the temperature parameter for softmax."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[267657497 | Kim et al. | 2023 | Citations: 4]",
                "snippets": "Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 175,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 175
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[268063805 | Yang et al. | 2024 | Citations: 1]",
                "snippets": "Logit-based knowledge typically refers to the output of the softmax layer of the teacher model. Early, or \"vanilla,\" knowledge distillation methods used the logits of a deep, complex model as \"dark knowledge\" [21]. Logit-based knowledge distillation is a simple but effective approach for achieving good performance in compression tasks and has gained popularity in solving various tasks and applications. The most popular logit-based knowledge for classification tasks is the soft target [21], (Ba et al., 2013). In this approach, the \"dark knowledge\" is usually evaluated using the Kullback-Leibler divergence loss, which minimizes the difference between the student and teacher logits. The vanilla approach to KD involves joint training of the distillation and student losses. The student loss is typically set as the cross-entropy loss between the ground truth and the softmax class prediction of the student model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[11536917 | Ba et al. | 2013 | Citations: 2119]": "Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models."
                },
                "metadata": [
                    {
                        "section_title": "C. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 697,
                        "end": 1603,
                        "sentence_offsets": [
                            {
                                "start": 697,
                                "end": 792
                            },
                            {
                                "start": 793,
                                "end": 911
                            },
                            {
                                "start": 912,
                                "end": 1102
                            },
                            {
                                "start": 1103,
                                "end": 1197
                            },
                            {
                                "start": 1198,
                                "end": 1372
                            },
                            {
                                "start": 1373,
                                "end": 1463
                            },
                            {
                                "start": 1464,
                                "end": 1603
                            }
                        ],
                        "ref_mentions": [
                            "11536917"
                        ],
                        "quote": "Logit-based knowledge typically refers to the output of the softmax layer of the teacher model. Early, or \"vanilla,\" knowledge distillation methods used the logits of a deep, complex model as \"dark knowledge\" [21]. Logit-based knowledge distillation is a simple but effective approach for achieving good performance in compression tasks and has gained popularity in solving various tasks and applications. The most popular logit-based knowledge for classification tasks is the soft target [21], (Ba et al., 2013). In this approach, the \"dark knowledge\" is usually evaluated using the Kullback-Leibler divergence loss, which minimizes the difference between the student and teacher logits. The vanilla approach to KD involves joint training of the distillation and student losses. The student loss is typically set as the cross-entropy loss between the ground truth and the softmax class prediction of the student model."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[269317596 | Guo et al. | 2024 | Citations: 3]",
                "snippets": "Firstly, we will recap the Knowledge Distillation method proposed by Hinton et al. [11]. This widely used method involves using category similarity as a guide for student networks. To regularize the network's learning, temperature is introduced to soften the initial categorical information, also referred to as 'dark knowledge'. The output probability of the teacher network and student network can be calculated as Equations ( 1) and (2). \n\nwhere T represents temperature, which can adjust the softening of the output probability. z i z j are the logits input for softmax, p i represents the output probability of each category in the teacher network, and q i represents the output probability of each category in the student network. Cross-entropy between the distilled teacher and student models calculates the soft loss. The hard loss is calculated based on the predicted values of the student model and the true values. \n\nThe ground truth label (also known as the hard target label) for the i-th sample is represented by y i . The teacher and student models' predicted probability are represented by p i and q i , respectively. The total loss function is calculated as below. \n\nThe loss function used in Knowledge Distillation involves two types of losses: soft target loss L soft and hard target loss L hard . The former guides the student to replicate the teacher's probability distribution, whereas the latter reflects the guidance from the actual ground truth labels. The parameter \u03b1 balances the effect of these two losses. During the Knowledge Distillation process, the student receives both the challenging and soft target knowledge. The loss function can be written as follows: \n\nwhere the loss of the cross-entropy is represented by L CE . The softmax function is represented by \u03c3. y represents the ground truth label. The output logits of the student and teacher networks are denoted by z S and z T . The balancing hyperparameter is represented by \u03b1.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1966,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 88
                            },
                            {
                                "start": 89,
                                "end": 180
                            },
                            {
                                "start": 181,
                                "end": 329
                            },
                            {
                                "start": 330,
                                "end": 440
                            },
                            {
                                "start": 443,
                                "end": 532
                            },
                            {
                                "start": 533,
                                "end": 736
                            },
                            {
                                "start": 737,
                                "end": 825
                            },
                            {
                                "start": 826,
                                "end": 925
                            },
                            {
                                "start": 928,
                                "end": 1032
                            },
                            {
                                "start": 1033,
                                "end": 1133
                            },
                            {
                                "start": 1134,
                                "end": 1181
                            },
                            {
                                "start": 1184,
                                "end": 1316
                            },
                            {
                                "start": 1317,
                                "end": 1477
                            },
                            {
                                "start": 1478,
                                "end": 1534
                            },
                            {
                                "start": 1535,
                                "end": 1646
                            },
                            {
                                "start": 1647,
                                "end": 1691
                            },
                            {
                                "start": 1694,
                                "end": 1754
                            },
                            {
                                "start": 1755,
                                "end": 1833
                            },
                            {
                                "start": 1834,
                                "end": 1916
                            },
                            {
                                "start": 1917,
                                "end": 1966
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Firstly, we will recap the Knowledge Distillation method proposed by Hinton et al. [11]. This widely used method involves using category similarity as a guide for student networks. To regularize the network's learning, temperature is introduced to soften the initial categorical information, also referred to as 'dark knowledge'. The output probability of the teacher network and student network can be calculated as Equations ( 1) and (2). \n\nwhere T represents temperature, which can adjust the softening of the output probability. z i z j are the logits input for softmax, p i represents the output probability of each category in the teacher network, and q i represents the output probability of each category in the student network. Cross-entropy between the distilled teacher and student models calculates the soft loss. The hard loss is calculated based on the predicted values of the student model and the true values. \n\nThe ground truth label (also known as the hard target label) for the i-th sample is represented by y i . The teacher and student models' predicted probability are represented by p i and q i , respectively. The total loss function is calculated as below. \n\nThe loss function used in Knowledge Distillation involves two types of losses: soft target loss L soft and hard target loss L hard . The former guides the student to replicate the teacher's probability distribution, whereas the latter reflects the guidance from the actual ground truth labels. The parameter \u03b1 balances the effect of these two losses. During the Knowledge Distillation process, the student receives both the challenging and soft target knowledge. The loss function can be written as follows: \n\nwhere the loss of the cross-entropy is represented by L CE . The softmax function is represented by \u03c3. y represents the ground truth label. The output logits of the student and teacher networks are denoted by z S and z T . The balancing hyperparameter is represented by \u03b1."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[269921267 | Mei et al. | 2024 | Citations: 20]",
                "snippets": "The knowledge distillation technique adopts a typical teacher-student training framework, as shown in Figure 2. In this framework, the \"teacher\" model represents a highly complex deep neural network.In contrast, the Student model is designed to be more streamlined and aims to achieve a similar level of performance with fewer parameters.The process begins with the pre-training of the Teacher model, a step that allows it to accumulate broad and deep knowledge on the specified data set.Instead of directly replicating the weights of the \"teacher\" model, the subsequent knowledge transfer phase uses its output -particularly soft probability distributions (rather than hard classification labels).This process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The knowledge distillation technique adopts a typical teacher-student training framework, as shown in Figure 2. In this framework, the \"teacher\" model represents a highly complex deep neural network.In contrast, the Student model is designed to be more streamlined and aims to achieve a similar level of performance with fewer parameters.The process begins with the pre-training of the Teacher model, a step that allows it to accumulate broad and deep knowledge on the specified data set.Instead of directly replicating the weights of the \"teacher\" model, the subsequent knowledge transfer phase uses its output -particularly soft probability distributions (rather than hard classification labels).This process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 40,
                "key": "[270077283 | Zhao et al. | 2024 | Citations: 3]",
                "snippets": "Hinton et al. [12] first proposed to use the soft outputs of the pretrained teacher network as dark knowledge to supervise the training of the student network. They introduced a temperature hyperparameter T and formulated the problem as \"knowledge distillation\". The student network is forced to learn the soft targets of the teacher network, which are obtained through using a high temperature T on the softmax inputs. In the process of knowledge transfer, soft targets often contain richer information than one-hot targets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 804,
                        "end": 1329,
                        "sentence_offsets": [
                            {
                                "start": 709,
                                "end": 975
                            },
                            {
                                "start": 975,
                                "end": 1134
                            },
                            {
                                "start": 1134,
                                "end": 1236
                            },
                            {
                                "start": 1236,
                                "end": 1392
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Hinton et al. [12] first proposed to use the soft outputs of the pretrained teacher network as dark knowledge to supervise the training of the student network. They introduced a temperature hyperparameter T and formulated the problem as \"knowledge distillation\". The student network is forced to learn the soft targets of the teacher network, which are obtained through using a high temperature T on the softmax inputs. In the process of knowledge transfer, soft targets often contain richer information than one-hot targets."
                    }
                ]
            },
            {
                "idx": 41,
                "key": "[270389751 | Li et al. | 2024 | Citations: 1]",
                "snippets": "Knowledge distillation (KD), as initially proposed by Hinton et al. 10 , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model. This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge. This approach has given rise to variations, including intermediate feature layer distillation and multistage soft label distillation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 491,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 194
                            },
                            {
                                "start": 194,
                                "end": 356
                            },
                            {
                                "start": 356,
                                "end": 489
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Knowledge distillation (KD), as initially proposed by Hinton et al. 10 , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model. This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge. This approach has given rise to variations, including intermediate feature layer distillation and multistage soft label distillation."
                    }
                ]
            },
            {
                "idx": 42,
                "key": "[271244914 | Giakoumoglou et al. | 2024 | Citations: 0]",
                "snippets": "The concept of Knowledge Distillation (KD) was first introduced by Hinton et al. [20]. It involves extracting \"dark knowledge\" from accurate teacher models to guide the learning process of student models. This is achieved by utilizing the Kullback-Leibler (KL) loss to regularize the output probabilities of student models, aligning them with those of their teacher models when given the same inputs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 400,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 86
                            },
                            {
                                "start": 87,
                                "end": 204
                            },
                            {
                                "start": 205,
                                "end": 400
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The concept of Knowledge Distillation (KD) was first introduced by Hinton et al. [20]. It involves extracting \"dark knowledge\" from accurate teacher models to guide the learning process of student models. This is achieved by utilizing the Kullback-Leibler (KL) loss to regularize the output probabilities of student models, aligning them with those of their teacher models when given the same inputs."
                    }
                ]
            },
            {
                "idx": 43,
                "key": "[271356843 | Yang et al. | 2025 | Citations: 0]",
                "snippets": "In knowledge distillation, the student network is trained under the guidance and supervision of the teacher network, which draws on the a priori knowledge and generalization ability of the pre-trained teacher network, and helps to improve the learning efficiency and effectiveness.\n\nOffline distillation follows the classical ''teacher-student'' paradigm, so that the student network, in the process of imitating the teacher network, not only learns the decision-making behavior of the teacher network on the training samples, but also passes on the deep patterns and abstract knowledge contained in the teacher network, thus realizing the effective transfer and compression of knowledge, and the degree of fit of the student network to the output of the teacher network is quantified by the loss function. The degree of fitting of the teacher network output results by the loss function can be expressed as follows: \n\nwhere L task denotes the learning performance of the student network on the original task, Ls is the output vector of the student network and Lt is the output vector of the teacher network, which L KD is used to quantify the difference between the student network and the teacher network at the output level. In order to integrate the loss of the student network's own learning task with the loss of imitating the teacher network's output, a balance coefficient is introduced, which \u03bb is used to regulate the relative weights of the two in the total loss function.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In knowledge distillation, the student network is trained under the guidance and supervision of the teacher network, which draws on the a priori knowledge and generalization ability of the pre-trained teacher network, and helps to improve the learning efficiency and effectiveness.\n\nOffline distillation follows the classical ''teacher-student'' paradigm, so that the student network, in the process of imitating the teacher network, not only learns the decision-making behavior of the teacher network on the training samples, but also passes on the deep patterns and abstract knowledge contained in the teacher network, thus realizing the effective transfer and compression of knowledge, and the degree of fit of the student network to the output of the teacher network is quantified by the loss function. The degree of fitting of the teacher network output results by the loss function can be expressed as follows: \n\nwhere L task denotes the learning performance of the student network on the original task, Ls is the output vector of the student network and Lt is the output vector of the teacher network, which L KD is used to quantify the difference between the student network and the teacher network at the output level. In order to integrate the loss of the student network's own learning task with the loss of imitating the teacher network's output, a balance coefficient is introduced, which \u03bb is used to regulate the relative weights of the two in the total loss function.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 44,
                "key": "[272969060 | Huang et al. | 2024 | Citations: 0]",
                "snippets": "Logits-based KD was first proposed in [9], where the student learns the logits knowledge from the teacher through KL divergence. In contrast to direct supervised learning through ground truth, logits-based KD using soft labels can elicit more 'dark knowledge' [9], thereby enhancing the performance of the student network without altering its architecture. The logits distillation losses are represented as follows: \n\nP j denotes the class probability derived from the logits z \u2208 R C after undergoing Softmax. \u03c4 is the temperature scaling hyper-parameter which enables the production of different probability distributions: \n\nFeature-based KD transfers the knowledge of intermediate layers features from the teacher to the student. Generally, we can formulate such distillation methods as: \n\nwhere F t i and F s i are the corresponding features of the teacher and student at i-th layer. w(\u2022) is a mapping function that aligns the dimensions of features from the student to the teacher.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Preliminaries",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 985,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 128
                            },
                            {
                                "start": 129,
                                "end": 356
                            },
                            {
                                "start": 357,
                                "end": 415
                            },
                            {
                                "start": 418,
                                "end": 509
                            },
                            {
                                "start": 510,
                                "end": 623
                            },
                            {
                                "start": 626,
                                "end": 731
                            },
                            {
                                "start": 732,
                                "end": 789
                            },
                            {
                                "start": 792,
                                "end": 886
                            },
                            {
                                "start": 887,
                                "end": 985
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Logits-based KD was first proposed in [9], where the student learns the logits knowledge from the teacher through KL divergence. In contrast to direct supervised learning through ground truth, logits-based KD using soft labels can elicit more 'dark knowledge' [9], thereby enhancing the performance of the student network without altering its architecture. The logits distillation losses are represented as follows: \n\nP j denotes the class probability derived from the logits z \u2208 R C after undergoing Softmax. \u03c4 is the temperature scaling hyper-parameter which enables the production of different probability distributions: \n\nFeature-based KD transfers the knowledge of intermediate layers features from the teacher to the student. Generally, we can formulate such distillation methods as: \n\nwhere F t i and F s i are the corresponding features of the teacher and student at i-th layer. w(\u2022) is a mapping function that aligns the dimensions of features from the student to the teacher."
                    }
                ]
            },
            {
                "idx": 45,
                "key": "[273811396 | Yu et al. | 2024 | Citations: 2]",
                "snippets": "The working mechanism of vanilla KD is often comprehended via the process of \"teaching\" (Wang et al., 2020), whereby the transfer of \"dark knowledge\" occurs through the use of soft labels, known as logits, provided by the teacher to the student.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215745611 | Wang et al. | 2020 | Citations: 700]": "Deep neural models, in recent years, have been successful in almost every field, even solving the most complex problem statements. However, these models are huge in size with millions (and even billions) of parameters, demanding heavy computation power and failing to be deployed on edge devices. Besides, the performance boost is highly dependent on redundant labeled data. To achieve faster speeds and to handle the problems caused by the lack of labeled data, knowledge distillation (KD) has been proposed to transfer information learned from one model to another. KD is often characterized by the so-called \u2018Student-Teacher\u2019 (S-T) learning framework and has been broadly applied in model compression and knowledge transfer. This paper is about KD and S-T learning, which are being actively studied in recent years. First, we aim to provide explanations of what KD is and how/why it works. Then, we provide a comprehensive survey on the recent progress of KD methods together with S-T frameworks typically used for vision tasks. In general, we investigate some fundamental questions that have been driving this research area and thoroughly generalize the research progress and technical details. Additionally, we systematically analyze the research status of KD in vision applications. Finally, we discuss the potentials and open challenges of existing methods and prospect the future directions of KD and S-T learning."
                },
                "metadata": [
                    {
                        "section_title": "II. RELATED WORK",
                        "pdf_hash": "",
                        "start": 209,
                        "end": 439,
                        "sentence_offsets": [
                            {
                                "start": 209,
                                "end": 439
                            }
                        ],
                        "ref_mentions": [
                            "215745611"
                        ],
                        "quote": "The working mechanism of vanilla KD is often comprehended via the process of \"teaching\" (Wang et al., 2020), whereby the transfer of \"dark knowledge\" occurs through the use of soft labels, known as logits, provided by the teacher to the student."
                    }
                ]
            },
            {
                "idx": 46,
                "key": "[274436184 | Li et al. | 2024 | Citations: 0]",
                "snippets": "Learning node-specific temperatures addresses the limitations inherent in the temperature mechanism used in traditional distillation methods. In conventional knowledge distillation, a predefined temperature parameter is used to generate soft targets. The teacher model's informative dark knowledge is embedded in the soft targets, which infer the probability that a node belongs to a given class. The softmax function with a fixed temperature is defined as follows: \n\nwhere the temperature parameter \u03c4 is used to control the softness of each prediction. The distribution of predictions becomes smoother as the temperature rises, and sharper as the temperature falls. Thus, the temperature is responsible for regulating the balance between the true label knowledge and the dark knowledge. However, using a fixed temperature treats the softening of all teacher logits equally, ignoring variations in node representations. This uniform approach fails to provide precise guidance for individual nodes, which can hinder knowledge transfer. Moreover, existing research has shown that temperature scaling can improve model performance (Z. (Zhang et al., 2020). \n\nTo confront these issues, we propose an entropy-based approach to learn node-specific temperatures. This approach assigns a unique temperature \u03c4i to each node, controlling the degree of softening individually. The softmax function with node-specific temperatures is defined as: \n\nwhere the temperature parameter \u03c4i is the specific temperature of the i-th node. Obviously, \u03c4i determines the softening direction of each node. The temperature of the node is related to the teacher's confidence for each respective node (Zhichun Guo et al., 2023). Therefore, we can judge the teacher's confidence for each node through the entropy of the teacher's logits. The lower the entropy value, the higher the teacher's confidence for this node. Specifically, we calculate the node-specific temperature through the probability distribution of the teacher and the confidence of the teacher, defined as follows:",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[219558831 | Zhang et al. | 2020 | Citations: 119]": "It has been recently demonstrated that multi-generational self-distillation can improve generalization. Despite this intriguing observation, reasons for the enhancement remain poorly understood. In this paper, we first demonstrate experimentally that the improved performance of multi-generational self-distillation is in part associated with the increasing diversity in teacher predictions. With this in mind, we offer a new interpretation for teacher-student training as amortized MAP estimation, such that teacher predictions enable instance-specific regularization. Our framework allows us to theoretically relate self-distillation to label smoothing, a commonly used technique that regularizes predictive uncertainty, and suggests the importance of predictive diversity in addition to predictive uncertainty. We present experimental results using multiple datasets and neural network architectures that, overall, demonstrate the utility of predictive diversity. Finally, we propose a novel instance-specific label smoothing technique that promotes predictive diversity without the need for a separately trained teacher model. We provide an empirical evaluation of the proposed method, which, we find, often outperforms classical label smoothing."
                },
                "metadata": [
                    {
                        "section_title": "Learning node-specific temperatures",
                        "pdf_hash": "",
                        "start": 418,
                        "end": 2473,
                        "sentence_offsets": [
                            {
                                "start": 418,
                                "end": 559
                            },
                            {
                                "start": 560,
                                "end": 668
                            },
                            {
                                "start": 669,
                                "end": 814
                            },
                            {
                                "start": 815,
                                "end": 883
                            },
                            {
                                "start": 886,
                                "end": 971
                            },
                            {
                                "start": 972,
                                "end": 1084
                            },
                            {
                                "start": 1085,
                                "end": 1205
                            },
                            {
                                "start": 1206,
                                "end": 1337
                            },
                            {
                                "start": 1338,
                                "end": 1452
                            },
                            {
                                "start": 1453,
                                "end": 1549
                            },
                            {
                                "start": 1550,
                                "end": 1575
                            },
                            {
                                "start": 1578,
                                "end": 1677
                            },
                            {
                                "start": 1678,
                                "end": 1787
                            },
                            {
                                "start": 1788,
                                "end": 1855
                            },
                            {
                                "start": 1858,
                                "end": 1938
                            },
                            {
                                "start": 1939,
                                "end": 2001
                            },
                            {
                                "start": 2002,
                                "end": 2121
                            },
                            {
                                "start": 2122,
                                "end": 2229
                            },
                            {
                                "start": 2230,
                                "end": 2309
                            },
                            {
                                "start": 2310,
                                "end": 2473
                            }
                        ],
                        "ref_mentions": [
                            "219558831"
                        ],
                        "quote": "Learning node-specific temperatures addresses the limitations inherent in the temperature mechanism used in traditional distillation methods. In conventional knowledge distillation, a predefined temperature parameter is used to generate soft targets. The teacher model's informative dark knowledge is embedded in the soft targets, which infer the probability that a node belongs to a given class. The softmax function with a fixed temperature is defined as follows: \n\nwhere the temperature parameter \u03c4 is used to control the softness of each prediction. The distribution of predictions becomes smoother as the temperature rises, and sharper as the temperature falls. Thus, the temperature is responsible for regulating the balance between the true label knowledge and the dark knowledge. However, using a fixed temperature treats the softening of all teacher logits equally, ignoring variations in node representations. This uniform approach fails to provide precise guidance for individual nodes, which can hinder knowledge transfer. Moreover, existing research has shown that temperature scaling can improve model performance (Z. (Zhang et al., 2020). \n\nTo confront these issues, we propose an entropy-based approach to learn node-specific temperatures. This approach assigns a unique temperature \u03c4i to each node, controlling the degree of softening individually. The softmax function with node-specific temperatures is defined as: \n\nwhere the temperature parameter \u03c4i is the specific temperature of the i-th node. Obviously, \u03c4i determines the softening direction of each node. The temperature of the node is related to the teacher's confidence for each respective node (Zhichun Guo et al., 2023). Therefore, we can judge the teacher's confidence for each node through the entropy of the teacher's logits. The lower the entropy value, the higher the teacher's confidence for this node. Specifically, we calculate the node-specific temperature through the probability distribution of the teacher and the confidence of the teacher, defined as follows:"
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.045744,
        "cot": "To answer the query about technical implementation details of transferring 'dark knowledge' in knowledge distillation from teacher to student networks, I'll organize the information from the quotes into meaningful dimensions that cover the key technical aspects.\n\nFirst, I need to establish an \"Introduction/Background\" section to provide foundational understanding of what knowledge distillation and dark knowledge are.\n\nThen I'll focus on the technical implementation details requested in the query:\n1. Temperature scaling - This is a key technical mechanism in knowledge distillation that controls the \"softness\" of probability distributions.\n2. Soft label generation - How soft targets are created from teacher networks.\n3. Loss function formulations - The specific mathematical formulations used in knowledge distillation.\n4. Additional implementation details - Other technical aspects beyond the three main areas specified.\n\nFor each dimension, I need to decide whether a synthesis or list format is more appropriate:\n\n1. Introduction/Background - This should be a synthesis to provide a coherent overview of knowledge distillation and dark knowledge concepts.\n\n2. Temperature Scaling Implementation - This should be a synthesis to explain the technical implementation, mathematical formulation, and purpose of temperature scaling in knowledge distillation.\n\n3. Soft Label Generation Techniques - This should be a synthesis as the generation of soft labels is a coherent process that builds on temperature scaling.\n\n4. Loss Function Formulations - This should be a synthesis to explain the various loss functions used in knowledge distillation, their components and purposes.\n\n5. Additional Technical Implementation Details - This should be a list format as it will cover various distinct techniques that complement the core methods.\n\nThis organization allows for a comprehensive and structured answer that directly addresses the technical aspects requested in the query.",
        "plan": {
            "Introduction/Background to Knowledge Distillation and Dark Knowledge (synthesis)": [
                0,
                3,
                8,
                15,
                34,
                36,
                39,
                42
            ],
            "Temperature Scaling Implementation (synthesis)": [
                1,
                5,
                6,
                12,
                13,
                15,
                16,
                17,
                18,
                19,
                23,
                25,
                27,
                28,
                32,
                33,
                35,
                38,
                40,
                46
            ],
            "Soft Label Generation Techniques (synthesis)": [
                0,
                1,
                2,
                4,
                8,
                11,
                15,
                17,
                20,
                21,
                26,
                29,
                31,
                37,
                40,
                45
            ],
            "Loss Function Formulations (synthesis)": [
                1,
                2,
                4,
                5,
                6,
                9,
                10,
                13,
                14,
                17,
                19,
                20,
                23,
                24,
                25,
                26,
                30,
                32,
                33,
                35,
                37,
                38,
                43,
                44
            ],
            "Additional Technical Implementation Details (list)": [
                0,
                1,
                4,
                7,
                9,
                10,
                14,
                17,
                22,
                26,
                28,
                41,
                44
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction/Background to Knowledge Distillation and Dark Knowledge",
                "tldr": "Knowledge distillation is a technique that transfers knowledge from a complex teacher model to a simpler student model by using the teacher's output probability distributions. The key insight is the transfer of \"dark knowledge\" - the information embedded in the teacher's confidence scores for incorrect classes - which helps the student model achieve better generalization. (9 sources)",
                "text": "\nKnowledge distillation (KD) is a technique in deep learning where a smaller, more efficient neural network (the \"student\") is trained to mimic the behavior of a larger, pre-trained model (the \"teacher\") <Paper corpusId=\"265502078\" paperTitle=\"(Witt et al., 2024)\" isShortName></Paper>. Rather than directly transferring model parameters, KD aligns the output distributions of both models, allowing the student to inherit the teacher's generalization capabilities <Paper corpusId=\"269921267\" paperTitle=\"(Mei et al., 2024)\" isShortName></Paper>. This approach was first introduced by Hinton et al., who demonstrated that ensembles of models could be compressed into a single, more deployable model while maintaining performance <Paper corpusId=\"227013462\" paperTitle=\"(Nayak et al., 2020)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.\n\nThe core concept in knowledge distillation is the transfer of \"dark knowledge\" - the latent information hidden in the confidences assigned by the teacher to incorrect categories <Paper corpusId=\"159041346\" paperTitle=\"(Nayak et al., 2019)\" isShortName></Paper>. Unlike traditional training that uses one-hot encoded labels, KD leverages the teacher's soft probability distributions across all classes <Paper corpusId=\"211096976\" paperTitle=\"(Mobahi et al., 2020)\" isShortName></Paper>. These soft labels capture nuances in decision boundaries and provide richer guidance to the student model than hard ground-truth labels alone <Paper corpusId=\"265502078\" paperTitle=\"(Witt et al., 2024)\" isShortName></Paper>.\n\nThe typical implementation of knowledge distillation follows a teacher-student framework with distinct phases <Paper corpusId=\"269921267\" paperTitle=\"(Mei et al., 2024)\" isShortName></Paper>:\n\n1. Pre-train a complex teacher model that performs and generalizes well on the target dataset\n2. Generate predictions (soft targets) from the teacher model on available data\n3. Train the smaller student model using both the original labels and the teacher's predictions\n\nThe student model benefits from this additional \"dark knowledge\" and often performs better than if it was trained solely on the actual labels <Paper corpusId=\"211096976\" paperTitle=\"(Mobahi et al., 2020)\" isShortName></Paper>. This is typically achieved by minimizing a loss function that incorporates both the traditional cross-entropy loss with ground truth labels and a distillation loss that measures the difference between the student's and teacher's predicted probability distributions <Paper corpusId=\"234336288\" paperTitle=\"(Jaiswal et al., 2021)\" isShortName></Paper>. Many implementations use the Kullback-Leibler (KL) divergence to regularize the student model's output probabilities, aligning them with those of the teacher model <Paper corpusId=\"271244914\" paperTitle=\"(Giakoumoglou et al., 2024)\" isShortName></Paper>.\n\nThis approach has proven particularly effective for enhancing the performance of lightweight student networks by leveraging the comprehensive knowledge embedded in larger teacher networks <Paper corpusId=\"267657497\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>, making it a valuable technique for deploying efficient models in resource-constrained environments.",
                "citations": [
                    {
                        "id": "(Witt et al., 2024)",
                        "snippets": [
                            "Knowledge Distillation (KD), depicted in Fig. 1, is a technique in deep learning where a smaller NN model (often called the \"student\") is trained to mimic the behavior of a larger, pre-trained model (referred to as the \"teacher\") [27]. This is accomplished not by transferring the model parameters directly, but rather by aligning the output distributions of both models. Traditional training methods involve training a model directly on ground-truth labels, using a cross-entropy loss that measures the discrepancy between the model's predictions and these true labels. In contrast, KD employs a divergence-based loss, such as the Kullback-Leibler (KL) divergence, to measure the difference between the student's predicted probabilities and those of the teacher model. This divergence provides insights into how closely the student is able to mimic the behavior of its teacher. A distinct feature of KD is the use of \"softened\" labels. In traditional classification tasks, hard labels are used, which unequivocally classify a data point into one category. However, the teacher model in KD provides \"soft\" labels in the form of probabilities, indicating the confidence levels across various categories. These probabilities can be further softened using a temperature parameter T to yield a smoother distribution, capturing the nuances of decision boundaries and offering richer guidance to the student model. This process allows the student to inherit not just the overt knowledge from the ground-truth labels but also the implicit, or \"dark\", knowledge embedded in the teacher model's predictions."
                        ],
                        "paper": {
                            "corpus_id": 265502078,
                            "title": "Decentralized and Incentivized Federated Learning: A Blockchain-Enabled Framework Utilising Compressed Soft-Labels and Peer Consistency",
                            "authors": [
                                {
                                    "authorId": "2115471566",
                                    "name": "Leon Witt"
                                },
                                {
                                    "authorId": "2067050909",
                                    "name": "Usama Zafar"
                                },
                                {
                                    "authorId": "2115288561",
                                    "name": "KuoYeh Shen"
                                },
                                {
                                    "authorId": "46223357",
                                    "name": "Felix Sattler"
                                },
                                {
                                    "authorId": "2150385385",
                                    "name": "Dan Li"
                                },
                                {
                                    "authorId": "2268713838",
                                    "name": "Songtao Wang"
                                },
                                {
                                    "authorId": "2242938198",
                                    "name": "Wojciech Samek"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Transactions on Services Computing",
                            "n_citations": 4
                        },
                        "score": 0.7255859375
                    },
                    {
                        "id": "(Mei et al., 2024)",
                        "snippets": [
                            "The knowledge distillation technique adopts a typical teacher-student training framework, as shown in Figure 2. In this framework, the \"teacher\" model represents a highly complex deep neural network.In contrast, the Student model is designed to be more streamlined and aims to achieve a similar level of performance with fewer parameters.The process begins with the pre-training of the Teacher model, a step that allows it to accumulate broad and deep knowledge on the specified data set.Instead of directly replicating the weights of the \"teacher\" model, the subsequent knowledge transfer phase uses its output -particularly soft probability distributions (rather than hard classification labels).This process not only involves the traditional cross-entropy loss, but also introduces the distillation loss that reflects the difference in the predicted probability distribution."
                        ],
                        "paper": {
                            "corpus_id": 269921267,
                            "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
                            "authors": [
                                {
                                    "authorId": "2298916636",
                                    "name": "Taiyuan Mei"
                                },
                                {
                                    "authorId": "2298918720",
                                    "name": "Yun Zi"
                                },
                                {
                                    "authorId": "2222987403",
                                    "name": "X. Cheng"
                                },
                                {
                                    "authorId": "2297725659",
                                    "name": "Zijun Gao"
                                },
                                {
                                    "authorId": "2297735971",
                                    "name": "Qi Wang"
                                },
                                {
                                    "authorId": "2302372513",
                                    "name": "Haowei Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)",
                            "n_citations": 20
                        },
                        "score": 0.8330078125
                    },
                    {
                        "id": "(Nayak et al., 2020)",
                        "snippets": [
                            "The core idea of Knowledge Distillation, as discussed in the seminal paper by Hinton et al. [7], is to transfer the (input to output) learned mapping function from Teacher to Student via sharing the \"dark knowledge\" extracted by the Teacher on the training images. This typically is achieved via matching the soft targets (or soft labels, i.e., output of softmax layer) predicted by the Student to that of the Teacher for the same inputs. This is the distillation mechanism that enables transfer of the better generalization capability (i.e., the \"knowledge\") of the Teacher to the Student."
                        ],
                        "paper": {
                            "corpus_id": 227013462,
                            "title": "Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "143747407",
                                    "name": "Gaurav Kumar Nayak"
                                },
                                {
                                    "authorId": "2217000",
                                    "name": "Konda Reddy Mopuri"
                                },
                                {
                                    "authorId": "1429640900",
                                    "name": "Anirban Chakraborty"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                            "n_citations": 19
                        },
                        "score": 0.73193359375
                    },
                    {
                        "id": "(Hinton et al., 2015)",
                        "snippets": [
                            "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                        ],
                        "paper": {
                            "corpus_id": 7200347,
                            "title": "Distilling the Knowledge in a Neural Network",
                            "authors": [
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "49959210",
                                    "name": "J. Dean"
                                }
                            ],
                            "year": 2015,
                            "venue": "arXiv.org",
                            "n_citations": 19742
                        },
                        "score": 0
                    },
                    {
                        "id": "(Nayak et al., 2019)",
                        "snippets": [
                            "The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability. For performing the knowledge distillation, one can use the training data from the target distribution or an arbitrary data. Typically, the data used to perform the distillation is called 'Transfer set'. In order to maximize the information provided per sample, we can make the soft targets to have a high entropy (non-peaky). This is generally achieved by using a high temperature at the softmax layer (Hinton et al., 2015). Also, because of non-peaky soft-labels, the training gradients computed on the loss will have less variance and enable to use higher learning rates leading to quick convergence."
                        ],
                        "paper": {
                            "corpus_id": 159041346,
                            "title": "Zero-Shot Knowledge Distillation in Deep Networks",
                            "authors": [
                                {
                                    "authorId": "143747407",
                                    "name": "Gaurav Kumar Nayak"
                                },
                                {
                                    "authorId": "2217000",
                                    "name": "Konda Reddy Mopuri"
                                },
                                {
                                    "authorId": "7155274",
                                    "name": "Vaisakh Shaj"
                                },
                                {
                                    "authorId": "144682140",
                                    "name": "R. Venkatesh Babu"
                                },
                                {
                                    "authorId": "1429640900",
                                    "name": "Anirban Chakraborty"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 245
                        },
                        "score": 0.82470703125
                    },
                    {
                        "id": "(Mobahi et al., 2020)",
                        "snippets": [
                            "The original formulation achieves this by training the student model using the output probability distribution of the teacher model in addition to original labels. The student model benefits from this \"dark knowledge\" (extra information in soft predictions) and often performs better than if it was trained on the actual labels."
                        ],
                        "paper": {
                            "corpus_id": 211096976,
                            "title": "Self-Distillation Amplifies Regularization in Hilbert Space",
                            "authors": [
                                {
                                    "authorId": "3232655",
                                    "name": "H. Mobahi"
                                },
                                {
                                    "authorId": "1682124",
                                    "name": "Mehrdad Farajtabar"
                                },
                                {
                                    "authorId": "1745169",
                                    "name": "P. Bartlett"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 235
                        },
                        "score": 0.72802734375
                    },
                    {
                        "id": "(Jaiswal et al., 2021)",
                        "snippets": [
                            "Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model",
                            ".In general, knowledge is transferred from the 'T' to 'S' by minimizing a loss function in which the target is the distribution of class probabilities predicted by 'T'. This probability distribution has the correct class at a very high probability (close to '1') with all other class probabilities very close to '0'. As such, it does not provide much information beyond the ground truth labels already provided in the dataset. For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process."
                        ],
                        "paper": {
                            "corpus_id": 234336288,
                            "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification",
                            "authors": [
                                {
                                    "authorId": "2139643077",
                                    "name": "Bhavesh Jaiswal"
                                },
                                {
                                    "authorId": "32136431",
                                    "name": "Nagendra Gajjar"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Journal of Advanced Computer Science and Applications",
                            "n_citations": 0
                        },
                        "score": 0.7685546875
                    },
                    {
                        "id": "(Giakoumoglou et al., 2024)",
                        "snippets": [
                            "The concept of Knowledge Distillation (KD) was first introduced by Hinton et al. [20]. It involves extracting \"dark knowledge\" from accurate teacher models to guide the learning process of student models. This is achieved by utilizing the Kullback-Leibler (KL) loss to regularize the output probabilities of student models, aligning them with those of their teacher models when given the same inputs."
                        ],
                        "paper": {
                            "corpus_id": 271244914,
                            "title": "Relational Representation Distillation",
                            "authors": [
                                {
                                    "authorId": "2196360101",
                                    "name": "Nikolaos Giakoumoglou"
                                },
                                {
                                    "authorId": "2292259667",
                                    "name": "Tania Stathaki"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.82177734375
                    },
                    {
                        "id": "(Kim et al., 2023)",
                        "snippets": [
                            "Knowledge distillation (KD) is a technique used to enhance the performance of lightweight student networks by leveraging the dark knowledge embedded in large teacher networks."
                        ],
                        "paper": {
                            "corpus_id": 267657497,
                            "title": "Maximizing discrimination capability of knowledge distillation with energy function",
                            "authors": [
                                {
                                    "authorId": "2268350036",
                                    "name": "Seonghak Kim"
                                },
                                {
                                    "authorId": "2156910329",
                                    "name": "Gyeongdo Ham"
                                },
                                {
                                    "authorId": "2268370058",
                                    "name": "Suin Lee"
                                },
                                {
                                    "authorId": "2268310103",
                                    "name": "Donggon Jang"
                                },
                                {
                                    "authorId": "2145154407",
                                    "name": "Daeshik Kim"
                                }
                            ],
                            "year": 2023,
                            "venue": "Knowledge-Based Systems",
                            "n_citations": 4
                        },
                        "score": 0.77783203125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Temperature Scaling Implementation",
                "tldr": "Temperature scaling is a core technique in knowledge distillation that controls the \"softness\" of probability distributions by dividing logits by a temperature parameter T before applying softmax. Higher temperature values smooth out the probability distributions, revealing more of the dark knowledge in teacher outputs and helping student networks learn subtle class relationships. (13 sources)",
                "text": "\nKnowledge distillation relies on temperature scaling to extract the rich \"dark knowledge\" contained in a teacher model's output distributions. The implementation begins with applying a modified softmax function to the logits (pre-softmax activations) of both teacher and student networks <Paper corpusId=\"198179767\" paperTitle=\"(Zhao et al., 2019)\" isShortName></Paper> <Paper corpusId=\"219687782\" paperTitle=\"(Trofimov et al., 2020)\" isShortName></Paper>:\n\n```\np_i(T) = exp(z_i/T) / \u03a3_j exp(z_j/T)\n```\n\nwhere z_i represents the logit for class i, and T is the temperature parameter <Paper corpusId=\"265444951\" paperTitle=\"(Su et al., 2023)\" isShortName></Paper> <Paper corpusId=\"269317596\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.\n\nThe temperature parameter T is crucial for controlling the \"softness\" of the probability distribution. When T=1, the equation reduces to the standard softmax function, producing sharper distributions with high confidence for the predicted class. When T>1, which is typical in knowledge distillation, the distribution becomes smoother, emphasizing the relative differences between class probabilities <Paper corpusId=\"214623014\" paperTitle=\"(Algan et al., 2020)\" isShortName></Paper> <Paper corpusId=\"252596141\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper>.\n\nThis smoothing effect is essential because it reveals the \"dark knowledge\" - the information about how the teacher model relates different classes to each other. For instance, if an image contains features that slightly resemble multiple classes, these subtle similarities become more apparent in the softened distribution <Paper corpusId=\"234336288\" paperTitle=\"(Jaiswal et al., 2021)\" isShortName></Paper> <Paper corpusId=\"236984375\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>.\n\nThe implementation requires careful consideration of the temperature value. If T is too small, the distribution remains peaky and similar to one-hot labels, losing the rich class similarity information. If T is too large, the distribution becomes too uniform, potentially over-emphasizing irrelevant classes <Paper corpusId=\"235421655\" paperTitle=\"(Lee et al., 2021)\" isShortName></Paper> <Paper corpusId=\"260447668\" paperTitle=\"(Cui et al., 2022)\" isShortName></Paper>.\n\nRecent research has explored adaptive temperature approaches that assign different temperatures to different samples based on their characteristics. For example, Li et al. proposed an entropy-based method that learns node-specific temperatures by considering the teacher's confidence for each node <Paper corpusId=\"274436184\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nDuring implementation, both teacher and student networks must use the same temperature value when generating soft targets and predictions, respectively. This ensures the student learns to mimic the teacher's softened output distribution accurately <Paper corpusId=\"253655030\" paperTitle=\"(Borza et al., 2022)\" isShortName></Paper> <Paper corpusId=\"265384964\" paperTitle=\"(Xie et al., 2023)\" isShortName></Paper>.\n\nThe temperature parameter also affects gradient scaling. To compensate for the smaller gradients produced by higher temperatures, a factor of T\u00b2 is often incorporated into the distillation loss term <Paper corpusId=\"219687782\" paperTitle=\"(Trofimov et al., 2020)\" isShortName></Paper>. This scaling ensures that the gradients from both the soft targets and hard labels are of similar magnitude, facilitating effective optimization.",
                "citations": [
                    {
                        "id": "(Zhao et al., 2019)",
                        "snippets": [
                            "Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process.\n\nLet us respectively denote x and y as the input of the DNNs and one-hot labels of our architecture. We let P T be the teacher network's softmax output as P T = sof tmax(a T ). Specifically, P T is obtained by applying softmax function on the un-normalized log probability values a T . Similarly, the same image fed to the student network to get the predictions P S = sof tmax(a S ). In the intermediate layers of the DNN, we denote the activation tensor A \u2208 R C\u00d7X\u00d7W with its corresponding layer. The pairs of teacher and student attention maps are denoted as F (A j T ) and F (A j S ) in vectorized form respectively [21]. And the standard cross entropy is denoted as H. Hinton et al. [19] extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 the student network will be trained to optimize the following loss function:"
                        ],
                        "paper": {
                            "corpus_id": 198179767,
                            "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching",
                            "authors": [
                                {
                                    "authorId": "50981688",
                                    "name": "Haoran Zhao"
                                },
                                {
                                    "authorId": "144326521",
                                    "name": "Xin Sun"
                                },
                                {
                                    "authorId": "1964397",
                                    "name": "Junyu Dong"
                                },
                                {
                                    "authorId": "10944885",
                                    "name": "Changrui Chen"
                                },
                                {
                                    "authorId": "2087106420",
                                    "name": "Zihe Dong"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE Transactions on Cybernetics",
                            "n_citations": 59
                        },
                        "score": 0.70361328125
                    },
                    {
                        "id": "(Trofimov et al., 2020)",
                        "snippets": [
                            "The knowledge distillation (KD) assumes two models: a teacher and a student. The teacher is typically a large and accurate network or an ensemble. The student is trained to fit the softmax outputs of the teacher together with ground truth labels. The idea is that outputs of the teacher capture not only the information provided by ground truth labels but also the probabilities of other classes -\"dark knowledge\". The knowledge distillation can be summarized as follows. \n\nLet z i be logits (pre-softmax activations) and q i -probabilities of classes as predicted by a neural network. Knowledge distillation smooths z i with the temperature \u03c4 \n\nNeural networks often do very confident predictions (close to 0 or 1) and smoothing helps to provide for student more information during training [11]. The KD loss is a linear combination of the logistic loss and cross-entropy between predictions of the teacher and the student \n\nwhere z T i , z S i are logits of the teacher and the student, H(p, q) = \u2212p log(q) is the cross-entropy function. The factor \u03c4 2 is used for scaling gradients of both parts of the loss function to be the same order."
                        ],
                        "paper": {
                            "corpus_id": 219687782,
                            "title": "Multi-Fidelity Neural Architecture Search With Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "144035647",
                                    "name": "I. Trofimov"
                                },
                                {
                                    "authorId": "143888131",
                                    "name": "Nikita Klyuchnikov"
                                },
                                {
                                    "authorId": "49614468",
                                    "name": "Mikhail Salnikov"
                                },
                                {
                                    "authorId": "8792436",
                                    "name": "Alexander N. Filippov"
                                },
                                {
                                    "authorId": "51139941",
                                    "name": "Evgeny Burnaev"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Access",
                            "n_citations": 15
                        },
                        "score": 0.7265625
                    },
                    {
                        "id": "(Su et al., 2023)",
                        "snippets": [
                            "The goal of KD [8] is to transfer the dark knowledge encapsulated in the soft probability output of the teacher model to the student model. In classification tasks, the softened probabilities are computed via the temperature-scaled softmax function, given by \n\nwhere p i (T ) is the probability output for class i softened by the temperature hyperparameter T , y i represents the logit for class i, and C is the total number of classes. Typically, T is set to greater than 1 in KD. The higher value of T produces softer probabilities, which are crucial for unveiling the dark knowledge hidden in the inter-class relationships captured by the teacher. The core idea of KD lies in minimizing the KL divergence loss function to align the soft logits of the teacher and student. The loss of KD L KD is defined as \n\nwhere p T and p S are the output logits of the teacher T and the student S, respectively."
                        ],
                        "paper": {
                            "corpus_id": 265444951,
                            "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2268314644",
                                    "name": "Chi-Ping Su"
                                },
                                {
                                    "authorId": "1569686364",
                                    "name": "Ching-Hsun Tseng"
                                },
                                {
                                    "authorId": "2338265886",
                                    "name": "Bin Pu"
                                },
                                {
                                    "authorId": "2338506240",
                                    "name": "Lei Zhao"
                                },
                                {
                                    "authorId": "2328588941",
                                    "name": "Zhuangzhuang Chen"
                                },
                                {
                                    "authorId": "2116351339",
                                    "name": "Shin-Jye Lee"
                                }
                            ],
                            "year": 2023,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.8193359375
                    },
                    {
                        "id": "(Guo et al., 2024)",
                        "snippets": [
                            "Firstly, we will recap the Knowledge Distillation method proposed by Hinton et al. [11]. This widely used method involves using category similarity as a guide for student networks. To regularize the network's learning, temperature is introduced to soften the initial categorical information, also referred to as 'dark knowledge'. The output probability of the teacher network and student network can be calculated as Equations ( 1) and (2). \n\nwhere T represents temperature, which can adjust the softening of the output probability. z i z j are the logits input for softmax, p i represents the output probability of each category in the teacher network, and q i represents the output probability of each category in the student network. Cross-entropy between the distilled teacher and student models calculates the soft loss. The hard loss is calculated based on the predicted values of the student model and the true values. \n\nThe ground truth label (also known as the hard target label) for the i-th sample is represented by y i . The teacher and student models' predicted probability are represented by p i and q i , respectively. The total loss function is calculated as below. \n\nThe loss function used in Knowledge Distillation involves two types of losses: soft target loss L soft and hard target loss L hard . The former guides the student to replicate the teacher's probability distribution, whereas the latter reflects the guidance from the actual ground truth labels. The parameter \u03b1 balances the effect of these two losses. During the Knowledge Distillation process, the student receives both the challenging and soft target knowledge. The loss function can be written as follows: \n\nwhere the loss of the cross-entropy is represented by L CE . The softmax function is represented by \u03c3. y represents the ground truth label. The output logits of the student and teacher networks are denoted by z S and z T . The balancing hyperparameter is represented by \u03b1."
                        ],
                        "paper": {
                            "corpus_id": 269317596,
                            "title": "Shared Knowledge Distillation Network for Object Detection",
                            "authors": [
                                {
                                    "authorId": "2158003499",
                                    "name": "Zhen Guo"
                                },
                                {
                                    "authorId": "2297246012",
                                    "name": "Pengzhou Zhang"
                                },
                                {
                                    "authorId": "2297096848",
                                    "name": "Peng Liang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Electronics",
                            "n_citations": 3
                        },
                        "score": 0.75048828125
                    },
                    {
                        "id": "(Algan et al., 2020)",
                        "snippets": [
                            "In the original work, the authors used distillation to transfer knowledge from the big teacher network to a much smaller student network without decreasing the performance. The idea is mainly motivated by learning from soft labels where the similarity of each instance to each class is emphasized by temperature hyperparameter.\n\nClass probabilities on softmax output, beyond the true class probability, are usually very low. But, compared with each other, some classes may have a much higher probability than others, and this carries important information about that data instance, which is also called as dark knowledge. By making probability distribution smoother, this relation is emphasized, as shown in Equation 2.\n\nInstead of being trained on hard labels, the student network is trained on the weighted sum of hard labels and soft labels produced by the teacher network. So, the loss function is defined as follows,\n\nwhere q i represents the soft labels produced by the teacher network using temperature T and y i represents the given label."
                        ],
                        "paper": {
                            "corpus_id": 214623014,
                            "title": "Label Noise Types and Their Effects on Deep Learning",
                            "authors": [
                                {
                                    "authorId": "1454230053",
                                    "name": "G. Algan"
                                },
                                {
                                    "authorId": "152986771",
                                    "name": "I. Ulusoy"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 53
                        },
                        "score": 0.73095703125
                    },
                    {
                        "id": "(Nguyen et al., 2022)",
                        "snippets": [
                            "Knowledge Distillation (KD) Hinton et al. (2015) employs a technique to transfer the learned knowledge from a pre-trained teacher model to another model with less or similar complexity (student model). The model transfer process is implemented in two steps. In the first step, a surrogate output probability function, namely a temperature-softmax function, is utilized. By adding a temperature scaling variable T, the conventional softmax function then becomes: \n\nwhere zj is the output set corresponding to class j of the given DNN with batch of input data X. \n\nThe subscript l denotes the index of softmax output which corresponds to the prediction on class l of the DNN. The intention of adding the variable T is to adjust the slope of the softmax function in the classifier as shown in equation 16. As we can see, when we increase the value of T, the slope of the softmax function will decrease significantly. With large temperature scale values, over the same output range of data, the range of values represented by input zj is larger. Then, the output value tuple created by the teacher and student carries considerably more information. Therefore, the learning process between teacher and student is more effective. \n\nIn the second step, to help implement the transfer of knowledge from teacher to student, the authors in Hinton et al. (2015) presented a new loss function, called distillation loss function (also known as soft-loss function). This loss function comprises two terms: the intrinsic entropy function of the teacher and the cross-entropy function between the teacher and the student's outputs, which can be expressed as follows: \n\nThe purpose of this function is to compare the output distribution between teacher p(X) and student model q(X). In terms of information theory, this measurement shows the under-performance of the distribution set created by student, when the output distribution set of the teacher is taken as the sample distribution set, with precision of 100%. By minimizing this function, we reduce the functional difference between the two deep networks. As a result, the student model tends to make it's output distribution become more identical to the teacher model's behavior."
                        ],
                        "paper": {
                            "corpus_id": 252596141,
                            "title": "Label driven Knowledge Distillation for Federated Learning with non-IID Data",
                            "authors": [
                                {
                                    "authorId": "2162621330",
                                    "name": "Minh-Duong Nguyen"
                                },
                                {
                                    "authorId": "145436642",
                                    "name": "Viet Quoc Pham"
                                },
                                {
                                    "authorId": "2233724",
                                    "name": "D. Hoang"
                                },
                                {
                                    "authorId": "1389575160",
                                    "name": "Long Tran-Thanh"
                                },
                                {
                                    "authorId": "30479205",
                                    "name": "Diep N. Nguyen"
                                },
                                {
                                    "authorId": "2054135428",
                                    "name": "W. Hwang"
                                }
                            ],
                            "year": 2022,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.791015625
                    },
                    {
                        "id": "(Jaiswal et al., 2021)",
                        "snippets": [
                            "Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model",
                            ".In general, knowledge is transferred from the 'T' to 'S' by minimizing a loss function in which the target is the distribution of class probabilities predicted by 'T'. This probability distribution has the correct class at a very high probability (close to '1') with all other class probabilities very close to '0'. As such, it does not provide much information beyond the ground truth labels already provided in the dataset. For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process."
                        ],
                        "paper": {
                            "corpus_id": 234336288,
                            "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification",
                            "authors": [
                                {
                                    "authorId": "2139643077",
                                    "name": "Bhavesh Jaiswal"
                                },
                                {
                                    "authorId": "32136431",
                                    "name": "Nagendra Gajjar"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Journal of Advanced Computer Science and Applications",
                            "n_citations": 0
                        },
                        "score": 0.7685546875
                    },
                    {
                        "id": "(Wang et al., 2021)",
                        "snippets": [
                            "Knowledge distillation (Figure 3) is put forward by Hinton et al. (Hinton et al., 2015). It is a widely used knowledge transfer technology in the deep learning field. First, a well-trained, robust, high-precision teacher network is needed. Its output is softened with temperature T to provide more information entropy, which extracts hidden knowledge behind its output layer. Then, a relatively small student network is trained to imitate the teacher network's probability output distribution, obtaining a better output result. The main idea of knowledge distillation. The label of the input image is cat; the probability is expressed as {0, 1, 0}. After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q'', which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics",
                            "To improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become",
                            "where Q T s , Q T t are the softmax probability distribution of the student network and teacher network after softening according to temperature T."
                        ],
                        "paper": {
                            "corpus_id": 236984375,
                            "title": "Combine-Net: An Improved Filter Pruning Algorithm",
                            "authors": [
                                {
                                    "authorId": "2109643902",
                                    "name": "Jinghan Wang"
                                },
                                {
                                    "authorId": "2151302904",
                                    "name": "Guangyue Li"
                                },
                                {
                                    "authorId": "2107940856",
                                    "name": "Wenzhao Zhang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Inf.",
                            "n_citations": 3
                        },
                        "score": 0.765625
                    },
                    {
                        "id": "(Lee et al., 2021)",
                        "snippets": [
                            "[11] proposed the concept of soft label which is distilled from the teacher model to the student model. Soft labels are the probabilities of an input belonging to each class, and can be estimated by a softmax function. Here, a temperature factor is introduced to control the importance of each soft label, which contains the informative dark knowledge from the teacher model. By increasing the temperature, the logits can contain richer information than one-hot labels. However, if the temperature becomes too large, the probability of irrelevant classes will also be over-emphasized."
                        ],
                        "paper": {
                            "corpus_id": 235421655,
                            "title": "Energy-efficient Knowledge Distillation for Spiking Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2109519891",
                                    "name": "Dongjin Lee"
                                },
                                {
                                    "authorId": "120425550",
                                    "name": "Seongsik Park"
                                },
                                {
                                    "authorId": "2157129258",
                                    "name": "Jongwan Kim"
                                },
                                {
                                    "authorId": "2111845679",
                                    "name": "Wuhyeong Doh"
                                },
                                {
                                    "authorId": "2999019",
                                    "name": "Sungroh Yoon"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 12
                        },
                        "score": 0.76025390625
                    },
                    {
                        "id": "(Cui et al., 2022)",
                        "snippets": [
                            "Compared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part. \n\n3) Loss Function for KD. The proposed KD-based neural network lightweight method is depicted in Fig. 3. After the teacher network is trained, the student network is trained using a combination of distillation loss and ordinary MSE loss."
                        ],
                        "paper": {
                            "corpus_id": 260447668,
                            "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback",
                            "authors": [
                                {
                                    "authorId": "2172485573",
                                    "name": "Yiming Cui"
                                },
                                {
                                    "authorId": "47093519",
                                    "name": "Jiajia Guo"
                                },
                                {
                                    "authorId": "2113999930",
                                    "name": "Zheng Cao"
                                },
                                {
                                    "authorId": "120710335",
                                    "name": "Huaze Tang"
                                },
                                {
                                    "authorId": "2257212132",
                                    "name": "Chao-Kai Wen"
                                },
                                {
                                    "authorId": "2227268421",
                                    "name": "Shi Jin"
                                },
                                {
                                    "authorId": "2288090155",
                                    "name": "Xin Wang"
                                },
                                {
                                    "authorId": "2240356164",
                                    "name": "Xiaolin Hou"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Transactions on Communications",
                            "n_citations": 3
                        },
                        "score": 0.83837890625
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Learning node-specific temperatures addresses the limitations inherent in the temperature mechanism used in traditional distillation methods. In conventional knowledge distillation, a predefined temperature parameter is used to generate soft targets. The teacher model's informative dark knowledge is embedded in the soft targets, which infer the probability that a node belongs to a given class. The softmax function with a fixed temperature is defined as follows: \n\nwhere the temperature parameter \u03c4 is used to control the softness of each prediction. The distribution of predictions becomes smoother as the temperature rises, and sharper as the temperature falls. Thus, the temperature is responsible for regulating the balance between the true label knowledge and the dark knowledge. However, using a fixed temperature treats the softening of all teacher logits equally, ignoring variations in node representations. This uniform approach fails to provide precise guidance for individual nodes, which can hinder knowledge transfer. Moreover, existing research has shown that temperature scaling can improve model performance (Z. (Zhang et al., 2020). \n\nTo confront these issues, we propose an entropy-based approach to learn node-specific temperatures. This approach assigns a unique temperature \u03c4i to each node, controlling the degree of softening individually. The softmax function with node-specific temperatures is defined as: \n\nwhere the temperature parameter \u03c4i is the specific temperature of the i-th node. Obviously, \u03c4i determines the softening direction of each node. The temperature of the node is related to the teacher's confidence for each respective node (Zhichun Guo et al., 2023). Therefore, we can judge the teacher's confidence for each node through the entropy of the teacher's logits. The lower the entropy value, the higher the teacher's confidence for this node. Specifically, we calculate the node-specific temperature through the probability distribution of the teacher and the confidence of the teacher, defined as follows:"
                        ],
                        "paper": {
                            "corpus_id": 274436184,
                            "title": "Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2327960212",
                                    "name": "Chengyu Li"
                                },
                                {
                                    "authorId": "2320341",
                                    "name": "Debo Cheng"
                                },
                                {
                                    "authorId": "2152233401",
                                    "name": "Guixian Zhang"
                                },
                                {
                                    "authorId": "2316406036",
                                    "name": "Yi Li"
                                },
                                {
                                    "authorId": "2186753918",
                                    "name": "Shichao Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.80810546875
                    },
                    {
                        "id": "(Borza et al., 2022)",
                        "snippets": [
                            "Knowledge distillation (Bucila et al., 2006) was proposed as an effective and elegant compression technique to derive a lighter and faster network (student) from a more complex one (teacher), by penalizing the difference between their logits. Later, this mechanism was formalized by (Hinton et al., 2015) to distill the \"dark-knowledge\" from the teacher to the student. The authors noticed that a powerful and confident teacher does not bring more knowledge than ground truth data, as its prediction tends to be a narrow probability distribution with a single peak for the ground truth class. To alleviate this issue, the teacher's logits are \"softened\" by a temperature scaling factor of the softmax activation. In such a manner, the lightweight network can infer what other classes were found similar by the teacher network. More formally, this can be expressed as \n\nwhere \u03c4 is the temperature of the softmax function (equation from (Hinton et al., 2015)). When \u03c4 is greater than 1, the small probabilities of the softmax function are increased and the output is \"softened\". \n\nIn the classical setup, during training, a Kullback-Libeler divergence loss term is employed to ensure that the student network mimics the teacher's softened predictions. Other methods proposed using the root-mean-square error loss [9] or distilling from hardlabels (Touvron et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 253655030,
                            "title": "Effective Online Knowledge Distillation via Attention-Based Model Ensembling",
                            "authors": [
                                {
                                    "authorId": "144583200",
                                    "name": "D. Borza"
                                },
                                {
                                    "authorId": "1821352",
                                    "name": "A. Darabant"
                                },
                                {
                                    "authorId": "74810447",
                                    "name": "Tudor Alexandru Ileni"
                                },
                                {
                                    "authorId": "150020994",
                                    "name": "Alexandru-Ion Marinescu"
                                }
                            ],
                            "year": 2022,
                            "venue": "Mathematics",
                            "n_citations": 2
                        },
                        "score": 0.78076171875
                    },
                    {
                        "id": "(Xie et al., 2023)",
                        "snippets": [
                            "Knowledge distillation (Bucila et al., 2006) involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements. In the process of knowledge distillation, logits are used as a basis for comparing the outputs of the student model with those of the teacher model. By examining logits, the model can measure the certainty or confidence of its predictions before applying the softmax function to obtain the final probability. The standard cross-entropy loss depends on the predicted probabilities and ground truth labels. Additionally, the loss function is extended to include the standard cross-entropy loss between the predictions of the student model and the ground truth labels, as well as an additional loss term that measures the discrepancy between the softened probabilities (obtained through a higher temperature softmax) of the teacher model's predictions and the corresponding predictions of the student model. This additional term ensures that the student model not only learns to predict the correct labels but also aims to replicate the softened outputs of the teacher model, effectively transferring its knowledge to the student model. By jointly minimizing these two loss terms, the student model can learn to generalize better and imitate the behavior of the more complex teacher model. The calculation of the probability for the class is as follows: \n\nHere, T represents the \"temperature\" of knowledge distillation. When T = 1, it corresponds to a normalized exponential function. With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\""
                        ],
                        "paper": {
                            "corpus_id": 265384964,
                            "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2268047558",
                                    "name": "Jinzhou Xie"
                                },
                                {
                                    "authorId": "2268031861",
                                    "name": "Hongmin Zhao"
                                }
                            ],
                            "year": 2023,
                            "venue": "Fire",
                            "n_citations": 6
                        },
                        "score": 0.88232421875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Soft Label Generation Techniques",
                "tldr": "Soft label generation is central to knowledge distillation, where instead of using one-hot encoded labels, the teacher's softened probability distributions across all classes are used to train the student model. These soft labels contain valuable \"dark knowledge\" about inter-class similarities that helps student models generalize better. (13 sources)",
                "text": "\nKnowledge distillation's effectiveness relies heavily on the generation of soft labels that capture the rich \"dark knowledge\" embedded in a teacher model's outputs. Soft labels are created from the teacher's softmax outputs, providing more nuanced supervision than one-hot vectors by including relative probabilities across all classes, including incorrect ones <Paper corpusId=\"159041346\" paperTitle=\"(Nayak et al., 2019)\" isShortName></Paper>. This information is particularly valuable because it encodes the similarity relationships between different classes that the teacher model has learned during its training <Paper corpusId=\"270077283\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.\n\nThe typical implementation of soft label generation involves processing the output logits (pre-softmax activations) of the teacher model through a softmax function with an elevated temperature parameter <Paper corpusId=\"235489777\" paperTitle=\"(Zhao et al., 2021)\" isShortName></Paper>. The resulting probability distribution becomes the target for the student model to mimic during training. This process can be formalized as:\n\n```\nP_T = softmax(a_T/T)\n```\n\nwhere a_T represents the teacher's logits and T is the temperature parameter <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.\n\nSeveral approaches exist for incorporating soft labels into student training. The most common method is to combine a standard cross-entropy loss (with hard labels) and a Kullback-Leibler divergence loss that aligns the student's softened outputs with the teacher's soft labels <Paper corpusId=\"263789814\" paperTitle=\"(Ding et al., 2020)\" isShortName></Paper>. This dual-objective training can be expressed as:\n\n```\nL = \u03bb\u2081L_CE(y, P_S) + \u03bb\u2082L_KD(P_T, P_S)\n```\n\nwhere \u03bb\u2081 and \u03bb\u2082 are weighting factors for balancing the two loss components <Paper corpusId=\"237453494\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\nRecent research has explored alternative formulations for soft label generation. For instance, Zhao et al. proposed decoupling the KL divergence into target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD) components, finding that the non-target class information is particularly important for effective knowledge transfer <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"247476179\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper>.\n\nBeyond logit-based approaches, some methods transfer knowledge using intermediate feature representations. Zhao et al. note that features from different layers of deep neural networks can also serve as valuable dark knowledge for guiding student training <Paper corpusId=\"198179767\" paperTitle=\"(Zhao et al., 2019)\" isShortName></Paper>. For example, Zhang et al. employed a hint learning loss that encourages similarity between corresponding branches of teacher and student networks <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>.\n\nAnother technique to enhance the effectiveness of soft labels is the top score difference (TSD) approach, which creates softer distributions by penalizing the teacher for having too much confidence in its primary prediction. This helps the teacher distribute confidence to secondary classes, resulting in more informative soft targets for the student <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"54986302\" paperTitle=\"(Yang et al., 2018)\" isShortName></Paper>.\n\nThe data used for generating soft labels, known as the \"transfer set,\" can be either the original training data or arbitrary data. To maximize information transfer per sample, it's beneficial to generate soft labels with high entropy (non-peaky distributions), typically achieved through higher temperature values <Paper corpusId=\"159041346\" paperTitle=\"(Nayak et al., 2019)\" isShortName></Paper>.\n\nInterestingly, research has shown that even relatively weak teacher models can provide valuable soft labels, suggesting that the regularization effect of soft targets may be as important as the specific class similarity information they contain <Paper corpusId=\"264590688\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"219962714\" paperTitle=\"(Yuan et al., 2020)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Nayak et al., 2019)",
                        "snippets": [
                            "The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability. For performing the knowledge distillation, one can use the training data from the target distribution or an arbitrary data. Typically, the data used to perform the distillation is called 'Transfer set'. In order to maximize the information provided per sample, we can make the soft targets to have a high entropy (non-peaky). This is generally achieved by using a high temperature at the softmax layer (Hinton et al., 2015). Also, because of non-peaky soft-labels, the training gradients computed on the loss will have less variance and enable to use higher learning rates leading to quick convergence."
                        ],
                        "paper": {
                            "corpus_id": 159041346,
                            "title": "Zero-Shot Knowledge Distillation in Deep Networks",
                            "authors": [
                                {
                                    "authorId": "143747407",
                                    "name": "Gaurav Kumar Nayak"
                                },
                                {
                                    "authorId": "2217000",
                                    "name": "Konda Reddy Mopuri"
                                },
                                {
                                    "authorId": "7155274",
                                    "name": "Vaisakh Shaj"
                                },
                                {
                                    "authorId": "144682140",
                                    "name": "R. Venkatesh Babu"
                                },
                                {
                                    "authorId": "1429640900",
                                    "name": "Anirban Chakraborty"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 245
                        },
                        "score": 0.82470703125
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "Hinton et al. [12] first proposed to use the soft outputs of the pretrained teacher network as dark knowledge to supervise the training of the student network. They introduced a temperature hyperparameter T and formulated the problem as \"knowledge distillation\". The student network is forced to learn the soft targets of the teacher network, which are obtained through using a high temperature T on the softmax inputs. In the process of knowledge transfer, soft targets often contain richer information than one-hot targets."
                        ],
                        "paper": {
                            "corpus_id": 270077283,
                            "title": "Self-Knowledge Distillation via Progressive Associative Learning",
                            "authors": [
                                {
                                    "authorId": "2292247795",
                                    "name": "Haoran Zhao"
                                },
                                {
                                    "authorId": "2297863565",
                                    "name": "Yanxian Bi"
                                },
                                {
                                    "authorId": "2304028661",
                                    "name": "Shuwen Tian"
                                },
                                {
                                    "authorId": "2237771070",
                                    "name": "Jian Wang"
                                },
                                {
                                    "authorId": "40075749",
                                    "name": "Peiying Zhang"
                                },
                                {
                                    "authorId": "2292213958",
                                    "name": "Zhaopeng Deng"
                                },
                                {
                                    "authorId": "2301512529",
                                    "name": "Kai Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Electronics",
                            "n_citations": 3
                        },
                        "score": 0.71142578125
                    },
                    {
                        "id": "(Hinton et al., 2015)",
                        "snippets": [
                            "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                        ],
                        "paper": {
                            "corpus_id": 7200347,
                            "title": "Distilling the Knowledge in a Neural Network",
                            "authors": [
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "49959210",
                                    "name": "J. Dean"
                                }
                            ],
                            "year": 2015,
                            "venue": "arXiv.org",
                            "n_citations": 19742
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhao et al., 2021)",
                        "snippets": [
                            "The idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels.\n\nLet us denote x, y as the input of the DNN and one-hot labels of our architecture. \u03c6 s and \u03c6 t represent the student network and teacher network with parameters W s and W t respectively. Given an input image x, the teacher network outputs the final predictions as P T which are obtained by applying the softmax function on the un-normalized log proba-bility values a T , i.e. P T = sof tmax(a T ). Similarly, the same image is fed into the student network to obtain the predictions P S = sof tmax(a S ).\n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss:\n\nIt contains the relative probabilities of incorrect classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4, the student network will be trained to optimize the following loss function:\n\nwhere \u03bb is a second hyper parameter controlling the trade-off between the two losses."
                        ],
                        "paper": {
                            "corpus_id": 235489777,
                            "title": "Knowledge Distillation via Instance-level Sequence Learning",
                            "authors": [
                                {
                                    "authorId": "50981688",
                                    "name": "Haoran Zhao"
                                },
                                {
                                    "authorId": "144326521",
                                    "name": "Xin Sun"
                                },
                                {
                                    "authorId": "1964397",
                                    "name": "Junyu Dong"
                                },
                                {
                                    "authorId": "2087106420",
                                    "name": "Zihe Dong"
                                },
                                {
                                    "authorId": "2108144626",
                                    "name": "Qiong Li"
                                }
                            ],
                            "year": 2021,
                            "venue": "Knowledge-Based Systems",
                            "n_citations": 24
                        },
                        "score": 0.77490234375
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "Knowledge distillation (Hinton et al., 2015) transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence (Hinton et al., 2015), the mean squared error (Fang et al., 2021) and the Jensen-Shannon divergence (Yin et al., 2019), etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. (Zhao et al., 2022) pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence."
                        ],
                        "paper": {
                            "corpus_id": 256900863,
                            "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK",
                            "authors": [
                                {
                                    "authorId": "2135847074",
                                    "name": "Xiongtao Zhang"
                                },
                                {
                                    "authorId": "2206403890",
                                    "name": "Zezong Yin"
                                },
                                {
                                    "authorId": "3247526",
                                    "name": "Yunliang Jiang"
                                },
                                {
                                    "authorId": "1390650781",
                                    "name": "Yizhang Jiang"
                                },
                                {
                                    "authorId": "2340356",
                                    "name": "Da-Song Sun"
                                },
                                {
                                    "authorId": "2189281",
                                    "name": "Yong Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.88525390625
                    },
                    {
                        "id": "(Ding et al., 2020)",
                        "snippets": [
                            "Hinton et al. [23] first propose KD to transfer dark knowledge from the teacher to the student. The softmax outputs encode richer knowledge than one-hot labels and can provide extra supervisory signals."
                        ],
                        "paper": {
                            "corpus_id": 263789814,
                            "title": "Multi-level Knowledge Distillation via Knowledge Alignment and Correlation",
                            "authors": [
                                {
                                    "authorId": "2064424445",
                                    "name": "Fei Ding"
                                },
                                {
                                    "authorId": "2257087597",
                                    "name": "Yin Yang"
                                },
                                {
                                    "authorId": "2256778126",
                                    "name": "Hongxin Hu"
                                },
                                {
                                    "authorId": "2095713717",
                                    "name": "V. Krovi"
                                },
                                {
                                    "authorId": "2140495064",
                                    "name": "Feng Luo"
                                }
                            ],
                            "year": 2020,
                            "venue": "",
                            "n_citations": 4
                        },
                        "score": 0.763671875
                    },
                    {
                        "id": "(Li et al., 2021)",
                        "snippets": [
                            "Knowledge distillation (Hinton et al., 2015) is first introduced for transferring knowledge from high-capability networks (teacher models) to small networks (student models) via soft labels",
                            "the dark knowledge in soft labels can be helpful by transferring knowledge from head classes to tail classes",
                            "We denote x a training image with its hard label y and soft label y",
                            "The feature vector f will be sent to two linear classifiers G hard and G sof t to get output logits z hard = G hard (f ) and z sof t = G sof t (f ). Let z denote the output logits of teacher model, then the soft label is given by:\n\nwhere i is the category index and T is the temperature which is set to 2 by default. Then, the knowledge distillation loss is written as:\n\n(4) For hard label supervision, we utilize the standard cross entropy loss L ce . Thus, the final loss is the combination of these two losses:\n\nwhere both of \u03bb 1 and \u03bb 2 are the weight of each loss and set to 1 in our experiments."
                        ],
                        "paper": {
                            "corpus_id": 237453494,
                            "title": "Self Supervision to Distillation for Long-Tailed Visual Recognition",
                            "authors": [
                                {
                                    "authorId": "1652032884",
                                    "name": "Tianhao Li"
                                },
                                {
                                    "authorId": "2109120086",
                                    "name": "Limin Wang"
                                },
                                {
                                    "authorId": "39914710",
                                    "name": "Gangshan Wu"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 103
                        },
                        "score": 0.82470703125
                    },
                    {
                        "id": "(Zhao et al., 2022)",
                        "snippets": [
                            "State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we re-formulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the \"difficulty\" of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megviiresearch/mdistiller."
                        ],
                        "paper": {
                            "corpus_id": 247476179,
                            "title": "Decoupled Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2112526021",
                                    "name": "Borui Zhao"
                                },
                                {
                                    "authorId": "144802953",
                                    "name": "Quan Cui"
                                },
                                {
                                    "authorId": "2067622132",
                                    "name": "Renjie Song"
                                },
                                {
                                    "authorId": "2153278419",
                                    "name": "Yiyu Qiu"
                                },
                                {
                                    "authorId": "1387852255",
                                    "name": "Jiajun Liang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 549
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhao et al., 2019)",
                        "snippets": [
                            "Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process.\n\nLet us respectively denote x and y as the input of the DNNs and one-hot labels of our architecture. We let P T be the teacher network's softmax output as P T = sof tmax(a T ). Specifically, P T is obtained by applying softmax function on the un-normalized log probability values a T . Similarly, the same image fed to the student network to get the predictions P S = sof tmax(a S ). In the intermediate layers of the DNN, we denote the activation tensor A \u2208 R C\u00d7X\u00d7W with its corresponding layer. The pairs of teacher and student attention maps are denoted as F (A j T ) and F (A j S ) in vectorized form respectively [21]. And the standard cross entropy is denoted as H. Hinton et al. [19] extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 the student network will be trained to optimize the following loss function:"
                        ],
                        "paper": {
                            "corpus_id": 198179767,
                            "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching",
                            "authors": [
                                {
                                    "authorId": "50981688",
                                    "name": "Haoran Zhao"
                                },
                                {
                                    "authorId": "144326521",
                                    "name": "Xin Sun"
                                },
                                {
                                    "authorId": "1964397",
                                    "name": "Junyu Dong"
                                },
                                {
                                    "authorId": "10944885",
                                    "name": "Changrui Chen"
                                },
                                {
                                    "authorId": "2087106420",
                                    "name": "Zihe Dong"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE Transactions on Cybernetics",
                            "n_citations": 59
                        },
                        "score": 0.70361328125
                    },
                    {
                        "id": "(Zhang et al., 2020)",
                        "snippets": [
                            "This subsection illustrates the objective functions of teacher and student networks, we introduce two additional loss functions to help our model efficiently transfer dark knowledge (i.e., the global type constraints and the relevance among relations) from teacher to student. \n\nThe Teacher Network In vanilla knowledge distillation, the teacher network is trained to fit one-hot labels. However, the ultimate goal of teacher is to provide better guidances for student, rather than achieve high accuracy simply. Inspired by (Yang et al., 2018), we introduce a top score difference (TSD) loss to make the teacher's distribution softer. More concretely, we first pick up K classes with the highest confidence scores from the teacher's output, and then compute the gap between the confidence scores of the primary class and other K-1 classes",
                            "Based on the global statistics, K is set to 3 empirically. We add the penalty term to standard ground-truth loss L T GT when training the teacher, facilitating it to distribute confidence to a few secondary relations",
                            "The Student Network Typically, knowledge distillation transfers dark knowledge from the final output of teacher. (Chen et al., 2017) demonstrate that using the intermediate representation of teacher as hint can stabilize the training process and improve the final performance of student. Here, we utilize the Kullback-Leibler divergence to measure the differences of corresponding branches between the teacher and student networks as hint learning loss",
                            "Intuitively, it encourages the results of MAA to be similar with the scaled global logits of the teacher network. The loss of knowledge distillation is calculated as the sum of L HT and L KD with a weight factor \u03bb ht . As a result, the updated loss of student network is defined as follows:"
                        ],
                        "paper": {
                            "corpus_id": 212855595,
                            "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
                            "authors": [
                                {
                                    "authorId": "47295143",
                                    "name": "Zhenyu Zhang"
                                },
                                {
                                    "authorId": "2269366",
                                    "name": "Xiaobo Shu"
                                },
                                {
                                    "authorId": "48613402",
                                    "name": "Yu Bowen"
                                },
                                {
                                    "authorId": "2079682",
                                    "name": "Tingwen Liu"
                                },
                                {
                                    "authorId": "48019474",
                                    "name": "Jiapeng Zhao"
                                },
                                {
                                    "authorId": "2108645146",
                                    "name": "Quangang Li"
                                },
                                {
                                    "authorId": "48358041",
                                    "name": "Li Guo"
                                }
                            ],
                            "year": 2020,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 35
                        },
                        "score": 0.869140625
                    },
                    {
                        "id": "(Yang et al., 2018)",
                        "snippets": [
                            "We focus on the problem of training a deep neural network in generations. The flowchart is that, in order to optimize the target network (student), another network (teacher) with the same architecture is first trained, and used to provide part of supervision signals in the next stage. While this strategy leads to a higher accuracy, many aspects (e.g., why teacher-student optimization helps) still need further explorations.This paper studies this problem from a perspective of controlling the strictness in training the teacher network. Existing approaches mostly used a hard distribution (e.g., one-hot vectors) in training, leading to a strict teacher which itself has a high accuracy, but we argue that the teacher needs to be more tolerant, although this often implies a lower accuracy. The implementation is very easy, with merely an extra loss term added to the teacher network, facilitating a few secondary classes to emerge and complement to the primary class. Consequently, the teacher provides a milder supervision signal (a less peaked distribution), and makes it possible for the student to learn from inter-class similarity and potentially lower the risk of over-fitting. Experiments are performed on standard image classification tasks (CIFAR100 and ILSVRC2012). Although the teacher network behaves less powerful, the students show a persistent ability growth and eventually achieve higher classification accuracies than other competitors. Model ensemble and transfer feature extraction also verify the effectiveness of our approach."
                        ],
                        "paper": {
                            "corpus_id": 54986302,
                            "title": "Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students",
                            "authors": [
                                {
                                    "authorId": "2154170445",
                                    "name": "Chenglin Yang"
                                },
                                {
                                    "authorId": "3041937",
                                    "name": "Lingxi Xie"
                                },
                                {
                                    "authorId": "2383133",
                                    "name": "Siyuan Qiao"
                                },
                                {
                                    "authorId": "145081362",
                                    "name": "A. Yuille"
                                }
                            ],
                            "year": 2018,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 136
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "KD [33] provides additional information about semantic similarity to model learning through the use of dark knowledge generated by self-distillation",
                            "Compared with pseudo labels, dark knowledge from the teacher contains the similarity information between classes (Yuan et al., 2020). With the incorporation of self-distillation, a weak teacher with much lower accuracy than students can still significantly improve the clustering accuracy of students. The success of selfdistillation even with a weak teacher is not solely due to the shared similarity information between classes, but rather due to the regularization of the distilled knowledge (Yuan et al., 2020)."
                        ],
                        "paper": {
                            "corpus_id": 264590688,
                            "title": "Towards Generalized Multi-stage Clustering: Multi-view Self-distillation",
                            "authors": [
                                {
                                    "authorId": "2186275686",
                                    "name": "Jiatai Wang"
                                },
                                {
                                    "authorId": "2244022079",
                                    "name": "Zhiwei Xu"
                                },
                                {
                                    "authorId": "2262805047",
                                    "name": "Xin Wang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.76220703125
                    },
                    {
                        "id": "(Yuan et al., 2020)",
                        "snippets": [
                            "Knowledge Distillation (KD) aims to distill the knowledge of a cumbersome teacher model into a lightweight student model. Its success is generally attributed to the privileged information on similarities among categories provided by the teacher model, and in this sense, only strong teacher models are deployed to teach weaker students in practice. In this work, we challenge this common belief by following experimental observations: 1) beyond the acknowledgment that the teacher can improve the student, the student can also enhance the teacher significantly by reversing the KD procedure; 2) a poorly-trained teacher with much lower accuracy than the student can still improve the latter significantly. To explain these observations, we provide a theoretical analysis of the relationships between KD and label smoothing regularization. We prove that 1) KD is a type of learned label smoothing regularization and 2) label smoothing regularization provides a virtual teacher model for KD. From these results, we argue that the success of KD is not fully due to the similarity information between categories from teachers, but also to the regularization of soft targets, which is equally or even more important. Based on these analyses, we further propose a novel Teacher-free Knowledge Distillation (Tf-KD) framework, where a student model learns from itself or manually-designed regularization distribution. The Tf-KD achieves comparable performance with normal KD from a superior teacher, which is well applied when a stronger teacher model is unavailable. Meanwhile, Tf-KD is generic and can be directly deployed for training deep neural networks. Without any extra computation cost, Tf-KD achieves up to 0.65\\% improvement on ImageNet over well-established baseline models, which is superior to label smoothing regularization."
                        ],
                        "paper": {
                            "corpus_id": 219962714,
                            "title": "Revisiting Knowledge Distillation via Label Smoothing Regularization",
                            "authors": [
                                {
                                    "authorId": "2087091296",
                                    "name": "Li Yuan"
                                },
                                {
                                    "authorId": "40983412",
                                    "name": "Francis E. H. Tay"
                                },
                                {
                                    "authorId": "2108696460",
                                    "name": "Guilin Li"
                                },
                                {
                                    "authorId": "2155456820",
                                    "name": "Tao Wang"
                                },
                                {
                                    "authorId": "33221685",
                                    "name": "Jiashi Feng"
                                }
                            ],
                            "year": 2020,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 527
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Loss Function Formulations",
                "tldr": "Knowledge distillation loss functions typically combine cross-entropy loss on hard labels with KL divergence between teacher and student soft outputs. Various formulations exist, including temperature-scaled softmax, weighting mechanisms, and specialized losses that separately handle target and non-target class knowledge. (13 sources)",
                "text": "\nThe loss function is a crucial component in knowledge distillation as it determines how effectively dark knowledge is transferred from teacher to student networks. The standard formulation, introduced by Hinton et al., combines two objectives: (1) the traditional cross-entropy loss between student predictions and ground truth labels, and (2) a distillation loss that aligns the student's probability distribution with the teacher's softened distribution <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.\n\nThis combined loss function can be formally expressed as:\n\n```\nL = \u03bb\u2081L_CE(y, P_S) + \u03bb\u2082L_KD(P_T, P_S)\n```\n\nwhere L_CE is the cross-entropy loss with hard labels, L_KD is the Kullback-Leibler (KL) divergence between teacher and student probability distributions, and \u03bb\u2081 and \u03bb\u2082 are hyperparameters that balance these two components <Paper corpusId=\"235489777\" paperTitle=\"(Zhao et al., 2021)\" isShortName></Paper> <Paper corpusId=\"237453494\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\nThe KL divergence loss term specifically targets the transfer of dark knowledge by measuring the difference between the teacher's and student's softened probability distributions:\n\n```\nL_KD = T\u00b2 * KL(softmax(z_T/T), softmax(z_S/T))\n```\n\nwhere z_T and z_S are the logits from teacher and student networks, respectively, and T is the temperature parameter <Paper corpusId=\"219687782\" paperTitle=\"(Trofimov et al., 2020)\" isShortName></Paper>. The T\u00b2 factor is crucial for scaling gradients appropriately, ensuring that gradients from both hard and soft targets are comparable in magnitude <Paper corpusId=\"219687782\" paperTitle=\"(Trofimov et al., 2020)\" isShortName></Paper>.\n\nMany implementations use \u03b1 and \u03b2 as weighting coefficients instead of \u03bb\u2081 and \u03bb\u2082, with the loss function expressed as:\n\n```\nL = \u03b1L_soft + \u03b2L_hard\n```\n\nwhere L_soft is the KL divergence loss and L_hard is the standard cross-entropy loss <Paper corpusId=\"266555759\" paperTitle=\"(Gupta et al., 2023)\" isShortName></Paper> <Paper corpusId=\"3759373\" paperTitle=\"(Hashemi et al., 2018)\" isShortName></Paper>.\n\nRecent research has proposed more sophisticated loss formulations. Zhao et al. introduced a decoupled approach that separates the KL divergence into target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD) components <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"247476179\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper>. This decoupling allows more flexible and efficient transfer of dark knowledge by addressing the limitations of the traditional coupled formulation, which can suppress the effectiveness of non-target class knowledge distillation <Paper corpusId=\"247476179\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper>.\n\nBeyond logit-based approaches, some loss functions incorporate feature-based knowledge transfer. Zhang et al. proposed a hint learning loss that encourages similarity between corresponding intermediate representations of teacher and student networks <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>. Their formulation combines this hint learning loss with the traditional KL divergence:\n\n```\nL = L_GT + \u03bb_ht(L_HT + L_KD)\n```\n\nwhere L_GT is the ground truth loss, L_HT is the hint learning loss, and L_KD is the KL divergence loss <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>.\n\nAnother innovative approach is the top score difference (TSD) loss, which creates softer distributions by penalizing the teacher for having excessive confidence in its primary prediction <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"54986302\" paperTitle=\"(Yang et al., 2018)\" isShortName></Paper>. This encourages the teacher to distribute confidence among secondary classes, resulting in more informative soft targets <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>.\n\nFor multi-teacher distillation scenarios, Chen et al. proposed ensemble-based approaches where outputs from multiple teachers are combined to provide stronger guidance for the student model <Paper corpusId=\"232147231\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>. In these setups, the loss function typically includes terms for each teacher-student pair.\n\nSome researchers have also explored alternative divergence measures beyond KL divergence, such as mean squared error <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233033712\" paperTitle=\"(Fang et al., 2021)\" isShortName></Paper> and Jensen-Shannon divergence <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"209405263\" paperTitle=\"(Yin et al., 2019)\" isShortName></Paper>, finding that these can sometimes provide better performance for specific applications.",
                "citations": [
                    {
                        "id": "(Hinton et al., 2015)",
                        "snippets": [
                            "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                        ],
                        "paper": {
                            "corpus_id": 7200347,
                            "title": "Distilling the Knowledge in a Neural Network",
                            "authors": [
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "49959210",
                                    "name": "J. Dean"
                                }
                            ],
                            "year": 2015,
                            "venue": "arXiv.org",
                            "n_citations": 19742
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhao et al., 2021)",
                        "snippets": [
                            "The idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels.\n\nLet us denote x, y as the input of the DNN and one-hot labels of our architecture. \u03c6 s and \u03c6 t represent the student network and teacher network with parameters W s and W t respectively. Given an input image x, the teacher network outputs the final predictions as P T which are obtained by applying the softmax function on the un-normalized log proba-bility values a T , i.e. P T = sof tmax(a T ). Similarly, the same image is fed into the student network to obtain the predictions P S = sof tmax(a S ).\n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss:\n\nIt contains the relative probabilities of incorrect classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4, the student network will be trained to optimize the following loss function:\n\nwhere \u03bb is a second hyper parameter controlling the trade-off between the two losses."
                        ],
                        "paper": {
                            "corpus_id": 235489777,
                            "title": "Knowledge Distillation via Instance-level Sequence Learning",
                            "authors": [
                                {
                                    "authorId": "50981688",
                                    "name": "Haoran Zhao"
                                },
                                {
                                    "authorId": "144326521",
                                    "name": "Xin Sun"
                                },
                                {
                                    "authorId": "1964397",
                                    "name": "Junyu Dong"
                                },
                                {
                                    "authorId": "2087106420",
                                    "name": "Zihe Dong"
                                },
                                {
                                    "authorId": "2108144626",
                                    "name": "Qiong Li"
                                }
                            ],
                            "year": 2021,
                            "venue": "Knowledge-Based Systems",
                            "n_citations": 24
                        },
                        "score": 0.77490234375
                    },
                    {
                        "id": "(Li et al., 2021)",
                        "snippets": [
                            "Knowledge distillation (Hinton et al., 2015) is first introduced for transferring knowledge from high-capability networks (teacher models) to small networks (student models) via soft labels",
                            "the dark knowledge in soft labels can be helpful by transferring knowledge from head classes to tail classes",
                            "We denote x a training image with its hard label y and soft label y",
                            "The feature vector f will be sent to two linear classifiers G hard and G sof t to get output logits z hard = G hard (f ) and z sof t = G sof t (f ). Let z denote the output logits of teacher model, then the soft label is given by:\n\nwhere i is the category index and T is the temperature which is set to 2 by default. Then, the knowledge distillation loss is written as:\n\n(4) For hard label supervision, we utilize the standard cross entropy loss L ce . Thus, the final loss is the combination of these two losses:\n\nwhere both of \u03bb 1 and \u03bb 2 are the weight of each loss and set to 1 in our experiments."
                        ],
                        "paper": {
                            "corpus_id": 237453494,
                            "title": "Self Supervision to Distillation for Long-Tailed Visual Recognition",
                            "authors": [
                                {
                                    "authorId": "1652032884",
                                    "name": "Tianhao Li"
                                },
                                {
                                    "authorId": "2109120086",
                                    "name": "Limin Wang"
                                },
                                {
                                    "authorId": "39914710",
                                    "name": "Gangshan Wu"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 103
                        },
                        "score": 0.82470703125
                    },
                    {
                        "id": "(Trofimov et al., 2020)",
                        "snippets": [
                            "The knowledge distillation (KD) assumes two models: a teacher and a student. The teacher is typically a large and accurate network or an ensemble. The student is trained to fit the softmax outputs of the teacher together with ground truth labels. The idea is that outputs of the teacher capture not only the information provided by ground truth labels but also the probabilities of other classes -\"dark knowledge\". The knowledge distillation can be summarized as follows. \n\nLet z i be logits (pre-softmax activations) and q i -probabilities of classes as predicted by a neural network. Knowledge distillation smooths z i with the temperature \u03c4 \n\nNeural networks often do very confident predictions (close to 0 or 1) and smoothing helps to provide for student more information during training [11]. The KD loss is a linear combination of the logistic loss and cross-entropy between predictions of the teacher and the student \n\nwhere z T i , z S i are logits of the teacher and the student, H(p, q) = \u2212p log(q) is the cross-entropy function. The factor \u03c4 2 is used for scaling gradients of both parts of the loss function to be the same order."
                        ],
                        "paper": {
                            "corpus_id": 219687782,
                            "title": "Multi-Fidelity Neural Architecture Search With Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "144035647",
                                    "name": "I. Trofimov"
                                },
                                {
                                    "authorId": "143888131",
                                    "name": "Nikita Klyuchnikov"
                                },
                                {
                                    "authorId": "49614468",
                                    "name": "Mikhail Salnikov"
                                },
                                {
                                    "authorId": "8792436",
                                    "name": "Alexander N. Filippov"
                                },
                                {
                                    "authorId": "51139941",
                                    "name": "Evgeny Burnaev"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Access",
                            "n_citations": 15
                        },
                        "score": 0.7265625
                    },
                    {
                        "id": "(Gupta et al., 2023)",
                        "snippets": [
                            "The original knowledge distillation work primarily focuses on single-label classification. In this context, a custom loss function L is designed that considers both the hard labels from ground truth and soft labels from a teacher model. Importantly, the soft labels are obtained using a T-temperature softmax activation function. \n\nThe softmax activation function returns probabilities P (z i , T ) is defined as: \n\nTherefore, the complete loss L can be represented as: \n\nL = \u03b1L soft + \u03b2L hard (Hashemi et al., 2018) where \u03b1 and \u03b2 are hyperparameters, t i are the logits from the teacher model, s i are the logits from the student model, c i is the ground truth for the i-th instance, N is the total number of instances, and T is the temperature parameter for softmax."
                        ],
                        "paper": {
                            "corpus_id": 266555759,
                            "title": "PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models",
                            "authors": [
                                {
                                    "authorId": "2276489527",
                                    "name": "Neelesh Gupta"
                                },
                                {
                                    "authorId": "2110123989",
                                    "name": "Pengmiao Zhang"
                                },
                                {
                                    "authorId": "2243055156",
                                    "name": "Rajgopal Kannan"
                                },
                                {
                                    "authorId": "2202866066",
                                    "name": "Viktor K. Prasanna"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Conference on High Performance Extreme Computing",
                            "n_citations": 4
                        },
                        "score": 0.79345703125
                    },
                    {
                        "id": "(Hashemi et al., 2018)",
                        "snippets": [
                            "The explosion in workload complexity and the recent slow-down in Moore's law scaling call for new approaches towards efficient computing. Researchers are now beginning to use recent advances in machine learning in software optimizations, augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efficient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the first step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research."
                        ],
                        "paper": {
                            "corpus_id": 3759373,
                            "title": "Learning Memory Access Patterns",
                            "authors": [
                                {
                                    "authorId": "33798741",
                                    "name": "Milad Hashemi"
                                },
                                {
                                    "authorId": "2329092656",
                                    "name": "Kevin Swersky"
                                },
                                {
                                    "authorId": "2119124568",
                                    "name": "Jamie A. Smith"
                                },
                                {
                                    "authorId": "46369381",
                                    "name": "Grant Ayers"
                                },
                                {
                                    "authorId": "2655459",
                                    "name": "Heiner Litz"
                                },
                                {
                                    "authorId": "1698747",
                                    "name": "Jichuan Chang"
                                },
                                {
                                    "authorId": "117272782",
                                    "name": "Christos Kozyrakis"
                                },
                                {
                                    "authorId": "1770926",
                                    "name": "Parthasarathy Ranganathan"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 205
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "Knowledge distillation (Hinton et al., 2015) transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence (Hinton et al., 2015), the mean squared error (Fang et al., 2021) and the Jensen-Shannon divergence (Yin et al., 2019), etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. (Zhao et al., 2022) pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence."
                        ],
                        "paper": {
                            "corpus_id": 256900863,
                            "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK",
                            "authors": [
                                {
                                    "authorId": "2135847074",
                                    "name": "Xiongtao Zhang"
                                },
                                {
                                    "authorId": "2206403890",
                                    "name": "Zezong Yin"
                                },
                                {
                                    "authorId": "3247526",
                                    "name": "Yunliang Jiang"
                                },
                                {
                                    "authorId": "1390650781",
                                    "name": "Yizhang Jiang"
                                },
                                {
                                    "authorId": "2340356",
                                    "name": "Da-Song Sun"
                                },
                                {
                                    "authorId": "2189281",
                                    "name": "Yong Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.88525390625
                    },
                    {
                        "id": "(Zhao et al., 2022)",
                        "snippets": [
                            "State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we re-formulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the \"difficulty\" of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megviiresearch/mdistiller."
                        ],
                        "paper": {
                            "corpus_id": 247476179,
                            "title": "Decoupled Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2112526021",
                                    "name": "Borui Zhao"
                                },
                                {
                                    "authorId": "144802953",
                                    "name": "Quan Cui"
                                },
                                {
                                    "authorId": "2067622132",
                                    "name": "Renjie Song"
                                },
                                {
                                    "authorId": "2153278419",
                                    "name": "Yiyu Qiu"
                                },
                                {
                                    "authorId": "1387852255",
                                    "name": "Jiajun Liang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 549
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2020)",
                        "snippets": [
                            "This subsection illustrates the objective functions of teacher and student networks, we introduce two additional loss functions to help our model efficiently transfer dark knowledge (i.e., the global type constraints and the relevance among relations) from teacher to student. \n\nThe Teacher Network In vanilla knowledge distillation, the teacher network is trained to fit one-hot labels. However, the ultimate goal of teacher is to provide better guidances for student, rather than achieve high accuracy simply. Inspired by (Yang et al., 2018), we introduce a top score difference (TSD) loss to make the teacher's distribution softer. More concretely, we first pick up K classes with the highest confidence scores from the teacher's output, and then compute the gap between the confidence scores of the primary class and other K-1 classes",
                            "Based on the global statistics, K is set to 3 empirically. We add the penalty term to standard ground-truth loss L T GT when training the teacher, facilitating it to distribute confidence to a few secondary relations",
                            "The Student Network Typically, knowledge distillation transfers dark knowledge from the final output of teacher. (Chen et al., 2017) demonstrate that using the intermediate representation of teacher as hint can stabilize the training process and improve the final performance of student. Here, we utilize the Kullback-Leibler divergence to measure the differences of corresponding branches between the teacher and student networks as hint learning loss",
                            "Intuitively, it encourages the results of MAA to be similar with the scaled global logits of the teacher network. The loss of knowledge distillation is calculated as the sum of L HT and L KD with a weight factor \u03bb ht . As a result, the updated loss of student network is defined as follows:"
                        ],
                        "paper": {
                            "corpus_id": 212855595,
                            "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
                            "authors": [
                                {
                                    "authorId": "47295143",
                                    "name": "Zhenyu Zhang"
                                },
                                {
                                    "authorId": "2269366",
                                    "name": "Xiaobo Shu"
                                },
                                {
                                    "authorId": "48613402",
                                    "name": "Yu Bowen"
                                },
                                {
                                    "authorId": "2079682",
                                    "name": "Tingwen Liu"
                                },
                                {
                                    "authorId": "48019474",
                                    "name": "Jiapeng Zhao"
                                },
                                {
                                    "authorId": "2108645146",
                                    "name": "Quangang Li"
                                },
                                {
                                    "authorId": "48358041",
                                    "name": "Li Guo"
                                }
                            ],
                            "year": 2020,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 35
                        },
                        "score": 0.869140625
                    },
                    {
                        "id": "(Yang et al., 2018)",
                        "snippets": [
                            "We focus on the problem of training a deep neural network in generations. The flowchart is that, in order to optimize the target network (student), another network (teacher) with the same architecture is first trained, and used to provide part of supervision signals in the next stage. While this strategy leads to a higher accuracy, many aspects (e.g., why teacher-student optimization helps) still need further explorations.This paper studies this problem from a perspective of controlling the strictness in training the teacher network. Existing approaches mostly used a hard distribution (e.g., one-hot vectors) in training, leading to a strict teacher which itself has a high accuracy, but we argue that the teacher needs to be more tolerant, although this often implies a lower accuracy. The implementation is very easy, with merely an extra loss term added to the teacher network, facilitating a few secondary classes to emerge and complement to the primary class. Consequently, the teacher provides a milder supervision signal (a less peaked distribution), and makes it possible for the student to learn from inter-class similarity and potentially lower the risk of over-fitting. Experiments are performed on standard image classification tasks (CIFAR100 and ILSVRC2012). Although the teacher network behaves less powerful, the students show a persistent ability growth and eventually achieve higher classification accuracies than other competitors. Model ensemble and transfer feature extraction also verify the effectiveness of our approach."
                        ],
                        "paper": {
                            "corpus_id": 54986302,
                            "title": "Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students",
                            "authors": [
                                {
                                    "authorId": "2154170445",
                                    "name": "Chenglin Yang"
                                },
                                {
                                    "authorId": "3041937",
                                    "name": "Lingxi Xie"
                                },
                                {
                                    "authorId": "2383133",
                                    "name": "Siyuan Qiao"
                                },
                                {
                                    "authorId": "145081362",
                                    "name": "A. Yuille"
                                }
                            ],
                            "year": 2018,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 136
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2021)",
                        "snippets": [
                            "After obtaining two pre-trained domain-mixed teachers, we employ knowledge distillation (KD), a technique to distilling knowledge by minimizing the KL-divergence between outputs of these two models. Here we adapt it to extract \"dark knowledge\" from these two complementary teachers. The pipeline of multi-teacher KD is shown in Fig. 1 (b), including two pre-trained domain-mixed teachers and one student with the same network architecture as teacher. The outputs of two teachers are ensembled as a stronger guidance to supervise the training of the student model on unlabeled target data. Besides, the student model is also supervised by the labels on the small amount of labeled target data."
                        ],
                        "paper": {
                            "corpus_id": 232147231,
                            "title": "Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation",
                            "authors": [
                                {
                                    "authorId": "3461777",
                                    "name": "Shuaijun Chen"
                                },
                                {
                                    "authorId": "1641711590",
                                    "name": "Xu Jia"
                                },
                                {
                                    "authorId": "12874945",
                                    "name": "Jianzhong He"
                                },
                                {
                                    "authorId": "6658208",
                                    "name": "Yongjie Shi"
                                },
                                {
                                    "authorId": "2144167531",
                                    "name": "Jianzhuang Liu"
                                }
                            ],
                            "year": 2021,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 64
                        },
                        "score": 0.78564453125
                    },
                    {
                        "id": "(Fang et al., 2021)",
                        "snippets": [
                            "Despite exciting progress in pre-training for visual-linguistic (VL) representations, very few aspire to a small VL model. In this paper, we study knowledge distillation (KD) to effectively compress a transformer based large VL model into a small VL model. The major challenge arises from the inconsistent regional visual tokens extracted from different detectors of Teacher and Student, resulting in the misalignment of hidden representations and attention distributions. To address the problem, we retrain and adapt the Teacher by using the same region proposals from Student\u2019s detector while the features are from Teacher\u2019s own object detector. With aligned network inputs, the adapted Teacher is capable of transferring the knowledge through the intermediate representations. Specifically, we use the mean square error loss to mimic the attention distribution inside the transformer block, and present a token-wise noise contrastive loss to align the hidden state by contrasting with negative representations stored in a sample queue. To this end, we show that our proposed distillation significantly improves the performance of small VL models on image captioning and visual question answering tasks. It reaches 120.8 in CIDEr score on COCO captioning, an improvement of 5.1 over its non-distilled counterpart; and an accuracy of 69.8 on VQA 2.0, a 0.8 gain from the baseline. Our extensive experiments and ablations confirm the effectiveness of VL distillation in both pre-training and fine-tuning stages."
                        ],
                        "paper": {
                            "corpus_id": 233033712,
                            "title": "Compressing Visual-linguistic Model via Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2113601486",
                                    "name": "Zhiyuan Fang"
                                },
                                {
                                    "authorId": "2110257737",
                                    "name": "Jianfeng Wang"
                                },
                                {
                                    "authorId": "2148941781",
                                    "name": "Xiaowei Hu"
                                },
                                {
                                    "authorId": "29957038",
                                    "name": "Lijuan Wang"
                                },
                                {
                                    "authorId": "1784500",
                                    "name": "Yezhou Yang"
                                },
                                {
                                    "authorId": "2145253136",
                                    "name": "Zicheng Liu"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 99
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yin et al., 2019)",
                        "snippets": [
                            "We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We ``invert'' a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance - (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning."
                        ],
                        "paper": {
                            "corpus_id": 209405263,
                            "title": "Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion",
                            "authors": [
                                {
                                    "authorId": "1989015",
                                    "name": "Hongxu Yin"
                                },
                                {
                                    "authorId": "2824500",
                                    "name": "Pavlo Molchanov"
                                },
                                {
                                    "authorId": "49969902",
                                    "name": "Zhizhong Li"
                                },
                                {
                                    "authorId": "2974008",
                                    "name": "J. \u00c1lvarez"
                                },
                                {
                                    "authorId": "36508529",
                                    "name": "Arun Mallya"
                                },
                                {
                                    "authorId": "2433269",
                                    "name": "Derek Hoiem"
                                },
                                {
                                    "authorId": "144874163",
                                    "name": "N. Jha"
                                },
                                {
                                    "authorId": "1690538",
                                    "name": "J. Kautz"
                                }
                            ],
                            "year": 2019,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 569
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Additional Technical Implementation Details",
                "tldr": "Knowledge distillation implementation requires careful consideration of various technical details beyond the core components, including transfer set selection, learning rate adjustments, and specialized techniques that enhance the transfer of dark knowledge. (18 sources)",
                "text": "\nHere are key technical implementation details that practitioners should consider when implementing knowledge distillation:\n\n1. **Transfer Set Selection**: The data used for distillation (the \"transfer set\") can be either the original training data or arbitrary unlabeled data. To maximize information transfer per sample, it's beneficial to generate soft labels with high entropy (non-peaky distributions) <Paper corpusId=\"159041346\" paperTitle=\"(Nayak et al., 2019)\" isShortName></Paper>.\n\n2. **Learning Rate Considerations**: Due to the softer, less peaky nature of teacher labels, the training gradients typically have lower variance, enabling the use of higher learning rates which can lead to faster convergence <Paper corpusId=\"159041346\" paperTitle=\"(Nayak et al., 2019)\" isShortName></Paper>.\n\n3. **Top Score Difference (TSD)**: This technique creates softer distributions by penalizing the teacher for having excessive confidence in its primary prediction, encouraging it to distribute confidence among secondary classes and producing more informative soft targets <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"54986302\" paperTitle=\"(Yang et al., 2018)\" isShortName></Paper>.\n\n4. **Alternative Divergence Measures**: While KL divergence is the standard approach, some implementations use mean squared error <Paper corpusId=\"233033712\" paperTitle=\"(Fang et al., 2021)\" isShortName></Paper> or Jensen-Shannon divergence <Paper corpusId=\"209405263\" paperTitle=\"(Yin et al., 2019)\" isShortName></Paper> as alternative measures for aligning teacher and student distributions <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\n5. **Decoupled Knowledge Distillation**: This approach separates the KL divergence into target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD) components for more flexible knowledge transfer <Paper corpusId=\"247476179\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\n6. **Multi-Teacher Distillation**: This technique combines outputs from multiple teacher models to provide stronger guidance for the student model, particularly effective when different teachers have complementary knowledge <Paper corpusId=\"232147231\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>.\n\n7. **Intermediate Feature Distillation**: Beyond logits, knowledge can be transferred through intermediate feature representations from different layers of the teacher network <Paper corpusId=\"198179767\" paperTitle=\"(Zhao et al., 2019)\" isShortName></Paper> <Paper corpusId=\"272969060\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>.\n\n8. **Hint Learning**: This approach encourages similarity between corresponding branches of teacher and student networks, stabilizing the training process and improving final performance <Paper corpusId=\"212855595\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>.\n\n9. **Adaptive Temperature Approaches**: Instead of a fixed temperature, some methods use adaptive approaches that assign different temperatures to samples based on their characteristics <Paper corpusId=\"225040647\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>.\n\n10. **Temperature Selection Considerations**: Choosing an appropriate temperature value is critical - too small and the distribution remains peaky (like one-hot labels), too large and the distribution becomes too uniform, resulting in information loss <Paper corpusId=\"260447668\" paperTitle=\"(Cui et al., 2022)\" isShortName></Paper>.\n\n11. **Self-Distillation**: Some approaches distill knowledge from a model to itself across different distorted versions of the same data, without requiring separate teacher and student models <Paper corpusId=\"251066725\" paperTitle=\"(Liang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>.\n\n12. **Student-Aware Teacher Training**: Recent approaches focus on training teachers that are specifically designed to be \"student-friendly,\" considering the student's capacity during the teacher training process <Paper corpusId=\"231925118\" paperTitle=\"(Park et al., 2021)\" isShortName></Paper>.\n\n13. **Feature Alignment**: When teacher and student architectures differ significantly, techniques for aligning feature dimensions between networks are necessary for effective knowledge transfer <Paper corpusId=\"272969060\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>.\n\n14. **Combining Multiple Distillation Losses**: Many implementations combine different types of distillation losses (e.g., attention-based, hidden representation-based, and dark knowledge-based) to capture different aspects of the teacher's knowledge <Paper corpusId=\"231648215\" paperTitle=\"(Feng et al., 2021)\" isShortName></Paper>.\n\n15. **Progressive Distillation**: For cases with a large gap between teacher and student capacities, a sequence of intermediate-sized \"teacher assistants\" can bridge the gap and facilitate more effective knowledge transfer <Paper corpusId=\"203642130\" paperTitle=\"(Cho et al., 2019)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Nayak et al., 2019)",
                        "snippets": [
                            "The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability. For performing the knowledge distillation, one can use the training data from the target distribution or an arbitrary data. Typically, the data used to perform the distillation is called 'Transfer set'. In order to maximize the information provided per sample, we can make the soft targets to have a high entropy (non-peaky). This is generally achieved by using a high temperature at the softmax layer (Hinton et al., 2015). Also, because of non-peaky soft-labels, the training gradients computed on the loss will have less variance and enable to use higher learning rates leading to quick convergence."
                        ],
                        "paper": {
                            "corpus_id": 159041346,
                            "title": "Zero-Shot Knowledge Distillation in Deep Networks",
                            "authors": [
                                {
                                    "authorId": "143747407",
                                    "name": "Gaurav Kumar Nayak"
                                },
                                {
                                    "authorId": "2217000",
                                    "name": "Konda Reddy Mopuri"
                                },
                                {
                                    "authorId": "7155274",
                                    "name": "Vaisakh Shaj"
                                },
                                {
                                    "authorId": "144682140",
                                    "name": "R. Venkatesh Babu"
                                },
                                {
                                    "authorId": "1429640900",
                                    "name": "Anirban Chakraborty"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 245
                        },
                        "score": 0.82470703125
                    },
                    {
                        "id": "(Zhang et al., 2020)",
                        "snippets": [
                            "This subsection illustrates the objective functions of teacher and student networks, we introduce two additional loss functions to help our model efficiently transfer dark knowledge (i.e., the global type constraints and the relevance among relations) from teacher to student. \n\nThe Teacher Network In vanilla knowledge distillation, the teacher network is trained to fit one-hot labels. However, the ultimate goal of teacher is to provide better guidances for student, rather than achieve high accuracy simply. Inspired by (Yang et al., 2018), we introduce a top score difference (TSD) loss to make the teacher's distribution softer. More concretely, we first pick up K classes with the highest confidence scores from the teacher's output, and then compute the gap between the confidence scores of the primary class and other K-1 classes",
                            "Based on the global statistics, K is set to 3 empirically. We add the penalty term to standard ground-truth loss L T GT when training the teacher, facilitating it to distribute confidence to a few secondary relations",
                            "The Student Network Typically, knowledge distillation transfers dark knowledge from the final output of teacher. (Chen et al., 2017) demonstrate that using the intermediate representation of teacher as hint can stabilize the training process and improve the final performance of student. Here, we utilize the Kullback-Leibler divergence to measure the differences of corresponding branches between the teacher and student networks as hint learning loss",
                            "Intuitively, it encourages the results of MAA to be similar with the scaled global logits of the teacher network. The loss of knowledge distillation is calculated as the sum of L HT and L KD with a weight factor \u03bb ht . As a result, the updated loss of student network is defined as follows:"
                        ],
                        "paper": {
                            "corpus_id": 212855595,
                            "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
                            "authors": [
                                {
                                    "authorId": "47295143",
                                    "name": "Zhenyu Zhang"
                                },
                                {
                                    "authorId": "2269366",
                                    "name": "Xiaobo Shu"
                                },
                                {
                                    "authorId": "48613402",
                                    "name": "Yu Bowen"
                                },
                                {
                                    "authorId": "2079682",
                                    "name": "Tingwen Liu"
                                },
                                {
                                    "authorId": "48019474",
                                    "name": "Jiapeng Zhao"
                                },
                                {
                                    "authorId": "2108645146",
                                    "name": "Quangang Li"
                                },
                                {
                                    "authorId": "48358041",
                                    "name": "Li Guo"
                                }
                            ],
                            "year": 2020,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 35
                        },
                        "score": 0.869140625
                    },
                    {
                        "id": "(Yang et al., 2018)",
                        "snippets": [
                            "We focus on the problem of training a deep neural network in generations. The flowchart is that, in order to optimize the target network (student), another network (teacher) with the same architecture is first trained, and used to provide part of supervision signals in the next stage. While this strategy leads to a higher accuracy, many aspects (e.g., why teacher-student optimization helps) still need further explorations.This paper studies this problem from a perspective of controlling the strictness in training the teacher network. Existing approaches mostly used a hard distribution (e.g., one-hot vectors) in training, leading to a strict teacher which itself has a high accuracy, but we argue that the teacher needs to be more tolerant, although this often implies a lower accuracy. The implementation is very easy, with merely an extra loss term added to the teacher network, facilitating a few secondary classes to emerge and complement to the primary class. Consequently, the teacher provides a milder supervision signal (a less peaked distribution), and makes it possible for the student to learn from inter-class similarity and potentially lower the risk of over-fitting. Experiments are performed on standard image classification tasks (CIFAR100 and ILSVRC2012). Although the teacher network behaves less powerful, the students show a persistent ability growth and eventually achieve higher classification accuracies than other competitors. Model ensemble and transfer feature extraction also verify the effectiveness of our approach."
                        ],
                        "paper": {
                            "corpus_id": 54986302,
                            "title": "Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students",
                            "authors": [
                                {
                                    "authorId": "2154170445",
                                    "name": "Chenglin Yang"
                                },
                                {
                                    "authorId": "3041937",
                                    "name": "Lingxi Xie"
                                },
                                {
                                    "authorId": "2383133",
                                    "name": "Siyuan Qiao"
                                },
                                {
                                    "authorId": "145081362",
                                    "name": "A. Yuille"
                                }
                            ],
                            "year": 2018,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 136
                        },
                        "score": 0
                    },
                    {
                        "id": "(Fang et al., 2021)",
                        "snippets": [
                            "Despite exciting progress in pre-training for visual-linguistic (VL) representations, very few aspire to a small VL model. In this paper, we study knowledge distillation (KD) to effectively compress a transformer based large VL model into a small VL model. The major challenge arises from the inconsistent regional visual tokens extracted from different detectors of Teacher and Student, resulting in the misalignment of hidden representations and attention distributions. To address the problem, we retrain and adapt the Teacher by using the same region proposals from Student\u2019s detector while the features are from Teacher\u2019s own object detector. With aligned network inputs, the adapted Teacher is capable of transferring the knowledge through the intermediate representations. Specifically, we use the mean square error loss to mimic the attention distribution inside the transformer block, and present a token-wise noise contrastive loss to align the hidden state by contrasting with negative representations stored in a sample queue. To this end, we show that our proposed distillation significantly improves the performance of small VL models on image captioning and visual question answering tasks. It reaches 120.8 in CIDEr score on COCO captioning, an improvement of 5.1 over its non-distilled counterpart; and an accuracy of 69.8 on VQA 2.0, a 0.8 gain from the baseline. Our extensive experiments and ablations confirm the effectiveness of VL distillation in both pre-training and fine-tuning stages."
                        ],
                        "paper": {
                            "corpus_id": 233033712,
                            "title": "Compressing Visual-linguistic Model via Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2113601486",
                                    "name": "Zhiyuan Fang"
                                },
                                {
                                    "authorId": "2110257737",
                                    "name": "Jianfeng Wang"
                                },
                                {
                                    "authorId": "2148941781",
                                    "name": "Xiaowei Hu"
                                },
                                {
                                    "authorId": "29957038",
                                    "name": "Lijuan Wang"
                                },
                                {
                                    "authorId": "1784500",
                                    "name": "Yezhou Yang"
                                },
                                {
                                    "authorId": "2145253136",
                                    "name": "Zicheng Liu"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 99
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yin et al., 2019)",
                        "snippets": [
                            "We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We ``invert'' a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance - (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning."
                        ],
                        "paper": {
                            "corpus_id": 209405263,
                            "title": "Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion",
                            "authors": [
                                {
                                    "authorId": "1989015",
                                    "name": "Hongxu Yin"
                                },
                                {
                                    "authorId": "2824500",
                                    "name": "Pavlo Molchanov"
                                },
                                {
                                    "authorId": "49969902",
                                    "name": "Zhizhong Li"
                                },
                                {
                                    "authorId": "2974008",
                                    "name": "J. \u00c1lvarez"
                                },
                                {
                                    "authorId": "36508529",
                                    "name": "Arun Mallya"
                                },
                                {
                                    "authorId": "2433269",
                                    "name": "Derek Hoiem"
                                },
                                {
                                    "authorId": "144874163",
                                    "name": "N. Jha"
                                },
                                {
                                    "authorId": "1690538",
                                    "name": "J. Kautz"
                                }
                            ],
                            "year": 2019,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 569
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "Knowledge distillation (Hinton et al., 2015) transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence (Hinton et al., 2015), the mean squared error (Fang et al., 2021) and the Jensen-Shannon divergence (Yin et al., 2019), etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. (Zhao et al., 2022) pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence."
                        ],
                        "paper": {
                            "corpus_id": 256900863,
                            "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK",
                            "authors": [
                                {
                                    "authorId": "2135847074",
                                    "name": "Xiongtao Zhang"
                                },
                                {
                                    "authorId": "2206403890",
                                    "name": "Zezong Yin"
                                },
                                {
                                    "authorId": "3247526",
                                    "name": "Yunliang Jiang"
                                },
                                {
                                    "authorId": "1390650781",
                                    "name": "Yizhang Jiang"
                                },
                                {
                                    "authorId": "2340356",
                                    "name": "Da-Song Sun"
                                },
                                {
                                    "authorId": "2189281",
                                    "name": "Yong Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.88525390625
                    },
                    {
                        "id": "(Zhao et al., 2022)",
                        "snippets": [
                            "State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we re-formulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the \"difficulty\" of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megviiresearch/mdistiller."
                        ],
                        "paper": {
                            "corpus_id": 247476179,
                            "title": "Decoupled Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2112526021",
                                    "name": "Borui Zhao"
                                },
                                {
                                    "authorId": "144802953",
                                    "name": "Quan Cui"
                                },
                                {
                                    "authorId": "2067622132",
                                    "name": "Renjie Song"
                                },
                                {
                                    "authorId": "2153278419",
                                    "name": "Yiyu Qiu"
                                },
                                {
                                    "authorId": "1387852255",
                                    "name": "Jiajun Liang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 549
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2021)",
                        "snippets": [
                            "After obtaining two pre-trained domain-mixed teachers, we employ knowledge distillation (KD), a technique to distilling knowledge by minimizing the KL-divergence between outputs of these two models. Here we adapt it to extract \"dark knowledge\" from these two complementary teachers. The pipeline of multi-teacher KD is shown in Fig. 1 (b), including two pre-trained domain-mixed teachers and one student with the same network architecture as teacher. The outputs of two teachers are ensembled as a stronger guidance to supervise the training of the student model on unlabeled target data. Besides, the student model is also supervised by the labels on the small amount of labeled target data."
                        ],
                        "paper": {
                            "corpus_id": 232147231,
                            "title": "Semi-supervised Domain Adaptation based on Dual-level Domain Mixing for Semantic Segmentation",
                            "authors": [
                                {
                                    "authorId": "3461777",
                                    "name": "Shuaijun Chen"
                                },
                                {
                                    "authorId": "1641711590",
                                    "name": "Xu Jia"
                                },
                                {
                                    "authorId": "12874945",
                                    "name": "Jianzhong He"
                                },
                                {
                                    "authorId": "6658208",
                                    "name": "Yongjie Shi"
                                },
                                {
                                    "authorId": "2144167531",
                                    "name": "Jianzhuang Liu"
                                }
                            ],
                            "year": 2021,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 64
                        },
                        "score": 0.78564453125
                    },
                    {
                        "id": "(Zhao et al., 2019)",
                        "snippets": [
                            "Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process.\n\nLet us respectively denote x and y as the input of the DNNs and one-hot labels of our architecture. We let P T be the teacher network's softmax output as P T = sof tmax(a T ). Specifically, P T is obtained by applying softmax function on the un-normalized log probability values a T . Similarly, the same image fed to the student network to get the predictions P S = sof tmax(a S ). In the intermediate layers of the DNN, we denote the activation tensor A \u2208 R C\u00d7X\u00d7W with its corresponding layer. The pairs of teacher and student attention maps are denoted as F (A j T ) and F (A j S ) in vectorized form respectively [21]. And the standard cross entropy is denoted as H. Hinton et al. [19] extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 the student network will be trained to optimize the following loss function:"
                        ],
                        "paper": {
                            "corpus_id": 198179767,
                            "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching",
                            "authors": [
                                {
                                    "authorId": "50981688",
                                    "name": "Haoran Zhao"
                                },
                                {
                                    "authorId": "144326521",
                                    "name": "Xin Sun"
                                },
                                {
                                    "authorId": "1964397",
                                    "name": "Junyu Dong"
                                },
                                {
                                    "authorId": "10944885",
                                    "name": "Changrui Chen"
                                },
                                {
                                    "authorId": "2087106420",
                                    "name": "Zihe Dong"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE Transactions on Cybernetics",
                            "n_citations": 59
                        },
                        "score": 0.70361328125
                    },
                    {
                        "id": "(Huang et al., 2024)",
                        "snippets": [
                            "Logits-based KD was first proposed in [9], where the student learns the logits knowledge from the teacher through KL divergence. In contrast to direct supervised learning through ground truth, logits-based KD using soft labels can elicit more 'dark knowledge' [9], thereby enhancing the performance of the student network without altering its architecture. The logits distillation losses are represented as follows: \n\nP j denotes the class probability derived from the logits z \u2208 R C after undergoing Softmax. \u03c4 is the temperature scaling hyper-parameter which enables the production of different probability distributions: \n\nFeature-based KD transfers the knowledge of intermediate layers features from the teacher to the student. Generally, we can formulate such distillation methods as: \n\nwhere F t i and F s i are the corresponding features of the teacher and student at i-th layer. w(\u2022) is a mapping function that aligns the dimensions of features from the student to the teacher."
                        ],
                        "paper": {
                            "corpus_id": 272969060,
                            "title": "Harmonizing knowledge Transfer in Neural Network with Unified Distillation",
                            "authors": [
                                {
                                    "authorId": "2180087917",
                                    "name": "Yaomin Huang"
                                },
                                {
                                    "authorId": "2257133494",
                                    "name": "Zaoming Yan"
                                },
                                {
                                    "authorId": "2242115637",
                                    "name": "Chaomin Shen"
                                },
                                {
                                    "authorId": "152786529",
                                    "name": "Faming Fang"
                                },
                                {
                                    "authorId": "2323414943",
                                    "name": "Guixu Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "European Conference on Computer Vision",
                            "n_citations": 0
                        },
                        "score": 0.8115234375
                    },
                    {
                        "id": "(Li et al., 2020)",
                        "snippets": [
                            "Wu et al. train the student network by taking the output of the teacher network as the soft target, and achieve the knowledge transfer by replacing the L2 loss with cross entropy loss (Wu et al., 2015). Adriana Romero et al. fit the complexity of the teacher network by inputting more no-tag data into the student network. Junho Yim et al. optimized the knowledge transfer by refining knowledge distillation to the layer (Yim et al., 2017)."
                        ],
                        "paper": {
                            "corpus_id": 225040647,
                            "title": "Robustness-aware 2-bit quantization with real-time performance for neural network",
                            "authors": [
                                {
                                    "authorId": "2109348932",
                                    "name": "Xiaobin Li"
                                },
                                {
                                    "authorId": "2292508",
                                    "name": "Hongxu Jiang"
                                },
                                {
                                    "authorId": "2118018442",
                                    "name": "Shuangxi Huang"
                                },
                                {
                                    "authorId": "152978298",
                                    "name": "Fangzheng Tian"
                                }
                            ],
                            "year": 2020,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.7265625
                    },
                    {
                        "id": "(Cui et al., 2022)",
                        "snippets": [
                            "Compared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part. \n\n3) Loss Function for KD. The proposed KD-based neural network lightweight method is depicted in Fig. 3. After the teacher network is trained, the student network is trained using a combination of distillation loss and ordinary MSE loss."
                        ],
                        "paper": {
                            "corpus_id": 260447668,
                            "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback",
                            "authors": [
                                {
                                    "authorId": "2172485573",
                                    "name": "Yiming Cui"
                                },
                                {
                                    "authorId": "47093519",
                                    "name": "Jiajia Guo"
                                },
                                {
                                    "authorId": "2113999930",
                                    "name": "Zheng Cao"
                                },
                                {
                                    "authorId": "120710335",
                                    "name": "Huaze Tang"
                                },
                                {
                                    "authorId": "2257212132",
                                    "name": "Chao-Kai Wen"
                                },
                                {
                                    "authorId": "2227268421",
                                    "name": "Shi Jin"
                                },
                                {
                                    "authorId": "2288090155",
                                    "name": "Xin Wang"
                                },
                                {
                                    "authorId": "2240356164",
                                    "name": "Xiaolin Hou"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Transactions on Communications",
                            "n_citations": 3
                        },
                        "score": 0.83837890625
                    },
                    {
                        "id": "(Liang et al., 2022)",
                        "snippets": [
                            "Instead of imposing a fixed prior distribution, knowledge distillation was first proposed by Hinton in [12] to provide sample-level non-uniform soft labels. They demonstrated that the \"dark knowledge\" lies in the output distributions from a large capacity teacher network and benefits the student's representation learning. Recent works mainly explored to better transfer the \"dark knowledge\" and improve the efficiency from various aspects, such as reducing the difference between the teacher and student [3,(Cho et al., 2019)(Mirzadeh et al., 2019)(Zhu et al., 2021), designing student-friendly architecture [16,(Park et al., 2021), improving the distillation efficiency (Furlanello et al., 2018)(Kim et al., 2020)(Xu et al., 2019)(Yun et al., 2020) and explaining the distillation's working mechanism [1](Stanton et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 251066725,
                            "title": "Efficient One Pass Self-distillation with Zipf's Label Smoothing",
                            "authors": [
                                {
                                    "authorId": "1387852255",
                                    "name": "Jiajun Liang"
                                },
                                {
                                    "authorId": "2304362074",
                                    "name": "Linze Li"
                                },
                                {
                                    "authorId": "2056410266",
                                    "name": "Z. Bing"
                                },
                                {
                                    "authorId": "2112526021",
                                    "name": "Borui Zhao"
                                },
                                {
                                    "authorId": "2179286812",
                                    "name": "Yao Tang"
                                },
                                {
                                    "authorId": "2179192518",
                                    "name": "Bo Lin"
                                },
                                {
                                    "authorId": "1934546",
                                    "name": "Haoqiang Fan"
                                }
                            ],
                            "year": 2022,
                            "venue": "European Conference on Computer Vision",
                            "n_citations": 19
                        },
                        "score": 0.869140625
                    },
                    {
                        "id": "(Furlanello et al., 2018)",
                        "snippets": [
                            "Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction."
                        ],
                        "paper": {
                            "corpus_id": 4110009,
                            "title": "Born Again Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2067208583",
                                    "name": "Tommaso Furlanello"
                                },
                                {
                                    "authorId": "32219137",
                                    "name": "Zachary Chase Lipton"
                                },
                                {
                                    "authorId": "143902495",
                                    "name": "Michael Tschannen"
                                },
                                {
                                    "authorId": "7326223",
                                    "name": "L. Itti"
                                },
                                {
                                    "authorId": "2047844",
                                    "name": "Anima Anandkumar"
                                }
                            ],
                            "year": 2018,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1034
                        },
                        "score": 0
                    },
                    {
                        "id": "(Park et al., 2021)",
                        "snippets": [
                            "We propose a novel knowledge distillation approach to facilitate the transfer of dark knowledge from a teacher to a student. Contrary to most of the existing methods that rely on effective training of student models given pretrained teachers, we aim to learn the teacher models that are friendly to students and, consequently, more appropriate for knowledge transfer. In other words, at the time of optimizing a teacher model, the proposed algorithm learns the student branches jointly to obtain student-friendly representations. Since the main goal of our approach lies in training teacher models and the subsequent knowledge distillation procedure is straightforward, most of the existing knowledge distillation methods can adopt this technique to improve the performance of diverse student models in terms of accuracy and convergence speed. The proposed algorithm demonstrates outstanding accuracy in several well-known knowledge distillation techniques with various combinations of teacher and student models even in the case that their architectures are heterogeneous and there is no prior knowledge about student models at the time of training teacher networks."
                        ],
                        "paper": {
                            "corpus_id": 231925118,
                            "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2122903402",
                                    "name": "D. Park"
                                },
                                {
                                    "authorId": "9959922",
                                    "name": "Moonsu Cha"
                                },
                                {
                                    "authorId": "48366572",
                                    "name": "C. Jeong"
                                },
                                {
                                    "authorId": "122204255",
                                    "name": "Daesin Kim"
                                },
                                {
                                    "authorId": "40030651",
                                    "name": "Bohyung Han"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 101
                        },
                        "score": 0
                    },
                    {
                        "id": "(Feng et al., 2021)",
                        "snippets": [
                            "For dark knowledge based distillation, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits as follows:\n\nwhere g s and g t are the logits from the student and teacher respectively. T KD denotes the temperature value which controls the smoothness of the output distribution. Note that for the regression problem, the above loss is reformulated as the mean square error between the student's and the teacher's logits.\n\nWe combined the above three types of loss as our final KD loss, namely: L KD = L att + L hidden + L dark ."
                        ],
                        "paper": {
                            "corpus_id": 231648215,
                            "title": "Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2152751276",
                                    "name": "Lingyun Feng"
                                },
                                {
                                    "authorId": "2642333",
                                    "name": "Minghui Qiu"
                                },
                                {
                                    "authorId": "2110479359",
                                    "name": "Yaliang Li"
                                },
                                {
                                    "authorId": "16215052",
                                    "name": "Haitao Zheng"
                                },
                                {
                                    "authorId": "2115382645",
                                    "name": "Ying Shen"
                                }
                            ],
                            "year": 2021,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 10
                        },
                        "score": 0.73583984375
                    },
                    {
                        "id": "(Cho et al., 2019)",
                        "snippets": [
                            "In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don\u2019t make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher\u2019s training early. Our results generalize across datasets and models."
                        ],
                        "paper": {
                            "corpus_id": 203642130,
                            "title": "On the Efficacy of Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "39878282",
                                    "name": "Jang Hyun Cho"
                                },
                                {
                                    "authorId": "73710317",
                                    "name": "B. Hariharan"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 619
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mirzadeh et al., 2019)",
                        "snippets": [
                            "Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."
                        ],
                        "paper": {
                            "corpus_id": 212908749,
                            "title": "Improved Knowledge Distillation via Teacher Assistant",
                            "authors": [
                                {
                                    "authorId": "145156788",
                                    "name": "Seyed Iman Mirzadeh"
                                },
                                {
                                    "authorId": "1682124",
                                    "name": "Mehrdad Farajtabar"
                                },
                                {
                                    "authorId": "2112839418",
                                    "name": "Ang Li"
                                },
                                {
                                    "authorId": "153898744",
                                    "name": "Nir Levine"
                                },
                                {
                                    "authorId": "2063980545",
                                    "name": "Akihiro Matsukawa"
                                },
                                {
                                    "authorId": "144600887",
                                    "name": "H. Ghasemzadeh"
                                }
                            ],
                            "year": 2019,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 1081
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.228021
    }
}