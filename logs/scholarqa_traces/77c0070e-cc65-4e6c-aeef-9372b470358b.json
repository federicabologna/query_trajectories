{
    "query": "Provide a detailed timeline of the major LLaVA model releases (e.g., LLaVA 1.5, LLaVA-NeXT, LLaVA-OneVision) and describe the specific improvements and innovations introduced with each iteration.",
    "user_id": "lib_user",
    "task_id": "77c0070e-cc65-4e6c-aeef-9372b470358b",
    "timestamp": "2025-06-23T22:37:33.667197",
    "n_retrieval": 256,
    "n_retrieved": 253,
    "n_candidates": 19,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.284565,
    "decomposed_query": {
        "rewritten_query": "Timeline of major LLaVA model releases (LLaVA 1.5, LLaVA-NeXT, LLaVA-OneVision) and their specific improvements and innovations.",
        "keyword_query": "LLaVA model releases timeline improvements innovations LLaVA 1.5 LLaVA-NeXT LLaVA-OneVision",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010707,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
            "venue": "",
            "year": 2025,
            "reference_count": 30,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.23859, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292400830",
                    "name": "S. Riggi"
                },
                {
                    "authorId": "2042077694",
                    "name": "T. Cecconello"
                },
                {
                    "authorId": "2352941747",
                    "name": "A. Pilzer"
                },
                {
                    "authorId": "2352939581",
                    "name": "S. Palazzo"
                },
                {
                    "authorId": "2299008238",
                    "name": "N. Gupta"
                },
                {
                    "authorId": "2298907506",
                    "name": "A. Hopkins"
                },
                {
                    "authorId": "2258840598",
                    "name": "C. Trigilio"
                },
                {
                    "authorId": "2349648144",
                    "name": "G. Umana"
                }
            ],
            "abstract": "The advent of next-generation radio telescopes is set to transform radio astronomy by producing massive data volumes that challenge traditional processing methods. Deep learning techniques have shown strong potential in automating radio analysis tasks, yet are often constrained by the limited availability of large annotated datasets. Recent progress in self-supervised learning has led to foundational radio vision models, but adapting them for new tasks typically requires coding expertise, limiting their accessibility to a broader astronomical community. Text-based AI interfaces offer a promising alternative by enabling task-specific queries and example-driven learning. In this context, Large Language Models (LLMs), with their remarkable zero-shot capabilities, are increasingly used in scientific domains. However, deploying large-scale models remains resource-intensive, and there is a growing demand for AI systems that can reason over both visual and textual data in astronomical analysis. This study explores small-scale Vision-Language Models (VLMs) as AI assistants for radio astronomy, combining LLM capabilities with vision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio images from multiple surveys, enriched with 38k image-caption pairs from the literature. The fine-tuned models show clear improvements over base models in radio-specific tasks, achieving ~30% F1-score gains in extended source detection, but they underperform pure vision models and exhibit ~20% drop on general multimodal tasks. Inclusion of caption data and LoRA fine-tuning enhances instruction-following and helps recover ~10% accuracy on standard benchmarks. This work lays the foundation for future advancements in radio VLMs, highlighting their potential and limitations, such as the need for better multimodal alignment, higher-quality datasets, and mitigation of catastrophic forgetting.",
            "corpus_id": 277452239,
            "sentences": [
                {
                    "corpus_id": "277452239",
                    "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
                    "text": "LLaVA (Large Language and Vision Assistant) (Liu et al., 2023) is a state-of-the-art multimodal model that integrates both visual and textual understanding, combining the capabilities of large language models (LLMs) with vision processing abilities. Its primary function is to interpret and generate responses to input that includes both images and text, making it ideal for tasks like visual question answering (VQA), image captioning, and other vision-language tasks. Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks. Various variants or specialization of the first LLaVA models have been produced so far. For example, TinyLLaVA (Zhou et al., 2024;Jia et al., 2024) is a compact refactored variant of the original LLaVA 1.5 model, designed to enable easier inclusion of alternative light vision and LLM models, thus significantly reducing overall model size and resource requirements. LLaVA-Med (Li et al., 2023a) is a specialized variant of the LLaVA model designed to assist in medical image analysis and diagnostics by fine-tuning its multimodal capabilities on medical datasets such as X-rays, MRIs, and other healthcare-related visual data.",
                    "score": 0.5779386198684945,
                    "section_title": "Model overview",
                    "char_start_offset": 10302,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 249
                        },
                        {
                            "start": 250,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1241
                        },
                        {
                            "start": 1242,
                            "end": 1550
                        },
                        {
                            "start": 1551,
                            "end": 1638
                        },
                        {
                            "start": 1639,
                            "end": 1917
                        },
                        {
                            "start": 1918,
                            "end": 2178
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8369140625
                }
            ],
            "relevance_judgement": 0.8369140625,
            "relevance_judgment_input_expanded": "# Title: Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks\n# Venue: \n# Authors: S. Riggi, T. Cecconello, A. Pilzer, S. Palazzo, N. Gupta, A. Hopkins, C. Trigilio, G. Umana\n## Abstract\nThe advent of next-generation radio telescopes is set to transform radio astronomy by producing massive data volumes that challenge traditional processing methods. Deep learning techniques have shown strong potential in automating radio analysis tasks, yet are often constrained by the limited availability of large annotated datasets. Recent progress in self-supervised learning has led to foundational radio vision models, but adapting them for new tasks typically requires coding expertise, limiting their accessibility to a broader astronomical community. Text-based AI interfaces offer a promising alternative by enabling task-specific queries and example-driven learning. In this context, Large Language Models (LLMs), with their remarkable zero-shot capabilities, are increasingly used in scientific domains. However, deploying large-scale models remains resource-intensive, and there is a growing demand for AI systems that can reason over both visual and textual data in astronomical analysis. This study explores small-scale Vision-Language Models (VLMs) as AI assistants for radio astronomy, combining LLM capabilities with vision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio images from multiple surveys, enriched with 38k image-caption pairs from the literature. The fine-tuned models show clear improvements over base models in radio-specific tasks, achieving ~30% F1-score gains in extended source detection, but they underperform pure vision models and exhibit ~20% drop on general multimodal tasks. Inclusion of caption data and LoRA fine-tuning enhances instruction-following and helps recover ~10% accuracy on standard benchmarks. This work lays the foundation for future advancements in radio VLMs, highlighting their potential and limitations, such as the need for better multimodal alignment, higher-quality datasets, and mitigation of catastrophic forgetting.\n## Model overview\nLLaVA (Large Language and Vision Assistant) (Liu et al., 2023) is a state-of-the-art multimodal model that integrates both visual and textual understanding, combining the capabilities of large language models (LLMs) with vision processing abilities. Its primary function is to interpret and generate responses to input that includes both images and text, making it ideal for tasks like visual question answering (VQA), image captioning, and other vision-language tasks. Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks. Various variants or specialization of the first LLaVA models have been produced so far. For example, TinyLLaVA (Zhou et al., 2024;Jia et al., 2024) is a compact refactored variant of the original LLaVA 1.5 model, designed to enable easier inclusion of alternative light vision and LLM models, thus significantly reducing overall model size and resource requirements. LLaVA-Med (Li et al., 2023a) is a specialized variant of the LLaVA model designed to assist in medical image analysis and diagnostics by fine-tuning its multimodal capabilities on medical datasets such as X-rays, MRIs, and other healthcare-related visual data.",
            "reference_string": "[277452239 | Riggi et al. | 2025 | Citations: 1]"
        },
        {
            "title": "HumanVLM: Foundation for Human-Scene Vision-Language Model",
            "venue": "Information Fusion",
            "year": 2024,
            "reference_count": 89,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.03034, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2082462168",
                    "name": "Dawei Dai"
                },
                {
                    "authorId": "2329189750",
                    "name": "Xu Long"
                },
                {
                    "authorId": "2136494548",
                    "name": "Yutang Li"
                },
                {
                    "authorId": "2310835404",
                    "name": "Yuanhui Zhang"
                },
                {
                    "authorId": "2147222435",
                    "name": "Shuy Xia"
                }
            ],
            "abstract": "Human-scene vision-language tasks are increasingly prevalent in diverse social applications, yet recent advancements predominantly rely on models specifically tailored to individual tasks. Emerging research indicates that large vision-language models (VLMs) can enhance performance across various downstream vision-language understanding tasks. However, general-domain models often underperform in specialized fields. This study introduces a domain-specific Large Vision-Language Model, Human-Scene Vision-Language Model (HumanVLM), designed to provide a foundation for human-scene Vision-Language tasks. Specifically, (1) we create a large-scale human-scene multimodal image-text dataset (HumanCaption-10M) sourced from the Internet to facilitate domain-specific alignment; (2) develop a captioning approach for human-centered images, capturing human faces, bodies, and backgrounds, and construct a high-quality Human-Scene image-text dataset (HumanCaptionHQ, about 311k pairs) that contain as much detailed information as possible about human; (3) Using HumanCaption-10M and HumanCaptionHQ, we train a HumanVLM. In the experiments, we then evaluate our HumanVLM across varous downstream tasks, where it demonstrates superior overall performance among multimodal models of comparable scale, particularly excelling in human-related tasks and significantly outperforming similar models, including Qwen2VL and ChatGPT-4o. HumanVLM, alongside the data introduced, will stimulate the research in human-around fields.",
            "corpus_id": 273821149,
            "sentences": [
                {
                    "corpus_id": "273821149",
                    "title": "HumanVLM: Foundation for Human-Scene Vision-Language Model",
                    "text": "Liu at el. introduced an end-to-end trained large vision-language assistant (LLaVA [51]) on instruction-following data for general purpose visual and language understanding, which gained widespread attention upon release. Subsequent research has further enhanced LLaVA's performance. For instance, LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios. LLaVA-Interactive [53] serves as a comprehensive demonstration platform, incorporating features such as image chatting, segmentation, and generation and editing capabilities, significantly expanding LLaVA's original functionalities. MoE-LLaVA [54], a sparse LVLM architecture based on Mixture of Experts (MoE), was developed to tackle performance degradation in multimodal sparse learning. MG-LLaVA [55] enhanced the model's visual processing capabilities by introducing multi-granularity visual streams, allowing it to handle features at various resolutions and object centers. \n\nLLaVA has set new standards for efficiency and effectiveness in multimodal learning and has quickly been adapted across various domains. For example, LLaVA-based models, including LLaVA-Med [18], PathChat [19], QUILT-LLaVA [56], PA-LLaVA [57], have been designed for medical image understanding, where they outperform traditional methods. Zheng et al. [58] developed the first large-scale open-source dataset, MMTab, to address the multimodal table understanding problem and trained a multifunctional tableformat LLM called Table-LLaVA. In the power sector, Wang et al. [21] proposed Power-LLaVA, a large vision-language assistant designed for reliable inspection of power transmission lines, showcasing strong capabilities in this field. In the food domain, Fnu Mohbat et al. [20] introduced LLaVA-Chef, trained on a carefully selected recipe dataset, enabling it to recognize ingredients and generate detailed recipes. In this study, we aim to construct a unified multimodal Vision-Language Model for human-scene tasks.",
                    "score": 0.5150143029420995,
                    "section_title": "Various Vision-Language Applications",
                    "char_start_offset": 8442,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 283
                        },
                        {
                            "start": 284,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 688
                        },
                        {
                            "start": 689,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 1034
                        },
                        {
                            "start": 1037,
                            "end": 1173
                        },
                        {
                            "start": 1174,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1573
                        },
                        {
                            "start": 1574,
                            "end": 1775
                        },
                        {
                            "start": 1776,
                            "end": 1957
                        },
                        {
                            "start": 1958,
                            "end": 2058
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1227,
                            "end": 1231,
                            "matchedPaperCorpusId": "258999820"
                        },
                        {
                            "start": 1242,
                            "end": 1246,
                            "matchedPaperCorpusId": "270437603"
                        },
                        {
                            "start": 1260,
                            "end": 1264,
                            "matchedPaperCorpusId": "266149936"
                        },
                        {
                            "start": 1389,
                            "end": 1393,
                            "matchedPaperCorpusId": "270391758"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8046875
                }
            ],
            "relevance_judgement": 0.8046875,
            "relevance_judgment_input_expanded": "# Title: HumanVLM: Foundation for Human-Scene Vision-Language Model\n# Venue: Information Fusion\n# Authors: Dawei Dai, Xu Long, Yutang Li, Yuanhui Zhang, Shuy Xia\n## Abstract\nHuman-scene vision-language tasks are increasingly prevalent in diverse social applications, yet recent advancements predominantly rely on models specifically tailored to individual tasks. Emerging research indicates that large vision-language models (VLMs) can enhance performance across various downstream vision-language understanding tasks. However, general-domain models often underperform in specialized fields. This study introduces a domain-specific Large Vision-Language Model, Human-Scene Vision-Language Model (HumanVLM), designed to provide a foundation for human-scene Vision-Language tasks. Specifically, (1) we create a large-scale human-scene multimodal image-text dataset (HumanCaption-10M) sourced from the Internet to facilitate domain-specific alignment; (2) develop a captioning approach for human-centered images, capturing human faces, bodies, and backgrounds, and construct a high-quality Human-Scene image-text dataset (HumanCaptionHQ, about 311k pairs) that contain as much detailed information as possible about human; (3) Using HumanCaption-10M and HumanCaptionHQ, we train a HumanVLM. In the experiments, we then evaluate our HumanVLM across varous downstream tasks, where it demonstrates superior overall performance among multimodal models of comparable scale, particularly excelling in human-related tasks and significantly outperforming similar models, including Qwen2VL and ChatGPT-4o. HumanVLM, alongside the data introduced, will stimulate the research in human-around fields.\n## Various Vision-Language Applications\nLiu at el. introduced an end-to-end trained large vision-language assistant (LLaVA [51]) on instruction-following data for general purpose visual and language understanding, which gained widespread attention upon release. Subsequent research has further enhanced LLaVA's performance. For instance, LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios. LLaVA-Interactive [53] serves as a comprehensive demonstration platform, incorporating features such as image chatting, segmentation, and generation and editing capabilities, significantly expanding LLaVA's original functionalities. MoE-LLaVA [54], a sparse LVLM architecture based on Mixture of Experts (MoE), was developed to tackle performance degradation in multimodal sparse learning. MG-LLaVA [55] enhanced the model's visual processing capabilities by introducing multi-granularity visual streams, allowing it to handle features at various resolutions and object centers. \n\nLLaVA has set new standards for efficiency and effectiveness in multimodal learning and has quickly been adapted across various domains. For example, LLaVA-based models, including LLaVA-Med [18], PathChat [19], QUILT-LLaVA [56], PA-LLaVA [57], have been designed for medical image understanding, where they outperform traditional methods. Zheng et al. [58] developed the first large-scale open-source dataset, MMTab, to address the multimodal table understanding problem and trained a multifunctional tableformat LLM called Table-LLaVA. In the power sector, Wang et al. [21] proposed Power-LLaVA, a large vision-language assistant designed for reliable inspection of power transmission lines, showcasing strong capabilities in this field. In the food domain, Fnu Mohbat et al. [20] introduced LLaVA-Chef, trained on a carefully selected recipe dataset, enabling it to recognize ingredients and generate detailed recipes. In this study, we aim to construct a unified multimodal Vision-Language Model for human-scene tasks.",
            "reference_string": "[273821149 | Dai et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs",
            "venue": "",
            "year": 2024,
            "reference_count": 65,
            "citation_count": 10,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.01818, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118051520",
                    "name": "Qizhe Zhang"
                },
                {
                    "authorId": "2292408664",
                    "name": "Aosong Cheng"
                },
                {
                    "authorId": "2331417542",
                    "name": "Ming Lu"
                },
                {
                    "authorId": "2275104296",
                    "name": "Renrui Zhang"
                },
                {
                    "authorId": "2333364107",
                    "name": "Zhiyong Zhuo"
                },
                {
                    "authorId": "2268711797",
                    "name": "Jiajun Cao"
                },
                {
                    "authorId": "2333442704",
                    "name": "Shaobo Guo"
                },
                {
                    "authorId": "2331326229",
                    "name": "Qi She"
                },
                {
                    "authorId": "2332857566",
                    "name": "Shanghang Zhang"
                }
            ],
            "abstract": "Large vision-language models (LVLMs) generally contain significantly more visual tokens than their textual counterparts, resulting in a considerable computational burden. Recent efforts have been made to tackle this issue by pruning visual tokens early within the language model. Most existing works use attention scores between text and visual tokens to assess the importance of visual tokens. However, in this study, we first analyze the text-visual attention in the language model and find that this score is not an ideal indicator for token pruning. Based on the analysis, We propose VisPruner, a plug-and-play method that utilizes visual cues for more effective token pruning in LVLMs. Specifically, we first use visual attention to select a limited number of significant tokens. Then, we remove duplicate tokens from the remaining ones based on their similarity. By retaining diverse tokens alongside the initially selected important tokens, we maximally preserve the visual information of the input image. Experimental results demonstrate that our VisPruner sustains strong performance across various VLM architectures and reduction ratios, significantly outperforming existing methods based on text-visual attention. Notably, without any training, VisPruner can reduce the FLOPs of LLaVA-1.5-7B by 91% and inference latency by 75%, while maintaining comparable performance. Our code is available at https://github.com/Theia-4869/VisPruner.",
            "corpus_id": 274437586,
            "sentences": [
                {
                    "corpus_id": "274437586",
                    "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs",
                    "text": "LLaVA-1.5 [34]. LLaVA is one of the most widely used open-source vision-language models, and its simple design, low tuning cost, and outstanding performance make it a cornerstone in the field of multi-modal models. Specifically, LLaVA employs a pre-trained CLIP as the visual encoder and Vicuna as the text decoder. A simple linear projector connects the two modules, enabling the LLM to accept visual tokens of CLIP as input. Meanwhile, visual instruction tuning allows the model to handle vision-language tasks. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement. LLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM. Video-LLaVA [32]. On the basis of image understanding, Video-LLaVA extends this capability to video comprehension. It unifies representations of images and videos through alignment before projection. The overall architecture remains consistent with LLaVA: the visual encoder encodes continuous video frames individually, and the representations are concatenated as inputs to the LLM. After joint training, Video-LLaVA is capable of understanding both image and video data. Qwen-VL [3]. Qwen-VL is another widely used opensource vision-language model. Similar to LLaVA, it includes a visual encoder (OpenCLIP) and a text decoder (Qwen LLM).",
                    "score": 0.340807846142792,
                    "section_title": "Model architectures",
                    "char_start_offset": 36898,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 15
                        },
                        {
                            "start": 16,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 315
                        },
                        {
                            "start": 316,
                            "end": 426
                        },
                        {
                            "start": 427,
                            "end": 513
                        },
                        {
                            "start": 514,
                            "end": 707
                        },
                        {
                            "start": 708,
                            "end": 724
                        },
                        {
                            "start": 725,
                            "end": 749
                        },
                        {
                            "start": 750,
                            "end": 895
                        },
                        {
                            "start": 896,
                            "end": 999
                        },
                        {
                            "start": 1000,
                            "end": 1139
                        },
                        {
                            "start": 1140,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1363
                        },
                        {
                            "start": 1364,
                            "end": 1381
                        },
                        {
                            "start": 1382,
                            "end": 1478
                        },
                        {
                            "start": 1479,
                            "end": 1563
                        },
                        {
                            "start": 1564,
                            "end": 1747
                        },
                        {
                            "start": 1748,
                            "end": 1836
                        },
                        {
                            "start": 1837,
                            "end": 1849
                        },
                        {
                            "start": 1850,
                            "end": 1914
                        },
                        {
                            "start": 1915,
                            "end": 2003
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 10,
                            "end": 14,
                            "matchedPaperCorpusId": "263672058"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78173828125
                }
            ],
            "relevance_judgement": 0.78173828125,
            "relevance_judgment_input_expanded": "# Title: Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs\n# Venue: \n# Authors: Qizhe Zhang, Aosong Cheng, Ming Lu, Renrui Zhang, Zhiyong Zhuo, Jiajun Cao, Shaobo Guo, Qi She, Shanghang Zhang\n## Abstract\nLarge vision-language models (LVLMs) generally contain significantly more visual tokens than their textual counterparts, resulting in a considerable computational burden. Recent efforts have been made to tackle this issue by pruning visual tokens early within the language model. Most existing works use attention scores between text and visual tokens to assess the importance of visual tokens. However, in this study, we first analyze the text-visual attention in the language model and find that this score is not an ideal indicator for token pruning. Based on the analysis, We propose VisPruner, a plug-and-play method that utilizes visual cues for more effective token pruning in LVLMs. Specifically, we first use visual attention to select a limited number of significant tokens. Then, we remove duplicate tokens from the remaining ones based on their similarity. By retaining diverse tokens alongside the initially selected important tokens, we maximally preserve the visual information of the input image. Experimental results demonstrate that our VisPruner sustains strong performance across various VLM architectures and reduction ratios, significantly outperforming existing methods based on text-visual attention. Notably, without any training, VisPruner can reduce the FLOPs of LLaVA-1.5-7B by 91% and inference latency by 75%, while maintaining comparable performance. Our code is available at https://github.com/Theia-4869/VisPruner.\n## Model architectures\nLLaVA-1.5 [34]. LLaVA is one of the most widely used open-source vision-language models, and its simple design, low tuning cost, and outstanding performance make it a cornerstone in the field of multi-modal models. Specifically, LLaVA employs a pre-trained CLIP as the visual encoder and Vicuna as the text decoder. A simple linear projector connects the two modules, enabling the LLM to accept visual tokens of CLIP as input. Meanwhile, visual instruction tuning allows the model to handle vision-language tasks. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement. LLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM. Video-LLaVA [32]. On the basis of image understanding, Video-LLaVA extends this capability to video comprehension. It unifies representations of images and videos through alignment before projection. The overall architecture remains consistent with LLaVA: the visual encoder encodes continuous video frames individually, and the representations are concatenated as inputs to the LLM. After joint training, Video-LLaVA is capable of understanding both image and video data. Qwen-VL [3]. Qwen-VL is another widely used opensource vision-language model. Similar to LLaVA, it includes a visual encoder (OpenCLIP) and a text decoder (Qwen LLM).",
            "reference_string": "[274437586 | Zhang et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Vision Language Models in Medicine",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 76,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01863, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2348482300",
                    "name": "Beria Chingnabe Kalpelbe"
                },
                {
                    "authorId": "2348482178",
                    "name": "Angel Gabriel Adaambiik"
                },
                {
                    "authorId": "2348517139",
                    "name": "Wei Peng"
                }
            ],
            "abstract": "With the advent of Vision-Language Models (VLMs), medical artificial intelligence (AI) has experienced significant technological progress and paradigm shifts. This survey provides an extensive review of recent advancements in Medical Vision-Language Models (Med-VLMs), which integrate visual and textual data to enhance healthcare outcomes. We discuss the foundational technology behind Med-VLMs, illustrating how general models are adapted for complex medical tasks, and examine their applications in healthcare. The transformative impact of Med-VLMs on clinical practice, education, and patient care is highlighted, alongside challenges such as data scarcity, narrow task generalization, interpretability issues, and ethical concerns like fairness, accountability, and privacy. These limitations are exacerbated by uneven dataset distribution, computational demands, and regulatory hurdles. Rigorous evaluation methods and robust regulatory frameworks are essential for safe integration into healthcare workflows. Future directions include leveraging large-scale, diverse datasets, improving cross-modal generalization, and enhancing interpretability. Innovations like federated learning, lightweight architectures, and Electronic Health Record (EHR) integration are explored as pathways to democratize access and improve clinical relevance. This review aims to provide a comprehensive understanding of Med-VLMs' strengths and limitations, fostering their ethical and balanced adoption in healthcare.",
            "corpus_id": 276776838,
            "sentences": [
                {
                    "corpus_id": "276776838",
                    "title": "Vision Language Models in Medicine",
                    "text": "The model's architecture enables rapid pre-training, requiring less than a week of computational time on single GPU setups, while maintaining high efficiency and generalization across different tasks. The research also acknowledges potential limitations, including risks associated with frozen LLM outputs and the ongoing need for advancement in multimodal AI development. \n\n2) LLaVa: The idea behind LLaVa [16] is to develop an efficient foundation language model termed \"LLaVA\" (Language and Vision Assistant), which integrates both language understanding and visual comprehension. The primary objective is to enhance the capabilities of language models in multimodal contexts, enabling them to better interpret and generate text based on visual inputs. This involves not only improving performance in traditional language tasks but also expanding functionality to include complex visual reasoning and description tasks. \n\nThe authors employed a two-step training process. Initially, they pre-trained the LLaVA model on a filtered dataset comprising 595,000 image-text pairs, sourced from the CC3M dataset. This filtering process involved selecting noun-phrases based on their frequency to ensure diverse representation across concepts in the dataset. The model was pre-trained for one epoch using a learning rate of 2e-3 and a batch size of 128. Following pre-training, the model underwent finetuning on a specialized dataset, LLaVA-Instruct-158K, for three epochs with a reduced learning rate of 2e-5 and a batch size of 32. Various optimization techniques, such as using the Adam optimizer with no weight decay and enabling BF16 and TF32, were implemented to balance speed and precision during training. \n\nThe results demonstrate that the LLaVA model exhibits significant improvements in responding to multimodal tasks compared to previous models. It effectively generates detailed and contextually appropriate descriptions of images, showcasing an ability to understand and relate visual elements to textual instructions. Performance benchmarks indicate that LLaVA not only maintains high accuracy in traditional language tasks but also excels in complex visual reasoning scenarios, establishing it as a robust tool for applications requiring integration of language and vision. \n\n3) LLaMa Adapter v2: LLaMA-Adapter [20], introduced in early 2023, was one of the pioneering attempts to extend Large Language Models (LLMs) to handle visual inputs through parameter-efficient adaptation.",
                    "score": 0.35280593420545425,
                    "section_title": "A. Sate-of-the-art VL models",
                    "char_start_offset": 15672,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 200
                        },
                        {
                            "start": 201,
                            "end": 372
                        },
                        {
                            "start": 375,
                            "end": 583
                        },
                        {
                            "start": 584,
                            "end": 755
                        },
                        {
                            "start": 756,
                            "end": 922
                        },
                        {
                            "start": 925,
                            "end": 974
                        },
                        {
                            "start": 975,
                            "end": 1108
                        },
                        {
                            "start": 1109,
                            "end": 1253
                        },
                        {
                            "start": 1254,
                            "end": 1348
                        },
                        {
                            "start": 1349,
                            "end": 1528
                        },
                        {
                            "start": 1529,
                            "end": 1708
                        },
                        {
                            "start": 1711,
                            "end": 1852
                        },
                        {
                            "start": 1853,
                            "end": 2027
                        },
                        {
                            "start": 2028,
                            "end": 2284
                        },
                        {
                            "start": 2287,
                            "end": 2491
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69677734375
                }
            ],
            "relevance_judgement": 0.69677734375,
            "relevance_judgment_input_expanded": "# Title: Vision Language Models in Medicine\n# Venue: arXiv.org\n# Authors: Beria Chingnabe Kalpelbe, Angel Gabriel Adaambiik, Wei Peng\n## Abstract\nWith the advent of Vision-Language Models (VLMs), medical artificial intelligence (AI) has experienced significant technological progress and paradigm shifts. This survey provides an extensive review of recent advancements in Medical Vision-Language Models (Med-VLMs), which integrate visual and textual data to enhance healthcare outcomes. We discuss the foundational technology behind Med-VLMs, illustrating how general models are adapted for complex medical tasks, and examine their applications in healthcare. The transformative impact of Med-VLMs on clinical practice, education, and patient care is highlighted, alongside challenges such as data scarcity, narrow task generalization, interpretability issues, and ethical concerns like fairness, accountability, and privacy. These limitations are exacerbated by uneven dataset distribution, computational demands, and regulatory hurdles. Rigorous evaluation methods and robust regulatory frameworks are essential for safe integration into healthcare workflows. Future directions include leveraging large-scale, diverse datasets, improving cross-modal generalization, and enhancing interpretability. Innovations like federated learning, lightweight architectures, and Electronic Health Record (EHR) integration are explored as pathways to democratize access and improve clinical relevance. This review aims to provide a comprehensive understanding of Med-VLMs' strengths and limitations, fostering their ethical and balanced adoption in healthcare.\n## A. Sate-of-the-art VL models\nThe model's architecture enables rapid pre-training, requiring less than a week of computational time on single GPU setups, while maintaining high efficiency and generalization across different tasks. The research also acknowledges potential limitations, including risks associated with frozen LLM outputs and the ongoing need for advancement in multimodal AI development. \n\n2) LLaVa: The idea behind LLaVa [16] is to develop an efficient foundation language model termed \"LLaVA\" (Language and Vision Assistant), which integrates both language understanding and visual comprehension. The primary objective is to enhance the capabilities of language models in multimodal contexts, enabling them to better interpret and generate text based on visual inputs. This involves not only improving performance in traditional language tasks but also expanding functionality to include complex visual reasoning and description tasks. \n\nThe authors employed a two-step training process. Initially, they pre-trained the LLaVA model on a filtered dataset comprising 595,000 image-text pairs, sourced from the CC3M dataset. This filtering process involved selecting noun-phrases based on their frequency to ensure diverse representation across concepts in the dataset. The model was pre-trained for one epoch using a learning rate of 2e-3 and a batch size of 128. Following pre-training, the model underwent finetuning on a specialized dataset, LLaVA-Instruct-158K, for three epochs with a reduced learning rate of 2e-5 and a batch size of 32. Various optimization techniques, such as using the Adam optimizer with no weight decay and enabling BF16 and TF32, were implemented to balance speed and precision during training. \n\nThe results demonstrate that the LLaVA model exhibits significant improvements in responding to multimodal tasks compared to previous models. It effectively generates detailed and contextually appropriate descriptions of images, showcasing an ability to understand and relate visual elements to textual instructions. Performance benchmarks indicate that LLaVA not only maintains high accuracy in traditional language tasks but also excels in complex visual reasoning scenarios, establishing it as a robust tool for applications requiring integration of language and vision. \n\n3) LLaMa Adapter v2: LLaMA-Adapter [20], introduced in early 2023, was one of the pioneering attempts to extend Large Language Models (LLMs) to handle visual inputs through parameter-efficient adaptation.",
            "reference_string": "[276776838 | Kalpelbe et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models",
            "venue": "International Conference on Social Networks Analysis, Management and Security",
            "year": 2023,
            "reference_count": 15,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2401.00127",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.00127, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2003814068",
                    "name": "Ashhadul Islam"
                },
                {
                    "authorId": "2363338690",
                    "name": "Md. Rafiul Biswas"
                },
                {
                    "authorId": "2034351",
                    "name": "W. Zaghouani"
                },
                {
                    "authorId": "102804035",
                    "name": "S. Belhaouari"
                },
                {
                    "authorId": "2275236843",
                    "name": "Zubair Shah"
                }
            ],
            "abstract": "The synergy of language and vision models has given rise to Large Language and Vision Assistant models (LLVAs), designed to engage users in rich conversational experiences intertwined with image-based queries. These comprehensive multimodal models seamlessly integrate vision encoders with Large Language Models (LLMs), expanding their applications in general-purpose language and visual comprehension. The advent of Large Multimodal Models (LMMs) heralds a new era in Artificial Intelligence (AI) assistance, extending the horizons of AI utilization. This paper takes a unique perspective on LMMs, exploring their efficacy in performing image classification tasks using tailored prompts designed for specific datasets. We also investigate the LLVAs zero-shot learning capabilities. Our study includes a benchmarking analysis across four diverse datasets: MNIST, Cats Vs. Dogs, Hymnoptera (Ants Vs. Bees), and an unconventional dataset comprising Pox Vs. Non-Pox skin images. The results of our experiments demonstrate the model's remarkable performance, achieving classification accuracies of 85%, 100%, 77%, and 79% for the respective datasets without any fine-tuning. To bolster our analysis, we assess the model's performance post fine-tuning for specific tasks. In one instance, fine-tuning is conducted over a dataset comprising images of faces of children with and without autism. Prior to fine-tuning, the model demonstrated a test accuracy of 55%, which significantly improved to 83% post fine-tuning. These results, coupled with our prior findings, underscore the transformative potential of LLVAs and their versatile applications in real-world scenarios.",
            "corpus_id": 266693714,
            "sentences": [
                {
                    "corpus_id": "266693714",
                    "title": "Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models",
                    "text": "While LLaVA-1.5 has shown promise in various aspects, it's important to acknowledge the limitations associated with this model: \n\n\u2022 LLaVA employs full image patches, potentially extending training iterations, with current visual resamplers unable to match its efficiency due to differences in trainable parameters [9]. \u2022 The model currently lacks the ability to process multiple images, limited by the available instruction-following data and context length [9]. \u2022 Despite reduced hallucination tendencies, LLaVA still has the potential to produce hallucinations and misinformation, mandating cautious use in critical applications [9]. In spite of these limitations, the achievements of LLaVA-1.5 show the extraordinary potential of multimodal models in the realm of visual reasoning and instruction-following tasks. Our experiments clearly demonstrate its notable accomplishments in zero-shot classification, charting a promising course for future research and innovation. With fine-tuning, the model shows even greater promise to be effective in different critical domains. While researchers work to mitigate its limitations, LLaVA-1.5 remains a guiding light of progress, providing invaluable insights and an easily reproducible framework to advance the frontiers of multimodal AI and elevate its practical utility.",
                    "score": 0.3904157507118883,
                    "section_title": "B. Conclusion",
                    "char_start_offset": 14038,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 130,
                            "end": 318
                        },
                        {
                            "start": 319,
                            "end": 462
                        },
                        {
                            "start": 463,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 816
                        },
                        {
                            "start": 817,
                            "end": 973
                        },
                        {
                            "start": 974,
                            "end": 1075
                        },
                        {
                            "start": 1076,
                            "end": 1318
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6572265625
                }
            ],
            "relevance_judgement": 0.6572265625,
            "relevance_judgment_input_expanded": "# Title: Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models\n# Venue: International Conference on Social Networks Analysis, Management and Security\n# Authors: Ashhadul Islam, Md. Rafiul Biswas, W. Zaghouani, S. Belhaouari, Zubair Shah\n## Abstract\nThe synergy of language and vision models has given rise to Large Language and Vision Assistant models (LLVAs), designed to engage users in rich conversational experiences intertwined with image-based queries. These comprehensive multimodal models seamlessly integrate vision encoders with Large Language Models (LLMs), expanding their applications in general-purpose language and visual comprehension. The advent of Large Multimodal Models (LMMs) heralds a new era in Artificial Intelligence (AI) assistance, extending the horizons of AI utilization. This paper takes a unique perspective on LMMs, exploring their efficacy in performing image classification tasks using tailored prompts designed for specific datasets. We also investigate the LLVAs zero-shot learning capabilities. Our study includes a benchmarking analysis across four diverse datasets: MNIST, Cats Vs. Dogs, Hymnoptera (Ants Vs. Bees), and an unconventional dataset comprising Pox Vs. Non-Pox skin images. The results of our experiments demonstrate the model's remarkable performance, achieving classification accuracies of 85%, 100%, 77%, and 79% for the respective datasets without any fine-tuning. To bolster our analysis, we assess the model's performance post fine-tuning for specific tasks. In one instance, fine-tuning is conducted over a dataset comprising images of faces of children with and without autism. Prior to fine-tuning, the model demonstrated a test accuracy of 55%, which significantly improved to 83% post fine-tuning. These results, coupled with our prior findings, underscore the transformative potential of LLVAs and their versatile applications in real-world scenarios.\n## B. Conclusion\nWhile LLaVA-1.5 has shown promise in various aspects, it's important to acknowledge the limitations associated with this model: \n\n\u2022 LLaVA employs full image patches, potentially extending training iterations, with current visual resamplers unable to match its efficiency due to differences in trainable parameters [9]. \u2022 The model currently lacks the ability to process multiple images, limited by the available instruction-following data and context length [9]. \u2022 Despite reduced hallucination tendencies, LLaVA still has the potential to produce hallucinations and misinformation, mandating cautious use in critical applications [9]. In spite of these limitations, the achievements of LLaVA-1.5 show the extraordinary potential of multimodal models in the realm of visual reasoning and instruction-following tasks. Our experiments clearly demonstrate its notable accomplishments in zero-shot classification, charting a promising course for future research and innovation. With fine-tuning, the model shows even greater promise to be effective in different critical domains. While researchers work to mitigate its limitations, LLaVA-1.5 remains a guiding light of progress, providing invaluable insights and an easily reproducible framework to advance the frontiers of multimodal AI and elevate its practical utility.",
            "reference_string": "[266693714 | Islam et al. | 2023 | Citations: 3]"
        },
        {
            "title": "StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 42,
            "citation_count": 31,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.10253",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.10253, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2232930725",
                    "name": "Yanda Li"
                },
                {
                    "authorId": "144876211",
                    "name": "Chi Zhang"
                },
                {
                    "authorId": "2116565951",
                    "name": "Gang Yu"
                },
                {
                    "authorId": "2051262469",
                    "name": "Zhibin Wang"
                },
                {
                    "authorId": "2107058893",
                    "name": "Bin Fu"
                },
                {
                    "authorId": "2604251",
                    "name": "Guosheng Lin"
                },
                {
                    "authorId": "12459603",
                    "name": "Chunhua Shen"
                },
                {
                    "authorId": "2232790293",
                    "name": "Ling Chen"
                },
                {
                    "authorId": "49020088",
                    "name": "Yunchao Wei"
                }
            ],
            "abstract": "The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.",
            "corpus_id": 261049617,
            "sentences": [
                {
                    "corpus_id": "261049617",
                    "title": "StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data",
                    "text": "In order to gauge the effectiveness of our data generation strategy, we have elected to utilize the open-sourced LLaVA [20,21] model as our multimodal LLM model. It should be noted that our pipeline is model-agnostic, meaning the datasets generated via our approach can be employed for training a variety of other models. To set the stage for a detailed exposition of our pipeline, this preliminary section provides an overview of the structure and training strategies used in the LLaVA model. For a more indepth understanding, the reader may refer to the original publication [21]. Architecture. The architecture of the LLaVA model elegantly combines a pre-trained LLM, specifically the Vicuna-13B [5], and a pre-trained visual encoder, known as CLIP-ViT-L/14 [28]. This fusion of text and visual processing abilities is facilitated by the incorporation of a learnable linear layer. This linear layer has the primary task of projecting the image features, developed by the CLIP encoder, into the word embedding space of the LLM. The resultant projected embeddings effectively function as tokens within the LLM, creating a synergy between text and visual data streams. It's worth mentioning that in LLaVA-1.5 [20], the linear projection layer has been substituted with a twolayer MLP. Additionally, the LLM model has been replaced with Vicuna-1.5-13B, and the input image size has been increased from 224x224 to 336x336, thereby elevating the model's multimodal capabilities to a greater extent. A detailed illustration of this model structure can be found in Figure 2 of this paper. \n\nTraining and datasets. The core of LLaVA's training process revolves around visual instruction tuning, which necessitates a triplet of data inputs: images, questions, and corresponding answers. The goal here is to prompt the model to predict the next tokens in the answers conditioned on visual tokens and instruction tokens (e.g., a question) in an auto-regressive manner. \n\nThe training of LLaVA is split into two distinct stages. Each stage has a specific focus and uses different data and optimized parameters.",
                    "score": 0.37371036733874363,
                    "section_title": "Preliminary",
                    "char_start_offset": 9581,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 582
                        },
                        {
                            "start": 583,
                            "end": 596
                        },
                        {
                            "start": 597,
                            "end": 766
                        },
                        {
                            "start": 767,
                            "end": 883
                        },
                        {
                            "start": 884,
                            "end": 1029
                        },
                        {
                            "start": 1030,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1284
                        },
                        {
                            "start": 1285,
                            "end": 1351
                        },
                        {
                            "start": 1352,
                            "end": 1495
                        },
                        {
                            "start": 1496,
                            "end": 1583
                        },
                        {
                            "start": 1586,
                            "end": 1608
                        },
                        {
                            "start": 1609,
                            "end": 1779
                        },
                        {
                            "start": 1780,
                            "end": 1959
                        },
                        {
                            "start": 1962,
                            "end": 2018
                        },
                        {
                            "start": 2019,
                            "end": 2100
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 761,
                            "end": 765,
                            "matchedPaperCorpusId": "231591445"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64892578125
                }
            ],
            "relevance_judgement": 0.64892578125,
            "relevance_judgment_input_expanded": "# Title: StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data\n# Venue: arXiv.org\n# Authors: Yanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin Fu, Guosheng Lin, Chunhua Shen, Ling Chen, Yunchao Wei\n## Abstract\nThe remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of multimodal Large Language Models (LLMs). A primary research objective of such models is to align visual and textual modalities effectively while comprehending human instructions. Current methodologies often rely on annotations derived from benchmark datasets to construct image-dialogue datasets for training purposes, akin to instruction tuning in LLMs. However, these datasets often exhibit domain bias, potentially constraining the generative capabilities of the models. In an effort to mitigate these limitations, we propose a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning. This approach harnesses the power of generative models, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content. Additionally, datasets can be arbitrarily scaled. This not only provides greater flexibility compared to existing methodologies but also significantly enhances several model capabilities. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in more than ten commonly assessed capabilities. Additionally, our model achieves state-of-the-art results across multiple widely recognized multimodal benchmarks.\n## Preliminary\nIn order to gauge the effectiveness of our data generation strategy, we have elected to utilize the open-sourced LLaVA [20,21] model as our multimodal LLM model. It should be noted that our pipeline is model-agnostic, meaning the datasets generated via our approach can be employed for training a variety of other models. To set the stage for a detailed exposition of our pipeline, this preliminary section provides an overview of the structure and training strategies used in the LLaVA model. For a more indepth understanding, the reader may refer to the original publication [21]. Architecture. The architecture of the LLaVA model elegantly combines a pre-trained LLM, specifically the Vicuna-13B [5], and a pre-trained visual encoder, known as CLIP-ViT-L/14 [28]. This fusion of text and visual processing abilities is facilitated by the incorporation of a learnable linear layer. This linear layer has the primary task of projecting the image features, developed by the CLIP encoder, into the word embedding space of the LLM. The resultant projected embeddings effectively function as tokens within the LLM, creating a synergy between text and visual data streams. It's worth mentioning that in LLaVA-1.5 [20], the linear projection layer has been substituted with a twolayer MLP. Additionally, the LLM model has been replaced with Vicuna-1.5-13B, and the input image size has been increased from 224x224 to 336x336, thereby elevating the model's multimodal capabilities to a greater extent. A detailed illustration of this model structure can be found in Figure 2 of this paper. \n\nTraining and datasets. The core of LLaVA's training process revolves around visual instruction tuning, which necessitates a triplet of data inputs: images, questions, and corresponding answers. The goal here is to prompt the model to predict the next tokens in the answers conditioned on visual tokens and instruction tokens (e.g., a question) in an auto-regressive manner. \n\nThe training of LLaVA is split into two distinct stages. Each stage has a specific focus and uses different data and optimized parameters.",
            "reference_string": "[261049617 | Li et al. | 2023 | Citations: 31]"
        },
        {
            "title": "GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.19552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305201677",
                    "name": "Hosam Elgendy"
                },
                {
                    "authorId": "2265656556",
                    "name": "Ahmed Sharshar"
                },
                {
                    "authorId": "2292596478",
                    "name": "Ahmed Aboeitta"
                },
                {
                    "authorId": "2292603520",
                    "name": "Yasser Ashraf"
                },
                {
                    "authorId": "2327863134",
                    "name": "Mohsen Guizani"
                }
            ],
            "abstract": "Detecting temporal changes in geographical landscapes is critical for applications like environmental monitoring and urban planning. While remote sensing data is abundant, existing vision-language models (VLMs) often fail to capture temporal dynamics effectively. This paper addresses these limitations by introducing an annotated dataset of video frame pairs to track evolving geographical patterns over time. Using fine-tuning techniques like Low-Rank Adaptation (LoRA), quantized LoRA (QLoRA), and model pruning on models such as Video-LLaVA and LLaVA-NeXT-Video, we significantly enhance VLM performance in processing remote sensing temporal changes. Results show significant improvements, with the best performance achieving a BERT score of 0.864 and ROUGE-1 score of 0.576, demonstrating superior accuracy in describing land-use transformations.",
            "corpus_id": 273638057,
            "sentences": [
                {
                    "corpus_id": "273638057",
                    "title": "GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing",
                    "text": "Pre-training VLMs is typically computationally intensive and timeconsuming. Consequently, fine-tuning presents an effective alternative that preserves most of the model's parameters while enhancing performance on downstream tasks. Fine-tuned models can often outperform the original general models, utilizing fewer computing resources and requiring less training time [25]. This advantage motivates the use of Parameter-Efficient Fine-Tuning (PEFT) methods for tasks involving geographical change detection. \n\nIn our work, we focus on fine-tuning two distinct models that have demonstrated a robust understanding of temporal data through video processing within the VLM framework for question-answering and captioning. The first model, LLaVA-NeXT [20], was introduced in early 2024, offering improved reasoning and world knowledge compared to other large models. It exhibits data efficiency comparable to SOTA models such as LLaVA-1.5 [19], while delivering higher image resolution and enhanced visual conversation capabilities. Shortly after the release of LLaVA-NeXT, a video variant was introduced, named LLaVA-NeXT-Video, which has demonstrated strong performance in zero-shot video tasks. \n\nThe second model utilized for comparison is Video-LLaVA [15], which excels in understanding visual language for downstream tasks and surpasses many existing video language models across various  benchmarks. Both projects have multiple variations based on the number of parameters for the models. For simplicity, we have chosen to use the 7B parameter variation from both models. The 7B variations can be fine-tuned with PEFT techniques on a single GPU, making them particularly well-suited for our dataset.",
                    "score": 0.456056531462391,
                    "section_title": "Model Fine-tuning",
                    "char_start_offset": 11891,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 75
                        },
                        {
                            "start": 76,
                            "end": 230
                        },
                        {
                            "start": 231,
                            "end": 373
                        },
                        {
                            "start": 374,
                            "end": 507
                        },
                        {
                            "start": 510,
                            "end": 718
                        },
                        {
                            "start": 719,
                            "end": 862
                        },
                        {
                            "start": 863,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1193
                        },
                        {
                            "start": 1196,
                            "end": 1402
                        },
                        {
                            "start": 1403,
                            "end": 1491
                        },
                        {
                            "start": 1492,
                            "end": 1574
                        },
                        {
                            "start": 1575,
                            "end": 1702
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 368,
                            "end": 372,
                            "matchedPaperCorpusId": "268157336"
                        },
                        {
                            "start": 935,
                            "end": 939,
                            "matchedPaperCorpusId": "263672058"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6220703125
                },
                {
                    "corpus_id": "273638057",
                    "title": "GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing",
                    "text": "Table 1 presents results across various models and scoring metrics. All experiments were run on a single 40GB GPU, optimizing LoRA configurations and pruning ratios for a balance of accuracy and memory efficiency. Although larger VLMs may perform better, they require substantially more computational resources. \n\nSince prior work lacked image descriptions or did not employ the fMoW dataset for captioning, we evaluate Video-LLaVA and LLaVA-NeXT-Video as zero-shot baselines. Without fine-tuning, these models performed poorly across all metrics and failed to capture meaningful semantic differences, even under BERT-based evaluation. \n\nWe then applied few-shot fine-tuning with 10K (10%) and 100K samples using LoRA (r = 64, \u03b1 = 128), tuning 178M parameters. Performance improved notably, with the 100K setup achieving a BERT score of 0.864. \n\nTo enhance efficiency, QLoRA with 4-bit quantization reduced memory usage by 75% without sacrificing accuracy, achieving a BLEU score of 0.250, comparable to the LoRA-based model. \n\nWe pruned 5% of the model parameters to reduce size while preserving accuracy. However, pruning widened the performance gap between the 10K and 100K datasets, indicating a greater data requirement to offset pruning-induced degradation. Notably, the LLaVA-Next video model outperformed Video-LLaVA, thanks to its sparse structure, achieving a BERT score of 0.823-only 0.03 lower than the best result. In contrast, Video-LLaVA faced significant challenges with pruning due to its dense architecture, making it unsuitable for pruning. \n\nTable 2 summarizes our ablation study to validate the chosen hyperparameters. We explored several \u03b1 and r ratios to determine their impact on performance. Increasing the fine-tuned parameters to 1.7B by setting r = 640 and \u03b1 = 1280 did not yield significant performance gains, highlighting diminishing returns at higher parameter counts. Modifying the \u03b1 and r ratio to 4:1 also resulted in negligible improvements or degraded performance. Therefore, we adopted the r = 64 and \u03b1 = 128 configuration for subsequent experiments.",
                    "score": 0.36631413476951646,
                    "section_title": "Results & Discussion",
                    "char_start_offset": 19377,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 67
                        },
                        {
                            "start": 68,
                            "end": 213
                        },
                        {
                            "start": 214,
                            "end": 311
                        },
                        {
                            "start": 314,
                            "end": 476
                        },
                        {
                            "start": 477,
                            "end": 635
                        },
                        {
                            "start": 638,
                            "end": 760
                        },
                        {
                            "start": 761,
                            "end": 843
                        },
                        {
                            "start": 846,
                            "end": 1025
                        },
                        {
                            "start": 1028,
                            "end": 1106
                        },
                        {
                            "start": 1107,
                            "end": 1263
                        },
                        {
                            "start": 1264,
                            "end": 1427
                        },
                        {
                            "start": 1428,
                            "end": 1559
                        },
                        {
                            "start": 1562,
                            "end": 1639
                        },
                        {
                            "start": 1640,
                            "end": 1716
                        },
                        {
                            "start": 1717,
                            "end": 1899
                        },
                        {
                            "start": 1900,
                            "end": 2000
                        },
                        {
                            "start": 2001,
                            "end": 2087
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.408447265625
                }
            ],
            "relevance_judgement": 0.6220703125,
            "relevance_judgment_input_expanded": "# Title: GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing\n# Venue: arXiv.org\n# Authors: Hosam Elgendy, Ahmed Sharshar, Ahmed Aboeitta, Yasser Ashraf, Mohsen Guizani\n## Abstract\nDetecting temporal changes in geographical landscapes is critical for applications like environmental monitoring and urban planning. While remote sensing data is abundant, existing vision-language models (VLMs) often fail to capture temporal dynamics effectively. This paper addresses these limitations by introducing an annotated dataset of video frame pairs to track evolving geographical patterns over time. Using fine-tuning techniques like Low-Rank Adaptation (LoRA), quantized LoRA (QLoRA), and model pruning on models such as Video-LLaVA and LLaVA-NeXT-Video, we significantly enhance VLM performance in processing remote sensing temporal changes. Results show significant improvements, with the best performance achieving a BERT score of 0.864 and ROUGE-1 score of 0.576, demonstrating superior accuracy in describing land-use transformations.\n## Model Fine-tuning\nPre-training VLMs is typically computationally intensive and timeconsuming. Consequently, fine-tuning presents an effective alternative that preserves most of the model's parameters while enhancing performance on downstream tasks. Fine-tuned models can often outperform the original general models, utilizing fewer computing resources and requiring less training time [25]. This advantage motivates the use of Parameter-Efficient Fine-Tuning (PEFT) methods for tasks involving geographical change detection. \n\nIn our work, we focus on fine-tuning two distinct models that have demonstrated a robust understanding of temporal data through video processing within the VLM framework for question-answering and captioning. The first model, LLaVA-NeXT [20], was introduced in early 2024, offering improved reasoning and world knowledge compared to other large models. It exhibits data efficiency comparable to SOTA models such as LLaVA-1.5 [19], while delivering higher image resolution and enhanced visual conversation capabilities. Shortly after the release of LLaVA-NeXT, a video variant was introduced, named LLaVA-NeXT-Video, which has demonstrated strong performance in zero-shot video tasks. \n\nThe second model utilized for comparison is Video-LLaVA [15], which excels in understanding visual language for downstream tasks and surpasses many existing video language models across various  benchmarks. Both projects have multiple variations based on the number of parameters for the models. For simplicity, we have chosen to use the 7B parameter variation from both models. The 7B variations can be fine-tuned with PEFT techniques on a single GPU, making them particularly well-suited for our dataset.\n\n## Results & Discussion\nTable 1 presents results across various models and scoring metrics. All experiments were run on a single 40GB GPU, optimizing LoRA configurations and pruning ratios for a balance of accuracy and memory efficiency. Although larger VLMs may perform better, they require substantially more computational resources. \n\nSince prior work lacked image descriptions or did not employ the fMoW dataset for captioning, we evaluate Video-LLaVA and LLaVA-NeXT-Video as zero-shot baselines. Without fine-tuning, these models performed poorly across all metrics and failed to capture meaningful semantic differences, even under BERT-based evaluation. \n\nWe then applied few-shot fine-tuning with 10K (10%) and 100K samples using LoRA (r = 64, \u03b1 = 128), tuning 178M parameters. Performance improved notably, with the 100K setup achieving a BERT score of 0.864. \n\nTo enhance efficiency, QLoRA with 4-bit quantization reduced memory usage by 75% without sacrificing accuracy, achieving a BLEU score of 0.250, comparable to the LoRA-based model. \n\nWe pruned 5% of the model parameters to reduce size while preserving accuracy. However, pruning widened the performance gap between the 10K and 100K datasets, indicating a greater data requirement to offset pruning-induced degradation. Notably, the LLaVA-Next video model outperformed Video-LLaVA, thanks to its sparse structure, achieving a BERT score of 0.823-only 0.03 lower than the best result. In contrast, Video-LLaVA faced significant challenges with pruning due to its dense architecture, making it unsuitable for pruning. \n\nTable 2 summarizes our ablation study to validate the chosen hyperparameters. We explored several \u03b1 and r ratios to determine their impact on performance. Increasing the fine-tuned parameters to 1.7B by setting r = 640 and \u03b1 = 1280 did not yield significant performance gains, highlighting diminishing returns at higher parameter counts. Modifying the \u03b1 and r ratio to 4:1 also resulted in negligible improvements or degraded performance. Therefore, we adopted the r = 64 and \u03b1 = 128 configuration for subsequent experiments.",
            "reference_string": "[273638057 | Elgendy et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 308,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.02477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2353440123",
                    "name": "Xiaofeng Han"
                },
                {
                    "authorId": "2282098650",
                    "name": "Shunpeng Chen"
                },
                {
                    "authorId": "2354260566",
                    "name": "Zenghuang Fu"
                },
                {
                    "authorId": "2353852983",
                    "name": "Zhe Feng"
                },
                {
                    "authorId": "2354113244",
                    "name": "Lue Fan"
                },
                {
                    "authorId": "2353389657",
                    "name": "Dong An"
                },
                {
                    "authorId": "1490749570",
                    "name": "Changwei Wang"
                },
                {
                    "authorId": "2263619001",
                    "name": "Li Guo"
                },
                {
                    "authorId": "35965884",
                    "name": "Weiliang Meng"
                },
                {
                    "authorId": "2125434570",
                    "name": "Xiaopeng Zhang"
                },
                {
                    "authorId": "2106213769",
                    "name": "Rongtao Xu"
                },
                {
                    "authorId": "2261685321",
                    "name": "Shibiao Xu"
                }
            ],
            "abstract": "Robot vision has greatly benefited from advancements in multimodal fusion techniques and vision-language models (VLMs). We systematically review the applications of multimodal fusion in key robotic vision tasks, including semantic scene understanding, simultaneous localization and mapping (SLAM), 3D object detection, navigation and localization, and robot manipulation. We compare VLMs based on large language models (LLMs) with traditional multimodal fusion methods, analyzing their advantages, limitations, and synergies. Additionally, we conduct an in-depth analysis of commonly used datasets, evaluating their applicability and challenges in real-world robotic scenarios. Furthermore, we identify critical research challenges such as cross-modal alignment, efficient fusion strategies, real-time deployment, and domain adaptation, and propose future research directions, including self-supervised learning for robust multimodal representations, transformer-based fusion architectures, and scalable multimodal frameworks. Through a comprehensive review, comparative analysis, and forward-looking discussion, we provide a valuable reference for advancing multimodal perception and interaction in robotic vision. A comprehensive list of studies in this survey is available at https://github.com/Xiaofeng-Han-Res/MF-RV.",
            "corpus_id": 277510513,
            "sentences": [
                {
                    "corpus_id": "277510513",
                    "title": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision",
                    "text": "Visual-language pretraining is the foundational technique for VLMs, allowing models to learn deep relationships between images and text through the joint training of large-scale imagetext pairs, which improves the model's performance on crossmodal tasks. Flamingo [203] is a classic example, combining few-shot learning and cross-modal reasoning to quickly adapt to new tasks with minimal examples, especially excelling in image captioning and visual question answering (VQA). Gemini [11] further extends this approach by jointly training largescale data from images, text, and audio, enhancing the model's cross-modal reasoning ability, particularly excelling in visual reasoning tasks. Kosmos-2 [210] strengthens the joint modeling of images and language through self-supervised learning, allowing it to handle more complex tasks, such as deep reasoning and generation of image and text. Additionally, PaLM-E [204] combines improvements in multimodal learning and reasoning capabilities, enabling it to process multimodal data such as images, text, and audio, performing well in cross-modal reasoning and generation tasks. \n\nThe core innovation of these models lies in joint training, enhancing the synergy between images and text, making them more widely applicable in multimodal tasks. MiniGPT-4 [205] and LLaVA [211] further refine this by aligning visuallanguage representations, improving the accuracy and flexibility of image-to-text generation. LLaVA [211] (Language and Vision Assistant) is an innovative visual-language model that employs instruction tuning, giving the model a high degree of flexibility in handling various tasks. LLaVA [211]'s ability to adjust to different tasks using natural language instructions enables it to not only respond rapidly to traditional visuallanguage tasks like VQA and image captioning, but also to adjust its execution strategy dynamically for new tasks. Through instruction tuning, LLaVA [211] can generate precise image descriptions or answer visual-related questions based on different task demands. This flexible task-switching capability makes LLaVA [211] particularly advantageous in complex environments, excelling in tasks such as cross-modal and visual reasoning.",
                    "score": 0.344822749619036,
                    "section_title": "Transformer Variants and Large Vision-Language Models",
                    "char_start_offset": 66742,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 254
                        },
                        {
                            "start": 255,
                            "end": 476
                        },
                        {
                            "start": 477,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 889
                        },
                        {
                            "start": 890,
                            "end": 1124
                        },
                        {
                            "start": 1127,
                            "end": 1289
                        },
                        {
                            "start": 1290,
                            "end": 1453
                        },
                        {
                            "start": 1454,
                            "end": 1642
                        },
                        {
                            "start": 1643,
                            "end": 1904
                        },
                        {
                            "start": 1905,
                            "end": 2052
                        },
                        {
                            "start": 2053,
                            "end": 2222
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 264,
                            "end": 269,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 1316,
                            "end": 1321,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 1460,
                            "end": 1465,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 1649,
                            "end": 1654,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 1939,
                            "end": 1944,
                            "matchedPaperCorpusId": "258179774"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6083984375
                }
            ],
            "relevance_judgement": 0.6083984375,
            "relevance_judgment_input_expanded": "# Title: Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision\n# Venue: arXiv.org\n# Authors: Xiaofeng Han, Shunpeng Chen, Zenghuang Fu, Zhe Feng, Lue Fan, Dong An, Changwei Wang, Li Guo, Weiliang Meng, Xiaopeng Zhang, Rongtao Xu, Shibiao Xu\n## Abstract\nRobot vision has greatly benefited from advancements in multimodal fusion techniques and vision-language models (VLMs). We systematically review the applications of multimodal fusion in key robotic vision tasks, including semantic scene understanding, simultaneous localization and mapping (SLAM), 3D object detection, navigation and localization, and robot manipulation. We compare VLMs based on large language models (LLMs) with traditional multimodal fusion methods, analyzing their advantages, limitations, and synergies. Additionally, we conduct an in-depth analysis of commonly used datasets, evaluating their applicability and challenges in real-world robotic scenarios. Furthermore, we identify critical research challenges such as cross-modal alignment, efficient fusion strategies, real-time deployment, and domain adaptation, and propose future research directions, including self-supervised learning for robust multimodal representations, transformer-based fusion architectures, and scalable multimodal frameworks. Through a comprehensive review, comparative analysis, and forward-looking discussion, we provide a valuable reference for advancing multimodal perception and interaction in robotic vision. A comprehensive list of studies in this survey is available at https://github.com/Xiaofeng-Han-Res/MF-RV.\n## Transformer Variants and Large Vision-Language Models\nVisual-language pretraining is the foundational technique for VLMs, allowing models to learn deep relationships between images and text through the joint training of large-scale imagetext pairs, which improves the model's performance on crossmodal tasks. Flamingo [203] is a classic example, combining few-shot learning and cross-modal reasoning to quickly adapt to new tasks with minimal examples, especially excelling in image captioning and visual question answering (VQA). Gemini [11] further extends this approach by jointly training largescale data from images, text, and audio, enhancing the model's cross-modal reasoning ability, particularly excelling in visual reasoning tasks. Kosmos-2 [210] strengthens the joint modeling of images and language through self-supervised learning, allowing it to handle more complex tasks, such as deep reasoning and generation of image and text. Additionally, PaLM-E [204] combines improvements in multimodal learning and reasoning capabilities, enabling it to process multimodal data such as images, text, and audio, performing well in cross-modal reasoning and generation tasks. \n\nThe core innovation of these models lies in joint training, enhancing the synergy between images and text, making them more widely applicable in multimodal tasks. MiniGPT-4 [205] and LLaVA [211] further refine this by aligning visuallanguage representations, improving the accuracy and flexibility of image-to-text generation. LLaVA [211] (Language and Vision Assistant) is an innovative visual-language model that employs instruction tuning, giving the model a high degree of flexibility in handling various tasks. LLaVA [211]'s ability to adjust to different tasks using natural language instructions enables it to not only respond rapidly to traditional visuallanguage tasks like VQA and image captioning, but also to adjust its execution strategy dynamically for new tasks. Through instruction tuning, LLaVA [211] can generate precise image descriptions or answer visual-related questions based on different task demands. This flexible task-switching capability makes LLaVA [211] particularly advantageous in complex environments, excelling in tasks such as cross-modal and visual reasoning.",
            "reference_string": "[277510513 | Han et al. | 2025 | Citations: 4]"
        },
        {
            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "reference_count": 156,
            "citation_count": 867,
            "influential_citation_count": 201,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.03326, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2310709478",
                    "name": "Bo Li"
                },
                {
                    "authorId": "2145784327",
                    "name": "Yuanhan Zhang"
                },
                {
                    "authorId": "2325209062",
                    "name": "Dong Guo"
                },
                {
                    "authorId": "2310650738",
                    "name": "Renrui Zhang"
                },
                {
                    "authorId": "2310758205",
                    "name": "Feng Li"
                },
                {
                    "authorId": "2267467406",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "2300086932",
                    "name": "Kaichen Zhang"
                },
                {
                    "authorId": "2315071527",
                    "name": "Yanwei Li"
                },
                {
                    "authorId": "2315193840",
                    "name": "Ziwei Liu"
                },
                {
                    "authorId": "2264692022",
                    "name": "Chunyuan Li"
                }
            ],
            "abstract": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.",
            "corpus_id": 271719914,
            "sentences": [
                {
                    "corpus_id": "271719914",
                    "title": "LLaVA-OneVision: Easy Visual Task Transfer",
                    "text": "\u2022 The Ablation blog [64] summarizes our empirical exploration except the visual instruction data itself, including the choice of architectures (scaling of LLM & vision encoder), visual representations (resolution & #tokens), as well as training strategies (trainable modules & high-quality data) in the pursuit of data scaling success. \u2022 The Interleave blog [68] describes the strategies to extend and improve the capability in new scenarios including multi-image, multi-frame (video) and multi-view (3D), while maintaining the single-image performance. \n\nThese explorations, conducted within a fixed compute budget, aimed to offer useful insights along the way as we navigate the project, rather than push performance limits. During the process, we have also been accumulating and curating a large collection of the high-quality datasets from January to June. By consolidating these insights and execute the experiments with \"yolo run\" on newly accumulated larger datasets, we introduce LLaVA-OneVision. We implement the new model with the available compute, without extensively de-risking individual components. This leaves room for further improvements in capabilities through additional data and model scaling following our recipe, Please see the detailed development timeline in Section A. In particular, our paper makes the following contributions: \n\n\u2022 Large multimodal models. We develop LLaVA-OneVision, a family of open large multimodal models (LMMs) that improves the performance boundaries of open LMMs in three important vision settings, including single-image, multi-image, and video scenarios. \u2022 Emerging Capabilities with Task Transfer. Our design in modeling and data representations allow task transfer across different scenarios, suggesting a simple approach to yield new emgerging capabilities. In particular, LLaVA-OneVision demonstrate strong video understanding through task transfer from images. \u2022 Open-source. To pave the way towards building a general-purpose visual assistant, we release the following assets to the public: the generated multimodal instruction data, the codebase, the model checkpoints, and a visual chat demo.",
                    "score": 0.4223314301858626,
                    "section_title": "Introduction",
                    "char_start_offset": 1791,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 553
                        },
                        {
                            "start": 556,
                            "end": 726
                        },
                        {
                            "start": 727,
                            "end": 860
                        },
                        {
                            "start": 861,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1113
                        },
                        {
                            "start": 1114,
                            "end": 1354
                        },
                        {
                            "start": 1357,
                            "end": 1383
                        },
                        {
                            "start": 1384,
                            "end": 1607
                        },
                        {
                            "start": 1608,
                            "end": 1651
                        },
                        {
                            "start": 1652,
                            "end": 1813
                        },
                        {
                            "start": 1814,
                            "end": 1918
                        },
                        {
                            "start": 1919,
                            "end": 1933
                        },
                        {
                            "start": 1934,
                            "end": 2153
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.58203125
                }
            ],
            "relevance_judgement": 0.58203125,
            "relevance_judgment_input_expanded": "# Title: LLaVA-OneVision: Easy Visual Task Transfer\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li\n## Abstract\nWe present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.\n## Introduction\n\u2022 The Ablation blog [64] summarizes our empirical exploration except the visual instruction data itself, including the choice of architectures (scaling of LLM & vision encoder), visual representations (resolution & #tokens), as well as training strategies (trainable modules & high-quality data) in the pursuit of data scaling success. \u2022 The Interleave blog [68] describes the strategies to extend and improve the capability in new scenarios including multi-image, multi-frame (video) and multi-view (3D), while maintaining the single-image performance. \n\nThese explorations, conducted within a fixed compute budget, aimed to offer useful insights along the way as we navigate the project, rather than push performance limits. During the process, we have also been accumulating and curating a large collection of the high-quality datasets from January to June. By consolidating these insights and execute the experiments with \"yolo run\" on newly accumulated larger datasets, we introduce LLaVA-OneVision. We implement the new model with the available compute, without extensively de-risking individual components. This leaves room for further improvements in capabilities through additional data and model scaling following our recipe, Please see the detailed development timeline in Section A. In particular, our paper makes the following contributions: \n\n\u2022 Large multimodal models. We develop LLaVA-OneVision, a family of open large multimodal models (LMMs) that improves the performance boundaries of open LMMs in three important vision settings, including single-image, multi-image, and video scenarios. \u2022 Emerging Capabilities with Task Transfer. Our design in modeling and data representations allow task transfer across different scenarios, suggesting a simple approach to yield new emgerging capabilities. In particular, LLaVA-OneVision demonstrate strong video understanding through task transfer from images. \u2022 Open-source. To pave the way towards building a general-purpose visual assistant, we release the following assets to the public: the generated multimodal instruction data, the codebase, the model checkpoints, and a visual chat demo.",
            "reference_string": "[271719914 | Li et al. | 2024 | Citations: 867]"
        },
        {
            "title": "LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models",
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "year": 2024,
            "reference_count": 29,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.16224, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268720100",
                    "name": "Jingyi Wang"
                },
                {
                    "authorId": "2317982861",
                    "name": "Jianzhong Ju"
                },
                {
                    "authorId": "2317980688",
                    "name": "Jian Luan"
                },
                {
                    "authorId": "2268680150",
                    "name": "Zhidong Deng"
                }
            ],
            "abstract": "Recent advances in large vision-language models (VLMs) typically employ vision encoders based on the Vision Transformer (ViT) architecture. The division of the images into patches by ViT results in a fragmented perception, thereby hindering the visual understanding capabilities of VLMs. In this paper, we propose an innovative enhancement to address this limitation by introducing a Scene Graph Expression (SGE) module in VLMs. This module extracts and structurally expresses the complex semantic information within images, thereby improving the foundational perception and understanding abilities of VLMs. Extensive experiments demonstrate that integrating our SGE module significantly enhances the VLM's performance in vision-language tasks, indicating its effectiveness in preserving intricate semantic details and facilitating better visual understanding.",
            "corpus_id": 272146052,
            "sentences": [
                {
                    "corpus_id": "272146052",
                    "title": "LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models",
                    "text": "The comparison results of LLaVA-SG and baseline models are summarized in Table I following the evaluation metrics of [30]. The reported results of the compared models are collected from the corresponding papers. Analyzing the experimental results in Table I, it is evident that our LLaVA-SG model shows significant and consistent improvements over   I. MMBench assesses large vision-language models across multiple capability dimensions including LR for Logical Reasoning, AR for Attribute Reasoning, RR for Relation Reasoning, FP-S for Fine-grained Perception (Single Instance), FP-C for Fine-grained Perception (Cross Instance) and CP for Coarse Perception. Analyzing the comparative results in this table, our LLaVA-SG shows significant improvements over LLaVA-1.5 in the capabilities that require entity perception and relationship analysis, specifically in RR, FP-S, FP-C, and CP. \n\nWe present example outputs of LLaVA-1.5 and LLaVA-SG in Fig. 3. The middle column shows the masks of entities included in the scene graph of the SGE module. With the visual semantic information expressed by SGE, LLaVA-SG exhibits enhanced multimodal capabilities. For example, in the first case shown in Fig. 3, without explicitly preserving the visual semantic information in the image, the counting problem becomes difficult for LLaVA. Relying solely on the fragmented visual tokens output by ViT makes it challenging to accurately determine the number of dogs in the image. However, equipped with the SGE module, our LLaVA-SG can leverage tokens that explicitly represent entities in the image, enabling it to provide an accurate count of the dogs.",
                    "score": 0.3975910123550702,
                    "section_title": "B. Overall Performance Assessments",
                    "char_start_offset": 10571,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 122
                        },
                        {
                            "start": 123,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 885
                        },
                        {
                            "start": 888,
                            "end": 951
                        },
                        {
                            "start": 952,
                            "end": 1044
                        },
                        {
                            "start": 1045,
                            "end": 1151
                        },
                        {
                            "start": 1152,
                            "end": 1325
                        },
                        {
                            "start": 1326,
                            "end": 1464
                        },
                        {
                            "start": 1465,
                            "end": 1639
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.58056640625
                }
            ],
            "relevance_judgement": 0.58056640625,
            "relevance_judgment_input_expanded": "# Title: LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models\n# Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing\n# Authors: Jingyi Wang, Jianzhong Ju, Jian Luan, Zhidong Deng\n## Abstract\nRecent advances in large vision-language models (VLMs) typically employ vision encoders based on the Vision Transformer (ViT) architecture. The division of the images into patches by ViT results in a fragmented perception, thereby hindering the visual understanding capabilities of VLMs. In this paper, we propose an innovative enhancement to address this limitation by introducing a Scene Graph Expression (SGE) module in VLMs. This module extracts and structurally expresses the complex semantic information within images, thereby improving the foundational perception and understanding abilities of VLMs. Extensive experiments demonstrate that integrating our SGE module significantly enhances the VLM's performance in vision-language tasks, indicating its effectiveness in preserving intricate semantic details and facilitating better visual understanding.\n## B. Overall Performance Assessments\nThe comparison results of LLaVA-SG and baseline models are summarized in Table I following the evaluation metrics of [30]. The reported results of the compared models are collected from the corresponding papers. Analyzing the experimental results in Table I, it is evident that our LLaVA-SG model shows significant and consistent improvements over   I. MMBench assesses large vision-language models across multiple capability dimensions including LR for Logical Reasoning, AR for Attribute Reasoning, RR for Relation Reasoning, FP-S for Fine-grained Perception (Single Instance), FP-C for Fine-grained Perception (Cross Instance) and CP for Coarse Perception. Analyzing the comparative results in this table, our LLaVA-SG shows significant improvements over LLaVA-1.5 in the capabilities that require entity perception and relationship analysis, specifically in RR, FP-S, FP-C, and CP. \n\nWe present example outputs of LLaVA-1.5 and LLaVA-SG in Fig. 3. The middle column shows the masks of entities included in the scene graph of the SGE module. With the visual semantic information expressed by SGE, LLaVA-SG exhibits enhanced multimodal capabilities. For example, in the first case shown in Fig. 3, without explicitly preserving the visual semantic information in the image, the counting problem becomes difficult for LLaVA. Relying solely on the fragmented visual tokens output by ViT makes it challenging to accurately determine the number of dogs in the image. However, equipped with the SGE module, our LLaVA-SG can leverage tokens that explicitly represent entities in the image, enabling it to provide an accurate count of the dogs.",
            "reference_string": "[272146052 | Wang et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Exploring Multimodal Large Language Models for Radiology Report Error-checking",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 27,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.13103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2182815661",
                    "name": "Jinge Wu"
                },
                {
                    "authorId": "2275715280",
                    "name": "Yunsoo Kim"
                },
                {
                    "authorId": "2275354702",
                    "name": "Eva C. Keller"
                },
                {
                    "authorId": "2275129334",
                    "name": "Jamie Chow"
                },
                {
                    "authorId": "2275239607",
                    "name": "Adam P. Levine"
                },
                {
                    "authorId": "48573031",
                    "name": "N. Pontikos"
                },
                {
                    "authorId": "2275359351",
                    "name": "Zina Ibrahim"
                },
                {
                    "authorId": "2257359718",
                    "name": "Paul Taylor"
                },
                {
                    "authorId": "2276324181",
                    "name": "Michelle C. Williams"
                },
                {
                    "authorId": "2241781737",
                    "name": "Honghan Wu"
                }
            ],
            "abstract": "This paper proposes one of the first clinical applications of multimodal large language models (LLMs) as an assistant for radiologists to check errors in their reports. We created an evaluation dataset from real-world radiology datasets (including X-rays and CT scans). A subset of original reports was modified to contain synthetic errors by introducing three types of mistakes:\"insert\",\"remove\", and\"substitute\". The evaluation contained two difficulty levels: SIMPLE for binary error-checking and COMPLEX for identifying error types. At the SIMPLE level, our fine-tuned model significantly enhanced performance by 47.4% and 25.4% on MIMIC-CXR and IU X-ray data, respectively. This performance boost is also observed in unseen modality, CT scans, as the model performed 19.46% better than the baseline model. The model also surpassed the domain expert's accuracy in the MIMIC-CXR dataset by 1.67%. Notably, among the subsets (N=21) of the test set where a clinician did not achieve the correct conclusion, the LLaVA ensemble mode correctly identified 71.4% of these cases. However, all models performed poorly in identifying mistake types, underscoring the difficulty of the COMPLEX level. This study marks a promising step toward utilizing multimodal LLMs to enhance diagnostic accuracy in radiology. The ensemble model demonstrated comparable performance to clinicians, even capturing errors overlooked by humans.",
            "corpus_id": 266374969,
            "sentences": [
                {
                    "corpus_id": "266374969",
                    "title": "Exploring Multimodal Large Language Models for Radiology Report Error-checking",
                    "text": "LLaVA-Med is one of the first VLMs in the biomedical domain (21). LLaVA-Med is initialized with LLaVA-0 and then continuously trained with a comprehensive dataset of biomedical figure-caption pairs sourced from PubMed Central. Only the 7B model is available publicly. \n\nLLaVA-1.5: LLaVA-1.5 is a general domain VLM that uses the LLaMA2 model, which has a significant improvement in language understanding when compared with LLaMA, as the backbone LLM (18,20). There are two significant improvements besides the change of the backbone LLM. \n\nFirstly, the addition of an MLP vision-language connector enhanced the system's capabilities. \n\nSecondly, the integration of academic task-oriented data further enhanced its performance and effectiveness. LLaVA-1.5 is available in 2 model sizes, 7B and 13B models, and we used both models.",
                    "score": 0.3930550077602005,
                    "section_title": "LLaVA-Med:",
                    "char_start_offset": 11020,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 65
                        },
                        {
                            "start": 66,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 267
                        },
                        {
                            "start": 270,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 538
                        },
                        {
                            "start": 541,
                            "end": 634
                        },
                        {
                            "start": 637,
                            "end": 745
                        },
                        {
                            "start": 746,
                            "end": 830
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56103515625
                }
            ],
            "relevance_judgement": 0.56103515625,
            "relevance_judgment_input_expanded": "# Title: Exploring Multimodal Large Language Models for Radiology Report Error-checking\n# Venue: arXiv.org\n# Authors: Jinge Wu, Yunsoo Kim, Eva C. Keller, Jamie Chow, Adam P. Levine, N. Pontikos, Zina Ibrahim, Paul Taylor, Michelle C. Williams, Honghan Wu\n## Abstract\nThis paper proposes one of the first clinical applications of multimodal large language models (LLMs) as an assistant for radiologists to check errors in their reports. We created an evaluation dataset from real-world radiology datasets (including X-rays and CT scans). A subset of original reports was modified to contain synthetic errors by introducing three types of mistakes:\"insert\",\"remove\", and\"substitute\". The evaluation contained two difficulty levels: SIMPLE for binary error-checking and COMPLEX for identifying error types. At the SIMPLE level, our fine-tuned model significantly enhanced performance by 47.4% and 25.4% on MIMIC-CXR and IU X-ray data, respectively. This performance boost is also observed in unseen modality, CT scans, as the model performed 19.46% better than the baseline model. The model also surpassed the domain expert's accuracy in the MIMIC-CXR dataset by 1.67%. Notably, among the subsets (N=21) of the test set where a clinician did not achieve the correct conclusion, the LLaVA ensemble mode correctly identified 71.4% of these cases. However, all models performed poorly in identifying mistake types, underscoring the difficulty of the COMPLEX level. This study marks a promising step toward utilizing multimodal LLMs to enhance diagnostic accuracy in radiology. The ensemble model demonstrated comparable performance to clinicians, even capturing errors overlooked by humans.\n## LLaVA-Med:\nLLaVA-Med is one of the first VLMs in the biomedical domain (21). LLaVA-Med is initialized with LLaVA-0 and then continuously trained with a comprehensive dataset of biomedical figure-caption pairs sourced from PubMed Central. Only the 7B model is available publicly. \n\nLLaVA-1.5: LLaVA-1.5 is a general domain VLM that uses the LLaMA2 model, which has a significant improvement in language understanding when compared with LLaMA, as the backbone LLM (18,20). There are two significant improvements besides the change of the backbone LLM. \n\nFirstly, the addition of an MLP vision-language connector enhanced the system's capabilities. \n\nSecondly, the integration of academic task-oriented data further enhanced its performance and effectiveness. LLaVA-1.5 is available in 2 model sizes, 7B and 13B models, and we used both models.",
            "reference_string": "[266374969 | Wu et al. | 2023 | Citations: 4]"
        },
        {
            "title": "OpenVLA: An Open-Source Vision-Language-Action Model",
            "venue": "Conference on Robot Learning",
            "year": 2024,
            "reference_count": 110,
            "citation_count": 532,
            "influential_citation_count": 86,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.09246, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2159987907",
                    "name": "Moo Jin Kim"
                },
                {
                    "authorId": "31719101",
                    "name": "Karl Pertsch"
                },
                {
                    "authorId": "10737060",
                    "name": "Siddharth Karamcheti"
                },
                {
                    "authorId": "9961095",
                    "name": "Ted Xiao"
                },
                {
                    "authorId": "3117588",
                    "name": "A. Balakrishna"
                },
                {
                    "authorId": "2286638954",
                    "name": "Suraj Nair"
                },
                {
                    "authorId": "102801230",
                    "name": "Rafael Rafailov"
                },
                {
                    "authorId": "2306251665",
                    "name": "Ethan Foster"
                },
                {
                    "authorId": "2306259543",
                    "name": "Grace Lam"
                },
                {
                    "authorId": "2840758",
                    "name": "Pannag R. Sanketi"
                },
                {
                    "authorId": "2288210223",
                    "name": "Quan Vuong"
                },
                {
                    "authorId": "2283843631",
                    "name": "Thomas Kollar"
                },
                {
                    "authorId": "2319412766",
                    "name": "Benjamin Burchfiel"
                },
                {
                    "authorId": "1726802",
                    "name": "Russ Tedrake"
                },
                {
                    "authorId": "1779671",
                    "name": "Dorsa Sadigh"
                },
                {
                    "authorId": "2249615151",
                    "name": "Sergey Levine"
                },
                {
                    "authorId": "2283843948",
                    "name": "Percy Liang"
                },
                {
                    "authorId": "2284774407",
                    "name": "Chelsea Finn"
                }
            ],
            "abstract": "Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.",
            "corpus_id": 270440391,
            "sentences": [
                {
                    "corpus_id": "270440391",
                    "title": "OpenVLA: An Open-Source Vision-Language-Action Model",
                    "text": "When developing the OpenVLA model, we explored various design decisions in smaller-scale experiments before starting the final model training run. Concretely, we trained and evaluated OpenVLA models on BridgeData V2 [6] for our initial experiments, instead of training on the full OpenX mixture, to increase iteration speed and reduce computational cost. We summarize key learnings from these explorations below. VLM Backbone. Initially, we experimented with multiple VLM backbones. Apart from Prismatic [44], we tested fine-tuning IDEFICS-1 [84] and LLaVA [85] for robot action prediction. We found that LLaVA and IDEFICS-1 performed comparably on tasks with only one object in the scene, but LLaVA demonstrated stronger language grounding in tasks that involved multiple objects in the scene and required the policy to manipulate the correct object, i.e., the object specified in the language instruction. Concretely, LLaVA improved upon IDEFICS-1 by 35% in absolute success rate, averaged across five language grounding tasks in a BridgeData V2 sink environment. The fine-tuned Prismatic VLM policy achieved further improvements, outperforming the LLaVA policy by roughly 10% in absolute success rate across both simple single-object tasks and multi-object, language grounding tasks. We attribute this performance delta to improved spatial reasoning capabilities afforded by the fused SigLIP-DinoV2 backbones (see Section 3.1). In addition to the performance enhancements, Prismatic also provides a modular and easy-to-use codebase, so we ultimately chose it to be the backbone for the OpenVLA model. Image Resolution. The resolution of input images has significant impact on the computational requirements of VLA training, since higher-resolution images result in more image patch tokens and thus longer context lengths that quadratically increase training compute. We compared VLAs with 224 \u00d7 224px and 384 \u00d7 384px inputs, but found no performance difference in our evaluations, while the latter takes 3x longer to train. We thus opt for a resolution of 224 \u00d7 224px for the final OpenVLA model.",
                    "score": 0.3502130006590543,
                    "section_title": "OpenVLA Design Decisions",
                    "char_start_offset": 17591,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 146
                        },
                        {
                            "start": 147,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 412
                        },
                        {
                            "start": 413,
                            "end": 426
                        },
                        {
                            "start": 427,
                            "end": 482
                        },
                        {
                            "start": 483,
                            "end": 590
                        },
                        {
                            "start": 591,
                            "end": 907
                        },
                        {
                            "start": 908,
                            "end": 1065
                        },
                        {
                            "start": 1066,
                            "end": 1286
                        },
                        {
                            "start": 1287,
                            "end": 1430
                        },
                        {
                            "start": 1431,
                            "end": 1603
                        },
                        {
                            "start": 1604,
                            "end": 1621
                        },
                        {
                            "start": 1622,
                            "end": 1869
                        },
                        {
                            "start": 1870,
                            "end": 2026
                        },
                        {
                            "start": 2027,
                            "end": 2099
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 557,
                            "end": 561,
                            "matchedPaperCorpusId": "258179774"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55224609375
                }
            ],
            "relevance_judgement": 0.55224609375,
            "relevance_judgment_input_expanded": "# Title: OpenVLA: An Open-Source Vision-Language-Action Model\n# Venue: Conference on Robot Learning\n# Authors: Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, A. Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag R. Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, Chelsea Finn\n## Abstract\nLarge policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.\n## OpenVLA Design Decisions\nWhen developing the OpenVLA model, we explored various design decisions in smaller-scale experiments before starting the final model training run. Concretely, we trained and evaluated OpenVLA models on BridgeData V2 [6] for our initial experiments, instead of training on the full OpenX mixture, to increase iteration speed and reduce computational cost. We summarize key learnings from these explorations below. VLM Backbone. Initially, we experimented with multiple VLM backbones. Apart from Prismatic [44], we tested fine-tuning IDEFICS-1 [84] and LLaVA [85] for robot action prediction. We found that LLaVA and IDEFICS-1 performed comparably on tasks with only one object in the scene, but LLaVA demonstrated stronger language grounding in tasks that involved multiple objects in the scene and required the policy to manipulate the correct object, i.e., the object specified in the language instruction. Concretely, LLaVA improved upon IDEFICS-1 by 35% in absolute success rate, averaged across five language grounding tasks in a BridgeData V2 sink environment. The fine-tuned Prismatic VLM policy achieved further improvements, outperforming the LLaVA policy by roughly 10% in absolute success rate across both simple single-object tasks and multi-object, language grounding tasks. We attribute this performance delta to improved spatial reasoning capabilities afforded by the fused SigLIP-DinoV2 backbones (see Section 3.1). In addition to the performance enhancements, Prismatic also provides a modular and easy-to-use codebase, so we ultimately chose it to be the backbone for the OpenVLA model. Image Resolution. The resolution of input images has significant impact on the computational requirements of VLA training, since higher-resolution images result in more image patch tokens and thus longer context lengths that quadratically increase training compute. We compared VLAs with 224 \u00d7 224px and 384 \u00d7 384px inputs, but found no performance difference in our evaluations, while the latter takes 3x longer to train. We thus opt for a resolution of 224 \u00d7 224px for the final OpenVLA model.",
            "reference_string": "[270440391 | Kim et al. | 2024 | Citations: 532]"
        },
        {
            "title": "Improved Baselines with Visual Instruction Tuning",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2023,
            "reference_count": 71,
            "citation_count": 2824,
            "influential_citation_count": 578,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.03744",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.03744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2143856368",
                    "name": "Haotian Liu"
                },
                {
                    "authorId": "2243126534",
                    "name": "Chunyuan Li"
                },
                {
                    "authorId": "1527091339",
                    "name": "Yuheng Li"
                },
                {
                    "authorId": "2256122200",
                    "name": "Yong Jae Lee"
                }
            ],
            "abstract": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available.",
            "corpus_id": 263672058,
            "sentences": [
                {
                    "corpus_id": "263672058",
                    "title": "Improved Baselines with Visual Instruction Tuning",
                    "text": "We also incorporate ShareGPT [46] data and scale up the LLM to 13B as in [3,8,39]. Results on MM-Vet shows the most significant improvement when scaling the LLM to 13B, suggesting the importance of the base LLM's capability for visual conversations. \n\nLLaVA-1.5. We denote this final model with all the modifications as LLaVA-1.5 (the last two rows in Table 2), which achieves an impressive performance that significantly outperforms the original LLaVA [36]. \n\nComputational cost. For LLaVA-1.5, we use the same pretraining dataset, and keep the training iterations and batch size roughly the same for instruction tuning as LLaVA [36]. \n\nDue to the increased image input resolution to 336 2 , the training of LLaVA-1.5 is \u223c2\u00d7 as long as LLaVA: \u223c6 hours of pretraining and \u223c20 hours of visual instruction tuning, using 8\u00d7 A100s.",
                    "score": 0.3942626858250983,
                    "section_title": "Scaling the Data and Model",
                    "char_start_offset": 13081,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 82
                        },
                        {
                            "start": 83,
                            "end": 249
                        },
                        {
                            "start": 252,
                            "end": 262
                        },
                        {
                            "start": 263,
                            "end": 458
                        },
                        {
                            "start": 461,
                            "end": 480
                        },
                        {
                            "start": 481,
                            "end": 635
                        },
                        {
                            "start": 638,
                            "end": 827
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 453,
                            "end": 457,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 630,
                            "end": 634,
                            "matchedPaperCorpusId": "258179774"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53369140625
                },
                {
                    "corpus_id": "263672058",
                    "title": "Improved Baselines with Visual Instruction Tuning",
                    "text": "Our study originates from LLaVA and builds a road map by carefully making effective contributions from the perspectives of the input, model, and data. \n\nFirst, we unveil that the fully-connected vision-language connector in LLaVA is surprisingly powerful and dataefficient, and we establish stronger and more feasible baselines built upon the LLaVA framework. We report that two simple improvements, namely, an MLP cross-modal connector and incorporating academic task related data such as VQA, are orthogonal to the framework of LLaVA, and when used with LLaVA, lead to better multimodal understanding capabilities. In contrast to InstructBLIP [14] or Qwen-VL [3], which trains specially designed visual resamplers on hundreds of millions or even billions of image-text paired data, LLaVA uses one of the simplest architecture design for LMMs and requires only training a simple fullyconnected projection layer on merely 600K image-text pairs. Our final model can finish training in \u223c1 day on a single 8-A100 machine and achieves state-of-the-art results on a wide range of benchmarks. Moreover, unlike Qwen-VL [3] that includes in-house data in training, LLaVA utilizes only publicly available data. \n\nNext, we delve into an early exploration of other open problems of large multimodal models. Our findings include: (1) Scaling to high-resolution image inputs. We show that LLaVA's architecture is versatile in scaling to higher resolutions by simply dividing images into grids and maintains its data efficiency; with the increased resolution, it improves the model's detailed perception capabilities and reduces hallucination. (2) Compositional capabilities. We find that large multimodal models are capable of generalizing to compositional capabilities. For example, training on long-form language reasoning together with shorter visual reasoning can improve the model's writing capability for multimodal questions. (3) Data efficiency. We show that randomly downsampling LLaVA's training data mixture by up to 75% does not significantly decrease the model's performance, suggesting that the possibility of a more sophisticated dataset compression strategy can further improve LLaVA's already efficient training pipeline. (4) Data scaling.",
                    "score": 0.41783436746868696,
                    "section_title": "Introduction",
                    "char_start_offset": 1829,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 153,
                            "end": 359
                        },
                        {
                            "start": 360,
                            "end": 616
                        },
                        {
                            "start": 617,
                            "end": 944
                        },
                        {
                            "start": 945,
                            "end": 1086
                        },
                        {
                            "start": 1087,
                            "end": 1201
                        },
                        {
                            "start": 1204,
                            "end": 1295
                        },
                        {
                            "start": 1296,
                            "end": 1362
                        },
                        {
                            "start": 1363,
                            "end": 1629
                        },
                        {
                            "start": 1630,
                            "end": 1661
                        },
                        {
                            "start": 1662,
                            "end": 1757
                        },
                        {
                            "start": 1758,
                            "end": 1919
                        },
                        {
                            "start": 1920,
                            "end": 1940
                        },
                        {
                            "start": 1941,
                            "end": 2225
                        },
                        {
                            "start": 2226,
                            "end": 2243
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.490966796875
                },
                {
                    "corpus_id": "263672058",
                    "title": "Improved Baselines with Visual Instruction Tuning",
                    "text": "Despite the promising results demonstrated by LLaVA-1.5, several limitations must be acknowledged. First, LLaVA-1.5 utilizes full image patches, potentially prolonging each training iteration. While visual resamplers [3,14,32] reduce the number of visual patches in LLMs, they currently cannot achieve convergence as efficiently as LLaVA with a comparable amount of training data, probably due to more trainable parameters in the resamplers. The development of a sample-efficient visual resampler could pave the way for future scaling-up of instruction-following multimodal models. Second, LLaVA-1.5 is not yet capable of processing multiple images due to the lack of such instruction-following data, and the limit of the context length. Third, although LLaVA-1.5 exhibits proficiency in following complex instructions, its problem-solving capabilities can still be limited in certain domains, which could be improved with a more capable language model and with high-quality, targeted visual instruction tuning data. Finally, despite its significantly reduced propensity for hallucination, LLaVA-1.5 is not exempt from producing hallucinations and occasionally disseminating misinformation, and should be used with caution in critical applications (e.g. medical).",
                    "score": 0.3408084741493757,
                    "section_title": "C. Limitations",
                    "char_start_offset": 40297,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 98
                        },
                        {
                            "start": 99,
                            "end": 192
                        },
                        {
                            "start": 193,
                            "end": 441
                        },
                        {
                            "start": 442,
                            "end": 581
                        },
                        {
                            "start": 582,
                            "end": 737
                        },
                        {
                            "start": 738,
                            "end": 1016
                        },
                        {
                            "start": 1017,
                            "end": 1253
                        },
                        {
                            "start": 1254,
                            "end": 1263
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.4833984375
                }
            ],
            "relevance_judgement": 0.53369140625,
            "relevance_judgment_input_expanded": "# Title: Improved Baselines with Visual Instruction Tuning\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee\n## Abstract\nLarge multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available.\n## Introduction\nOur study originates from LLaVA and builds a road map by carefully making effective contributions from the perspectives of the input, model, and data. \n\nFirst, we unveil that the fully-connected vision-language connector in LLaVA is surprisingly powerful and dataefficient, and we establish stronger and more feasible baselines built upon the LLaVA framework. We report that two simple improvements, namely, an MLP cross-modal connector and incorporating academic task related data such as VQA, are orthogonal to the framework of LLaVA, and when used with LLaVA, lead to better multimodal understanding capabilities. In contrast to InstructBLIP [14] or Qwen-VL [3], which trains specially designed visual resamplers on hundreds of millions or even billions of image-text paired data, LLaVA uses one of the simplest architecture design for LMMs and requires only training a simple fullyconnected projection layer on merely 600K image-text pairs. Our final model can finish training in \u223c1 day on a single 8-A100 machine and achieves state-of-the-art results on a wide range of benchmarks. Moreover, unlike Qwen-VL [3] that includes in-house data in training, LLaVA utilizes only publicly available data. \n\nNext, we delve into an early exploration of other open problems of large multimodal models. Our findings include: (1) Scaling to high-resolution image inputs. We show that LLaVA's architecture is versatile in scaling to higher resolutions by simply dividing images into grids and maintains its data efficiency; with the increased resolution, it improves the model's detailed perception capabilities and reduces hallucination. (2) Compositional capabilities. We find that large multimodal models are capable of generalizing to compositional capabilities. For example, training on long-form language reasoning together with shorter visual reasoning can improve the model's writing capability for multimodal questions. (3) Data efficiency. We show that randomly downsampling LLaVA's training data mixture by up to 75% does not significantly decrease the model's performance, suggesting that the possibility of a more sophisticated dataset compression strategy can further improve LLaVA's already efficient training pipeline. (4) Data scaling.\n\n## Scaling the Data and Model\nWe also incorporate ShareGPT [46] data and scale up the LLM to 13B as in [3,8,39]. Results on MM-Vet shows the most significant improvement when scaling the LLM to 13B, suggesting the importance of the base LLM's capability for visual conversations. \n\nLLaVA-1.5. We denote this final model with all the modifications as LLaVA-1.5 (the last two rows in Table 2), which achieves an impressive performance that significantly outperforms the original LLaVA [36]. \n\nComputational cost. For LLaVA-1.5, we use the same pretraining dataset, and keep the training iterations and batch size roughly the same for instruction tuning as LLaVA [36]. \n\nDue to the increased image input resolution to 336 2 , the training of LLaVA-1.5 is \u223c2\u00d7 as long as LLaVA: \u223c6 hours of pretraining and \u223c20 hours of visual instruction tuning, using 8\u00d7 A100s.\n\n## C. Limitations\nDespite the promising results demonstrated by LLaVA-1.5, several limitations must be acknowledged. First, LLaVA-1.5 utilizes full image patches, potentially prolonging each training iteration. While visual resamplers [3,14,32] reduce the number of visual patches in LLMs, they currently cannot achieve convergence as efficiently as LLaVA with a comparable amount of training data, probably due to more trainable parameters in the resamplers. The development of a sample-efficient visual resampler could pave the way for future scaling-up of instruction-following multimodal models. Second, LLaVA-1.5 is not yet capable of processing multiple images due to the lack of such instruction-following data, and the limit of the context length. Third, although LLaVA-1.5 exhibits proficiency in following complex instructions, its problem-solving capabilities can still be limited in certain domains, which could be improved with a more capable language model and with high-quality, targeted visual instruction tuning data. Finally, despite its significantly reduced propensity for hallucination, LLaVA-1.5 is not exempt from producing hallucinations and occasionally disseminating misinformation, and should be used with caution in critical applications (e.g. medical).",
            "reference_string": "[263672058 | Liu et al. | 2023 | Citations: 2824]"
        },
        {
            "title": "Visual Instruction Tuning",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 63,
            "citation_count": 4921,
            "influential_citation_count": 1049,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.08485",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.08485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2143856368",
                    "name": "Haotian Liu"
                },
                {
                    "authorId": "2109737569",
                    "name": "Chunyuan Li"
                },
                {
                    "authorId": "31060482",
                    "name": "Qingyang Wu"
                },
                {
                    "authorId": "144756076",
                    "name": "Yong Jae Lee"
                }
            ],
            "abstract": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.",
            "corpus_id": 258179774,
            "sentences": [
                {
                    "corpus_id": "258179774",
                    "title": "Visual Instruction Tuning",
                    "text": "The broader impact of LLaVA, a general-purpose visual assistant, has potential benefits and risks associated with its deployment and release. Some considerations are unique to LLaVA due to its visual nature, while others share similarities with existing instruction-following LLMs (e.g., Alpaca, Vicuna, etc.). As LLaVA is built upon LLaMA, Vicuna, and CLIP, it inherits some of the issues associated with LLMs and vision encoders. In the following, we outline both the risks and mitigation strategies in place for the release of this model. \n\nMalicious input. To minimize potential misuse and harmful consequences, we employ two precautionary measures for LLaVA: (1) OpenAI Filter API for user input text to prevent harmful or inappropriate text instructions from being processed by the model, and (2) NSFW Filter for uploaded user images to detect and block Not Safe For Work (NSFW) content or any other potentially harmful image inputs. \n\nHallucination. Similar to LLMs, LLaVA might generate outputs that aren't grounded in facts or input data. This raises concerns about inferences made, especially in critical applications (e.g., medical). \n\nBiases. Bias can be transferred from the base models to LLaVA, both from the vision encoder (CLIP) and the language decoder (LLaMA/Vicuna). This may lead to biased outcomes or unfair representations of diverse content. \n\nEnergy consumption. Though energy consumption is not a primary concern for LLaVA due to a smaller pretraining dataset (see details in Sec. C), it may become a concern when scaling up the pretraining dataset or increasing the model size, e.g., to a larger LLaMA version like the 65B model. \n\nEvaluation complexities. Assessing the performance of LLaVA is challenging as it involves both language and visual tasks. Our evaluation benchmark covers several aspects, including accuracy, concept coverage, reasoning ability, and creativity. However, additional aspects need consideration, such as the degree of visual content hallucination and fine-grained understanding of visual content.",
                    "score": 0.3446945978214133,
                    "section_title": "A Broader Impact",
                    "char_start_offset": 28413,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 310
                        },
                        {
                            "start": 311,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 541
                        },
                        {
                            "start": 544,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 939
                        },
                        {
                            "start": 942,
                            "end": 956
                        },
                        {
                            "start": 957,
                            "end": 1047
                        },
                        {
                            "start": 1048,
                            "end": 1144
                        },
                        {
                            "start": 1147,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1286
                        },
                        {
                            "start": 1287,
                            "end": 1365
                        },
                        {
                            "start": 1368,
                            "end": 1387
                        },
                        {
                            "start": 1388,
                            "end": 1506
                        },
                        {
                            "start": 1507,
                            "end": 1656
                        },
                        {
                            "start": 1659,
                            "end": 1683
                        },
                        {
                            "start": 1684,
                            "end": 1780
                        },
                        {
                            "start": 1781,
                            "end": 1902
                        },
                        {
                            "start": 1903,
                            "end": 2051
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52099609375
                }
            ],
            "relevance_judgement": 0.52099609375,
            "relevance_judgment_input_expanded": "# Title: Visual Instruction Tuning\n# Venue: Neural Information Processing Systems\n# Authors: Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee\n## Abstract\nInstruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.\n## A Broader Impact\nThe broader impact of LLaVA, a general-purpose visual assistant, has potential benefits and risks associated with its deployment and release. Some considerations are unique to LLaVA due to its visual nature, while others share similarities with existing instruction-following LLMs (e.g., Alpaca, Vicuna, etc.). As LLaVA is built upon LLaMA, Vicuna, and CLIP, it inherits some of the issues associated with LLMs and vision encoders. In the following, we outline both the risks and mitigation strategies in place for the release of this model. \n\nMalicious input. To minimize potential misuse and harmful consequences, we employ two precautionary measures for LLaVA: (1) OpenAI Filter API for user input text to prevent harmful or inappropriate text instructions from being processed by the model, and (2) NSFW Filter for uploaded user images to detect and block Not Safe For Work (NSFW) content or any other potentially harmful image inputs. \n\nHallucination. Similar to LLMs, LLaVA might generate outputs that aren't grounded in facts or input data. This raises concerns about inferences made, especially in critical applications (e.g., medical). \n\nBiases. Bias can be transferred from the base models to LLaVA, both from the vision encoder (CLIP) and the language decoder (LLaMA/Vicuna). This may lead to biased outcomes or unfair representations of diverse content. \n\nEnergy consumption. Though energy consumption is not a primary concern for LLaVA due to a smaller pretraining dataset (see details in Sec. C), it may become a concern when scaling up the pretraining dataset or increasing the model size, e.g., to a larger LLaMA version like the 65B model. \n\nEvaluation complexities. Assessing the performance of LLaVA is challenging as it involves both language and visual tasks. Our evaluation benchmark covers several aspects, including accuracy, concept coverage, reasoning ability, and creativity. However, additional aspects need consideration, such as the degree of visual content hallucination and fine-grained understanding of visual content.",
            "reference_string": "[258179774 | Liu et al. | 2023 | Citations: 4921]"
        },
        {
            "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 82,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.05067, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313875208",
                    "name": "Jia-Xin Zhao"
                },
                {
                    "authorId": "2342467513",
                    "name": "Boyuan Sun"
                },
                {
                    "authorId": "2339423925",
                    "name": "Xiang Chen"
                },
                {
                    "authorId": "2339268195",
                    "name": "Xihan Wei"
                },
                {
                    "authorId": "2339266488",
                    "name": "Qibin Hou"
                }
            ],
            "abstract": "In this paper, we introduce LLaVA-Octopus, a novel video multimodal large language model. LLaVA-Octopus adaptively weights features from different visual projectors based on user instructions, enabling us to leverage the complementary strengths of each projector. We observe that different visual projectors exhibit distinct characteristics when handling specific tasks. For instance, some projectors excel at capturing static details, while others are more effective at processing temporal information, and some are better suited for tasks requiring temporal coherence. By dynamically adjusting feature weights according to user instructions, LLaVA-Octopus dynamically selects and combines the most suitable features, significantly enhancing the model's performance in multimodal tasks. Experimental results demonstrate that LLaVA-Octopus achieves excellent performance across multiple benchmarks, especially in tasks such as video question answering, long video understanding, and comprehensive multi-choices benchmarks, highlighting its broad application potential.",
            "corpus_id": 275405668,
            "sentences": [
                {
                    "corpus_id": "275405668",
                    "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding",
                    "text": "Currently, multimodal large language models can be categorized into community models and proprietary models. Proprietary models [3,[52][53][54]64] often achieve better performance but are not open-sourced. Meanwhile, community models [17,23,27,29,30,35,38,39,80,81,85,91], which have seen rapid performance improvements, are garnering increasing attention due to their open-source nature, including model architecture, weights, and even training data. LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously. \n\nBased on the ideas of LLaVA, several variant series have emerged, such as the mPLUG-owl series. mPLUG-owl [80] introduces a new paradigm for training large language models through modularity, and the latest version, mPLUG-owl3 [78], can even understand 2-hour movie videos. BLIP-2 [31] uses Q-Former [88] to connect the visual and linguistic modalities. In BLIP-3 [76], Q-Former is replaced by more scalable visual token samplers, such as perceptual resamplers. We observe that numerous methods have explored various visual projectors. However, to the best of our knowledge, we are the first to classify these projectors and analyze their complementarity.",
                    "score": 0.47668504563924624,
                    "section_title": "Multimodal Large Language Model",
                    "char_start_offset": 6442,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 108
                        },
                        {
                            "start": 109,
                            "end": 205
                        },
                        {
                            "start": 206,
                            "end": 451
                        },
                        {
                            "start": 452,
                            "end": 724
                        },
                        {
                            "start": 725,
                            "end": 852
                        },
                        {
                            "start": 853,
                            "end": 1066
                        },
                        {
                            "start": 1069,
                            "end": 1164
                        },
                        {
                            "start": 1165,
                            "end": 1342
                        },
                        {
                            "start": 1343,
                            "end": 1422
                        },
                        {
                            "start": 1423,
                            "end": 1530
                        },
                        {
                            "start": 1531,
                            "end": 1604
                        },
                        {
                            "start": 1605,
                            "end": 1724
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1350,
                            "end": 1354,
                            "matchedPaperCorpusId": "256390509"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51123046875
                }
            ],
            "relevance_judgement": 0.51123046875,
            "relevance_judgment_input_expanded": "# Title: LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding\n# Venue: arXiv.org\n# Authors: Jia-Xin Zhao, Boyuan Sun, Xiang Chen, Xihan Wei, Qibin Hou\n## Abstract\nIn this paper, we introduce LLaVA-Octopus, a novel video multimodal large language model. LLaVA-Octopus adaptively weights features from different visual projectors based on user instructions, enabling us to leverage the complementary strengths of each projector. We observe that different visual projectors exhibit distinct characteristics when handling specific tasks. For instance, some projectors excel at capturing static details, while others are more effective at processing temporal information, and some are better suited for tasks requiring temporal coherence. By dynamically adjusting feature weights according to user instructions, LLaVA-Octopus dynamically selects and combines the most suitable features, significantly enhancing the model's performance in multimodal tasks. Experimental results demonstrate that LLaVA-Octopus achieves excellent performance across multiple benchmarks, especially in tasks such as video question answering, long video understanding, and comprehensive multi-choices benchmarks, highlighting its broad application potential.\n## Multimodal Large Language Model\nCurrently, multimodal large language models can be categorized into community models and proprietary models. Proprietary models [3,[52][53][54]64] often achieve better performance but are not open-sourced. Meanwhile, community models [17,23,27,29,30,35,38,39,80,81,85,91], which have seen rapid performance improvements, are garnering increasing attention due to their open-source nature, including model architecture, weights, and even training data. LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously. \n\nBased on the ideas of LLaVA, several variant series have emerged, such as the mPLUG-owl series. mPLUG-owl [80] introduces a new paradigm for training large language models through modularity, and the latest version, mPLUG-owl3 [78], can even understand 2-hour movie videos. BLIP-2 [31] uses Q-Former [88] to connect the visual and linguistic modalities. In BLIP-3 [76], Q-Former is replaced by more scalable visual token samplers, such as perceptual resamplers. We observe that numerous methods have explored various visual projectors. However, to the best of our knowledge, we are the first to classify these projectors and analyze their complementarity.",
            "reference_string": "[275405668 | Zhao et al. | 2025 | Citations: 5]"
        },
        {
            "title": "Teaching Human Behavior Improves Content Understanding Abilities Of LLMs",
            "venue": "",
            "year": 2024,
            "reference_count": 93,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.00942, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108291106",
                    "name": "Somesh Singh"
                },
                {
                    "authorId": "2237424078",
                    "name": "I. HariniS."
                },
                {
                    "authorId": "50793081",
                    "name": "Yaman Kumar Singla"
                },
                {
                    "authorId": "3180819",
                    "name": "V. Baths"
                },
                {
                    "authorId": "1753278",
                    "name": "R. Shah"
                },
                {
                    "authorId": "2261910102",
                    "name": "Changyou Chen"
                },
                {
                    "authorId": "2261737967",
                    "name": "Balaji Krishnamurthy"
                }
            ],
            "abstract": "Communication is defined as\"Who says what to whom with what effect\". A message from a communicator generates downstream receiver effects, also known as behavior. Receiver behavior, being a downstream effect of the message, carries rich signals about it. Even after carrying signals about the message, the behavior data is often ignored while training large language models. We show that training LLMs on receiver behavior can actually help improve their content-understanding abilities. Specifically, we show that training LLMs to predict the receiver behavior of likes and comments improves the LLM's performance on a wide variety of downstream content understanding tasks. We show this performance increase over 46 video and image understanding tasks over 26 benchmark datasets across both 0-shot and fine-tuning settings, outperforming many supervised baselines. Moreover, since receiver behavior, such as likes and comments, is collected by default on the internet and does not need any human annotations to be useful, the performance improvement we get after training on this data is essentially free-lunch. We release the receiver behavior cleaned comments and likes of 750k images and videos collected from multiple platforms along with our instruction-tuning data.",
            "corpus_id": 269502355,
            "sentences": [
                {
                    "corpus_id": "269502355",
                    "title": "Teaching Human Behavior Improves Content Understanding Abilities Of LLMs",
                    "text": "After extensive filtering and cleaning, we are left with 730k samples of videos and images across the two platforms which we use for the next steps. \n\nAfter collecting user behavior over image and video content, we design tasks to teach large vision and language models (VLMs) to simulate user behavior. For this, we use an instruction fine-tuning format. Given a video or an image and the other metadata like time of post and channel, we ask the model to simulate user behavior of likes and comments. See Fig 2, Listing 1 for examples. We choose LLaMA-Vid (Li et al., 2023b) as our base model to teach it the user behavior. We call the resultant model Behavior-LLaVA (Large Language and Vision Assistant) (Liu et al., 2023a). We test Behavior-LLaVA on a diverse variety of tasks, evaluating its capabilities on image, video, text, and audio understanding tasks. We compare Behavior-LLaVA against its base model, LLaMA-Vid, and other supervised baselines. Further, to show the impact of behavior, we train another version of LLaMA-Vid, where we train it on the same set of videos and images as Behavior-LLaVA but do not include behavior information. We call this model Ad-LLaVA. \n\nWe make the following contributions with this work: 1) Behavior-LLaVA Instruction Fine-Tuning: We explore the idea of learning human behavior, resulting in better content understanding. We test this for action-level behavior data such as receiver comments, likes, and replay graphs. We collect a dataset called BLIFT, consisting of 400k images and 330k videos, along with their receiver behavior. Then, LLaMA-Vid is trained for the task of predicting receiver comments and upvotes given a media (a video or an image) (Listing 1). We show that using this simple task formulation over behavioral data collected in the wild, results in performance improvement over a hierarchy of tasks. We get improvements over the base LLaMA-Vid across 46 tasks over 26 benchmark datasets in both zero-shot and fine-tuned settings. We show this over low-level content understanding tasks like object and activity recognition and also over high-level tasks like topic and emotion detection.",
                    "score": 0.3429416350976343,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 5373,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 151,
                            "end": 303
                        },
                        {
                            "start": 304,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 501
                        },
                        {
                            "start": 502,
                            "end": 536
                        },
                        {
                            "start": 537,
                            "end": 624
                        },
                        {
                            "start": 625,
                            "end": 726
                        },
                        {
                            "start": 727,
                            "end": 862
                        },
                        {
                            "start": 863,
                            "end": 955
                        },
                        {
                            "start": 956,
                            "end": 1149
                        },
                        {
                            "start": 1150,
                            "end": 1178
                        },
                        {
                            "start": 1181,
                            "end": 1366
                        },
                        {
                            "start": 1367,
                            "end": 1463
                        },
                        {
                            "start": 1464,
                            "end": 1577
                        },
                        {
                            "start": 1578,
                            "end": 1710
                        },
                        {
                            "start": 1711,
                            "end": 1864
                        },
                        {
                            "start": 1865,
                            "end": 1994
                        },
                        {
                            "start": 1995,
                            "end": 2152
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.50439453125
                }
            ],
            "relevance_judgement": 0.50439453125,
            "relevance_judgment_input_expanded": "# Title: Teaching Human Behavior Improves Content Understanding Abilities Of LLMs\n# Venue: \n# Authors: Somesh Singh, I. HariniS., Yaman Kumar Singla, V. Baths, R. Shah, Changyou Chen, Balaji Krishnamurthy\n## Abstract\nCommunication is defined as\"Who says what to whom with what effect\". A message from a communicator generates downstream receiver effects, also known as behavior. Receiver behavior, being a downstream effect of the message, carries rich signals about it. Even after carrying signals about the message, the behavior data is often ignored while training large language models. We show that training LLMs on receiver behavior can actually help improve their content-understanding abilities. Specifically, we show that training LLMs to predict the receiver behavior of likes and comments improves the LLM's performance on a wide variety of downstream content understanding tasks. We show this performance increase over 46 video and image understanding tasks over 26 benchmark datasets across both 0-shot and fine-tuning settings, outperforming many supervised baselines. Moreover, since receiver behavior, such as likes and comments, is collected by default on the internet and does not need any human annotations to be useful, the performance improvement we get after training on this data is essentially free-lunch. We release the receiver behavior cleaned comments and likes of 750k images and videos collected from multiple platforms along with our instruction-tuning data.\n## INTRODUCTION\nAfter extensive filtering and cleaning, we are left with 730k samples of videos and images across the two platforms which we use for the next steps. \n\nAfter collecting user behavior over image and video content, we design tasks to teach large vision and language models (VLMs) to simulate user behavior. For this, we use an instruction fine-tuning format. Given a video or an image and the other metadata like time of post and channel, we ask the model to simulate user behavior of likes and comments. See Fig 2, Listing 1 for examples. We choose LLaMA-Vid (Li et al., 2023b) as our base model to teach it the user behavior. We call the resultant model Behavior-LLaVA (Large Language and Vision Assistant) (Liu et al., 2023a). We test Behavior-LLaVA on a diverse variety of tasks, evaluating its capabilities on image, video, text, and audio understanding tasks. We compare Behavior-LLaVA against its base model, LLaMA-Vid, and other supervised baselines. Further, to show the impact of behavior, we train another version of LLaMA-Vid, where we train it on the same set of videos and images as Behavior-LLaVA but do not include behavior information. We call this model Ad-LLaVA. \n\nWe make the following contributions with this work: 1) Behavior-LLaVA Instruction Fine-Tuning: We explore the idea of learning human behavior, resulting in better content understanding. We test this for action-level behavior data such as receiver comments, likes, and replay graphs. We collect a dataset called BLIFT, consisting of 400k images and 330k videos, along with their receiver behavior. Then, LLaMA-Vid is trained for the task of predicting receiver comments and upvotes given a media (a video or an image) (Listing 1). We show that using this simple task formulation over behavioral data collected in the wild, results in performance improvement over a hierarchy of tasks. We get improvements over the base LLaMA-Vid across 46 tasks over 26 benchmark datasets in both zero-shot and fine-tuned settings. We show this over low-level content understanding tasks like object and activity recognition and also over high-level tasks like topic and emotion detection.",
            "reference_string": "[269502355 | Singh et al. | 2024 | Citations: 4]"
        },
        {
            "title": "GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 71,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.19325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2142395483",
                    "name": "M. S. Danish"
                },
                {
                    "authorId": "2320806255",
                    "name": "Muhammad Akhtar Munir"
                },
                {
                    "authorId": "143624413",
                    "name": "Syed Roshaan Ali Shah"
                },
                {
                    "authorId": "2268410475",
                    "name": "Kartik Kuckreja"
                },
                {
                    "authorId": "2242949919",
                    "name": "F. Khan"
                },
                {
                    "authorId": "2333230919",
                    "name": "Paolo Fraccaro"
                },
                {
                    "authorId": "2333231235",
                    "name": "Alexandre Lacoste"
                },
                {
                    "authorId": "2333275134",
                    "name": "Salman Khan"
                }
            ],
            "abstract": "While numerous recent benchmarks focus on evaluating generic Vision-Language Models (VLMs), they do not effectively address the specific challenges of geospatial applications. Generic VLM benchmarks are not designed to handle the complexities of geospatial data, an essential component for applications such as environmental monitoring, urban planning, and disaster management. Key challenges in the geospatial domain include temporal change detection, large-scale object counting, tiny object detection, and understanding relationships between entities in remote sensing imagery. To bridge this gap, we present GEOBench-VLM, a comprehensive benchmark specifically designed to evaluate VLMs on geospatial tasks, including scene understanding, object counting, localization, fine-grained categorization, segmentation, and temporal analysis. Our benchmark features over 10,000 manually verified instructions and spanning diverse visual conditions, object types, and scales. We evaluate several state-of-the-art VLMs to assess performance on geospatial-specific challenges. The results indicate that although existing VLMs demonstrate potential, they face challenges when dealing with geospatial-specific tasks, highlighting the room for further improvements. Notably, the best-performing LLaVa-OneVision achieves only 41.7% accuracy on MCQs, slightly more than GPT-4o, which is approximately double the random guess performance. Our benchmark is publicly available at https://github.com/The-AI-Alliance/GEO-Bench-VLM .",
            "corpus_id": 274422305,
            "sentences": [
                {
                    "corpus_id": "274422305",
                    "title": "GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks",
                    "text": "Model Variability and Capabilities: We selected both generic and geospatial-specific VLMs, prioritizing recently developed models with advanced capabilities across various tasks. Generic VLMs include LLaVA-1.5 [30], LLaVA-NeXT [29], LLaVA-OneVision [23], Sphinx [27], Ferret [62], InternVL2 [9], and Qwen2-VL [54]. Also, we added GPT-4o, a closed-source, commercially available model, to compare its adaptability to remote sensing tasks with open-source models. These VLMs demonstrate robust performance in tasks such as scene understanding, and finegrained visual classification. For geospatial-specific VLMs, we selected GeoChat [19], RS-LLaVA [5], SkySenseGPT [34], EarthDial [47], and LHRS-Bot-Nova [36], models tailored for satellite and aerial image interpretation. Domain Specific Model Suitability: We consider VLMs based on their relevance to geospatial tasks. Despite being trained on satellite and aerial data, domain-specific models [5,19,34,36,47] may struggle with counting and spatial relationships due to dataset or architecture constraints. Meanwhile, we evaluate whether generic VLMs can effectively handle scene understanding, object detection, and visual reasoning in remote sensing. This selection ensures a focused study of VLM performance across diverse geospatial applications. Open vs Closed Models: Open-source models like LLaVA [23,29,30], Qwen2-VL [54], GeoChat [19], and others provide transparency, aiding in understanding their strengths and limitations. Closed models like GPT-4o, despite their lack of transparency, demonstrate strong generalization and performance on complex tasks, benefiting from proprietary datasets and advanced architectures. Evaluating both ensures a comprehensive assessment of VLM capabilities.",
                    "score": 0.5029675009201882,
                    "section_title": "Selection of VLMs",
                    "char_start_offset": 11741,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 314
                        },
                        {
                            "start": 315,
                            "end": 461
                        },
                        {
                            "start": 462,
                            "end": 580
                        },
                        {
                            "start": 581,
                            "end": 771
                        },
                        {
                            "start": 772,
                            "end": 869
                        },
                        {
                            "start": 870,
                            "end": 1057
                        },
                        {
                            "start": 1058,
                            "end": 1203
                        },
                        {
                            "start": 1204,
                            "end": 1301
                        },
                        {
                            "start": 1302,
                            "end": 1485
                        },
                        {
                            "start": 1486,
                            "end": 1681
                        },
                        {
                            "start": 1682,
                            "end": 1753
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 210,
                            "end": 214,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 291,
                            "end": 294,
                            "matchedPaperCorpusId": "266521410"
                        },
                        {
                            "start": 631,
                            "end": 635,
                            "matchedPaperCorpusId": "265456719"
                        },
                        {
                            "start": 646,
                            "end": 649,
                            "matchedPaperCorpusId": "269354784"
                        },
                        {
                            "start": 703,
                            "end": 707,
                            "matchedPaperCorpusId": "267412588"
                        },
                        {
                            "start": 945,
                            "end": 948,
                            "matchedPaperCorpusId": "269354784"
                        },
                        {
                            "start": 948,
                            "end": 951,
                            "matchedPaperCorpusId": "265456719"
                        },
                        {
                            "start": 954,
                            "end": 957,
                            "matchedPaperCorpusId": "267412588"
                        },
                        {
                            "start": 1362,
                            "end": 1365,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 1390,
                            "end": 1394,
                            "matchedPaperCorpusId": "265456719"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5029296875
                }
            ],
            "relevance_judgement": 0.5029296875,
            "relevance_judgment_input_expanded": "# Title: GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks\n# Venue: arXiv.org\n# Authors: M. S. Danish, Muhammad Akhtar Munir, Syed Roshaan Ali Shah, Kartik Kuckreja, F. Khan, Paolo Fraccaro, Alexandre Lacoste, Salman Khan\n## Abstract\nWhile numerous recent benchmarks focus on evaluating generic Vision-Language Models (VLMs), they do not effectively address the specific challenges of geospatial applications. Generic VLM benchmarks are not designed to handle the complexities of geospatial data, an essential component for applications such as environmental monitoring, urban planning, and disaster management. Key challenges in the geospatial domain include temporal change detection, large-scale object counting, tiny object detection, and understanding relationships between entities in remote sensing imagery. To bridge this gap, we present GEOBench-VLM, a comprehensive benchmark specifically designed to evaluate VLMs on geospatial tasks, including scene understanding, object counting, localization, fine-grained categorization, segmentation, and temporal analysis. Our benchmark features over 10,000 manually verified instructions and spanning diverse visual conditions, object types, and scales. We evaluate several state-of-the-art VLMs to assess performance on geospatial-specific challenges. The results indicate that although existing VLMs demonstrate potential, they face challenges when dealing with geospatial-specific tasks, highlighting the room for further improvements. Notably, the best-performing LLaVa-OneVision achieves only 41.7% accuracy on MCQs, slightly more than GPT-4o, which is approximately double the random guess performance. Our benchmark is publicly available at https://github.com/The-AI-Alliance/GEO-Bench-VLM .\n## Selection of VLMs\nModel Variability and Capabilities: We selected both generic and geospatial-specific VLMs, prioritizing recently developed models with advanced capabilities across various tasks. Generic VLMs include LLaVA-1.5 [30], LLaVA-NeXT [29], LLaVA-OneVision [23], Sphinx [27], Ferret [62], InternVL2 [9], and Qwen2-VL [54]. Also, we added GPT-4o, a closed-source, commercially available model, to compare its adaptability to remote sensing tasks with open-source models. These VLMs demonstrate robust performance in tasks such as scene understanding, and finegrained visual classification. For geospatial-specific VLMs, we selected GeoChat [19], RS-LLaVA [5], SkySenseGPT [34], EarthDial [47], and LHRS-Bot-Nova [36], models tailored for satellite and aerial image interpretation. Domain Specific Model Suitability: We consider VLMs based on their relevance to geospatial tasks. Despite being trained on satellite and aerial data, domain-specific models [5,19,34,36,47] may struggle with counting and spatial relationships due to dataset or architecture constraints. Meanwhile, we evaluate whether generic VLMs can effectively handle scene understanding, object detection, and visual reasoning in remote sensing. This selection ensures a focused study of VLM performance across diverse geospatial applications. Open vs Closed Models: Open-source models like LLaVA [23,29,30], Qwen2-VL [54], GeoChat [19], and others provide transparency, aiding in understanding their strengths and limitations. Closed models like GPT-4o, despite their lack of transparency, demonstrate strong generalization and performance on complex tasks, benefiting from proprietary datasets and advanced architectures. Evaluating both ensures a comprehensive assessment of VLM capabilities.",
            "reference_string": "[274422305 | Danish et al. | 2024 | Citations: 8]"
        },
        {
            "title": "LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 24,
            "citation_count": 10,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.01331, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2287831755",
                    "name": "Musashi Hinck"
                },
                {
                    "authorId": "2294573012",
                    "name": "M. L. Olson"
                },
                {
                    "authorId": "2294574310",
                    "name": "David Cobbley"
                },
                {
                    "authorId": "3156011",
                    "name": "Shao-Yen Tseng"
                },
                {
                    "authorId": "95340164",
                    "name": "Vasudev Lal"
                }
            ],
            "abstract": "We train a suite of multimodal foundation models (MMFM) using the popular LLaVA framework with the recently released Gemma family of large language models (LLMs). Of particular interest is the 2B parameter Gemma model, which provides opportunities to construct capable small-scale MMFMs. In line with findings from other papers in this space, we test the effect of ablating three design features: pretraining the connector, utilizing a more powerful image backbone, and increasing the size of the language backbone. The resulting models, which we call LLaVA-Gemma, exhibit moderate performance on an array of evaluations, but fail to improve past the current comparably sized SOTA models. Closer analysis of performance shows mixed effects; skipping pretraining tends to reduce performance, larger vision models sometimes improve performance, and increasing language model size has inconsistent effects. We publicly release training recipes, code and weights for our models for the LLaVA-Gemma models.",
            "corpus_id": 268857227,
            "sentences": [
                {
                    "corpus_id": "268857227",
                    "title": "LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model",
                    "text": "In this paper, we introduce LLaVA-Gemma, a suite of vision-language assistants trained from the Gemma Large Language Model (LLM) variants, Gemma-2B and Gemma-7B [17].Our work is inspired by the rapid progress in small but capable visual language models (VLMs), such as LLaVA-Phi [23], which have demonstrated remarkable efficiency and effectiveness in various language understanding tasks.LLaVA-Gemma distinguishes itself among small VLMs due to the public release of similarly trained, different-sized LLMs Gemma-2B and Gemma-7B.\n\nThe unique release of the Gemma models offers an opportunity to contrast model performance in relation to pa-rameter size and visual encoding capabilities.By possessing two variants with different parameter sizes, LLaVA-Gemma allows researchers to investigate the trade-offs between computational efficiency and the richness of visual and linguistic understanding.With these two variants, we perform a deeper exploration of how varying levels of model complexity influence the effectiveness of visual encoding, providing valuable insights into the optimization of small VLMs for diverse tasks and environments.Furthermore, the use of significantly more unique tokens, at 256k, offers an opportunity to investigate how a massively increased token set effects multi-modal performance.\n\nRecent advancements in (LLMs) [20] and multimodal foundation models (MMFMs) [7] have propelled the interest and development of Large Multimodal Models (LMMs).Notable models like GPT-4 [1], LLaVA [9,10], and their derivatives have demonstrated significant performance in vision-language tasks such as Visual Question Answering (VQA) and image captioning [5].However, the computational demands of deploying these models have led to the exploration of small-scale LMMs.Our work aims to provide a unified analysis of small-scale LMMs, examining how model selections, training recipes, and data contribute to performance, which is distinct from existing works such as LLaVA-Phi.\n\nOur contributions are as follows:\n\n1. We introduce LLaVA-Gemma, a MMFM that leverages the compact yet powerful Gemma language models for efficient multimodal interactions.",
                    "score": 0.3643696642728001,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 166
                        },
                        {
                            "start": 166,
                            "end": 389
                        },
                        {
                            "start": 389,
                            "end": 530
                        },
                        {
                            "start": 532,
                            "end": 687
                        },
                        {
                            "start": 687,
                            "end": 896
                        },
                        {
                            "start": 896,
                            "end": 1142
                        },
                        {
                            "start": 1142,
                            "end": 1314
                        },
                        {
                            "start": 1316,
                            "end": 1474
                        },
                        {
                            "start": 1474,
                            "end": 1673
                        },
                        {
                            "start": 1673,
                            "end": 1782
                        },
                        {
                            "start": 1782,
                            "end": 1989
                        },
                        {
                            "start": 1991,
                            "end": 2024
                        },
                        {
                            "start": 2026,
                            "end": 2162
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1346,
                            "end": 1350,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1514,
                            "end": 1517,
                            "matchedPaperCorpusId": "258179774"
                        },
                        {
                            "start": 1669,
                            "end": 1672,
                            "matchedPaperCorpusId": "152282269"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5009765625
                }
            ],
            "relevance_judgement": 0.5009765625,
            "relevance_judgment_input_expanded": "# Title: LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model\n# Venue: arXiv.org\n# Authors: Musashi Hinck, M. L. Olson, David Cobbley, Shao-Yen Tseng, Vasudev Lal\n## Abstract\nWe train a suite of multimodal foundation models (MMFM) using the popular LLaVA framework with the recently released Gemma family of large language models (LLMs). Of particular interest is the 2B parameter Gemma model, which provides opportunities to construct capable small-scale MMFMs. In line with findings from other papers in this space, we test the effect of ablating three design features: pretraining the connector, utilizing a more powerful image backbone, and increasing the size of the language backbone. The resulting models, which we call LLaVA-Gemma, exhibit moderate performance on an array of evaluations, but fail to improve past the current comparably sized SOTA models. Closer analysis of performance shows mixed effects; skipping pretraining tends to reduce performance, larger vision models sometimes improve performance, and increasing language model size has inconsistent effects. We publicly release training recipes, code and weights for our models for the LLaVA-Gemma models.\n## Introduction\nIn this paper, we introduce LLaVA-Gemma, a suite of vision-language assistants trained from the Gemma Large Language Model (LLM) variants, Gemma-2B and Gemma-7B [17].Our work is inspired by the rapid progress in small but capable visual language models (VLMs), such as LLaVA-Phi [23], which have demonstrated remarkable efficiency and effectiveness in various language understanding tasks.LLaVA-Gemma distinguishes itself among small VLMs due to the public release of similarly trained, different-sized LLMs Gemma-2B and Gemma-7B.\n\nThe unique release of the Gemma models offers an opportunity to contrast model performance in relation to pa-rameter size and visual encoding capabilities.By possessing two variants with different parameter sizes, LLaVA-Gemma allows researchers to investigate the trade-offs between computational efficiency and the richness of visual and linguistic understanding.With these two variants, we perform a deeper exploration of how varying levels of model complexity influence the effectiveness of visual encoding, providing valuable insights into the optimization of small VLMs for diverse tasks and environments.Furthermore, the use of significantly more unique tokens, at 256k, offers an opportunity to investigate how a massively increased token set effects multi-modal performance.\n\nRecent advancements in (LLMs) [20] and multimodal foundation models (MMFMs) [7] have propelled the interest and development of Large Multimodal Models (LMMs).Notable models like GPT-4 [1], LLaVA [9,10], and their derivatives have demonstrated significant performance in vision-language tasks such as Visual Question Answering (VQA) and image captioning [5].However, the computational demands of deploying these models have led to the exploration of small-scale LMMs.Our work aims to provide a unified analysis of small-scale LMMs, examining how model selections, training recipes, and data contribute to performance, which is distinct from existing works such as LLaVA-Phi.\n\nOur contributions are as follows:\n\n1. We introduce LLaVA-Gemma, a MMFM that leverages the compact yet powerful Gemma language models for efficient multimodal interactions.",
            "reference_string": "[268857227 | Hinck et al. | 2024 | Citations: 10]"
        },
        {
            "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
            "venue": "European Conference on Computer Vision",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 121,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.11703, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292215241",
                    "name": "Ruyi Xu"
                },
                {
                    "authorId": "1390925224",
                    "name": "Yuan Yao"
                },
                {
                    "authorId": "2292210274",
                    "name": "Zonghao Guo"
                },
                {
                    "authorId": "2292213035",
                    "name": "Junbo Cui"
                },
                {
                    "authorId": "1749325163",
                    "name": "Zanlin Ni"
                },
                {
                    "authorId": "2130368185",
                    "name": "Chunjiang Ge"
                },
                {
                    "authorId": "2257036129",
                    "name": "Tat-Seng Chua"
                },
                {
                    "authorId": null,
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2273551430",
                    "name": "Maosong Sun"
                },
                {
                    "authorId": "2292186450",
                    "name": "Gao Huang"
                }
            ],
            "abstract": "Visual encoding constitutes the basis of large multimodal models (LMMs) in understanding the visual world. Conventional LMMs process images in fixed sizes and limited resolutions, while recent explorations in this direction are limited in adaptivity, efficiency, and even correctness. In this work, we first take GPT-4V and LLaVA-1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy. To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution. LLaVA-UHD includes three key components: (1) An image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding, (2) a compression module that further condenses image tokens from visual encoders, and (3) a spatial schema to organize slice tokens for LLMs. Comprehensive experiments show that LLaVA-UHD outperforms established LMMs trained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our model built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088) resolution images using only 94% inference computation, and achieves 6.4 accuracy improvement on TextVQA. Moreover, the model can be efficiently trained in academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of LLaVA-1.5). We make the data and code publicly available at https://github.com/thunlp/LLaVA-UHD.",
            "corpus_id": 268531413,
            "sentences": [
                {
                    "corpus_id": "268531413",
                    "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
                    "text": "Visual encoding constitutes the basis of large multimodal models (LMMs) in understanding the visual world. Conventional LMMs process images in fixed sizes and limited resolutions, while recent explorations in this direction are limited in adaptivity, efficiency, and even correctness. In this work, we first take GPT-4V and LLaVA-1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy. To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution. LLaVA-UHD includes three key components: (1) An image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding, (2) a compression module that further condenses image tokens from visual encoders, and (3) a spatial schema to organize slice tokens for LLMs. Comprehensive experiments show that LLaVA-UHD outperforms established LMMs trained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our model built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088) resolution images using only 94% inference computation, and achieves 6.4 accuracy improvement on TextVQA. Moreover, the model can be efficiently trained in academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of LLaVA-1.5). We make the data and code publicly available at https://github.com/thunlp/LLaVA-UHD.",
                    "score": 0.3477734602899395,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5
                }
            ],
            "relevance_judgement": 0.5,
            "relevance_judgment_input_expanded": "# Title: LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images\n# Venue: European Conference on Computer Vision\n# Authors: Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, Gao Huang\n## Abstract\nVisual encoding constitutes the basis of large multimodal models (LMMs) in understanding the visual world. Conventional LMMs process images in fixed sizes and limited resolutions, while recent explorations in this direction are limited in adaptivity, efficiency, and even correctness. In this work, we first take GPT-4V and LLaVA-1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy. To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution. LLaVA-UHD includes three key components: (1) An image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding, (2) a compression module that further condenses image tokens from visual encoders, and (3) a spatial schema to organize slice tokens for LLMs. Comprehensive experiments show that LLaVA-UHD outperforms established LMMs trained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our model built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088) resolution images using only 94% inference computation, and achieves 6.4 accuracy improvement on TextVQA. Moreover, the model can be efficiently trained in academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of LLaVA-1.5). We make the data and code publicly available at https://github.com/thunlp/LLaVA-UHD.\n",
            "reference_string": "[268531413 | Xu et al. | 2024 | Citations: 121]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "275118957",
            "title": "MVTamperBench: Evaluating Robustness of Vision-Language Models",
            "text": "Model Families and Versions Figure 13 provides a comprehensive taxonomy of Large Multimodal Models (LMMs), showcasing their diversity across families, versions, and first-generation re-  This taxonomy highlights the wide range of LMMs currently available, organized as follows: \n\n\u2022 InternVL Family: Spanning models from Intern-VL1.1 (Chen et al., 2024c;Gao et al., 2024;Chen et al., 2024b) to the advanced Intern-VL2.5- MPO (Chen et al., 2024d), this family emphasizes efficiency and adaptability for diverse tampering and multimodal tasks. \n\nSuccessive iterations demonstrate marked improvements, particularly in handling temporal disruptions such as Dropping and Substitution. The InternVL2-5-8B models showcases its ability to handle fine-grained spatiotemporal reasoning, highlighting its dominance in high-resource benchmarks. \n\n\u2022 LLaVA Family: Starting with LLaVa-NEXT (Li et al., 2024d), which struggled across most benchmarks, this family has evolved with models like LLaVa-OneVision (Li et al., 2024b) and LLaVa-Video (Zhang et al., 2024), demonstrating significant improvements in task-specific video understanding through op- timized pretraining and alignment techniques. Despite the advancements, LLaVa-OneVision & LLaVa-Video continues to face challenges in handling complex temporal disruptions, unlike Chat-UniVi, which has emerged as a robust alternative. \n\n\u2022 VILA Family: The VILA series, including VILA-1.5 (Lin et al., 2024b)   For low-performing models (Figure 14a), Llava-OneVision exhibits consistently weak performance across tampering types, even at larger parameter sizes (e.g., Llava-OneVision-72B). This suggests potential architectural and training data limitations, particularly for temporal coherence tasks. Interestingly, Qwen2-VL-7B underperforms significantly compared to its larger counterpart, Qwen2-VL-72B, which achieves a notable improvement. This indicates that increasing model size, combined with its training paradigm, positively impacts robustness for this family.",
            "score": 0.6896530461849129,
            "section_title": "Overview of Large Multimodality Models (LMMs)",
            "char_start_offset": 20007,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 280,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 1425,
                    "end": 1444,
                    "matchedPaperCorpusId": "266174746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.342041015625
        },
        {
            "corpus_id": "277452239",
            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
            "text": "LLaVA (Large Language and Vision Assistant) (Liu et al., 2023) is a state-of-the-art multimodal model that integrates both visual and textual understanding, combining the capabilities of large language models (LLMs) with vision processing abilities. Its primary function is to interpret and generate responses to input that includes both images and text, making it ideal for tasks like visual question answering (VQA), image captioning, and other vision-language tasks. Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks. Various variants or specialization of the first LLaVA models have been produced so far. For example, TinyLLaVA (Zhou et al., 2024;Jia et al., 2024) is a compact refactored variant of the original LLaVA 1.5 model, designed to enable easier inclusion of alternative light vision and LLM models, thus significantly reducing overall model size and resource requirements. LLaVA-Med (Li et al., 2023a) is a specialized variant of the LLaVA model designed to assist in medical image analysis and diagnostics by fine-tuning its multimodal capabilities on medical datasets such as X-rays, MRIs, and other healthcare-related visual data.",
            "score": 0.5779386198684945,
            "section_title": "Model overview",
            "char_start_offset": 10302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2178
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8369140625
        },
        {
            "corpus_id": "270380326",
            "title": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs",
            "text": "We also include several prominent open-source models to provide a broader context for our evaluations. Specifically, we compare our VideoLLaMA 2 with VistaLLaMA (Ma et al., 2023), ChatUniVi (Jin et al., 2023b), LLaMA-VID (Li et al., 2023d), Video-LLaVA (Lin et al., 2023a), VideoChat2 (Li et al., 2023c), LLaVA-NeXT-Video Series13 (Liu et al., 2024b), VILA 1.5 (Lin et al., 2024), PLLaVA (Xu et al., 2024a), and LLaVA-OneVision (Li et al., 2024a). These open-source models are crucial for evaluating the performance of VideoLLaMA 2 within the context of accessible and reproducible research.",
            "score": 0.5607743263157056,
            "section_title": "Open-Source Models",
            "char_start_offset": 19609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 591
                }
            ],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 379,
                    "matchedPaperCorpusId": "266174746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2303466796875
        },
        {
            "corpus_id": "268248187",
            "title": "Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation",
            "text": "For the experiments, we tried to measure SOTA MLLM models: LLaVA 1.5 and LLaVA-NeXT, ShareGPT4V, and ChatGPT4. We were unable to measure the newly released Gemini Ultra, as it outright refused to work with images of people. We've also made improvements to the state-of-the-art specialized model MiVOLO [14] to ensure fair competition among cutting-edge models. Figure 1 demonstrates an example of work of evaluated models and figure 2 provides a graphical representation.",
            "score": 0.5195498466506502,
            "section_title": "Introduction",
            "char_start_offset": 2119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 471
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.193603515625
        },
        {
            "corpus_id": "273821149",
            "title": "HumanVLM: Foundation for Human-Scene Vision-Language Model",
            "text": "Liu at el. introduced an end-to-end trained large vision-language assistant (LLaVA [51]) on instruction-following data for general purpose visual and language understanding, which gained widespread attention upon release. Subsequent research has further enhanced LLaVA's performance. For instance, LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios. LLaVA-Interactive [53] serves as a comprehensive demonstration platform, incorporating features such as image chatting, segmentation, and generation and editing capabilities, significantly expanding LLaVA's original functionalities. MoE-LLaVA [54], a sparse LVLM architecture based on Mixture of Experts (MoE), was developed to tackle performance degradation in multimodal sparse learning. MG-LLaVA [55] enhanced the model's visual processing capabilities by introducing multi-granularity visual streams, allowing it to handle features at various resolutions and object centers. \n\nLLaVA has set new standards for efficiency and effectiveness in multimodal learning and has quickly been adapted across various domains. For example, LLaVA-based models, including LLaVA-Med [18], PathChat [19], QUILT-LLaVA [56], PA-LLaVA [57], have been designed for medical image understanding, where they outperform traditional methods. Zheng et al. [58] developed the first large-scale open-source dataset, MMTab, to address the multimodal table understanding problem and trained a multifunctional tableformat LLM called Table-LLaVA. In the power sector, Wang et al. [21] proposed Power-LLaVA, a large vision-language assistant designed for reliable inspection of power transmission lines, showcasing strong capabilities in this field. In the food domain, Fnu Mohbat et al. [20] introduced LLaVA-Chef, trained on a carefully selected recipe dataset, enabling it to recognize ingredients and generate detailed recipes. In this study, we aim to construct a unified multimodal Vision-Language Model for human-scene tasks.",
            "score": 0.5150143029420995,
            "section_title": "Various Vision-Language Applications",
            "char_start_offset": 8442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 1227,
                    "end": 1231,
                    "matchedPaperCorpusId": "258999820"
                },
                {
                    "start": 1242,
                    "end": 1246,
                    "matchedPaperCorpusId": "270437603"
                },
                {
                    "start": 1260,
                    "end": 1264,
                    "matchedPaperCorpusId": "266149936"
                },
                {
                    "start": 1389,
                    "end": 1393,
                    "matchedPaperCorpusId": "270391758"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8046875
        },
        {
            "corpus_id": "271719914",
            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
            "text": "As the primary data source, our principle for single-image data has always been that quality outweighs quantity. Given limited resources, we strive to use high-quality data to maximize the performance. \n\nThe first version of the LLaVA-NeXT models (LLaVA-NeXT-Vicuna-7B/13B, Mistral-7B, Hermes-Yi-34B), comprising 760K data samples [82], includes 665K samples from LLaVA-1.5 [81], 3,247 samples from AI2D [53], 18,317 samples from ChartQA [101], 10,194 samples from DocVQA [103], 20,000 samples from DVQA [49], 40,093 samples from SynthDOG-EN [58], and 15,131 samples from user requests on LLaVA's demo, re-annotated with GPT-4V. In the subsequent iteration, we added 20,000 samples from COCO Caption [78], forming a new 790K version. This 790K dataset supported the second release of LLaVA-NeXT models (LLaVA-NeXT-LLaMA3-8B, LLaVA-NeXT-Qwen-72B, LLaVA-NeXT-Qwen-110B). \n\nIn subsequent collections, we accumulated open-sourced datasets from the Internet and referred to the dataset collection processes of other advanced LMMs, such as Qwen-VL [8], DeepSeek-VL [89], Intern-VL [22], Vision-Flan [146], UReader [150], Idefics-2 (Cauldron) [63], and Cambrian. During the data iteration process, we strictly adhered to the initial LLaVA-1.5 strategy. For each dataset, we manually inspected and ensured its quality and QA format. We also designed specific formatting prompts to make data from different sources compatible with each other, thus avoiding conflicts. \n\nSome data sources, such as AI2D and ChartQA, appear in different dataset collections and may be duplicated. Since Cauldron includes special formatting prompts, its data is not straightforward to re-format.",
            "score": 0.5133108418782123,
            "section_title": "E.1 Single-Image Data Curation",
            "char_start_offset": 46928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 201
                },
                {
                    "start": 204,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 868
                },
                {
                    "start": 871,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1666
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 404,
                    "end": 408,
                    "matchedPaperCorpusId": "2682274"
                },
                {
                    "start": 438,
                    "end": 443,
                    "matchedPaperCorpusId": "247593713"
                },
                {
                    "start": 472,
                    "end": 477,
                    "matchedPaperCorpusId": "220280200"
                },
                {
                    "start": 504,
                    "end": 508,
                    "matchedPaperCorpusId": "4445015"
                },
                {
                    "start": 542,
                    "end": 546,
                    "matchedPaperCorpusId": "250924870"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.181396484375
        },
        {
            "corpus_id": "273098155",
            "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
            "text": "We further evaluate LLaVA-Critic's performance in providing reward signals for iterative DPO. LLaVA-OneVision's supervised fine-tuned checkpoint is used as the base policy model, and question-image pairs from LLaVA-RLHF [38] serve as the multimodal instructions. For each pair, K = 5 candidate responses are generated through random decod- ing (with a temperature of 0.7 and top-p of 0.9) to ensure response diversity. LLaVA-Critic is employed as described in Sec. 4.3 to construct the pairwise feedback data, which is then used for one epoch of DPO training. We perform iterative DPO for M = 3 rounds in total. \n\nTo assess the effectiveness of LLaVA-Critic's reward signals, we evaluate the final LMM checkpoint on 6 open-ended multimodal benchmarks: four imagebased tasks (LLaVA-in-the-Wild [24], LLaVA-Wilder [15], WildVision-Bench [28] and LiveBench [48]), one videobased task (Video Detailed Captioning [15]), and one hallucination benchmark (MMHal-Bench [38]). We compare LLaVA-Critic with two baselines: (1) reward model from LLaVA-RLHF [38], which is trained on human preferences, and (2) a naive baseline that replaces LLaVA-Critic with LLaVA-OneVision as a zero-shot reward model. \n\nAs shown in Table 6, preferences provided by LLaVA-Critic significantly improve LLaVA-OneVision's visual chat capacities and reduce hallucination across challenging tasks. LLaVA-Critic consistently surpasses other baseline reward models on 5 out of 6 benchmarks for the 7B base model and all 6 benchmarks for the 72B base model. Despite the preference alignment conducted solely with images, LLaVA-Critic also enhances LLaVA-OneVision's performance in Video Detailed Captioning (+0.12 on OV-7B and +0.26 on OV-7B), demonstrating its ability to generalize to both image and video contexts.",
            "score": 0.5132158827045502,
            "section_title": "Preference Learning",
            "char_start_offset": 26353,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1781
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3359375
        },
        {
            "corpus_id": "274422305",
            "title": "GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks",
            "text": "Model Variability and Capabilities: We selected both generic and geospatial-specific VLMs, prioritizing recently developed models with advanced capabilities across various tasks. Generic VLMs include LLaVA-1.5 [30], LLaVA-NeXT [29], LLaVA-OneVision [23], Sphinx [27], Ferret [62], InternVL2 [9], and Qwen2-VL [54]. Also, we added GPT-4o, a closed-source, commercially available model, to compare its adaptability to remote sensing tasks with open-source models. These VLMs demonstrate robust performance in tasks such as scene understanding, and finegrained visual classification. For geospatial-specific VLMs, we selected GeoChat [19], RS-LLaVA [5], SkySenseGPT [34], EarthDial [47], and LHRS-Bot-Nova [36], models tailored for satellite and aerial image interpretation. Domain Specific Model Suitability: We consider VLMs based on their relevance to geospatial tasks. Despite being trained on satellite and aerial data, domain-specific models [5,19,34,36,47] may struggle with counting and spatial relationships due to dataset or architecture constraints. Meanwhile, we evaluate whether generic VLMs can effectively handle scene understanding, object detection, and visual reasoning in remote sensing. This selection ensures a focused study of VLM performance across diverse geospatial applications. Open vs Closed Models: Open-source models like LLaVA [23,29,30], Qwen2-VL [54], GeoChat [19], and others provide transparency, aiding in understanding their strengths and limitations. Closed models like GPT-4o, despite their lack of transparency, demonstrate strong generalization and performance on complex tasks, benefiting from proprietary datasets and advanced architectures. Evaluating both ensures a comprehensive assessment of VLM capabilities.",
            "score": 0.5029675009201882,
            "section_title": "Selection of VLMs",
            "char_start_offset": 11741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1753
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 214,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 291,
                    "end": 294,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "265456719"
                },
                {
                    "start": 646,
                    "end": 649,
                    "matchedPaperCorpusId": "269354784"
                },
                {
                    "start": 703,
                    "end": 707,
                    "matchedPaperCorpusId": "267412588"
                },
                {
                    "start": 945,
                    "end": 948,
                    "matchedPaperCorpusId": "269354784"
                },
                {
                    "start": 948,
                    "end": 951,
                    "matchedPaperCorpusId": "265456719"
                },
                {
                    "start": 954,
                    "end": 957,
                    "matchedPaperCorpusId": "267412588"
                },
                {
                    "start": 1362,
                    "end": 1365,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1390,
                    "end": 1394,
                    "matchedPaperCorpusId": "265456719"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5029296875
        },
        {
            "corpus_id": "270045537",
            "title": "ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models",
            "text": "We perform a comprehensive comparison with state-of-the-art models on 7 different benchmarks (Tab.5).Our model achieves consistent improvements compared to LLaVA-1.5.Our 7B model even exhibits comparable performance with LLaVA-1.5 13B and LLaVA-NExT 7B [31].\n\nOn OCR benchmarks like TextVQA and DocVQA, our model outperforms the LLaVA-1.For grounding benchmarks, our model and LLaVA are trained with the same set of grounding data.The comparison between them is fair.On RefCOCO, RefCOCO+, and RefCOCOg, ConvLLaVA exhibits consistent improvement when increasing resolution (Tab.6).ConvLLaVA outperforms LLaVA-7B and 13B model on all 8 test splits.This demonstrates the benefits of higher resolution for grounding tasks.Our 7B model also surpasses 13B LLaVA model on all 8 benchmarks.",
            "score": 0.5027058522276,
            "section_title": "Quantitative Results",
            "char_start_offset": 22388,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 101
                },
                {
                    "start": 101,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 258
                },
                {
                    "start": 260,
                    "end": 337
                },
                {
                    "start": 337,
                    "end": 431
                },
                {
                    "start": 431,
                    "end": 467
                },
                {
                    "start": 467,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 580
                },
                {
                    "start": 580,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 782
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.375244140625
        },
        {
            "corpus_id": "268513094",
            "title": "PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset",
            "text": "An overall VHE as shown in Tab. 5 is useful for providing a big picture of which MLLM hallucinates the most (or the least). The leading open-source MLLMs are LLaVA-OneVision, followed by Molmo and InternVL-1.5. Since their vision encoders and LLMs vary, the results are insufficient to conclude which component is the most effective to mitigate hallucinations. That said, comparisons among the same model series remains meaningful. Consider the LLaVA series for instance. While one would normally expect that a larger LLM yields a better MLLM, as LLaVA-1.6-L vs LLaVA-1.6, the difference between LLaVA-1.5-L and LLaVA-1.5 is marginal (0.270 vs 0.265). In order to analyze and consequently understand such an counterintuitive result, PhD enables a zoom-in analysis in mode-oriented (Sec. 4.3) and task-oriented (Sec. 4.4) styles, unavailable in the previous benchmarks. One more advantage of PhD compared to its predecessors lies in its discrimination ability. The relatively small performance gap between GPT-4o and the top open-source models as measured by POPE and AMBER might lead to an overly optimistic interpretation that the open-source alternatives are catching up with the proprietary model. In fact, a substantial gap remains, as revealed by PhD. Fig. 3 further illustrate the qualitative results, where GPT-4o exhibits fewer hallucinations in its response. This is achieved either with stronger visual encoders, as the cases of LLaVA-OneVision, InternVL-1.5, and Cambrian-1 using SoViT-400m/14, InternViT-6B or hybrid vision structure, or supporting higher image resolutions, see Molmo and MiniGPT4-v2 that accept multiscale or larger input.",
            "score": 0.49879696342904334,
            "section_title": "Using PhD for Overall VHE",
            "char_start_offset": 22800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1652
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.146240234375
        },
        {
            "corpus_id": "274598288",
            "title": "Black Swan: Abductive and Defeasible Video Reasoning in Unpredictable Events",
            "text": "Default settings for resolution are used, and max number of frames are 32. 5. VILA [16] VILA 1.5 is the latest in the VILA series of models, one of the first models to support multi-image understanding. We use the Llama-3-VILA1.5-8b-Fix model. We uniformly sample 5 frames for each part of the video. \n\n6. LLaVA-Video [43] LLaVA was first introduced as a multimodal model with performance rivaling GPT-4. Following the release of LLaVA 1.5 and LLaVA-NeXT, LLaVA-Video is the most advanced version of this model, specifically trained on a new 178K video dataset. Specifically, we use the LLaVA-Video-7B-Qwen2 model specification. When asking questions, we directly upload the entire video clip. In the case of Detective , the clip has the V main blacked out. 32 frames are used. We also test the 72B LLaVA-Video (LLaVA-Video-72B-Qwen2) model; however, due to compute limitations and setup issues, we were only able to run that model on a subset of the data. The results are detailed in Appendix F.3. \n\nOur implementation of all open-source models follows their respective instructions on GitHub and Huggingface, and we tried our best to recreate the same environment as the original developers for each model. We do not do batch inference on any of these models, and instead query them iteratively in chat mode only (each chat conversations for every question is independent). For the generative variant, in order to obtain different results each time, we turn sampling on. \n\nWe use the following prompts for each task: \n\nForecaster -Gen Describe what could happen next, by explaining the sequence of actions leading to the outcome.",
            "score": 0.49327525104421743,
            "section_title": "E. Baselines",
            "char_start_offset": 35823,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1472
                },
                {
                    "start": 1475,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1631
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30078125
        },
        {
            "corpus_id": "270521771",
            "title": "Long Story Short: Story-level Video Understanding from 20K Short Films",
            "text": "In this section, we evaluate recent open-source and commercial vision-language models on SF20K-Test. In particular, we benchmark eleven open-source models: Frozen-BiLM [6], mPLUG-Owl2 [80], TimeChat [52], Video-Llava [36], MovieChat [59], Llava-OneVision [32], Long-Llava [67], LongVA [84], Llava-Next-Video [85], Llama-3.2-Vision [43], and Pixtral [3]; and two commercial models: LloVi [83] (based on GPT-3.5 [45]), and GPT-4o-mini [46]. \n\nSee Appendix E for more details on prompts and metrics. The models are tested in a zero-shot video questionanswering setting, adapted for long-form video understanding by incorporating subtitles and sampling the maximum number of frames possible to fit in a single A100 GPU. For multiple-choice questions, we compute the accuracy score by extracting the option letter choice from the model's response. For open-ended questions, we rely on GPT-3.5 to compute the similarity between the predicted and correct answers, as detailed in [39]. All methods are evaluated across three modalities to better assess the contribution of each: Vision-Only (V) with video frames, Language-Only (L) with subtitles, and Vision-Language (VL) combining both. The results are summarized in Table 4. Multiple-Choice Question Answering (MCQA). Our results reveal that several models exhibit strong performance, with GPT-4o-mini achieving the highest accuracy of 79.4%. \n\nNotably, recent open-source models are closing the gap with commercial ones; for example, Llava-OneVision-7B lags be-  hind by just 1.2 percentage points. However, there remains significant room for improvement, as human performance reaches 89.8%. \n\nOpen-Ended Question Answering (OEQA). This task is more challenging, as evidenced by significantly lower accuracy across all models, ranging from 1.6% to 60.0%.",
            "score": 0.48481416638623087,
            "section_title": "Baselines",
            "char_start_offset": 18573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1387
                },
                {
                    "start": 1390,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1637
                },
                {
                    "start": 1640,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1800
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 171,
                    "matchedPaperCorpusId": "232478955"
                },
                {
                    "start": 272,
                    "end": 276,
                    "matchedPaperCorpusId": "272397612"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.260009765625
        },
        {
            "corpus_id": "268513094",
            "title": "PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset",
            "text": "Joint mode-task analytics per model is shown in Tab. 6. LLaVA-OneVision struggles with sentiment recognition and counting, especially when faced with textual or CCS distractions, underscoring the need for improvement in these areas. Similarly, Molmo also faces these challenges, but its counting performance under CCS distractions is notably better than LLaVA-OneVision's (0.737 vs 0.563). The above zoom-in analytics will be informative for MLLM developers to prioritize their efforts on model refinement.",
            "score": 0.4829971945354091,
            "section_title": "Using PhD for Task-Oriented VHE",
            "char_start_offset": 28700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 506
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.157958984375
        },
        {
            "corpus_id": "272986933",
            "title": "World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering",
            "text": "In this paper, we employ two types of leading methods: LLaVA-1.5 (Liu et al., 2023a) uses a CLIP-pretrained ViT-L/14 (Radford et al., 2021) as a vision encoder, a projector and an LLM, and LLaVA-NeXT (Liu et al., 2024a) increases the input image resolution by applying an adaptive image cropping strategy to concatenate all vision tokens. To ensure a fair and comprehensive comparison Table 1 and Table 2 present results both excluding and including the ShareGPT4V dataset, as well as results from the incorporation of our dataset. Table 3 We have reproduced LLaVA-NeXT with a learning rate of ViT to 1/10 of the base learning rate for the reason that LLaVA-NeXT only publishes their evaluation code. The learning rate for the PT stage is set to 1e Data Processing Details During the data construction pipeline, we employ NLTK (Bird, 2006) tool to extract noun phrases from the captions, and the resulting set of phrases is then post-processed using WordNet (Miller, 1995) to remove duplicates and filter out inaccurately named entities. The total amount of final data after consistency filtering will not be completely consistent for different VLMs and we show the details in Appendix C.1. The checkpoints of the VLM we used in our data processing are the original checkpoints of the official release. For LLaVA-1.5, which is not trained with the ShareGPT4V dataset, LLaVA-NEXT is trained with part of the ShareGPT4V dataset. The detailed GPU hours can be found in Appendix C.2 and we show the visualization of our W2C samples in Appendix C.3.",
            "score": 0.48207032420604123,
            "section_title": "Implementation Details",
            "char_start_offset": 16037,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1544
                }
            ],
            "ref_mentions": [
                {
                    "start": 65,
                    "end": 83,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 117,
                    "end": 139,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 827,
                    "end": 839,
                    "matchedPaperCorpusId": "1438450"
                },
                {
                    "start": 958,
                    "end": 972,
                    "matchedPaperCorpusId": "1671874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.295166015625
        },
        {
            "corpus_id": "273098584",
            "title": "Dog-IQA: Standard-guided Zero-shot MLLM for Mix-grained Image Quality Assessment",
            "text": "(Liu et al., 2024c), LLaVA-v1.5-13b (Liu et al., 2024c), LLaVA-Next (Liu et al., 2024b), mPLUG-Owl (Ye et al., 2023), mPLUG-Owl2 (Ye et al., 2024b), and mPLUG-Owl3 (Ye et al., 2024a). All models are tested using 5 words with the whole image as input, reflecting their fundamental zero-shot IQA capabilities. The results are shown in Table 6, from which we can draw the following conclusions. First, from the version's perspective, the trend shows that the higher the version is, the better the model's performance is. Second, when we consider different models, mPLUG-Owl3 demonstrates a clear performance advantage, and LLaVA-Next gains sub-optimal performance. Therefore, we choose mPLUG-Owl3 as our scoring model.",
            "score": 0.47809677915389304,
            "section_title": "ABLATION STUDY",
            "char_start_offset": 30636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 715
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 116,
                    "matchedPaperCorpusId": "258352455"
                },
                {
                    "start": 129,
                    "end": 147,
                    "matchedPaperCorpusId": "265050943"
                },
                {
                    "start": 164,
                    "end": 182,
                    "matchedPaperCorpusId": "271843557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.143798828125
        },
        {
            "corpus_id": "275405668",
            "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding",
            "text": "Currently, multimodal large language models can be categorized into community models and proprietary models. Proprietary models [3,[52][53][54]64] often achieve better performance but are not open-sourced. Meanwhile, community models [17,23,27,29,30,35,38,39,80,81,85,91], which have seen rapid performance improvements, are garnering increasing attention due to their open-source nature, including model architecture, weights, and even training data. LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously. \n\nBased on the ideas of LLaVA, several variant series have emerged, such as the mPLUG-owl series. mPLUG-owl [80] introduces a new paradigm for training large language models through modularity, and the latest version, mPLUG-owl3 [78], can even understand 2-hour movie videos. BLIP-2 [31] uses Q-Former [88] to connect the visual and linguistic modalities. In BLIP-3 [76], Q-Former is replaced by more scalable visual token samplers, such as perceptual resamplers. We observe that numerous methods have explored various visual projectors. However, to the best of our knowledge, we are the first to classify these projectors and analyze their complementarity.",
            "score": 0.47668504563924624,
            "section_title": "Multimodal Large Language Model",
            "char_start_offset": 6442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1724
                }
            ],
            "ref_mentions": [
                {
                    "start": 1350,
                    "end": 1354,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51123046875
        },
        {
            "corpus_id": "277313523",
            "title": "PAVE: Patching and Adapting Video Large Language Models",
            "text": "Our experiments consider ScanQA [2] and SQA3D [44] datasets, using their corresponding train / test splits. Following previous work [84], we report the CIDEr (C), BLEU-4 (B-4), METEOR (M), ROUGE(R), and top-1 Exact Match (EM@1) metrics on ScanQA and report EM@1 on SQA3D. Similarly, we also report model inference FLOPs and parameters for PAVE. \n\nImplementation details. To encode 3D cues, we use the 3D encoder from LLaVA-3D [84]. It encodes camera pose, RGB images, and depth information into multi-view features defined on the RGB image plane. We again build PAVE with LLaVA-OneVision [28]. Specifically, we evenly sample 32 RGB-D frames with their camera poses from the scanning. The 3D encoder creates a sequence of 2D feature maps (i.e., multi-view features), leading to around 18K 3D tokens per scan. The visual encoder in LLaVA-OneVision separately embeds the 32 key RGB frames into video tokens. PAVE further integrates video tokens and 3D tokens. We train the PAVE 1 epoch with ScanQA / 2 epochs with SQA3D training set, and then evaluating PAVE performance on their test sets, respectively. \n\nBaselines. We again compare our method with two types of baselines: (1) Video LLMs for general video understanding, e.g., LLaVA-OneVision [28] and VideoChat2 [33], with zero-shot inference; and (2) Task-specific models fine-tuned on the target dataset, e.g., LEO [22], 3D-LLM [20], Scene-LLM [15] and LLaVA-3D [84]. We also add a baseline that directly fine-tunes LLaVA-OneVision with LoRA without using 3D cues, denoted as LLaVA-OV-7B-FT.",
            "score": 0.47220667753133116,
            "section_title": "Experiment protocol.",
            "char_start_offset": 17857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 344
                },
                {
                    "start": 347,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1543
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 35,
                    "matchedPaperCorpusId": "245334889"
                },
                {
                    "start": 46,
                    "end": 50,
                    "matchedPaperCorpusId": "252907411"
                },
                {
                    "start": 1262,
                    "end": 1266,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 1380,
                    "end": 1384,
                    "matchedPaperCorpusId": "260356619"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.375
        },
        {
            "corpus_id": "271710333",
            "title": "Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models",
            "text": "Models Details. To validate the effectiveness of our SID, we conduct experiments on four representative LVLMs: InstructBLIP (Dai & et al, 2023), Shikra (Chen et al., 2023a), LLaVA-1.5 (Liu et al., 2024b), and LLaVA-NeXT (Li et al., 2024). InstructBLIP employs Q-former (Li et al., 2023c) to condense image tokens to 32, as a result, we are unable to visualize the dynamic token pruning process of InstructBLIP like Fig. 3 and 4. Shikra, LLaVA-1.5, and LLaVA-NeXT directly leverage linear projection layers as vision-language connectors to align multimodal features. Shikra and LLaVA-1.5 encode 256 and 576 image tokens to LVLMs. LLaVA-NeXT increases the input vision resolution by 4\u00d7 to capture more visual details, resulting in 4\u00d7 more encoded vision tokens than LLaVA-1.5. All LVLMs utilize pre-trained vision encoders like CLIP (Radford et al., 2021) vision encoder, as well as pre-trained LLMs as language decoders, such as Vicuna v1.1 (Chiang & Li, 2023), LLaMA 2 (Touvron et al., 2023b), and recently released LLaMA 3 (Meta, 2024). We provide results at the 7 Billion (B) scale, and larger-scale results are in the Appendix A.7. \n\nImplementation Details. For sampling and greedy decoding, we adopt the default hyperparameter settings. As for Dola (Chuang et al., 2024), it is designed to alleviate hallucinations (i.e., improve factuality) of LLM by contrasting the differences in logits obtained from projecting the later layers versus premature layers.",
            "score": 0.4716180284486271,
            "section_title": "A.2 DETAILED EXPERIMENTAL SETTINGS",
            "char_start_offset": 32841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 16,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1460
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 203,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 269,
                    "end": 287,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 831,
                    "end": 853,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.333740234375
        },
        {
            "corpus_id": "275118957",
            "title": "MVTamperBench: Evaluating Robustness of Vision-Language Models",
            "text": "Our benchmarking efforts encompass 45 models across diverse categories and tampering scenarios, including Drop, Mask, Repeat, Rotate, and Substitute (Table 3). The results reveal significant variability in performance, influenced by model size, architecture, and training data. Models such as VILA1.5-40B and InternVL2.5-8B emerged as top performers, achieving consistent resilience across all tampering types, with overall scores of 0.879 and 0.875, respectively. This highlights the importance of architectural innovations and advanced training techniques for tampering robustness. In contrast, early-generation models like LLaVA-OneVision underperformed across all categories, with overall scores as low as 0.001, reflecting limitations in temporal coherence and token alignment. \n\nSpecialized models for video tasks, such as Chat-UniVi and Video-LLaVA, demonstrated substantial improvements over base LLaVA models. Chat-UniVi-7B-v1.5 achieved an overall score of 0.658, significantly outperforming LLaVA-OneVision, showcasing its ability to handle complex temporal manipulations. Meanwhile, Video-LLaVA-7B-HF maintained robust performance across categories, further validating the effectiveness of unified tokenization and video-specific optimizations. However, LLaVA-Video, despite efforts to improve alignment and pretraining, continues to struggle with certain tampering types, reflecting the challenges of adapting image-centric architectures to video modalities. \n\nInterestingly, medium-sized models like Phi3.5-Vision demonstrated notable performance improvements compared to earlier iterations such as Phi3-Vision, indicating that scaling alone does not account for robustness gains. Specialized models like Ovis1.6-Gemma2-9B showcased strengths in spatial tampering scenarios (e.g., Mask and Rotate) but struggled with temporal disruptions like Repeat and Substitute. This trend underscores the importance of task-specific optimizations. \n\nFuture Benchmarking Plans While our analysis has covered an extensive set of models, several promising entries are yet to be evaluated.",
            "score": 0.4634792705688212,
            "section_title": "Benchmarking Efforts",
            "char_start_offset": 29464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 782
                },
                {
                    "start": 785,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1471
                },
                {
                    "start": 1474,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1949
                },
                {
                    "start": 1952,
                    "end": 2087
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.426025390625
        },
        {
            "corpus_id": "266933417",
            "title": "Hallucination Benchmark in Medical Visual Question Answering",
            "text": "Among the three scenarios, NOTA has the lowest accuracy for all the models, indicating its challenge to the current LLVMs.In general, the models with improved backbone models, LLaVA-v1.5-7Band LLaVA-v1.5-13B,performs much better than all the the models based on LLaVA-v0 (LLaVA-Med, LLaVA-Med-pvqa, LLaVA-Med-rad and LLaVA-Med-slake).We also find that finetuning in domain-specific data does not guarantee a performance boost in hallucination evaluation as LLaVA-Med performs worse than LLaVA-v0-7B.To conclude, LLaVA-v1.5-13B is more robust than GPT-4-turbo-vision in two scenarios (FAKE and SWAP) and less irrelevant predictions, making it less prone to hallucinations.",
            "score": 0.46166557095915683,
            "section_title": "CONCLUSION",
            "char_start_offset": 4180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 189
                },
                {
                    "start": 189,
                    "end": 208
                },
                {
                    "start": 208,
                    "end": 334
                },
                {
                    "start": 334,
                    "end": 499
                },
                {
                    "start": 499,
                    "end": 671
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11163330078125
        },
        {
            "corpus_id": "278170893",
            "title": "Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains",
            "text": "Additionally, some of these sub-tasks (e.g., geographic understanding) involve image types and task categories that are not present in VISC-150K, which emphasize the crossdomain generalization capabilities of our method. RQ3: How does the number of input images impact performance? \n\nTo investigate how the number of input images impacts our method, we conducted a detailed analysis of LLaVA-OneVision on the MMIU benchmark. The instances are grouped into different buckets by the number of input images. \n\nAs demonstrated in Figure 5, when handling 3-8 images, LLaVA-OneVision-VISC achieves remarkable improvements, suggesting successful identi- fication of cross-image relationships in mediumsized image sets. This capability persists even with larger inputs (11-14 images), where LLaVA-OneVision-VISC maintains superior performance compared to baseline models while avoiding performance degradation from information overload. However, when processing more than 15 images, the performance of LLaVA-OneVision-VISC exhibits slight degradation, which may be attributed to amplified noise levels or interference from irrelevant data patterns in prolonged image sequences. RQ4: Does VISC-150K affect general ability? \n\nAlthough our method demonstrates remarkable improvements in multi-image tasks, it is crucial to evaluate whether these gains come at the expense of general task performance. To investigate this issue, we adopt Qwen2-VL as the base model and conduct analysis on four benchmarks in other domains. These benchmarks are based on single-image inputs and evaluate the fine-tuned model's performance from various perspectives, including hallucination, single-image data language capability, domain-specific knowledge, and mathematical reasoning. Specifically: HallusionBench (Guan et al., 2023) is designed to assess VLMs' ability to comprehend and interpret visual data; MMStar (Chen et al., 2024a) requires advanced multi-modal capabilities for accurate interpretation; MMMU (Yue et al., 2024) focuses on evaluating models' ability to apply domain-specific knowledge; Math-Vista (Lu et al., 2023) integrates mathematical reasoning with visual tasks. The results are presented in Table 2.",
            "score": 0.4593531795359166,
            "section_title": "Discussions",
            "char_start_offset": 20808,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 281
                },
                {
                    "start": 284,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1213
                },
                {
                    "start": 1216,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 1986,
                    "end": 2004,
                    "matchedPaperCorpusId": "265466525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4013671875
        },
        {
            "corpus_id": "267750968",
            "title": "Exploring the Limits of Zero Shot Vision Language Models for Hate Meme Detection: The Vulnerabilities and their Interpretations",
            "text": "We We used instruction fine-tuned IDEFICS 9B model with the checkpoint HuggingFaceM4/idefics-9b-instruct for our experiments. \n\nLLAVA-1.5: LLAVA-1.5 (Liu et al. 2023) is an enhanced version of LLAVA. LLAVA combines LLAMA/Vicuna as the language model and CLIP as the vision encoder. Compared to LLAVA, LLAVA-1.5 has enhanced capabilities due to the addition of an MLP vision-language connector and integration of academic task-oriented data. We have used two different LLAVA-1.5 models with 7B and 13B parameters. The checkpoints of these models are llavahf/llava-1.5-7b-hf and llava-hf/llava-1. Microsoft Azure for providing us with the credits to use their OpenAI services. We used the provided APIs to infer GPT-4O from their documentation to run our experiments8 and for our case, we used a combination of single image with text as input to remain consistent with the zero-shot setup. BASELINE: Our work focuses on the evaluation of zeroshot capability of VLMs. To have a fair comparison with a very recent prior work presented in (Cao et al. 2023), we consider two different baselines: (i) complete zero-shot assessment, and (ii) fine-tuned only on FHM (a very diverse dataset) for two epochs (instead of ten; to be very near to the zero-shot setup) and evaluation across all six datasets. These setups not only allow for fair evaluation, but also help in assessing the generalization capability, which is often ignored. Two samples (one negative and one positive) are also provided with the test sample in the prompt, as noted in the mentioned paper; hence we present results as mean and standard deviation across three different seeds for each setup.",
            "score": 0.45770101349371517,
            "section_title": "Models",
            "char_start_offset": 10271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 128,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1656
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.291015625
        },
        {
            "corpus_id": "273638057",
            "title": "GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing",
            "text": "Pre-training VLMs is typically computationally intensive and timeconsuming. Consequently, fine-tuning presents an effective alternative that preserves most of the model's parameters while enhancing performance on downstream tasks. Fine-tuned models can often outperform the original general models, utilizing fewer computing resources and requiring less training time [25]. This advantage motivates the use of Parameter-Efficient Fine-Tuning (PEFT) methods for tasks involving geographical change detection. \n\nIn our work, we focus on fine-tuning two distinct models that have demonstrated a robust understanding of temporal data through video processing within the VLM framework for question-answering and captioning. The first model, LLaVA-NeXT [20], was introduced in early 2024, offering improved reasoning and world knowledge compared to other large models. It exhibits data efficiency comparable to SOTA models such as LLaVA-1.5 [19], while delivering higher image resolution and enhanced visual conversation capabilities. Shortly after the release of LLaVA-NeXT, a video variant was introduced, named LLaVA-NeXT-Video, which has demonstrated strong performance in zero-shot video tasks. \n\nThe second model utilized for comparison is Video-LLaVA [15], which excels in understanding visual language for downstream tasks and surpasses many existing video language models across various  benchmarks. Both projects have multiple variations based on the number of parameters for the models. For simplicity, we have chosen to use the 7B parameter variation from both models. The 7B variations can be fine-tuned with PEFT techniques on a single GPU, making them particularly well-suited for our dataset.",
            "score": 0.456056531462391,
            "section_title": "Model Fine-tuning",
            "char_start_offset": 11891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1702
                }
            ],
            "ref_mentions": [
                {
                    "start": 368,
                    "end": 372,
                    "matchedPaperCorpusId": "268157336"
                },
                {
                    "start": 935,
                    "end": 939,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6220703125
        },
        {
            "corpus_id": "270063538",
            "title": "Matryoshka Multimodal Models",
            "text": "LLaVA-1.5-M 3 We evaluate LLaVA-1.5-M 3 on the common multimodal understanding and reasoning benchmarks. Results are shown in Table 1. LLaVA-1.5-M 3 with full tokens maintains the performance of LLaVA-1.5 across diverse benchmarks. More importantly, our approach shows strong performance even with 1 or 9 tokens. Specifically, in MMBench, a comprehensive multimodal understanding benchmark, LLaVA-1.5-M 3 with 9 tokens surpasses Qwen-VL-Chat with 256 tokens, and achieves similar performance as Qwen-VL-Chat with even 1 token. Compared with Instruct-BLIP [59], LLaVA-1.5M 3 with 9 tokens surpasses InstructBLIP-7B and InstructBLIP-13B across all benchmarks. This demonstrates that our model has both flexibility and strong empirical performance under diverse number of visual tokens. \n\nLLaVA-NeXT-M 3 We use the proposed Matryoshka Multimodal Models to finetune LLaVA-NeXT, and compare LLaVA-NeXT-M 3 with SS, which denotes the setting where the LLaVA-NeXT is trained under a Specific Scale of visual tokens also for 1 epoch. We also include the oracle upperbound performance. Specifically, 'Oracle' denotes the case where the best tradeoff between visual tokens and performance is picked for each test instance. Specifically, for each test instance, we select the the scale with the fewest amount of tokens but can answer the question correctly. Results are shown in Table 2. Our approach, M 3 , is at least as good as SS, while performing better on tasks such as document understanding (TextVQA and ChartQA) and common benchmarks such as MMBench [23]. \n\nOur results also show that dataset level biases towards the visual token scales do exist. For example, ScienceQA maintains consistent performance across all visual token scales. AI2D and MMBench only encounter a small performance drop for even as few as 9 to 1 tokens. On the other hand, dense visual perception tasks such as TextVQA and DocVQA show a significant performance drop with fewer tokens.",
            "score": 0.45456997368261176,
            "section_title": "Image Understanding",
            "char_start_offset": 16919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1955
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40576171875
        },
        {
            "corpus_id": "268384843",
            "title": "Can We Talk Models Into Seeing the World Differently?",
            "text": "Here we provide an overview of all used models from the main paper. \n\nQwen-VL-Chat (Bai et al., 2023b) Adds vision capabilities to Qwen-7B (Bai et al., 2023a). We set a repetition penalty of 1.2 for this model. \n\nQwen-VL Plus/Max (Qwen Team, 2024) \n\nAliBaba's proprietary larger variants of Qwen-VL-Chat. Access only via API. \n\nCogAgent (Hong et al., 2023) A special model for interaction with graphical user interfaces (GUIs) at high-resolution. \n\nCogVLM (Wang et al., 2023) Adds \"trainable visual expert module\" in LLM layers to combine vision and language. \n\nEmu2 (Sun et al., 2023a) The 37B model claims \"strong multi-modal in-context learning abilities\". \n\nInstructBLIP (Dai et al., 2023) Connects frozen vision encoders and LLMs through a trainable Q-Former. Uses Vicuna or FLAN-T5 as LLMs. \n\nLLaVA v1.5 (Liu et al., 2023a) Improvements of LLaVA with modifications on the image encoder, the projector, and task-specific data. Uses Vicuna-7/13B as LLM. \n\nLLaVA-NeXT (Liu et al., 2024) Successor of LLaVA v1.5 supporting higher resolutions through patching, and using better SFT training data for training, claiming \"improved reasoning, OCR, and world knowledge\" (Liu et al., 2024). The 34B version switches from Vicuna-7/13B to Nous Hermes 2 Yi 34B. \n\nMoE-LLaVA v1.5 (Lin et al., 2024) Variants of LLaVA v1.5 employing 4 sparsely activated Mixture-of-Experts (MoE), and smaller LLMs (Qwen, Phi-2, StableLM). \n\nLLaVA-RLHF (Sun et al., 2023c) Variants of LLaVA v1.5 aligned with Factually Augmented RLHF (Fact-RLHF) (Sun et al., 2023c).",
            "score": 0.4517293910240905,
            "section_title": "A OVERVIEW OF VLMS",
            "char_start_offset": 36999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 70,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 210
                },
                {
                    "start": 213,
                    "end": 247
                },
                {
                    "start": 250,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 446
                },
                {
                    "start": 449,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 659
                },
                {
                    "start": 662,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1254
                },
                {
                    "start": 1257,
                    "end": 1412
                },
                {
                    "start": 1415,
                    "end": 1539
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.340576171875
        },
        {
            "corpus_id": "278481332",
            "title": "MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks",
            "text": "To ensure a fair comparison, all evaluated models, with the exception of doubao-1-5-vision-pro and Gemini Flash2.0, are based on 7B-LLMs. Specifically, LLaVA-v1.6 is based on Vicuna-7B-v1.5 [55], LLaVA-Med-v1.5 utilizes Mistral-7B-Instruct-v0.2 [19], and Hu-atuoGPT integrates the LLaVA-1.5 model architecture with LLaMA-3-8B [50]. The configurations of all models were set according to their respective open-source codes. The results of these models are summarized in Table 3, with the first five rows presenting the performance of general LVLMs, the next two rows representing medical LVLMs, and the final three rows showing the results of our fine-tuned version of LLaVA-Med based on different training sets. \n\nMost general LVLMs perform poorly on MM-Skin. For instance, doubao-1-5-vision-pro-32k achieves only a Bleu-4 score of 2.16 in the pathology modality, reflecting its inability to generate relevant responses. LLaVA-v1.6-7B shows better recall but still scores low in Bleu-4 (2.14), indicating limited accuracy. Specialized models like LLaVA-Med-7B and HuatuoGPT-7B perform better, with LLaVA-Med-7B achieving 7.70 in Bleu-4 for pathology, and HuatuoGPT-7B scoring 4.42. The higher recall of HuatuoGPT-7B suggests a stronger understanding of medical images, highlighting the benefits of specialized training. The fine-tuned SkinVL-MM model, trained on MM-Skin, shows substantial improvement, achieving a Bleu-4 of 22.04 in pathology. It improves by nearly 10% in Bleu-4  compared to InternVL 2.5-8B, the best general model for dermoscopic images, demonstrating the effectiveness of domain-specific fine-tuning.",
            "score": 0.44992035489059157,
            "section_title": "Evaluation on Visual Question Answering",
            "char_start_offset": 19277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1621
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 194,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.247802734375
        },
        {
            "corpus_id": "277112989",
            "title": "Visual Position Prompt for MLLM based Visual Grounding",
            "text": "Transferability studies hold two implications for our work: First, whether the proposed VPP can be transferred to models beyond LLaVA-v1.5 and similarly enhance their visual grounding capabilities; second, whether the proposed VPP-LLaVA can be transferred to other datasets in a zero-shot manner. To address these concerns, we apply the proposed VPP to LLaVA-NeXT-7B using the main experiment's training strategy, with results in Table III. Additionally, in Table IV, we present the zero-shot visual grounding test of VPP-LLaVA-7B on the ReferIt dataset. \n\nFrom Table III, we observe that the proposed VPP method can be transferred to other models, thereby enhancing their visual grounding performance. \n\nFrom Table IV, it can be observed that compared to LLaVA-7B, VPP-LLaVA-7B demonstrates a significant improvement in zero-shot visual grounding performance on the ReferIt dataset. Specifically, VPP-LLaVA-7B achieves an accuracy boost of absolutely 8.6% on the ReferIt val split and 9.11% on the ReferIt test split.",
            "score": 0.44965305543730016,
            "section_title": "E. Transferability Studies for VPP",
            "char_start_offset": 25340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1018
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3662109375
        },
        {
            "corpus_id": "277150595",
            "title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning",
            "text": "LLaVA-MORE extends the widely recognized LLaVA architecture by incorporating small-and medium-scale LLMs. As shown in Table 1, our experiments first investigate the relationship between model size and performance across various benchmarks. Among the small-scale LLMs, we compare the LLaVA version based on Phi-2.7B [71] with our versions based on Phi-4-3.8B and Gemma-2-2B. Notably, both versions of LLaVA-MORE consistently outperform the existing baseline across multiple benchmarks. Notably, Phi-4-3.8B achieves the highest scores on most benchmarks, particularly excelling in the MMMU performance, where it surpasses Gemma-2-2B by 5.4%. Similar improvements are also evident in the SEED dataset, where Phi4-3.8B achieved significantly higher performance than other small-scale LLMs. These results underscore Phi-4-3.8B's superior reasoning and generalization capabilities. \n\nConversely, among medium-scale models, LLaVA-1.5-7B and LLaVA-1.5-LLaMA3-8B provide stronger baselines, with LLaVA-1.5-LLaMA3-8B achieving the highest score in MME-P (1544.4). However, our LLaVA-MORE models consistently surpass these baselines, demonstrating superior performance across both VQA and MLLM benchmarks. In particular, Gemma-2-9B emerges as the best-performing model, especially excelling in VQA benchmarks. It achieves the highest scores on GQA and AI2D, significantly outperforming both baselines and other models within the LLaVA-MORE family. In MLLM benchmarks, LLaVA-MORE combined with LLaMA-3.1-8B demonstrates instead strong capabilities on the MMB dataset. This highlights the model's proficiency in understanding complex multimodal inputs and its ability to reason across different modalities. Notably, our models exhibit less sensitivity to object hallucinations, as seen by the results achieved on the POPE benchmark, and demonstrate superior performance on the SEED dataset, further highlighting their robustness in multimodal reasoning tasks.",
            "score": 0.44653751848554357,
            "section_title": "Assessing the Optimal LLM Choice",
            "char_start_offset": 16092,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 875
                },
                {
                    "start": 878,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1946
                }
            ],
            "ref_mentions": [
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "266755915"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.380126953125
        },
        {
            "corpus_id": "276903457",
            "title": "CASP: Compression of Large Multimodal Models Based on Attention Sparsity",
            "text": "Models. For the experiments on image-language benchmarks, we use three different LMMs namely LLaVA1.5-7B2 [27], LLaVA1.5-13B 3 , and LLaVA-Next-7B 4 (i.e., LLaVA1.6-7B) [26]. We also use LLaVA-Next-Video-7B5 [56] for the video-language experiments. All the abovementioned models use Llama2-7B [41] or 13B as their underlying LLM (depending on the model size) and CLIP [39] as their vision encoder. LLaVA1.5 and LLaVA-Next encode the input image to 576 and a dynamic number of visual tokens, respectively, while LLaVA-Next-Video uses 144 visual tokens for each frame in the video. Metrics and Benchmarks. Our main evaluation metric is perplexity (PPL), which is the metric commonly used for quantization methods in the literature [9,24,42]. PPL is defined as the exponentiation of the average negative log-likelihood of a sequence of tokens [20]. We further evaluate CASP on different downstream tasks that use their specific metrics. Except for the PPL results, all other results on downstream datasets are obtained using the LMMs-eval framework6 [55]. In order to measure PPL, we use LiveBench (LiveB) [47] and LLaVA-Bench-Wilder (LWilder) [20] as open-ended QA datasets and LLaVA-Bench-COCO (LCOCO) [20] as an imagecaptioning dataset. For downstream task performance analysis, multi-choice QA benchmarks such as SEED-Bench [21], MMU [54], ScienceQA (SQA) [30], MME [10], and MMBench [28] benchmarks are used.",
            "score": 0.44585515419783406,
            "section_title": "Experimental Setting",
            "char_start_offset": 15291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 7
                },
                {
                    "start": 8,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1410
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 368,
                    "end": 372,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "253237200"
                },
                {
                    "start": 732,
                    "end": 735,
                    "matchedPaperCorpusId": "271271084"
                },
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "121680873"
                },
                {
                    "start": 1141,
                    "end": 1145,
                    "matchedPaperCorpusId": "121680873"
                },
                {
                    "start": 1201,
                    "end": 1205,
                    "matchedPaperCorpusId": "121680873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.344970703125
        },
        {
            "corpus_id": "274166071",
            "title": "FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual Token Compression",
            "text": "MLLMs. Early models like BLIP2 [25] and Instruct-BLIP [14] designed a Q-Former to bridge encoded visual information into the input space of LLMs. These approaches typically require complex training processes to train the image-text alignment module. Methods represented by Flamingo [1] proposed incorporating encoded image information into LLM layers using cross-attention. Fuyu8B [4] entirely discarded the visual encoder, directly inputting image patches into the LLM. LLaVA [33], on the other hand, uses an MLP layer to directly bridge encoded image information into the LLM's input space, making the model architecture and training process much more straightforward. Consequently, many subsequent multimodal large models have made improvements based on LLaVA [7,18,29,34,35]. For instance, LLaVA 1.5 [34] optimized data quality, while LLaVA-Next [35] introduced adaptive image segmentation techniques to support highresolution images. \n\nHigh-resolution MLLM. Recently, various MLLMs have adopted high-resolution images as input to capture finegrained image information. In the early stages, most MLLMs use a fixed size of 224 for their inputs. LLaVA-1.5 [34] and BLiVA [19] increased the image size to 336 to achieve better performance. Qwen-VL [3] expanded the size further to 448. It first trained with a fixed image scale of 224 and then fine-tuned by increasing the resolution to 448. Vary [47] and Mini-Gemini [27] additionally introduced a Vision Encoder specifically for high-resolution images. SPHINX [31], Monkey [28], and LLaVA-UHD [48] resize the images to a fixed resolution and then split them into several patches. These patches are individually encoded before being fed to the LLM. Furthermore, LLaVA-NeXT [30] employs a series of predefined resolutions, first matching the input image to the closest resolution, and then segmenting it into sub-images. Qwen2-VL [46] directly uses the original resolution of the image and encodes it into dynamically variable-length visual tokens by modifying the structure of ViT [15].",
            "score": 0.4433739890138676,
            "section_title": "Related Work",
            "char_start_offset": 3818,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 7,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 2038
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 35,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 54,
                    "end": 58,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 282,
                    "end": 285,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 477,
                    "end": 481,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 772,
                    "end": 775,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 804,
                    "end": 808,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1158,
                    "end": 1162,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1173,
                    "end": 1177,
                    "matchedPaperCorpusId": "261049015"
                },
                {
                    "start": 1526,
                    "end": 1530,
                    "matchedPaperCorpusId": "265150038"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4306640625
        },
        {
            "corpus_id": "274131643",
            "title": "TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models",
            "text": "Video-LLaMA [49] and Video-LLaMA2 [5] incorporate video, audio and language modalities to support tasks oriented toward video and audio. LLaVA-NeXT-Video [52] fine-tunes LLaVA-NeXT on video data, with a variant that applies DPO [34] for improved performance. LITA [11] employs a slow-fast design [9,41] to capture spatial and temporal information more effectively. \n\nTraining-free video LLMs extend image LLMs for video understanding without requiring additional fine-tuning on video data. As a pioneering approach, IG-VLM [13] constructs a grid-view image from video frames, which is then fed directly into a frozen image LLM with a specially designed prompt. While promising, the image grid approach has limitations, such as reduced resolution and the limited number of frames it can include, which we further discuss in the next section. FreeVA [40] explores various temporal aggregation methods, but similarly uses a limited number of frames. The current state-of-the-art SF-LLaVA [45] adopts the slow-fast design, which is proven to be effective in action recognition [9,41], and in LITA, as mentioned earlier. SF-LLaVA designs a slow pathway compressing fewer frames, and a fast pathway heavily compressing more frames. While both SF-LLaVA and our method use a twostream design, our Thumbnail-and-Sampling strategy leads to significantly better performance on various video understanding benchmarks, while maintaining a better token efficiency. We extend the discussion in Sec. 4.",
            "score": 0.44313225503944775,
            "section_title": "Related Works",
            "char_start_offset": 4774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 364
                },
                {
                    "start": 367,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1486
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 232,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 296,
                    "end": 299,
                    "matchedPaperCorpusId": "54463801"
                },
                {
                    "start": 1073,
                    "end": 1076,
                    "matchedPaperCorpusId": "54463801"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38671875
        },
        {
            "corpus_id": "271218708",
            "title": "FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models",
            "text": "We employ a student model and a teacher model to perform free dialogues. We evaluate LLaVA-NeXT-8B, FIRE100K-LLaVA, and FIRE-LLaVA as the student model, and use FD-LLaVA to act as the teacher model. We report the average turn (AT), average dialogue refinement (ADR), average turn refinement (ATR), and refinement ratio (RR) on FIRE-Bench. Results are shown in Tab. 4. \n\nWe observe that a LLaVA model trained on FIRE has improved feedback-refining ability. On the ADR, ATR, and RR metrics, FIRE-LLaVA achieves more than 50% improvements by LLaVA-NeXT, making an efficient user-agent interaction. Meanwhile, adding FIRE-1M to training data has better performance than only using FIRE-100K, showing the data quality of FIRE-1M. \n\nWe also show the detailed results on 8 seen source datasets and 8 new source datasets, as shown in Tab. 5 and Tab. 6, respectively. Our models achieve improvements on both seen and new datasets, showing the generalization of feedback-refining ability across different types of data and tasks. In Fig. 7, we present the performance curve in FIRE-Bench concerning the number of turns in dialogues, evaluating LLaVA-NeXT, FIRE100K-LLaVA, and FIRE-LLaVA. We report the percentage of correctly answered samples (those scores greater than 8) after each turn. As the number of turns increases, the percentage of correctly answered samples rises across all three models. 46.57% and 46.77% of the test data is correctly answered in the first turn, for the LLaVA-NeXT model and FIRE100K-LLaVA respectively. For FIRE-LLaVA, 49.60% of the data is correctly answered in the first turn, and this increases to 69.19% after five turns, with 19.59% of the samples being corrected based on feedback. Compared to the LLaVA-NeXT model, FIRE-LLaVA shows an additional 6.79% improvement (from 49.60% \u2192 69.19% vs. 46.57% \u2192 59.37%), highlighting the effectiveness of FIRE-LLaVA when trained on FIRE.",
            "score": 0.44242492685978463,
            "section_title": "Evaluation in Free Dialogues",
            "char_start_offset": 18821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 367
                },
                {
                    "start": 370,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1902
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.427734375
        },
        {
            "corpus_id": "273234426",
            "title": "VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning",
            "text": "Models We evaluate a diverse family of state-of-the-art models with various sizes (ranging from 0.5B to 80B) and different LLM backbones on our benchmark. Specifically, for image-to-text VLLMs, we select Open Flamingo (9B) [Awadalla et al., 2023], IDEFICS (9/80B) [Laurenc \u00b8on et al., 2023], IDEFICS-v2 (8B) [Laurenc \u00b8on et al., 2024], Otter (9B) [Li et al., 2023b], InternLM-XComposer2 (7B) [Zhang et al., 2023], LLaVA-Next (Vicuna-7B) [Liu et al., 2024a], Qwen-VL-Chat (9B) [Bai et al., 2023], Emu2-Chat (34B) [Sun et al., 2023a], VILA (7B) [Lin et al., 2024], Mantis (-Idefics2) [Jiang et al., 2024a], Phi-3-Vision (4B) [Abdin et al., 2024], LongVA [Zhang et al., 2024b], InternLM-XComposer2.5 (7B) [Zhang et al., 2024a], and LLaVA-OneVision (0.5/7/72B) [Li et al., 2024]. For Text-to-image VLLMs, we use GILL (7B) [Koh et al., 2023], SEED-LLaMA (8B, 14B) [Ge et al., 2024], Emu1 (14B) [Sun et al., 2023b], Emu2-Gen (34B) [Sun et al., 2023a]. We also evaluate GPT4V [OpenAI, 2023] on our benchmark. We use officially released model weights or GPT4 API and adopt greedy decoding for reproducibility.",
            "score": 0.4412727425200724,
            "section_title": "Experiment Setup",
            "char_start_offset": 16916,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1101
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 290,
                    "matchedPaperCorpusId": "259287020"
                },
                {
                    "start": 543,
                    "end": 561,
                    "matchedPaperCorpusId": "266174746"
                },
                {
                    "start": 818,
                    "end": 836,
                    "matchedPaperCorpusId": "258959284"
                },
                {
                    "start": 969,
                    "end": 983,
                    "matchedPaperCorpusId": "237347130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17626953125
        },
        {
            "corpus_id": "271404168",
            "title": "How Good (Or Bad) Are LLMs at Detecting Misleading Visualizations?",
            "text": "ChatGPT, Copilot, and Gemini are developed by proprietary companies, whereas LLaVA-NeXT is an open-source multimodal LLM. Notably, at the time of our experiments, LLaVA-NeXT has achieved state-of-the-art results across various multimodal LLM benchmarks among open-source LLMs. Due to hardware constraints, our experiments were conducted with the 7B parameter variant of LLaVA-NeXT, despite the more advanced state-of-the-art 34B parameter model. Nevertheless, including LLaVA-NeXt in our evaluation covers an important landscape of LLM research. The LLaVA-NeXt model was run on a server with an Nvidia Titan RTX 24GB graphical processing unit. On the other hand, the proprietary models were evaluated via their Application Programming Interfaces (APIs). We have made the program code of the experiments publicly available on the OSF 1 . \n\nDataset: Our study compiled an evaluation dataset from previous research on misleading charts circulated on the internet [24]. The dataset was collected through search engines and social media. The chart images are annotated with the issues identified from the original web page or social media post. A total of 74 unique chart issues were identified in the study, providing a diverse sample of misleading charts that the general public may encounter. \n\nIn the initial phase of our exploratory study, we focused on the five most frequently identified issues: (1) Truncated Axis, (2) 3D Chart, (3) Missing Title, (4) Dual Axis, and (5) Misrepresentation. We subsequently expanded to include the ten most common issues in Experiment Two by including (6) Missing Axis Title, (7) Missing Legend, (8) Inconsistent Tick Intervals, (9) Not Data, and (10) Selective Data. In the third experiment, the study was further broadened to cover up to 21 issues due to a tie for the 20th spot, with both the 20th and 21st issues appearing an equal number of 25 times in the dataset. These additional issues included (11) 21) Inconsistent Binning Size.",
            "score": 0.4378082663850128,
            "section_title": "METHODOLOGY",
            "char_start_offset": 18407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 960,
                    "end": 964,
                    "matchedPaperCorpusId": "248266819"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1849365234375
        },
        {
            "corpus_id": "269304163",
            "title": "Temporal Grounding of Activities using Multimodal Large Language Models",
            "text": "Instruction-tuning For the baseline, we ran inference on a non-modified LLaVA 7B (LLaVA-1.5-7B)model using the general prompt strategy introduced in Section 3.1.We then instruction-tuned the model using the dataset described in Section 4.1 using the finetune_task_lora.shscript1 provided by LLaVA over 1 epoch, with a learning rate of 2e \u22124 , for a total of 1.4k steps.Total time for instruction-tuning took around 2 hours on an NVIDIA H100 GPU.Due to the longer context generated by describing each frame in the video, we chose Qwen 7B (Bai et al., 2023) for the text-based LLM as it has a context length of 8192 tokens.Using LLaMA 2 (Touvron et al., 2023), we ran into issues with context length, as the model only has half the context length of that of Qwen models.Qwen 7B 20.1 7.9 2.5 14.9 15.6 7.0 0.8 12.7 GPT-4 Vision GPT-4 28.9 10.9 6.3 20.7\n\nTable 1: R@IoU for base LLaVA, instruction-tuned LLaVA using the general prompting strategy.\n\nFor comparison, we also show video-based LLM (Huang et al., 2023), GPT-4 Vision metrics.\n\nTwo-stage LLM runs Next, we compare metrics of different combinations of multimodal LLM and text-based LLMs as listed in Table 2.We use the activity prompting strategy in our multimodal LLMs to optimize for performance.For comparison, we run models on the state-of-the-art task-specific model (ViGA) by Cui et al. (2022) and a state-of-the-art video-based LLM (Reid et al., 2024), Gemini.\n\nFor inference in all experiments, LLaVA 7B (LLaVA-1.5-7B2) and 16B (LLaVA-1.6-Vicuna-13B3",
            "score": 0.43769658091739994,
            "section_title": "Experimental details",
            "char_start_offset": 12761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 271
                },
                {
                    "start": 271,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 621
                },
                {
                    "start": 621,
                    "end": 768
                },
                {
                    "start": 768,
                    "end": 849
                },
                {
                    "start": 851,
                    "end": 943
                },
                {
                    "start": 945,
                    "end": 1033
                },
                {
                    "start": 1035,
                    "end": 1164
                },
                {
                    "start": 1164,
                    "end": 1254
                },
                {
                    "start": 1254,
                    "end": 1423
                },
                {
                    "start": 1425,
                    "end": 1482
                },
                {
                    "start": 1482,
                    "end": 1514
                }
            ],
            "ref_mentions": [
                {
                    "start": 1338,
                    "end": 1355,
                    "matchedPaperCorpusId": "248266558"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.154541015625
        },
        {
            "corpus_id": "273186947",
            "title": "DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination",
            "text": "LVLM Models We select three of the most representative LVLM models for evaluation: LLaVA-1.5-7b, LLaVA-NeXT-7b, and InstructBLIP-7b. For visual encoder, LLaVA-1.5 and LLaVA-NeXT share the same ViT backbone, both using ViT-L-336px pretrained from CLIP-L/14-336px (Radford et al., 2021). In contrast, InstructBLIP uses ViTg/14 pretrained from EVA-CLIP (Sun et al., 2023). All three models use Vicuna2 (Chiang et al., 2023) as the LLM module. \n\nRegarding the connection module between the two modalities, LLaVA-1.5 and LLaVA-NeXT use MLP layers to bridge feature gap between vision and text modalities without changing the amount of image tokens in the LLM. Conversely, Instruct-BLIP employs Q-Former (Zhang et al., 2024) for modality alignment, which standardized the number of visual tokens in LLM to 32. \n\nOur approach is based on LLaVA-1.5 in the analysis of Section 3.3. For more insights into generalizability, we also test our method on InstructBLIP, which has a significantly different structure compared to LLaVA-1.5, and we find that the performance still surpasses that of original model. This demonstrates that mitigating the impact of outlier tokens in the visual encoder is effective in alleviating hallucination across different projection modules. \n\nBaselines We select two popular and trainingfree contrastive decoding methods: VCD (Leng et al., 2024) and M3ID (Favero et al., 2024). Both approaches aim to enhance the impact of visual features during the LLM decoding phase by eliminating language priors. VCD generates negative logits using Gaussian blurring, while M3ID generates negative logits using pure text that without visual information. Additionally, we include the original model for comparison to highlight the improvements over the baseline model. For detailed experimental hyperparameter settings of these baselines, please refer to Appendix A.",
            "score": 0.4373099586799263,
            "section_title": "Experimental Settings",
            "char_start_offset": 12462,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 803
                },
                {
                    "start": 806,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1260
                },
                {
                    "start": 1263,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1873
                }
            ],
            "ref_mentions": [
                {
                    "start": 262,
                    "end": 284,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 350,
                    "end": 368,
                    "matchedPaperCorpusId": "266976992"
                },
                {
                    "start": 698,
                    "end": 718,
                    "matchedPaperCorpusId": "257767201"
                },
                {
                    "start": 1346,
                    "end": 1365,
                    "matchedPaperCorpusId": "265466833"
                },
                {
                    "start": 1375,
                    "end": 1396,
                    "matchedPaperCorpusId": "268553740"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34033203125
        },
        {
            "corpus_id": "273963071",
            "title": "Aligned Vector Quantization for Edge-Cloud Collabrative Vision-Language Models",
            "text": "First, LLaVA-AlignedVQ consistently demonstrates high accuracy, staying within \u22122.23% to +1.6% of the original LLaVA model and \u22120.86% to +1.82% of LLaVA 1+ , achieving seven top two accuracy among the eleven results Although the variants LLaVA-JPEG-90 and LLaVA-JPEG-90 1+ achieve competitive high accuracy, lower than LLaVA-AlignedVQ by approximately \u22120.16% and \u22120.66% on average, they incur a larger transmission overhead as indicated in Table 3, leading to longer inference latency as shown in Section 4.4. Although using JPEG-10 greatly reduces the transmission overhead compared to JPEG-90, from 26.47KB to 4.14KB per sample as reported in Table 3, it suffers from a significant accuracy drop of 5.09% on average. \n\nSecond, models fine-tuned with compressed images or compressed intermediate features, such as LLaVA-AlignedVQ and LLaVA-JPEG-90 1+ , achieve even higher accuracy than , leads to an accuracy drop of 1.2% on average. While LLaVA-AlignedVQ shows accuracy improvements of 0.85% on VQA-v2, 0.72% on GQA, and 0.78%, 0.07%, 0.44% on POPE-rand, POPE-pop, POPEadv respectively, while LLaVA-JPEG-90 1+ achieves gains of 0.33% on VQA-v2 and 0.08% on GQA. We hypothesize that this improvement comes from the augmentation effects of compressed data during fine-tuning. When fine-tuning the LoRA parameters in the original LLaVA model for an additional epoch, the model revisits the same data without acquiring new information, may cause overfitting on the train data. In contrast, fine-tuning with compressed data, either images or features, exposes the model to more diverse input space, thereby enhancing accuracy.",
            "score": 0.4340804208154419,
            "section_title": "Experimental Settings",
            "char_start_offset": 22226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1624
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2587890625
        },
        {
            "corpus_id": "276161198",
            "title": "PixFoundation: Are We Heading in the Right Direction with Pixel-level Vision Foundation Models?",
            "text": "Model Checkpoint LISA xinlai/LISA-7B-v1-explanatory GLAMM MBZUAI/GLaMM-FullScope GLAMM-RegCap MBZUAI/GLaMM-RegCap-RefCOCOg Llava-G Haozhangcx/llava grounding gd vp Llava 1.5 (7B) liuhaotian/llava-v1.5-7b Llava 1.5 (13B) liuhaotian/llava-v1.5-13b Cambrian-1 (8B) nyu-visionx/cambrian-8b \n\nTable 2. Hugging Face model checkpoints used in our benchmarks.",
            "score": 0.4319601804915312,
            "section_title": "Model Name",
            "char_start_offset": 28570,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 288,
                    "end": 351
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040924072265625
        },
        {
            "corpus_id": "273403785",
            "title": "debiaSAE: Benchmarking and Mitigating Vision-Language Model Bias",
            "text": "We evaluate the performance and biases of various VLMs across different sizes and architectures, including both open and closed-source models. For the open-source models, we assess the LLaVa v1.6 series and the PaliGemma-2 series, specifically: llava-v1. 6-34b-hf, llava-v1.6-vicuna-7b, llava-v1.6-mistral-7b-hf Liu et al. (2023), google/paligemma2-10b-pt-224, and google/paligemma2-10b-pt-448 Steiner et al. (2024). Additionally, we evaluate two CLIP models, CLIP L-224 and CLIP L-336, with the latter serving as the Vision Transformer (ViT) component for all the LLaVa models mentioned. For the closed-source models, we include Google's May 2024 release: gemini-1.5-flash-001 Radford et al. (2021); Goldin et al. (2024). For reproducibility, we set the temperature to 0 in all cases. More details about hardware and API versioning can be found in Appendix A.3",
            "score": 0.43148009462610737,
            "section_title": "Models",
            "char_start_offset": 7664,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 861
                }
            ],
            "ref_mentions": [
                {
                    "start": 678,
                    "end": 699,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.161865234375
        },
        {
            "corpus_id": "277043430",
            "title": "FastVID: Dynamic Density Pruning for Fast Video Large Language Models",
            "text": "Video LLMs. With the rapid advancement of LMMs [1,6,32] and MLLMs [2,15,[19][20][21]33], there has been growing interest in Video LLMs. Existing Video LLMs can be broadly categorized based on how they process video tokens: general Video LLMs and Video LLMs with trainingtime compression. General Video LLMs [5,14,17,18,35,43] directly process raw video tokens or apply pooling. Video-LLaVA [17] leverages shared projection layers to obtain unified visual representations. LLaVA-OneVision [14] demonstrates strong video understanding through task transfer from images. LLaVA-Video [43] creates a high-quality synthetic dataset for video instruction-following. To better capture the spatiotemporal structure of video, some models [35,43] introduce additional designs for video positional information. Qwen2-VL [35] propose M-RoPE to decompose rotary embedding into temporal, height, and width components. LLaVA-Video 1 introduces newline tokens to distinguish spatial and temporal positions effectively. \n\nVideo LLMs with training-time compression [13,22,28,29,37,45] aim to significantly reduce the number of video tokens, enabling longer video sequences. PLLaVA [37] introduces an adaptive average structure pooling to extend image LLMs. Video-ChatGPT [22] extracts both spatial and temporal features through temporal and spatial pooling respectively. Chat-UniVi [13] progressively clusters visual tokens and provides multi-scale features. MovieChat [29] introduces a memory management mechanism to enhance long video understanding. LongVU [28] employs crossmodal query and inter-frame dependencies to adaptively reduce video redundancy. Apollo [45] explores scaling consistency and uses the Perceiver Resampler [12]. \n\nHowever, general Video LLMs remain the dominant paradigm, with LLaVA-OneVision being widely adopted due to its adaptability and superior performance. Moreover, Video LLMs with training-time compression often exhibit lower performance compared to their general counterparts.",
            "score": 0.43113363433916874,
            "section_title": "Related Work",
            "char_start_offset": 4713,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1717
                },
                {
                    "start": 1720,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 69,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 69,
                    "end": 72,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 72,
                    "end": 76,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1046,
                    "end": 1050,
                    "matchedPaperCorpusId": "265157455"
                },
                {
                    "start": 1050,
                    "end": 1053,
                    "matchedPaperCorpusId": "259108333"
                },
                {
                    "start": 1056,
                    "end": 1059,
                    "matchedPaperCorpusId": "260333927"
                },
                {
                    "start": 1252,
                    "end": 1256,
                    "matchedPaperCorpusId": "259108333"
                },
                {
                    "start": 1363,
                    "end": 1367,
                    "matchedPaperCorpusId": "265157455"
                },
                {
                    "start": 1450,
                    "end": 1454,
                    "matchedPaperCorpusId": "260333927"
                },
                {
                    "start": 1712,
                    "end": 1716,
                    "matchedPaperCorpusId": "232110866"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.335693359375
        },
        {
            "corpus_id": "270620510",
            "title": "Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs",
            "text": "To investigate whether the reasoning ability of ChatGPT constrains state-of-the-art VLMs, we implemented a Prism pipeline that decouples GPT-4o by using it as both the perception and reasoning module.The result reveals that this Prism pipeline, with post-processing, achieves an overall accuracy of 61%, nearly identical to the end-to-end GPT-4o performance of 61.6%.\n\nHow does Language Model Size Affect Perception Ability?During evaluation, we observe that the LLaVA-v1.5series shows no significant improvement when using a larger language model (Vicuna-13B instead of Vicuna-7B, etc.).This suggests that perception performance may be independent of the language model size when using a relatively low-resolution vision backbone.However, the situation appears to differ with LLaVA-NeXT.Quantitative results for the LLaVA-NeXT series tell that scaling up the language model slightly enhances model perception, particularly when using query-specify instructions.Through a detailed qualitative analysis, we identified the primary factors contributing to the superior performance of larger LLaVA-NeXT models over smaller ones as follows:\n\n(1) More Elaborate Expression: Models equipped with a larger language encoder exhibit enhanced ability to articulate visual information.More detailed and organized narratives make it easier for the reasoning module to answer the question; (2) More Adaptive to Instruction: Larger language backbones entitle the model with a better understanding of instructions, yielding more suitable textual visual information for reasoning, particularly in response to query-specific instructions.In Fig. 3, we provide some qualitative results about the two typical modes.",
            "score": 0.42828323001294943,
            "section_title": "Detailed Analysis",
            "char_start_offset": 17426,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 200,
                    "end": 367
                },
                {
                    "start": 369,
                    "end": 424
                },
                {
                    "start": 424,
                    "end": 473
                },
                {
                    "start": 473,
                    "end": 588
                },
                {
                    "start": 588,
                    "end": 731
                },
                {
                    "start": 731,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 962
                },
                {
                    "start": 962,
                    "end": 1135
                },
                {
                    "start": 1137,
                    "end": 1273
                },
                {
                    "start": 1273,
                    "end": 1620
                },
                {
                    "start": 1620,
                    "end": 1695
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.380126953125
        },
        {
            "corpus_id": "271719914",
            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
            "text": "We further evaluate LLaVA-OneVision in multi-image interleaved settings, where users may ask questions between multiples images. In particular, we perform comprehensive assessment on the diverse subtasks of LLaVA-Interleave Bench [68], such as Spot the Difference [45], Image Edit Instruction (IEI) [68], Visual Storytelling (VST) [40], Text-rich VQA (TR-VQA) [85], Multi-image VQA (MI-VQA) [117], Raven Puzzle [24], Q-Bench (QB) [139], and NLVR2 [125]). We also utilize several multi-view benchmarks for evaluation, which depict 3D environments with multiple viewpoints, including 3D Dialogue (3D-Chat) and Task Decomposition (3D-TD) from 3D-LLM [38], ScanQA [5], ALFRED [122], and nuScenes VQA [9]. We refer to these datasets as in-domain evaluations, since our training data includes the training split of them. \n\nMoreover, we conduct evaluations on different out-domain tasks, which reveals the generalization capability of our approach. They include the multi-image split of math QA benchmark MathVerse [165] and science QA benchmark SciVerse [34], multi-image perception benchmark BLINK [31], MMMU-(multi-image) [157] that contains all multi-image QA in MMMU, and MuirBench [135] spanning 12 diverse multi-image tasks. \n\nAs shown in Table 4, LLaVA-OneVision (SI) consistently outperforms existing multi-image LMMs in all benchmarks. After additional tuning on multi-image and video data, LLaVA-OneVision shows a marked improvement over GPT-4V in specific areas, with significant margins. This highlights its strong performance in complex tasks such as multi-image reasoning, identifying differences, and understanding 3D environments. In addition, we observe a consistent performance enhancement on after the one-vision training stage, which is more evident on multi-view benchmarks that are absent  in single-image data. This demonstrates the significance of our one-vision paradigm for empowering LMMs with comprehensive visual capbalities.",
            "score": 0.42635068712989077,
            "section_title": "Multi-Image Benchmarks",
            "char_start_offset": 23302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1948
                }
            ],
            "ref_mentions": [
                {
                    "start": 447,
                    "end": 452,
                    "matchedPaperCorpusId": "19435386"
                },
                {
                    "start": 647,
                    "end": 651,
                    "matchedPaperCorpusId": "260356619"
                },
                {
                    "start": 660,
                    "end": 663,
                    "matchedPaperCorpusId": "245334889"
                },
                {
                    "start": 672,
                    "end": 677,
                    "matchedPaperCorpusId": "208617407"
                },
                {
                    "start": 696,
                    "end": 699,
                    "matchedPaperCorpusId": "221340919"
                },
                {
                    "start": 1118,
                    "end": 1123,
                    "matchedPaperCorpusId": "265466525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28466796875
        },
        {
            "corpus_id": "277104362",
            "title": "Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided Visual Tokenizer and Manufacturing Process",
            "text": "The zero-shot anomaly detection results are presented in Tab. 1. Although state-of-the-art LMMs such as LLaVA-OneVision-ov-7B [16] and Qwen2-VL-72B [31] demonstrate strong zero-shot performance on MVTec-AD, WFDD, and PCB-Bank (see Sec. A.1), our Triad-ov-7B remains highly competitive. Notably, by integrating manufacturing processes, Triad-ov-7B surpasses LLaVA-OneVision-ov-7B by 1.6%, highlighting the advantage of manufacturing-aware reasoning in IAD. In contrast, most general-purpose LMMs fail to integrate manufacturing information, resulting in a significant performance drop. Interestingly, we also observe that Myriad benefits from manufacturing processes, likely due to its built-in visual enhancement mechanisms. \n\nTo evaluate the adaptability of our approach, we provide two versions of Triad based on different LMM backbones, LLaVA-1.6-7B and LLaVA-OneVision-ov-7B. When starting with the relatively weak LLaVA-1.6-7B model, Triad yields substantial gains of 8.1% (from 76.9% to 85.0% on MVTec-AD) and 3.5% (from 63.8% to 67.3% on WFDD). Moreover, equipping Triad-llava-1.6-7B with manufacturing processes adds a further 2.5% and 2.6% improvement, respectively. Similarly, Triad-ov-7B outperforms the original LLaVA-OneVision-ov-7B and gains an additional 1.4% and 0.9% boost from manufacturing-aware reasoning on MVTec-AD and WFDD, respectively. These results confirm that our approach generalizes effectively across different model architectures and continues to enhance performance even at higher baseline accuracies (e.g., above 90%). \n\nTable 2 shows 1-shot results. Providing a single reference image generally degrades the performance of most general-purpose LMMs due to their limited instructionfollowing capabilities.",
            "score": 0.42568618709117123,
            "section_title": "Quantity Results",
            "char_start_offset": 17857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1739
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3271484375
        },
        {
            "corpus_id": "270063538",
            "title": "Matryoshka Multimodal Models",
            "text": "The broader impact of M 3 , a framework with nested visual representations, has potential benefits and risks associated with its deployment and release. Our model is trained using the exact same architecture and data of LLaVA-1.5 [5] and LLaVA-NeXT [4]. All the concerns are same as LLaVA. Specifically, as one example, LLaVA conducts instruction tuning using GPT-4 and GPT-4V generated data. The bias from GPT-4 and GPT-4V would still exist in LLaVA.",
            "score": 0.42400409168682485,
            "section_title": "A Broader Impact",
            "char_start_offset": 25780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 451
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1341552734375
        },
        {
            "corpus_id": "271719914",
            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
            "text": "\u2022 The Ablation blog [64] summarizes our empirical exploration except the visual instruction data itself, including the choice of architectures (scaling of LLM & vision encoder), visual representations (resolution & #tokens), as well as training strategies (trainable modules & high-quality data) in the pursuit of data scaling success. \u2022 The Interleave blog [68] describes the strategies to extend and improve the capability in new scenarios including multi-image, multi-frame (video) and multi-view (3D), while maintaining the single-image performance. \n\nThese explorations, conducted within a fixed compute budget, aimed to offer useful insights along the way as we navigate the project, rather than push performance limits. During the process, we have also been accumulating and curating a large collection of the high-quality datasets from January to June. By consolidating these insights and execute the experiments with \"yolo run\" on newly accumulated larger datasets, we introduce LLaVA-OneVision. We implement the new model with the available compute, without extensively de-risking individual components. This leaves room for further improvements in capabilities through additional data and model scaling following our recipe, Please see the detailed development timeline in Section A. In particular, our paper makes the following contributions: \n\n\u2022 Large multimodal models. We develop LLaVA-OneVision, a family of open large multimodal models (LMMs) that improves the performance boundaries of open LMMs in three important vision settings, including single-image, multi-image, and video scenarios. \u2022 Emerging Capabilities with Task Transfer. Our design in modeling and data representations allow task transfer across different scenarios, suggesting a simple approach to yield new emgerging capabilities. In particular, LLaVA-OneVision demonstrate strong video understanding through task transfer from images. \u2022 Open-source. To pave the way towards building a general-purpose visual assistant, we release the following assets to the public: the generated multimodal instruction data, the codebase, the model checkpoints, and a visual chat demo.",
            "score": 0.4223314301858626,
            "section_title": "Introduction",
            "char_start_offset": 1791,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 553
                },
                {
                    "start": 556,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2153
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58203125
        },
        {
            "corpus_id": "271088459",
            "title": "LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models",
            "text": "Multi-image Results. As reported in after our M4-Instruct tuning. After adding DPO, our 7B model attains SoTA performance on VDD and VideoChat-GPT benchmarks, surpassing the previous LLaVA-NeXT-Video (34B). This demonstrates the effective temporal understanding and reasoning capabilities of our model across sequential frames. Note that we calculate the average scores by multiplying a weight of 10 times by the score of Video Detailed Description and VideoChat-GPT. \n\nMulti-view (3D) Results. For 3D perception in Table 3, our model also obtains leading results for both indoor and outdoor scenarios on five in-domain benchmarks. Compared to 3D-LLM and Point-LLM with additional point clouds as input, LLaVA-NeXT-Interleave only accepts multi-view images to interpret the 3D world, attaining significantly higher scores in challenging 3D scenarios. \n\nMulti-patch (single-image) Results. We also add 307k (40%) of original LLaVA-NeXT single-image data, which makes our model capable of doing single-image tasks. We use the anyres training for single-image data, which divides an image into multiple patches, forming another multiimage setting. As shown in Table 4, we maintain the singleimage performance of LLaVA-NeXT-Image. As singleimage data is of high quality and diversity, adding singleimage data also improves the instruction-following ability and enables task transfer from single-image to multi-image, which is demonstrated in Section 6.",
            "score": 0.42175056642798353,
            "section_title": "Main Results",
            "char_start_offset": 14338,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1448
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.359130859375
        },
        {
            "corpus_id": "271329151",
            "title": "SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models",
            "text": "Experimental Settings. We perform all experiments on a system with 8 Nvidia A100 80G graphics cards. SF-LLaVA is built upon LLaVA-NeXT (Liu et al., 2024) 7B and 34B models. We use their pre-trained weights available on HuggingFace4 . To deal with long sequences, we follow LLaVA-NeXT-Video (Zhang et al., 2024b) to apply the rotary position embedding (RoPE) (Su et al., 2024), and use the scaling factor of 2, which doubles the context length to 8192 tokens. \n\nInput and Model Settings. SF-LLaVA takes as inputs a video with arbitrary size and length, and uniformly samples N = 50 frames as key frames. The key frames are resized to 336 \u00d7 336, and the visual encoder (i.e., OpenAI's CLIP-L-14) will output 24\u00d724 tokens for each of them. For the Slow pathway, we uniformly select N slow = 10 frame features from F v and pool their extracted features to 10 \u00d7 12 \u00d7 24; for the Fast pathway, we use features of all frames (i.e., N fast = N = 50) and pool their extracted features to 50 \u00d7 4 \u00d7 4. Thus, SF-LLaVA uses 10 \u00d7 12 \u00d7 24 + 50 \u00d7 4 \u00d7 4 = 3680 visual tokens in total, and we choose this as the maximum number since the inference on the SF-LLaVA-34B model already reaches 80G GPU memory. The SlowFast video tokens are then concatenated with the text tokens as inputs to the LLM.",
            "score": 0.420519878135559,
            "section_title": "IMPLEMENTATION DETAILS",
            "char_start_offset": 14703,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1277
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 375,
                    "matchedPaperCorpusId": "233307138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.283447265625
        },
        {
            "corpus_id": "271161986",
            "title": "Constructing Concept-based Models to Mitigate Spurious Correlations with Minimal Human Effort",
            "text": "Motivated by the observed imperfection of LLaVA annotations, we develop an optional refinement process for improved annotation quality.Devising adequate refinement requires us first to understand the imperfections of LLaVA.We examine randomly selected probing images and their corresponding response pairs to identify errors made by LLaVA.Our inspection reveals LLaVA often confuses the image background with the main object.For instance, in classifying letter opener and can opener in ImageNet-Opener, wood material is used as a visual concept (we will provide more details in Sec. 6).We observe LLaVA makes frequent mistakes to answer if the object is made of wood material, especially when the object is placed on a wooden table.Once identifying LLaVA's errors, the next step entails correcting the errors with nearly no human effort, leveraging an LLM and VFMs.Recently, exploiting different VFMs to perform (sub-)tasks invoked by LLMs has been actively explored [7,8,12,42].LLM learns to utilize tools (e.g., VFMs) through in-context learning and activates these tools to accomplish a specific task.Drawing insights from probing LLaVA's errors, our task is set to rectify annotations by eliminating potentially confusing backgrounds in images before querying LLaVA.To accomplish this, we employ Langchain [6] for implementation.We constitute a chain of tools composed of BLIP-2 [20], Grounding DINO [26] and SAM [17], leveraging the in-context learning capability of LLMs by providing specific examples that guide the activation of VFMs.For instance, the instructions, ignore the background (Fig. 3), trigger the chain of tools.The LLM first activates the visual question answering model, BLIP-2, to identify the main object in the image.Next, it uses Grounding DINO to obtain the bounding box of the main object identified by BLIP-2.Then, the LLM applies SAM to segment the object associated with the bounding box.Finally, the backgroundremoved outputs from the series of tools are used to query LLaVA, making the annotations more accurate.",
            "score": 0.4189406718402387,
            "section_title": "Automatic Annotation Refinement",
            "char_start_offset": 15327,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 135,
                    "end": 223
                },
                {
                    "start": 223,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 425
                },
                {
                    "start": 425,
                    "end": 586
                },
                {
                    "start": 586,
                    "end": 732
                },
                {
                    "start": 732,
                    "end": 865
                },
                {
                    "start": 865,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1104
                },
                {
                    "start": 1104,
                    "end": 1270
                },
                {
                    "start": 1270,
                    "end": 1333
                },
                {
                    "start": 1333,
                    "end": 1542
                },
                {
                    "start": 1542,
                    "end": 1633
                },
                {
                    "start": 1633,
                    "end": 1743
                },
                {
                    "start": 1743,
                    "end": 1839
                },
                {
                    "start": 1839,
                    "end": 1920
                },
                {
                    "start": 1920,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 967,
                    "end": 970,
                    "matchedPaperCorpusId": "235351128"
                },
                {
                    "start": 972,
                    "end": 975,
                    "matchedPaperCorpusId": "253734854"
                },
                {
                    "start": 1383,
                    "end": 1387,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1826171875
        },
        {
            "corpus_id": "269043091",
            "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions",
            "text": "[63]: GPT-4V marks a significant advancement, allowing users to instruct GPT-4 to analyze image inputs.OpenAI conducted extensive safety evaluations and preparations for GPT-4V, building on the safety work done for GPT-4.The training process involved predicting the next word in a document, utilizing a vast dataset of text and image data.GPT-4V inherits both text and vision capabilities, presenting novel features at their intersection.The system card outlines OpenAI's preparation, early access period, safety evaluations, red team assessments, and implemented mitigations before broad release.\n\nLLaVA [50]: LLaVA, an open-source multimodal framework designed to enhance LLMs for understanding both language and images.It utilizes language-only GPT-4 to generate data for instruction-following tasks in a multimodal context.LLaVA integrates a vision encoder from CLIP with the LLM, enabling it to process visual information alongside language.The model undergoes pre-training on image-text pairs and fine-tuning for end-to-end multimodal understanding, resulting in a versatile multimodal chatbot.LLaVA demonstrates impressive multimodal chat abilities and achieves an 85.1% relative score compared to GPT-4 on a synthetic multimodal instruction-following dataset.Upon fine-tuning on the Science QA dataset, LLaVA and GPT-4 together achieve a new state-of-the-art accuracy of 92.53%.\n\nFlamingo [3]: Flamingo introduces novel architectural features to seamlessly integrate vision-only and language-only models.By incorporating interleaved cross-attention layers with frozen language-only self-attention layers, Flamingo excels in handling sequences of intermixed visual and textual data.It adopts a Perceiver-based architecture to convert input sequence data, like videos, into a fixed set of visual tokens.",
            "score": 0.4183571036788612,
            "section_title": "GPT-4V",
            "char_start_offset": 11107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 103,
                    "end": 221
                },
                {
                    "start": 221,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 438
                },
                {
                    "start": 438,
                    "end": 597
                },
                {
                    "start": 599,
                    "end": 722
                },
                {
                    "start": 722,
                    "end": 827
                },
                {
                    "start": 827,
                    "end": 946
                },
                {
                    "start": 946,
                    "end": 1100
                },
                {
                    "start": 1100,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1386
                },
                {
                    "start": 1388,
                    "end": 1512
                },
                {
                    "start": 1512,
                    "end": 1689
                },
                {
                    "start": 1689,
                    "end": 1809
                }
            ],
            "ref_mentions": [
                {
                    "start": 1397,
                    "end": 1400,
                    "matchedPaperCorpusId": "248476411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49658203125
        },
        {
            "corpus_id": "274166071",
            "title": "FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual Token Compression",
            "text": "Training datasets. We pretrain and fine-tune the proposed model with only open-source data. For pretraining, we follow the popular llava series to use llava-pretrain-558k. For fine-tuning, we have made every effort to align our training data with LLaVA-NeXT [35]. However, as the tens of thousands of real user data used by LLaVA-NeXT is not released, we follow Open-LLaVA-NeXT [11] to use 200K ALLaVA-Instruct-VFLAN-4V [8] data as a substitute. Additionally, since TextVQA [44] has been included in the training data of most existing LMMs, we retain it to enable fair comparisons with other LMMs. As a result, the fine-tuning data includes 1M samples, covering sharegpt4v-mix665k [12], ALLaVA-Instruct-VFLAN-4V, DocVQA [41], SynDog-EN [23], ChartQA [40], DVQA [21], AI2D [22], and GeoQA+ [9]. The specific data configuration are available in the supplementary. Benchmarks. We use 10 popular benchmarks to evaluate our method, including ( 1) General question answering benchmarks such as GQA [20] and ScienceQA [38]; \n\n(2) Optical character based visual question answering benchmark such as TextVQA [44]; (3) MLLM benchmarks for specific abilities, like POPE [26], MM-Vet [50] and LLaVA-in-the-wild [33]; (4) Comprehensive MLLM benchmarks such as MME [17] (Perception and Cognition), MMBench [36] and MMBench-CN.",
            "score": 0.41822735624795526,
            "section_title": "Datasets and Benchmarks",
            "char_start_offset": 17874,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1312
                }
            ],
            "ref_mentions": [
                {
                    "start": 474,
                    "end": 478,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 720,
                    "end": 724,
                    "matchedPaperCorpusId": "220280200"
                },
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "250924870"
                },
                {
                    "start": 750,
                    "end": 754,
                    "matchedPaperCorpusId": "247593713"
                },
                {
                    "start": 761,
                    "end": 765,
                    "matchedPaperCorpusId": "4445015"
                },
                {
                    "start": 772,
                    "end": 776,
                    "matchedPaperCorpusId": "2682274"
                },
                {
                    "start": 789,
                    "end": 792,
                    "matchedPaperCorpusId": "235253782"
                },
                {
                    "start": 992,
                    "end": 996,
                    "matchedPaperCorpusId": "152282269"
                },
                {
                    "start": 1011,
                    "end": 1015,
                    "matchedPaperCorpusId": "252383606"
                },
                {
                    "start": 1099,
                    "end": 1103,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 1159,
                    "end": 1163,
                    "matchedPaperCorpusId": "258740697"
                },
                {
                    "start": 1172,
                    "end": 1176,
                    "matchedPaperCorpusId": "260611572"
                },
                {
                    "start": 1199,
                    "end": 1203,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.203857421875
        },
        {
            "corpus_id": "263672058",
            "title": "Improved Baselines with Visual Instruction Tuning",
            "text": "Our study originates from LLaVA and builds a road map by carefully making effective contributions from the perspectives of the input, model, and data. \n\nFirst, we unveil that the fully-connected vision-language connector in LLaVA is surprisingly powerful and dataefficient, and we establish stronger and more feasible baselines built upon the LLaVA framework. We report that two simple improvements, namely, an MLP cross-modal connector and incorporating academic task related data such as VQA, are orthogonal to the framework of LLaVA, and when used with LLaVA, lead to better multimodal understanding capabilities. In contrast to InstructBLIP [14] or Qwen-VL [3], which trains specially designed visual resamplers on hundreds of millions or even billions of image-text paired data, LLaVA uses one of the simplest architecture design for LMMs and requires only training a simple fullyconnected projection layer on merely 600K image-text pairs. Our final model can finish training in \u223c1 day on a single 8-A100 machine and achieves state-of-the-art results on a wide range of benchmarks. Moreover, unlike Qwen-VL [3] that includes in-house data in training, LLaVA utilizes only publicly available data. \n\nNext, we delve into an early exploration of other open problems of large multimodal models. Our findings include: (1) Scaling to high-resolution image inputs. We show that LLaVA's architecture is versatile in scaling to higher resolutions by simply dividing images into grids and maintains its data efficiency; with the increased resolution, it improves the model's detailed perception capabilities and reduces hallucination. (2) Compositional capabilities. We find that large multimodal models are capable of generalizing to compositional capabilities. For example, training on long-form language reasoning together with shorter visual reasoning can improve the model's writing capability for multimodal questions. (3) Data efficiency. We show that randomly downsampling LLaVA's training data mixture by up to 75% does not significantly decrease the model's performance, suggesting that the possibility of a more sophisticated dataset compression strategy can further improve LLaVA's already efficient training pipeline. (4) Data scaling.",
            "score": 0.41783436746868696,
            "section_title": "Introduction",
            "char_start_offset": 1829,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 153,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1201
                },
                {
                    "start": 1204,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2225
                },
                {
                    "start": 2226,
                    "end": 2243
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.490966796875
        },
        {
            "corpus_id": "273346680",
            "title": "MMAD: A Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection",
            "text": "We compare the performance of over a dozen models, including commercial APIs, interleaved MLLMs, industrial MLLMs, and vision-centric MLLMs, as shown in Table 2. All models outperform the random baseline. The open-source models perform the best, with the average accuracy of the GPT-4o and Gemini-1.5-pro models reaching 74.9% and 73%, respectively. However, their cost-efficient counterparts, GPT-4o-mini and Gemini-1.5-flash, only achieved 66.3% and 68.9%, respectively, falling short of the best open-source model, InternVL2-76B, which achieved 70.8%. AnomalyGPT performs poorly overall, primarily due to its training on the IAD task in a fixed question-and-answer format, leading to severe overfitting issues. It demonstrates decent performance in anomaly discrimination because we specifically adapted the question format to suit its training. Similarly, the vision-centric MLLMs, Cambrian-1 and SPHINX, do not exhibit superior performance on the fine-grained visual tasks of MMAD, likely due to their foundational language models not being advanced enough. Among the general open-source MLLMs, earlier models like Qwen-VL-Chat and LLaVA-1.5 underperform compared to newer models like LLaVA-OneVision and MiniCPM-V2.6, indicating that advancements in general capabilities benefit performance on IAD tasks. MMAD uses a default 1-shot format, providing a normal image for comparison with the test image. Thus, multi-image understanding, especially image comparison, is crucial, while LLaVA-NEXT-Interleave, trained for this, does not perform outstandingly. LLaVA-NeXT-34B and InternVL2-76B, due to their larger scales, achieve the top two performances among open-source models, highlighting the importance of model size. \n\nHuman evaluation. We conduct a preliminary human evaluation using 177 examples randomly sampled from the entire benchmark. Eight evaluators were divided into two groups: 3 industrial anomaly detection researchers as experts and 5 ordinary participants. As shown in",
            "score": 0.4170520249575047,
            "section_title": "EXPERIMENTAL RESULTS",
            "char_start_offset": 19035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1723
                },
                {
                    "start": 1726,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 1990
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2763671875
        },
        {
            "corpus_id": "268876083",
            "title": "Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns",
            "text": "Interestingly, our evaluation revealed that larger models did not consistently perform better in some tasks.The LLaVA-v1.5-13Bmodel performed worse than its 7B model in GEN, ERR, and VQA.Also, this trend is observed in LLaVA-v1.6models: 7B model outperforming in DDx and VQA.This result suggests that adding more parameters does not directly translate to performance improvements.",
            "score": 0.4164640017778124,
            "section_title": "Larger models do not perform well",
            "char_start_offset": 12772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 108,
                    "end": 126
                },
                {
                    "start": 126,
                    "end": 187
                },
                {
                    "start": 187,
                    "end": 229
                },
                {
                    "start": 229,
                    "end": 275
                },
                {
                    "start": 275,
                    "end": 380
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.223388671875
        },
        {
            "corpus_id": "271244415",
            "title": "In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery",
            "text": "Table 4: Comparison on the impact of additional visual input. All experiments are performed with LLaVa-NeXT as the underlying model, either providing or excluding a plot of the best previous function in the prompts (respectively ICSR-V and ICSR columns). We report the averages with their errors.",
            "score": 0.41428681404310974,
            "section_title": "A.3 Comparison of Text-Only and Vision-Language Models",
            "char_start_offset": 37020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 296
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326904296875
        },
        {
            "corpus_id": "273532279",
            "title": "LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding",
            "text": "Quantitative Results. Table 1 presents our experimental results on multiple video understanding benchmarks. \n\nOur results compares favorably to all the baselines across various video understanding benchmarks. For example, on VideoMME (Fu et al., 2024), our LongVU outperforms VideoChat2 (Li et al., 2024b), LLaVA-OneVision (Li et al., 2024a) by 6.0% and 2.4% respectively. Notably, on VideoMME Long subset (Fu et al., 2024), our model surpasses LLaVA-OneVision (Li et al., 2024a) by 12.8%. These results indicate the strong video understanding capabilities of our model. Note that our model achieves significant improved performance with a much smaller training dataset, comparing to LLaVA-OneVision (Li et al., 2024a) trained on OneVision-1.6M (multi-image, video) that has not yet been made publicly available1 . With the same video training dataset from VideoChat2-IT (Li et al., 2024b), our LongVU shows much higher performance than VideoChat2 (Li et al., 2024b), \u223c10% accuracy improvement in average. Interestingly, we also find that our model can even beat proprietary model GPT4-o (OpenAI, 2024) on MVBench (Li et al., 2024b) with densely sampled video input and reduce the accuracy gap comparing to proprietary models on other video benchmarks. \n\nWe also scale our LongVU with a lightweight LLM, Llama3.2-3B (Llama, 2024), to further demonstrate the strong video understanding capabilities. We observe the consistent improvement of our light-weight LongVU over baselines in Table 2. Our method outperforms Phi-3.5-vision-instruct (Abdin et al., 2024) on VideoMME (Long) by margin of 3.4% accuracy. This set of experiments validate the effectiveness of our method even scaling to a smaller size.",
            "score": 0.4142844590684387,
            "section_title": "Video Understanding",
            "char_start_offset": 18672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 107
                },
                {
                    "start": 110,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1702
                }
            ],
            "ref_mentions": [
                {
                    "start": 287,
                    "end": 305,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 871,
                    "end": 889,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 948,
                    "end": 966,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 1114,
                    "end": 1132,
                    "matchedPaperCorpusId": "265466214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27197265625
        },
        {
            "corpus_id": "273549699",
            "title": "Needle In A Video Haystack: A Scalable Synthetic Evaluator for Video MLLMs",
            "text": "In Table 11, we report the whole result on 9 VNBench tasks and the overall score for each model on the main split of VNBench. We summarize the result as follows: \n\n1) Proprietary models perform better than open-source models on most VNBench tasks. In terms of overall accuracy, the highest performance among open-source models (58.7% for LLaVA-OneVision-72B) and the highest performance among proprietary models (66.7% for Gemini 1.5 Pro) differ by 8.0%. Additionally, the average accuracy of proprietary models is also significantly higher than that of open-source models. \n\n2) Performance on multiple-needle long-dependency tasks is lower than on single-needle shortdependency tasks. When comparing the accuracy across different tasks, we find that most models Table 2: Evaluation Results on VNBench. VNBench comprises three synthetic tasks constructed using the VideoNIAH method, with each task divided into three splits. \"E\" denotes intra-frame editing needles, while \"I\" represents inter-frame inserting needles. The numbers \"1\" and \"2\" refer to the difficulty levels of the sub-tasks, with \"1\" indicating simple and \"2\" indicating hard. In total, we evaluated 3 proprietary models and 9 open-source models across these tasks. \n\nProprietary Models \n\nGemini 1.5 Pro (Reid et al., 2024)  perform much better on retrieval tasks than on ordering and counting. For most proprietary models, they can retrieve almost all the inserted information (for example, 100% accuracy on Retrieval-E task). For some open-source models (such as ST-LLM, LLaVA-NeXT-Video, Qwen2-VL, LLaVA-OneVision), the retrieval accuracy is also significantly higher than on the other two tasks. \n\n3) The gap between open and proprietary models in the ordering task is enormous. The most advanced proprietary models are far ahead of other models in the ordering task (with Gemini 1.5 Pro at 72.9% accuracy on ordering task and GPT-4o at 73.4%), while most open-source models are nearly incapable of completing the ordering task with the exception of the LLaVA-OneVision series.",
            "score": 0.41322639534133787,
            "section_title": "MAIN RESULTS",
            "char_start_offset": 17661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 161
                },
                {
                    "start": 164,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1665
                },
                {
                    "start": 1668,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 2047
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.159423828125
        },
        {
            "corpus_id": "269449774",
            "title": "MileBench: Benchmarking MLLMs in Long Context",
            "text": "We provide basic information of benchmarked models in Table 7, as well as brief introduction for each model to highlight their characteristics:\n\n\u2022 GPT-4V (OpenAI, 2023) and GPT-4o8 are developed by OpenAI and are deemed the most powerful vision-language models for comprehension and generation.\u2022 Gemini 1.0 (Anil et al., 2023) and Gemini 1.5 (Reid et al., 2024) are models developed by Google, demonstrating competitive ability among closed-source MLLMs.\n\n\u2022 Claude 3 Opus9 is a vision-language model recently released by Anthropic, impressing the community with its extra-long 200K context length.\u2022 LLaVA-v1.5 (Liu et al., 2023a)  \u2022 LLaVA-v1.6(Liu et al., 2024b) takes a step further on the basis of LLaVA-v1.5.It is able to process images with any resolution and releases more variants based on different LLMs.\n\n\u2022 ALLaVA-Longer (Chen et al., 2024) is a lite version of LLaVA with enhanced complex reasoning ability, even achieving competitive results with larger models.\n\n\u2022 Yi-VL (Young et al., 2024) is based on LLaVA architecture and adopts a 3-stage training process.\n\n\u2022 Cheetor (Li et al., 2023c) proposes to use a Visual Prompt Generator to capture residual visual details, which might be vital for model performance.\n\n\u2022 Qwen-VL-Chat (Bai et al., 2023) is a 7B model trained on billions of multimodal samples.\n\n\u2022 VILA (Lin et al., 2023) adopts a LLaVA-like structure but is trained with interleaved image-text data.\n\n\u2022 Mantis (Jiang et al., 2024) uses LLaMA310 as language model and is trained with multi-image data.\n\n\u2022 Open flamingo (Awadalla et al., 2023) the open-source implementation of Flamingo (Alayrac et al., 2022), which serves as the foundation of subsequent multi-image and video comprehension models.",
            "score": 0.4131420861842255,
            "section_title": "C.1 Details of Evaluation Models",
            "char_start_offset": 23204,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 145,
                    "end": 294
                },
                {
                    "start": 294,
                    "end": 454
                },
                {
                    "start": 456,
                    "end": 597
                },
                {
                    "start": 597,
                    "end": 643
                },
                {
                    "start": 643,
                    "end": 711
                },
                {
                    "start": 711,
                    "end": 811
                },
                {
                    "start": 813,
                    "end": 971
                },
                {
                    "start": 973,
                    "end": 1071
                },
                {
                    "start": 1073,
                    "end": 1223
                },
                {
                    "start": 1225,
                    "end": 1315
                },
                {
                    "start": 1317,
                    "end": 1421
                },
                {
                    "start": 1423,
                    "end": 1522
                },
                {
                    "start": 1524,
                    "end": 1719
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.370361328125
        },
        {
            "corpus_id": "273403951",
            "title": "Harnessing Webpage UIs for Text-Rich Visual Understanding",
            "text": "We evaluate our models against various baselines, including LLaVA-1.5 series (Liu et al., 2024a) and LLaVA-1.6 (NeXT) series (Liu et al., 2024b). To ensure a fair comparison, given the different backbones of UIX from the original LLaVA checkpoints, we re-implemented three baselines: LLaVA-Vicuna, Llama3.1, Qwen2, utilizing the same training data as LLaVA. We also compare our models with Pix2Struct (Lee et al., 2023), S4 (Gao et al., 2024), SeeClick (Cheng et al., 2024), Co-gAgent (Hong et al., 2023), and ScreenAI (Baechler et al., 2024). Furthermore, we include GPT-4V and GPT-4o as strong baselines to provide a comprehensive evaluation. Enhanced GUI Grounding Capability: Meanwhile, we observe that, compared with general figure grounding (Table 3), current MLLMs fail to handle grounding tasks in complex GUI scenarios. Conversely, trained with MultiUI, which encompasses two web grounding tasks, UIX also demonstrates enhanced GUI grounding capability.",
            "score": 0.41312325189751276,
            "section_title": "BASELINES",
            "char_start_offset": 18249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 962
                }
            ],
            "ref_mentions": [
                {
                    "start": 77,
                    "end": 96,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 424,
                    "end": 442,
                    "matchedPaperCorpusId": "268253650"
                },
                {
                    "start": 453,
                    "end": 473,
                    "matchedPaperCorpusId": "267069082"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2587890625
        },
        {
            "corpus_id": "271909100",
            "title": "CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs",
            "text": "Network architecture: In this work, we consider two LVLMs architectures: MobileVLM-v2 [14] and LLaVA-1.5 [42]. Both models follow the same overall structure: a pre-trained CLIP vision encoder and a pre-trained LLM. The visual tokens produced by the frozen vision encoder are projected using either a linear layer or a small projection module and passed as input to the LLM. \n\nLLaVA opts for a pre-trained Vicuna LLM [12] while MobileVLM-v2 uses Mo-bileLLaMA [13], except for their 7B variant, which also uses a Vicuna model. Both use the same ViT-L-14 @ 336px CLIP visual encoder [56]. For efficiency purposes, MobileVLM-v2 uses a projection module that halves the number of visual tokens. We consider the following model variants in our comparisons: MobileVLM-v2 (1.7B, 3B and 7B), and LLaVA-1.5 7B. For data annotation, we use ViT-H/14 DFN [19] as our pre-trained CLIP model. \n\nTraining details: For all of our experiments, unless specified otherwise, we start from the pre-trained MobileVLM-v2 and LLaVA-1.5 models. The models are then fine-tuned for 1 epoch using DPO-based optimization on the constructed CLIP-DPO dataset. We use the following hyperparameters for fine-tuning the models: AdamW optimizer [45] with a batch size of 256, a learning rate of 5e \u2212 7, decreased to 0 using a cosine scheduler, a warm-up of 0.01, and a weight decay set to 0. Both during training and testing, the input images are cropped and resized to 336 \u00d7 336px. The training was performed on 8 A100 GPUs using Pytorch [54]. For the larger models, e.g. LLaVA-1.5 7B, to fit them in memory, we use the Zero-3 strategy [58,59] and decrease the batch size to 64.",
            "score": 0.4110838686936489,
            "section_title": "Implementation details",
            "char_start_offset": 20514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1643
                }
            ],
            "ref_mentions": [
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1601,
                    "end": 1605,
                    "matchedPaperCorpusId": "233289729"
                },
                {
                    "start": 1605,
                    "end": 1608,
                    "matchedPaperCorpusId": "221191193"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.396728515625
        },
        {
            "corpus_id": "277467445",
            "title": "Context-Aware Human Behavior Prediction Using Multimodal Large Language Models: Challenges and Insights",
            "text": "Observing the results from the changing number of ICL examples across the models in Fig. 4, we observe a general trend that the performance improves with the growing number of ICL examples, providing evidence in favor of H3. All models show a consistent improvement in all three metrics as the number of ICL examples increases from 0 (i.e., zero-shot) to 15, except for Qwen-7B, whose accuracy score considerably drops when the number of ICL examples grows from 0 to 5, especially in blind and image settings. Furthermore, both Qwen models encounter a performance saturation in all three visual-based representations, especially with more than 10 examples. This is possibly related to the inconsistent quality of the examples since they are randomly sampled over the entire evaluation dataset. Another possible reason can be potential overfitting with small example sets, causing it to focus too heavily on specific features from those few examples rather than generalizing appropriately. \n\nUnfortunately, the LLaVA models seem to be unsuitable for ICL. This is potentially caused by its prompt template that always first renders all images before processing the text 2 . The missing tokenize-in-place ability leads to the mismatching of example and query images, therefore consistently resulting in failures and generating corrupted output tokens. Hence, we do not report the ICL results of the LLaVA models 2 https://huggingface.co/llava-hf/LLaVA-NeXT-Video-7B-hf/discussions/3 in Fig. 4. Moreover, we also observed that LLaVA models perform poorly in generating structured output, making it difficult to extract the predicted interaction labels.",
            "score": 0.4106219558491371,
            "section_title": "B. Number of In-Context Learning Examples",
            "char_start_offset": 24432,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 988
                },
                {
                    "start": 991,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1648
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1917724609375
        },
        {
            "corpus_id": "274306424",
            "title": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding",
            "text": "Leveraging breakthroughs in large language models within natural language processing, Multimodal Large Language Models (MLLMs) [3,4,6,13,14,16,18,36,60,64,79,85] have demonstrated robust visual understanding capabilities. \n\nLLaVA [50] pioneered the paradigm of visual instruction tuning, inspiring a wave of subsequent work. Research on general-purpose MLLMs encompasses various directions, including: i) exploring the use of high-resolution image inputs to enhance model perceptual abilities, with models like LLaVA-Next [51], SPHINX [48], Monkey [43], InternLM-XComposer2 [20], LLaVA-UHD [89], NVLM [17] employing image slicing methods, and others like LLaVA-HR [59], Mini-Gemini [41], Eagle [76], and MG-LLaVA [108] utilizing high-resolution vision encoders for additional vision encoding; ii) investigating diverse approaches for pre-training [45,63,91] and fine-tuning data [37,82], and iii) extending to multi image [28,37] or video tasks [44,90].",
            "score": 0.40987005762362905,
            "section_title": "General MLLMs",
            "char_start_offset": 6146,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 224,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 953
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 132,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 134,
                    "end": 137,
                    "matchedPaperCorpusId": "252222320"
                },
                {
                    "start": 140,
                    "end": 143,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 155,
                    "end": 158,
                    "matchedPaperCorpusId": "266361876"
                },
                {
                    "start": 847,
                    "end": 851,
                    "matchedPaperCorpusId": "266174746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33837890625
        },
        {
            "corpus_id": "268513172",
            "title": "SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant",
            "text": "We evaluate SQ-LLaVA on ten benchmarks, covering a range of academic Visual Question Answering (VQA) tasks and recent instruction tuning tasks designed for large vision language models.The academic VQA includes VQA-v2 [10] and VizWiz [11].GQA [13] is a fine-grained real-world visual reasoning and questionanswering benchmark.ScienceQA [25] is a benchmark with rich subjects (natural science, language science, and social science).TextVQA [39] requires the model to recognize the texts in the image.LLaVA (in the wild) and MM-Vet [53] use GPT4 to assess the capability of the models for testing.With manually designed questions, MM-Bench and MMBench-CN [23] evaluate the model's vision-related reasoning and perception for English and Chinese, respectively.POPE [20] is a benchmark for evaluating object hallucination [34].\n\nIn Table 1, we quantitatively compare between SQ-LLaVA and existing models.SQ-LLaVA-7B and SQ-LLaVA-13B trained on two instruction datasets [5,21] outperform previous methods in six out of ten visual instruction tuning tasks.To be specific, SQ-LLaVA-7B achieves 17.2% improvement over LLaVA-v1.5-7BTable 1: Comparison with state-of-the-art methods on ten benchmarks.After training on the same instruction data, SQ-LLaVA traind on [21] and SQ-LLaVA * trained on [5] surpass their baseline model LLaVA-v1.5 and ShareGPT4V on 9 out of 10 and 6 out of 10 benchmarks in the 7B scale, and 8 out of 10 and 6 out of 10 in the 13B scale.The best results are bold and the second-best results are underlined.on the LLaVA (in the wild) benchmark, indicating the superior capabilities of our model in tasks such as detailed description and complex reasoning.Also, SQ-LLavA-7B improves over previous methods on ScienceQA, indicating that our model excels in understanding and reasoning over scientific content and can effectively handle multi-modal information.",
            "score": 0.40852000310307246,
            "section_title": "Zero-shot Multilingual Capability",
            "char_start_offset": 23087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 185,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 326
                },
                {
                    "start": 326,
                    "end": 431
                },
                {
                    "start": 431,
                    "end": 499
                },
                {
                    "start": 499,
                    "end": 595
                },
                {
                    "start": 595,
                    "end": 757
                },
                {
                    "start": 757,
                    "end": 823
                },
                {
                    "start": 825,
                    "end": 900
                },
                {
                    "start": 900,
                    "end": 1050
                },
                {
                    "start": 1050,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1191
                },
                {
                    "start": 1191,
                    "end": 1453
                },
                {
                    "start": 1453,
                    "end": 1522
                },
                {
                    "start": 1522,
                    "end": 1670
                },
                {
                    "start": 1670,
                    "end": 1872
                }
            ],
            "ref_mentions": [
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "260611572"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "259837088"
                },
                {
                    "start": 762,
                    "end": 766,
                    "matchedPaperCorpusId": "258740697"
                },
                {
                    "start": 818,
                    "end": 822,
                    "matchedPaperCorpusId": "52176506"
                },
                {
                    "start": 965,
                    "end": 968,
                    "matchedPaperCorpusId": "265308687"
                },
                {
                    "start": 968,
                    "end": 971,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1255,
                    "end": 1259,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1286,
                    "end": 1289,
                    "matchedPaperCorpusId": "265308687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.317138671875
        },
        {
            "corpus_id": "272689854",
            "title": "TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings",
            "text": "In \u2022 The proposed text-guided strategy demonstrates substantial improvements over the baseline. Compared with the original LLaVA-1.5, TG-LLaVA achieve much better performance. As shown in the first four rows. our method leads on the majority of evaluation datasets. \n\nIt is noteworthy that TG-LLaVA demonstrates an average improvement of 1.5% over the original LLaVA-1.5 across ten datasets when using Vicuna-7B, highlighting the method's significant value. When juxtaposed with the baseline LLaVA-1.5 Vicuna-7B model, we enhance performance metrics by +2.2% on MM-Bench, +2.4% on both MMStar and MMMU, and +3.2% on LLaVABenchs, respectively. For LLaVA-1.5 with Vicuna-13B, we also achieve an average performance improvement of 1%. Specifically, we see a +1.6% gain on MMStar, a +2.0% gain on MMMU, and a +3.2% gain on MME. These impressive results further validate the contribution of the proposed TG-LLaVA architecture to visual feature optimization, highlighting the favorable impact of our method. \n\n\u2022 The proposed TG-FOM and TG-DP modules can be universally applied as a modular plugnin to mainstream VLM frameworks. As shown in the rest part of Table 1. we further validate the versatility of our proposed method under various settings. We replace CLIP with SigLIP and substitute Vicuna with Llama3 and Qwen2 on top of the original LLaVA-1.5 framework. We compare these settings with our method as the baseline. The results in Table 1 confirm that our method continues to maintain a leading advantage across most datasets, demonstrating that the proposed TG-LLaVA exhibits excellent generalizability and possesses strong potential for adaptation to a wide range of VLM architectures.",
            "score": 0.40816804166559073,
            "section_title": "Genuine Improvement Over the Baseline",
            "char_start_offset": 19349,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 265
                },
                {
                    "start": 268,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1689
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.408447265625
        },
        {
            "corpus_id": "271064540",
            "title": "A Single Transformer for Scalable Vision-Language Modeling",
            "text": "We select various open-source LVLMs for comparison to better understand the capabilities of SOLO.Based on the release time and capabilities of LVLMs, we select 3 groups of LVLMs to better understand the current development phase of SOLO.Level-1 LVLMs represent the pioneering generation, which initiate the integration of visual encoders with pre-trained LLMs, with releases prior to October 2023.Level-2 LVLMs, released before early 2024, typically feature a more refined selection of instruction fine-tuning data to enhance performance.Level-3 marks the state-of-the-art (SoTA) LVLMs, released within the last five months, incorporating advanced training recipes, superior LLM backbones, and support for high-resolution images.\n\n\u2022 Level-1: (1) OpenFlamingo v2 (Awadalla et al., 2023), (2) MiniGPT4 v2 (Chen et al., 2023a), (3) VisualGLM (Du et al., 2022), ( 4) InstructBLIP (Dai et al., 2023), (5) LLaVA v1 (Liu et al., 2023a).\u2022 Level-2: (6) LLaVA v1.5 (Liu et al., 2024a), ( 7) mPLUG-Owl v2 (Ye et al., 2024), ( 8) InternLM-XComposer (Zhang et al., 2023a), ( 9) MiniCPM-v1 (Hu et al., 2023), \u2022 Level-3: (10) Monkey (Li et al., 2024b).( 11) LLaVA-NEXT (Liu et al., 2024b), (12) MiniCPM-v2 (Hu et al., 2024b), ( 13) DeepSeek-VL (Lu et al., 2024).\n\nNote that each LVLM may have multiple variants based on different LLM sizes and architectures.Whenever possible, we opt for the variant equipped with a 7B Mistral LLM.",
            "score": 0.4076054464002522,
            "section_title": "MODEL",
            "char_start_offset": 17423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 237
                },
                {
                    "start": 237,
                    "end": 397
                },
                {
                    "start": 397,
                    "end": 538
                },
                {
                    "start": 538,
                    "end": 729
                },
                {
                    "start": 731,
                    "end": 929
                },
                {
                    "start": 929,
                    "end": 1137
                },
                {
                    "start": 1137,
                    "end": 1247
                },
                {
                    "start": 1249,
                    "end": 1343
                },
                {
                    "start": 1343,
                    "end": 1416
                }
            ],
            "ref_mentions": [
                {
                    "start": 839,
                    "end": 856,
                    "matchedPaperCorpusId": "247519241"
                },
                {
                    "start": 909,
                    "end": 927,
                    "matchedPaperCorpusId": "248496506"
                },
                {
                    "start": 955,
                    "end": 973,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 994,
                    "end": 1011,
                    "matchedPaperCorpusId": "265050943"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2039794921875
        },
        {
            "corpus_id": "273098155",
            "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
            "text": "As illustrated in Table 5, with only 9.4k input prompts, the reward signal provided by LLaVA-Critic substantially improve the base model's performance across various openended visual chat benchmarks. It achieves the best improvements of +10.1 on LLaVA-W, +3.0 on LLaVA-Wilder, +8.8 on WildVision-Bench, along with the second-highest gains of + 4.4 on LiveBench and +0.13 on MMHal-Bench, respectively. At the same time, the overall capacities of LLaVA-v1. 5 other comprehensive benchmarks. This is superior to other competing methods, which either result in smaller performance gains or achieve improvements by compromising the overall capabilities on other benchmarks. \n\nInference Time Search Applying LLaVA-Critic for bestof-n sampling [37] further enhances LMM performance during inference. For the LLaVA-OV-7B checkpoint after iterative DPO training, we generate n = 5 responses for each question with a temperature of 0.7 and top-p of 0.9, then use LLaVA-Critic-7B to select the best responses. As shown in Table 7, this results in additional gains of +1.7 on LLaVA-W and +3.2 on LLaVA-Wilder.",
            "score": 0.4066499007216939,
            "section_title": "Preference Learning",
            "char_start_offset": 29442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1097
                }
            ],
            "ref_mentions": [
                {
                    "start": 737,
                    "end": 741,
                    "matchedPaperCorpusId": "221665105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.241455078125
        },
        {
            "corpus_id": "276903205",
            "title": "Integrating Frequency-Domain Representations with Low-Rank Adaptation in Vision-Language Models",
            "text": "For SigLIP + LoRA, there is a slight improvement as the rank increases, but it plateaus beyond a certain point. The SigLIP + DFT + LoRA configuration exhibits a sharp increase in the BLEU score with higher ranks. \n\nThe qualitative results presented in these Tables III, IV, V, respectively, demonstrate the effectiveness of our proposed method in the generation of captions and visual question answering (VQA). Through visual examples, we compare the output of our model with the Llava 7B model, highlighting the improvements in contextual accuracy, visual detail preservation, and the ability to generate coherent and contextually appropriate captions. \n\nTable III presents the qualitative results of our model on selected frames extracted from a video recorded using a smartphone. The recorded video link and the corresponding model responses are available here. For    Llava7B: This is an image of a tree-lined street with two pictures side by side. \n\ncomparison, we evaluated our model alongside the Llava 7B model asking both models the same question: \"Describe the image in one sentence.\" The first response corresponds to our model and the second to Llava 7B. \n\nOur model provides detailed output, including the objects present in the image along with their colors, while Llava 7B fails to detect such details. In practical applications, this difference is significant. For instance, if the robot  Fig. 4: Model performance on variation of rank needs to be instructed to \"go to the red car\" or \"follow the red car,\" our model's response can be directly used to issue precise commands, unlike Llava 7B. Similar observations can be made across other images, where our model consistently outperforms Llava 7B in providing detailed and actionable outputs. Next, we present in Table IV our model's response to real images captured by a RGB camera. A RealSense camera was mounted on top of the UGV to capture images in a parking environment. In this scenario, we evaluated the responses of our model and the Llava 7B model using four different images. For each image, we asked the question: \"What do you see in this image? Respond in one sentence.\" Our proposed model provides more detailed descriptions of the robot's surroundings, offering specific information about objects and their characteristics.",
            "score": 0.4061259477435389,
            "section_title": "V. RESULTS AND DISCUSSION",
            "char_start_offset": 13470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 212
                },
                {
                    "start": 215,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2304
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2252197265625
        },
        {
            "corpus_id": "277065890",
            "title": "Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection",
            "text": "(2) Greedily encoding all sub-areas of an image provides limited benefits. The primary difference between SEM-CLIP and LLaVA-NeXT-7B lies in their vision encoding strategies. SEMCLIP selectively focuses on the most queryrelevant sub-areas. On the contrary, LLaVa-NeXT takes a more aggressive approach by dividing the image into a 2\u00d72 grid, encoding each section separately, and appending the additional visual tokens to the encoded overview tokens. The technique of SEMCLIP is shown to be particularly effective for tasks requiring detailed comprehension, such as V * , where it achieves a 9.5% higher accuracy than LLaVA-1.5-7B with the same model size. \n\nBy generating extra visual tokens, the crop-and-encode method enables higher-resolution analysis. However, its effectiveness diminishes for tasks that do not demand finegrained details, especially with smaller images. For ex-ample, on POPE, an object detection benchmark, LLaVA-NeXT achieves only a 1.2% accuracy improvement, as its images (approximately 333\u00d7600 pixels on average) already contain sufficient information for object presence detection, rendering the additional visual tokens redundant. A similar trend is observed in SQA, a scientific question-answering dataset, where only an overview image is necessary to answer the questions-resulting in degraded performance due to the extra visual tokens. \n\n(3) The upper bound grows as more pretraining or larger model size. We also conduct \u03c8 optimal experiments on both of the other LLaVA variants for comparison. In this setup, the cropped sub-images encoded by LLaVA-NeXT are replaced with a single sub-image. As shown in the bottom section of Tab. 2, there remains a significant gap between the models' actual performance and their theoretical upper bound. \n\nOur observations indicate that, with expanded training data and an extended visual token length during training, the upper bound of the 7B-parameter LLaVA model increases by an average of 4.9%. This suggests that the upper bound of SEMCLIP is not static-it grows as the base VLM undergoes more training with additional data, even when the model size remains unchanged. \n\nSimilarly, LLaVA-1.5-13B",
            "score": 0.4060825091981768,
            "section_title": "Where is the upper bound of SEMCLIP?",
            "char_start_offset": 20070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1367
                },
                {
                    "start": 1370,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1773
                },
                {
                    "start": 1776,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2144
                },
                {
                    "start": 2147,
                    "end": 2171
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.409423828125
        },
        {
            "corpus_id": "277452282",
            "title": "XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?",
            "text": "In this section, we present examples of the VQA (Fig. 6), image captioning (Fig. 8 and Fig. 9), and visual grounding tasks (Fig. 7 ). What's more, we construct a detailed table (Tab. 9) analyzing model performance and error causes for each L-3 subtask. We then use examples to thoroughly illustrate the errors for each subtask. \n\nIn this section, we present a case study analysis of the error types made by LLaVa-Next, Qwen2-VL, and LLaVA-OneVision on various sub-tasks in XLRS-Bench. We classify the errors into the following 5 categories, following the MMT-Bench [67]: \n\nPerception Error : MLLMs often struggle to recognize, classify, or detect objects and content in images, Table 7. Experimental results of L-3 capability on the reasoning dimension of VQA tasks. Models are ranked according to their average performance. Rows corresponding to proprietary models are highlighted in gray for distinction. \"RP\", \"AD\", \"ECR\", \"CCR\" and \"RCCD\" each indicate a specific task domain: Route Planning, Anomaly Detection, Environmental Conditional Reasoning, Counting with Complex Reasoning and Regional Counting with Change Detection. Lack of Knowledge : MLLMs lack the domainspecific knowledge needed to answer specialized questions, such as identifying ship wake information in remote sensing images (see Fig. 21).",
            "score": 0.4060184318726312,
            "section_title": "A.6. Samples and Hard Cases of XLRS-Bench",
            "char_start_offset": 39590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 327
                },
                {
                    "start": 330,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1311
                }
            ],
            "ref_mentions": [
                {
                    "start": 565,
                    "end": 569,
                    "matchedPaperCorpusId": "269362969"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.161865234375
        },
        {
            "corpus_id": "266053531",
            "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
            "text": "General VLM benchmarks. In Tab. 1, we evaluate our method on common comprehensive Vision-Language benchmarks, MME [27] and MMBench [26]. Prompt Highlighter on LLaVA-v1.5 demonstrates a consistent performance improvement compared to well-trained models by designating the entire image as the highlighted part in the input context. Notably, though these benchmarks primarily assess overall performance with single-token generation and are not designed for user interactions, we still get a competitive place in both MMBench and MME perception. Additionally, further benchmarking with different hyper-parameter selection validates our performance enhancements with LLaVA-v1.5 13B, as evidenced by gains on POPE (85.9 -87.8, +1.9), MMB CN (63.4 -64.0, +0.6), and SQA I (71.6 -72.4, +0.8). With LLaVA-NeXT-34B [45], we achieve a SOTA performance on MMB test with 81.3.",
            "score": 0.4052953371931793,
            "section_title": "Quantitative Evaluation",
            "char_start_offset": 24730,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 863
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28515625
        },
        {
            "corpus_id": "270123045",
            "title": "Enhancing Large Vision Language Models with Self-Training on Image Comprehension",
            "text": "Table 1: Performance of STIC compared with the original LVLM model across vision-language reasoning tasks.For LLaVA-v1.5 (Vicuna 7B), we directly report the values in the paper of POVID, and \"-\" indicates an unreported value.We present our main results in Table 1 and detail the benchmark performances of STIC (LLaVA-v1.67B) on MMBench and MM-Vet in Figure 4.In Appendix A, we present detailed results for MMBench in Table 5 and MM-Vet in Table 6.Our results show a consistent and significant improvement of STIC over the original models (LLaVA-v1.5 and LLaVA-v1.6)across all seven datasets.This improvement is achieved using only self-constructed preference data and a small portion of the model's SFT dataset, which had already been used for fine-tuning the original model.On average, STIC improves LLaVA-v1.5 by 1.7%, increasing from 45.3% to 47.0%.and LLaVA-v1.6 by a notable score of 4.0%, increasing from 54.7% to 58.7%.The improvement is comprehensive, as detailed in Tables 5 and 6, where STIC consistently enhances performance across all evaluation tasks and targets.Moreover, while STIC improves both LLaVA-v1.5 and LLaVA-v1.6,a more significant improvement is observed in the more advanced model, LLaVA-v1.6.This trend suggests that the extent of self-improvement could be correlated with the model's inherent capabilities.",
            "score": 0.40360902500811513,
            "section_title": "Main Results",
            "char_start_offset": 22898,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 106,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 321
                },
                {
                    "start": 321,
                    "end": 359
                },
                {
                    "start": 359,
                    "end": 447
                },
                {
                    "start": 447,
                    "end": 565
                },
                {
                    "start": 565,
                    "end": 591
                },
                {
                    "start": 591,
                    "end": 775
                },
                {
                    "start": 775,
                    "end": 852
                },
                {
                    "start": 852,
                    "end": 926
                },
                {
                    "start": 926,
                    "end": 1076
                },
                {
                    "start": 1076,
                    "end": 1137
                },
                {
                    "start": 1137,
                    "end": 1219
                },
                {
                    "start": 1219,
                    "end": 1334
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.420654296875
        },
        {
            "corpus_id": "273507889",
            "title": "PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction",
            "text": "What is the year above the clock in the picture? \n\nThe year above the clock in the picture is 1856. \n\nWhat color is the girl on the left wearing in the picture? \n\nThe girl on the left is wearing a green dress. improvement, the model's performance remains comparable to the original on 16 different benchmarks. Notably, for fine-grained benchmarks like TextVQA, DocVQA, and OCRVQA, images contain a large amount of text and even documents, which request a dense and fine-grained under-standing of the image. Even in this case, our approach still maintain performance at the original level. This indicates that our method successfully compresses redundant information while preserving the most critical image content. \n\nIn the case of LLaVA-1.5, which processes fewer image tokens per sample, the acceleration is not as pronounced as with LLaVA-NeXT. However, it still offers a nearly 20% improvement in speed with comparable performance. This underscores the potential of our method to enhance training efficiency across different model configurations. Higher resolution at a lower cost. The PyramidDrop is proposed to reduce the redundancy within image tokens, and as we observed above, it enjoys higher speedup with the increase of the image/text token ratio. In this part, we explore its performance with higher image/text token ratio. In detail, LLaVA-NeXT is designed with a flexible image processing strategy in which an image is divided into a maximum of four local patches and a global patch, leading to at most 2880 image tokens. We denote it as LLaVA-NeXT-p5 and experiment on the LLaVA-NeXT-p9 by increasing the maximum local patches into 8 patches. \n\nAs shown in Table 4, with the increased image/text ratio, PyramidDrop reaches a higher speedup that only 269 GPU hours is used for training, which is only 55% of the vanilla LLaVA-Next-p9. Besides the superb speedup, the model trained with PyramidDrop achieves a slightly higher average performance across the 16 benchmarks. We argue too many image tokens with redundant information may confuse the LVLMs and hinder their performance, while our PyramidDrop efficiently reduce the image tokens number and helps the LVLM to focus on the critical information.",
            "score": 0.40201786905476633,
            "section_title": "Model",
            "char_start_offset": 21806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 51,
                    "end": 99
                },
                {
                    "start": 102,
                    "end": 160
                },
                {
                    "start": 163,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2218
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3564453125
        },
        {
            "corpus_id": "276409039",
            "title": "video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model",
            "text": "The main results on VideoMME, NExT-QA and the Riv-aBench are shown in Table 2. No subtitles are given to any of the models under test for VideoMME. As performance references, we include GPT-4o (checkpoint at 2024-08-06) and Gemini-1.5-pro, with their results on VideoMME as reported in Fu et al. (2024). When testing GPT-4o with videos, each video is split into images at a frame rate of 2 fps with a maximum of 30 frames due to token limitation, and the sequence of images is sent as the input. For open-source models, we compare video-SALMONN-o1 to LLaVA-OneVision (Li et al., 2024a) (same visual encoder and LLM backbone), together with video-SALMONN (Sun et al., 2024b) and Video-LLaMA 2 (Cheng et al., 2024) as the two most recent audio-visual LLMs. \n\nProprietary LLM performance on RivaBench: For the two proprietary LLMs, GPT-4o underperforms Gemini-1.5pro on StandUp and Academic test sets due to the lack of audio information. This indicates that RivaBench provides challenging questions that require more audio-visual joint understanding compared to VideoMME. On the Syn-thDec set, since only the visual part is synthesized, GPT-4o demonstrated a stronger ability. Moreover, by performing reasoning with GPT-4o and Gemini-1.5-pro, larger improvements are found on StandUp and Academic test sets than VideoMME and NExT-QA, indicating the necessity of reasoning on RivaBench. \n\nOpen-source LLM performance comparison: Audiovisual SFT on video-SALMONN-o1 already yields better performance than LLaVA-OneVision on VideoMME due to the ability to comprehend speech and audio information, whereas no obvious improvements are found on the other benchmarks. The main improvements on other benchmarks come from pDPO, which achieved 4.1%, 8.1% and 5.8% absolute accuracy improvements on NExT-QA, StandUp and Academic test sets respectively compared to the SFT model.",
            "score": 0.401746445585606,
            "section_title": "Main Results",
            "char_start_offset": 20694,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1865
                }
            ],
            "ref_mentions": [
                {
                    "start": 654,
                    "end": 673,
                    "matchedPaperCorpusId": "270703250"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2255859375
        },
        {
            "corpus_id": "270703034",
            "title": "VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models",
            "text": "We assess twelve LVLMs, comprising ten open-source models (7B unspecified), including VideoChat-GPT, Valley2, Video-LLaMA2, VideoChat2, Video-LLaVA, LLaMA-VID, VideoLaVIT, MiniGPT4-  [Luo et al., 2023] LLaMA2-7B -0.07 0.29 44.4 11.5 2.8 Video-LLaMA2 [Zhang et al., 2023b] LLaMA2-7B 0.36 0.84 90.9 12.7 10 VideoChat2 [Li et al., 2023c] Vicuna-7B-v0 -0.24 0.15 29.7 25.8 7.8 Video-LLaVA [Lin et al., 2023] Vicuna-7B-v1.5 0.36 0.91 95.1 20.3 17.8 LLaMA-VID [Li et al., 2023d] Vicuna-7B-v1.5 0.29 0.83 89.9 26.6 21 VideoLaVIT [Jin et al., 2024] LLaMA2-7B 0.36 0.91 94.9 21.3 18.9 MiniGPT4-Video [Ataallah et al., 2024] Mistral-7B Video, PLLaVA, and LLaVA-NeXT-Video-DPO, and two closed-source models, Gemini-1.5-Proand GPT-4o.To make a fair comparison, we set all these baselines following their original setting including the number of frames and generation hyper-parameters.",
            "score": 0.40032937287521475,
            "section_title": "Setups",
            "char_start_offset": 20233,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 711
                },
                {
                    "start": 711,
                    "end": 722
                },
                {
                    "start": 722,
                    "end": 872
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 271,
                    "matchedPaperCorpusId": "259075356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08538818359375
        },
        {
            "corpus_id": "260334037",
            "title": "Bridging the Gap: Exploring the Capabilities of Bridge-Architectures for Complex Visual Reasoning Tasks",
            "text": "We summarize the models that we considered in Table 3. FT stands for finetuning i.e the model was finetuned on NLVR2 and LP (Linear Probing) implies that we finetuned only a linear classifier layer on the embeddings of the pretrained model. We also evaluate the zero-shot capabilities of LLaVA on NLVR2. We provided the prompt \"Given two images side by side predict if the following statement is True or False in a single word\". This proved to be ineffective and the model accuracy was equivalent to that of random guessing. (Wei et al., 2022a) showed that using chain of thought prompting elicits reasoning in large language models. Since LlaVA was multimodally instruction finetuned we leverage the same for our model (LLaVA ZS + COT). CoT provided a small improvement in performance which demonstrates the dormant zero-shot multimodal capabilities of LLaVA. Next, we tried to bootstrap LLaVA by feeding it its own CoT reasoning (LLAVA ZS + COT + BS) in the same vein as (Huang et al., 2022). But unlike Huang et al. (2022), we dont finetune LLaVA using bootstrapping but evaluate it in the zero-shot setting. Although the performance is still worse than linear probe results of X-VLM and ViLT, our bootstrapping method provides a small boost in performance which demonstrates the zero-shot capabilities of LLaVA and gives us hope for bridge architectures. We provide some qualitative examples in Table 4 where we can see that LLaVA with CoT is able to reason out the answer and we also see an example where LLaVA with CoT bootstrapping is able to correct its own answer. We believe LLaVA would perform better if we make it reason step by step in conversational mode as done by Wei et al. (2022a). This is an aspect we will explore in our future work. \n\nThe first half of the table summarizes state-ofthe-art multimodal models, we observe that these models perform well on the NLVR2 task as can be seen from Table 3. Notably, even with just a linear probe we achieve significant improvements in performance.",
            "score": 0.3984454342015643,
            "section_title": "Analysis",
            "char_start_offset": 22209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 2009
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27880859375
        },
        {
            "corpus_id": "273025917",
            "title": "Toward a Holistic Evaluation of Robustness in CLIP Models",
            "text": "However, in row 2, when the category list is challenging for CLIP (SigLIP-SO-14), LLaVA provides over a 20% improvement across three splits. The same trend is observed in row 3, where CLIP (ViT-L/14-336) serves as the category selection network for CLIP ViT-L/14-336 (WIT) and the corresponding LLaVA. Since LLaVA and CLIP use the same visual encoder, we speculate that LLaVA's language model excels when CLIP's text and visual tokens are difficult to align for classification. Conversely, when CLIP handles the token comparison easily, LLaVA's language model may over-extract information from visual tokens, leading to a performance drop. \n\nSecond, the choice of language model (LLM) in LLaVA has a significant impact on classification accuracy. For instance, Mistral-Instruct-V2 consistently underperforms compared to the other LLMs, while Vicuna-V2-7B generally provides the best results. Additionally, the choice of visual encoder is equally important: LLaVA models with SigLIP-SO-14 consistently outperform those using ViT-L/14-336, aligning with recent research [61], [95], [96]. \n\nThese findings suggest that analyzing the visual encoder or the LLM in isolation does not fully explain LLaVA's performance in image classification. The interaction between these components is crucial and represents a promising area for further research.",
            "score": 0.3981258610261802,
            "section_title": "IX. VISUAL AND LANGUAGE ENCODER INTERACTION: A CLASSIFICATION PERSPECTIVE",
            "char_start_offset": 45510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1342
                }
            ],
            "ref_mentions": [
                {
                    "start": 1068,
                    "end": 1072,
                    "matchedPaperCorpusId": "267627175"
                },
                {
                    "start": 1080,
                    "end": 1084,
                    "matchedPaperCorpusId": "266976992"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.288818359375
        },
        {
            "corpus_id": "269043145",
            "title": "Heron-Bench: A Benchmark for Evaluating Vision Language Models in Japanese",
            "text": "For model evaluation, in addition to the proposed Heron-Bench, we also use LLaVA-Bench (COCO) and LLaVA-Bench (In-the-Wild), which were translated into Japanese using DeepL and manually modulated.We evaluate both open VLMs that are publicly available and closed VLMs that can be accessed via APIs.The following models were evaluated:\n\nClosed: GPT-4V [1,2], Claude 3 Opus [34], Gemini Pro Vision [35] Open: Heron GIT (proposed in this paper), Heron BLIP v1, LLaVA-1.6 [11], LLaVA-1.5 [10], Qwen-VL [12], Japanese Stable VLM [36], EvoVLM-JP[37]",
            "score": 0.39782845129281297,
            "section_title": "Evaluation Benchmarks",
            "char_start_offset": 7985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 196,
                    "end": 297
                },
                {
                    "start": 297,
                    "end": 333
                },
                {
                    "start": 335,
                    "end": 542
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.215576171875
        },
        {
            "corpus_id": "272146052",
            "title": "LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models",
            "text": "The comparison results of LLaVA-SG and baseline models are summarized in Table I following the evaluation metrics of [30]. The reported results of the compared models are collected from the corresponding papers. Analyzing the experimental results in Table I, it is evident that our LLaVA-SG model shows significant and consistent improvements over   I. MMBench assesses large vision-language models across multiple capability dimensions including LR for Logical Reasoning, AR for Attribute Reasoning, RR for Relation Reasoning, FP-S for Fine-grained Perception (Single Instance), FP-C for Fine-grained Perception (Cross Instance) and CP for Coarse Perception. Analyzing the comparative results in this table, our LLaVA-SG shows significant improvements over LLaVA-1.5 in the capabilities that require entity perception and relationship analysis, specifically in RR, FP-S, FP-C, and CP. \n\nWe present example outputs of LLaVA-1.5 and LLaVA-SG in Fig. 3. The middle column shows the masks of entities included in the scene graph of the SGE module. With the visual semantic information expressed by SGE, LLaVA-SG exhibits enhanced multimodal capabilities. For example, in the first case shown in Fig. 3, without explicitly preserving the visual semantic information in the image, the counting problem becomes difficult for LLaVA. Relying solely on the fragmented visual tokens output by ViT makes it challenging to accurately determine the number of dogs in the image. However, equipped with the SGE module, our LLaVA-SG can leverage tokens that explicitly represent entities in the image, enabling it to provide an accurate count of the dogs.",
            "score": 0.3975910123550702,
            "section_title": "B. Overall Performance Assessments",
            "char_start_offset": 10571,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 885
                },
                {
                    "start": 888,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1639
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58056640625
        },
        {
            "corpus_id": "277452239",
            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
            "text": "In Fig. 2, we present the benchmark evaluation results for the base LLaVA-OneVision models (0.5B, 7B, 72B), shown in blue histograms, compared against alternative VLM models: TinyLLaVA 3.1B (green histogram), Qwen2VL models (2B, 7B, 72B) (red histograms), and InternVL models (2B, 8B) (orange histograms). For each benchmark, we reported the classification F1-score for individual classes, as well as the average F1-score across all classes (labeled as 'AVG' in the plots). As expected, smaller models (0.5B-3.1 B) perform consistently worse across most benchmarks, while larger models (Qwen2VL 72B, InternVL 8B, and LLaVA 72B) tend to achieve the best performance, particularly in B3 (radio galaxy detection), B4 (artifact detection), and B6 (FR-I vs. FR-II classification). In B1 (extended/diffuse source detection) and B2 (morphology classification), performance remains generally low across all models, with no significant advantage for any specific one. The best results are observed in B3 and B4, where LLaVA 7B/72B models achieve competitive or slightly better performance compared to recently released VLMs. For instance, in artifact detection (B4), they attain a respectable 50-60% F1-score in a zero-shot setting. B5 (peculiar/complex morphology classification) and B6 (FR-I vs. FR-II classification) present significant challenges for all models, including the largest ones. Overall, the results indicate poor performance across all benchmarks, underscoring the need for models specialized in astronomical data.",
            "score": 0.3970419109985262,
            "section_title": "Zero-shot performance",
            "char_start_offset": 30000,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1522
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.346435546875
        },
        {
            "corpus_id": "263672058",
            "title": "Improved Baselines with Visual Instruction Tuning",
            "text": "We also incorporate ShareGPT [46] data and scale up the LLM to 13B as in [3,8,39]. Results on MM-Vet shows the most significant improvement when scaling the LLM to 13B, suggesting the importance of the base LLM's capability for visual conversations. \n\nLLaVA-1.5. We denote this final model with all the modifications as LLaVA-1.5 (the last two rows in Table 2), which achieves an impressive performance that significantly outperforms the original LLaVA [36]. \n\nComputational cost. For LLaVA-1.5, we use the same pretraining dataset, and keep the training iterations and batch size roughly the same for instruction tuning as LLaVA [36]. \n\nDue to the increased image input resolution to 336 2 , the training of LLaVA-1.5 is \u223c2\u00d7 as long as LLaVA: \u223c6 hours of pretraining and \u223c20 hours of visual instruction tuning, using 8\u00d7 A100s.",
            "score": 0.3942626858250983,
            "section_title": "Scaling the Data and Model",
            "char_start_offset": 13081,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 249
                },
                {
                    "start": 252,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 827
                }
            ],
            "ref_mentions": [
                {
                    "start": 453,
                    "end": 457,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 630,
                    "end": 634,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53369140625
        },
        {
            "corpus_id": "278338871",
            "title": "Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading",
            "text": "We present two generative models, DalEye-Llama and DalEye-LLaVA, each fine-tuned with distinct methods and hyperparameters optimized for their specific architectures and objectives. \n\nDalEye-Llama This model is fine-tuned using Unsloth with the Meta-Llama-3.1-8B backbone loaded in 4-bit precision. Training employs Low-Rank Adaptation (LoRA) with rank r = 16, scaling factor \u03b1 = 16, and applies RS-LoRA regularization. LoRA targets transformer modules: q_proj, k_proj, v_proj, up_proj, down_proj, o_proj, and gate_proj. The model is trained for 2 epochs with a linear scheduler and warm-up of 10 steps, batch size of 1, gradient accumulation over 2 steps, AdamW-8bit optimizer with learning rate 1 \u00d7 10 \u22124 , and weight decay of 0.01. \n\nDalEye-LLaVA This model employs a teacherforcing strategy. Specifically, loss computation is restricted exclusively to tokens corresponding to possible questions, contrasting the likelihood of generating the correct question against two distractors. The computed losses for each question candidate are inverted, serving as logits for softmax normalization and subsequently optimized via crossentropy against the true label. The LLaVA backbone remains frozen, and training employs LoRA with dropout rate of 0.1, RS-LoRA, and rank r = 8. The hyperparameter search includes learning rates of 1 \u00d7 10 \u22125 , 3 \u00d7 10 \u22125 , 1 \u00d7 10 \u22124 , fixation embedding hidden sizes of 512, 1024, and early stopping after 8 epochs without validation improvement. \n\nAll neural network-based models were trained using the PyTorch Lightning (Falcon and team, 2024) library on NVIDIA A100-40GB and L40S-48GB GPUs. \n\nWe utilize the Hugging Face implementation of LLaVA-OneVision, specifically LLaVA-hf/LLaVAonevision-qwen2-0.5b-si-hf. Additionally, we employ the gpt-4o-2024-08-06 version of GPT-4o.",
            "score": 0.3939843900141019,
            "section_title": "A.2 Generative Models",
            "char_start_offset": 34703,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 184,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1805
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1602783203125
        },
        {
            "corpus_id": "266374969",
            "title": "Exploring Multimodal Large Language Models for Radiology Report Error-checking",
            "text": "LLaVA-Med is one of the first VLMs in the biomedical domain (21). LLaVA-Med is initialized with LLaVA-0 and then continuously trained with a comprehensive dataset of biomedical figure-caption pairs sourced from PubMed Central. Only the 7B model is available publicly. \n\nLLaVA-1.5: LLaVA-1.5 is a general domain VLM that uses the LLaMA2 model, which has a significant improvement in language understanding when compared with LLaMA, as the backbone LLM (18,20). There are two significant improvements besides the change of the backbone LLM. \n\nFirstly, the addition of an MLP vision-language connector enhanced the system's capabilities. \n\nSecondly, the integration of academic task-oriented data further enhanced its performance and effectiveness. LLaVA-1.5 is available in 2 model sizes, 7B and 13B models, and we used both models.",
            "score": 0.3930550077602005,
            "section_title": "LLaVA-Med:",
            "char_start_offset": 11020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 267
                },
                {
                    "start": 270,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 830
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56103515625
        },
        {
            "corpus_id": "275975310",
            "title": "Learning to Inference Adaptively for Multimodal Large Language Models",
            "text": "MLLMs face a major challenge in deployment due to their high computational costs during inference. Several works have designed lightweight model architectures to reduce the costs. Examples include Phi-2 [23], TinyGPT-V [63] and LLaVA-\u03d5 [68]. Vary-toy [58] enhances performance through specialized vision vocabulary in smaller models. TinyLLaVA [65] and LLaVA-OneVision [30] learn smallscale models with curated training data and pipeline. MoE-LLaVA [34] and LLaVA-MoD [50] improve efficiency by incorporating mixture-of-experts architectures and parameter sparsity techniques. Recent works also investigate input token selection, as an input image or video can produce a large number of vision tokens. MADTP [6] and LLaVA-PruMerge [49] introduce token pruning and merging technique to reduce the tokens counts. Recently, Pham et al. [45] propose to selectively disabling attention mechanisms for visual tokens in MLLMs. \n\nWhile our approach also aims to improve the efficiency of MLLMs, it focuses on dynamically adjusting an MLLM to fit varying latency budgets during inference. This makes our approach orthogonal to prior efforts on developing inherently more efficient MLLMs. Through our experiments, we will demonstrate that our approach is compatible with lightweight models and integrates seamlessly with existing token-pruning techniques (e.g., [8,49]).",
            "score": 0.3930292223073426,
            "section_title": "Related Work",
            "char_start_offset": 6777,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 919
                },
                {
                    "start": 922,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1360
                }
            ],
            "ref_mentions": [
                {
                    "start": 708,
                    "end": 711,
                    "matchedPaperCorpusId": "268248344"
                },
                {
                    "start": 1352,
                    "end": 1355,
                    "matchedPaperCorpusId": "268358224"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41015625
        },
        {
            "corpus_id": "277621021",
            "title": "InstructionBench: An Instructional Video Understanding Benchmark",
            "text": "As illustrated in Increasing the number of input frames significantly enhances the performance of closed-source models. GPT-4o shows a 4.76% improvement when we input 16 frames instead of 8 frames to it. GPT-4V and Gemini Pro Vision also get better results when the input video frames increase. \n\nHowever, for open-source models, increasing frame input offers little to no benefit and can even decrease performance, especially for those lacking temporal and spatial modeling. For example, LLaVA-NeXT-Video sees a 1.5% overall performance drop in our InstructionBench when the input video frames increase from 8 to 16. Despite processing more frames at 1 frame per second, LLaMA-VID underperforms compared to 8-frame Video-LLaVA, which uses Language-Bind's (Zhu et al. 2023) video encoder for better temporal and spatial attention integration. Furthermore, LLaMA-VID compresses one video frame to just 1-2 tokens, resulting in significant information loss of video frames. This approach leads to LLaMA-VID achieving the lowest results in Fine-Grained tasks on our InstructionBench. These findings suggest that existing Video-LLMs must improve their ability to process multiple frames to fully capture video information.",
            "score": 0.3928291462101442,
            "section_title": "Overall Results",
            "char_start_offset": 17257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1218
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1727294921875
        },
        {
            "corpus_id": "276928973",
            "title": "Painting with Words: Elevating Detailed Image Captioning with Benchmark and Alignment Learning",
            "text": "(Anthropic., 2024), Gemini-1.5-Pro (Team et al., 2023), GPT-4v (OpenAI., 2024b), GPT-4o (OpenAI., 2024a), Cambrian-34B (Tong et al., 2024), VILA-40B (Lin et al., 2024), XComposer-2.5-7B (Zhang et al., 2024), and InternVL-2-8B/26B (Chen et al., 2024b). \n\nFEEDQUILL-7B achieves state-of-the-art performance in detailed image captioning, surpassing GPT-4o by 2.21 points. Remarkably, it also outperforms GPT-4v on LLaVA-W, showing strong capability in visual chatting. Despite being trained solely on the captioning task, our model maintains its strong performance while achieving a 1.8-point improvement on MMVet and a 0.7-point increase on MMStar compared to LLaVA-Onevision-7B. Additionally, it retains most of its capabilities after preference optimization -a feat that many aligned models, such as BHDS (Amirloo et al., 2024), CSR (Zhou et al., 2024b), and RLAIF-V (Yu et al., 2024b), fail to accomplish.",
            "score": 0.3925713619015251,
            "section_title": "ABLATIONS",
            "char_start_offset": 28293,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 251
                },
                {
                    "start": 254,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 906
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 167,
                    "matchedPaperCorpusId": "266174746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.086181640625
        },
        {
            "corpus_id": "276408432",
            "title": "USER-VLM 360: Personalized Vision Language Models with User-aware Tuning for Social Human-Robot Interactions",
            "text": "User-aware Personalization As demonstrated in Table 1, the User-VLM 360\u00b0, in both its 3B and 10B sizes, consistently outperforms baseline models across both benchmarks. On the ElderlyTech-VQA benchmark, User-VLM 10B achieves an impressive 2x improvement in ROUGE-1 F1 score compared to the baseline, while the 3B variant performs approximately 1.5x better. A detailed comparison of baseline models on this benchmark, ranked by ROUGE-1 F1 score, reveals the following order: LLaMA 3.2 11B, LLaVA 1.5 7B, Pixtral 12B, and LLaVA 1.6 7B. Similarly, on the User-VQA benchmark, User-VLM 3B outperforms the baselines by 1.2x, while the 10B variant achieves a 1.3x improvement. When ranking baselines on this benchmark by ROUGE-1 F1 score, LLaVA 1.5 leads, followed by LLaVA 1.6, LLaMA 3.2, and Pixtral. These results underscore the efficacy of User-VLM 360\u00b0in addressing the challenges of these tasks and its superior performance across varying model sizes. General Purpose Understanding Despite the primary focus of training on human user images, which could lead to concerns about catastrophic forgetting and reduced performance on general-purpose tasks, User-VLM 360\u00b0demonstrates robust generalization capabilities. As summarized in Table 2, the model achieves competitive results across four widely adopted general-purpose benchmarks. Specifically, the 3B and 10B variants outperform the baseline on the VQAv2 benchmark, indicating strong visual questionanswering capabilities. On the COCO benchmark, the model performs comparably, with a minimal 0.16-point difference from the top-performing model, LLaVA 1.5. Similarly, on the \"in the wild\" benchmark, the model shows a negligible 0.04-point gap from LLaVA 1.6, highlighting its adaptability to diverse, unstructured data. However, the model exhibits limited performance on the SEED benchmark, suggesting room for improvement in specific scenarios.",
            "score": 0.39130573447631406,
            "section_title": "Comparative Analysis",
            "char_start_offset": 23496,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1897
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39208984375
        },
        {
            "corpus_id": "268531413",
            "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
            "text": "Comprehensive experiments on 9 benchmarks show that LLaVA-UHD significantly improves the capabilities of LMMs, outperforming established counterparts trained with 2-3 orders of magnitude more data.Notably, our model built on LLaVA-1.5 336\u00d7336 supports 672\u00d71088 resolution images using only 94% inference computation, and achieves 6.4 accuracy improvement on TextVQA and 3.2 accuracy improvement on POPE.The advantage enlarges with more extreme aspect ratios.We also show that instruction tuning on ViT parameters is sufficient for adaptation to a broad range of images.Moreover, the model can be efficiently trained in academic settings, within 23 hours (vs.26 hours of LLaVA-1.5) on 8 A100 GPUs.\n\nThe contribution of this work can be summarized as threefold: (1) We perform the first mechanistic investigation of GPT-4V from the perspective of visual encoding strategy and expose systematic flaws.(2) We present LLaVA-UHD, a large multimodal model that can efficiently perceive any aspect ratio and high-resolution images.(3) We conduct comprehensive experiments to demonstrate the effectiveness of LLaVA-UHD on 9 popular benchmarks, and also provide analysis for deeper understanding of the model.",
            "score": 0.390938727181972,
            "section_title": "Introduction",
            "char_start_offset": 3866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 458
                },
                {
                    "start": 458,
                    "end": 569
                },
                {
                    "start": 569,
                    "end": 658
                },
                {
                    "start": 658,
                    "end": 696
                },
                {
                    "start": 698,
                    "end": 898
                },
                {
                    "start": 898,
                    "end": 1023
                },
                {
                    "start": 1023,
                    "end": 1199
                }
            ],
            "ref_mentions": [
                {
                    "start": 1023,
                    "end": 1026,
                    "matchedPaperCorpusId": "248476411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.355712890625
        },
        {
            "corpus_id": "266693714",
            "title": "Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models",
            "text": "While LLaVA-1.5 has shown promise in various aspects, it's important to acknowledge the limitations associated with this model: \n\n\u2022 LLaVA employs full image patches, potentially extending training iterations, with current visual resamplers unable to match its efficiency due to differences in trainable parameters [9]. \u2022 The model currently lacks the ability to process multiple images, limited by the available instruction-following data and context length [9]. \u2022 Despite reduced hallucination tendencies, LLaVA still has the potential to produce hallucinations and misinformation, mandating cautious use in critical applications [9]. In spite of these limitations, the achievements of LLaVA-1.5 show the extraordinary potential of multimodal models in the realm of visual reasoning and instruction-following tasks. Our experiments clearly demonstrate its notable accomplishments in zero-shot classification, charting a promising course for future research and innovation. With fine-tuning, the model shows even greater promise to be effective in different critical domains. While researchers work to mitigate its limitations, LLaVA-1.5 remains a guiding light of progress, providing invaluable insights and an easily reproducible framework to advance the frontiers of multimodal AI and elevate its practical utility.",
            "score": 0.3904157507118883,
            "section_title": "B. Conclusion",
            "char_start_offset": 14038,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 130,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1318
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6572265625
        },
        {
            "corpus_id": "273233509",
            "title": "MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models",
            "text": "For simplicity, all our experiments used five retrieved or ground-truth image examples. However, it is worth exploring how many examples LVLMs can effectively leverage. As noted in \u00a7 2.3, the perspective aspect of our benchmark includes an average of 20.4 ground-truth examples. To investigate further, we perform an analysis focusing on the perspective and others aspects, covering a total of 892 questions. As shown in Figure 5, we evaluated LLaVA-Next-Interleave using 1, 2, 3, 5, 10, 20 GT examples, averaging the results across three random seeds for sampling the GT examples. LLaVA-Next-Interleave saw the greatest improvement of 5.64% with just one GT example. Performance continued to increase steadily, reaching a peak at 10 GT examples, which was 0.29% higher than with 20 GT examples. One possible explanation could be LLaVA-Next-Interleave may not able to better leverage visually augmented knowledge in long context scenarios. Moreover, the complexity of questions affects the number of images needed too, one ground-truth example sometimes help the model the most on MRAG-BENCH. We encourage the research on adaptatively deciding the number of necessary images based on the complexity of questions.",
            "score": 0.3897954905616031,
            "section_title": "HOW MANY GROUND-TRUTH IMAGE EXAMPLES ARE NEEDED?",
            "char_start_offset": 19580,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1212
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2841796875
        },
        {
            "corpus_id": "270560503",
            "title": "Reminding Multimodal Large Language Models of Object-aware Knowledge with Retrieved Tags",
            "text": "In this section, we first present the training details of TUNA and benchmarks.Then we introduce quantitative and qualitative comparison with popular open-source models, followed by detailed analysis experiments and ablation studies.\n\nTraining Details.TUNA is finetuned on instruction data for one epoch, following existing works (Liu et al., 2023a;Chen et al., 2023c).\n\nWe consider two different instruction-following datasets in our experiments: LLaVA-665K (Liu et al., 2023a)  As the size of LLM and different choices of instruction-following data can significantly improve the model performance, we mark the models gray that are equipped with a larger 13B language model or finetuned from currently unavailable datasets of higher quality and quantity.Specifically, LLaVA-1.6 (or LLaVA-NeXT)1 is finetuned from larger instruction-following data of higher quality, with additional user instruct data.Besides, it equips the better vision encoder with dynamic high resolution, known as AnyRes (AR).\n\nAlthough it is not a fair comparison, we still outperform LLaVA-1.6 in MMB CN , MMB and POPE, and the corresponding 13B models in MMB CN , MMB, POPE and LLaVA-W.\n\nHow Can TUNA Improve the Recognition of Novel Objects and Entities?As visualized in Fig. 2 (Top), with our 15M large-scale datastore, the new retrieval mapping could greatly compensate for the original LLaVA multimodal connector that learns from around 1M data.With the additional mappings from retrieval data, TUNA is expected to show particularly improvements over questions towards novel objects or entities in the given input image.We show sub-tasks from MME (Fu et al., 2023) and MMB (Liu et al., 2023b) that consists of such questions in Tab.Table 2: Comparison with SoTA methods on 12 benchmarks.Our model achieves the best performance on 12 benchmarks compared with LLMs that are finetuned from the same instruction tuning (IT) datasets with the same configuration on the vision encoder (V-Enc.)and language model (Vicuna-7B).Best results are in bold.",
            "score": 0.38954293221372593,
            "section_title": "Experiment",
            "char_start_offset": 16187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 78,
                    "end": 232
                },
                {
                    "start": 234,
                    "end": 251
                },
                {
                    "start": 251,
                    "end": 368
                },
                {
                    "start": 370,
                    "end": 754
                },
                {
                    "start": 754,
                    "end": 901
                },
                {
                    "start": 901,
                    "end": 997
                },
                {
                    "start": 999,
                    "end": 1160
                },
                {
                    "start": 1162,
                    "end": 1229
                },
                {
                    "start": 1229,
                    "end": 1423
                },
                {
                    "start": 1423,
                    "end": 1598
                },
                {
                    "start": 1598,
                    "end": 1710
                },
                {
                    "start": 1710,
                    "end": 1765
                },
                {
                    "start": 1765,
                    "end": 1965
                },
                {
                    "start": 1965,
                    "end": 1996
                },
                {
                    "start": 1996,
                    "end": 2021
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26123046875
        },
        {
            "corpus_id": "272689854",
            "title": "TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings",
            "text": "Visual language models primarily consist of a visual encoder and a large language model, representing prominent architectures in the multimodal domain. Researchers have proposed numerous architectures (Li et al. 2023a;Zhu et al. 2024;Chen et al. 2023c) for integrating visual features into advanced LLM inference pipelines. Llama-Adapter (Zhang et al. 2023) proposes to generate language answer with taking the image input as condition. Flamingo (Alayrac et al. 2022) and LLaVA (Liu et al. 2024c)  Inspired by the compact structure and outstanding performance of LLaVA-1.5 (Liu et al. 2024a), we use LLaVA-1.5 as our baseline and incorporate a text-guided approach, similar to other LLaVA-based methods. Unlike most of these methods, which create additional datasets to enhance performance, our improvements focus entirely on the model architecture itself. This approach can further enhance the performance of methods that rely on extra datasets. The results in fifth and sixth lines of Table 1 verified this point.",
            "score": 0.38931046379188206,
            "section_title": "Related Work Vision Language Models",
            "char_start_offset": 5135,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1015
                }
            ],
            "ref_mentions": [
                {
                    "start": 446,
                    "end": 467,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 478,
                    "end": 496,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38427734375
        },
        {
            "corpus_id": "270559998",
            "title": "Unveiling the Ignorance of MLLMs: Seeing Clearly, Answering Incorrectly",
            "text": "Experiment results on the MMVU benchmark. To evaluate our proposed dataset, we fine-tune several state-of-the-art MLLMs, including LLaVA-OneVision-7B [55], InternVL2-8B [15], VILA1.5-13B [59], and Phi3.5-Vision-Instruct [1], using MMVU training set (Sec. 3.5) and assess their performance on the MMVU benchmark. Weight merging is utilized in the experiment. As shown in Tab. 2, models fine-tuned with our dataset consistently outperformed their baselines across most metrics. Notably, LLaVA-OneVision-7B + MMVU-Train demonstrated improvements in character/number recognition (68.29% vs. 62.20%), numerical understanding (55.56% vs. 41.98%), and activity recognition (80.00% vs. 75.56%). These results indicate that the paired positive-negative data can reduce biased responses. Comparison of instruction tuning datasets generated by GPT. We compare the MMVU-Train dataset with previous GPT-4V generated instruction tuning datasets, such as LLaVA 158k [56], SVIT 158k [89], and LRV [53] on the MMVU benchmark (as shown in Tab. 3). The results demon-strate that our dataset enables the model to outperform the model trained with previous datasets.",
            "score": 0.3893098871496629,
            "section_title": "Analysis of MMVU Training Set",
            "char_start_offset": 23273,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1145
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 154,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 951,
                    "end": 955,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 981,
                    "end": 985,
                    "matchedPaperCorpusId": "259251834"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.243408203125
        },
        {
            "corpus_id": "270045796",
            "title": "LOVA3: Learning to Visual Question Answering, Asking and Assessment",
            "text": "Implementation Details. To ensure a fair comparison, we train the LOVA 3 -7B model without tuning any hyperparameters of LLaVA-1.5 [42] from its original supervised finetuing stage. The model is trained for one epoch across three tasks: VQA, GenQA, and EvalQA. Specifically, we employ the AdamW [46] optimizer with a learning rate of 2 \u00d7 10 \u22125 and a total batch size of 128. The training process takes 24.5 hours on an 8 Nvidia A100 (40G) GPU setup. Moreover, we also replace the LLM from Vicuna-7B [15] to Phi-1.5B [41] to evaluate smaller LLMs. We train LLaVA-Phi-1.5 and LOVA 3 -1.5B by using the same training recipe. The only difference of training with Phi-1.5 is that we increase the learning rate from 2 \u00d7 10 \u22125 to 4 \u00d7 10 \u22125 to ensure a higher performance. The model LLaVA-Phi-1.5 is trained with the original 665K VQA instruction data as the baseline. The model LOVA 3 -1.5B is trained with our proposed 1.5M mixture data including VQA, GenQA, EvalQA data. \n\nTable 3: Results on five generic tasks including VQAv2 [26], GQA [30], VizWiz [27], ScienceQA [50], and POPE [39]. The first two columns represent the results on held-in datasets marked as * , and the last three columns represent the held-out datasets. The best result on each subtask is bolded. MM-Vet. In Tab. 5, we compare LOVA 3 -7B with other approaches on MM-Vet, which is a challenging benchmark including numerous complex VQA samples that demand integration of several multimodal capabilities for answering. As illustrated in Tab. 5, the results show that our LOVA 3 -7B outperforms LLaVA-1.5 by 4.0% at an average. Such improvement demonstrates the effectiveness of LOVA 3 -7B in solving these challenging multimodal questions.",
            "score": 0.3892116967390029,
            "section_title": "Datasets and Settings",
            "char_start_offset": 20496,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1704
                }
            ],
            "ref_mentions": [
                {
                    "start": 295,
                    "end": 299,
                    "matchedPaperCorpusId": "53592270"
                },
                {
                    "start": 1023,
                    "end": 1027,
                    "matchedPaperCorpusId": "8081284"
                },
                {
                    "start": 1033,
                    "end": 1037,
                    "matchedPaperCorpusId": "152282269"
                },
                {
                    "start": 1046,
                    "end": 1050,
                    "matchedPaperCorpusId": "3831582"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.314208984375
        },
        {
            "corpus_id": "270258328",
            "title": "Hear Me, See Me, Understand Me: Audio-Visual Autism Behavior Recognition",
            "text": "We conducted further fine-tuning of LLaVA under four settings: vision only, audiovisual, visual-speech, and audio-visual-speech, to evaluate how different modalities contribute to the recognition performance (see Table IV).(1) A noteworthy observation is that although LLaVA's visual encoder is identical to CLIP's, the V-only model's performance (53.45%) significantly surpasses that of the original CLIP equipped with a linear probe (45.03%).This improvement indicates that the LLM component in LLaVA effectively boosts the perception encoder's classifica- Please explain why we can infer those behaviors based on the given information.",
            "score": 0.38914270516102867,
            "section_title": "Table II compares the recognition performance of various baselines. Table III provides the ImageBind + Whisper model's performance with different fusion strategies. Table IV provides",
            "char_start_offset": 25367,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 223,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 638
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2274169921875
        },
        {
            "corpus_id": "274514911",
            "title": "Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion",
            "text": "For baseline comparisons among small models, we chose Vila 1.5 3B [24] and Phi 3.5-Vision-Instruct [1]. For the larger models, we select the baselines: LLaVA Next 8B [25], Vila 8B [24], Mini-Gemini-HD 8B [23] and Cambrain 8B [41], using LLama 3 8B Instruct as the language backbone. \n\nResults. In the Table 2, we present the results of Florence-VL compared to various baselines across a range of benchmarks, along with the number of visual tokens used. For the smaller-sized model, our model outperforms Vila 3B, and surpasses Phi 3.5 Vision on 12 out of 24 tasks. Notably, Phi 3.5 Vision utilizes 500 billion vision and text tokens [1], with its training data being proprietary and significantly larger than ours. Nonetheless, our Florence-VL 3B remains competitive with this model. For the larger-sized model, our model shows a significant improvement over other baselines on most benchmarks. Notably, our model significantly outperforms Cambrain-8B, which utilizes multiple vision encoders and combines their image features, whereas we achieve superior results using only a single vision encoder.",
            "score": 0.38882726628206576,
            "section_title": "Experiments",
            "char_start_offset": 14871,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1099
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 70,
                    "matchedPaperCorpusId": "266174746"
                },
                {
                    "start": 180,
                    "end": 184,
                    "matchedPaperCorpusId": "266174746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29736328125
        },
        {
            "corpus_id": "274150456",
            "title": "VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation",
            "text": "LMMs with advanced video understanding capabilities have garnered significant research attention. For the closed-source models, GPT-4o [26] and Google's Gemini-1.5 [2,51] demonstrate SOTA video analysis performance. \n\nMeanwhile, the open-source community has made notable strides [7, 9, 12, 13, 19, 21, 25, 27, 29, 34, 38, 39, 41, 42, 44, 46, 48, 52, 63-67, 69, 71]. Notably, the LLaVa series [40] has been updated to the LLaVa-Video [73] and LLaVa-OneVision models [30], along with the release of all training data. The Qwen-VL model [3] has also been upgraded to the Qwen2-VL version [57], and the first open-source multimodal mixture-of-experts (MoE) model, Aria [31], has recently been introduced. These contributions have significantly narrowed the gap between closed-source and opensource models in video understanding. To accelerate the development of LMMs in video analysis, the establishment of more comprehensive benchmarks is essential. In the early phase, researchers primarily relied on benchmarks featuring short videos [11,22,23,50], such as MSVD-QA [62], MSRVTT-QA [62], NExT-QA [60], and MVBench [35]. However, these benchmarks have limitations due to their short video durations, averaging less than 50 seconds. This brevity restricts their ability to comprehensively evaluate the temporal understanding capabilities of LMMs, thereby hindering further advancements in LMMs development. To address these limitations, benchmarks like ActivityNet-QA [68] and EgoSchema [49] have extended video durations to approximately 180 seconds on average. More recently, research has introduced even more comprehensive benchmarks [43,58,70]. For instance, MovieChat-1K [53] assesses LMMs us-ing movie videos with an average duration of 500 seconds, while LongVideoBench [59] focuses on long-context interleaved evaluation with an average duration of 473 seconds.",
            "score": 0.38820327492595663,
            "section_title": "Related Work",
            "char_start_offset": 8139,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 215
                },
                {
                    "start": 218,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1866
                }
            ],
            "ref_mentions": [
                {
                    "start": 393,
                    "end": 397,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1038,
                    "end": 1041,
                    "matchedPaperCorpusId": "834612"
                },
                {
                    "start": 1041,
                    "end": 1044,
                    "matchedPaperCorpusId": "232417303"
                },
                {
                    "start": 1044,
                    "end": 1047,
                    "matchedPaperCorpusId": "258840892"
                },
                {
                    "start": 1065,
                    "end": 1069,
                    "matchedPaperCorpusId": "3864050"
                },
                {
                    "start": 1081,
                    "end": 1085,
                    "matchedPaperCorpusId": "3864050"
                },
                {
                    "start": 1095,
                    "end": 1099,
                    "matchedPaperCorpusId": "234763093"
                },
                {
                    "start": 1113,
                    "end": 1117,
                    "matchedPaperCorpusId": "265466214"
                },
                {
                    "start": 1465,
                    "end": 1469,
                    "matchedPaperCorpusId": "69645185"
                },
                {
                    "start": 1484,
                    "end": 1488,
                    "matchedPaperCorpusId": "261031047"
                },
                {
                    "start": 1634,
                    "end": 1638,
                    "matchedPaperCorpusId": "268201547"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.442626953125
        },
        {
            "corpus_id": "266174746",
            "title": "VILA: On Pre-training for Visual Language Models",
            "text": "visual language tasks. We perform a comprehensive comparison with state-of-the-art models on 12 visual language benchmarks in Table 5 Table 5. Comparison with state-of-the-art methods on 12 visual-language benchmarks. Our models consistently outperform LLaVA-1.5 under a head-to-head comparison, using the same prompts and the same base LLM (Vicuna-1.5 is based on Llama-2), showing the effectiveness of visual-language pre-training. We mark the best performance bold and the second-best underlined. Benchmark names are abbreviated due to space limits. VQA-v2 [25]; GQA [29]; VisWiz [26]; SQA I : ScienceQA-IMG [41]; VQA T : TextVQA [55]; POPE [36]; MME [24]; MMB: MMBench [40]; MMB CN : MMBench-Chinese [40]; SEED: SEED-Bench [33]; LLaVA W : LLaVA-Bench (In-the-Wild) [39]; MM-Vet [68]. \n\n* The training images of the datasets are observed during training. We also tried adding the ShareGPT4V [13] to the SFT blend on top of VILA-13B (last row), leading to a significant improvement on LLaVA-Bench and MM-Vet (marked in green). \n\nSize Model MMLU [27] BBH [58]  LLaVA-1.5 [38]), our model achieves consistent improvements over most datasets at different model sizes under a head-to-head setting (using the same prompts and base LLM; Vicuna-1.5 is based on Llama-2). Remarkably, we 7B model is able to outperform LLaVA-1.5 13B on VisWiz [26] and TextVQA [55] by a large margin thanks to the pre-training. Our 7B model even outperforms the 13B LLaVA model on these datasets.",
            "score": 0.3879331603029491,
            "section_title": "Quantitative Evaluation",
            "char_start_offset": 17196,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1472
                }
            ],
            "ref_mentions": [
                {
                    "start": 560,
                    "end": 564,
                    "matchedPaperCorpusId": "8081284"
                },
                {
                    "start": 570,
                    "end": 574,
                    "matchedPaperCorpusId": "152282269"
                },
                {
                    "start": 583,
                    "end": 587,
                    "matchedPaperCorpusId": "3831582"
                },
                {
                    "start": 611,
                    "end": 615,
                    "matchedPaperCorpusId": "252383606"
                },
                {
                    "start": 633,
                    "end": 637,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 1336,
                    "end": 1340,
                    "matchedPaperCorpusId": "3831582"
                },
                {
                    "start": 1353,
                    "end": 1357,
                    "matchedPaperCorpusId": "85553602"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.254150390625
        },
        {
            "corpus_id": "277272125",
            "title": "On the Perception Bottleneck of VLMs for Chart Understanding",
            "text": "Even though the original CLIP has provided crucial information, much more than that reflected by the poor retrieval accuracy, we find that LLaVAs based on these enhanced CLIPs still achieve significantly better performance. Consistent with the results from the CLIP evaluation ( \u00a73.2), NegCLIP-LLaVAs demonstrate the best performance across most benchmarks. Specifically, for in-distribution datasets, NegCLIP-LLaVAs achieve improvements exceeding 5 absolute points on FigureQA, DVQA, and PlotQA. Additionally, the improvements observed on MathVista and ChartX highlight the generalization capability of LLaVAs built upon our enhanced CLIP models. On average, compared to the Unfrozen-CLIP baseline, models based on NegCLIP exhibit notable gains: LLaVA-Chart-13B improves by 2.6 absolute points, while LLaVA-Chart-Phi achieves an even larger improvement of 5.0 absolute points. \n\nIn addition, we conduct data scaling experiments during the third-stage chart-specific tuning while keeping the vision encoder frozen. As illustrated in Figure 4, all LLaVA models show a consistent improvement in performance as training data increases, highlighting the effectiveness of chart-specific tuning in alleviating the extraction bottleneck. Moreover, LLaVAs leveraging enhanced CLIPs consistently surpass those utilizing the original CLIP, with NegCLIP-LLaVAs achieving the highest performance across the scaling process. These results further confirm that while chartspecific tuning helps mitigate the extraction bottleneck and improves LVLMs' chart understanding, addressing the vision encoder bottleneck remains crucial for achieving even greater performance gains.",
            "score": 0.3879149808071368,
            "section_title": "The Impact of Enhanced CLIPs to LVLMs",
            "char_start_offset": 14839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1658
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4482421875
        },
        {
            "corpus_id": "272910642",
            "title": "DARE: Diverse Visual Question Answering with Robustness Evaluation",
            "text": "This allows the model to handle text and image input jointly. We use llava-hf/llava-v1.6vicuna-7b-hf. \n\nFor all the models, we opt for their default, suggested hyper-parameters: e.g., temperature is set to 0 with GPT-4 and Gemini, while we use the default generation configuration of Idefics2 and LLaVa, corresponding to greedy decoding.",
            "score": 0.38761210264460677,
            "section_title": "Model Selection and Hyper-Parameters",
            "char_start_offset": 28534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 101
                },
                {
                    "start": 104,
                    "end": 337
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20263671875
        },
        {
            "corpus_id": "277349282",
            "title": "SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language Models for Long-Form Video Understanding",
            "text": "(ii) fewer training stages (e.g., 2 vs. 3 of Qwen2-VL) and (iii) frozen vision encoder. This aligns with prior findings (Zhang et al., 2025b) that, when the model size gets larger, higher input resolution and more training stages with fully tunable parameters are pivotal for improving the image performance. Given that our model is video-centric and these enhancements significantly increase training costs, we leave their exploration for future work. \n\nThird, SF-LLaVA-1.5's image capability benefits from joint video-image training. SF-LLaVA-1.5, jointly optimized on video and image data, outperforms SF-LLaVA-1.5-Image on most benchmarks. To confirm the improvements are not solely due to longer training, we conduct a second-stage training for SF-LLaVA-1.5-Image using only image data. However, the performance gap remains, indicating that joint training is the primary factor. Additionally, the improvements are more significant on Knowledge and General benchmarks (e.g., +1.2% on MMMU and +10.1% on MM-Vet at the 1B scale). We hypothesize this is Table 5: Results of SF-LLaVA-1.5 with different design choices on video understanding. \n\nbecause our video data mainly comes from lifestyle scenarios, which could not directly benefit text-rich tasks. A deeper analysis of joint training will be provided in Sec. 4.4.2.",
            "score": 0.3874005993690188,
            "section_title": "Image Understanding Results",
            "char_start_offset": 18964,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1141
                },
                {
                    "start": 1144,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1323
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.252685546875
        },
        {
            "corpus_id": "269214091",
            "title": "BLINK: Multimodal Large Language Models Can See but Not Perceive",
            "text": "Open-source multimodal LLMs make similar errors.Our comparative analysis of diverse multimodal LLMs reveals striking similarities in the cases where they fail at, highlighting that these shared mistakes are largely influenced by their vision encoder, rather than differences in model size or language model components.This is particularly apparent in the comparison between LLaVA-v1.5-7B(1187 mistakes in total) and LLaVA-v1.5-13B(1147 mistakes in total), two models of different sizes that nonetheless demonstrated 899 common mistakes.In a similar vein, when we compared LLaVA-v1.5-7Bwith other equal-sized models using different language model components, like LLaVA-internLM2-7B, the number of common errors remained high (959 mistakes).Whereas LLaVA-v1.5-7Bonly shares 782 and 655 common mistakes with QwenVLMax and GPT4V, respectively.\n\nGPT-4V Errors: For each task, 10 error instances were randomly selected, and we manually analyze the total of 140 error instances sampled randomly across all tasks as follows: Recognition failure on detailed small regions or edges (28.5%) : the model fails to tell nuanced details, especially circles in visual correspondence, semantic correspondence, functional correspondence, relative depth, relative reflectance and boxes in object localization; Failure to detect the location of the circled point(20%): the model fails to locate the circled point labeled in the images; Failure to recognize spatial relations (14.3%): the model fails to identify the spatial relations between left and right, or up and down; Reasoning errors (12.9%): while the model correctly interprets the images and the question, it fails to derive accurate reasoning for inference; Failure to convey the overall scene impression (8.6%): the model fails to adequately capture the general atmosphere or setting of a scene; Rejection to answer (6.4%): the model refuses to generate an answer; Failure to ground or infer items mentioned in the question (5.7%):\n\nThe model is unable to locate the specific item referenced in the question within the image.",
            "score": 0.3872685925355094,
            "section_title": "C.3 Error analysis",
            "char_start_offset": 37893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 48,
                    "end": 318
                },
                {
                    "start": 318,
                    "end": 387
                },
                {
                    "start": 387,
                    "end": 430
                },
                {
                    "start": 430,
                    "end": 536
                },
                {
                    "start": 536,
                    "end": 585
                },
                {
                    "start": 585,
                    "end": 740
                },
                {
                    "start": 740,
                    "end": 761
                },
                {
                    "start": 761,
                    "end": 840
                },
                {
                    "start": 842,
                    "end": 1974
                },
                {
                    "start": 1976,
                    "end": 2068
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1959228515625
        },
        {
            "corpus_id": "266521081",
            "title": "VCoder: Versatile Vision Encoders for Multimodal Large Language Models",
            "text": "We use LLaVA-1.5 [41] as our base MLLM. LLaVA-1.5 uses CLIP-ViT-L-336px [56] as the image encoder (Im-Coder) with a two-layer MLP as projection and Vicuna-1.5 [77] as the LLM. Inside our VCoder, we also use a CLIP-ViT-L-336px to encode the control inputs and Comparison to baseline Multimodal LLMs on the COST validation dataset for Object Identification. We compare our VCoder to existing off-the-shelf baseline MLLMs: MiniGPT-4 [78], InstructBLIP [14], LLaVA-1.5 [41], and CogVLM [68]. We also train three different variants of LLaVA-1.5 on the COST dataset: COST IT mixes the COST training data with the instruction tuning data; Soft-Prompted uses a set of learnable tokens, and ImCoder uses an RGB image as the control input. Our VCoder adapted LLaVA-1.5 performs the best on all three object perception tasks. Note: \u27e8\u2022\u27e9 denotes input tokens to LLM with seg representing segmentation map, img representing RGB image, prompt representing learnable prompt, and query representing the user question. We also evaluate the performance of GPT-4V [51] on the COST dataset using the publicly accessible paid API released by OpenAI. Our VCoder-adapted LLaVA-1.5 shows the best performance on object identification among all MLLMs. \n\nproject the features into the LLM embedding space using modality-specific two-layer MLPs. We resize the visual inputs to 336\u00d7336 resolution (corresponds to 576 tokens) for our MLLM. During training, we load the instructiontuned weights from LLaVA-1.5 and keep those frozen while only tuning the MLP component of our VCoder. We use the publicly available OneFormer [26] model trained on COCO [38] with DiNAT-L [19,20] backbone to obtain the segmentation map.",
            "score": 0.38540373981771375,
            "section_title": "Experiments",
            "char_start_offset": 20773,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1685
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34814453125
        },
        {
            "corpus_id": "272647612",
            "title": "MemeIntent: Benchmarking Intent Description Generation for Memes",
            "text": "Next, we selected two of the best open-sourced models for experiments. \n\nVision Language Model (Llava 1.6) Because intent description generation is a vision-language task, it is natural to use a vision-language model (VLM) to generate intent descriptions. In our experiments, we used Llava 1.6 (Liu et al., 2023), one of the most popular open-source vision language models with state-of-the-art performance in many visual reasoning tasks. We chose the variant llava-v1.6-mistral-7b-hf5 for its superior performance among the Llana-Next variants with model size no more than 10B. It contains Mistral-7B-Instruct-v0.26 as the base language model and CLIP-ViT-L-336px (Radford et al., 2021) as the vision encoder. \n\nAided Large Language Model (Llava 1.6 + Llama 3) We also experimented with a pure large language model (LLM) with the aid of an image captioner. In other words, we employed a twostaged pipeline including (1) image captioning and (2) text-based intent description generation. For image captioning, we again leveraged Llava 1.6 to generate the captions for the memes. These captions, which describe the images themselves, would act as a proxy for the actual image to the LLM7 . We then used Llama 38 to generate intents from the caption and other textual inputs. Llama 3, the most capable open-source LLM as of now (May 2024), has a decoder-only transformer architecture and was trained on 15 trillion tokens from public data. We used the variant Meta-Llama-3-8B-Instruct9 . \n\nIn all experiments, we kept the hyperparameters of the models the same with the default values and only tuned the max_new_tokens, setting its final value to 100 for intent description generation and 500 for background knowledge generation in both models.",
            "score": 0.3853377717324228,
            "section_title": "Two Models",
            "char_start_offset": 18451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 73,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1742
                }
            ],
            "ref_mentions": [
                {
                    "start": 665,
                    "end": 687,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.292724609375
        },
        {
            "corpus_id": "273025917",
            "title": "Toward a Holistic Evaluation of Robustness in CLIP Models",
            "text": "Modern large multimodal models (LLMs), such as LLaVA [18], typically use a frozen pre-trained visual encoder from CLIP as their visual backbone, with instruction fine-tuning applied to the linear projector and the language model components. This raises an important question: how does the interaction between a shared visual encoder and distinct language models affect the classification performance of LLaVA compared to CLIP-like models? \n\nDriven by this, we compare the classification accuracy of CLIP and LLaVA to investigate how the interaction between the shared visual encoder and their distinct language models influences overall performance. In this section, \"LLaVA\" and \"CLIP\" refer to their training paradigms rather than specific model implementations. We also include SigLIP [46] as another representative of CLIP-like models. \n\nOur evaluation is conducted on three splits of the ImageNet-D dataset [19]: Background, Texture, and Material. This dataset, generated by a text-to-image diffusion model, poses significant classification challenges. We adopt a VQAstyle approach for LLaVA's classification, providing it with Observations: We report the results on ImageNet-D in Table III and summarize the observations as follows. First, extending the findings of [19], which uses CLIP (ViT/14) solely as a category selection model, we observe that the interactions between the language and vision encoders in selection networks can vary significantly. When the category list is easy for CLIP, LLaVA models with the same visual encoder do not improve classification. However, when CLIP struggles with the category list, LLaVA with the same visual encoder offers classification benefits. For example, in row 1, when the most confused categories of ResNet-50 are easy for CLIP, LLaVA models with the same visual encoder show no improvement. Similarly, in row 2, when SigLIP-SO-14 performs well at classification, LLaVA models exhibit a performance drop. However, in row 2, when the category list is challenging for CLIP (SigLIP-SO-14), LLaVA provides over a 20% improvement across three splits.",
            "score": 0.38515235264968695,
            "section_title": "IX. VISUAL AND LANGUAGE ENCODER INTERACTION: A CLASSIFICATION PERSPECTIVE",
            "char_start_offset": 43551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2099
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 57,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "257767223"
                },
                {
                    "start": 911,
                    "end": 915,
                    "matchedPaperCorpusId": "268724151"
                },
                {
                    "start": 1271,
                    "end": 1275,
                    "matchedPaperCorpusId": "268724151"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.371826171875
        },
        {
            "corpus_id": "268512907",
            "title": "X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment",
            "text": "Typically, when training languages with different character systems, the performance of a relatively highly resourced language may deteriorate (Pires et al., 2019) and GQA scores of LLaVA1.5 and X-LLaVA, they showed 8.2 and 0.7 points higher performance, respectively.However, for VQA2.0,LLaVA1.5'sperformance was 4.5 points higher.During analysis, we observed that X-LLaVA generally performed better on GQA and BVQA, which asked about relationships and knowledge.\n\nComparison of X-LLaVA with KoLLaVA.\n\nKoLLaVA 3 is the Korean version of LLaVA1.5, a model trained after automatically translating CC3M, VQA2.0,GQA, and Visual Genome data used in LLaVA1.5.Additionally, it was trained using the Korean version of the BVQA.However, as only the 7B model is currently publicly available, it may be challenging were used to evaluate the same levels.However, the published LLaVA1.5 13B model shows an average of 0.96 points higher in english than that of the 7B model, X-LLaVA demonstrates a 5.2 point higher result in korean than KoLLaVA.\n\nComparison X-LLaVA with LLaVA1.5(O or B).\n\nLLaVA1.5 was trained on about 1.5 times more data (665K VIFs) then X-LLaVA.Nevertheless, BVQA data has never been utilized for training, which may be disadvantageous for the BVQA evaluation.We trained LLaVA1.5 on Korean and English data for three 3 epochs to tune the BVQA for a fair evaluation.LLaVA1.5(B) in Table 3 shows the results of the model tuned using the BVQA data.The results show a significant improvement in Korean performance on the BVQA.On the other hand, this model, being biased towards VQA data, showed lower performance in the writing evaluation (LV).Conversely, LLaVA1.5(O) in Table 3, a model trained on the LLaVA1.5 with mvif data, 3 github.com/tabtoyou/KoLLaVA",
            "score": 0.3840632407485105,
            "section_title": "The effect of multilingual training.",
            "char_start_offset": 21170,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 288
                },
                {
                    "start": 288,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 464
                },
                {
                    "start": 466,
                    "end": 501
                },
                {
                    "start": 503,
                    "end": 609
                },
                {
                    "start": 609,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 720
                },
                {
                    "start": 720,
                    "end": 843
                },
                {
                    "start": 843,
                    "end": 1032
                },
                {
                    "start": 1034,
                    "end": 1075
                },
                {
                    "start": 1077,
                    "end": 1152
                },
                {
                    "start": 1152,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1372
                },
                {
                    "start": 1372,
                    "end": 1452
                },
                {
                    "start": 1452,
                    "end": 1529
                },
                {
                    "start": 1529,
                    "end": 1647
                },
                {
                    "start": 1647,
                    "end": 1760
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 163,
                    "matchedPaperCorpusId": "174798142"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30712890625
        },
        {
            "corpus_id": "277313863",
            "title": "ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in Vision-Language Models",
            "text": "We assess the generalization capacity of ST-VLM's spatiotemporal reasoning by evaluating it across various tasks and domains in a zero-shot setting for Q2. First, Tab. 7 shows that ST-VLM trained with STKit outperforms LLaVA-OneVision by 2.7% in average accuracy even on general video understanding benchmarks, and it further surpasses LLaVA-N-Video-32B [47] on average, which is the larger model than ours. Notably, ST-VLM achieves a 7.4% improvement on PerceptionTest [31], which is a benchmark that thoroughly evaluates the comprehension of fundamental physical properties in videos, by leveraging kinematic priors from STKit. See the supplement for further qualitative analyses. \n\nFurthermore, Fig. 6 compares the performance of our ST-VLM with LLaVA-OneVision on various spatiotemporal reasoning tasks other than proposed STKit-Bench. For spatial reasoning, we design a region-aware VideoQA task by combining Referring Expression Generation and VideoQA. We reformulate existing VideoQA datasets, TVQA+ [18] and DramaQA [10], by providing a specific region and posing questions related to that region. For temporal reasoning, we conduct temporal grounding tasks on TVQA+, ActivityNet [2], and Charades-STA [14], requiring the model to predict timestamps for query actions. Notably, TVQA+ involves both spatial and temporal reasoning, as it requires answers with supporting timestamps based on the question and specified region. In Fig. 6, on temporalaware tasks, ST-VLM demonstrates improved performance in both ActivityNet and Charades-STA datasets, measured by mean Intersection over Union (mIoU) and Recall at IoU 0.3 (R@0.3). For instance, R@0.3 on ActivityNet increases by 17.0%. Additionally, in the spatial-aware task, ST-VLM outperforms LLaVA-OneVision in DramaQA by a margin of 4.6% in accuracy. Finally, in TVQA+, both spatial and temporal grounding abilities of ST-VLM are improved compared to LLaVA-OneVision.",
            "score": 0.38391692907667385,
            "section_title": "Generalized spatio-temporal understanding",
            "char_start_offset": 24043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1925
                }
            ],
            "ref_mentions": [
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "258840892"
                },
                {
                    "start": 1007,
                    "end": 1011,
                    "matchedPaperCorpusId": "135466290"
                },
                {
                    "start": 1024,
                    "end": 1028,
                    "matchedPaperCorpusId": "218538025"
                },
                {
                    "start": 1188,
                    "end": 1191,
                    "matchedPaperCorpusId": "1710722"
                },
                {
                    "start": 1210,
                    "end": 1214,
                    "matchedPaperCorpusId": "31663499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26171875
        },
        {
            "corpus_id": "272832380",
            "title": "MM-CamObj: A Comprehensive Multimodal Dataset for Camouflaged Object Scenarios",
            "text": "Single-image Tasks Table 4 shows the performance of existing LVLMs and our CamObj-Llava on five single-image tasks. Our CamObj-Llava-7B/13B models, trained with CamObj-Align and CamObj-Instruct, outperform all other open-source models in these tasks. Compared to Llava-v1.5-7B/13B, our CamObj-Llava-7B/13B achieves an average improvement of 23.2% and 16.9% across these tasks. \n\nCamObj-Llava excels in camouflaged object recognition. In the Easy and Hard VQA tasks, we evaluate the ability of LVLMs to recognize and classify camouflaged objects. Although our model underperforms compared to the closedsource models in the Hard VQA task, CamObj-Llava-7B surpasses GPT-4o by 2.4% in the Easy VQA task, with CamObj-Llava-13B further improving upon this by 0.9%. \n\nCamObj-Llava surpasses closed-source LVLMs in object localization, scene understanding, and counting. In the Bbox Location task, which primarily evaluates the localization ability of LVLMs for camouflaged objects, our CamObj-Llava-7B/13B outperforms all advanced closedsource models. Compared to the top-performing Gemini-1.5-pro, CamObj-Llava-13B achieves a 14.4% improvement in the mIoU metric. The Image Caption task focuses on assessing the model's understanding of camouflaged scenes. In this task, CamObj-Llava-7B/13B also outperforms leading closed-source models. Specifically, CamObj-Llava-7B, with only 7B parameters, surpasses GPT-4o by 3.2% in CLIP-Score. The Count choice task evaluates the counting abilities of LVLMs, and once again, our CamObj-Llava exceeds all closed-source models.",
            "score": 0.38357885827091615,
            "section_title": "Main Results",
            "char_start_offset": 24816,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 376
                },
                {
                    "start": 379,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1559
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1671142578125
        },
        {
            "corpus_id": "264833352",
            "title": "LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing",
            "text": "Preprint. \n\narXiv:2311.00571v1 [cs.CV] 1 Nov 2023 \n\n\u2022 Open-source. We make our system and code base publicly available to facilitate future improvements in the community. \n\nIn the rest of this paper, Section 2 reviews related work. Section 3 describes the interface, workflow, and AI skills of LLaVA-Interactive. Section 4 presents a case study of developing an AI agent to assist photographic artists using LLaVA-Interactive. Section 5 presents a preliminary evaluation of LLaVA-Interactive. \n\n2 Related Works LMM with Visual Output and Interaction. Most existing LMMs are developed to support visual chat -image understanding and reasoning. There are several exploratory studies to enable LMM to support image output such as image generation/editing and segmentation, demonstrated in GILL [8], CM3leon [29], Emu [20], DreamLLM [3], Kosmos-G [19] and MGIE [4]. The idea is generalized to other modalities such as video and audio in NextGPT [26]. In contrast to model training to enable image output, another line of research is to prompt engineer LLM for multimodal tool use such as Visual ChatGPT [25], X-GPT [30], MM-REACT [28], VisProg [7], and ViperGPT [21], \n\nwhere expert vision models with image output are activated in the inference time without any model training. Both research lines have demonstrated the extended capabilities with image output in LLMs. Similar to them, LLaVA-Interactive also supports image generation/editing and segmentation. LLaVA-Interactive is different from existing works in two aspects: (i) LLaVA-Interactive is cheap in development, as it is a synergy of the inference stages of three models. There is no model training, and no prompt engineering of LLM. (ii) Importantly, LLaVA-Interactive emphasizes the support of visual interaction, where a user can draw strokes to specify the human intent in segmentation and generation/editing, a unique capability that existing visual assistant systems do not have. \n\nUnified Multimodal Modeling.",
            "score": 0.3831987934604141,
            "section_title": "Introduction",
            "char_start_offset": 2165,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 12,
                    "end": 49
                },
                {
                    "start": 52,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 170
                },
                {
                    "start": 173,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 492
                },
                {
                    "start": 495,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1945
                },
                {
                    "start": 1948,
                    "end": 1976
                }
            ],
            "ref_mentions": [
                {
                    "start": 1111,
                    "end": 1115,
                    "matchedPaperCorpusId": "254926770"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.290283203125
        },
        {
            "corpus_id": "277272852",
            "title": "LLaVAction: evaluating and training multi-modal large language models for action recognition",
            "text": "We developed a series of methods that empirically improved performance. We report values for the full model (Table 2), each added method (Table 3), and leave-one-out ablations (Appendix 10 and Appendix Table 7) to assess their contributions. \n\nWe start with the zero-shot evaluation of closedsource GPT-4o and GPT-4o-mini, and open-source models LLaVA-Video-7B, LLaVA-OV-7B, and LLaVA-OV-0.5B (Table 2). GPT-4o from OpenAI performs best, with an accuracy of 52.2. Of the open-source models, LLaVA-Video-7B performs the best (35.7; Table 2). We therefore use LLaVA-Video-7B and fine-tune the model with a series of innovations that leads to a 21-point improvement over GPT-4o (Ours: 71.7 8-frames, 73.4 16-frames vs. GPT-4o 52.2 8-frames 1 ; Table 2). Using our methods, including the testtime action memory, we reach 74.1 (Table 2). \n\nNext, we outline the methods that achieved our SOTA results. Our experiments suggested that fine-tuning the model with only the caption and open-ended question answering (GPT-4o distillation) results in a performance degradation (i.e., zero-shot was 35.7 vs. distilled was only 21.9 using LLaVA-Video-7B). Therefore, we always include the MQA task in the following studies. \n\nTraining with random distractors or adversarial distractors (AVION). In Table 3, we compare LLaVA-Video-7B training with random distractors and adversarial distractors using AVION's predictions. There is a 8.9 difference in accuracy, showing the effectiveness of training against adversarial distractors. \n\nPerspective Prompt (Egocentric). As shown in Table 3, when we fix the distractors from random sampling, using the egocentric prompt gives a 0.5 point improvement over using the allocentric prompt. \n\nVision token supervision.",
            "score": 0.38303209845454567,
            "section_title": "Results on EPIC-KITCHENS-100-MQA",
            "char_start_offset": 20109,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 241
                },
                {
                    "start": 244,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1208
                },
                {
                    "start": 1211,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1515
                },
                {
                    "start": 1518,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1714
                },
                {
                    "start": 1717,
                    "end": 1742
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.466796875
        },
        {
            "corpus_id": "273532785",
            "title": "MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models",
            "text": "Results on LLaVA-v1.5 As present in Tab. 1, applying MIA-DPO to LLaVA-v1.5 achieves improvements of 1.2%/5.8%/2.3%/2.1%/3.5% on five multi-image benchmarks, which demonstrates the effectiveness of MIA-DPO. As for the challenging MMMU benchmark that requires complex domain-specific knowledge, MIA-DPO enables LLaVA-v1.5 to achieve a 1.2% improvement. The experimental results on MMMU demonstrate that MIA-DPO enhances the LLaVA-v1.5's reasoning ability on multi-image problems. Additionally, on the BLINK dataset that includes multi-view and spatial relationship reasoning, MIA-DPO significantly boosts the performance of LLaVA-v1.5 by 5.8%. Such an improvement highlights the effectiveness of MIA-DPO in enhancing the model's ability to understand and reason under multi-image scenarios. \n\nComparison with Preference Optimization Baselines In Tab. 1, we compare MIA-DPO with three preference optimization baselines (LLaVA-RLHF, HA-DPO, POVID) on LLaVA-v1.5. Thanks to our multi-image attention-based method for constructing the DPO data, MIA-DPO achieves significant advantages on the reported five multi-image benchmarks compared to the baselines. \n\nMore LVLM Architectures We also applied MIA-DPO to other LVLM architectures, such as the recent InternLM-XC2.5 model. As shown in Tab. 1, MIA-DPO boosts the performance of 1.2%/0.8%/11.1%/4.5%/4.1% across the five benchmarks, resulting in an average improvement of 4.3%. The results on LLaVA-1.5 and InternLM-XC2.5 demonstrate that MIA-DPO is general and effective for different LVLM architectures. Notably, despite the Supervised Fine-tuning (SFT) phase of InternLM-XC2.5 involving multi-image data, our MIA-DPO still further boosts performance on multi-image benchmarks.",
            "score": 0.3824228723373575,
            "section_title": "RESULTS ON MULTI-IMAGES BENCHMARKS",
            "char_start_offset": 21709,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1724
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.489013671875
        },
        {
            "corpus_id": "268531413",
            "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
            "text": "We report the main experimental results in Table 1, from which we have the following observations:\n\n(1) LLaVA-UHD outperforms strong baselines on popular benchmarks.This includes strong general baselines trained on 2-3 orders of magnitude more data such as Qwen-VL and InstructBLIP, and also high-resolution LMMs that require significantly more computation such as Fuyu-8B, OtterHD-8B, Monkey and SPHINX-2k.The results show that LLaVA-UHD can properly deal with native-resolution images for strong performance, as well as good data and computation efficiency.( 2) LLaVA-UHD achieves significant improvements over the LLaVA-1.5 backbone.Notably, by simply perceiving images in native high-resolution, LLaVA-UHD achieves 6.4 accuracy improvement on TextVQA and 3.2 accuracy improvement on POPE.The reason is that the blurred content in low-resolution images can prevent LMMs from accurately identifying the challenging fine-grained objects and optical characters.The results demonstrate the fundamental role of perceiving native high-resolution images in various multimodal tasks, and the effectiveness of LLaVA-UHD in addressing the problem.\n\n(3) In terms of resolution and efficiency, compared with LLaVA-1.5 associated fixed 336 \u00d7 336 resolution, LLaVA-UHD supports 672\u00d71088 resolution images in any aspect ratio using only 94% inference computation.The results indicate promising scalability of LLaVA-UHD to potentially larger resolutions in future.",
            "score": 0.3820077781463892,
            "section_title": "Main Results",
            "char_start_offset": 21492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 100,
                    "end": 165
                },
                {
                    "start": 165,
                    "end": 407
                },
                {
                    "start": 407,
                    "end": 559
                },
                {
                    "start": 559,
                    "end": 636
                },
                {
                    "start": 636,
                    "end": 792
                },
                {
                    "start": 792,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1140
                },
                {
                    "start": 1142,
                    "end": 1351
                },
                {
                    "start": 1351,
                    "end": 1451
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.301025390625
        },
        {
            "corpus_id": "276937786",
            "title": "Oasis: One Image is All You Need for Multimodal Instruction Data Synthesis",
            "text": "We validate Oasis on medical benchmarks in Tab. 7. We sample 15k images from the MedTrinity-25M dataset and create 2k medical training data with Oasis. We SFT the LLaVA-NeXT baseline with 4k sampled LLaVA data and 2k LLaVA data + 2k synthesized medical data, respectively. The table below shows great performance improvements across 3 medical benchmarks with our data.",
            "score": 0.38154061551665275,
            "section_title": "A.5. Application on medical area",
            "char_start_offset": 30184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 368
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.220703125
        },
        {
            "corpus_id": "268248187",
            "title": "Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation",
            "text": "We set the temperature for all models to 0.0. For ChatGPT, we additionally set the parameter seed to 1234 and n to 1. The latter is necessary due to reports that just the seed and temperature are not sufficient to ensure deterministic and reproducible results for vision model via API. \n\nHowever, for different models, a zero temperature setting can lead to various issues due to the nature of LLMs. ChatGPT4V might occasionally fail to provide an answer, typically for queries that are either difficult or do not pass the safety system checks. We decided to remove such samples, although it gives ChatGPT4V a slight advantage. LLaVA [25], LLaVA-NeXT [23], and LLaVA-based ShareGPT4V [5] may sometimes return an age range instead of a specific age. Since we have sampling disabled and the temperature is set to 0.0, we cannot workaround this; thus, we take the midpoint of the range in such cases. However, the number of such samples is very low, and this does not significantly impact the results. \n\nFor all models, we attempted to use the maximum possible resolution with the goal to measure the maximum possible performance without taking into accound speed of inference. Thus: \n\n-For ChatGPT, we used full-sized original crops to allow the model to split them into tiles (if large enough), which gives ChatGPT another slight advantage. -For LLaVA 1.5 and ShareGPT4V, we used the original 336x336 resolution. \n\n-For LLaVA-NeXT, we utilized the new Dynamic High Resolution technique, although its maximum resolution is still restricted to 672x448 or 448x672.",
            "score": 0.3811395278694926,
            "section_title": "A typical answer looks like: [female; 40]",
            "char_start_offset": 12815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 285
                },
                {
                    "start": 288,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1560
                }
            ],
            "ref_mentions": [
                {
                    "start": 634,
                    "end": 638,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1326904296875
        },
        {
            "corpus_id": "260682715",
            "title": "SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs",
            "text": "We further validate our dataset by fine tuning LLaVA-13B, which demonstrated the highest accuracy among the models in the 0-shot evaluation setting. For this purpose, we utilized the LLaVA-13B checkpoint, released after feature alignment and multi-modal instruction fine-tuning [9]. We evaluated the fine-tuned models using the 0-shot evaluation method introduced in the previous section. We conducted a two-step fine-tuning process for LLaVA-13B on the SciGraphQA dataset. First, we fine-tuned it for 1 epoch on the full dataset, which consisted of 295K samples. Next, we performed fine-tuning on a 30K subset of SciGraphQA, where the question prompts were augmented with the data tables extracted using DePlot from the graphs. \n\nWe used a LLaVA checkpoint with LLAMA-2-13B-chat backbone as base model for finetuning. We employeed a cosine learning rate schedule with a warmup ratio of 3% and a learning rate of 5 \u00d7 10 \u22126 . This learning rate was deliberately lower than the original 2 \u00d7 10 \u22125 used for the instruction tuning of LLaVA and also lower than the learning rate of 3 \u00d7 10 \u22124 used for LLAMA-2-13B [9], [25]. We made this decision based on early experiments, which indicated that end-to-end fine-tuning with higher learning rates, like 2 \u00d7 10 \u22125 , resulted in degraded performance. Thus, it underscored the significance of using an appropriate learning rate for successful fine-tuning. \n\nWe utilized LoRa to enable us to train small-scale adapters on the LLaMa-2-chat-13B backbone [25]. We used a LoRa rank of 64 and a LoRa dropout rate of 0.05. Together with DeepSpeed Zero-2 distributed training, this setup allowed us to train with 4 A100-80Gb GPUs on Azure with a per-device batch size of 16 and a gradient accumulation step of 2, resulting in a effective global batch size of 128.",
            "score": 0.3809157841681453,
            "section_title": "Fine-tuning",
            "char_start_offset": 15822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1795
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.250244140625
        },
        {
            "corpus_id": "276574714",
            "title": "InsightVision: A Comprehensive, Multi-Level Chinese-based Benchmark for Evaluating Implicit Visual Semantics in Large Vision Language Models",
            "text": "Given the impressive performance of LVLMs in tackling image understanding challenges, we evaluated the following LVLMs: InternVL2 [8], Qwen2-VL [38], MiniCPM-V-2 6 [41], DeepSeek-VL [28], LLaVA-OneVision [22], and GPT4o [1]. These models were selected based on their topranking performance in the OpenCompass leaderboard [10]. Notably, Qwen2-VL-72B [38] stands out as the leading open-source LVLMs, while GPT-4o [1] is widely regarded as one of the excellent closed-source LVLM. Detailed descriptions of these models are provided in the Appendix D.",
            "score": 0.38015429482863855,
            "section_title": "Experiments",
            "char_start_offset": 19621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 548
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2264404296875
        },
        {
            "corpus_id": "271097827",
            "title": "What to Say and When to Say it: Live Fitness Coaching as a Testbed for Situated Interaction",
            "text": "The number of frames provided to the model vary across the baselines. For InstructBLIP, the latest frame is shown. For Video-LLaVA and Video-LLaMA, 8 uniformly sampled frames from the most recent 5 second window were shown. For Video-ChatGPT, LLaVA-NeXT-Video, and LLaVA-Vid, 20, 8, and 20, frames per 5 second interval were provided, respectively -frames were accumulated during evaluation for each feedback interval (E.g., when generating a feedback at 10s, the frames from the first 5s interval are kept). The history of responses is provided in the prompt for all models.",
            "score": 0.3793058888855703,
            "section_title": "Number of frames.",
            "char_start_offset": 37574,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 575
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.091064453125
        },
        {
            "corpus_id": "273507645",
            "title": "LLaVA-KD: A Framework of Distilling Multimodal Large Language Models",
            "text": "As shown in Table 1, In the context of 1B and 2B model scales, our LLaVA-KD demonstrates significant advantages. Specifically, with 1B parameters, we surpass SPHINX-Tiny (Gao et al., 2024) Table 1: Benchmarked results with SoTA MLLMs. Compared with counterparts, our LLaVA-KD achieves highly competitive results than current small-scale MLLM models and the recently released LLaVA-MOD (Shu et al., 2024) that employs MoE strategies. Optimal and sub-optimal results are in bold and underline, respectively. grey and blue backgrounds respectively represent the most direct MLLM distillation method and our approach. AVG: The average of the nine benchmarks for comprehensive comparison except MMMU. \u2020 : reproduced results using the official code. 5 for more details) Furthermore, our model surpasses LLaVA-MoD (Shu et al., 2024), a model that mitigates hallucination through preference distillation, by achieving an average improvement of 1.1% across the seven reported benchmarks, excluding VQAv2, POPE, and MMMU. It's worth noting that LLaVA-MoD introduces a MoE structure in the s-MLLM, resulting in large total parameters. Meanwhile, LLaVA-MoD is trained on nearly five times the amount of data compared to our approach (Refer to Table 5). Moreover, it can be observed that our LLaVA-KD-1B achieves comparable results with recent the state-of-the-art s-MLLM MoE-LLaVA-2B (Lin et al., 2024) and surpasses TinyLLaVA-2B (Zhou et al., 2024), despite having only half the model size. It also can be observed that, with 2B parameters, our LLaVa-KD-2B also achieves the leading performance compared to existing small-scale MLLM models, outperforming the previous art Imp-2B (Shao et al., 2024) by 0.9%.",
            "score": 0.3780528441570802,
            "section_title": "BENCHMARKED RESULTS WITH THE STATE-OF-THE-ARTS",
            "char_start_offset": 22517,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1696
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1993408203125
        },
        {
            "corpus_id": "277272852",
            "title": "LLaVAction: evaluating and training multi-modal large language models for action recognition",
            "text": "Our final method, called LLaVAction, is based on the opensource LLaVA-OneVision 0.5 and 7B models [25], and on LLaVA-Video-7B [81]. Thus, we benchmark these as baselines. We present results for both the 0.5 billion (0.5B) and 7 billion (7B) parameter models of LLaVA-OneVision, and the LLaVA-Video-7B, as it does not provide a 0.5B alternative. In all experiments, we started from their released checkpoints [33][34][35] (Appendix 7). In the following, we highlight the methods that we developed that empirically improve performance in action recognition. GPT-4o distillation. Distilling GPT-4o represents a prevalent, albeit contentious strategy aimed at enhancing open-source MLLMs. However, as shown in Table 1, since GPT-4o itself struggles with our empirically hard distractors, distilling from GPT-4o alone harms the performance on our proposed EPIC-KITCHENS-100-MQA benchmark if not trained on MQA. Following common practices [30,81], we craft two ways for GPT-4o to annotate. One is to provide captions of the video clips, and the other is to generate open-ended question-answer pairs. The details are in the Appendix 8. \n\nEgocentric vs. Allocentric prompt perspective. Since the videos are taken from the first-person perspective, we were interested in understanding whether controlling the perspective in the instruction prompt makes a difference. As shown in Table 3, switching from the third-person (allocentric) perspective to the first-person (egocentric) perspective improved the results. We present the prompts in the Appendix 9.2. \n\nAdversarial distractors from action recognition models. We ran inference for AVION and TIM on EPIC-KITCHENS-100 using their officially released code and weights to obtain their predictions on both training and validation sets. We sort the predictions by model confidence and keep the top 5 predictions as distractors.",
            "score": 0.37800041888894653,
            "section_title": "LLaVAction models",
            "char_start_offset": 10163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1547
                },
                {
                    "start": 1550,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1867
                }
            ],
            "ref_mentions": [
                {
                    "start": 933,
                    "end": 937,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.404296875
        },
        {
            "corpus_id": "276287954",
            "title": "DeepSeek on a Trip: Inducing Targeted Visual Hallucinations via Representation Vulnerabilities",
            "text": "Multimodal Large Language Models (MLLMs) have significantly advanced AIdriven perception by integrating vision and language understanding. One of the most widely used closed-source MLLMs is OpenAI's GPT-4 Omni (GPT-4o), which extends the capabilities of previous GPT models by incorporating a visual modality. However, its proprietary nature restricts transparency, self-hosting, and rigorous independent evaluation. Despite these limitations, GPT-4o has demonstrated state-of-the-art performance in multimodal reasoning tasks [32]. \n\nIn contrast, open-source MLLMs such as LLaVA (Large Language and Vision Assistant) provide researchers with accessible model architectures, training frameworks, and weights. LLaVA extends LLaMA through CLIP-based visionlanguage integration and has become a benchmark for evaluating multimodal understanding. The latest stable iteration, LLaVA-1.5, is widely used for benchmarking, while LLaVA-1.6 introduces further refinements [22]. Other notable open-source MLLMs include MiniGPT-4 [36], BLIP-2 [20], OpenFlamingo [4], Qwen-VL [5], and DeepSeek Janus Pro [10]. While open-source models foster innovation and reproducibility, their exposed architectures also present a broader attack surface for adversarial vulnerabilities.",
            "score": 0.37781915717935033,
            "section_title": "Multimodal Large Language Models",
            "char_start_offset": 2781,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1260
                }
            ],
            "ref_mentions": [
                {
                    "start": 527,
                    "end": 531,
                    "matchedPaperCorpusId": "271213241"
                },
                {
                    "start": 963,
                    "end": 967,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42041015625
        },
        {
            "corpus_id": "272910746",
            "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness",
            "text": "To further validate the effectiveness of 2D LMM-based Architecture and ensure fairness as much as possible, we choose LLaVA-1.5 as the base model and replace the LLaVA-3D-Instruct-86K dataset in stage 1 with the MM-Scan QA [38] training data. We record and evaluate the performance of LLaVA-3D under different training data ratios. Besides, we further fine-tune LEO [18] on full MM-Scan QA training data based on the officially released model checkpoint. Both models utilize Vicuna-7B as the LLM, ensuring comparable parameter counts. As illustrated in Fig. 5, LLaVA-3D surpasses LEO's full-step performance even when trained on less than 300 steps, indicating better data efficiency and 3.5x faster training convergence speed.",
            "score": 0.37779697335488993,
            "section_title": "B. Training Convergence Speed",
            "char_start_offset": 27236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 727
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29931640625
        },
        {
            "corpus_id": "273026182",
            "title": "Why context matters in VQA and Reasoning: Semantic interventions for VLM input modalities",
            "text": "a. Semantic Entropy for VQ Answering (lower is better) b. Semantic Entropy for VQ Reasoning (lower is better)   For PaliGemma, adding image and context information significantly reduces uncertainty in VQ answering, making the model much more confident in its predictions. It seems that providing additional context, regardless of its content, leads the model to be more self-assured. This intriguing pattern shows large overconfidence in PaliGemma, which does not always have to be beneficial as it can, e.g., lead to silent failures, where the model is extremely confident in its wrong predictions (Bender et al., 2021;Jaeger et al., 2023). \n\nFor all LLaVA models, we observe overall an inverse relationship between answer uncertainty and reasoning uncertainty, with LLaVA 1.5 exhibiting the highest uncertainty in VQ answering but the lowest in VQ reasoning. \n\nWhen the image is added, LLaVA-Vicuna and LLaVA-NeXT show reduced uncertainty in VQ answering but increased uncertainty in VQ reasoning, as the models, in the question-only configuration, only acknowledge the absence of the image and therefore reason with high confidence. Complementary context slightly decreases model uncertainty, indicating a marginal increase in confidence for both VQ answering and reasoning. This effect is minor though, as shown in Figure 3 b. where all LLaVA models exhibit nearly identical semantic entropies for I+Q and Q+I+C+, as well as for Q+I A and Q+I A +C+. Contradictory contextual information, on the other hand, significantly increases uncertainty in the model answers. Its effect on reasoning is also particularly pronounced in LLaVA 1.5 but remains relatively minor for LLaVA-Vicuna and LLaVA-NeXT. Thus, the LLaVA models appear to be slightly influenced by reinforcing information sources but are more easily unsettled by contradictory ones. \n\nModel Failure Detection through Semantic Entropy Interpretating VLMs' behavior through the lense of model uncertainty is crucial for identifying and understanding failures, including hallucinations and silent failures. Specifically, for PaliGemma, silent failures cannot be dismissed due to the model's extreme overconfidence, despite its prediction accuracy being comparable to that of LLaVA models.",
            "score": 0.37701602877220797,
            "section_title": "MODEL UNCERTAINTY",
            "char_start_offset": 22581,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1843
                },
                {
                    "start": 1846,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2246
                }
            ],
            "ref_mentions": [
                {
                    "start": 599,
                    "end": 620,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.307373046875
        },
        {
            "corpus_id": "268384865",
            "title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",
            "text": "Our final SFT data mixture contains a variety of datasets, mostly follow LLaVA-1.5 [74] and LLaVA-NeXT [75].Specifically,\n\n-To encourage the model to provide long-form detailed responses and perform conversations, we follow previous work, use the existing GPT-4 generated data (LLaVA-Conv and LLaVA-Complex [76]) and the existing GPT-4V generated data (ShareGPT-4V [15]) for model training.We also experimented with LAION-GPT4V, but did not observe further performance improvement, thus not included in the final mixture.-To enhance the model with better vision-language (VL) understanding capability, we use a variety of academic task oriented VL datasets.These datasets are either in the form of image captioning, or in the form of VQA with short answers.Specifically,\n\n\u2022 For natural images: VQAv2 [38], GQA [46], OKVQA [82], A-OKVQA [98], and COCO Captions [18]; \u2022 For text-rich images: OCRVQA [86], and TextCaps [103];\n\n\u2022 For document and chart understanding: DVQA [51], ChartQA [83], AI2D [52],\n\nDocVQA [85], InfoVQA [84], and SynthDog-En [53]; -To enhance the model's text-only instruction following capability, we also blend in a small amount of text-only SFT data.\n\nThe academic task oriented image captioning and VQA datasets are formatted into the instruction-following format, following LLaVA-1.5 [74], with detailed prompts summarized in Table 5.Note that our sampling/mixing procedure is performed once offline and stored as a fixed deterministic snapshot of our pre-training mixture.This means, with the exception of our ablations on the pre-training mixture itself, all models in this paper are trained on the same examples in the same order.",
            "score": 0.3768871785927305,
            "section_title": "A.3 Visual Instruction Tuning Data",
            "char_start_offset": 31912,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 108,
                    "end": 121
                },
                {
                    "start": 123,
                    "end": 390
                },
                {
                    "start": 390,
                    "end": 521
                },
                {
                    "start": 521,
                    "end": 657
                },
                {
                    "start": 657,
                    "end": 757
                },
                {
                    "start": 757,
                    "end": 770
                },
                {
                    "start": 772,
                    "end": 922
                },
                {
                    "start": 924,
                    "end": 999
                },
                {
                    "start": 1001,
                    "end": 1172
                },
                {
                    "start": 1174,
                    "end": 1358
                },
                {
                    "start": 1358,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1657
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.216552734375
        },
        {
            "corpus_id": "269502355",
            "title": "Teaching Human Behavior Improves Content Understanding Abilities Of LLMs",
            "text": "To evaluate the generalizability of behavior data across modalities, we extend our evaluation to audio summarization and text sentiment analysis and observe improvements of 19.5% (Table 13). Figures 6,7,10,and 5,8,9, show several randomly sampled qualitative examples for dense captions generated by Behavior-LLaVA over images and videos respectively. It can be noticed that despite not being explicitly trained for this task, the model performs quite well, picking up various artistic, cognitive, and object and material properties. From Table 9, while Behavior-LLaVA shows a decrease in correctness over LLaMA-Vid, it shows significant improvement in other aspects, including detail and quality. On these aspects, it even comes close to 2.5X larger models (LLaVA-1.6 (34B)). \n\nNext, in Table 11 we compare the signals from behavioral data of perception and action. For this, we compare Behavior-LLaVA trained on BLIFT and Behaviour-LLaVA trained on Salicon salient regions and objects. Further, within BLIFT, we compare the performance from predicting singled out behaviours including likes/views, titles, comments. It can be noted that training on just Salicon results in a performance decrease for the lower-level task of action recognition (MSRVTT-QA) but improves on the higher-level task of Emotion recognition. However, the gains are smaller than those observed with training on BLIFT.",
            "score": 0.3762389022260915,
            "section_title": "DISCUSSION",
            "char_start_offset": 29976,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1393
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.216552734375
        },
        {
            "corpus_id": "271855335",
            "title": "Revisiting Multi-Modal LLM Evaluation",
            "text": "In this paper, all the open source MLLMs are loaded directly from HuggingFace, the detail models are below: Salesforce/blip2-flan-t5-xl LLaVA1.5-7b llava-hf/llava-1.5-7b-hf LLaVA1.5-13b llava-hf/llava-1.5-13b-hf GPT-4v/4o are not open sourced, therefore we are unable to identify the models. We utilize the API released by OpenAI to evaluate four datasets on GPT-4v/4o.",
            "score": 0.3761324559195809,
            "section_title": "D Model Details",
            "char_start_offset": 25966,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 369
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07763671875
        },
        {
            "corpus_id": "270878096",
            "title": "Why do LLaVA Vision-Language Models Reply to Images in English?",
            "text": "The resource efficiency of LLaVA has already enabled several works creating multilingual LLaVA models.Andersland (2024) trains an Amharic LLaVA model by employing methods previously used for languages with data scarcity such as machine translation, dataset augmentation and dataset expansion.Similarly Shin et al. (2024) train an English-Korean-Chinese trilingual LLaVA model using vocabulary expansion and a trilingual pretraining data mixture.Looking at the failures of VLMs, Song et al. (2024) focus on three key aspects: multilinguality, complex reasoning and multimodality.To address these issues, they propose three interventions that show significant improvements in model performance.The translate-test approach enhances multilingual processing, visual programming simplifies complex reasoning, and image captioning boosts multimodal understanding.\n\nIn our approach, we specifically focus on developing mechanisms to understand IFL by analyzing the linguistic and visual encoder integration, aiming to explain language fidelity issues even in the face of complex multimodal inputs.",
            "score": 0.3758886529832256,
            "section_title": "Multimodal Understanding and Multilinguality",
            "char_start_offset": 5308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 102,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 578
                },
                {
                    "start": 578,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 856
                },
                {
                    "start": 858,
                    "end": 1089
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.368896484375
        },
        {
            "corpus_id": "275458756",
            "title": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?",
            "text": "[44], alongside six recent open-source MLLMs like Qwen2-VL [46] and LLaVA-OneVision [22]. Despite their strong offline performance, these models struggle with online-style queries (e.g., What is happening now?), showing a significant gap from human performance. Further experiments on recent streaming models, such as Flash-VStream [57], reveal an even wider performance gap compared to offline counterparts, highlighting a substantial research space for further exploration and improvement.",
            "score": 0.37538739693080775,
            "section_title": "Introduction",
            "char_start_offset": 5093,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 491
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.148681640625
        },
        {
            "corpus_id": "274023112",
            "title": "VCBench: A Controllable Benchmark for Symbolic and Abstract Challenges in Video Cognition",
            "text": "As shown in Table 3 and summarized in Figure 4, all models exhibit a consistent decline in accuracy as task difficulty rises, highlighting the particular challenge of abstract cognitive tasks. Most models show a roughly 10-point accuracy drop from Easy to Medium levels, with an additional 5-point decline at the Difficult level. Similarly, LLaVA-NEXT-Video-34B shows a sharp performance decline at the Difficult level, especially in TR and SR dimensions. Under the most challenging conditions, even Qwen2-VL-72B achieves only 23.3% accuracy in Scene 7 (Sky Battle), approaching the level of random guessing. The evaluation also reveals that, while Video-LLaMA2 and LLaVA-NEXT-Video-34B perform similarly at the Easy level, LLaVA-NEXT-34B begins to outperform Video-LLaMA2 as tasks become more chal- lenging. These results highlight the current limitations of VLMs in processing complex abstract scenes, emphasizing the need for advancements to improve their robustness in handling complex videos. All results are provided in Appendix A.4.",
            "score": 0.3746443116110275,
            "section_title": "Performance Across Different Difficulty Levels",
            "char_start_offset": 10057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1039
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.167724609375
        },
        {
            "corpus_id": "270562689",
            "title": "Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning",
            "text": "We comprehensively evaluate 12 state-of-the-art VLLMs of various model families and sizes (3-37B) including LLaVA-v1.5 (7B) [Liu et al., 2023a], LLaVA-Next (7B/13B) [Liu et al., 2024], Qwen-VL-Chat (9B) [Bai et al., 2023], InternLM-XComposer2 (7B) [Dong et al., 2024], VILA (2.7B/7B) [Lin et al., 2024], Emu2-Chat (37B) [Sun et al., 2023], IDEFICS1 (9B) [Lauren\u00e7on et al., 2023], IDEFICS2 (8B) [Lauren\u00e7on et al., 2024], Phi-3-Vision (4B) [Abdin et al., 2024], and GPT4-V [OpenAI, 2023].The training data of LLaVA family models contains only single-image pairs, while the others contain interleaved image-text data.To ensure reproducibility, we use greedy decoding for the open-sourced models and use the \"2024-05-13\" API version for GPT4-V.\n\nPerformance In Table 3, we present the overall performance comparison of all the models we have tested.We establish the random chance performance baseline by assuming the model generates uniform random choices when answering multiple-choice questions.And when the question type is free-form, the random model gets an accuracy of zero.We can see that on the dimension of perception, the best open-source model can perform close to the performance of the state-of-the-art close-sourced GPT4-V model.However, on other dimensions like reasoning, these open-source models all lag behind GPT4-V by a great margin.Most interestingly, on the dimension of visual world knowledge and multi-hop reasoning, no open-source model can reliably outperform the random chance baseline, indicating a vast space for exploring the design of VLMs that are able to perform these tasks.",
            "score": 0.3745213497085199,
            "section_title": "Models",
            "char_start_offset": 14519,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 486
                },
                {
                    "start": 486,
                    "end": 614
                },
                {
                    "start": 614,
                    "end": 740
                },
                {
                    "start": 742,
                    "end": 845
                },
                {
                    "start": 845,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1076
                },
                {
                    "start": 1076,
                    "end": 1239
                },
                {
                    "start": 1239,
                    "end": 1349
                },
                {
                    "start": 1349,
                    "end": 1604
                }
            ],
            "ref_mentions": [
                {
                    "start": 284,
                    "end": 302,
                    "matchedPaperCorpusId": "266174746"
                },
                {
                    "start": 354,
                    "end": 378,
                    "matchedPaperCorpusId": "259287020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.247314453125
        },
        {
            "corpus_id": "271231424",
            "title": "Emotion Recognition from Videos Using Multimodal Large Language Models",
            "text": "Integrating LLaVA textual features: Using LLaVA [11] to describe video frames separately and integrating these frame-specific textual features into the ViPER-VATF [9] framework resulted in a mean Pearson correlation of 0.2895, indicating the viability of this alternative approach. However, integrating Video-LLaVA [12] textual was still better (up to 0.3011 vs. 0.2895).",
            "score": 0.3737905019126434,
            "section_title": "\u2022",
            "char_start_offset": 22500,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 371
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 166,
                    "matchedPaperCorpusId": "252568643"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.228271484375
        },
        {
            "corpus_id": "261049617",
            "title": "StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data",
            "text": "In order to gauge the effectiveness of our data generation strategy, we have elected to utilize the open-sourced LLaVA [20,21] model as our multimodal LLM model. It should be noted that our pipeline is model-agnostic, meaning the datasets generated via our approach can be employed for training a variety of other models. To set the stage for a detailed exposition of our pipeline, this preliminary section provides an overview of the structure and training strategies used in the LLaVA model. For a more indepth understanding, the reader may refer to the original publication [21]. Architecture. The architecture of the LLaVA model elegantly combines a pre-trained LLM, specifically the Vicuna-13B [5], and a pre-trained visual encoder, known as CLIP-ViT-L/14 [28]. This fusion of text and visual processing abilities is facilitated by the incorporation of a learnable linear layer. This linear layer has the primary task of projecting the image features, developed by the CLIP encoder, into the word embedding space of the LLM. The resultant projected embeddings effectively function as tokens within the LLM, creating a synergy between text and visual data streams. It's worth mentioning that in LLaVA-1.5 [20], the linear projection layer has been substituted with a twolayer MLP. Additionally, the LLM model has been replaced with Vicuna-1.5-13B, and the input image size has been increased from 224x224 to 336x336, thereby elevating the model's multimodal capabilities to a greater extent. A detailed illustration of this model structure can be found in Figure 2 of this paper. \n\nTraining and datasets. The core of LLaVA's training process revolves around visual instruction tuning, which necessitates a triplet of data inputs: images, questions, and corresponding answers. The goal here is to prompt the model to predict the next tokens in the answers conditioned on visual tokens and instruction tokens (e.g., a question) in an auto-regressive manner. \n\nThe training of LLaVA is split into two distinct stages. Each stage has a specific focus and uses different data and optimized parameters.",
            "score": 0.37371036733874363,
            "section_title": "Preliminary",
            "char_start_offset": 9581,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1583
                },
                {
                    "start": 1586,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1959
                },
                {
                    "start": 1962,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2100
                }
            ],
            "ref_mentions": [
                {
                    "start": 761,
                    "end": 765,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64892578125
        },
        {
            "corpus_id": "277313523",
            "title": "PAVE: Patching and Adapting Video Large Language Models",
            "text": "Moving forward, we demonstrate that PAVE can be applied to different models and various model scales. We train PAVE using pretrained LLaVA-Onevision [28] and Video-LLaVA [36] as base models. We choose LLaVA-OneVision as it provides models at both 0.5B and 7B scales, and Video-LLaVA since its pre-training set and model structure differ from those of LLaVA-OneVision, potentially impacting PAVE's adaptation. We conduct experiments on the ScanQA dataset, reporting top-1 Exact Match (EM@1) as the metric. ting of multiple tasks share overlapping side-channels (e.g., high-frame-rate video). In this case, multiple patches can be learned jointly, leading to potential enhancement across their corresponding side-channels. As a first step to demonstrate this possibility, we design an experiment to integrate high frame rate video patch and audio-visual path for high frame rate audio-visual QA. Specifically, we build on the patch learned for injecting high frame rate videos (PAVE-7B model in Table 3), and train an additional patch for audio-visual QA on the AVSD training set. During training, the old patch is kept frozen and only the audio-visual patch is learned. During the inference, we activate both patches, and PAVE takes input key frames (i.e. low frame rate video), high frame rate video, and audio as input. Table 6 shows that our multi-task learning leads to a notable improvement (+7.5 in CIDEr scores) on the AVSD test set, demonstrating the possibility and the benefit of training multiple patches together. We provide further discussion in Sec. 5. \n\nResults visualization and diagnosis. Finally, we visualize sample results of 3D QA and audio-visual QA in Figure 3. An interesting failure case of PAVE, as we previously mentioned in Sec. 4.1 is shown in sample (6), where the piano is present in the audio but never appears visually in the video, creating a conflict between the auditory and visual information. In this case, PAVE produces inaccurate answers.",
            "score": 0.37329815647480347,
            "section_title": "Further Analysis",
            "char_start_offset": 23149,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1565
                },
                {
                    "start": 1568,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 1977
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 174,
                    "matchedPaperCorpusId": "265281544"
                },
                {
                    "start": 1779,
                    "end": 1782,
                    "matchedPaperCorpusId": "249097890"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.353759765625
        },
        {
            "corpus_id": "273482814",
            "title": "Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers",
            "text": "We extensively evaluate the effectiveness of our method with different language towers. As shown in Figure 7, replacing the original Vicuna-7B-v1.5 language model with Vicuna-13B-v1.5 (Zheng et al., 2024), Meta-Llama-3-8B-Instruct (Dubey et al., 2024), andMistral-7B-Instruct-v0.2 (Jiang et al., 2023), Victor remains highly effective and significantly outperforms the two baseline methods. For both Meta-Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2, Victor demonstrates minimal performance drop and a slow decay in performance as the number of visual tokens decreases. Notably, for these two models, when the number of visual tokens is reduced by half, the method shows no performance degradation at all. \n\nWe further demonstrate the performance of our method on a different vision-language model design: LLaVA-NeXT (LLaVA-v1.6) (Liu et al., 2024a). LLaVA-NeXT follows a similar architecture to LLaVA-v1.5 but increases the number of visual tokens from 576 to 2,880 by incorporating different aspect ratios, enhancing the model's capabilities. Additionally, LLaVA-NeXT utilizes Qwen2-7B-Instruct (Yang et al., 2024) as its language tower, benefiting from its extended context length. In our experiments, we reduce the number of visual tokens to 512, 256, 128, 64, 32, and 16. As indicated in Figure 7d, our method remains highly effective in the LLaVA-NeXT setting, consistently outperforming both FastV and the Perceiver Resampler. We show the results of the ablation study on which layer to drop the visual tokens (hyperparameter k) in Figure 8. In terms of throughput improvement, it is clear that the earlier we drop visual tokens, the more efficient the model becomes. For lower-layer numbers, such as k = 1 or k = 2, the model's efficiency significantly increases, with throughput reaching nearly a 4\u00d7 improvement.",
            "score": 0.3725883104324428,
            "section_title": "DIFFERENT LANGUAGE TOWERS",
            "char_start_offset": 23269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 709
                },
                {
                    "start": 712,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1825
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 204,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 834,
                    "end": 853,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.437744140625
        },
        {
            "corpus_id": "268876083",
            "title": "Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns",
            "text": "All the baseline models perform better with eye gaze patterns in DDx.The largest increase from 'No Gaze' (2.59%) to 'Gaze' (11.10%) was seen by the LLaVA-Med model.This increase in performance is not seen in all the models in other tasks.Still, we observe the improvement in at least one baseline model in all the tasks.In the error detection task, we see the largest increase in LLaVA-v0, a 43.03% increase from 'No Gaze' (28.75%) to 'Gaze' (71.78%).LLaVA-Med and CXR-LLaVA performance also increased with eye gaze patterns.Apart from these models, LLaVA-v1.6,LLaVA-v1.6M,and LLaVA-v1.5-13Bmodels saw an increase in performance for the VQA task.These findings highlight the effectiveness of incorporating this additional human-computer interaction to enhance model performance.",
            "score": 0.37238478727632507,
            "section_title": "Eye gaze enhances DDx most evidently",
            "char_start_offset": 11265,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 69,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 238
                },
                {
                    "start": 238,
                    "end": 320
                },
                {
                    "start": 320,
                    "end": 451
                },
                {
                    "start": 451,
                    "end": 525
                },
                {
                    "start": 525,
                    "end": 561
                },
                {
                    "start": 561,
                    "end": 573
                },
                {
                    "start": 573,
                    "end": 591
                },
                {
                    "start": 591,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 778
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22802734375
        },
        {
            "corpus_id": "277272125",
            "title": "On the Perception Bottleneck of VLMs for Chart Understanding",
            "text": "Building on the observed advantages of LLaVAs based on enhanced CLIPs, we aim to further verify whether these enhanced CLIPs can raise the performance ceiling of their corresponding LLaVAs. If we further scale up the taskspecific training data -it is possible that the original CLIP already encodes all the required information and the LLaVA training can extract that knowledge through more instruction tuning data. To this end, we conduct larger-scale chart understanding tuning in the third stage of LLaVA training to mitigate the extraction bottleneck. In this specific ablation experiment, we experiment with individual datasets to avoid potential confounders such as the transfer effect between different datasets. Specifically, we choose DVQA and PlotQA as they offer large training sets suitable to this scaling study. \n\nSpecifically, we conduct training for both CLIP and LLaVA using the DVQA and PlotQA datasets separately, leading to the two specialized models: LLaVA-PlotQA and LLaVA-DVQA. For CLIP training, we utilize a total of 2 million samples from DVQA and 3 million samples from PlotQA. We still incorporate both standard training data and hard negative variants, following the hard negative generation strategy and hyperparameter configuration detailed in Section 3.4. For LLaVA training, we adhere to the three-stage training process using the LLaVA-v1.5-13B model, as outlined in Section 4.1. In the third and final chart-specific tuning stage, we train LLaVA models using 800K samples from each dataset separately, allowing us to systematically investigate the performance ceiling under this setting.",
            "score": 0.37233908785208014,
            "section_title": "Scaling Chart Understanding Tuning",
            "char_start_offset": 16536,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 825
                },
                {
                    "start": 828,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1622
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.457275390625
        },
        {
            "corpus_id": "273403986",
            "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
            "text": "Conditional Image Generation: The subset of MultiGen-20M dataset (Qin et al., 2023) including canny-to-image, inpainting, and colorization is employed to equip the model with the ability to generate images under specific conditions and constraints. \n\nImage Understanding: Fine-tuning on the subset of LLaVA-OneVision (Li et al., 2024a) and Cambrain (Tong et al., 2024) to enhance the model's image comprehension capabilities. Data about math/reasoning and cross-duplicated data in the two datasets are removed.",
            "score": 0.3710090098284056,
            "section_title": "MULTIMODAL PRETRAINING AND INSTRUCT TUNING",
            "char_start_offset": 17611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 510
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280029296875
        },
        {
            "corpus_id": "265281544",
            "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
            "text": "Additionally, we evaluate LVLMs using several benchmark toolkits for visual instruction tuning. These benchmark toolkits provide a detailed assessment of the model's capabilities through robust evaluation metrics. Video-LLaVA outperform InstructBLIP-7B by 24.9%, 12.2%, and 5.8% on MMBench, LLaVA-Bench, and MM-Vet, respectively. It is worth noting that Video-LLaVA-7B still demonstrates advanced performance compared to larger LLM models, surpassing InstructBLIP-13B by 6.4% on MM-Vet and IDEFICS-80B (Lauren\u00e7on et al., 2023) by 6.4% on MMBench. These results demonstrate that Video-LLaVA exhibits a strong understanding of semantic aspects of scenes, enabling it to answer open-ended and free-form natural language questions about images.",
            "score": 0.37060886185362546,
            "section_title": "Evaluation under Image Benchmark Toolkits",
            "char_start_offset": 16199,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 740
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259521484375
        },
        {
            "corpus_id": "270878491",
            "title": "Certainly Uncertain: A Benchmark and Metric for Multimodal Epistemic and Aleatoric Awareness",
            "text": "LLaVA with LoRA-SFT.We include results with LoRA-SFT on LLaVA-v1.5-7b in Table 6, which show consistent performance improvement when trained with our data.\n\nComparing 7B to 13B models.We conduct experiments to study the performance of a larger model across different uncertainty awareness categories.These results are presented in Table 7.\n\nWe observe consistent performance improvements over LLaVA-1.5-7B-LoRA and LLaVA-1.5-13B-LoRA[27] with the augmentation of CERTAINLYUNCERTAIN during the instruction-tuning phase.When instruction-tuned with only our data (i.e., Ours-only), compared to the results on the 7B-LoRA model, a larger model 13B-LoRA only marginally improves on confidence-weighted accuracy and    ECE (IDK).However, when mixing our data with LLaVA instruction tuning data (i.e., Ours+LLaVA Data), the resulting 13B model clearly outperforms 7B on both metrics.\n\nIn addition, we observe that the model performance on LAVE idk metrics stay on par for 7B and 13B models with the same training data, while they can still be differentiated by our proposed metric, which further highlights the importance of confidence-weighted accuracy.",
            "score": 0.3702249555679055,
            "section_title": "E Additional Results",
            "char_start_offset": 32776,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 20,
                    "end": 155
                },
                {
                    "start": 157,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 339
                },
                {
                    "start": 341,
                    "end": 433
                },
                {
                    "start": 433,
                    "end": 518
                },
                {
                    "start": 518,
                    "end": 723
                },
                {
                    "start": 723,
                    "end": 876
                },
                {
                    "start": 878,
                    "end": 1147
                }
            ],
            "ref_mentions": [
                {
                    "start": 433,
                    "end": 437,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29296875
        },
        {
            "corpus_id": "268531413",
            "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
            "text": "LLaVA-UHD 14.63 81.4 (-0.3) 61.8 (-3.4) 64.5 (-3.2) 85.1 (-4.0) 71.5 (-0.5) 54.0 (-2.1) w/ MLP 113.65 81.3 (-0.3) 62.0 (-3.4) 63.9 (-3.0) 85.2 (-4.0) 70.9 (-0.6) 54.3 (-2.0) w/ MLP & FP.[24] 80.10 79.6 (-1.6) 61.9 (-2.4) 58.5 (-7.6) 84.4 (-4.7) 69.4 (-1.7) 52.2 (-2.1) for better performance.(4) We remove the spatial schema from LLaVA-UHD.The performance degradation demonstrates the effectiveness and necessity of spatial schema in informing the dynamic slice positions for LMMs.\n\nLLaVA-UHD generalizes to images with extreme aspect ratios.We further investigate the generalization capability of LLaVA-UHD by constructing an extended version of existing benchmarks.Specifically, we expand the aspect ratio of an image by doubling the length of its longer side through padding.From the results in Table 3, we can see that the advantage of LLaVA-UHD increases as compared with LLaVA-1.5 and alternatives.The reason is that LLaVA-UHD perceives images in native aspect ratios.In comparison, LMMs that encode images in fixed aspect ratios will suffer from significant distortion in the content shapes.Moreover, this also causes the computation to be unevenly distributed along the width and height of the image content.\n\nInstruction-tuning ViT parameters is sufficient for adaptation.We investigate the effect of tuning ViT parameters at different training stages, including pretraining and instruction-tuning.From the results in Table 4, we observe that: (1) Updating ViT during instruction-tuning is sufficient to achieve good performance.",
            "score": 0.3695719917898276,
            "section_title": "Analytic Results",
            "char_start_offset": 24108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 340
                },
                {
                    "start": 340,
                    "end": 481
                },
                {
                    "start": 483,
                    "end": 542
                },
                {
                    "start": 542,
                    "end": 667
                },
                {
                    "start": 667,
                    "end": 778
                },
                {
                    "start": 778,
                    "end": 904
                },
                {
                    "start": 904,
                    "end": 974
                },
                {
                    "start": 974,
                    "end": 1098
                },
                {
                    "start": 1098,
                    "end": 1216
                },
                {
                    "start": 1218,
                    "end": 1281
                },
                {
                    "start": 1281,
                    "end": 1407
                },
                {
                    "start": 1407,
                    "end": 1538
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.153076171875
        },
        {
            "corpus_id": "272986978",
            "title": "MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning",
            "text": "This section covers the methodology used to report results for Phi-3-Vision [3], LLaVA-OneVision [74], InternVL2 [21] and MiniCPM-V2 [169]. When available, we reported the results published by the original authors, either in their technical reports or on public leaderboards 8 . When not available, we implemented inference runners using publicly released checkpoints. Commonly, we followed [72]'s implementations that we adapted on our own internal fork of lm-eval-harness [36,118]. To verify the validity of our inference implementations, we ensured we could reproduce previously published results within standard deviation. Below, we share details for each model implementation:",
            "score": 0.369523794336434,
            "section_title": "A.7 Methodology for Running Competitor Models",
            "char_start_offset": 61668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 681
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2078857421875
        },
        {
            "corpus_id": "262054900",
            "title": "An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models",
            "text": "The experiments are conducted to answer three research questions. \n\n1 Which scaling factor matters? We study the relative contribution of three scaling-up factors to the performance improvement of LLaVA. The results are summarized in Table 3 (a). \n\n\u2022 Model size. Increasing the model size consistently improves the overall performance. We conjecture that larger data size is essential to train a larger model. For example, if we only train on LLaVA-80K data, we see smaller gain when model size becomes larger. \u2022 Image resolution. By fixing the CLIP ViT image encoder, we compare the variants that are pre-trained to take image resolution 224\u00d7224 and 336\u00d7336, and find that the higher resolution consistently yields 2-3 points improvement across all four LLM sizes. \u2022 Data mixing. Larger models tend to have higher capability of fitting the instruction data. \n\nBy mixing the language-only instruction data (ShareGPT) with LLaVA-80K, we can improve model performance by 2 points, compared to training on multimodal instruction data only. \n\nIn Table 3 (b), we present our result on MM-Bench [13], which contains a set of 2,974 questions, which evaluate models' reasoning skills of six categories. The combination of the three factors improve the baseline LLaVA 7B model, reported in [13]. \n\n2 When should the parameter-efficient training method be considered? As model size increases, it becomes necessary to consider using tuning methods that are more efficient than fullmodel fine-tuning. LoRA and QLoRA are well-known parameter-efficient tuning methods. As shown in Table 4, we report compute cost using GPU hours per node, because the unit can be equivalent to the price $13.63/hour (ND A100 v4 series) on Azure 7 . The total cost can be estimated by multiplying the #hours and #epochs. \n\nIn Table 4(a), we train both the 33B and 65B model with LoRA rank 8 and 64 for 1 epoch on the LLaVA-80K instruction-tuning dataset. For models with 33B parameters and above, as we increase the LoRA rank values, we notice an increase in both performance and cost until full-model tuning reaches its maximum performance for a specific model size.",
            "score": 0.3694556770803329,
            "section_title": "Scaling up LLaVA",
            "char_start_offset": 7403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 68,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 246
                },
                {
                    "start": 249,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1788
                },
                {
                    "start": 1791,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2135
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.180419921875
        },
        {
            "corpus_id": "271534005",
            "title": "LLaVA-Read: Enhancing Reading Ability of Multimodal Language Models",
            "text": "{\"class\": \"C\", \"DLN\": \"1132456789\", \"DOB\": \"08/23/1971\", \"Name\": \"Ima Cardholder\", \"Address\": \"Anytown, State, Zip\", \"EXP\": \"08/23/2014\", \"ISS\": \"California\", \"SEX\": \"F\", \"HGT\": \"5'5\", \"WGT\": \"123\", \"EYES\": \"Brown\", \"HAIR\": \"Brown\", \"DONOR\": \"N/A\"} LLaVA-Read {\"class\": \"C\", \"DLN\": \"1234568\", \"DOB\": \"08/31/1977\", \"Name\": \"Ima Cardholder\", \"Address\": \"2570 24th Street, Anytown, CA 95818\", \"EXP\": \"08/31/2014\", \"ISS\": \"08/31/2009\", \"SEX\": \"F\", \"HGT\": \"5-05\", \"WGT\": \"125\", \"EYES\": \"Brown\", \"HAIR\": \"Brown\", \"DONOR\": \"VETERAN\"} text are embedded in images. In addition, LLaVA-Read with combined higher resolution encoder (i.e., LLaVA-Read-H) further improves the performance of the model, especially on ChartVQA and TextVQA. For ChartVQA, adding layout information improves 30% performance improvement in terms of QA accuracy. When adding high-resolution visual encoders, the model performance improves further by about 20%. The layout information within a chart image is too complex to reconstruct with a heuristic function, and a high-resolution visual encoder can help in this case. For TextVQA, it shows the importance of visual encoders in scene text understanding as the performance becomes better as the resolution of visual encoders increases. This observation is consistent with what we find in Section 4.1.",
            "score": 0.36905701081461495,
            "section_title": "Main Results",
            "char_start_offset": 26682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1315
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.110107421875
        },
        {
            "corpus_id": "274788477",
            "title": "SPHERE: Unveiling Spatial Blind Spots in Vision-Language Models Through Hierarchical Evaluation",
            "text": "We describe the experimental setup used to evaluate the performance of state-of-the-art VLMs on SPHERE tasks. All experiments were performed on a NVIDIA GeForce RTX 3090 GPU. Models: Our evaluation encompasses two model categories: general-purpose VLMs and VLMs with enhanced spatial capabilities. The generalpurpose VLMs evaluated are Phi-3.5-Vision (Abdin et al., 2024), LLaVA-NeXT (Liu et al., 2024), LLaVA-OneVision (Li et al., 2024a), Instruct-BLIP (Dai et al., 2023), Idefics2 (HuggingFaceM4, 2024), InternVL2.5 (Chen et al., 2024b), Qwen-VL (Bai et al., 2023), Qwen2-VL (Wang et al., 2024), Llama-3.2-Vision (Meta, 2024), Gemini 2.0 Flash (Google, 2024), and GPT-4o (OpenAI, 2024). The VLMs specifically trained for spatial understanding and reasoning are SpatialBot (Cai et al., 2024), SpaceMantis (AI, 2024b), and SpatialRGPT (Cheng et al., 2024). For SpatialRGPT, we use the RGB-only version as the model requires highly accurate mask proposals to make use of depth maps. All model implementations follow their released versions on Hugging Face. Metrics: We computed both the validity and accuracy of the VLM responses. Since the VLM generative responses are open-ended, we deem a response as valid if it is a relevant answer to the question, that is, a number is given for a counting question and the response contains one of the options for a multiple-choice question (MCQ). Accuracy measures the correctness of the model responses, and invalid responses are treated as incorrect. For MCQs, we set the baseline accuracy for each question as 1/#options, since this is the probability of selecting the correct answer purely by random guessing.",
            "score": 0.36819835260611655,
            "section_title": "Setup",
            "char_start_offset": 14294,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1653
                }
            ],
            "ref_mentions": [
                {
                    "start": 454,
                    "end": 472,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 518,
                    "end": 538,
                    "matchedPaperCorpusId": "266521410"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3251953125
        },
        {
            "corpus_id": "272593374",
            "title": "LIME: Less Is More for MLLM Evaluation",
            "text": "Comparing the overall scores of LIME and Origin benchmarks, we observe that certain model series, such as Cambrian and LLaVA-1.5, experience a decline in overall scores. Conversely, the CogVLM and LLaVA-OneVision series show an improvement, with CogVLM2 and XComposer-4KHD experiencing significant increases of 4% and 6%, respectively. \n\nTab 6 provides more detailed experimental results. Regarding caption subtasks, most models demonstrate good performance. These tasks involve generating or assessing descriptions of the content in images, which indicates that current MLLMs possess strong image content recognition capabilities. \n\nAs for the VQA task, current MLLMs perform relatively well on TextVQA, ChatQA, and ScienceQA, where the questions directly ask about facts in the picture. However, their performance is relatively lower on OK-VQA, infoVQA, and AI2D, which require additional commonsense knowledge or complex reasoning to answer the questions. This demonstrates that current MLLMs exhibit significant image content recognition capabilities but are limited in their ability to perform complex reasoning using additional knowledge. We believe this limitation may be due to constraints in the language model component of MLLMs.",
            "score": 0.36815976267172473,
            "section_title": "MAIN RESULT",
            "char_start_offset": 12745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 335
                },
                {
                    "start": 338,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1239
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275634765625
        },
        {
            "corpus_id": "272770508",
            "title": "FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs",
            "text": "We show the hallucination comparison of the seven mainstream LVLMs on our FIHA-v1 in Table 3. From this Table, we have several observations. 1) It's worth highlighting that GPT-4V excels in both image and caption Q&A pairs, achieving the best performance among the evaluated models. 2) The second-best performer is InstructBLIP, which significantly outperforms other models except GPT-4V across most metrics. 3) Additionally, we have observed that model parameters are also significant factors affecting performance. For instance, LLaVA-1.5-13B provides a more comprehensive improvement over the LLaVA-1.5-7B. \n\nIn addition, we also show the performance of 7 main- stream LVLMs on FIHA-v1 based on the Visual Genome dataset. The results show a similar trend as compared to the performance in MSCOCO datasets. Specifically, the GPT-4V performs best and MiniGPT-4 performs the worst. LLaVA-1.5-13B performs better than LLaVA-1.5-7B, which also indicates that the model parameter size influences the performance.",
            "score": 0.36771651232631997,
            "section_title": "Overall Results on Datasets Generated by FIHA",
            "char_start_offset": 13618,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1009
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2406005859375
        },
        {
            "corpus_id": "268032058",
            "title": "Cross-Modal Projection in Multimodal LLMs Doesn't Really Project Visual Attributes to Textual Space",
            "text": "For each example, we randomly shuffle the order of classes inside <classes_string> to avoid any position bias. We fine-tune the projection layers of the LLaVA-1.5 model for 1 epoch using the default hyper-parameters (Liu et al., 2023b). During inference, we perform zero-shot classification using the same prompt above for the MLLM with the updated projection. \n\nSetting 2: Fine-tuning the MLLM end-to-end. Alternatively, we fine-tune all the MLLM parameters, i.e., the projection layers and the LLM parameters concurrently by maximizing the next token- prediction likelihood of the MLLM. In other words, we update both \u03d5 and \u03b8, where \u03b8 denotes the LLM paramters. We use the same strategy to construct X a and X q as in the previous setting. Again, we fine-tune the LLaVA-1.5 model for 1 epoch using the default hyper-parameters. Similar to the above setting, after training the MLLM, we perform zeroshot domain-specific image classification using the X q constructed above. We fine-tune the MLLM using these 2 strategies for each of the 4 datasets from different domains. Image datasets. The 4 image classification datasets correspond to the following tasks: leaf disease classification, visual texture detection, skin disease identification, and humanitarian category classification. Figure 3 1 shows the image classification performance (macro-averaged F 1 scores and accuracy) of the MLLMs under various settings. For reference, we include zero-shot classification performance of CLIP4 , which is the visual encoder of the LLaVA-1.5 model (see Appendix A.1 for details). First, it is worth noting that the zero-shot performance of the original LLaVA-1.5 model is notably worse than CLIP's zero-shot performance. This indicates that while domain-specific image attributes are present in the pre-projection image embeddings that are obtained from a frozen vision encoder (i.e., X v ), they are not being used by the MLLM parameters. This can be attributed to the corpus used to train MLLMs like LLaVA, which comprises natural images.",
            "score": 0.36765240829611434,
            "section_title": "Introduction",
            "char_start_offset": 5658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 360
                },
                {
                    "start": 363,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2035
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24365234375
        },
        {
            "corpus_id": "278502509",
            "title": "DriveSOTIF: Advancing Perception SOTIF Through Multimodal Large Language Models",
            "text": "For image captioning benchmarking, we evaluate the Blip [32] and Blip2 models [33] (pre-trained on the COCO Caption dataset [26]) across various model sizes. For VQA tasks, we benchmark a total of five open-sourced models, including LLaVA 1.5 [34], LLava-next [35], LLaVA-Video [36], Idelics2 [37] and newly released Qwen2-VL [38] that is developed based on Qwen2 [39]. These models are classified by architecture, model size, and quantization method, focusing on three specific variants: 2B, 7/8B, and 13/34B. The 2B variant is particularly suitable for mobile device deployment due to its lightweight and performance. The 7/8B model is well-suited for on-vehicle computing, where there is a need for both performance and efficiency. The 13/34B variant, due to its larger size and resource requirements, is more appropriate for cloud-based deployments. The model benchmarking and fine-tuning presented are completed on a desktop equipped with an Intel i9-10900K, 128GB of memory, and two Nvidia RTX 3090 GPUs.",
            "score": 0.3669916771167652,
            "section_title": "A. Benchmarking",
            "char_start_offset": 18528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1010
                }
            ],
            "ref_mentions": [
                {
                    "start": 56,
                    "end": 60,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 78,
                    "end": 82,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2705078125
        },
        {
            "corpus_id": "273638057",
            "title": "GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing",
            "text": "Table 1 presents results across various models and scoring metrics. All experiments were run on a single 40GB GPU, optimizing LoRA configurations and pruning ratios for a balance of accuracy and memory efficiency. Although larger VLMs may perform better, they require substantially more computational resources. \n\nSince prior work lacked image descriptions or did not employ the fMoW dataset for captioning, we evaluate Video-LLaVA and LLaVA-NeXT-Video as zero-shot baselines. Without fine-tuning, these models performed poorly across all metrics and failed to capture meaningful semantic differences, even under BERT-based evaluation. \n\nWe then applied few-shot fine-tuning with 10K (10%) and 100K samples using LoRA (r = 64, \u03b1 = 128), tuning 178M parameters. Performance improved notably, with the 100K setup achieving a BERT score of 0.864. \n\nTo enhance efficiency, QLoRA with 4-bit quantization reduced memory usage by 75% without sacrificing accuracy, achieving a BLEU score of 0.250, comparable to the LoRA-based model. \n\nWe pruned 5% of the model parameters to reduce size while preserving accuracy. However, pruning widened the performance gap between the 10K and 100K datasets, indicating a greater data requirement to offset pruning-induced degradation. Notably, the LLaVA-Next video model outperformed Video-LLaVA, thanks to its sparse structure, achieving a BERT score of 0.823-only 0.03 lower than the best result. In contrast, Video-LLaVA faced significant challenges with pruning due to its dense architecture, making it unsuitable for pruning. \n\nTable 2 summarizes our ablation study to validate the chosen hyperparameters. We explored several \u03b1 and r ratios to determine their impact on performance. Increasing the fine-tuned parameters to 1.7B by setting r = 640 and \u03b1 = 1280 did not yield significant performance gains, highlighting diminishing returns at higher parameter counts. Modifying the \u03b1 and r ratio to 4:1 also resulted in negligible improvements or degraded performance. Therefore, we adopted the r = 64 and \u03b1 = 128 configuration for subsequent experiments.",
            "score": 0.36631413476951646,
            "section_title": "Results & Discussion",
            "char_start_offset": 19377,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 311
                },
                {
                    "start": 314,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2087
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.408447265625
        },
        {
            "corpus_id": "268553875",
            "title": "PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model",
            "text": "With the release of GPT-V [35] and Gemini [47], more attention and efforts from open-source and research communities are shifting from large language models (LLM) to large multi-modal models (LMM).LLaVA [30], BLIP-2 [23], and Flamingo [1] are three representative works, where the core idea of both LLaVA and BLIP-2 is to map visual features into the input space of LLM to implement multi-modal capabilities, while Flamingo employs deeper feature fusion",
            "score": 0.3661971243335198,
            "section_title": "Large Multimodal Models",
            "char_start_offset": 5551,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 453
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 207,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 235,
                    "end": 238,
                    "matchedPaperCorpusId": "248476411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.234375
        },
        {
            "corpus_id": "272593236",
            "title": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks",
            "text": "We use LLaVA (Vicuna-7B) [53] and LLaVA (Llama-2-13B) [3] as the target LVLMs for evaluation. Both models utilize CLIP [34] ViT-L-14 as their vision encoder. However, they differ in their language decoders: LLaVA (Vicuna-7B) utilizes the Vicuna language model [25] with 7 parameters, whereas LLaVA (Llama-2-13B) [2] is built on LLaMA-2-13B-Chat, featuring 13 billion parameters. LLaMA-2-13B-Chat has undergone extensive instruction tuning and iterative RLHF on high-quality red-teaming data, making it more robust and resilient to jailbreak attacks. Additionally, by including LLaVA (Llama-2-13B) in our evaluation, we aimed to assess the scalability and effectiveness of our defense mechanism across larger LVLMs.",
            "score": 0.3661789768873527,
            "section_title": "B. LVLM models",
            "char_start_offset": 19533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 714
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 54,
                    "end": 57,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 119,
                    "end": 123,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.319580078125
        },
        {
            "corpus_id": "273025804",
            "title": "The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs",
            "text": "Furthermore, different MLLMs exhibit consistent trends in concept understanding. For instance, LLaVA-OneVision underperforms in natural attribute association, the same pattern is also observed with QWen2-VL and mPLUG-Owl3 models. Additionally, there is a significant disparity in deduction success rates among the different MLLMs. For example, Gemini-1.5-Flash performs well with \"push\", \"carry\", and \"break\" affordances, while the other MLLMs show relatively weaker capabilities. Conversely, Qwen2-VL excels in \"imprint\" affordance, and LLaVA-OneVision demonstrates strength in the 'clean' affordance, whereas Gemini-1.5-Flash struggles. \n\nComparison of Different Categories. Although single-step association shows consistent improvements across various concepts, performance differences emerge in specific categories. For instance, fresh, ripe, and rusty attributes gain an association success ratio close to 1, but natural and ripe attributes with a lower performance compared to the random success ratio of 0.5. This phenomenon is more pronounced in the deduction success ratio. Several model implementations exhibit strong deduction capabilities across different categories. For instance, Gemini-1.5-Flash achieves a 91.6 deduction success ratio in the 'painted' attribute. However, some categories still demonstrate lower performance, highlighting insufficient object understanding.",
            "score": 0.36605096374120927,
            "section_title": "\u2026 \uff1f \uff1f \uff1f",
            "char_start_offset": 39469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 638
                },
                {
                    "start": 641,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1388
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1331787109375
        },
        {
            "corpus_id": "272397804",
            "title": "Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems",
            "text": "The Large Language and Visual Assistant (LLaVA)( 15) is an open-source multimodal model that is designed to interpret and generate results based on both visual and textual input (10). It leverages the LLaMa ( 16) model and incorporates the pre-trained CLIP visual encoder for processing visual content. The encoder extracts visual features from input images and links them to lan-guage embedding through a trainable projection matrix, effectively translating visual features into language embedding tokens and bridging the gap between text and images. Although trained on smaller datasets than closed-source multimodal GPT models, Llava purports to demonstrate behavior analogous to the proprietary models. \n\nThe LLaVA-NeXT (an updated version of LLaVa) (6) focuses on enhancing multimodal instruction following capabilities on data generated to follow detailed visual and textual instructions, for interactive and complex visual tasks. In contrast, CLIP learns generalizable visual representations from large-scale natural language supervision aligning image and text embedding to enable zero-shot learning across diverse vision. \n\nThe LLaVA-NeXT model was also employed in this study for both classification tasks with different task-specific instructions. We first selected the initial prompt to query the model to generate the description of each image. To get the desired output as a discrete class name, we further queried the model with the generated description and follow-up prompt to get output as class names. \n\nThe five different initial prompts that were adopted to generate the description of congested/noncongested dataset are as follows: \n\nP1: Classify whether highway lanes are congested or not in the image. P2: Classify whether highway lanes are congested or not in the image. P3: Classify whether in the image highway lanes are congested or not. P4: Classify whether the highway have congested lane or free-lane in the image. P5: Check whether the highway lanes are congested or not in the image. \n\nThe follow-up prompt selected corresponding to each of these initial prompts were",
            "score": 0.3657224266664957,
            "section_title": "LLaVA model",
            "char_start_offset": 12561,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 706
                },
                {
                    "start": 709,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2016
                },
                {
                    "start": 2019,
                    "end": 2100
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4833984375
        },
        {
            "corpus_id": "277151246",
            "title": "Exploring the Hidden Reasoning Process of Large Language Models by Misleading Them",
            "text": "This suggests that the model indeed abstracts and interprets specific image inputs, and may leverage the original abstraction mechanism of the language model. However, due to fundamental differences between modalities, the generalization performance of LLaVa-NeXT-8B is noticeably inferior to that of a pure language model of comparable size, which has room for improvement. Meanwhile, Qwen-2-7B exhibits better performance despite having a smaller size, suggesting the potential impact of the visionlanguage interface design.",
            "score": 0.36545972962600204,
            "section_title": "MisFT on VLMs",
            "char_start_offset": 25869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 526
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.199462890625
        },
        {
            "corpus_id": "269042942",
            "title": "DesignQA: A Multimodal Benchmark for Evaluating Large Language Models' Understanding of Engineering Documentation",
            "text": "MLLMs in our evaluation: four closed-source models -OpenAI's gpt-4o (GPT-4o) [16], OpenAI's gpt-4-1106-vision-preview (GPT-4) [1], Google AI's models/gemini-1.0-pro-vision (Gemini-1.0) 5, and Anthropic's claude-3-opus-20240229 (Claude-Opus) 6 -and one open-source model: llava-1.5-13b (LLaVA-1.5) [17]. Note that all future references to GPT-4o, GPT-4, Gemini-1.0, Claude-Opus, or LLaVA-1.5 refer to these specific model versions unless otherwise stated. GPT-4, Gemini-1.0, and Claude-Opus were originally selected for their strong performances as closed-source models on existing multimodal benchmarks [9,14]. For example, Gemini-1.0 Ultra, Claude-Opus, and GPT-4V(ision) were the top 5https://ai.google.dev/gemini-api/docs/models/gemini?authuser=1 6https://www.anthropic.com/news/claude-3-family three models, as of April 2024, on the MMMU benchmark, which consists of 11.5k multimodal questions from college curriculum content [24]. GPT-4o was released towards the end of the time of this work. It holds the 1st place spot on the MMMU leaderboard [24], as of August 2024. It was added to our closed-source model evaluations both because of this notable performance and because of potential for comparison with GPT-4. In this way, DesignQA can be used to continue to measure progress of models as they develop over time. LLaVA-1.5 is selected because of its promise (and the promise of its derivatives) as an open-source MLLM [41].",
            "score": 0.3654017037510981,
            "section_title": "MLLM Models. We consider five recently developed",
            "char_start_offset": 40464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1433
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23095703125
        },
        {
            "corpus_id": "270869574",
            "title": "Curriculum Learning with Quality-Driven Data Selection",
            "text": "We use the LLaVA-v1.5-7B[25] architecture with model weights fully fine-tuned using LLaVA-1.5-mix-665kdata.Subsequently, we fine-tune this model with LoRA [14] during the follow-up experiments.In training, we keep the visual encoder, projector, and LLM weights frozen, and maximize the likelihood of with trainable parameters of LoRA only.We keep the rest of the training protocol the same to allow for a fair comparison.Scenario 1, which only includes LoRA tuning, takes approximately 16 hours on an NVIDIA Tesla A100 GPU with 40GB of memory, using DeepSpeed ZeRO Stage 3. We use the SVIT-core-157K [39] dataset for continuous fine-tuning to establish a baseline.And the same method is applied to fine-tune our data.[12], GQA [15], VisWiz [13], ScienceQA-IMG [27], TextVQA [33].More details can be found in the Evaluation Metrics section of the Appendix.\n\nWe report our main results in Table 1.Our method, using only 7000 samples of SVIT-core-157K, achieved higher performance across all benchmarks compared to the full data experiment setup.\n\nFurthermore, it surpassed the base model on SQA [27] and VisWiz [13], reaching state-of-the-art (SOTA) performance.In the efficient LoRA training setup, our data exceeded SVIT-core-157K [39] by 4.7 points in GQA [15], 2.0 points in VQAV2 [12], 1.0 point in TextVQA [33], 2.0 points in VisWiz [13], and 0.5 points in SQA [27].The improvements verify the better training effects of our data since less data amount and same model are used.",
            "score": 0.3649859137008725,
            "section_title": "Scenario 1: Training from LLaVA",
            "char_start_offset": 11952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 24,
                    "end": 102
                },
                {
                    "start": 102,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 421
                },
                {
                    "start": 421,
                    "end": 664
                },
                {
                    "start": 664,
                    "end": 717
                },
                {
                    "start": 717,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 855
                },
                {
                    "start": 857,
                    "end": 895
                },
                {
                    "start": 895,
                    "end": 1043
                },
                {
                    "start": 1045,
                    "end": 1160
                },
                {
                    "start": 1160,
                    "end": 1370
                },
                {
                    "start": 1370,
                    "end": 1481
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 717,
                    "end": 721,
                    "matchedPaperCorpusId": "8081284"
                },
                {
                    "start": 727,
                    "end": 731,
                    "matchedPaperCorpusId": "152282269"
                },
                {
                    "start": 740,
                    "end": 744,
                    "matchedPaperCorpusId": "3831582"
                },
                {
                    "start": 760,
                    "end": 764,
                    "matchedPaperCorpusId": "252383606"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 1093,
                    "end": 1097,
                    "matchedPaperCorpusId": "252383606"
                },
                {
                    "start": 1109,
                    "end": 1113,
                    "matchedPaperCorpusId": "3831582"
                },
                {
                    "start": 1257,
                    "end": 1261,
                    "matchedPaperCorpusId": "152282269"
                },
                {
                    "start": 1283,
                    "end": 1287,
                    "matchedPaperCorpusId": "8081284"
                },
                {
                    "start": 1310,
                    "end": 1314,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 1337,
                    "end": 1341,
                    "matchedPaperCorpusId": "3831582"
                },
                {
                    "start": 1365,
                    "end": 1369,
                    "matchedPaperCorpusId": "252383606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1878662109375
        },
        {
            "corpus_id": "268857227",
            "title": "LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model",
            "text": "In this paper, we introduce LLaVA-Gemma, a suite of vision-language assistants trained from the Gemma Large Language Model (LLM) variants, Gemma-2B and Gemma-7B [17].Our work is inspired by the rapid progress in small but capable visual language models (VLMs), such as LLaVA-Phi [23], which have demonstrated remarkable efficiency and effectiveness in various language understanding tasks.LLaVA-Gemma distinguishes itself among small VLMs due to the public release of similarly trained, different-sized LLMs Gemma-2B and Gemma-7B.\n\nThe unique release of the Gemma models offers an opportunity to contrast model performance in relation to pa-rameter size and visual encoding capabilities.By possessing two variants with different parameter sizes, LLaVA-Gemma allows researchers to investigate the trade-offs between computational efficiency and the richness of visual and linguistic understanding.With these two variants, we perform a deeper exploration of how varying levels of model complexity influence the effectiveness of visual encoding, providing valuable insights into the optimization of small VLMs for diverse tasks and environments.Furthermore, the use of significantly more unique tokens, at 256k, offers an opportunity to investigate how a massively increased token set effects multi-modal performance.\n\nRecent advancements in (LLMs) [20] and multimodal foundation models (MMFMs) [7] have propelled the interest and development of Large Multimodal Models (LMMs).Notable models like GPT-4 [1], LLaVA [9,10], and their derivatives have demonstrated significant performance in vision-language tasks such as Visual Question Answering (VQA) and image captioning [5].However, the computational demands of deploying these models have led to the exploration of small-scale LMMs.Our work aims to provide a unified analysis of small-scale LMMs, examining how model selections, training recipes, and data contribute to performance, which is distinct from existing works such as LLaVA-Phi.\n\nOur contributions are as follows:\n\n1. We introduce LLaVA-Gemma, a MMFM that leverages the compact yet powerful Gemma language models for efficient multimodal interactions.",
            "score": 0.3643696642728001,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 389
                },
                {
                    "start": 389,
                    "end": 530
                },
                {
                    "start": 532,
                    "end": 687
                },
                {
                    "start": 687,
                    "end": 896
                },
                {
                    "start": 896,
                    "end": 1142
                },
                {
                    "start": 1142,
                    "end": 1314
                },
                {
                    "start": 1316,
                    "end": 1474
                },
                {
                    "start": 1474,
                    "end": 1673
                },
                {
                    "start": 1673,
                    "end": 1782
                },
                {
                    "start": 1782,
                    "end": 1989
                },
                {
                    "start": 1991,
                    "end": 2024
                },
                {
                    "start": 2026,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 1346,
                    "end": 1350,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1514,
                    "end": 1517,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1669,
                    "end": 1672,
                    "matchedPaperCorpusId": "152282269"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5009765625
        },
        {
            "corpus_id": "277043430",
            "title": "FastVID: Dynamic Density Pruning for Fast Video Large Language Models",
            "text": "Video Large Language Models have shown impressive capabilities in video comprehension, yet their practical deployment is hindered by substantial inference costs caused by redundant video tokens. Existing pruning techniques fail to fully exploit the spatiotemporal redundancy inherent in video data. To bridge this gap, we perform a systematic analysis of video redundancy from two perspectives: temporal context and visual context. Leveraging this insight, we propose Dynamic Density Pruning for Fast Video LLMs termed FastVID. Specifically, FastVID dynamically partitions videos into temporally ordered segments to preserve temporal structure and applies a density-based token pruning strategy to maintain essential visual information. Our method significantly reduces computational overhead while maintaining temporal and visual integrity. Extensive evaluations show that FastVID achieves state-of-the-art performance across various short- and long-video benchmarks on leading Video LLMs, including LLaVA-OneVision and LLaVA-Video. Notably, FastVID effectively prunes 90% of video tokens while retaining 98.0% of LLaVA-OneVision's original performance. The code is available at https://github.com/LunarShen/FastVID.",
            "score": 0.3640465564494374,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.361572265625
        },
        {
            "corpus_id": "277251189",
            "title": "CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused Text-to-Image Generation",
            "text": "Automatic captioning. After the initial filtering, we ran automatic captioning for the collected data using the LLaVa-NeXT model [44]. To obtain the high quality captions, we varied the following hyperparameters: \n\n\u2022 Maximum number of output tokens: from 100 to 1024. Setting a small value resulted in a truncated description, or significant elements of the image were not revealed. We chose 512 tokens empirically. \n\nTable 2: Comparison of captions created by LLaVa-Next 34B [44] or people. The model quite often makes factual mistakes and does not know the titles and proper names.",
            "score": 0.3640335604639432,
            "section_title": "Captioning",
            "char_start_offset": 14039,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 212
                },
                {
                    "start": 215,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 583
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.287353515625
        },
        {
            "corpus_id": "274116880",
            "title": "Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization",
            "text": "Following previous works (Chen et al., 2023;Zhu et al., 2024;Pi et al., 2025), we select the LLaVA-v1.5-7B (Liu et al., 2024a) and LLaVA-v1.5-13B as base models for experiments, which allows for easy comparison with other existing works. The LLaVA's weights are pretrained and further finetuned using supervised fine-tuning (SFT) before applying our hallucination-targeted direct preference optimization. During the training phase, we employ Zero stage-3 optimization and use Vicuna-7B/13B and CLIP-VIT-L-336px as our LLM and vision encoder, respectively. The training is conducted with 2 epochs with a batch size of 64, a learning rate of 2e-6, weight decay as 0, LoRA rank as 64, and a beta value of 0.1. All experiments are run on one single machine with 8 A800 GPUs. \n\nThe total training time is 3 hours for LLaVA-v1.5-7B and 4 hours for LLaVA-v1.5-13B.",
            "score": 0.3625330006222525,
            "section_title": "Training details",
            "char_start_offset": 18618,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 857
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 61,
                    "matchedPaperCorpusId": "269157480"
                },
                {
                    "start": 61,
                    "end": 77,
                    "matchedPaperCorpusId": "268379605"
                },
                {
                    "start": 107,
                    "end": 126,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.295654296875
        },
        {
            "corpus_id": "269148679",
            "title": "MM-PhyQA: Multimodal Physics Question-Answering with Multi-image CoT Prompting",
            "text": "For all variants of LLaVA-1.5 that were tested, there was an increase in the accuracy score when MI-CoT Prompting was employed as seen in Figure 4a except in the case of LLaVA-1.5 7b model.A smaller number of trainable parameters meant that the model was not able to process the more complex multi-image input, leading to a sharp dip in the performance.The difference was the most significant in the case of LLaVA-1.5 13b trained with LoRA as 128, which also gave the best performance out of all the models tested when trained using MI-CoT Prompting.The MI-CoT Prompting trained version also exhibited high rouge scores as seen in Table 2.It can be observed from Figure 4b that the rouge scores were higher in the LLaVA-1.5 13b CoT variants, showcasing the fact that models that were able to leverage the MI-CoT prompt also showed a bump in the reasoning capabilities.A marked improvement in all metrics, when multiple images were provided in the prompt in the case of LLaVA-1.5 13b variants, provides evidence that the models were able to segregate and recognize the image that has to be used for each question present in a single prompt.",
            "score": 0.3624932597082431,
            "section_title": "Effect of Chain of Thought Prompting",
            "char_start_offset": 15209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 189,
                    "end": 353
                },
                {
                    "start": 353,
                    "end": 550
                },
                {
                    "start": 550,
                    "end": 639
                },
                {
                    "start": 639,
                    "end": 868
                },
                {
                    "start": 868,
                    "end": 1139
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29296875
        },
        {
            "corpus_id": "274436668",
            "title": "COAP: Memory-Efficient Training with Correlation-Aware Gradient Projection",
            "text": "Experimental Settings. We apply our method to a recent state-of-the-art Large Multimodal Model (LMM): LLaVA-v1.5-7B [27]. LLaVA connects the pre-trained CLIP ViT-L/14 [39] visual encoder and the large language model Vicuna [67] using a simple projection matrix for feature alignment. We perform fine-tuning on LLaVA based on this setup. We train the model using task-specific fine-tuning on the ScienceQA [33] dataset for 12 epochs with a batch size of 16 and a learning rate of 2 \u00d7 10 \u22125 on 1\u00d7A100 GPU. \n\nComparison Results. Table 6 compares the results of fine-tuning LLaVA-7B on the ScienceQA dataset using different optimizers. In a single GPU environment, training large models directly with the AdamW optimizer can lead to Out-of-memory (OOM) errors due to the significant GPU memory consumption by optimizer states. Although Deep-Speed's CPU-offload [41] feature can alleviate GPU memory pressure by migrating optimizer states to CPU memory, it significantly increases training latency due to frequent data transfers. Compared to DeepSpeed and GaLore, our training speed is improved by 6\u00d7 and 4\u00d7, respectively,  with accuracy improvements of 9.9% and 1.2%. Compared to LoRA, our training speed is 1.4\u00d7 faster while achieving the same performance without increasing the model size.",
            "score": 0.36229411986105325,
            "section_title": "Fine-tuning LLaVA-7B",
            "char_start_offset": 24587,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1287
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 120,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 167,
                    "end": 171,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 223,
                    "end": 227,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 405,
                    "end": 409,
                    "matchedPaperCorpusId": "252383606"
                },
                {
                    "start": 857,
                    "end": 861,
                    "matchedPaperCorpusId": "231632857"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.357666015625
        },
        {
            "corpus_id": "273162281",
            "title": "MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents",
            "text": "The LLaVA architecture itself is compatible with using multiple images as input, but the released model weights do not have the ability to handle multiple images. In other words, when you input multiple images, it will only focus on the contents of the first image. Therefore, we fine-tune LLaVA through multi-image captioning data to enable it to understand multiple images.",
            "score": 0.36205167040824504,
            "section_title": "E.1 MULTIPLE IMAGE INPUT IN LLAVA",
            "char_start_offset": 35700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 375
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.414306640625
        },
        {
            "corpus_id": "271039930",
            "title": "Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition",
            "text": "Figure 1 shows the distances between humanwritten stories and the stories generated by the models (the lower the better). Examples of modelgenerated stories are provided in Figure 2. In Figure 1, we observe that the stories generated by LLaVA obtain the best overall value (d HM = 0.099), followed by TAPM (d HM = 0.15). We notice that GLAC Net-generated stories exhibit the lowest distance regarding the repetition dimension. \n\nWe attribute this to GLAC Net's inference phase decoding heuristic, which penalises repetitive expressions (see Section 4.2). BLIP-2 stories are overall the farthest from stories written by humans. \n\nRegarding the two best-performing models, LLaVA and TAPM, two points are worth highlighting. First, despite a huge difference in model size-LLaVA is a powerful 7.5B parameter foundation model, while TAPM is 50 times smaller-TAPM's d HM value is only slightly higher than LLaVA's. Second, LLaVA outperforms TAPM with respect to coherence and visual grounding. We hypothesize that this advantage is due to LLaVA's more powerful language and vision backbone models. In the next section, we leverage the modular architecture of TAPM to test this hypothesis.",
            "score": 0.362046537550533,
            "section_title": "Results",
            "char_start_offset": 17841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 626
                },
                {
                    "start": 629,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1182
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.359619140625
        },
        {
            "corpus_id": "267617068",
            "title": "LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education",
            "text": "In Phase 6, our assessment focuses on LLaVA-Docent Version 2 about GPT-4. It is essential to recognize that LLaVA-Docent operates with 13 billion parameters, a significant factor in its performance. In comparison, GPT-4 is believed to be a larger model than LLaVA-Docent. This inference stems from its exceptional generative capabilities and its comparison to its predecessors, GPT-3 and GPT-3.5, which have 175 billion parameters (OpenAI, 2023). Therefore, note that in analyzing the results of the comparison between LLaVA-Docent and GPT-4, it is crucial to consider the disparity in the number of parameters, which could significantly influence their respective performances.",
            "score": 0.3618875169921253,
            "section_title": "Phase 6: evaluation result of LLaVA-Docent Version 2",
            "char_start_offset": 28534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 678
                }
            ],
            "ref_mentions": [
                {
                    "start": 431,
                    "end": 445,
                    "matchedPaperCorpusId": "143522942"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27880859375
        },
        {
            "corpus_id": "274023112",
            "title": "VCBench: A Controllable Benchmark for Symbolic and Abstract Challenges in Video Cognition",
            "text": "We evaluate 10 popular open-source LVLMs finetuned on video question-answer tasks, including MiniCPM-V [59], Video-LLaMA2 [6], Intern-Video2 [52], Video-LLaVA [29], LLaVA-NEXT- Video-34B [66], LLaVA-NEXT-Video-7B [66], InternLM-XComposer-2.5 [65], and Qwen2-VL models at different scales: Qwen2-VL-72B, Qwen2-VL-7B, and Qwen2-VL-2B [50]. Although InternVideo2 can encode audio, we standardize input across all video language models by extracting musical notes per second and converting them into text format. For fairness, all models are evaluated using their default settings. The prompts offer descriptions of scene tasks that incorporate abstract visual concepts. Additionally, following the setup [25], we use the prefix 'Best Option:' in the prompt. Further details on the models can be found in Appendix A.3.",
            "score": 0.3615472280196461,
            "section_title": "Setup",
            "char_start_offset": 8797,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 814
                }
            ],
            "ref_mentions": [
                {
                    "start": 701,
                    "end": 705,
                    "matchedPaperCorpusId": "265466214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.132568359375
        },
        {
            "corpus_id": "268875850",
            "title": "What Are We Measuring When We Evaluate Large Vision-Language Models? An Analysis of Latent Factors and Biases",
            "text": "Considering that different VLMs may exhibit different training behaviors, our analysis uses four popular VLMs: BLIP-2 (Li et al., 2023c), MiniGPT-4 (Zhu et al., 2023), LLaVA (Liu et al., 2023c), and mPLUG-Owl (Ye et al., 2023), which have mostly not been exposed to the datasets in focus.As minor exceptions, BLIP-2 and MiniGPT-4 were pretrained on COCO Caption and Web CapFilt.mPLUG-Owl has been exposed to COCO Caption, and LLaVA was pretrained on the three LLaVA datasets.We avoid models that have been finetuned on many VQA datasets, such as InstructBLIP (Dai et al., 2023), LLaVA 1.5 (Liu et al., 2023b), and Qwen-VL (Bai et al., 2023a).\n\nFor each model, we fine-tune the parameters that are trainable during their respective visionlanguage pretraining.On each source task, we train for 10K steps with a batch size of 192 for BLIP-2 and 128 for MiniGPT-4, mPLUG-Owl, and LLaVA.Other model details and hyperparameters are in Appendices C and D.",
            "score": 0.36152606140094334,
            "section_title": "Setup",
            "char_start_offset": 17441,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 288,
                    "end": 378
                },
                {
                    "start": 378,
                    "end": 475
                },
                {
                    "start": 475,
                    "end": 642
                },
                {
                    "start": 644,
                    "end": 758
                },
                {
                    "start": 758,
                    "end": 882
                },
                {
                    "start": 882,
                    "end": 948
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2330322265625
        },
        {
            "corpus_id": "277634461",
            "title": "Are Vision-Language Models Ready for Dietary Assessment? Exploring the Next Frontier in AI-Powered Food Image Recognition",
            "text": "The core innovation lies in decoupling visual encoding, enhancing the model's ability to process and generate text from visual inputs8 [9]. For this study, DeepSeek Janus-Pro is accessed via Replicate's API 9 . \u2022 LlaVA: this is an end-to-end trained multimodal model that employs a fully-connected vision-language connector 10 [22]. \n\nFor this study, we use the LlaVA-v1.6-mistral-7b model via Replicate's API 11 . Leveraging Mistral-7B, LlaVA enhances multimodal text generation and image-based reasoning while balancing performance and computational cost.",
            "score": 0.3613480964552602,
            "section_title": "Selected Vision-Language Models",
            "char_start_offset": 14463,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 557
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3974609375
        },
        {
            "corpus_id": "261049617",
            "title": "StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data",
            "text": "Public multimodal benchmarks We perform quantitative performance comparisons against various state-of-theart methods on different benchmarks, as illustrated in Table 1. Utilizing LLaVA-1.5-13B as the baseline, we integrate our synthesized data with its original dataset for training. Training is carried out with identical parameter configurations as LLaVA-1.5. The outcomes demonstrate substantial improvements on many benchmarks, emphasizing the enhanced performance achieved by our approach. zh Multi-image benchmark To validate the effectiveness of multi-image capabilities, we manually curated a benchmark of real images. This test set assessed performance based on differences, similarities, and logical relations. The evaluation metric used was the GPT-4 score mentioned. We used LLaVA-13B as the baseline and incorporated multiimage data in the second training phase. Since LLaVA itself lacks the capability for multi-image input, we modified the testing code for LLaVA to enable it to accept multiple sets of images simultaneously. The comparison with LLaVA results is shown in the Table 2, indicating a notable improvement across various multi-image capabilities despite adding less multi-image data in the process. \n\nComparison of various abilities. To validate the effectiveness of our generated data, we conducted comprehensive tests on distinct capabilities using a meticulously designed testing benchmark. Employing LLaVA-13B as our baseline, the quantitative comparison of the baseline results and ours is shown in the left part of Figure 5. Notably, our trained model consistently outperforms the LLaVA-13B baseline across all various capabilities on real-image benchmarks, which suggests the synthesized datasets' generalizability and our pipeline's robustness. \n\nTo better validate our superiority, we conduct a comparison of subcategory performance on MMBench, using the LLaVA-1.5-13B as the baseline. The tested subcategories in MMBench encompass six aspects: attribute reasoning (AR), coarse perception (CP), fine-grained perception (cross-instance) (FP-C), fine-grained perception (instance-",
            "score": 0.36106947621159047,
            "section_title": "Quantitative comparison to state-of-the-arts",
            "char_start_offset": 22066,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2114
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.394287109375
        },
        {
            "corpus_id": "269293766",
            "title": "An empirical study of LLaMA3 quantization: from LLMs to MLLMs",
            "text": "Nevertheless, both QLoRA and IR-QLoRA introduce inference bottlenecks primarily due to the dequantization process, which results in an increase in inference latency. The trade-off between the reduced memory footprint and the slight increase in latency is often acceptable for deployment in resourceconstrained environments where memory is the limiting factor. Further optimizations, such as hardware-specific tuning and algorithmic improvements, could mitigate this bottleneck and improve overall inference speed. For the MLLM model, we follow a common practice by conducting post-training quantization on the LLaMA3 part [11,39]. As shown in Table 11 and Table 12, we compare the ultra-low bit-width performance of LLaVA-Next-8B under GPTQ and AWQ in six visual-language benchmarks. We initially evaluate the pure language capabilities of LLaVA-Next-8B, as illustrated in Table 11. The fp16 precision PPL metrics of the LLaMA3 model, after being fine-tuned for visual tasks, worsened across three datasets compared to its performance on language tasks. This also suggests that when fine-tuned for visuallanguage tasks, the introduction of image tokens leads to a partial loss and forgetting of LLaMA3's inherent language abilities. The language capabilities of multimodal LLMs (MLLMs) show a loss trend consistent with pure LLMs under low-bit quantization. Subsequently, we tested the quantized LLaMA3 within the MLLM model on visual QA tasks. As shown in Table 12, under several advanced PTQ methods, the 4-bit MLLM exhibits a loss of less than 2% on multi-modal benchmarks, efficiently performing visual-language tasks with reduced model size. \n\nAt 3 bits, the performance loss ranges from 5% to 20%, with the highest loss, 20.75%, occurring on the MME cognition task. Notably, regardless of GPTQ or AWQ, we observe that the 2-bit LLaVA-Next-8B completely collapses in the six multi-modal QA tasks, with scores dropping to zero. Although SliM-LLM mitigates the performance collapse of LLaVA-Next-8B at 2 bits, it still shows a large performance degradation.",
            "score": 0.3610136074319215,
            "section_title": "LoRA-FT Methods",
            "char_start_offset": 16007,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2060
                }
            ],
            "ref_mentions": [
                {
                    "start": 622,
                    "end": 626,
                    "matchedPaperCorpusId": "258999941"
                },
                {
                    "start": 626,
                    "end": 629,
                    "matchedPaperCorpusId": "266174746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.205078125
        },
        {
            "corpus_id": "274131643",
            "title": "TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models",
            "text": "We dive deeper into the design choices of TS-LLaVA in this section. More results can be found in the Appendix. Number of frames We vary the maximum number of input frames used in TS-LLaVA, and present the results in Fig. 4a. Since the total number of visual tokens remains constant at 3456, using fewer frames results in a lower compression rate. For datasets like NExT-QA, which do not emphasize long-term video understanding, the reduced compression rate effectively compensates for any missing information from fewer frames. In contrast, for EgoSchema, which specifically targets long-term understanding, using more frames proves beneficial. One interesting observation is that the 7B and 34B models behave differently to reduced frame counts. The performance of the 34B model keeps increasing as we increase the number of frames, indicating it can handle more information as we increase frame numbers. How many frames per thumbnail image? We conduct experiments with varying numbers of frames per thumb-  [26] Table 5. Results obtained on Multitask Benchmarks. We highlight the top-performing training-free methods and underline the bestperforming video LLMs overall. Methods below the dashed line (--) are training-free, while those above it have been trained on extensive video data. We denote performance better than or comparable to ( ) or lags behind ( ) the main competing training-based video LLM (on MVBench: PLLaVA-34B, which uses the same backbones as TS-LLaVA; on MLVU: Video-LLaMA2-72B). nail image. The results are presented in Fig. 4b. Since the resolution of each image is fixed at 336\u00d7336 for the image encoder, including more frames in the thumbnail image means lower resolution for each frame. The results show that changing the number of frames does not affect the performance on NExT-QA and IntentQA significantly. While for EgoSchema, which requires better temporal understanding and involves longer videos, is more sensitive to the number of frames per thumbnail image. Using 6 frames per thumbnail image shows clear benefit over the counterparts, by providing enough temporal cues and not losing too much details due to reduced resolution.",
            "score": 0.3606550333439177,
            "section_title": "Design Choices of TS-LLaVA",
            "char_start_offset": 20622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2166
                }
            ],
            "ref_mentions": [
                {
                    "start": 1009,
                    "end": 1013,
                    "matchedPaperCorpusId": "259108333"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26171875
        },
        {
            "corpus_id": "278339256",
            "title": "Mitigating Image Captioning Hallucinations in Vision-Language Models",
            "text": "VLMs have made significant progress in multimodal tasks, including image captioning and visual question answering (VQA). Proprietary state-of-the-art (SOTA) VLMs such as ChatGPT [5], Gemini [6], and Claude demonstrate impressive multimodal capabilities but are not open source, restricting their accessibility for research and customization. In contrast, open-source VLMs such as LLaVA [7] and InstructBLIP [8] offer competitive performance. Currently, VLMs are widely adopted for real-world applications. For instance, LLaVA has been effectively applied to extract financial information from check images [9]. Despite these advancements, VLMs are prone to hallucinations, where the generated output deviates from visual input, particular under distribution shifts between pretraining and real-world test data. \n\nCurrent approaches for mitigating hallucinations can be grouped into four categories: (1) Visual input enhancement, focusing on improving the quality of visual features fed into VLMs. For example, methods like LLaVA-Next [10] and InternVL [11] adopt a multi-scale strategy: dividing the input image into smaller patches, resizing these patches to higher resolutions to emphasize local details, and downscaling the original image to capture global context. These processed images are then concatenated to create a richer visual representation. Similarly, Prismer [12] incorporates auxiliary vision experts/submodels to further diversify visual inputs and enhance performance. While effective, these methods require complete retraining of the VLM, which is computationally expensive and impractical for refining already-deployed models. (2) Fine-tuning VLMs, typically on domain-specific datasets, has shown promise in mitigating hallucinations. For instance, the authors in [13] fine-tuned LLaVA [7] for medical applications using newly collected domain-specific VQA data, significantly improving performance. Similarly, the authors in [14] apply RL to fine-tune VLMs such as CLIPCap [15], focusing on training the vision-language projector (42 million parameters).",
            "score": 0.36045364620143505,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 3112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2077
                }
            ],
            "ref_mentions": [
                {
                    "start": 606,
                    "end": 609,
                    "matchedPaperCorpusId": "273498277"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.440185546875
        },
        {
            "corpus_id": "274166071",
            "title": "FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual Token Compression",
            "text": "We have selected three types of approaches for comparative reference, with their results presented in Tab. 1, separated by different categories. The first type consists of various mainstream MLLMs. The second type includes MLLMs that take high-resolution inputs. The third type focuses on works dedicated to compressing visual tokens, for which we have chosen their best outcomes. The final section presents our baseline and methods. Given that the foundational LLMs, training datasets, and training configurations used in current MLLM research vary, most of the results in Tab. 1 are not directly comparable. However, these results can serve as a reference to illustrate the approximate performance level of our implemented baseline and methods. In the third part, specifically for our self-implemented LLaVA-Next and FocusLLaVA, we ensured strict alignment in the foundational large models, training datasets, and training settings, making them fairly comparable. Our findings indicate that our implemented baseline is highly competitive compared to both mainstream and high-resolution MLLMs. Furthermore, our proposed FocusLLaVA demonstrates clear improvements over this baseline. Table 1. Comparison with existing MLLMs on popular benchmarks. VQA T : TextVQA [44]; SQA: ScienceQA [38]; LLaVA W : LLaVA-bench-in-the-wild; MME P,C : Perception and Cognition in MME [17]; MMB C denotes MMBench-CN [36]. 2x1, and 1x2. The results are presented in Tab. 3. Here, \"Baseline\" refers to our reproduction of the LLaVA-Next, \"3-Branch\" represents the default implementation of our method, and \"7-Branch\" indicates the extension to 7 visual scales. The results show that adding more scales leads to a notable 1.1% increase in TextVQA over the \"3-branch\". We attribute this enhancement to the fact that the TextVQA task requires the recognition of fine-grained information, such as text, and the inclusion of more scales aids the model in better handling these detailed elements.",
            "score": 0.3601468670323568,
            "section_title": "Performance",
            "char_start_offset": 19202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 1263,
                    "end": 1267,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 1284,
                    "end": 1288,
                    "matchedPaperCorpusId": "252383606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.263671875
        },
        {
            "corpus_id": "277510582",
            "title": "Re-thinking Temporal Search for Long-Form Video Understanding",
            "text": "Question Grounding: For Question Grounding, we primarily use the LLaVA-OneVision 7B model, applying it to 8 uniformly sampled frames. The prompt adheres to the official release guidelines, and the specific template used is listed in Table 12. \n\nIterative Temporal Search: The default configuration for the image grid size is b = 8\u00d78. We set the return threshold \u03b8 at 0.6 for object-based and training-based scoring functions as trade-off in Figure 6. For the attention-based method, we typically use the sum of the attention scores from the target object in the last layer of each frame. This approach was chosen because using smaller models or shallower layers resulted in performance below the baseline. Additionally, the process terminates after three iterations to manage the high computational costs associated with using the 72B model. Downstream Question Answering: For downstream task evaluations, we experiment with the most prominent stateof-the-art (SOTA) models, both open-and closed-source, namely GPT4o and LLaVA-OneVision 72B. For GPT4o, we use the official API. For LLaVA, we employ the official code. The prompt template for this testing is listed in Table 13.",
            "score": 0.359995482419606,
            "section_title": "D. Implementation Details D.1. Implementation of Training-free T*",
            "char_start_offset": 29734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 242
                },
                {
                    "start": 245,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1177
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.358642578125
        },
        {
            "corpus_id": "273963071",
            "title": "Aligned Vector Quantization for Edge-Cloud Collabrative Vision-Language Models",
            "text": "We evaluate LLaVA-AlignedVQ using an NVIDIA Jetson AGX Xavier as the edge device and a workstation equipped with an A100 GPU as the cloud server. To simulate various network conditions, we measure the inference latency of LLaVA-AlignedVQ's visual encoder under different transmission bandwidths, ranging from 0.25Mbps to 4Mbps. We compare the inference latency of our edge-cloud collaborative solution, LLaVA-AlignedVQ, with two cloud-only approaches, LLaVA-JPEG-90 and LLaVA-JPEG-10, which transmit JPEG-compressed images. \n\nFigure 6 reports the inference latency, including the edge computation time, the transmission time, and the cloud execution time. For LLaVA-JPEG variants, the edge computation time is ignored as JPEG compression usually takes less than 1ms with proper optimization. Since the large language model is executed on the cloud in all setups, its execution time is excluded from this comparison. Overall, LLaVA-AlignedVQ achieves an inference speedup of 2-15\u00d7 compared to the cloud-only solution using JPEG90compressed images. Against JPEG10-compressed images, our solution still provides a 1.19-2.52\u00d7 inference speedups at poor bandwidth conditions (0.25-1 Mbps). Although JPEG-10 compression is competitive in latency, it compromises accuracy as highlighted in Section 4.2. (Zeghidour et al., 2021) and Grouped VQ (Yang et al., 2023) on the task performance of LLaVA-AlignedVQ. Table 4 shows that increasing the number of codebooks and groups yields no or minimal accuracy improvement but results in increased transmission overhead, which scales proportionally with the number of codebooks and groups. Therefore, adding more codebooks and groups is unnecessary in LLaVA-AlignedVQ.",
            "score": 0.3599148803303506,
            "section_title": "Performance on Inference Latency",
            "char_start_offset": 24813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 523
                },
                {
                    "start": 526,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1702
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.216064453125
        },
        {
            "corpus_id": "274141390",
            "title": "Generative Timelines for Instructed Visual Assembly",
            "text": "Semantic Cues Ins. Rem. Repl. Swap Ins. Rem. Repl. Swap Avg. Zero-shot MiniGPT-4 [56] 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Llava-1.5 [8] 12.5 7.5 2.5 0.0 0.0 0.0 0.9 0.0 2.9 GPT-4o 60.0 96. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Llava-1.5 [8] 8.8 7.5 2.5 0.0 0.0 0.0 1. instance at a time. Instructed visual assembly, in contrast, requires managing a large collection of at least 20 images per instruction, making these models unsuitable for the task. However, they were the closest available open-source models to serve as our baselines. \n\nIn the case of MiniGPT-4, when prompted with several images (our collection) and an instructed assembly task, it tends to produce generic captions for some images in the collection. We will showcase examples of these failure cases later in the manuscript. One notable advantage of MiniGPT-4 is the use of Q-former, which reduces the number of tokens needed to represent a single image. This allowed us to utilize this model without major modifications since our entire task fit within the context limit of MiniGPT-4. Conversely, for Llava-1.5 [8], a single image is represented by 576 tokens; with a collection of 20 images, the input sequence increases to approximately 11,520 tokens, far exceeding the original design limitations of LLMs [35]. We attempted to use vanilla Llava-1.5 but encountered significant context limitations that prevented the model from producing any meaningful output. We then modified Llava to reduce the token count per image by trying methods such as averaging all tokens, max pooling, or using the [CLS] token. We determined that using the [CLS] token was most effective for our task. Despite these efforts, Llava-1.5 was still unable to perform assembly tasks adequately.",
            "score": 0.359554154377823,
            "section_title": "Assembly Accuracy (%) Positional Cues",
            "char_start_offset": 35145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 44
                },
                {
                    "start": 45,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1739
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1434326171875
        },
        {
            "corpus_id": "272689771",
            "title": "Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models",
            "text": "GPT-4o \n\nZero-shot CoT \n\nFrequency of using reference objects and their ability to follow instruction prompts more closely. Here, however, we also investigate the off-the-shelf performance and capabilities of open-source VLMs, i.e., LLaVA-v1.6-34b (Liu et al., 2024). \n\nTable 7 shows the performances of LLaVA in Q-Spatial Bench using standard prompt. Surprisingly, LLaVA achieves 60.59 in Q-Spatial-ScanNet, outperforming most commercial VLMs including Gemini 1.5 Pro, Gemini 1.5 Flash, and GPT-4V. However, LLaVA falls more than 20 points when evaluated on Q-Spatial++. Interestingly, when qualitatively analyzing the outputs of the models, we find that they essentially \"predict\" numbers rather than demonstrating any intuitive reasoning behavior, as seen in more powerful VLMs. We initially speculated that this might indicate the model was trained on some collection of datasets including ScanNet data. However, we found no reference suggesting that LLaVA directly uses ScanNet in its training dataset. Additionally, we found LLaVA does not perform well when receiving zero-shot CoT prompts. When receiving zero-shot CoT prompts, the performances drop by over 20 and 4 points in Q-Spatial-ScanNet and Q-Spatial++. For SpatialPrompt, we adopt SpatialPrompt-Steps, leading to a decrease around 9 points in Q-Spatial-ScanNet. In sharp contrast, SpatialPrompt-Steps improves 9 points in Q-Spatial++. We hypothesize several reasons for this: (i) LLaVA's capabilities are not as developed as those in very large commercial models, and inducing reasoning structures in-context via prompts do not yet lead to the same level of improvements; (ii) LLaVA may fail to follow the instruction prompts effectively; (iii) LLaVA may not be proficient at visual comparison, so even if it increases the use of reference objects, it does not yield better performance.",
            "score": 0.359447035546552,
            "section_title": "GPT-4V",
            "char_start_offset": 21631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 9,
                    "end": 22
                },
                {
                    "start": 25,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 267
                },
                {
                    "start": 270,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1852
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.191650390625
        },
        {
            "corpus_id": "270062295",
            "title": "Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement",
            "text": "LLaVA-1.5-13B, and VILA-7B by an average of 16.1%, 7.1%, and 8.4%, respectively. On the MM-Hal benchmark, which uses GPT as an evaluator for a more comprehensive assessment of hallucinations, SIMA achieves 12.7%, 10.1%, and 5.1% performance improvement compared with LLaVA-1.5-7B, LLaVA-1.5-13B, and VILA-7B. Notably, despite our three critic metrics focusing primarily on object hallucination, SIMA also achieves the greatest improvement of 13.1% on the Mementos-Behavior benchmark based on LLaVA-1.5-7B model, which tests behavior hallucination arising from understanding sequential image inputs. This improvement is significant because there is a correlation between object hallucination and behavior hallucination in sequential image understanding (Wang et al., 2024b); reducing object hallucination increases the likelihood of correctly inferring the corresponding behavior. (b) \n\nSIMA also enhances the comprehension capabilities of LVLMs. As shown in Table 2, on the nine comprehensive and VQA benchmarks, although the improvements are not as pronounced as on the hallucination benchmarks, SIMA still achieves an average improvement of 3.5%, 2.1%, and 4.4% compared to LLaVA-1.5-7B, LLaVA-1.5-13B, and VILA-7B. This is superior to other preference tuning methods.",
            "score": 0.359378182122712,
            "section_title": "Benchmark evaluation",
            "char_start_offset": 15136,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1270
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21630859375
        },
        {
            "corpus_id": "268510101",
            "title": "Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models",
            "text": "To evaluate the effectiveness of our methods, we evaluate LLaVA-1.5 and LLaVA-1.5 with contrastive harmlessness LoRA on the +Opt image and +Adv image setting of HADES. Besides, we also evaluate these models on LLaVA-Bench to discuss the influence of contrastive harmlessness LoRA on the general multimodal abilities of MLLMs. \n\nThe evaluation results, detailed in Tab. 3 reveal that our contrastive harmlessness LoRA remarkably reduces the ASR of LLaVA-1.5. Specifically, its average ASR decreased from 79.20% to 6.67% in HADES opt and from 89.53% to 5.07% in HADES adv . Moreover, contrastive harmlessness LoRA doesn't significantly impact LLaVA-1.5's performance on LLaVA-Bench. The results suggest that finetuning MLLMs with image-related alignment data can significantly enhance their harmlessness alignment performance, while not influence other multimodal abilities.",
            "score": 0.35874361651389525,
            "section_title": "A.2 Results and Analysis",
            "char_start_offset": 33641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 872
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19580078125
        },
        {
            "corpus_id": "271534005",
            "title": "LLaVA-Read: Enhancing Reading Ability of Multimodal Language Models",
            "text": "This observation is consistent with what we find in Section 4.1. \n\nAblation Study on Text-rich Image VQA We first compare LLaVA-Read with LLaVA plus OCR, where OCR words are provided to LLaVA in the training. The gap between these two settings shows the benefits of the OCR tokenizer, where both OCR texts and boxes are used. LLaVA-Read w/o layout finetuning still shows better performance compared with LLaVA + OCR, validating the effectiveness of layout-aware pretraining. We also performed another ablation study on layout pretraining; LLaVA-Read models with specific pretraining tasks removed all show inferior performance. If we remove the 100k document-related finetuning dataset, the performance on document-oriented VQA will decrease. We find the model usually fails on the chart VQA after we manually inspect the results. The resolution of the visual encoder plays an important role in multimodal LLM since higher resolution usually means more details. If we add high-resolution visual encoder, we observe improvement on both scene text-centric VQA and document-oriented VQA. Furthermore, if we increase the resolution from 768 to 1024, the performance is enhanced. Removing the PaddleOCR from LLaVA-Read does not cause a model collapse but leads to performance degradation. 3 shows a generated example, for which LLaVA-Read needs first parse this image and then output results in the JSON format following the scheme in the user instruction. 5, the performance of LLaVA-Read in visual question answering significantly declines when OCR support is removed. Finding 4. Multimodal LLMs can recognize visual words, but they do not exhibit the same level of understanding when these words appear in text inputs.",
            "score": 0.3582098701414175,
            "section_title": "Main Results",
            "char_start_offset": 27933,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 67,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1716
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26171875
        },
        {
            "corpus_id": "276317830",
            "title": "Pixel-Level Reasoning Segmentation via Multi-turn Conversations",
            "text": "We compare segmentation-specific baselines on classical referring expression segmentation benchmarks to evaluate the generalizability of MIRAS. As detailed in Table 5, MIRAS' base configuration (last row) outperforms segmentation-specific models and demonstrates competitiveness against other MLLMs, even surpassing the latest OMG-LLaVA. Two findings emerge in results: (1) While fine-tuned MIRAS (Stage-2) shows a decline in general performance due to task-specific optimization, it retains advantages over Next-Chat and remains comparable to OMG-LLaVA. \n\n(2) The capacity of the base model determines the system's potential, evidenced by consistent improvements in MIRAS when evolving the foundational model from LLaVA-v1 (row 6) to LLaVA-v1.6 (row 9). These impressive results are mainly attributed to MIRAS's convolutional backbone (i.e., Con-vNeXt), which supports larger input images and enables semantic-assisted mask generation. This provides a solid foundation for achieving fine-grained segmentation in the next stage. However, this focus on task-specific patterns inherently introduces a trade-off, sacrificing some degree of generalization.",
            "score": 0.35817141908756833,
            "section_title": "Generalization Segmentation",
            "char_start_offset": 21308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1152
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39599609375
        },
        {
            "corpus_id": "270440800",
            "title": "Comparison Visual Instruction Tuning",
            "text": "Next, leveraging our CaD-LLaVA V 1 model to produce non-hallucinated, image-informed CaD data, we generate additional CaD instructions into the collection CaD-Inst V 2 (71K).Combining CaD-Inst V 1 and CaD-Inst V 2 we form CaD-Inst and train our final CaD-LLaVA V 2 7B and 13B LMMs to achieve (1) significant (up to 17.5%) absolute improvement over a large variety of recent SOTA LMMs over a variety of 5 CaD-related existing closed-QA evaluation benchmarks (namely BISON [17], SVO Probes [18], NLVR2 [19], EQBEN [20], and COLA [21]), and (2) strong (up to over 20%) relative improvements on our contributed open-QA CaD benchmark -CaD-QA .Additionally, as CaD-Inst can be safely mixed with the LLaVA VI data [22], we show in Tab. 3 that our CaD-LLaVA V 2 models effectively avoid forgetting the general capabilities of the corresponding LLaVA LMMs.\n\nOur contributions are as follows: (i) we contribute CaD-Inst -a large-scale visual instruction tuning dataset for enhancing CaD reasoning capabilities of LMMs; (ii) we contribute CaD-QA -an open QA evaluation benchmark for assessing CaD capabilities; (iii) we contribute and open source a CaD-VI methodology for collecting and enhancing CaD instruction tuning data; (iv) we demonstrate significant (up to 17.5%) improvements in CaD reasoning for LMMs trained using CaD-Inst as well as potential to scale CaD-Inst via self-improvement by CaD-Inst -trained models.",
            "score": 0.3580795060705416,
            "section_title": "body",
            "char_start_offset": 2930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 174,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 847
                },
                {
                    "start": 849,
                    "end": 1411
                }
            ],
            "ref_mentions": [
                {
                    "start": 471,
                    "end": 475,
                    "matchedPaperCorpusId": "102352061"
                },
                {
                    "start": 488,
                    "end": 492,
                    "matchedPaperCorpusId": "235458570"
                },
                {
                    "start": 500,
                    "end": 504,
                    "matchedPaperCorpusId": "53178856"
                },
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "257766245"
                },
                {
                    "start": 527,
                    "end": 531,
                    "matchedPaperCorpusId": "258546995"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.281005859375
        },
        {
            "corpus_id": "274859644",
            "title": "Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception",
            "text": "The image shows a military fighter jet in flight. The jet is predominantly gray with a darker gray nose cone and a tail section that features a red and white logo, which appears to be a stylized bird. \n\nThe tail also has the text \"ND\" and \"805\" in black. The aircraft is equipped with two engines, and the landing gear is extended, indicating that it is either taking off or preparing to land. The background of the image reveals an airport environment with a runway visible beneath the jet. \n\nThere are several cars and trucks parked on the tarmac, and a building with a blue roof can be seen in the distance. The sky is overcast, suggesting a cloudy day. The overall style of the image is a candid photograph capturing a moment of the jet's operation. \n\nLLaVA-Next: ships, which further improves its performance on visual reasoning benchmarks such as GQA [17]. However, performance on tasks like TextVQA [47] is hindered by limitations in the open-source OCR model and the threshold settings; a high threshold restricts the model's ability to capture finer textual details. \n\nLarge Multi-Modal Benchmarks. We further conduct the evaluation on five challenging large multi-modal benchmarks. The experimental results are shown in Table 4. It can be seen that both LLaVA-v1.5 and LLaVA-NeXT trained with DCE-1M achieve competitive performance on more complex LMM benchmarks, demonstrating that the improvements brought by DCE-1M are comprehensive. Our model outperforms both LLaVA-v1.5 [31] and LLaVA-NeXT [32] across all LMM benchmarks, demonstrating that high-quality image captions during pretraining significantly enhance model performance, even without altering the supervised fine-tuning (SFT) data. Compared to other image captioning methods, such as ShareGPT-4V [8], DCEgenerated captions provide richer and more comprehensive scene information, significantly boosting model performance across most LMM benchmarks. However, due to the lack of Chinese data in DCE-1M, the model performs poorly on MMBench-CN [34].",
            "score": 0.35800058603389795,
            "section_title": "Objecct Info:",
            "char_start_offset": 25374,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 50,
                    "end": 200
                },
                {
                    "start": 203,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 753
                },
                {
                    "start": 756,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2019
                }
            ],
            "ref_mentions": [
                {
                    "start": 857,
                    "end": 861,
                    "matchedPaperCorpusId": "152282269"
                },
                {
                    "start": 906,
                    "end": 910,
                    "matchedPaperCorpusId": "85553602"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.391845703125
        },
        {
            "corpus_id": "276929180",
            "title": "EAZY: Eliminating Hallucinations in LVLMs by Zeroing out Hallucinatory Image Tokens",
            "text": "B.1 Model Details. \n\nLLaVA-1.5. The LLaVA-1.5 Model [Liu et al., 2024c] leverages the linear projector layer to align the vision and text modalities, with 576 image tokens. It adopted the pre-trained vision transformer from CLIP [Radford et al., 2021] and the pre-trained language model as Vicuna [Chiang et al., 2023]. \n\nShikra. The Shikra Model [Chen et al., 2023] introduces referential dialogue capabilities in multimodal large language models (MLLMs) by handling spatial coordinate inputs and outputs in natural language. It utilizes a vision encoder, an alignment layer, and a Vicuna-based language model without requiring extra vocabularies, position encoders, pre-/post-detection modules, or external plug-ins. \n\nThe model enables interaction with images through natural pointing and location-aware responses, supporting tasks such as Referring Expression Comprehension (REC), PointQA, Image Captioning, and Visual Question Answering (VQA) with promising performance. \n\nLLaVA-Next. The LLaVA-Next Model [Liu et al., 2024b] enhances multimodal capabilities by increasing the input image resolution up to 672 \u00d7 672 pixels, supporting three aspect ratios. It utilizes an improved visual instruction tuning data mixture to bolster visual reasoning and OCR capabilities. The model employs a pre-trained vision transformer from CLIP and integrates with advanced language models like Vicuna and Mistral.",
            "score": 0.3579152567309107,
            "section_title": "B Extended Experiment",
            "char_start_offset": 33972,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 21,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 319
                },
                {
                    "start": 322,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1404
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 71,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 229,
                    "end": 251,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.398681640625
        },
        {
            "corpus_id": "276961560",
            "title": "R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization",
            "text": "Our cross-modal reasoning pipeline is designed to bridge the gap between language reasoning models and vision models by integrating visual formal representations. Formal language is a structured system with strict syntactic and semantic constraints that eliminate ambiguity, ensuring logical consistency. With formal description of visual contents, language reasoning models are able to see and reason over image elements. We use DeepSeek R1 [9] to generate reasoning processes on LLaVA-OneVision [17] and collect these data into the R1-Onevision dataset. Figure 2 provides the process of cross-modal reasoning. Examples of the generated data are provided in the supplementary. \n\nData Curation and Filtering. For visual reasoning, we aggregate diverse multimodal datasets covering natural images, OCR-based text extraction, charts, mathematical expressions, and scientific reasoning problems, selecting only those that support structured reasoning. The final dataset incorporates components from the LLaVA-OneVision dataset [17], augmented with domain-specific datasets tailored for complex inference tasks.",
            "score": 0.3578692604522502,
            "section_title": "Cross-Modal Reasoning Pipeline",
            "char_start_offset": 10470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 677
                },
                {
                    "start": 680,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1107
                }
            ],
            "ref_mentions": [
                {
                    "start": 497,
                    "end": 501,
                    "matchedPaperCorpusId": "271719914"
                },
                {
                    "start": 1024,
                    "end": 1028,
                    "matchedPaperCorpusId": "271719914"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.393798828125
        },
        {
            "corpus_id": "258960368",
            "title": "CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers",
            "text": "For experiments on LLaVA-1.5 (Liu et al., 2023a), we followed its supervised fine-tuning (SFT) strategy, which tunes the LLM (i.e., Vicuna (Chiang et al., 2023)) and projector (i.e., MLP) while freezing the ViT. We evenly sampled 10% of data from the SFT dataset of LLaVA-1.5 as our training dataset. We observed that using more data provided limited improvement in performance recovery for models after acceleration. As shown in Table 7, with only 10% of the SFT data, CrossGET nearly doubles the throughput of the model forward and improves the throughput of generation by nearly 50%, while maintaining more than 98% of the original models' capabilities on average. Table 7 also indicates that with similar computational cost and throughput, LLaVA-1.5-13B after acceleration achieves better overall performance than LLaVA-1.5-7B without acceleration, which further demonstrates that instead of training smaller models from scratch, CrossGET can efficiently create more capable models from large-scale ones.",
            "score": 0.35742885599015073,
            "section_title": "Experiments with LLaVA-1.5 on Various Datasets",
            "char_start_offset": 20113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1008
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.305908203125
        },
        {
            "corpus_id": "271719914",
            "title": "LLaVA-OneVision: Easy Visual Task Transfer",
            "text": "To validate the performance for single-image tasks in real-world scenories, we consider a comprehensive set of image benchmarks in Table 3. It can be categorized into three classes: \n\n(1) Chart, Diagram, and Document Understanding. As the main visual formats for structured OCR data, we evaluate the results on AI2D [54], ChartQA [101], DocVQA [103], and InfoVQA [102] benchmarks. Though current open-source models such as InternVL [22] and Cambrian [133] achieve performance comparable to commercial models, LLaVA-OneVision goes a step further, surpassing GPT-4V [109] and approaching the performance level of GPT-4o [110]. \n\n(2) Perception and Multi-discipline Reasoning. Including visual perception scenarios, we reveal the potentials of our model for more complex and challenging reasoning tasks. Specifically, we adopt the perception benchmarks including MME [151], MMBench [86], and MMVet [154], and reasoning benchmarks such as MathVerse [165], MathVista [90], and MMMU [157]. The results of LLaVA-OneVision significantly outperforms GPT-4V on various benchmarks, and comparable to GPT-4o on MathVista. This further confirms the superiority of our framework in visual perception and reasoning tasks. \n\n(3) Real-world Understanding and Visual Chat. We consider the evaluation of LMMs as generalpurpose assistant in the wild as the most important metrics, beyond the lab environments. To validate the capabilities in real-world scenarios, we utilize several widely-adopted benchmarks, including RealworldQA [141], Vibe-Eval [111], MM-LiveBench [161], and LLaVA-Bench-Wilder [65]. While our model still has room for improvement compared to GPT-4V and GPT-4o, it achieves competitive performance with open-source models of similar parameter size. Notably, our model performs well on MM-LiveBench [161], a benchmark for real-world internet content with constantly updated content, demonstrating the model's broad world knowledge and strong generalization abilities.",
            "score": 0.35677582537669444,
            "section_title": "Single-Image Benchmarks",
            "char_start_offset": 21308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 181
                },
                {
                    "start": 184,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 624
                },
                {
                    "start": 627,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1967
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "2682274"
                },
                {
                    "start": 330,
                    "end": 335,
                    "matchedPaperCorpusId": "247593713"
                },
                {
                    "start": 344,
                    "end": 349,
                    "matchedPaperCorpusId": "220280200"
                },
                {
                    "start": 977,
                    "end": 982,
                    "matchedPaperCorpusId": "265466525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.304443359375
        },
        {
            "corpus_id": "270560400",
            "title": "MMNeuron: Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model",
            "text": "Researchers have managed to fine-tune current general-domain MLLMs on specific domain cor- pus. For example, Kuckreja et al. (2024) train MLLM on the Remote Sensing multimodal dataset using LLaVA-1.5 architecture. LLaVA-Med (Li et al., 2023) was initialized with the generaldomain LLaVA and then continuously trained in a curriculum learning fashion, while VLAAD (Park et al., 2024) opts for Video-LLaMA (Zhang et al., 2023a) as the foundational model to assist LLM in comprehending video data from auto driving scenarios. There are also researches trying to enhance MLLM's performance in specific domains (Bazi et al., 2024;Seyfioglu et al., 2023;Shao et al., 2023;Tian et al., 2024). Despite these efforts, it has also been proved that general-domain MLLMs without further domain-specific fine-tuning have demonstrated some cross-domain capability on some less common domains (Verma et al., 2024;Lu et al., 2023). In our research, we select virgin (i.e., without further fine-tuning) LLaVA-NeXT and Instruct-BLIP as our baseline, hoping to bring insights into the interpretation of general-domain MLLM's cross-domain potential and the development of allaround MLLMs qualified for different domains.",
            "score": 0.3563938215307145,
            "section_title": "Cross-domain MLLM",
            "char_start_offset": 7952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1200
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 131,
                    "matchedPaperCorpusId": "265456719"
                },
                {
                    "start": 363,
                    "end": 382,
                    "matchedPaperCorpusId": "269191004"
                },
                {
                    "start": 404,
                    "end": 425,
                    "matchedPaperCorpusId": "259075356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19677734375
        },
        {
            "corpus_id": "269354784",
            "title": "RS-LLaVA: A Large Vision-Language Model for Joint Captioning and Question Answering in Remote Sensing Imagery",
            "text": "The RS-LLaVA model is based on the architecture of the LLaVA model [15].In our experiments, we explore two variants of pre-trained Vicuna-v1.5 [60] LLM variants, ranging in size from 7B to 13B, to initialize the language model for RS-LLaVA.Vicuna 1.5 is an open-source large language model developed by LMSYS.It is a fine-tuned version of the Llama 2 model, trained on user conversations collected from ShareGPT.Vicuna is licensed under the Llama 2 Community License Agreement.For image encoding, the model adopts the pre-trained vision backbone of CLIP-ViT (large) [46], which utilizes an image resolution of 336 \u00d7 336.\n\nTo facilitate fine-tuning, we employ LoRA with a rank (r) set to 64 and \u03b1 set to 16 as suggested by the original paper.We utilize the Adam optimizer, with a learning rate of 1 \u00d7 10 \u22124 .Figures 3-5",
            "score": 0.35563894744043034,
            "section_title": "Experimental Settings",
            "char_start_offset": 25633,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 72,
                    "end": 240
                },
                {
                    "start": 240,
                    "end": 309
                },
                {
                    "start": 309,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 477
                },
                {
                    "start": 477,
                    "end": 620
                },
                {
                    "start": 622,
                    "end": 741
                },
                {
                    "start": 741,
                    "end": 807
                },
                {
                    "start": 807,
                    "end": 818
                }
            ],
            "ref_mentions": [
                {
                    "start": 566,
                    "end": 570,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34716796875
        },
        {
            "corpus_id": "274965690",
            "title": "PolySmart @ TRECVid 2024 Video Captioning (VTT)",
            "text": "In contrast, when input is fed with frames, the vanilla LLaVA (LV) model performs more robustly than the LLaVA-NeXT-Video (LV-V) model across the board. The results in Table 1 reveal that LV achieves higher scores across most metrics, including BLEU, METEOR, and CIDEr. This indicates that frame-based inputs allow LV to capture detailed content more effectively than LV-V, which directly processes video inputs. The consistent advantage of frame-by-frame input suggests that LV better leverages individual frame details to understand and align with ground-truth captions. This approach likely aids the model in capturing nuances that might be lost when processing continuous video sequences as a single input.",
            "score": 0.35552557723217176,
            "section_title": "Frames as Input",
            "char_start_offset": 5281,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 710
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2393798828125
        },
        {
            "corpus_id": "265150082",
            "title": "Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision",
            "text": "Data To construct multimodal feedback and revision data, we utilize the LLaVA-SFT-127k dataset (Sun et al., 2023). We only use the first turn of each instance in the dataset. When fine-tuning VOL-CANO, we use the llava-1.5-mix665k as the visual instruction dataset (Liu et al., 2023b). \n\nModel For the proprietary LLM, we employ Ope-nAI's gpt-3.5-turbo (OpenAI, 2022). We use the LLaVA-SFT+ 7B model to generate the initial response when creating feedback data and LLaVA-1.5 7B and 13B as backbone models of VOLCANO (Liu et al., 2023b,c). Details of computation and hyperparameters used are in Appendix C and Appendix D, respectively.",
            "score": 0.355411711719425,
            "section_title": "Implementation details",
            "char_start_offset": 11917,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 285
                },
                {
                    "start": 288,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 634
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20849609375
        },
        {
            "corpus_id": "278502509",
            "title": "DriveSOTIF: Advancing Perception SOTIF Through Multimodal Large Language Models",
            "text": "Autonomously driven vehicles can struggle to correctly interpret lane markings and other road signs obscured by water and reflections on the wet road surface, increasing the risk of navigation errors. \n\nDrivers should reduce their speed, use fog lights if available, maintain a safe following distance, and stay focused on the road to adjust quickly to potentially unseen hazards. \n\nOther vehicles close to the overturned car might limit maneuvering options for an autonomous vehicle and complicate the scene understanding, necessitating advanced decision-making algorithms that incorporate predictions of other drivers' behaviors. released Qwen2-7B achieved the highest scores in BLEU, and ROUGE metrics. Additionally, it can be seen that larger models generally achieve better performance. For example, the 34B version of LLaVA-Next, implemented using 4-bit precision, outperformed its 13B and 7B variants. A similar trend is observed when comparing the Qwen2-VL and LLaVA 1.5 models. For the fine-tuned models, the LLaVA 1.5 -13B model deployed with 8-bit precision achieved the highest scores across all metrics, which is expected given its larger size. Interestingly, the Qwen2-VL 2B model delivered similar level of performance with only 1/7 the number of parameters, making it a highly efficient option for deployment. Similar to image captioning, fine-tuning also led to significant performance gains in VQA tasks. The BLEU-4 score for the LLaVA 1.5 model 13B version increased by 146.95% from 18.83 to 46.50, significantly improving the model's accuracy and contextual understanding in handling openended questions. Similar improvements can be seen across all other metrics, showing that the fine-tuned models consistently generate more relevant and contextually appropriate responses. \n\n3) Discussion: Based on the results from both the quantitative and qualitative analyses, it is evident that fine-tuning leads to significant improvements in model performance across image captioning and VQA tasks. Fine-tuned models consistently generate more accurate, detailed, and contextually relevant outputs compared to their baseline counterparts. \n\nAdditionally, our findings demonstrate that the DriveSOTIF dataset, despite its relatively small size of 1,114 images, can effectively enhance the performance of MLLMs through SFT.",
            "score": 0.3552806569284694,
            "section_title": "GT:",
            "char_start_offset": 26663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 203,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1794
                },
                {
                    "start": 1797,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2150
                },
                {
                    "start": 2153,
                    "end": 2333
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2841796875
        },
        {
            "corpus_id": "267759688",
            "title": "The Revolution of Multimodal Large Language Models: A Survey",
            "text": "GPT4Tools (Yang et al., 2023a) While in all these approaches the LLM does not directly handle the visual input which is instead processed by other external tools, in LLaVA-Plus (Liu et al., 2023h) the query image is directly input to the MLLM (i.e., LLaVA) and is therefore involved during the selection and invocation of the most helpful tool according to the user needs.This is achieved also thanks to the introduction of a new instruction-following use tool dataset, which is employed to fine-tune the MLLM.Domain-Specific MLLMs.Finally, in Table 13 we summarize the main characteristics of domainspecific MLLMs, also in this case indicating for each model the LLM used as starting point.",
            "score": 0.354854470039928,
            "section_title": "E Additional Details on Other Modalities and Applications",
            "char_start_offset": 54652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 510
                },
                {
                    "start": 510,
                    "end": 532
                },
                {
                    "start": 532,
                    "end": 691
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1624755859375
        },
        {
            "corpus_id": "264833352",
            "title": "LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing",
            "text": "In this paper, we have introduced LLaVA-Interactive, a research demo prototype that showcases the practical applications of large multimodal models featuring visual input, output, and interaction. LLaVA-Interactive is cost-effective in system development since it combines three pre-trained multimodal models of complementary skills using web services, without requiring additional model training: LLaVA for visual chat, SEEM for interactive image segmentation, and GLIGEN for grounded image generation and editing. At the system level, compared with other systems, LLaVA-Interactive is a fully vision-language multimodal system in terms of input, output, and interaction, particularly unique in supporting visual prompts for image segmentation and generation/editing. Our initial assessment of LLaVA-Interactive across a wide range of real-world application scenarios has demonstrated its excellent ability to perform new, complex tasks. We hope this will inspire further research into multimodal foundation models. \n\nWe identify several potential avenues for future research: (i) The abilities of LLaVA-Interactive are restricted by the performance limits of the utilized pre-trained models. Enhancing LLaVA-Interactive's specific skill could be achieved by replacing the module with a superior model variant or creating an improved individual model, such as LLaVA, SEEM, and GLIGEN. System development and individual model development can be de-coupled, allowing for a plug-and-play approach to system serving. We also hope to extend the system development framework by incorporating additional features like Instruct Pix2Pix [1] for image-level editing. (ii) Since LLaVA-Interactive is a composite of individual models, its capacity during each inference is determined by the existing abilities of those models. While more complex tasks can be accomplished through iterative activation of current skills for combined skills, no new skill emerges at each inference by interpolating in the neural network's hidden space. We encourage the community to develop multimodal foundation models with more unified modeling, allowing new capabilities to emerge through latent task composition. is the swan too big? Assistant In the image, the swan appears to be relatively large compared to the boat and the surrounding environment. However, it is not necessarily too big, as swans can vary in size depending on the species and individual.",
            "score": 0.3548058708387217,
            "section_title": "Conclusions and Future Work",
            "char_start_offset": 24256,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2186
                },
                {
                    "start": 2187,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2325
                },
                {
                    "start": 2326,
                    "end": 2432
                }
            ],
            "ref_mentions": [
                {
                    "start": 1629,
                    "end": 1632,
                    "matchedPaperCorpusId": "253581213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43994140625
        },
        {
            "corpus_id": "276558069",
            "title": "MOVE: A Mixture-of-Vision-Encoders Approach for Domain-Focused Vision-Language Processing",
            "text": "It is worth noting that MOVE, which uses between 196 and 576 visual tokens, strongly outperforms LLaVA-1.5 -another non-slicing model occupying 576 tokens per image-particularly on task-oriented benchmarks such as AI2D, ChartQA, and InfoVQA. \n\nInterestingly, scaling up the language backbone from 1.5B to 7B parameters yields consistent gains across most benchmarks, pointing to the benefits of larger LLMs in multimodal settings. Overall, these findings underscore the versatility of MOVE and highlight potential avenues for improvement, such as introducing specialized encoders for OCR or other domain-specific tasks. \n\nIn Figures 5 and 6, we present qualitative examples of how MOVE processes LaTeX formulas. The model's task is to generate LaTeX code for the given input images. We compare MOVE's output with that of LLaVA-OneVision 7B and observe a significantly closer match to the ground-truth formulas in MOVE's results.",
            "score": 0.35467211343324645,
            "section_title": "Evaluation Results",
            "char_start_offset": 15322,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 244,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 928
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.344970703125
        },
        {
            "corpus_id": "277150722",
            "title": "GAEA: A Geolocation Aware Conversational Model",
            "text": "Vision language models (VLMs) have been at the forefront of computer vision research; geo-localizable VLMs are in their nascent stages. Vision Language Models. Multimodal learning unifies different modalities by a common representation. By contrastively fitting text and images into the same feature space, CLIP [39] has revolutionized multimodal learning. LLMs like GPT2 [40] made strides in representing text and next token prediction. Visual question answering (VQA) was of interest before, but, after LLaVA [34] and BLIP2 [31] combined the conversational aspects of LLMs and the representational capabilities of CLIP-like models, many problems of VQA are addressed.After that, numerous modern works emerged, such as GeoChat [27], Qwen2.5-VL [12], LLaMA-3.2 Vision [10], and LLaVA-OneVision [30] as well as proprietary models like GPT4 [9] and Gemini [47]. \n\nEven though most of these models are excellent for general VQA, they perform poorly on specialized tasks in fields like medicine, statistics, and geolocalization. This inspires the need for specialized VLMs that can address specific tasks. Geo-localization is a crucial field in vision research with essential applications in forensics, social media, and exploration; see [16,46,50]. On a global scale, Weyand et al. [53] first introduced a classification-based approach on the Im2GPS [24] dataset. Vo et al. [51] introduced classification in multiple hierarchies, while CPlaNet [45] introduced a combinatorial partitioning technique for combining coarse hierarchies to predict finer ones. Over the years many other works like ISNs [25], TransGeo [59], TransLocator [52], \n\nand GeoDecoder [15] have made significant advancements in this classification-based worldwide geolocalization by introducing scene-based specialized encoders and hierarchical evaluation, auxiliary scene recognition, and twin encoder approach, and query-based encoder-decoder architecture respectively.",
            "score": 0.3546575604702941,
            "section_title": "Related Work",
            "char_start_offset": 5157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1633
                },
                {
                    "start": 1636,
                    "end": 1937
                }
            ],
            "ref_mentions": [
                {
                    "start": 372,
                    "end": 376,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 511,
                    "end": 515,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 728,
                    "end": 732,
                    "matchedPaperCorpusId": "265456719"
                },
                {
                    "start": 1234,
                    "end": 1238,
                    "matchedPaperCorpusId": "250721241"
                },
                {
                    "start": 1238,
                    "end": 1241,
                    "matchedPaperCorpusId": "23584435"
                },
                {
                    "start": 1241,
                    "end": 1244,
                    "matchedPaperCorpusId": "263135137"
                },
                {
                    "start": 1347,
                    "end": 1351,
                    "matchedPaperCorpusId": "2061602"
                },
                {
                    "start": 1371,
                    "end": 1375,
                    "matchedPaperCorpusId": "7449120"
                },
                {
                    "start": 1441,
                    "end": 1445,
                    "matchedPaperCorpusId": "51934305"
                },
                {
                    "start": 1609,
                    "end": 1613,
                    "matchedPaperCorpusId": "247802213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.288330078125
        },
        {
            "corpus_id": "272647612",
            "title": "MemeIntent: Benchmarking Intent Description Generation for Memes",
            "text": "However, three over four metrics 12 gave a statistically higher score for Llava in experiments where zero-shot and NoBK input were used. Besides, SelfCheckGPT-NLI gave statistically higher scores for Llava when where AutoBK was used. Therefore, none of these models can entirely outperform the other across settings. \n\n12 except BLEU which did not show statistical significance Learning setup (RQ2). In general, when Llama was used, few-shot was better than zero-shot. Conversely, the opposite happened when Llava was used. The superior performance of few-shot learning in Llama is aligned with the intuition that having demonstrations is useful. Meanwhile, Llava's inferior few-shot performance has been discussed by its authors that Llava was not explicitly trained to take multiple images as input 13 . \n\nThe general trend above does have a few exceptions: SelfCheckGPT gave statistically higher scores for few-shot learning in Llava AutoBK, and zero-shot learning in Llava HumanBK.",
            "score": 0.3545570447483626,
            "section_title": "Automatic Evaluation",
            "char_start_offset": 23510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 985
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.132568359375
        },
        {
            "corpus_id": "271947320",
            "title": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?",
            "text": "Recent model developments, including Flamingo [3], PaLI [29], \n\nPaLM-E [16], BLIP-2 [36], InstructBLIP [15], Otter [31], MiniGPT-4 [75], mPLUG-Owl [67], LLaVA [43], Qwen-VL [4], and VITA [18], bring unique perspectives to challenges such as scaling pre-training, enhancing instruction-following capabilities, and overcoming alignment issues. However, the performance of these models in the face of real scenarios has often not been revealed. \n\nHigh-resolution MLLMs. Empirical studies have shown that employing higher resolution is an effective solution for many tasks [4,41,40,51]. Approaches like LLaVA-Next [42] segment highresolution images into multiple patches, encoding each one independently before concatenating all  Other models, such as Monkey [40] and LLaVA-UHD [64], also split images into patches but subsequently compress them to avoid redundant tokens. Mini-Genimi [39] comprises twin encoders, one for high-resolution images and the other for low-resolution visual embedding. They work in an attention mechanism, where the low-resolution encoder generates visual queries, and the high-resolution counterpart provides candidate keys and values for reference. Conv-LlaVA [22] employs ConvNeXt instead of ViT as the vision encoder. Cambrian [61] uses a set of learnable latent queries that interact with multiple vision features via cross-attention layers. Additionally, SliME [73] stresses the importance of global features, compressing the local image patches twice but preserving all the global context. Although many of these models focus on improving resolution, none have been tested in a rigorous high-resolution benchmark, often providing only intuitive examples that lack informativeness and convincing results. Our proposed benchmark offers a rigorous evaluation to test the capabilities in understanding high-resolution images.",
            "score": 0.3542998125394542,
            "section_title": "Related Work",
            "char_start_offset": 6550,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 64,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1852
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.174072265625
        },
        {
            "corpus_id": "266933417",
            "title": "Hallucination Benchmark in Medical Visual Question Answering",
            "text": "The model's performance is measured by the classification accuracy of the prediction's token.If the model provides a token other than the given options, the prediction is regarded as wrong and irrelevant (i.e.# irr in Table 1).If the model provides a token in the given options but a wrong answer, then the prediction is regarded as wrong only.We conduct an ablation study for various prompt styles, aiming to rigorously assess the models' performance (Table 2).The ablation study with the largest open source model that we use, LLaVA-v1.5-13Bmodel, confirms the effect of different prompting and shows that L + D0 prompt is the best strategy for hallucination evaluation, which we use for further evaluation (Table 7).\n\nThe evaluation of hallucination for various models shows that the best LLaVA variant model is LLaVA-v1.5-13Bmodel (Table 1).GPT-4-turbo-vision model outperforms LLaVA-v1.5-13Bmodel on average, but LLaVA-v1.5-13Bmodel performs better in FAKE and SWAP scenarios.Also, regarding the number of irrelevant answers, LLaVA-v1.5-13Bperforms better than other models including GPT-4-turbo-vision.This is also confirmed by qualitative analysis of samples of response (Table 6).",
            "score": 0.35420903607207843,
            "section_title": "EXPERIMENT AND RESULTS",
            "char_start_offset": 2978,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 209
                },
                {
                    "start": 209,
                    "end": 227
                },
                {
                    "start": 227,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 462
                },
                {
                    "start": 462,
                    "end": 543
                },
                {
                    "start": 543,
                    "end": 719
                },
                {
                    "start": 721,
                    "end": 829
                },
                {
                    "start": 829,
                    "end": 845
                },
                {
                    "start": 845,
                    "end": 896
                },
                {
                    "start": 896,
                    "end": 932
                },
                {
                    "start": 932,
                    "end": 981
                },
                {
                    "start": 981,
                    "end": 1045
                },
                {
                    "start": 1045,
                    "end": 1108
                },
                {
                    "start": 1108,
                    "end": 1188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3115234375
        },
        {
            "corpus_id": "270737928",
            "title": "S3: A Simple Strong Sample-effective Multimodal Dialog System",
            "text": "The FROMAGe [13] architecture is based on the idea of combining a pre-trained frozen LLM with an equally pre-trained and frozen visual image encoder through just one layer of linear projection.This linear layer, although having a small number of trainable parameters, plays a crucial role in establishing the connection between the image and text modalities.For LLaVA model [19] the authors employed GPT-4 to create a multimodal visual-text instructional dataset for training.LLaVA combines a LLM Vicuna and a visual encoder based on ViT-L/14 from CLIP, linked by a linear projection layer.Qwen-VL [3] demonstrates a similar approach to LLaVA with fine-tuning of the projector and multi-level training.HoneyBee [5] shows that classical approaches to building projectors from LLaVA could be improved by applying complex and effective resempling to reduce the number of modality tokens.CogVLM [26] introduces a so-called \"visual expert\" module: a copy of some transformer blocks inside the language model that activate and update their weights only on image tokens.",
            "score": 0.354088045857002,
            "section_title": "Multimodal Models",
            "char_start_offset": 5768,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 358
                },
                {
                    "start": 358,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 590
                },
                {
                    "start": 590,
                    "end": 702
                },
                {
                    "start": 702,
                    "end": 884
                },
                {
                    "start": 884,
                    "end": 1063
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 16,
                    "matchedPaperCorpusId": "258947258"
                },
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2088623046875
        },
        {
            "corpus_id": "269921145",
            "title": "TinyLLaVA Factory: A Modularized Codebase for Small-scale Large Multimodal Models",
            "text": "TinyLLaVA Factory is related to the released codebase of LLaVA (Liu et al., 2023), but has several merits: 1) Our codebase is modularized with standard training&evaluating pipelines and flexible data preprocessing&model configurations, while the codebase of LLaVA neglects these characteristics from the perspective of software designing; 2) LLaVA treats vision tower and connector as the property of a LLM, while our TinyLLaVA Factory views LLM, vision tower and connector as the components of the LMMs, which is more extensible to integrate more capabilities, such as adding the vision generation component.We also note that the codebase released by prismatic-vlms (Karamcheti et al., 2024) uses this design for LMMs and enjoys the extensibility.Different from Prismatic-VLMs, our TinyLLaVA Factory adopts the design philosophy of the factory pattern in software engineering, and modularizes the entire system into interchangeable components, with each component already integrating a suite of cutting-edge models, standard data-processing pipelines, and popular training recipes.",
            "score": 0.35383821862329384,
            "section_title": "Comparison to Related Codebase",
            "char_start_offset": 4450,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 609
                },
                {
                    "start": 609,
                    "end": 748
                },
                {
                    "start": 748,
                    "end": 1082
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 81,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.263916015625
        },
        {
            "corpus_id": "267617032",
            "title": "Large Language Models: A Survey",
            "text": "Future LLMs are expected to be multi-modal and handle a variety of data types, such as text, images, and videos, audio, in a unified manner. This opens up possibilities for more diverse applications in fields like question answering, content generation, creative arts, and healthcare, robotics, and beyond. There are already several prominent multi-modal LLMs out there, including: LLAVA [214], LLAVA-Plus [215], GPT-4 [33], Qwen-vl [116], Next-GPT [216], but the trend is expected to be continued. Evaluation of these models also is a new research topic, especially conversational generative vision models [217]. Multi-modal LLMs can unlock huge potentials in a variety of tasks, and there has already been a descent progress in this direction, which needs a dedicated paper to discuss all its details.",
            "score": 0.353154740586822,
            "section_title": "C. Multi-modal Models",
            "char_start_offset": 114543,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 803
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2822265625
        },
        {
            "corpus_id": "273963071",
            "title": "Aligned Vector Quantization for Edge-Cloud Collabrative Vision-Language Models",
            "text": "Benchmarks. We evaluate LLaVA-AlignedVQ on eight diverse Vision Question Answering (VQA) benchmarks: VQA-v2 (Goyal et al., 2017) and GQA (Hudson & Manning, 2019) for open-ended questions, VizWiz (Gurari et al., 2018) for questions from visually impaired users, TextVQA (Singh et al., 2019) for text-rich queries, POPE (Li et al., 2023b) for model's degree of hallucination on three sampled subsets of COCO (Lin et al., 2014), random, common, andadversarial, MMBench (Liu et al., 2025) for all-round shuffling on multiple choice answers, LLaVA-Wild (Liu et al., 2023) and MM-Vet (Yu et al., 2023) for visual conversations on a diverse range of tasks. POPE reports F1 Score, LLaVA-Wild and MM-Vet utilize the answer score evaluated by GPT-4o (Achiam et al., 2023), and all the other datasets use accuracy. LLaVA-AlignedVQ uses the same codebook to compress intermediate feature for all these benchmarks. \n\nAlternatives for Comparison. We compare the accuracy and latency of LLaVA-AlignedVQ against the following alternative approaches: \n\n\u2022 LLaVA-Ori: Transmits raw images directly to the cloud for processing, with all computations handled on the cloud side. We make two main observations. First, LLaVA-AlignedVQ consistently demonstrates high accuracy, staying within \u22122.23% to +1.6% of the original LLaVA model and \u22120.86% to +1.82% of LLaVA 1+ , achieving seven top two accuracy among the eleven results Although the variants LLaVA-JPEG-90 and LLaVA-JPEG-90 1+ achieve competitive high accuracy, lower than LLaVA-AlignedVQ by approximately \u22120.16% and \u22120.66% on average, they incur a larger transmission overhead as indicated in Table 3, leading to longer inference latency as shown in Section 4.4.",
            "score": 0.3530025389844772,
            "section_title": "Experimental Settings",
            "char_start_offset": 21038,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1697
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 128,
                    "matchedPaperCorpusId": "8081284"
                },
                {
                    "start": 137,
                    "end": 161,
                    "matchedPaperCorpusId": "152282269"
                },
                {
                    "start": 195,
                    "end": 216,
                    "matchedPaperCorpusId": "3831582"
                },
                {
                    "start": 269,
                    "end": 289,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 548,
                    "end": 566,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2081298828125
        },
        {
            "corpus_id": "272693949",
            "title": "NVLM: Open Frontier-Class Multimodal LLMs",
            "text": "The main results are presented in Table 7, which includes outcomes from nine vision-language benchmarks and four text-only benchmarks. Our NVLM-1.0 72B models rival the leading proprietary models (e.g., GPT-4o) and open-access models, including LLaMA 3V (not yet publicly available) and InternVL 2. Specifically, the following observations can be drawn from Table 7: \n\n\u2022 NVLM-D 1.0 72B achieves the highest scores on OCRBench (853) and VQAv2 (85.4) among all leading proprietary and open-access models. Its MMMU score (59.7) also significantly surpasses all leading open-access models at the time of this report's publication, including LLaVA-OneVision 72B (56.8) [65] and InternVL-2-Llama3-76B (55.2) [112]. On AI2D, TextVQA, ChartQA, and DocVQA, it performs only slightly worse than the best-performing InternVL-2-Llama3-76B, matches very strong GPT-4o [108], and significantly outperforms other leading open-access models, including Cambrian-1 [139] and LLaVA-OneVision 72B [65]. \n\n\u2022 NVLM-H 1.0 72B achieves the highest MMMU (Val) score (60.2) among all multimodal LLMs that are open-access at the time of this report's publication. It also achieves the best MathVista score (66.6) within NVLM-1.0 family, which already outperforms many very strong models including GPT-4o [108], Gemini Pro 1.5 (Aug 2024) [36], InternVL-2-Pro [111]. This demonstrate its superb multimodal reasoning capability. \u2022 NVLM-X 1.0 72B also achieves frontier-class results and stands as the best-in-class crossattention-based multimodal LLMs, rivaling the yet-to-be-released Llama 3-V 70B [82].",
            "score": 0.35286461736370556,
            "section_title": "Main Results",
            "char_start_offset": 49741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 982
                },
                {
                    "start": 985,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1573
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1695556640625
        },
        {
            "corpus_id": "270521538",
            "title": "First Multi-Dimensional Evaluation of Flowchart Comprehension for Multimodal Large Language Models",
            "text": "In this section, we present a detailed comparison of the predicted results for a subset of models: GPT4o, Phi-3-Vision, LLaVA-Next-Vicuna-13B, Qwen-Chat-VL, LLaVA-Next-Vicuna-7B, LLaVA-V1.5-13B, Cogvlm2-Llama3-Chat-19B, and Cogvlm-Chat. Each subplot in Figure 14 compares the predicted results (in blue) with the actual answer labels (in red) for each model. The score below each subplot indicates the overall performance of the model based on its accuracy in predicting the correct category. GPT4o stands out with the highest accuracy, achieving a score of 83.81, indicating robust performance in aligning predictions with actual labels. Phi-3-Vision, while scoring 60.95, demonstrates a noticeable discrepancy in the \"No\" category with lower prediction accuracy. LLaVA-Next-Vicuna-13B, with a score of 62.86, shows moderate alignment but also exhibits substantial errors in the \"No\" category. Qwen-Chat-VL and Cogvlm-Chat, both scoring 50.48, indicate significant prediction errors and lower overall accuracy, particularly evident in the \"No\" and \"Un- known\" categories. LLaVA-Next-Vicuna-7B and LLaVA-V1.5-13B, scoring 52.38 and 53.55 respectively, also reflect moderate performance but with specific inaccuracies in the \"No\" category. Cogvlm2-Llama3-Chat-19B, with a score of 57.14, shows better performance than some other models but still falls short in accurately predicting the \"No\" responses. These results suggest that while certain models like GPT4o exhibit strong performance, others require significant improvements in understanding and predicting both \"Yes\" and \"No\" categories accurately. The varying scores underscore the necessity for further refinement in training methodologies and model architectures to enhance predictive accuracy across all categories. \n\nThe phenomenon where some models exhibit a near 100% probability in answering \"Yes\" can be attributed to question.",
            "score": 0.3528151471191383,
            "section_title": "F. Detailed Comparison of Logical Verification Task",
            "char_start_offset": 25652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1774
                },
                {
                    "start": 1777,
                    "end": 1891
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.249755859375
        },
        {
            "corpus_id": "276776838",
            "title": "Vision Language Models in Medicine",
            "text": "The model's architecture enables rapid pre-training, requiring less than a week of computational time on single GPU setups, while maintaining high efficiency and generalization across different tasks. The research also acknowledges potential limitations, including risks associated with frozen LLM outputs and the ongoing need for advancement in multimodal AI development. \n\n2) LLaVa: The idea behind LLaVa [16] is to develop an efficient foundation language model termed \"LLaVA\" (Language and Vision Assistant), which integrates both language understanding and visual comprehension. The primary objective is to enhance the capabilities of language models in multimodal contexts, enabling them to better interpret and generate text based on visual inputs. This involves not only improving performance in traditional language tasks but also expanding functionality to include complex visual reasoning and description tasks. \n\nThe authors employed a two-step training process. Initially, they pre-trained the LLaVA model on a filtered dataset comprising 595,000 image-text pairs, sourced from the CC3M dataset. This filtering process involved selecting noun-phrases based on their frequency to ensure diverse representation across concepts in the dataset. The model was pre-trained for one epoch using a learning rate of 2e-3 and a batch size of 128. Following pre-training, the model underwent finetuning on a specialized dataset, LLaVA-Instruct-158K, for three epochs with a reduced learning rate of 2e-5 and a batch size of 32. Various optimization techniques, such as using the Adam optimizer with no weight decay and enabling BF16 and TF32, were implemented to balance speed and precision during training. \n\nThe results demonstrate that the LLaVA model exhibits significant improvements in responding to multimodal tasks compared to previous models. It effectively generates detailed and contextually appropriate descriptions of images, showcasing an ability to understand and relate visual elements to textual instructions. Performance benchmarks indicate that LLaVA not only maintains high accuracy in traditional language tasks but also excels in complex visual reasoning scenarios, establishing it as a robust tool for applications requiring integration of language and vision. \n\n3) LLaMa Adapter v2: LLaMA-Adapter [20], introduced in early 2023, was one of the pioneering attempts to extend Large Language Models (LLMs) to handle visual inputs through parameter-efficient adaptation.",
            "score": 0.35280593420545425,
            "section_title": "A. Sate-of-the-art VL models",
            "char_start_offset": 15672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 372
                },
                {
                    "start": 375,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 922
                },
                {
                    "start": 925,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1708
                },
                {
                    "start": 1711,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2284
                },
                {
                    "start": 2287,
                    "end": 2491
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69677734375
        },
        {
            "corpus_id": "277435240",
            "title": "Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization",
            "text": "Our implementation also follows the LLaVA-OneVision framework2 , adapted for enhanced multilingual capabilities. The architecture consists of three main components: SigLIP-SO400M3 (Zhai et al., 2023) as the visual encoder, a 2-layer MLP projection module (with GELU activation functions (Hendrycks and Gimpel, 2016)), and Salamandra-7b-instruct4 (Gonzalez-Agirre et al., 2025) as the backbone LLM. Salamandra has been chosen for its high multilinguality, as it has been trained with 35 European languages. \n\nWe also adopt LLaVA-OneVision's curriculum learning training strategy, which progresses through four distinct stages: \n\n\u2022 Stage 1 (Language-Image Alignment): In this initial phase, only the MLP projector is trained, while both the visual encoder and LLM remain frozen. General image captions are employed to establish basic cross-modal connections. \n\n\u2022 The key innovation in our approach lies in the strategic injection of multilingual text-only data throughout these training stages detailed in \u00a73.4. After the Visual Instruction Tuning, the model is merged with the baseline LLM weights using linear interpolation.",
            "score": 0.35263004076031407,
            "section_title": "Model Framework",
            "char_start_offset": 10837,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 856
                },
                {
                    "start": 859,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1124
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 199,
                    "matchedPaperCorpusId": "257767223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.315185546875
        },
        {
            "corpus_id": "268264800",
            "title": "Effectiveness assessment of recent large vision-language models",
            "text": "Sect. 2 evaluates the recognition performance of MiniGPT-v2 [4], LLaVA-1.5 [8], and Shikra [9] in various specialized tasks. Among them, LLaVA-1.5 generally shows better recognition ability in both existence determination and object classification. However, quantitative analyses indicate that while these models exhibit certain cognitive capabilities in various specialized tasks without domain-specific finetuning, their recognition performance requires further enhancement. When directly applied to these tasks, they still achieve limited cognition and understanding of specialized domains. Apart from such limited cognition, other typical weaknesses of LVLMs, as revealed in qualitative investigations, such as object hallucination and text-to-image interference, are likely to result in inferior performance.",
            "score": 0.35251714025764447,
            "section_title": "Summary",
            "char_start_offset": 18433,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 813
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2529296875
        },
        {
            "corpus_id": "278636409",
            "title": "Exploring Implicit Visual Misunderstandings in Multimodal Large Language Models through Attention Analysis",
            "text": "We consider Qwen2VL (Wang et al., 2024b), InternVL2 (Chen et al., 2024c), and LLaVA-OneVision (Li et al., 2024). \n\nAll three series models are capable of understanding multiple images and interleaved image-text information. The inference mode remains consistent with the the methodology outlined in Section 2.1. We use the CoT paradigm to guide the model's responses on both easy and hard tasks. \n\nDetails. Qwen2VL series and InternVL2 series models utilize dynamic resolution, mapping different images to varying numbers of tokens. Due to the limited GPU memory, the maximum number of tokens varies depending on the number of total images included in the sample. LLaVA-OneVision series models resize all images to a fixed size, which means that each row and column vector in the attention matrix corresponds directly to the patches of the original images. Consequently, we can further extend our approach to a more granular level. For more experimental details, please refer to Appendix C.1. \n\nEvaluation. We evaluate the models from two perspectives: answer accuracy and attention accuracy. The former demonstrates the models' capability for visual understanding, and the later reflects the degree of IVMs. Similar to Section 2.3, attention accuracy is calculated on correctly answered samples using the LND, M-LND and MC-LND metrics across different N . The highest attention accuracy obtained is used as the final value. All inference processes are conducted on H100 GPUs.",
            "score": 0.3524402820644277,
            "section_title": "Experiments Setup",
            "char_start_offset": 15013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 115,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 992
                },
                {
                    "start": 995,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1476
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259033203125
        },
        {
            "corpus_id": "270766718",
            "title": "TEAM MIPAL at MEDIQA-M3G 2024: Large VQA Models for Dermatological Diagnosis",
            "text": "We adopt the pretrained LLaVA model as the baseline and subsequently conduct finetuning of both the projection matrix and the language model using low ranked adaption (LoRA) (Hu et al., 2021).We train the LLaVA for one epoch with a batch size of 8 and a gradient accumulation step of 16.The LoRA hyperparameter r was set to 128 and \u03b1 was set to 256.The learning rates were set to 2e-5 for the projection layer and 2e-4 following the original configuration.We similarly inputted single image per query as the model was not finetuned on multiple images.Throughout the finetuning process, which spanned 10 epochs, we utilized the validation dataset to select the checkpoint from the epoch with the lowest evaluation loss.We employed the pretrained LLaVA-Med model, which is a version of LLaVA that has been finetuned on a medical dataset (Li et al., 2024).Finetunning was done for 10 epochs with a batch size of 8 and gradient accumulation step of 16.\n\nWhen training only on the task data, we denote it by \"MEDIQA-M3G\".The results for training additionally on the synthetic data is denoted by \"Synthetic\".",
            "score": 0.3522690424134307,
            "section_title": "Finetuning LLaVA variants",
            "char_start_offset": 5285,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 192,
                    "end": 287
                },
                {
                    "start": 287,
                    "end": 349
                },
                {
                    "start": 349,
                    "end": 456
                },
                {
                    "start": 456,
                    "end": 551
                },
                {
                    "start": 551,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 853
                },
                {
                    "start": 853,
                    "end": 948
                },
                {
                    "start": 950,
                    "end": 1016
                },
                {
                    "start": 1016,
                    "end": 1102
                }
            ],
            "ref_mentions": [
                {
                    "start": 835,
                    "end": 852,
                    "matchedPaperCorpusId": "258999820"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20166015625
        },
        {
            "corpus_id": "268513172",
            "title": "SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant",
            "text": "Fig. 6: A qualitative evaluation of detailed image descriptions from three models.We highlight the words and sentences that represent how each model describes the main object in the image.\n\nonly describe the image from a general concept, such as \"car\", \"dog\", or \"everyday moment\".Even though ShareGPT4V can generate detailed descriptions, it still suffers from the issue of object hallucination, such as \"perhaps waiting\" and \"behind the duck sign\".By observation, SQ-LLaVA can describe the image with less unintended text, yielding higher reliability.Quantitative Analysis.SQ-LLaVA serves as a general-purpose vision-language model and enables zero-shot image captioning.As indicated in Table 2a, SQ-LLaVA achieves 73% and 66% averaged improvement over ClipCap and Dis-criTune on all datasets, indicating the effectiveness of visual instruction tuning.Compared with the baseline model LLaVA-V1.5 [21], SQ-LLaVA achieves 2% averaged improvement on all datasets, indicating the effectiveness of visual self-questioning.Also, we find SQ-LLaVA * surpasses the baseline model ShareGPT4V [5] on Nocaps out and Conceptual dataset, demonstrating the adaptability of the proposed method on unseen testing images from new domains.Moreover, as shown in Table 2b, SQ-LLaVA can be easily adapted to COCO captioning via instruction tuning on short descriptions.",
            "score": 0.3521689294007292,
            "section_title": "Detailed Image Description",
            "char_start_offset": 31722,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 82,
                    "end": 188
                },
                {
                    "start": 190,
                    "end": 281
                },
                {
                    "start": 281,
                    "end": 450
                },
                {
                    "start": 450,
                    "end": 553
                },
                {
                    "start": 553,
                    "end": 575
                },
                {
                    "start": 575,
                    "end": 673
                },
                {
                    "start": 673,
                    "end": 854
                },
                {
                    "start": 854,
                    "end": 1019
                },
                {
                    "start": 1019,
                    "end": 1222
                },
                {
                    "start": 1222,
                    "end": 1349
                }
            ],
            "ref_mentions": [
                {
                    "start": 898,
                    "end": 902,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 1084,
                    "end": 1087,
                    "matchedPaperCorpusId": "265308687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2235107421875
        },
        {
            "corpus_id": "274305727",
            "title": "NEMO: Can Multimodal LLMs Identify Attribute-Modified Objects?",
            "text": "We analyze average scores of MLLMs for all question types with varying model sizes, including LLaVA-NeXT [19] (8B to 110B) and InternVL2 [7] (1B to 76B), on both original and attribute-modified objects, as shown in Fig. 4. \n\nFor both original and attribute-modified objects, increasing the model size generally leads to improved performance initially. Smaller models such as InternVL2-1B and LLaVA-NeXT-8B show relatively low accuracy on both object types, while accuracy improves with models like InternVL2-26B and LLaVA-NeXT-Qwen-32B, suggesting MLLMs benefit from more parameters. \n\nHowever, as model size continues to increase, we observe a decline in accuracy, particularly on attributemodified objects. For example, from LLaVA-NeXT-Qwen-32B to LLaVA-NeXT-72B, performance decreases by 15.2 on attribute-modified objects and by 11.2 on original ob- jects. This indicate that, beyond a certain size, additional parameters may add complexity that hinders the model's adaptation to modified attributes. Question 3: Why does scaling up MLLM size not guarantee better performance? To answer this question, we separate vision encoders and LLMs in MLLMs to analyze how scaling up each component affects performance. Unlike Question 1, which focuses on pretrained vision encoders, we separate the vision encoder from the whole MLLMs after fine-tuning; then evaluate its image representation. We employ image-toimage matching for this experiment including Original-to-Original (O2O) and Attribute-modified-to-Original (A2O) matching. For each query, a matching is considered correct if one of the top five similar images belongs to the same category (details in SM). \n\nFig. 5 shows that fine-tuning the same pre-trained vision encoder with different LLMs in MLLMs yields varied performance. Vision encoders fine-tuned on InternViT-300M-448 perform similarly, possibly because their small scale MLLM limits further improvement.",
            "score": 0.35215520915120757,
            "section_title": "Question 2: How does MLLM size affect the gap?",
            "char_start_offset": 15031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 225,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1662
                },
                {
                    "start": 1665,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1922
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30517578125
        },
        {
            "corpus_id": "262824780",
            "title": "Aligning Large Multimodal Models with Factually Augmented RLHF",
            "text": "We use LLaVA-Bench (Liu et al., 2023a) and our MMHAL-BENCH as our main evaluation metrics for their high alignment with human preferences. In addition, we conducted tests on widelyrecognized Large Multimodal Model benchmarks. We employed MMBench (Liu et al., 2023b), a multi-modal benchmark offering an objective evaluation framework comprising 2,974 multiplechoice questions spanning 20 ability dimensions. This benchmark utilizes ChatGPT to juxtapose model predictions against desired choices, ensuring an equitable assessment of VLMs across varying instruction-following proficiencies. Furthermore, we incorporated POPE (Li et al., 2023d), a polling-based query technique, to offer an evaluation of Large Multimodal Model object perception tendencies. \n\nHigh-quality SFT data is crucial for capability benchmarks. By delving into the specific performances for the capability benchmarks (i.e., MMBench and POPE), we observe a notable improvement in capabilities brought by high-quality instruction-tuning data (LLaVA-SFT + ) in Tables 4 and 7. LLaVA-SFT + 7B model exemplifies this with an impressive performance of 52.1% on MMBench and an 82.7% F1 score on POPE, marking an improvement over the original LLaVA by margins of 13.4% and 6.7% respectively. However, it's worth noting that LLaVA-SFT + does LLaVA-RLHF shows subtle degradations at the 7b scale, but the 13b LLaVA-RLHF improves over LLaVA-SFT + by 3% on MMBench. This phenomenon is similar to the Alignment Tax observed in previous work (Bai et al., 2022a). Nonetheless, with our current empirical scaling law of LLaVA-RLHF, we believe RLHF alignment would not damage in general capabilities of LMMs for models of larger scales. \n\nRLHF improves human alignment benchmarks further.",
            "score": 0.35196946417416264,
            "section_title": "RESULTS",
            "char_start_offset": 19541,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 754
                },
                {
                    "start": 757,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1743
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.180419921875
        },
        {
            "corpus_id": "273023173",
            "title": "Unleashing the Potentials of Likelihood Composition for Multi-modal Language Models",
            "text": "We reported the results of applying likelihood composition on LLaVA series MLMs and 4 advanced MLMs with varying hyperparameters in Table .5,2 and Table .3 respectively. In Table .5, baseline is the MLM's intrinsic performance, while in Table .2 and Table .3, baselines are ensemble and majorityvote, which we refer as mutual-composition in our framework. \n\nResults on LLaVA Series As shown in Table .5, applied with self-composition methods mentioned in Sec. 4.1, LLaVA series' performance on the 9 datasets consistently improved, e.g., +12.08% for LLaVA-7B on MMVP, +4.39% for LLaVA-13B on MMBench, +6.96% for LLaVA1.5-7B on VSR, etc. Overall, for the early models in LLaVA family, i.e., LLaVA-7B and LLaVA-13B, which is not well developed relatively, self-composition methods improve their performance significantly. Also, aggressive self-composition, i.e., with \u03b1 = 1.0 works better in most cases than that with \u03b1 = 0.1, for LLaVA-7B and LLaVA-13B. \n\nFor those well-developed models, i.e., LLaVA1.5-7B, LLaVA1.5-13B, LLaVA1.6-7B and LLaVA1.6-13B, the improvement self-composition brings is not as significant as before. In more detail, for the best model, LLaVA1.6-13B, the Table 1: Self-composition methods bring a consistent improvement to all the LLaVA series models. (1) For debias, when \u03b1 is set to 1.0, it improves LLaVA-7B's performance on all 9 datasets and LLaVA-13B's performance on 7 datasets. When \u03b1 is set to 0.1, debias improves LLaVA1.5-7B and LLaVA1.5-13B's performance on 9 and 8 datasets respectively; improves LLaVA1.6-7B and LLaVA1.6-13B's",
            "score": 0.35060893681899125,
            "section_title": "Main Results",
            "char_start_offset": 16067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1563
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.272216796875
        },
        {
            "corpus_id": "268531413",
            "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
            "text": "From the results in Table 4, we observe that: (1) Updating ViT during instruction-tuning is sufficient to achieve good performance.In fact, we find that LLaVA-UHD can improve over LLaVA-1.5 even when ViT parameters are frozen in both pretraining and instruction tuning.(2) Further updating ViT during pretraining does not lead to better results.We hypothesize the reason is that jointly training ViT and resampler (from scratch) on limited pretraining data can lead to instability issues.\n\nCase Study.To provide a more intuitive understanding of the capabilities of LMMs in dealing with high-resolution images, we provide qualitative results for LLaVA-UHD and LLaVA-1.5 in Fig. 5.We can see that LLaVA-UHD can correctly identify the dense content in the timetable (Case 1), the text on the small poster (Case 2), and icons and text on the phone (Case 3) for fine-grained recognition and reasoning.In comparison, LLaVA-1.5 can only perceive coarse-grained information, and therefore tends to provide either uninformative (Cases 1 and 2) or incorrect/hallucinated answers (Case 3) in these challenging scenarios.The results demonstrate the effectiveness and advantage of LLaVA-UHD in perceiving native aspect ratio and high-resolution images for fine-grained multimodal capabilities.",
            "score": 0.35031636319646003,
            "section_title": "Analytic Results",
            "char_start_offset": 25515,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 269
                },
                {
                    "start": 269,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 488
                },
                {
                    "start": 490,
                    "end": 501
                },
                {
                    "start": 501,
                    "end": 680
                },
                {
                    "start": 680,
                    "end": 897
                },
                {
                    "start": 897,
                    "end": 1110
                },
                {
                    "start": 1110,
                    "end": 1281
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265380859375
        },
        {
            "corpus_id": "270440391",
            "title": "OpenVLA: An Open-Source Vision-Language-Action Model",
            "text": "When developing the OpenVLA model, we explored various design decisions in smaller-scale experiments before starting the final model training run. Concretely, we trained and evaluated OpenVLA models on BridgeData V2 [6] for our initial experiments, instead of training on the full OpenX mixture, to increase iteration speed and reduce computational cost. We summarize key learnings from these explorations below. VLM Backbone. Initially, we experimented with multiple VLM backbones. Apart from Prismatic [44], we tested fine-tuning IDEFICS-1 [84] and LLaVA [85] for robot action prediction. We found that LLaVA and IDEFICS-1 performed comparably on tasks with only one object in the scene, but LLaVA demonstrated stronger language grounding in tasks that involved multiple objects in the scene and required the policy to manipulate the correct object, i.e., the object specified in the language instruction. Concretely, LLaVA improved upon IDEFICS-1 by 35% in absolute success rate, averaged across five language grounding tasks in a BridgeData V2 sink environment. The fine-tuned Prismatic VLM policy achieved further improvements, outperforming the LLaVA policy by roughly 10% in absolute success rate across both simple single-object tasks and multi-object, language grounding tasks. We attribute this performance delta to improved spatial reasoning capabilities afforded by the fused SigLIP-DinoV2 backbones (see Section 3.1). In addition to the performance enhancements, Prismatic also provides a modular and easy-to-use codebase, so we ultimately chose it to be the backbone for the OpenVLA model. Image Resolution. The resolution of input images has significant impact on the computational requirements of VLA training, since higher-resolution images result in more image patch tokens and thus longer context lengths that quadratically increase training compute. We compared VLAs with 224 \u00d7 224px and 384 \u00d7 384px inputs, but found no performance difference in our evaluations, while the latter takes 3x longer to train. We thus opt for a resolution of 224 \u00d7 224px for the final OpenVLA model.",
            "score": 0.3502130006590543,
            "section_title": "OpenVLA Design Decisions",
            "char_start_offset": 17591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2099
                }
            ],
            "ref_mentions": [
                {
                    "start": 557,
                    "end": 561,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55224609375
        },
        {
            "corpus_id": "274464953",
            "title": "PatchFinder: Leveraging Visual Language Models for Accurate Information Retrieval Using Model Uncertainty",
            "text": "In order to get a baseline on how open-source VLMs perform on this specific task, we leveraged LLaVa-NeXT (7B) [20], Phi-3 (4.2B) [21], Idefics2 (8B) [22], and Donut (144M) [4]. LLaVa-NeXT (7B) is a VLM known for its robust performance in multimodal tasks, combining visual and textual data to improve information extraction accuracy. The improvements over LLaVA-1.5 [23] enabled LLaVA-NeXT to improve its accuracy on OCR-related tasks while keeping a minimalist design. Phi-3v (4.2B) is another VLM designed to handle document analysis tasks efficiently. It is recognized for its high accuracy in text extraction from noisy documents due to the rigorous OCR-related training that was deployed on the model, making it suitable for extracting precise information from various document types. Idefics2 (8B) is a model that integrates advanced visual and language processing capabilities. It has demonstrated strong performance in extracting detailed and accurate information from complex documents, aligning well with our requirement for accurate data extraction. Donut (144M) is one of the original state-of-the-art OCR-free document extraction models. It is known for its end-to-end design that is specifically focused on document extraction. Donut uses the architecture of a transformer visual encoder and a textual decoder. The model then converts target-type information into a structured JSON format for easy extraction. \n\nAs depicted in Table 1, Phi-3v exhibits noticeably better performance compared to the other models, although it achieves only 33% accuracy on Pennsylvania documents. These results fall short of our task requirements, showcasing that there is still room for improvement when it comes to out-of-the-box models. \n\nDespite our efforts to enhance the performance of LLaVa-NeXT and Idefics2 models, they performed poorly on this task. We experimented with various input resolutions and preprocessing techniques, prompts, and documents, but these models struggled due to the high variability in document layouts, scan qualities, and noise levels.",
            "score": 0.3497308535920627,
            "section_title": "Preliminary Benchmarks",
            "char_start_offset": 7310,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1735
                },
                {
                    "start": 1738,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 2066
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.406005859375
        },
        {
            "corpus_id": "276107383",
            "title": "SAISA: Towards Multimodal Large Language Models with Both Training and Inference Efficiency",
            "text": "In this section, we compare the computational costs of SAISA and LLaVA-1.5. We consider the computations of the LLM and the projector, as the computations of the visual encoder are identical in comparison. We consider the computations of the self-attention blocks and the FFNs in the LLM. \n\nFor LLM with n layers, we assume h is the hidden state size, m is the intermediate size of the FFNs, t is the number of text tokens, and v is the number of visual tokens. To comprehensively consider LLMs with and without grouped query attention (GQA) [1], we assume k is the output dimension of key/value matrices. For the projector, we assume d is the dimension of the input visual features. \n\nFor LLaVA-1.5, the FLOPs of the LLM are calculated as 2n(t+v)h(2h+3m+2k)+4n(t+v) 2 h, and the FLOPs of the projector are 2vhd + 2vh 2 . The total FLOPs are 2n(t + v)h(2h \n\nFor SAISA, visual tokens are multiplied only by key and value matrices, and the key-value sequence length is v in the attention operation. Therefore, the FLOPs of the LLM can be estimated by 2nth(2h+3m+2k)+4nvhk+4nt(t+v)h. The FLOPs of the projector are 2nvhd + 2nvh 2 . The total FLOPs are 2nth(2h + 3m + 2k) + 4nvhk + 4nt(t + v)h + 2nvhd + 2nvh 2 . \n\nFigure 4 compares the FLOPs of SAISA and LLaVA-1.5 with different numbers of tokens, based on Vicuna-7B-v1.5 [11]. SAISA achieves a higher computational efficiency than LLAVA-1.5 when processing the same numbers of tokens.",
            "score": 0.34810230764772276,
            "section_title": "Comparison of Computational Cost",
            "char_start_offset": 14048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1208
                },
                {
                    "start": 1211,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1433
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.228515625
        },
        {
            "corpus_id": "268531413",
            "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
            "text": "Visual encoding constitutes the basis of large multimodal models (LMMs) in understanding the visual world. Conventional LMMs process images in fixed sizes and limited resolutions, while recent explorations in this direction are limited in adaptivity, efficiency, and even correctness. In this work, we first take GPT-4V and LLaVA-1.5 as representative examples and expose systematic flaws rooted in their visual encoding strategy. To address the challenges, we present LLaVA-UHD, a large multimodal model that can efficiently perceive images in any aspect ratio and high resolution. LLaVA-UHD includes three key components: (1) An image modularization strategy that divides native-resolution images into smaller variable-sized slices for efficient and extensible encoding, (2) a compression module that further condenses image tokens from visual encoders, and (3) a spatial schema to organize slice tokens for LLMs. Comprehensive experiments show that LLaVA-UHD outperforms established LMMs trained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our model built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088) resolution images using only 94% inference computation, and achieves 6.4 accuracy improvement on TextVQA. Moreover, the model can be efficiently trained in academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of LLaVA-1.5). We make the data and code publicly available at https://github.com/thunlp/LLaVA-UHD.",
            "score": 0.3477734602899395,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5
        },
        {
            "corpus_id": "270619483",
            "title": "African or European Swallow? Benchmarking Large Vision-Language Models for Fine-Grained Object Classification",
            "text": "We isolate the Other Factors.Very high image resolution, which is highly beneficial for OCR-heavy tasks like chart understanding (Liu et al., 2024), does not seem to be relevant for fine-grained object classification.This stems from the comparison between LLaVA 1.5 and LLaVA-Next, where the latter's main difference w.r.t. the former is training with (and inference on) images of higher resolution.This is unsurprising as images in object classification datasets typically contain large centered objects, making larger resolution unnecessary for solving the task.The LLM and image encoder are likely also major factors for the ultimate performance but we cannot isolate them in this observational analysis; instead, we consider them in controlled experiments in \u00a75.",
            "score": 0.3476284295694193,
            "section_title": "Results",
            "char_start_offset": 15427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 29,
                    "end": 217
                },
                {
                    "start": 217,
                    "end": 399
                },
                {
                    "start": 399,
                    "end": 564
                },
                {
                    "start": 564,
                    "end": 766
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1954345703125
        },
        {
            "corpus_id": "277113160",
            "title": "TULIP: Towards Unified Language-Image Pretraining",
            "text": "One of the motivations for developing strong vision and language models is their applications as feature-encoders for large-scale multimodal models such as LLaVA [32,33]. To evaluate our model's performance in these applications, we fine-tune Llama-3.2 11B using a set of visual encoders using the LLaVA mixture data. We then evaluate their performance on several benchmarks, including the BLINK bench-mark [20] (which consists of 14 primarily perceptual tasks including correspondence, visual similarity, and depth estimation), the MMVP benchmark [46] (which tests a model's visual capability), and LLaVA Bench [32] (which tests a model's ability to perform conversation, detail description and complex reasoning). \n\nResults on the BLINK dataset are shown in Table 4. We can see here that TULIP performs strongly across all classes of problems, performing particularly well in vision-driven tasks compared to base methods, where TULIP outperforms GPT-4o in tasks such as spatial reasoning and localization. \n\nThe results on MMVP and LLaVA are shown in Table 5. While DINOv2-fine-tuned models perform well on the MMVP benchmark, they struggle with language-centric tasks, while CLIP-style models perform better on languagecentric tasks, but struggle with visual perception. TULIP allows the best of both worlds in a single model, outperforming DINOv2 and SigLIP in their respective best tasks. \n\nAblations. Table 5 also shows the performance of TULIP with several components removed. We can see that the largest improvements on MMVP are drawn from the imageimage contrastive learning, along with our base data training pipeline. Reconstruction serves to further improve both the vision and LLaVA benchmark performance. GeCo primarily improves the performance on vision-centric tasks. Interestingly, the LLaVA bench performance seems saturated (in regards to both scale and improvement), suggesting that improving performance on this task requires improvements in the large language model or visual adapter.",
            "score": 0.3472736050250441,
            "section_title": "Vision & Language Models",
            "char_start_offset": 26316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1393
                },
                {
                    "start": 1396,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 2006
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 166,
                    "end": 169,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 407,
                    "end": 411,
                    "matchedPaperCorpusId": "269214091"
                },
                {
                    "start": 548,
                    "end": 552,
                    "matchedPaperCorpusId": "266976992"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.348388671875
        },
        {
            "corpus_id": "262055661",
            "title": "Investigating the Catastrophic Forgetting in Multimodal Large Language Models",
            "text": "Equipped with EMT, we now investigate the hallucinations in MLLM fine-tuning. We use LLaVA-7b and LLaVA-13b as our based MLLM for fine-tuning. And we conduct fine-tuning experiments on MNIST, CIFAR-10, CIFAR-100, and miniImagenet, respectively. All of our fine-tuning experiments were conducted based on the LLaVA model released on July 4 th , 2023.5 \n\nLinear and LoRA Fine-Tuning As discussed by Liu et al. [7], the LLaVA model contains a frozen base vision encoder g(\u2022) and a pre-trained LLM f \u03d5 (\u2022) parameterized by \u03d5. For an input image X v , LLaVA first maps X v into a visual feature vector Z v by applying the visual encoder \n\nThen, LLaVA applies a linear adapted layer W , that maps the visual features into text feature spaces \n\nand concatenate H v with the embedding of language queries H q into a visual and text embedding vector \n\nas the input to the pre-trained LLM f \u03d5 (\u2022) to obtain responses. As for specific fine-tuning methods: (1) Linear fine-tuning method only fine-tunes the linear adapter layer W ; (2) LoRA fine-tuning method fine-tunes the linear adapter layer W and the pre-trained LLM f \u03d5 (\u2022) with LoRA [54].",
            "score": 0.3468629862304108,
            "section_title": "EMT on Multimodal Large Language Models Fine-Tuning",
            "char_start_offset": 16170,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1133
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.440185546875
        },
        {
            "corpus_id": "268041692",
            "title": "SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model",
            "text": "Comparison to Baseline. Recognizing the potential presence of multiple types of artifacts in a synthetic image, we consider the artifact classification as a multi-label classification task (see Section 3.3). The label space includes 13 kinds of artifacts (see Figure 2) and \"No artifacts\". Then, we evaluate the ability to identify artifacts on the testing set. As shown in Table 2, LLaVA, fine-tuned on SynArtifact-1K, demonstrates a remarkable ability in artifact classification and the accuracy reaches up to 45.66%, achieving a notable improvement of 25.66%, compared with the baseline LLaVA-v1.5 which is fine-tuned on LLaVA-v1.5-mix665K. Lacking synthetic images in LLaVA-v1.5-mix665K, LLaVA tends to categorize an input image as a normal image without artifacts. Instead, through fine-tuning on SynArtifact-1K, the accuracy of \"No artifacts\" further improves by 10.57% and showcases the enhanced ability to identify artifacts. Considering distortion is the most prevalent artifact, we also present the performance on this category in Table 2. The baseline LLaVA-v1.5 model performs extremely bad in identifying distortion in synthetic images, while ours achieve 52.23% F1 Score. \n\nUpdating LLM Parameters. As illustrated in Table 2, updating LLM parameters causes much worse performance. The potential reason is that the presented SynArtifact-1K dataset is small, tuning LLM on the small set will cause overfitting and thus worse performance.",
            "score": 0.34645222487071115,
            "section_title": "Artifact Classification Results",
            "char_start_offset": 14250,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1449
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447998046875
        },
        {
            "corpus_id": "271432107",
            "title": "Dallah: A Dialect-Aware Multimodal Large Language Model for Arabic",
            "text": "5  We evaluate Dallah using the LLaVA-Bench benchmark described in 4.2.1, specifically designed for multimodal models. We compare Dallah against two baselines: Peacock, an Arabic MLLM based on the InstructBlip architecture integrated with AraLLaMA, an LLM based on LLaMA-2; and PALO, a multilingual MLLM based on the LLaVA architecture integrated with Vicuna, an LLM based on LLaMA-2. PALO supports Arabic, along with nine other languages. We utilize both model-based evaluation and human evaluation. We describe each of these next. \n\nModel Evaluation. We follow the original methodology of LLaVA-Bench (Liu et al., 2023b) by calling the APIs of three different models: GPT-4, Cohere Command R+ 2 , and GPT-4-Turbo. We slightly modify the prompts to accommodate Arabic instead of English. \n\nHuman Evaluation. We conduct a human evaluation of three models using LLaVA-Bench. We present three well-educated annotators with images and questions-answer pairs generated by Dallah and two other baseline models, Peacock and PALO. \n\nTo ensure integrity, the names of the models are hidden throughout the evaluation process. Annotators are asked to rate the models' responses on a scale from 1 to 10, based on correctness 3 , helpfulness 4 and question-answer consistency 5 . Figure 6 presents the average scores (%) of the models Peacock, PALO, and Dallah as evaluated by four different evaluators: GPT-4, Command R+, GPT-4-Turbo, and Human.",
            "score": 0.3463278762849236,
            "section_title": "Results",
            "char_start_offset": 18111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1434
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3310546875
        },
        {
            "corpus_id": "273345528",
            "title": "LiveXiv - A Multi-Modal Live Benchmark Based on Arxiv Papers Content",
            "text": "We extensively evaluate our benchmark by employing a total of 17 LMMs. Specifically, we employ 5 models from the LLaVA family of models including LLaVA 1.5-7B and LLaVA 1.5-13B (Liu et al., 2023c), LLaVA-1.6-7B and LLaVA 1.6-34B (Liu et al., 2023b), and LLaVA One-Vision (Li et al., 2024b). Furthermore, we employ IntstructBLIP (Dai et al., 2023), InternVL2-2B and InternVL2-8B (Chen et al., 2023c), InternLM-Xcomposer2-4KHD (Dong et al., 2024b), InternLM-Xcomposer2.5 (Chen et al., 2023c), Mantis (Jiang et al., 2024), Phi3v (Abdin et al., 2024), Idefics2 (Laurenc \u00b8on et al., 2024b) and Idefics3 (Laurenc \u00b8on et al., 2024a), Qwen2-VL (Wang et al., 2024) and API models Claude-Sonnet (cla, 2024) and GPT-4o (OpenAI, 2023) for our evaluations. These models have been chosen because of their varying characteristics and strong performance on multiple current benchmarks. All the models (except GPT-4o and Cloude-Sonnet) are accessed from the huggingface API, which makes our framework modular for an extension to more models as they are being added to the hub in the future. \n\nAdditional LiveXiv versions: While this section mainly focuses on the analysis of the first version of LiveXiv, new versions are continuously being uploaded to the HuggingFace Hub, at the time of writing, four additional versions exist: one of past ArXiv papers (v0) and three of more recent papers (v2 -v4). Version 2 consists of 18K samples, introduces 4 new domains from physics (namely, physics.optics, physics.bio-ph, physics.app-ph,",
            "score": 0.34566509178684846,
            "section_title": "Models:",
            "char_start_offset": 19219,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1514
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 196,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 378,
                    "end": 398,
                    "matchedPaperCorpusId": "266521410"
                },
                {
                    "start": 469,
                    "end": 489,
                    "matchedPaperCorpusId": "266521410"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2371826171875
        },
        {
            "corpus_id": "271915843",
            "title": "EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large Language Model",
            "text": "Following LLaVA-v1.5 (Liu et al. 2023a), we employ Vicuna-7b-v1.5 (Zheng et al. 2023) as our foundational LLM. The training data is consistent with LLaVA-v1.5, encompassing both pre-training data and supervised finetuning data.",
            "score": 0.34550375605647865,
            "section_title": "Ablation Study Implementation Details",
            "char_start_offset": 16705,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 227
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.345458984375
        },
        {
            "corpus_id": "272593309",
            "title": "MVLLaVA: An Intelligent Agent for Unified and Flexible Novel View Synthesis",
            "text": "Fig. 1. Application cases of MVLLaVA in novel view synthesis: MVLLaVA generates novel view images around an object or from specified viewpoints, using a reference image or caption. It can also rotate the camera to produce the desired image. MVLLaVA unifies multi-view generation capabilities to synthesize novel views, with user-friendly and intuitive instructions. [17], [18]. MVLLaVA is designed to handle a range of taskspecific instructions, which is achieved by developing viewbased instruction templates and fine-tuning LLaVA to improve its adaptability and performance. After instruction tuning, MVLLaVA intelligently interprets the input and selects the appropriate downstream model to generate the corresponding multi-view images. This task-driven approach, as shown in Fig. 1, enhances the flexibility and scalability of the generation process. It enables MVLLaVA to seamlessly adapt to diverse inputs and user requirements, effectively addressing the limitations of existing multi-view diffusion models. \n\nIn summary, the contributions of this paper are as follows: \n\n\u2022 An intelligent agent MVLLaVA is proposed for novel view synthesis, which integrates multiple task-specific diffusion models with LLaVA. \u2022 The instruction templates tailored to different tasks are designed, enabling MVLLaVA to handle a range of diverse and flexible instructions with robustness. \u2022 The effectiveness of MVLLaVA has been demonstrated through experiments, validating its performance across various application scenarios.",
            "score": 0.3455001245446706,
            "section_title": "5.Img-2-3d-degree",
            "char_start_offset": 1778,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 7
                },
                {
                    "start": 8,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1014
                },
                {
                    "start": 1017,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1514
                }
            ],
            "ref_mentions": [
                {
                    "start": 366,
                    "end": 370,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 372,
                    "end": 376,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.414794921875
        },
        {
            "corpus_id": "266976992",
            "title": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
            "text": "We conduct additional experiments on Interleaved-MoF that further scale up the resolution to 336 and evaluate on more benchmarks. The summarized results in Table 7 reveal that Interleaved-MoF achieves comparable performance on most benchmarks while demonstrating improvements in benchmarks focused on visual grounding. We also observe that MMVP are more sensitive to the model's visual capabilities, underscoring the significance of our benchmark in assessing visual proficiency. method res #tokens MMVP LLV B LLV W MMB VQA T POPE VQA V2 MM-V LLaVA  7. Comparison with LLaVA-1.5 on 6 more benchmarks. Interleaved-MoF LLaVA-1.5 obtains performance on par with the original method while showing improvements on benchmarks evaluating visual grounding. Benchmark names are abbreviated due to space limits. LLV B : LLaVA Benchmark [31]; LLV W : LLaVA-In-the-Wild [30]; MMB: MMBench [32]; VQA T : TextVQA [52]; POPE: POPE [27]; VQA V2 : VQA-v2 [15]; MM-V: MM-Vet [64]. \n\nCan you see the key \"Z\" in the image?",
            "score": 0.34525163444097395,
            "section_title": "E.2. Scaling up to larger resolution",
            "char_start_offset": 29961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1002
                }
            ],
            "ref_mentions": [
                {
                    "start": 899,
                    "end": 903,
                    "matchedPaperCorpusId": "85553602"
                },
                {
                    "start": 938,
                    "end": 942,
                    "matchedPaperCorpusId": "8081284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0938720703125
        },
        {
            "corpus_id": "269157480",
            "title": "Self-Supervised Visual Preference Alignment",
            "text": "and the improved model (SeVa-7B), respectively. As shown in Fig. 6, Our SeVa demonstrates superiority over LLaVA in various aspects: stronger OCR ability, where SeVa could recognize the exact number of the gas price; less hallucinations can be observed in the second picture, where SeVa accurately comprehends the process of 'chocolate cake recipe' and produces the correct answers. It is also surprising that SeVa could potentially recovers more world knowledge after DPO training, as it gives detailed and accurately response to the introduction of a movie (the third picture). Finally, we found that through our DPO training, the models provide more detailed and helpful answers, as suggested by the last picture of a meal ordering. Following Vicuna [6], we also conduct a competition game between SeVa and LLaVA-1.5 under 7B and 13B settings on LLaVA W and MMVet, respectively. We introduce GPT-4 as judge to evaluate the score of each model's responses (similar to the evaluation process in these two",
            "score": 0.34520460570590017,
            "section_title": "Question vg:",
            "char_start_offset": 26407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1005
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11260986328125
        },
        {
            "corpus_id": "277510513",
            "title": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision",
            "text": "Visual-language pretraining is the foundational technique for VLMs, allowing models to learn deep relationships between images and text through the joint training of large-scale imagetext pairs, which improves the model's performance on crossmodal tasks. Flamingo [203] is a classic example, combining few-shot learning and cross-modal reasoning to quickly adapt to new tasks with minimal examples, especially excelling in image captioning and visual question answering (VQA). Gemini [11] further extends this approach by jointly training largescale data from images, text, and audio, enhancing the model's cross-modal reasoning ability, particularly excelling in visual reasoning tasks. Kosmos-2 [210] strengthens the joint modeling of images and language through self-supervised learning, allowing it to handle more complex tasks, such as deep reasoning and generation of image and text. Additionally, PaLM-E [204] combines improvements in multimodal learning and reasoning capabilities, enabling it to process multimodal data such as images, text, and audio, performing well in cross-modal reasoning and generation tasks. \n\nThe core innovation of these models lies in joint training, enhancing the synergy between images and text, making them more widely applicable in multimodal tasks. MiniGPT-4 [205] and LLaVA [211] further refine this by aligning visuallanguage representations, improving the accuracy and flexibility of image-to-text generation. LLaVA [211] (Language and Vision Assistant) is an innovative visual-language model that employs instruction tuning, giving the model a high degree of flexibility in handling various tasks. LLaVA [211]'s ability to adjust to different tasks using natural language instructions enables it to not only respond rapidly to traditional visuallanguage tasks like VQA and image captioning, but also to adjust its execution strategy dynamically for new tasks. Through instruction tuning, LLaVA [211] can generate precise image descriptions or answer visual-related questions based on different task demands. This flexible task-switching capability makes LLaVA [211] particularly advantageous in complex environments, excelling in tasks such as cross-modal and visual reasoning.",
            "score": 0.344822749619036,
            "section_title": "Transformer Variants and Large Vision-Language Models",
            "char_start_offset": 66742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1124
                },
                {
                    "start": 1127,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2222
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 269,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 1316,
                    "end": 1321,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1460,
                    "end": 1465,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1649,
                    "end": 1654,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1939,
                    "end": 1944,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6083984375
        },
        {
            "corpus_id": "269773149",
            "title": "Contextual Emotion Recognition using Large Vision Language Models",
            "text": "We next explore 3 vision language models (VLMs): CLIP, a closed-source (GPT-4) and an open source (LLaVA) VLM.\n\nwhere I e is image feature embeddings and T e is the text feature embeddings.CLIP can be used to perform zero shot classification by comparing distances between an image and various texts in a multimodal embedding space.We used the images from EMOTIC and compared the distances with each of the emotion labels and selected the six (average number of ground truth labels in validation set) labels with highest probabilities as our labels.\n\n2) GPT-4 Vision and LLaVA (Zero Shot): GPT-4 Vision is a proprietary model from OpenAI that can provide text-based responses given an image and text input.Large Language and Vision Assistant (LLaVA) [62] is an open source multipurpose multimodal model designed by combining CLIP's visual encoder [45] and LLAMA's language decoder [56].The model is fine-tuned end-to-end on the language-image instruction-following data generated using GPT-4 [63].\n\n3) LLaVA (Fine-Tuned): We use the EMOTIC data to finetune LLaVA with LoRA [60] 2 on the emotion recognition task.We experiment by fine-tuning LLaVA on the EMOTIC training set (17077 images, and 23706 individuals), EMOTIC validation set (2087 images, and 3330 individuals), and on a small dataset, created by selecting 100 images at random from the validation set.Furthermore, we perform data augmentation by shuffling the ground truth labels for each image in the validation set and using each image 3 times with different shuffled labels.\n\n4) Prompt Engineering: We used the images from EMOTIC and a text prompt: \"From suffering, pain, [...], and sympathy, pick the top labels that the person in the red bounding box is feeling at the same time.\"It has been shown that prompt engineering (e.g.chain of thought [64]) is an effective way to improve results.We also tested a prompt which included definitions of the emotions and specified a number of labels to output.",
            "score": 0.3448146386798101,
            "section_title": "B. End-to-End Vision Language Models",
            "char_start_offset": 12955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 112,
                    "end": 189
                },
                {
                    "start": 189,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 549
                },
                {
                    "start": 551,
                    "end": 706
                },
                {
                    "start": 706,
                    "end": 886
                },
                {
                    "start": 886,
                    "end": 997
                },
                {
                    "start": 999,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1362
                },
                {
                    "start": 1362,
                    "end": 1538
                },
                {
                    "start": 1540,
                    "end": 1746
                },
                {
                    "start": 1746,
                    "end": 1793
                },
                {
                    "start": 1793,
                    "end": 1855
                },
                {
                    "start": 1855,
                    "end": 1965
                }
            ],
            "ref_mentions": [
                {
                    "start": 1810,
                    "end": 1814,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.404541015625
        },
        {
            "corpus_id": "273901280",
            "title": "Generating Vehicular Icon Descriptions and Indications Using Large Vision-Language Models",
            "text": "These examples show that LLaVA can match other models when it detects visual content correctly, but it often produces hallucinations when it fails. Despite many 'vision failures,' LLaVA's scores were only slightly lower due to automated metrics focusing on word matches rather than meaning differences, which will be discussed further. \n\nFinding: GPT-4o performed best on the task, but is followed closely by Claude 3.5. LLaVA performed significantly worse. \n\nRQ2: Does few-shot prompting improve model performance? \n\nFor GPT-4o and Claude 3.5, the metrics show the performance generally improves with increasing k in the few-shot prompting. However, for LLaVA, whether few-shot is better than zero-shot depends on the metrics and the type of description. For visual descriptions, 1-shot is better than zero-shot for most metrics except Meteor and Rouge. For 2 Percentages calculated as SBERTscore\u2212mean mean and reported to two decimal places. functional descriptions, 1-shot is better than zeroshot on all metrics except CLIP. It is interesting to see that when k > 1, the performance of k-shot decreases for LLaVA. To see whether the improvement is significant, we conducted a Friedman test (see Appendix C.2), which shows that k-shot has statistically significant improvements in SBERT-Score over the 0-shot baseline for both GPT-4o and Claude 3.5. Claude 3.5 showed the largest improvement at k = 5 (+1.3%) 3 . GPT-4o had the highest gains at k = 5 (+3.73%), with k = 3 (+3.34%) and k = 1 (+3.03%) close behind. All improvement for k > 1 in GPT-4o were significant, although differences for k \u2208 [1, 3, 5] were minor. LLaVA showed no significant improvements for higher k levels. In few-shot prompting, prompts were selected from the training based on image similarity (see Section 3.2).",
            "score": 0.3447830237435895,
            "section_title": "RQ1: Which model performs best on this task?",
            "char_start_offset": 15438,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 335
                },
                {
                    "start": 338,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 515
                },
                {
                    "start": 518,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1790
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26708984375
        },
        {
            "corpus_id": "267782716",
            "title": "DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models",
            "text": "Comparison with Baseline Model. We first conducted comparisons against baseline MLLMs LLaVA-1.5 and Qwen-VL-Chat across four benchmarks: SEED, MMBench, GQA, and TextVQA. Our DualFocus mechanism notably enhances the performance of both methods, as outlined in Table 1. Specifically, our DualFocus improves LLaVA-1.5 with Vicuna-7B by 2.7, 2.3, 2.1, and 3.8, respectively. With the larger LLM, Vicuna-13B, DualFocus secures even more substantial gains: 2.8, 3.0, 4.2, and 4.2, on SEED, MM-Bench, GQA, and TextVQA, respectively. This trend is consistent when applying DualFocus to Qwen-VL-Chat, yielding boosts of 1.2, 2.6, 4.0 and 2.2 on the same benchmarks, respectively. These results highlight DualFocus's versatility and its capability to significantly elevate MLLM performance across diverse benchmarks. \n\nComparison with SoTA Model. Subsequently, we conduct a comparison of DualFocus with other SoTA MLLMs that vary in their input resolutions (Res), visual encoders (Encoder-V), and language models (LLM) on Table 2. We incorporate our DualFocus into LLaVA-1.5 Vicuna-13B and ShareGPT4V (Chen et al., 2023b), a derivative of LLaVA, named as LLaVA-1.5-DF and Share4V-DF, exhibit superior performance across four distinct benchmarks. Specifically, Share4V-DF surpasses its closest competitor by 2 on the SEED benchmark. Similarly, LLaVA-1.5-DF leads the second-best performer by 1.9 on the MMBench. The results are even more pronounced on the GQA and Text-VQA benchmarks, which demand a higher capacity for detailed perception. Specifically, DualFocus improved Share4V by 4.7 and 4.0 on these two benchmarks, respectively.",
            "score": 0.344776718274179,
            "section_title": "Main Results",
            "char_start_offset": 19893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1624
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37353515625
        },
        {
            "corpus_id": "258179774",
            "title": "Visual Instruction Tuning",
            "text": "The broader impact of LLaVA, a general-purpose visual assistant, has potential benefits and risks associated with its deployment and release. Some considerations are unique to LLaVA due to its visual nature, while others share similarities with existing instruction-following LLMs (e.g., Alpaca, Vicuna, etc.). As LLaVA is built upon LLaMA, Vicuna, and CLIP, it inherits some of the issues associated with LLMs and vision encoders. In the following, we outline both the risks and mitigation strategies in place for the release of this model. \n\nMalicious input. To minimize potential misuse and harmful consequences, we employ two precautionary measures for LLaVA: (1) OpenAI Filter API for user input text to prevent harmful or inappropriate text instructions from being processed by the model, and (2) NSFW Filter for uploaded user images to detect and block Not Safe For Work (NSFW) content or any other potentially harmful image inputs. \n\nHallucination. Similar to LLMs, LLaVA might generate outputs that aren't grounded in facts or input data. This raises concerns about inferences made, especially in critical applications (e.g., medical). \n\nBiases. Bias can be transferred from the base models to LLaVA, both from the vision encoder (CLIP) and the language decoder (LLaMA/Vicuna). This may lead to biased outcomes or unfair representations of diverse content. \n\nEnergy consumption. Though energy consumption is not a primary concern for LLaVA due to a smaller pretraining dataset (see details in Sec. C), it may become a concern when scaling up the pretraining dataset or increasing the model size, e.g., to a larger LLaMA version like the 65B model. \n\nEvaluation complexities. Assessing the performance of LLaVA is challenging as it involves both language and visual tasks. Our evaluation benchmark covers several aspects, including accuracy, concept coverage, reasoning ability, and creativity. However, additional aspects need consideration, such as the degree of visual content hallucination and fine-grained understanding of visual content.",
            "score": 0.3446945978214133,
            "section_title": "A Broader Impact",
            "char_start_offset": 28413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1144
                },
                {
                    "start": 1147,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1656
                },
                {
                    "start": 1659,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2051
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52099609375
        },
        {
            "corpus_id": "263672058",
            "title": "Improved Baselines with Visual Instruction Tuning",
            "text": "Despite the data efficiency of LLaVA-1.5 when compared with approaches like InstructBLIP [14], the training of LLaVA-1.5 still doubles when compared with LLaVA. In this section, we conduct experiments for further improving the data efficiency by randomly sub-sampling the training data mixture of LLaVA-1.5, with a sampling ratio ranging from 0.1 to 0.5. We visualize the relative performance of different sampling variants in Fig. 4. \n\nFirst, the full data mixture provides the best knowledge coverage, and allows the model to achieve the best overall performance. To our surprise, with only 50% of the samples, the model still maintains more than 98% of the full dataset performance. This suggests that there is room for further improvements in data efficiency. \n\nSecond, when downsampling the dataset to 50%, the model's performance on MMBench, ScienceQA, and POPE does not decrease at all, and it even slightly improves on MMBench. Similarly, the model's performance remains steady when further downscaling the data from 50% to 30%. These results show promise of having the less-is-more [61] benefit for multimodal models as well.",
            "score": 0.3445343899324057,
            "section_title": "Data Efficiency",
            "char_start_offset": 24591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1134
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.369384765625
        },
        {
            "corpus_id": "267751035",
            "title": "Can Large Multimodal Models Uncover Deep Semantics Behind Images?",
            "text": "\u2022 LLaVA-1.5 (Liu et al., 2023a) is an endto-end LMM extended from Vicuna (Chiang et al., 2023), augmented with vision encoder.",
            "score": 0.3441653753236377,
            "section_title": "F Large Multimodal Models",
            "char_start_offset": 30477,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280029296875
        },
        {
            "corpus_id": "269149019",
            "title": "UNIAA: A Unified Multi-modal Image Aesthetic Assessment Baseline and Benchmark",
            "text": ". We obtain the UNIAA-mPLUG-OWL2 using the same data of UNIAA-LLaVA, and find that its accuracy on the Aesthetic Perception is 77.89%, which is slightly lower than UNIAA-LLaVA and better than GPT-4V.This further proves the generality of IDCP.UNIAA-mPLUG-OWL2 also achieves significant improvements in both description and assessment compared to the vanilla model.",
            "score": 0.3440421550302091,
            "section_title": "UNIAA on mPLUG-OWL2",
            "char_start_offset": 29038,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 363
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09466552734375
        },
        {
            "corpus_id": "269149019",
            "title": "UNIAA: A Unified Multi-modal Image Aesthetic Assessment Baseline and Benchmark",
            "text": "We select LLaVA-1.5-7B[36] as the base model, which contains a ViT-L image encoder, a LLaMA-2-7B, and an MLP connector.\n\nDetailed training information is in the Appendix A.5.In UNIAA-Bench, we assess 13 variants on the 10 most current and competitive open-source MLLMs, as well as GPT-4V and Gemini-Pro-Vision.We also report the perception results of human aesthetic judgment.",
            "score": 0.34370120139227284,
            "section_title": "EXPERIMENTS",
            "char_start_offset": 25260,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 22,
                    "end": 119
                },
                {
                    "start": 121,
                    "end": 174
                },
                {
                    "start": 174,
                    "end": 310
                },
                {
                    "start": 310,
                    "end": 376
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1336669921875
        },
        {
            "corpus_id": "273185450",
            "title": "Human-in-the-loop Reasoning For Traffic Sign Detection: Collaborative Approach Yolo With Video-llava",
            "text": "The deployment of Intelligent Transportation Systems (ITS) is experiencing a significant rise in the paradigm of smart cities. One of the key components of ITS is traffic object detection, this technology leverages computer vision and image processing to identify specific objects (e.g., vehicles, pedestrians) within digital imagery. [1]. Traffic Sign Recognition arXiv:2410.05096v2 [cs.CV] 5 Mar 2025 (TSR) is one of the most crucial aspects in Autonomous Vehicles (AVs) [2], especially for interpreting speed limit signs that reflect the maximum permissible speed on the road. Expanding the diversity of a dataset used to train the YOLO algorithm, such as incorporating synthetic images alongside real-world images, has been shown to positively impact its object recognition performance. [3]. However, the performance of YOLO can be influenced by weather conditions. \n\nTo address this challenge under semi-real-world conditions, this paper proposes a novel method that leverages iterative reasoning with Video-LLaVA [4] to enhance the accuracy and reliability of YOLO's object detection, specifically for speed limit traffic signs. The process begins with a recorded video analyzed by YOLO v8 to detect speed limit signs and determine their values. A human expert then evaluates YOLO's performance. If YOLO fails to accurately identify the speed limit, its output is fed into Video-LLaVA. The expert prompts Video-LLaVA with general questions, using each response to refine subsequent prompts. This iterative approach helps Video-LLaVA to pinpoint YOLO's inaccuracies and correctly determine the speed limit. Figure 1 illustrates high-level overview reasoning. The recorded video output from YOLO v8 serves as input for Video-LLaVA at this stage. A human expert poses general questions, with each response subsequently becoming input for the next prompt. The human expert does not provide direct answers. Following this process and localizing the inaccuracies from YOLO, Video-LLaVA attempts to determine the correct speed limit. \n\nThe rest of the paper is organized as follows. Section 2 provides related work. The large Vision Language Model is described in Section 3. Section 4 provides an overview implementation of object detection with human experts in the loop prompting with Video-LLaVA.",
            "score": 0.34352150933361214,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 869
                },
                {
                    "start": 872,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2032
                },
                {
                    "start": 2035,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 335,
                    "end": 338,
                    "matchedPaperCorpusId": "198147317"
                },
                {
                    "start": 473,
                    "end": 476,
                    "matchedPaperCorpusId": "237518670"
                },
                {
                    "start": 791,
                    "end": 794,
                    "matchedPaperCorpusId": "235965683"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3828125
        },
        {
            "corpus_id": "275118957",
            "title": "MVTamperBench: Evaluating Robustness of Vision-Language Models",
            "text": "A closer examination of individual model trends across size categories (Figure 15) reveals several noteworthy patterns. \n\nSmall Models (<7B) Among small models (Figure 15a), Molmo-1B demonstrates exceptional robustness, outperforming several medium-sized models and achieving consistency across all tampering types. Another notable small model, Phi3.5-Vision, shows drastic improvement over its predecessor, Phi3-Vision, highlighting the impact of architectural updates and extended task-specific training. \n\nFor the VILA model family, VILA1.5-3B performs better than many other small models, showcasing the benefits of targeted optimization for long-form video understanding. However, its performance lags behind Molmo-1B and Phi3.5-Vision, indicating room for improvement in handling complex tampering scenarios. \n\nInterestingly, InternVL-2.5-4B matches or exceeds the performance of several medium-sized models, emphasizing the importance of efficient design over raw parameter counts. Models like Llava-OneVision-7B, however, remain among the weakest performers, suggesting that training approaches and architectural focus on static data limit their ability to handle tampering. \n\nMedium Models (7B-26B) For medium-sized models (Figure 15c), we observe considerable variability. Variants such as Molmo-D and Molmo-O exhibit similar performance, suggesting limited scalability within the family. Conversely, models like InternVL-2.5-8B maintain exceptional robustness, outperforming even larger models in the same category. \n\nPixtral, a medium-sized model that claims higher performance on other benchmarks, delivers average results here, highlighting that tamperingspecific robustness requires distinct optimization strategies. Similarly, Chat-UniVi demonstrates an advantage over other Llava-family models, reinforcing the role of video-specific training in achieving higher tampering detection performance. \n\nIn the VILA model family, VILA1.5-8B underperforms compared to both smaller and larger VILA models, making it an interesting anomaly. This drop in performance could stem from suboptimal parameter tuning or an architectural bottleneck that affects scalability. Meanwhile, VILA1.5-13B and VILA1.5-40B",
            "score": 0.34339920438762095,
            "section_title": "Models Analysis Based on Model Size",
            "char_start_offset": 23864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 122,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1182
                },
                {
                    "start": 1185,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1526
                },
                {
                    "start": 1529,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1912
                },
                {
                    "start": 1915,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2213
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.295654296875
        },
        {
            "corpus_id": "275570710",
            "title": "Embodied Scene Understanding for Vision Language Models via MetaVQA",
            "text": "Decided to turn left even though a vehicle was present. \n\nTurned left again when there was a vehicle in proximity. \n\nCollided with the grey pickup. Lost traction as the ego speed was fast. \n\nBounced off from previous collision and collided with another vehicle. shown, most models successfully generate valid token sequences, and their zero-shot performances are significantly better than random guessing, indicating that some embodied scene understanding capabilities are already present. GPT-4o achieves the best zero-shot performance as it contains the largest parameters with an unprecedented scale of pre-training data. In addition, the low parse fail rate in most baselines validates the answerability of questions. Noticeably, LLaVA-NeXT [15] is the only outlier, with a zero-shot accuracy lower than random guessing. The shocking underperformance of LLaVA-NeXT is attributed to two factors. To begin with, the VLM fails \n\nto generate legal answer tokens consistently (failure rate of 27%). In addition, the model, quite frequently, refuses to answer the asked questions. For example, when asked with relative distance questions, LLaVA-NeXT typically responds with \"...I cannot provide a definitive answer to your question without more information about the simulation or the specific positions of the objects within it.\" Consequently, LLaVA-NeXT reports surprisingly bad metrics. The subfigure (b) of Fig. 7 emphasizes the improvements of the benchmarked VLM after fine-tuning. The dotted contours draw out the zero-shot performance of VLMs,  and the solid contours describe their fine-tuned counterparts. As shown, fine-tuning on the MetaVQA Dataset results in elevated embodied scene understanding in general, with the most pronounced improvements observed in spatial questions. In addition, comparable gains in accuracy (along each question supertype) are reported across models, suggesting the generalizability of learning for the dataset. We take InternVL2-8B, the best performing VLM after finetuning, for a case study: Fig. 5 showcases questions successfully answered by the model after fine-tuning. The model not only gains spatial awareness with a better understanding of pedestrians' projected heading but also shows improved attentive power by reasoning about multiple observed objects.",
            "score": 0.3433736827605324,
            "section_title": "Maintained direction.",
            "char_start_offset": 23825,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 58,
                    "end": 114
                },
                {
                    "start": 117,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 188
                },
                {
                    "start": 191,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 927
                },
                {
                    "start": 930,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2305
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.264892578125
        },
        {
            "corpus_id": "270095007",
            "title": "Enhancing Vision-Language Model with Unmasked Token Alignment",
            "text": "The emergent multi-modal capabilities of GPT-4 (OpenAI, 2023) have attracted widespread attention, and there are various re-implementations of such capabilities using open-sourced vision and large language models (Liu et al., 2023;Zhu et al., 2023).We adopt the LLaVA framework Liu et al. (2023) and evaluate pre-trained models on the LLaVA-Bench Liu et al. (2023).The results are presented in Tab. 3. Note that all the results are obtained by fixing the vision encoders' parameters, which can directly reflect the representation quality of the pre-trained model.Notably, our model achieves the best results in the overall category.Compared to the original CLIP large model (Radford et al., 2021), we overall obtain an improvement of 2.2% accuracy.Using the same pre-training dataset and iterations, we also outperform EVA-02 (Fang et al., 2023a) for 1.4%.We compare the outputs generated by the two LLaVA models and highlight the difference in Fig. 2. We show that our approach can capture more fine-grained details to produce better answers.\n\nTable 3: Results on LLaVA-Bench (Liu et al., 2023).The results of CLIP and EVA-02 are obtained by our reimplementation with official checkpoints.Conversation, Detail, and Reasoning represent the performance of conversation, detailed description, and complex reasoning, respectively.The overall result is a summary of the results of the three categories.",
            "score": 0.34303876322423044,
            "section_title": "Multi-Modal Results",
            "char_start_offset": 15835,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 249,
                    "end": 365
                },
                {
                    "start": 365,
                    "end": 563
                },
                {
                    "start": 563,
                    "end": 632
                },
                {
                    "start": 632,
                    "end": 748
                },
                {
                    "start": 748,
                    "end": 856
                },
                {
                    "start": 856,
                    "end": 1043
                },
                {
                    "start": 1045,
                    "end": 1096
                },
                {
                    "start": 1096,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1327
                },
                {
                    "start": 1327,
                    "end": 1398
                }
            ],
            "ref_mentions": [
                {
                    "start": 674,
                    "end": 696,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.317626953125
        },
        {
            "corpus_id": "269502355",
            "title": "Teaching Human Behavior Improves Content Understanding Abilities Of LLMs",
            "text": "After extensive filtering and cleaning, we are left with 730k samples of videos and images across the two platforms which we use for the next steps. \n\nAfter collecting user behavior over image and video content, we design tasks to teach large vision and language models (VLMs) to simulate user behavior. For this, we use an instruction fine-tuning format. Given a video or an image and the other metadata like time of post and channel, we ask the model to simulate user behavior of likes and comments. See Fig 2, Listing 1 for examples. We choose LLaMA-Vid (Li et al., 2023b) as our base model to teach it the user behavior. We call the resultant model Behavior-LLaVA (Large Language and Vision Assistant) (Liu et al., 2023a). We test Behavior-LLaVA on a diverse variety of tasks, evaluating its capabilities on image, video, text, and audio understanding tasks. We compare Behavior-LLaVA against its base model, LLaMA-Vid, and other supervised baselines. Further, to show the impact of behavior, we train another version of LLaMA-Vid, where we train it on the same set of videos and images as Behavior-LLaVA but do not include behavior information. We call this model Ad-LLaVA. \n\nWe make the following contributions with this work: 1) Behavior-LLaVA Instruction Fine-Tuning: We explore the idea of learning human behavior, resulting in better content understanding. We test this for action-level behavior data such as receiver comments, likes, and replay graphs. We collect a dataset called BLIFT, consisting of 400k images and 330k videos, along with their receiver behavior. Then, LLaMA-Vid is trained for the task of predicting receiver comments and upvotes given a media (a video or an image) (Listing 1). We show that using this simple task formulation over behavioral data collected in the wild, results in performance improvement over a hierarchy of tasks. We get improvements over the base LLaMA-Vid across 46 tasks over 26 benchmark datasets in both zero-shot and fine-tuned settings. We show this over low-level content understanding tasks like object and activity recognition and also over high-level tasks like topic and emotion detection.",
            "score": 0.3429416350976343,
            "section_title": "INTRODUCTION",
            "char_start_offset": 5373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 151,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1178
                },
                {
                    "start": 1181,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2152
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50439453125
        },
        {
            "corpus_id": "270559556",
            "title": "VideoVista: A Versatile Benchmark for Video Understanding and Reasoning",
            "text": "In Figures 18-44, we provide a specific case for each proposed task type.The case shows sampled frames from the video, along with the corresponding questions, options and ground truth.Besides, we included the responses to the questions from two commercial models, GPT-4o and Gemini, as well as four high-performing open-source Video-LLMs: LLaMA-VID, LLaVA-NeXT, Videochat2, and VideoLLaVA.From these cases, we can also observe the significant gap between open-source Video-LLMs and commercial models.",
            "score": 0.3426383304911069,
            "section_title": "A.6 Case Data",
            "char_start_offset": 28943,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 73,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 389
                },
                {
                    "start": 389,
                    "end": 500
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18310546875
        },
        {
            "corpus_id": "273233636",
            "title": "Structured Spatial Reasoning with Open Vocabulary Object Detectors",
            "text": "LLaVA. We selected LLaVA [13] (Large Language and Vision Assistant) as a representative Vision-Language Model (VLM) due to its strong performance in zeroshot spatial reasoning, particularly for binary predicate prediction, as noted by SpatialVLM [3]. referring expressions. We evaluated Grounding DINO on our triplets, treating them as referring expressions, and reported the results in Table IV. The results demonstrate the superior performance of GroundingDINO compared to LLaVA, with an improvement of over 25%. Grounding DINO was also used as the object detector in the OPM module, with the results included in the same    We have demonstrated the performance of the entire pipeline, along with LLaVA and Grounding-DINO, on AVD-Spatial. \n\nMLP model, even when faced with visual or textual gaps. This resilience stems from the carefully selected geometric features and word embeddings, enabling the model to handle both visual and linguistic domain gaps effectively. OPM+SRM+PRM. To evaluate the proposed probabilistic structured approach, we have reported the accuracy of Intersection over Union (IoU) with threshold 0.5 compared to the ground-truth bounding box and mean IoU. The performance on the Semantic Abstraction dataset in Table III, we have also demonstrated the OPM performance to show the effectiveness of the SRM and PRM. The results show the robustness and effectiveness of the proposed ranking modules, outperforming the Semantic abstraction baseline presented in [2]. This baseline presented the results for mean IoU in 3D voxel representations, replicating their results for 2D IOU is non-trivial. However, given use of ground-truth depth, and the considerable gap, our model performs better. Table IV demonstrates results on AVD-Spatial, showing superior performance over LLaVA and Grounding-DINO. By incorporating structured approach, our method is explainable, allowing us to diagnose module-specific failures. In contrast, LLaVA is more resilient due to its integrated design, though less interpretable.",
            "score": 0.342122049039388,
            "section_title": "B. VLM Baselines",
            "char_start_offset": 17335,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 7,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2028
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 246,
                    "end": 249,
                    "matchedPaperCorpusId": "267069344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35986328125
        },
        {
            "corpus_id": "269010048",
            "title": "WebCode2M: A Real-World Dataset for Code Generation from Webpage Designs",
            "text": "Table 4 presents the benchmarking performance of several generalpurpose MLLMs using the WebCode2M test dataset. From this figure, we can observe several interesting findings: (1) Generating lengthy code is challenging. Almost all metrics for nearly all models drop significantly as the target code length increases. For example, as the dataset transitions from WebCode2M-Short to WebCode2M-Mid and finally to WebCode2M-Long, the highest TreeBLEU score for specialized models drops from 0.35 to 0.15, the highest CLIP similarity decreases from 0.73 to 0.69, and the highest Visual Score declines from 0.78 to 0.65. (2) Model size matters. \n\nIn LLaVA family, several models show a significant improvement across all metrics as model parameters increase, with LLaVA-v1.5-7B and LLaVA-onevision-7B achieving the best performance, while LLaVA-onevision-0.5B performs poorly across all metrics, indicating that MLLMs require more parameters to achieve better results in webpage generation tasks. (3) Most general-purpose MLLMs struggle with webpage code generation. Among these models, only GPT-4V matches the performance of our model trained on WebCode2M, while GPT-4o significantly outperforms all other models. All remaining general-purpose models generally underperform compared to specialized models, with consistently low scores across all metrics. Notably, GPT-4o significantly outperforms all specialized and other general-purpose MLLMs across all metrics. Moreover, its performance remains highly stable as the complexity increases, without showing significant degradation. For instance, as the dataset transitions from WebCode2M-Short to WebCode2M-Mid and then to WebCode2M-Long, the visual score changes from 0.85 to 0.81, and then to 0.82. However, our goal is not to have small models outperform super MLLMs with hundreds of billions of parameters, but to assist MLLMs in webpage generation and enable smaller models to achieve competitive performance.",
            "score": 0.3421023783289363,
            "section_title": "Benchmarking on the Test Datasets (RQ2)",
            "char_start_offset": 22352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1959
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26953125
        },
        {
            "corpus_id": "259501644",
            "title": "SVIT: Scaling up Visual Instruction Tuning",
            "text": "We further study the data quality, diversity strategy, balance strategy and scaling-up effects. 10% of the images are randomly sampled from SVIT as the held-out testing set for evaluation. The training split is denoted as SVITtrain. Note that, for saving the training cost, we implement ablation study with the LLaVA-v1.0 model and evaluate on MME benchmark. We denote the LLaVA-v1.0 model trained on SVIT data as SVIT-v1.0. \n\nData Quality. LLaVA-v1.0 employs LLaVA-Instruct-80K as the visual instruction tuning data. To demonstrate the quality of SVIT, we construct a subset of SVIT-train at same scale of LLaVA-Instruct-80K and fine-tune LLaVA-v1.0 by replacing LLaVA-Instruct-80K with the SVIT subset. Without loss of generality, the subset is constructed by randomly sampling 20K data from conversation, complex reasoning, referring QAs and detail description, leading to a subset of 80K data in total, denoted as SVIT-80K. We adopt the same training protocol and hyper-parameters as LLaVA-v1.0. The Table 3. Evaluating models fine-tuned on LLaVA-Instruct-80K, SVIT-80K (random selection), SVIT-80K-D (enhancing diversity), SVIT-80K-B (with \"Yes/No\" balancing) and SVIT-train (SVIT train split) on MME benchmark. Note that the base model is LLaVA-v1.0 (Liu et al., 2023b). For LLaVA-Instruct-80K, we evaluate the officially released checkpoint by ourselves. The evaluation results on MME benchmark are shown in Table 3. The model fine-tuned on SVIT-80K achieves higher performance (+8.2%) than the model fine-tined by LLaVA-Instruct-80K. Specially, the model fine-tuned on SVIT-80K outperforms on \"count\" (+109.1%), \"posters\"(+23.1%),",
            "score": 0.34193485788574274,
            "section_title": "Ablation Study",
            "char_start_offset": 22260,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 424
                },
                {
                    "start": 427,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1638
                }
            ],
            "ref_mentions": [
                {
                    "start": 1256,
                    "end": 1275,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.388916015625
        },
        {
            "corpus_id": "258547300",
            "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning",
            "text": "Enterpriselevel product models include OpenAI's yet-to-be-released vision-language version of GPT-4 [19], Google's PaLM-E [10], Baidu's ERNIE [5], Alibaba's Tongyi Qianwen [2], and Sensetime's SenseNova [27]. Academic multi-modal efforts include a variety of models such as LLaMA-Adapters [38], Mini-GPT4 [39], and LLaVA [17]. LLaMA-Adapters aims to adapt LLaMA [33] into an instructionfollowing model with an additional adapters module and multi-modal prompts. Mini-GPT4 follows BLIP-2's [15] architecture but replaces the language decoder with Vicuna [9], which better supports longer responses and multi-round conversations. LLaVA connects text and image modalities through a trainable projector matrix, which is a simple lightweight linear layer. However, since LLaVA trains both the vision encoder and language decoder on their instructing tuning dataset, its cost is relatively high compared to others. In contrast, based on the Flamingo model, Otter trains a few cross-gated attention layers to connect visual and language information and establish attention between in-context examples, leaving the vision encoder and language decoder frozen.",
            "score": 0.3417835693593646,
            "section_title": "Introduction & Motivation",
            "char_start_offset": 5422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1150
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.281494140625
        },
        {
            "corpus_id": "270094614",
            "title": "Benchmarking and Improving Detail Image Caption",
            "text": "Our detail caption synthesizing pipeline improves LVLM-generated caption quality effectively.\n\nAs shown in Table 5, for LLaVA-1.5-7B and LLaVA-1.5-13B, the detail caption quality is improved by a large fraction in terms of CAPTURE score.For more advanced LVLM like LLaVA-NEXT and Mini-Gemini-HD, the advantage of the proposed pipeline persists, demonstrating the effectiveness of the our data synthesizing strategy.We attribute the smaller fraction of improvement in LLaVA-NEXT and Mini-Gemini-HD to other vision and language tools' limited capabilities, which pose \"short boards\" compared with LVLMs trained with expert-annotated detail caption training data.\n\nOur pipeline enhances recall of visual elements effectively with little precision drop.As shown in Table 5, this tendency can be observed across all four LVLMs, indicating that the divide-andconquer strategy improves model's perception of detail visual elements effectively.Thanks to the hallucination filtering module, the performance drop in precision is suppressed, so that improvement on CAPTURE score is witnessed across all LVLMs.",
            "score": 0.3413174564493678,
            "section_title": "Main results",
            "char_start_offset": 21544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 95,
                    "end": 237
                },
                {
                    "start": 237,
                    "end": 415
                },
                {
                    "start": 415,
                    "end": 660
                },
                {
                    "start": 662,
                    "end": 749
                },
                {
                    "start": 749,
                    "end": 936
                },
                {
                    "start": 936,
                    "end": 1098
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36279296875
        },
        {
            "corpus_id": "274422737",
            "title": "Evaluating Vision-Language Models as Evaluators in Path Planning",
            "text": "Success on this task requires effective performance across three distinct levels: 1) attribute abstraction: recognizing the attributes that define a favorable path in a particular decision-making context; 2) low-level perception: demonstrating precise low-level perception to determine which path performs better based on the given criteria; and 3) information integration: integrating and synthesizing the perceived information to produce an answer. \n\nUsing this benchmark, we analyze the performance of 9 state-of-the-art (SOTA) VLMs, including both the closedsource GPT-4o and GPT-4o-mini [47] and 7 different opensource VLMs of different sizes (i.e., LLaVA-NeXT 7b and 13b [39], Qwen2-VL-7b [69], LLaVA-OneVision-7b [32], LLaMA-3.2-11b [17], and Intern-VL2 8b and 40b [12]). We find that these models struggle with the path evaluation task (e.g., Qwen2-VL-7b achieving only 50.2% accuracy). However, when providing these VLMs with verbalized path specifications, their performance significantly improves (e.g., 74.2% accuracy for Qwen2-VL-7b), which reveals a potential vision bottleneck of these VLMs. Our further analysis confirms these models' weakness in low-level perception, especially when they are tasked to perceive the clearance of a path with respect to surrounding obstacles, and this weakness could be more prominent when the environment and the path representation become more complex. We discover the source of this weakness from the vision encoders used by these VLMs, yet simply fine-tuning the VLMs end-to-end with the vision encoders does not address the issue. Rather, our experiments suggest performing taskspecific discriminative adaptation of these vision encoders.",
            "score": 0.3410897264406585,
            "section_title": "Introduction",
            "char_start_offset": 3939,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1692
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.332763671875
        },
        {
            "corpus_id": "268513172",
            "title": "SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant",
            "text": "Also, SQ-LLavA-7B improves over previous methods on ScienceQA, indicating that our model excels in understanding and reasoning over scientific content and can effectively handle multi-modal information.The improvement in ScienceQA suggests strong capabilities in multi-hop reasoning, comprehension of complex scientific concepts, and the ability to utilize context and explanations to derive correct answers.SQ-LLaVA-7B has a steady improvement over LLaVA-v1.5-7Band ShareGPT4V-7B on the POPE benchmark, and the 2% and 1% improvement indicates that our proposed method has better reliability and trustworthiness since POPE is a task designed to evaluate the phenomenon of object hallucination [20,34].In the bottom section of Table 1, the proposed SQ-LLaVA-13B surpasses previous works in six out of ten benchmarks, indicating the scalability of our method on larger LLM.Notably, the performance inconsistency on some datasets might be due to the unsupervised prototype extractor (lacking pixel-wise guidance) in our model.To mitigate this issue, we could leverage pseudo object masks (e.g., given by the pre-trained segment anything) in learning prototypes.Despite this limitation, all the improvements are achieved with significantly fewer trainable parameters compared to other methods [3,5,21].",
            "score": 0.341085533750941,
            "section_title": "Zero-shot Multilingual Capability",
            "char_start_offset": 24757,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 202,
                    "end": 408
                },
                {
                    "start": 408,
                    "end": 463
                },
                {
                    "start": 463,
                    "end": 701
                },
                {
                    "start": 701,
                    "end": 871
                },
                {
                    "start": 871,
                    "end": 1023
                },
                {
                    "start": 1023,
                    "end": 1158
                },
                {
                    "start": 1158,
                    "end": 1298
                }
            ],
            "ref_mentions": [
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "258740697"
                },
                {
                    "start": 697,
                    "end": 700,
                    "matchedPaperCorpusId": "52176506"
                },
                {
                    "start": 1289,
                    "end": 1292,
                    "matchedPaperCorpusId": "261101015"
                },
                {
                    "start": 1292,
                    "end": 1294,
                    "matchedPaperCorpusId": "265308687"
                },
                {
                    "start": 1294,
                    "end": 1297,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1580810546875
        },
        {
            "corpus_id": "274131643",
            "title": "TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models",
            "text": "To comprehensively evaluate the capabilities of our TS-LLaVA, we further conduct experiments on two challenging multitask video understanding benchmarks, namely MVBench and MLVU. \n\nMVBench We present the results in Tab. 5a. Among the training-free methods, our 34B model outperforms the proprietary model GPT-4V by a large margin, in both average accuracy and across most sub-tasks. Even our 7B model surpasses GPT-4V in average accuracy, showing the strong understanding capability and potential of our method. \n\nWhen comparing to the training-based video LLM, we focus on PLLaVA [44]. PLLaVA uses the same image LLM backbone as our model but is further trained on video data. In over half of the sub-tasks, our TS-LLaVA manages to obtain comparable or better performance than PLLaVA. However, there are still tasks where performance can be improved: (1) On some action centric tasks, TS-LLaVA delivers satisfactory results (AC and AP). However, it struggles with other action-centric tasks (e.g., AA, AL, and AS). ( 2) TS-LLaVA performs less effectively on tasks that require reasoning over moving objects (MA, MC and MD). ( 3) TS-LLaVA also shows lower performance on CI and OE tasks. \n\nGiven the nature of these task types, TS-LLaVA's lower performance is unsurprising. All of these tasks involve data types not seen during image LLM training, such as actions and moving objects. Without specific training on these data types, bridging the gap between our training-free approach and training-based methods remains challenging. MLVU The results are presented in Tab. 5b. Our method significantly outperforms the training-free counterpart, establishing the SOTA results across all sub-tasks. \n\nIn comparison with training-based video LLM, we take a challenging opponent, namely a 72B Video-LLaMA2. Remarkably, TS-LLaVA-34B achieves comparable or even superior performance to Video-LLaMA2-72B on more than half of the sub-tasks, despite using a much smaller LLM.",
            "score": 0.34092944066859054,
            "section_title": "Multitask Benchmarks",
            "char_start_offset": 18186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 181,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1693
                },
                {
                    "start": 1696,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1963
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31298828125
        },
        {
            "corpus_id": "263672058",
            "title": "Improved Baselines with Visual Instruction Tuning",
            "text": "Despite the promising results demonstrated by LLaVA-1.5, several limitations must be acknowledged. First, LLaVA-1.5 utilizes full image patches, potentially prolonging each training iteration. While visual resamplers [3,14,32] reduce the number of visual patches in LLMs, they currently cannot achieve convergence as efficiently as LLaVA with a comparable amount of training data, probably due to more trainable parameters in the resamplers. The development of a sample-efficient visual resampler could pave the way for future scaling-up of instruction-following multimodal models. Second, LLaVA-1.5 is not yet capable of processing multiple images due to the lack of such instruction-following data, and the limit of the context length. Third, although LLaVA-1.5 exhibits proficiency in following complex instructions, its problem-solving capabilities can still be limited in certain domains, which could be improved with a more capable language model and with high-quality, targeted visual instruction tuning data. Finally, despite its significantly reduced propensity for hallucination, LLaVA-1.5 is not exempt from producing hallucinations and occasionally disseminating misinformation, and should be used with caution in critical applications (e.g. medical).",
            "score": 0.3408084741493757,
            "section_title": "C. Limitations",
            "char_start_offset": 40297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1263
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4833984375
        },
        {
            "corpus_id": "274437586",
            "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs",
            "text": "LLaVA-1.5 [34]. LLaVA is one of the most widely used open-source vision-language models, and its simple design, low tuning cost, and outstanding performance make it a cornerstone in the field of multi-modal models. Specifically, LLaVA employs a pre-trained CLIP as the visual encoder and Vicuna as the text decoder. A simple linear projector connects the two modules, enabling the LLM to accept visual tokens of CLIP as input. Meanwhile, visual instruction tuning allows the model to handle vision-language tasks. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement. LLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM. Video-LLaVA [32]. On the basis of image understanding, Video-LLaVA extends this capability to video comprehension. It unifies representations of images and videos through alignment before projection. The overall architecture remains consistent with LLaVA: the visual encoder encodes continuous video frames individually, and the representations are concatenated as inputs to the LLM. After joint training, Video-LLaVA is capable of understanding both image and video data. Qwen-VL [3]. Qwen-VL is another widely used opensource vision-language model. Similar to LLaVA, it includes a visual encoder (OpenCLIP) and a text decoder (Qwen LLM).",
            "score": 0.340807846142792,
            "section_title": "Model architectures",
            "char_start_offset": 36898,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 15
                },
                {
                    "start": 16,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2003
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 14,
                    "matchedPaperCorpusId": "263672058"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78173828125
        },
        {
            "corpus_id": "275118957",
            "title": "MVTamperBench: Evaluating Robustness of Vision-Language Models",
            "text": "This indicates that increasing model size, combined with its training paradigm, positively impacts robustness for this family. \n\nIn the moderate-performing category (Figure 14c), several trends emerge. Llama3.2-11B and Llama3.2-90B exhibit very similar performance despite the significant increase in size. These models, trained on the same recipe and dataset, highlight that merely increasing parameter count does not drastically enhance tampering detection capabilities. Another interesting observation is the improvement shown by Phi3.5-Vision and Vintern-Beta over their predecessors (Phi3-Vision and Vintern). This suggests that targeted architectural modifications or additional task-specific training significantly contribute to robustness. \n\nFor high-performing models (Figure 14b), InternVL-2.5 dominates across tampering effects, with smaller versions (e.g., 4B) achieving comparable performance to their larger counterparts. This demonstrates that efficient architectural design and training strategies can offset limitations in model size. VILA1.5-40B, a model specifically designed for long-form video understanding, showcases exceptional robustness, emphasizing the importance of task-specific optimization. Notably, Molmo-72B and NVLM-72B exhibit below-average performance relative to other large models or their smaller counter parts, indicating inefficiencies in parameter utilization or potential overfitting to pretraining data and tasks. \n\nAnother noteworthy observation across categories is the consistency within certain model families. For example, the Llava-Video and Chat-UniVi models outperform their Llava-OneVision counterparts, demonstrating the importance of video-specific training for tampering detection. Conversely, while Molmo-1B excels among small models, its larger variant (Molmo-72B) does not scale proportionally in performance, reinforcing the need for efficient scaling and training strategies.",
            "score": 0.3406869995380988,
            "section_title": "Overview of Large Multimodality Models (LMMs)",
            "char_start_offset": 21888,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 129,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1457
                },
                {
                    "start": 1460,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1936
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2080078125
        },
        {
            "corpus_id": "268857227",
            "title": "LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model",
            "text": "We train a suite of multimodal foundation models (MMFM) using the popular LLaVA framework with the recently released Gemma family of large language models (LLMs). Of particular interest is the 2B parameter Gemma model, which provides opportunities to construct capable small-scale MMFMs. In line with findings from other papers in this space, we test the effect of ablating three design features: pretraining the connector, utilizing a more powerful image backbone, and increasing the size of the language backbone. The resulting models, which we call LLaVA-Gemma, exhibit moderate performance on an array of evaluations, but fail to improve past the current comparably sized SOTA models. Closer analysis of performance shows mixed effects; skipping pretraining tends to reduce performance, larger vision models sometimes improve performance, and increasing language model size has inconsistent effects. We publicly release training recipes, code and weights for our models for the LLaVA-Gemma models.",
            "score": 0.34053870671211073,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38427734375
        }
    ],
    "quotes": {
        "cost": 0.106938,
        "quotes": [
            {
                "idx": 0,
                "key": "[261049617 | Li et al. | 2023 | Citations: 31]",
                "snippets": "In order to gauge the effectiveness of our data generation strategy, we have elected to utilize the open-sourced LLaVA [20,21] model as our multimodal LLM model...It should be noted that our pipeline is model-agnostic, meaning the datasets generated via our approach can be employed for training a variety of other models...Architecture. The architecture of the LLaVA model elegantly combines a pre-trained LLM, specifically the Vicuna-13B [5], and a pre-trained visual encoder, known as CLIP-ViT-L/14 (Radford et al., 2021)...It's worth mentioning that in LLaVA-1.5 [20], the linear projection layer has been substituted with a twolayer MLP. Additionally, the LLM model has been replaced with Vicuna-1.5-13B, and the input image size has been increased from 224x224 to 336x336, thereby elevating the model's multimodal capabilities to a greater extent.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231591445 | Radford et al. | 2021 | Citations: 29867]": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."
                },
                "metadata": [
                    {
                        "section_title": "Preliminary",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 160,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 161
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In order to gauge the effectiveness of our data generation strategy, we have elected to utilize the open-sourced LLaVA [20,21] model as our multimodal LLM model"
                    },
                    {
                        "section_title": "Preliminary",
                        "pdf_hash": "",
                        "start": 162,
                        "end": 321,
                        "sentence_offsets": [
                            {
                                "start": 162,
                                "end": 321
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "It should be noted that our pipeline is model-agnostic, meaning the datasets generated via our approach can be employed for training a variety of other models"
                    },
                    {
                        "section_title": "Preliminary",
                        "pdf_hash": "",
                        "start": 583,
                        "end": 766,
                        "sentence_offsets": [
                            {
                                "start": 583,
                                "end": 596
                            },
                            {
                                "start": 597,
                                "end": 766
                            }
                        ],
                        "ref_mentions": [
                            "231591445"
                        ],
                        "quote": "Architecture. The architecture of the LLaVA model elegantly combines a pre-trained LLM, specifically the Vicuna-13B [5], and a pre-trained visual encoder, known as CLIP-ViT-L/14 (Radford et al., 2021)"
                    },
                    {
                        "section_title": "Preliminary",
                        "pdf_hash": "",
                        "start": 1169,
                        "end": 1496,
                        "sentence_offsets": [
                            {
                                "start": 1169,
                                "end": 1284
                            },
                            {
                                "start": 1285,
                                "end": 1351
                            },
                            {
                                "start": 1352,
                                "end": 1495
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "It's worth mentioning that in LLaVA-1.5 [20], the linear projection layer has been substituted with a twolayer MLP. Additionally, the LLM model has been replaced with Vicuna-1.5-13B, and the input image size has been increased from 224x224 to 336x336, thereby elevating the model's multimodal capabilities to a greater extent."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[263672058 | Liu et al. | 2023 | Citations: 2824]",
                "snippets": "LLaVA-1.5. We denote this final model with all the modifications as LLaVA-1.5 (the last two rows in Table 2), which achieves an impressive performance that significantly outperforms the original LLaVA (Liu et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258179774 | Liu et al. | 2023 | Citations: 4921]": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available."
                },
                "metadata": [
                    {
                        "section_title": "Scaling the Data and Model",
                        "pdf_hash": "",
                        "start": 252,
                        "end": 458,
                        "sentence_offsets": [
                            {
                                "start": 252,
                                "end": 262
                            },
                            {
                                "start": 263,
                                "end": 458
                            }
                        ],
                        "ref_mentions": [
                            "258179774"
                        ],
                        "quote": "LLaVA-1.5. We denote this final model with all the modifications as LLaVA-1.5 (the last two rows in Table 2), which achieves an impressive performance that significantly outperforms the original LLaVA (Liu et al., 2023)."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[266374969 | Wu et al. | 2023 | Citations: 4]",
                "snippets": "LLaVA-0: LLaVA-Med is initialized with LLaVA-0 and then continuously trained with a comprehensive dataset of biomedical figure-caption pairs sourced from PubMed Central.\n\nLLaVA-1.5: LLaVA-1.5 is a general domain VLM that uses the LLaMA2 model, which has a significant improvement in language understanding when compared with LLaMA, as the backbone LLM (18,20). There are two significant improvements besides the change of the backbone LLM. Firstly, the addition of an MLP vision-language connector enhanced the system's capabilities. Secondly, the integration of academic task-oriented data further enhanced its performance and effectiveness. LLaVA-1.5 is available in 2 model sizes, 7B and 13B models, and we used both models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "LLaVA-0: LLaVA-Med is initialized with LLaVA-0 and then continuously trained with a comprehensive dataset of biomedical figure-caption pairs sourced from PubMed Central.\n\nLLaVA-1.5: LLaVA-1.5 is a general domain VLM that uses the LLaMA2 model, which has a significant improvement in language understanding when compared with LLaMA, as the backbone LLM (18,20). There are two significant improvements besides the change of the backbone LLM. Firstly, the addition of an MLP vision-language connector enhanced the system's capabilities. Secondly, the integration of academic task-oriented data further enhanced its performance and effectiveness. LLaVA-1.5 is available in 2 model sizes, 7B and 13B models, and we used both models.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[271719914 | Li et al. | 2024 | Citations: 867]",
                "snippets": "Please see the detailed development timeline in Section A.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": []
            },
            {
                "idx": 4,
                "key": "[273638057 | Elgendy et al. | 2024 | Citations: 3]",
                "snippets": "In our work, we focus on fine-tuning two distinct models that have demonstrated a robust understanding of temporal data through video processing within the VLM framework for question-answering and captioning. The first model, LLaVA-NeXT [20], was introduced in early 2024, offering improved reasoning and world knowledge compared to other large models. It exhibits data efficiency comparable to SOTA models such as LLaVA-1.5 (Liu et al., 2023), while delivering higher image resolution and enhanced visual conversation capabilities. Shortly after the release of LLaVA-NeXT, a video variant was introduced, named LLaVA-NeXT-Video, which has demonstrated strong performance in zero-shot video tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[263672058 | Liu et al. | 2023 | Citations: 2824]": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available."
                },
                "metadata": [
                    {
                        "section_title": "Model Fine-tuning",
                        "pdf_hash": "",
                        "start": 510,
                        "end": 1193,
                        "sentence_offsets": [
                            {
                                "start": 510,
                                "end": 718
                            },
                            {
                                "start": 719,
                                "end": 862
                            },
                            {
                                "start": 863,
                                "end": 1028
                            },
                            {
                                "start": 1029,
                                "end": 1193
                            }
                        ],
                        "ref_mentions": [
                            "263672058"
                        ],
                        "quote": "In our work, we focus on fine-tuning two distinct models that have demonstrated a robust understanding of temporal data through video processing within the VLM framework for question-answering and captioning. The first model, LLaVA-NeXT [20], was introduced in early 2024, offering improved reasoning and world knowledge compared to other large models. It exhibits data efficiency comparable to SOTA models such as LLaVA-1.5 (Liu et al., 2023), while delivering higher image resolution and enhanced visual conversation capabilities. Shortly after the release of LLaVA-NeXT, a video variant was introduced, named LLaVA-NeXT-Video, which has demonstrated strong performance in zero-shot video tasks."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[273821149 | Dai et al. | 2024 | Citations: 2]",
                "snippets": "Liu at el. introduced an end-to-end trained large vision-language assistant (LLaVA [51]) on instruction-following data for general purpose visual and language understanding, which gained widespread attention upon release. Subsequent research has further enhanced LLaVA's performance. For instance, LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios. LLaVA-Interactive [53] serves as a comprehensive demonstration platform, incorporating features such as image chatting, segmentation, and generation and editing capabilities, significantly expanding LLaVA's original functionalities. MoE-LLaVA [54], a sparse LVLM architecture based on Mixture of Experts (MoE), was developed to tackle performance degradation in multimodal sparse learning. MG-LLaVA [55] enhanced the model's visual processing capabilities by introducing multi-granularity visual streams, allowing it to handle features at various resolutions and object centers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Various Vision-Language Applications",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1034,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 221
                            },
                            {
                                "start": 222,
                                "end": 283
                            },
                            {
                                "start": 284,
                                "end": 455
                            },
                            {
                                "start": 456,
                                "end": 688
                            },
                            {
                                "start": 689,
                                "end": 845
                            },
                            {
                                "start": 846,
                                "end": 1034
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Liu at el. introduced an end-to-end trained large vision-language assistant (LLaVA [51]) on instruction-following data for general purpose visual and language understanding, which gained widespread attention upon release. Subsequent research has further enhanced LLaVA's performance. For instance, LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios. LLaVA-Interactive [53] serves as a comprehensive demonstration platform, incorporating features such as image chatting, segmentation, and generation and editing capabilities, significantly expanding LLaVA's original functionalities. MoE-LLaVA [54], a sparse LVLM architecture based on Mixture of Experts (MoE), was developed to tackle performance degradation in multimodal sparse learning. MG-LLaVA [55] enhanced the model's visual processing capabilities by introducing multi-granularity visual streams, allowing it to handle features at various resolutions and object centers."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[274437586 | Zhang et al. | 2024 | Citations: 10]",
                "snippets": "LLaVA-1.5 (Liu et al., 2023). LLaVA is one of the most widely used open-source vision-language models, and its simple design, low tuning cost, and outstanding performance make it a cornerstone in the field of multi-modal models. Specifically, LLaVA employs a pre-trained CLIP as the visual encoder and Vicuna as the text decoder. A simple linear projector connects the two modules, enabling the LLM to accept visual tokens of CLIP as input. Meanwhile, visual instruction tuning allows the model to handle vision-language tasks. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement. LLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM. Video-LLaVA [32]. On the basis of image understanding, Video-LLaVA extends this capability to video comprehension. It unifies representations of images and videos through alignment before projection.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[263672058 | Liu et al. | 2023 | Citations: 2824]": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available."
                },
                "metadata": [
                    {
                        "section_title": "Model architectures",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1563,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 15
                            },
                            {
                                "start": 16,
                                "end": 214
                            },
                            {
                                "start": 215,
                                "end": 315
                            },
                            {
                                "start": 316,
                                "end": 426
                            },
                            {
                                "start": 427,
                                "end": 513
                            },
                            {
                                "start": 514,
                                "end": 707
                            },
                            {
                                "start": 708,
                                "end": 724
                            },
                            {
                                "start": 725,
                                "end": 749
                            },
                            {
                                "start": 750,
                                "end": 895
                            },
                            {
                                "start": 896,
                                "end": 999
                            },
                            {
                                "start": 1000,
                                "end": 1139
                            },
                            {
                                "start": 1140,
                                "end": 1273
                            },
                            {
                                "start": 1274,
                                "end": 1363
                            },
                            {
                                "start": 1364,
                                "end": 1381
                            },
                            {
                                "start": 1382,
                                "end": 1478
                            },
                            {
                                "start": 1479,
                                "end": 1563
                            }
                        ],
                        "ref_mentions": [
                            "263672058"
                        ],
                        "quote": "LLaVA-1.5 (Liu et al., 2023). LLaVA is one of the most widely used open-source vision-language models, and its simple design, low tuning cost, and outstanding performance make it a cornerstone in the field of multi-modal models. Specifically, LLaVA employs a pre-trained CLIP as the visual encoder and Vicuna as the text decoder. A simple linear projector connects the two modules, enabling the LLM to accept visual tokens of CLIP as input. Meanwhile, visual instruction tuning allows the model to handle vision-language tasks. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement. LLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM. Video-LLaVA [32]. On the basis of image understanding, Video-LLaVA extends this capability to video comprehension. It unifies representations of images and videos through alignment before projection."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[275405668 | Zhao et al. | 2025 | Citations: 5]",
                "snippets": "LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Multimodal Large Language Model",
                        "pdf_hash": "",
                        "start": 452,
                        "end": 1066,
                        "sentence_offsets": [
                            {
                                "start": 452,
                                "end": 724
                            },
                            {
                                "start": 725,
                                "end": 852
                            },
                            {
                                "start": 853,
                                "end": 1066
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[276776838 | Kalpelbe et al. | 2025 | Citations: 2]",
                "snippets": "The idea behind LLaVa [16] is to develop an efficient foundation language model termed \"LLaVA\" (Language and Vision Assistant), which integrates both language understanding and visual comprehension. The primary objective is to enhance the capabilities of language models in multimodal contexts, enabling them to better interpret and generate text based on visual inputs. This involves not only improving performance in traditional language tasks but also expanding functionality to include complex visual reasoning and description tasks. \n\nThe authors employed a two-step training process. Initially, they pre-trained the LLaVA model on a filtered dataset comprising 595,000 image-text pairs, sourced from the CC3M dataset. This filtering process involved selecting noun-phrases based on their frequency to ensure diverse representation across concepts in the dataset. The model was pre-trained for one epoch using a learning rate of 2e-3 and a batch size of 128. Following pre-training, the model underwent finetuning on a specialized dataset, LLaVA-Instruct-158K, for three epochs with a reduced learning rate of 2e-5 and a batch size of 32. Various optimization techniques, such as using the Adam optimizer with no weight decay and enabling BF16 and TF32, were implemented to balance speed and precision during training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A. Sate-of-the-art VL models",
                        "pdf_hash": "",
                        "start": 385,
                        "end": 1708,
                        "sentence_offsets": [
                            {
                                "start": 375,
                                "end": 583
                            },
                            {
                                "start": 584,
                                "end": 755
                            },
                            {
                                "start": 756,
                                "end": 922
                            },
                            {
                                "start": 925,
                                "end": 974
                            },
                            {
                                "start": 975,
                                "end": 1108
                            },
                            {
                                "start": 1109,
                                "end": 1253
                            },
                            {
                                "start": 1254,
                                "end": 1348
                            },
                            {
                                "start": 1349,
                                "end": 1528
                            },
                            {
                                "start": 1529,
                                "end": 1708
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The idea behind LLaVa [16] is to develop an efficient foundation language model termed \"LLaVA\" (Language and Vision Assistant), which integrates both language understanding and visual comprehension. The primary objective is to enhance the capabilities of language models in multimodal contexts, enabling them to better interpret and generate text based on visual inputs. This involves not only improving performance in traditional language tasks but also expanding functionality to include complex visual reasoning and description tasks. \n\nThe authors employed a two-step training process. Initially, they pre-trained the LLaVA model on a filtered dataset comprising 595,000 image-text pairs, sourced from the CC3M dataset. This filtering process involved selecting noun-phrases based on their frequency to ensure diverse representation across concepts in the dataset. The model was pre-trained for one epoch using a learning rate of 2e-3 and a batch size of 128. Following pre-training, the model underwent finetuning on a specialized dataset, LLaVA-Instruct-158K, for three epochs with a reduced learning rate of 2e-5 and a batch size of 32. Various optimization techniques, such as using the Adam optimizer with no weight decay and enabling BF16 and TF32, were implemented to balance speed and precision during training."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[277452239 | Riggi et al. | 2025 | Citations: 1]",
                "snippets": "Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Model overview",
                        "pdf_hash": "",
                        "start": 470,
                        "end": 1550,
                        "sentence_offsets": [
                            {
                                "start": 470,
                                "end": 704
                            },
                            {
                                "start": 705,
                                "end": 964
                            },
                            {
                                "start": 965,
                                "end": 1241
                            },
                            {
                                "start": 1242,
                                "end": 1550
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[277510513 | Han et al. | 2025 | Citations: 4]",
                "snippets": "The core innovation of these models lies in joint training, enhancing the synergy between images and text, making them more widely applicable in multimodal tasks. MiniGPT-4 [205] and LLaVA (Liu et al., 2023) further refine this by aligning visuallanguage representations, improving the accuracy and flexibility of image-to-text generation. LLaVA (Liu et al., 2023) (Language and Vision Assistant) is an innovative visual-language model that employs instruction tuning, giving the model a high degree of flexibility in handling various tasks. LLaVA (Liu et al., 2023)'s ability to adjust to different tasks using natural language instructions enables it to not only respond rapidly to traditional visuallanguage tasks like VQA and image captioning, but also to adjust its execution strategy dynamically for new tasks. Through instruction tuning, LLaVA (Liu et al., 2023) can generate precise image descriptions or answer visual-related questions based on different task demands. This flexible task-switching capability makes LLaVA [211] particularly advantageous in complex environments, excelling in tasks such as cross-modal and visual reasoning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258179774 | Liu et al. | 2023 | Citations: 4921]": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available."
                },
                "metadata": [
                    {
                        "section_title": "Transformer Variants and Large Vision-Language Models",
                        "pdf_hash": "",
                        "start": 1127,
                        "end": 2222,
                        "sentence_offsets": [
                            {
                                "start": 1127,
                                "end": 1289
                            },
                            {
                                "start": 1290,
                                "end": 1453
                            },
                            {
                                "start": 1454,
                                "end": 1642
                            },
                            {
                                "start": 1643,
                                "end": 1904
                            },
                            {
                                "start": 1905,
                                "end": 2052
                            },
                            {
                                "start": 2053,
                                "end": 2222
                            }
                        ],
                        "ref_mentions": [
                            "258179774",
                            "258179774",
                            "258179774",
                            "258179774"
                        ],
                        "quote": "The core innovation of these models lies in joint training, enhancing the synergy between images and text, making them more widely applicable in multimodal tasks. MiniGPT-4 [205] and LLaVA (Liu et al., 2023) further refine this by aligning visuallanguage representations, improving the accuracy and flexibility of image-to-text generation. LLaVA (Liu et al., 2023) (Language and Vision Assistant) is an innovative visual-language model that employs instruction tuning, giving the model a high degree of flexibility in handling various tasks. LLaVA (Liu et al., 2023)'s ability to adjust to different tasks using natural language instructions enables it to not only respond rapidly to traditional visuallanguage tasks like VQA and image captioning, but also to adjust its execution strategy dynamically for new tasks. Through instruction tuning, LLaVA (Liu et al., 2023) can generate precise image descriptions or answer visual-related questions based on different task demands. This flexible task-switching capability makes LLaVA [211] particularly advantageous in complex environments, excelling in tasks such as cross-modal and visual reasoning."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.022683,
        "cot": "Looking at the user query, they want a detailed timeline of major LLaVA model releases and the specific improvements and innovations in each iteration. After reviewing the quotes, I can identify several major LLaVA models mentioned: the original LLaVA, LLaVA-1.5, LLaVA-NeXT (also called LLaVA-1.6), LLaVA-OneVision, Video-LLaVA, and other variants like LLaVA-Med and LLaVA-Interactive.\n\nI'll organize the dimensions as follows:\n\n1. \"Introduction to LLaVA Models\" - This will be a synthesis paragraph providing background on what LLaVA models are and their general purpose in the field of multimodal AI.\n\n2. \"Original LLaVA (2023)\" - This will be a synthesis paragraph describing the first LLaVA model, its architecture, and training process.\n\n3. \"LLaVA-1.5\" - This will be a synthesis paragraph covering the improvements made in LLaVA-1.5 over the original version.\n\n4. \"LLaVA-NeXT/LLaVA-1.6\" - This will be a synthesis paragraph detailing the innovations introduced in this iteration.\n\n5. \"LLaVA-OneVision\" - This will be a synthesis paragraph explaining this variant's approach to handling multiple modalities.\n\n6. \"Video Variants of LLaVA\" - This will be a synthesis paragraph covering Video-LLaVA and LLaVA-NeXT-Video.\n\n7. \"Other LLaVA Variants\" - This will be a list format covering other mentioned variants like LLaVA-Med, LLaVA-Interactive, MoE-LLaVA, and MG-LLaVA.\n\nThe quotes provide good information about each model's improvements, though the exact release dates aren't clearly specified in most cases. I'll organize the quotes according to which model variant they describe.",
        "plan": {
            "Introduction to LLaVA Models (synthesis)": [
                10,
                7
            ],
            "Original LLaVA (2023) (synthesis)": [
                0,
                8,
                9
            ],
            "LLaVA-1.5 (synthesis)": [
                0,
                1,
                2,
                6,
                7,
                9
            ],
            "LLaVA-NeXT/LLaVA-1.6 (synthesis)": [
                4,
                6,
                7,
                9
            ],
            "LLaVA-OneVision (synthesis)": [
                5,
                7,
                9
            ],
            "Video Variants of LLaVA (synthesis)": [
                4,
                6
            ],
            "Other LLaVA Variants (list)": [
                2,
                5
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to LLaVA Models",
                "tldr": "LLaVA (Large Language and Vision Assistant) models represent a significant advancement in multimodal AI by combining language models with visual understanding capabilities. These models use instruction tuning to enable flexible processing of both visual and textual information across various tasks. (3 sources)",
                "text": "\nLLaVA (Large Language and Vision Assistant) emerged as a pioneering approach to multimodal AI, connecting vision encoders with large language models (LLMs) to enable comprehensive understanding of both visual and textual information <Paper corpusId=\"258179774\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. The core innovation of LLaVA lies in its joint training methodology, which enhances the synergy between images and text, making it widely applicable across diverse multimodal tasks <Paper corpusId=\"277510513\" paperTitle=\"(Han et al., 2025)\" isShortName></Paper>. \n\nWhat distinguishes LLaVA from previous approaches is its use of instruction tuning, which gives the model exceptional flexibility in handling various visual-language tasks. This instruction-based approach allows LLaVA to dynamically adjust its execution strategy based on natural language prompts, enabling it to perform tasks ranging from visual question answering (VQA) to image captioning and complex visual reasoning <Paper corpusId=\"277510513\" paperTitle=\"(Han et al., 2025)\" isShortName></Paper>. By combining a visual encoder like CLIP with powerful LLMs, LLaVA was the first model to effectively understand multimodal instructions and take actions accordingly <Paper corpusId=\"275405668\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.\n\nSince its introduction, the LLaVA family has expanded significantly, with each iteration bringing important improvements. LLaVA-1.5 enhanced the model by encoding different types of data into vectors of the same dimension, enabling it to handle more modalities. Later versions like LLaVA-NeXT focused on processing video data, while LLaVA-OneVision proposed a unified model capable of simultaneously handling multiple modalities including single images, multiple images, videos, and audio <Paper corpusId=\"275405668\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available."
                        ],
                        "paper": {
                            "corpus_id": 258179774,
                            "title": "Visual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2143856368",
                                    "name": "Haotian Liu"
                                },
                                {
                                    "authorId": "2109737569",
                                    "name": "Chunyuan Li"
                                },
                                {
                                    "authorId": "31060482",
                                    "name": "Qingyang Wu"
                                },
                                {
                                    "authorId": "144756076",
                                    "name": "Yong Jae Lee"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 4921
                        },
                        "score": 0.52099609375
                    },
                    {
                        "id": "(Han et al., 2025)",
                        "snippets": [
                            "The core innovation of these models lies in joint training, enhancing the synergy between images and text, making them more widely applicable in multimodal tasks. MiniGPT-4 [205] and LLaVA (Liu et al., 2023) further refine this by aligning visuallanguage representations, improving the accuracy and flexibility of image-to-text generation. LLaVA (Liu et al., 2023) (Language and Vision Assistant) is an innovative visual-language model that employs instruction tuning, giving the model a high degree of flexibility in handling various tasks. LLaVA (Liu et al., 2023)'s ability to adjust to different tasks using natural language instructions enables it to not only respond rapidly to traditional visuallanguage tasks like VQA and image captioning, but also to adjust its execution strategy dynamically for new tasks. Through instruction tuning, LLaVA (Liu et al., 2023) can generate precise image descriptions or answer visual-related questions based on different task demands. This flexible task-switching capability makes LLaVA [211] particularly advantageous in complex environments, excelling in tasks such as cross-modal and visual reasoning."
                        ],
                        "paper": {
                            "corpus_id": 277510513,
                            "title": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision",
                            "authors": [
                                {
                                    "authorId": "2353440123",
                                    "name": "Xiaofeng Han"
                                },
                                {
                                    "authorId": "2282098650",
                                    "name": "Shunpeng Chen"
                                },
                                {
                                    "authorId": "2354260566",
                                    "name": "Zenghuang Fu"
                                },
                                {
                                    "authorId": "2353852983",
                                    "name": "Zhe Feng"
                                },
                                {
                                    "authorId": "2354113244",
                                    "name": "Lue Fan"
                                },
                                {
                                    "authorId": "2353389657",
                                    "name": "Dong An"
                                },
                                {
                                    "authorId": "1490749570",
                                    "name": "Changwei Wang"
                                },
                                {
                                    "authorId": "2263619001",
                                    "name": "Li Guo"
                                },
                                {
                                    "authorId": "35965884",
                                    "name": "Weiliang Meng"
                                },
                                {
                                    "authorId": "2125434570",
                                    "name": "Xiaopeng Zhang"
                                },
                                {
                                    "authorId": "2106213769",
                                    "name": "Rongtao Xu"
                                },
                                {
                                    "authorId": "2261685321",
                                    "name": "Shibiao Xu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.6083984375
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously."
                        ],
                        "paper": {
                            "corpus_id": 275405668,
                            "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding",
                            "authors": [
                                {
                                    "authorId": "2313875208",
                                    "name": "Jia-Xin Zhao"
                                },
                                {
                                    "authorId": "2342467513",
                                    "name": "Boyuan Sun"
                                },
                                {
                                    "authorId": "2339423925",
                                    "name": "Xiang Chen"
                                },
                                {
                                    "authorId": "2339268195",
                                    "name": "Xihan Wei"
                                },
                                {
                                    "authorId": "2339266488",
                                    "name": "Qibin Hou"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.51123046875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Original LLaVA (2023)",
                "tldr": "The original LLaVA model introduced in 2023 combined CLIP-ViT-L/14 as a visual encoder with Vicuna-13B as its language model, creating an efficient multimodal AI system capable of visual reasoning and understanding. It employed a two-stage training approach with pre-training on filtered image-text pairs followed by instruction fine-tuning. (4 sources)",
                "text": "\nThe first LLaVA (Large Language and Vision Assistant) model, released in 2023, represented a significant step forward in multimodal AI by seamlessly integrating vision and language capabilities. The architecture elegantly combined a pre-trained large language model, specifically Vicuna-13B, with a pre-trained visual encoder, CLIP-ViT-L/14, to create a comprehensive multimodal system <Paper corpusId=\"261049617\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"231591445\" paperTitle=\"(Radford et al., 2021)\" isShortName></Paper>.\n\nThe developers implemented a strategic two-stage training process for LLaVA. The initial pre-training phase utilized a carefully filtered dataset of 595,000 image-text pairs derived from the CC3M dataset. This filtering involved selecting noun phrases based on their frequency to ensure diverse concept representation. The model was pre-trained for one epoch with a learning rate of 2e-3 and a batch size of 128. Following pre-training, LLaVA underwent fine-tuning on a specialized dataset called LLaVA-Instruct-158K for three epochs, using a reduced learning rate of 2e-5 and a batch size of 32 <Paper corpusId=\"276776838\" paperTitle=\"(Kalpelbe et al., 2025)\" isShortName></Paper>.\n\nEven in its first release, LLaVA demonstrated exceptional multimodal conversational abilities, often showing behavior comparable to GPT-4V when interpreting new images and following novel instructions. The model processed images at 224x224 resolution and used a linear projection layer to connect its visual and language components <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"261049617\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. This initial version established the foundation for the subsequent iterations that would further enhance the model's capabilities across diverse tasks and modalities.",
                "citations": [
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "In order to gauge the effectiveness of our data generation strategy, we have elected to utilize the open-sourced LLaVA [20,21] model as our multimodal LLM model",
                            "It should be noted that our pipeline is model-agnostic, meaning the datasets generated via our approach can be employed for training a variety of other models",
                            "Architecture. The architecture of the LLaVA model elegantly combines a pre-trained LLM, specifically the Vicuna-13B [5], and a pre-trained visual encoder, known as CLIP-ViT-L/14 (Radford et al., 2021)",
                            "It's worth mentioning that in LLaVA-1.5 [20], the linear projection layer has been substituted with a twolayer MLP. Additionally, the LLM model has been replaced with Vicuna-1.5-13B, and the input image size has been increased from 224x224 to 336x336, thereby elevating the model's multimodal capabilities to a greater extent."
                        ],
                        "paper": {
                            "corpus_id": 261049617,
                            "title": "StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data",
                            "authors": [
                                {
                                    "authorId": "2232930725",
                                    "name": "Yanda Li"
                                },
                                {
                                    "authorId": "144876211",
                                    "name": "Chi Zhang"
                                },
                                {
                                    "authorId": "2116565951",
                                    "name": "Gang Yu"
                                },
                                {
                                    "authorId": "2051262469",
                                    "name": "Zhibin Wang"
                                },
                                {
                                    "authorId": "2107058893",
                                    "name": "Bin Fu"
                                },
                                {
                                    "authorId": "2604251",
                                    "name": "Guosheng Lin"
                                },
                                {
                                    "authorId": "12459603",
                                    "name": "Chunhua Shen"
                                },
                                {
                                    "authorId": "2232790293",
                                    "name": "Ling Chen"
                                },
                                {
                                    "authorId": "49020088",
                                    "name": "Yunchao Wei"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 31
                        },
                        "score": 0.64892578125
                    },
                    {
                        "id": "(Radford et al., 2021)",
                        "snippets": [
                            "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."
                        ],
                        "paper": {
                            "corpus_id": 231591445,
                            "title": "Learning Transferable Visual Models From Natural Language Supervision",
                            "authors": [
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "2110935237",
                                    "name": "Jong Wook Kim"
                                },
                                {
                                    "authorId": "2004021329",
                                    "name": "Chris Hallacy"
                                },
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "40087786",
                                    "name": "Gabriel Goh"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "144864359",
                                    "name": "Girish Sastry"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "2051714782",
                                    "name": "Pamela Mishkin"
                                },
                                {
                                    "authorId": "2115193883",
                                    "name": "Jack Clark"
                                },
                                {
                                    "authorId": "2064404342",
                                    "name": "Gretchen Krueger"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 29867
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kalpelbe et al., 2025)",
                        "snippets": [
                            "The idea behind LLaVa [16] is to develop an efficient foundation language model termed \"LLaVA\" (Language and Vision Assistant), which integrates both language understanding and visual comprehension. The primary objective is to enhance the capabilities of language models in multimodal contexts, enabling them to better interpret and generate text based on visual inputs. This involves not only improving performance in traditional language tasks but also expanding functionality to include complex visual reasoning and description tasks. \n\nThe authors employed a two-step training process. Initially, they pre-trained the LLaVA model on a filtered dataset comprising 595,000 image-text pairs, sourced from the CC3M dataset. This filtering process involved selecting noun-phrases based on their frequency to ensure diverse representation across concepts in the dataset. The model was pre-trained for one epoch using a learning rate of 2e-3 and a batch size of 128. Following pre-training, the model underwent finetuning on a specialized dataset, LLaVA-Instruct-158K, for three epochs with a reduced learning rate of 2e-5 and a batch size of 32. Various optimization techniques, such as using the Adam optimizer with no weight decay and enabling BF16 and TF32, were implemented to balance speed and precision during training."
                        ],
                        "paper": {
                            "corpus_id": 276776838,
                            "title": "Vision Language Models in Medicine",
                            "authors": [
                                {
                                    "authorId": "2348482300",
                                    "name": "Beria Chingnabe Kalpelbe"
                                },
                                {
                                    "authorId": "2348482178",
                                    "name": "Angel Gabriel Adaambiik"
                                },
                                {
                                    "authorId": "2348517139",
                                    "name": "Wei Peng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.69677734375
                    },
                    {
                        "id": "(Riggi et al., 2025)",
                        "snippets": [
                            "Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks."
                        ],
                        "paper": {
                            "corpus_id": 277452239,
                            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
                            "authors": [
                                {
                                    "authorId": "2292400830",
                                    "name": "S. Riggi"
                                },
                                {
                                    "authorId": "2042077694",
                                    "name": "T. Cecconello"
                                },
                                {
                                    "authorId": "2352941747",
                                    "name": "A. Pilzer"
                                },
                                {
                                    "authorId": "2352939581",
                                    "name": "S. Palazzo"
                                },
                                {
                                    "authorId": "2299008238",
                                    "name": "N. Gupta"
                                },
                                {
                                    "authorId": "2298907506",
                                    "name": "A. Hopkins"
                                },
                                {
                                    "authorId": "2258840598",
                                    "name": "C. Trigilio"
                                },
                                {
                                    "authorId": "2349648144",
                                    "name": "G. Umana"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.8369140625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "LLaVA-1.5",
                "tldr": "LLaVA-1.5, released in 2023, introduced significant architectural improvements including a two-layer MLP connector replacing the linear projection layer, increased image resolution from 224\u00d7224 to 336\u00d7336, and incorporated more academic task-oriented instruction data. These enhancements resulted in substantial performance gains across multiple benchmarks while maintaining training efficiency. (5 sources)",
                "text": "\nBuilding upon the original LLaVA model, LLaVA-1.5 introduced several key architectural and data improvements that substantially enhanced its multimodal capabilities. One of the most significant changes was replacing the simple linear projection layer with a more sophisticated two-layer MLP (Multilayer Perceptron) to connect the vision and language components <Paper corpusId=\"261049617\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. This modification allowed for better integration between the visual encoder and language model, improving the model's ability to process multimodal information.\n\nLLaVA-1.5 also increased the input image resolution from 224\u00d7224 to 336\u00d7336 pixels, allowing the model to capture more detailed visual information <Paper corpusId=\"261049617\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. Additionally, the model upgraded its language foundation by adopting Vicuna-1.5-13B, which provided enhanced language understanding capabilities <Paper corpusId=\"261049617\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nAnother critical improvement in LLaVA-1.5 was the incorporation of more comprehensive and academic task-oriented visual question-answering data with response formatting prompts <Paper corpusId=\"266374969\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This expanded dataset enabled the model to perform better on specialized academic tasks while maintaining its general-purpose capabilities. The data-efficient approach allowed LLaVA-1.5 to achieve impressive results with just 1.2 million publicly available training examples, and the entire training process could be completed in approximately one day on a single 8-A100 node <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>.\n\nLLaVA-1.5 maintained the core architecture of using CLIP as the visual encoder (specifically CLIP-ViT-L) and Vicuna as the text decoder while enhancing the connector between them <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This model was made available in two sizes, 7B and 13B parameters, providing options for different computational constraints <Paper corpusId=\"266374969\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>.\n\nThe culmination of these improvements resulted in LLaVA-1.5 achieving state-of-the-art performance across 11 benchmarks at the time of its release <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>. The model's enhanced capabilities for encoding different types of data into vectors of the same dimension also laid the groundwork for handling a wider range of modalities in future iterations <Paper corpusId=\"275405668\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>. This significant leap in performance established LLaVA-1.5 as a cornerstone in multimodal AI research, with its simple design, low tuning cost, and outstanding performance making it widely adopted in the field <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "In order to gauge the effectiveness of our data generation strategy, we have elected to utilize the open-sourced LLaVA [20,21] model as our multimodal LLM model",
                            "It should be noted that our pipeline is model-agnostic, meaning the datasets generated via our approach can be employed for training a variety of other models",
                            "Architecture. The architecture of the LLaVA model elegantly combines a pre-trained LLM, specifically the Vicuna-13B [5], and a pre-trained visual encoder, known as CLIP-ViT-L/14 (Radford et al., 2021)",
                            "It's worth mentioning that in LLaVA-1.5 [20], the linear projection layer has been substituted with a twolayer MLP. Additionally, the LLM model has been replaced with Vicuna-1.5-13B, and the input image size has been increased from 224x224 to 336x336, thereby elevating the model's multimodal capabilities to a greater extent."
                        ],
                        "paper": {
                            "corpus_id": 261049617,
                            "title": "StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data",
                            "authors": [
                                {
                                    "authorId": "2232930725",
                                    "name": "Yanda Li"
                                },
                                {
                                    "authorId": "144876211",
                                    "name": "Chi Zhang"
                                },
                                {
                                    "authorId": "2116565951",
                                    "name": "Gang Yu"
                                },
                                {
                                    "authorId": "2051262469",
                                    "name": "Zhibin Wang"
                                },
                                {
                                    "authorId": "2107058893",
                                    "name": "Bin Fu"
                                },
                                {
                                    "authorId": "2604251",
                                    "name": "Guosheng Lin"
                                },
                                {
                                    "authorId": "12459603",
                                    "name": "Chunhua Shen"
                                },
                                {
                                    "authorId": "2232790293",
                                    "name": "Ling Chen"
                                },
                                {
                                    "authorId": "49020088",
                                    "name": "Yunchao Wei"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 31
                        },
                        "score": 0.64892578125
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "LLaVA-1.5 (Liu et al., 2023). LLaVA is one of the most widely used open-source vision-language models, and its simple design, low tuning cost, and outstanding performance make it a cornerstone in the field of multi-modal models. Specifically, LLaVA employs a pre-trained CLIP as the visual encoder and Vicuna as the text decoder. A simple linear projector connects the two modules, enabling the LLM to accept visual tokens of CLIP as input. Meanwhile, visual instruction tuning allows the model to handle vision-language tasks. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement. LLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM. Video-LLaVA [32]. On the basis of image understanding, Video-LLaVA extends this capability to video comprehension. It unifies representations of images and videos through alignment before projection."
                        ],
                        "paper": {
                            "corpus_id": 274437586,
                            "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs",
                            "authors": [
                                {
                                    "authorId": "2118051520",
                                    "name": "Qizhe Zhang"
                                },
                                {
                                    "authorId": "2292408664",
                                    "name": "Aosong Cheng"
                                },
                                {
                                    "authorId": "2331417542",
                                    "name": "Ming Lu"
                                },
                                {
                                    "authorId": "2275104296",
                                    "name": "Renrui Zhang"
                                },
                                {
                                    "authorId": "2333364107",
                                    "name": "Zhiyong Zhuo"
                                },
                                {
                                    "authorId": "2268711797",
                                    "name": "Jiajun Cao"
                                },
                                {
                                    "authorId": "2333442704",
                                    "name": "Shaobo Guo"
                                },
                                {
                                    "authorId": "2331326229",
                                    "name": "Qi She"
                                },
                                {
                                    "authorId": "2332857566",
                                    "name": "Shanghang Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 10
                        },
                        "score": 0.78173828125
                    },
                    {
                        "id": "(Wu et al., 2023)",
                        "snippets": [
                            "LLaVA-0: LLaVA-Med is initialized with LLaVA-0 and then continuously trained with a comprehensive dataset of biomedical figure-caption pairs sourced from PubMed Central.\n\nLLaVA-1.5: LLaVA-1.5 is a general domain VLM that uses the LLaMA2 model, which has a significant improvement in language understanding when compared with LLaMA, as the backbone LLM (18,20). There are two significant improvements besides the change of the backbone LLM. Firstly, the addition of an MLP vision-language connector enhanced the system's capabilities. Secondly, the integration of academic task-oriented data further enhanced its performance and effectiveness. LLaVA-1.5 is available in 2 model sizes, 7B and 13B models, and we used both models."
                        ],
                        "paper": {
                            "corpus_id": 266374969,
                            "title": "Exploring Multimodal Large Language Models for Radiology Report Error-checking",
                            "authors": [
                                {
                                    "authorId": "2182815661",
                                    "name": "Jinge Wu"
                                },
                                {
                                    "authorId": "2275715280",
                                    "name": "Yunsoo Kim"
                                },
                                {
                                    "authorId": "2275354702",
                                    "name": "Eva C. Keller"
                                },
                                {
                                    "authorId": "2275129334",
                                    "name": "Jamie Chow"
                                },
                                {
                                    "authorId": "2275239607",
                                    "name": "Adam P. Levine"
                                },
                                {
                                    "authorId": "48573031",
                                    "name": "N. Pontikos"
                                },
                                {
                                    "authorId": "2275359351",
                                    "name": "Zina Ibrahim"
                                },
                                {
                                    "authorId": "2257359718",
                                    "name": "Paul Taylor"
                                },
                                {
                                    "authorId": "2276324181",
                                    "name": "Michelle C. Williams"
                                },
                                {
                                    "authorId": "2241781737",
                                    "name": "Honghan Wu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.56103515625
                    },
                    {
                        "id": "(Liu et al._1, 2023)",
                        "snippets": [
                            "LLaVA-1.5. We denote this final model with all the modifications as LLaVA-1.5 (the last two rows in Table 2), which achieves an impressive performance that significantly outperforms the original LLaVA (Liu et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 263672058,
                            "title": "Improved Baselines with Visual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2143856368",
                                    "name": "Haotian Liu"
                                },
                                {
                                    "authorId": "2243126534",
                                    "name": "Chunyuan Li"
                                },
                                {
                                    "authorId": "1527091339",
                                    "name": "Yuheng Li"
                                },
                                {
                                    "authorId": "2256122200",
                                    "name": "Yong Jae Lee"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 2824
                        },
                        "score": 0.53369140625
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously."
                        ],
                        "paper": {
                            "corpus_id": 275405668,
                            "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding",
                            "authors": [
                                {
                                    "authorId": "2313875208",
                                    "name": "Jia-Xin Zhao"
                                },
                                {
                                    "authorId": "2342467513",
                                    "name": "Boyuan Sun"
                                },
                                {
                                    "authorId": "2339423925",
                                    "name": "Xiang Chen"
                                },
                                {
                                    "authorId": "2339268195",
                                    "name": "Xihan Wei"
                                },
                                {
                                    "authorId": "2339266488",
                                    "name": "Qibin Hou"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.51123046875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "LLaVA-NeXT/LLaVA-1.6",
                "tldr": "LLaVA-NeXT (also known as LLaVA-1.6) introduced a dynamic high-resolution design called AnyRes that adaptively selects the best aspect ratio for input images, enabling up to 4\u00d7 higher resolution processing. Released in early 2024, it significantly improved reasoning, OCR capabilities, and world knowledge while maintaining data efficiency comparable to LLaVA-1.5. (4 sources)",
                "text": "\nBuilding on the success of LLaVA-1.5, LLaVA-NeXT (also referred to as LLaVA-1.6) was introduced in early 2024 with substantial improvements in visual understanding capabilities <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. While LLaVA-1.5 increased resolution from 224\u00d7224 to a fixed 336\u00d7336, LLaVA-NeXT took a more sophisticated approach by implementing a dynamic high-resolution design called AnyRes <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>.\n\nThe most innovative aspect of LLaVA-NeXT is its ability to adaptively select the optimal aspect ratio based on the input image's resolution, allowing for resolution increases of up to 4\u00d7 compared to previous models <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This was achieved through a clever processing technique: rather than modifying the visual encoder, high-resolution images are split into multiple sub-images of the same size as the original input resolution. These sub-images are individually encoded and then concatenated before being fed into the language model <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nLLaVA-NeXT demonstrated significantly improved reasoning capabilities, OCR (optical character recognition) performance, and world knowledge compared to its predecessors while maintaining data efficiency comparable to LLaVA-1.5 <Paper corpusId=\"273638057\" paperTitle=\"(Elgendy et al., 2024)\" isShortName></Paper>. The model also provided enhanced visual conversation capabilities, making it more effective at interpreting and discussing visual content <Paper corpusId=\"273638057\" paperTitle=\"(Elgendy et al., 2024)\" isShortName></Paper>.\n\nShortly after its release, LLaVA-NeXT was extended to handle video data with the introduction of LLaVA-NeXT-Video, which demonstrated strong performance in zero-shot video tasks <Paper corpusId=\"273638057\" paperTitle=\"(Elgendy et al., 2024)\" isShortName></Paper> <Paper corpusId=\"275405668\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>. This video capability built on another innovation from the broader LLaVA ecosystem, Video-LLaVA, which unified the representations of images and videos through alignment before projection <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nThe advancement of LLaVA-NeXT was made possible by three key innovations: the AnyRes technique for processing high-resolution images, expansion of high-quality instruction datasets, and integration of the most advanced open-source LLMs available at the time <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>. These improvements collectively enhanced the model's capabilities across diverse tasks and set the stage for even more comprehensive multimodal models in the future.",
                "citations": [
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "LLaVA-1.5 (Liu et al., 2023). LLaVA is one of the most widely used open-source vision-language models, and its simple design, low tuning cost, and outstanding performance make it a cornerstone in the field of multi-modal models. Specifically, LLaVA employs a pre-trained CLIP as the visual encoder and Vicuna as the text decoder. A simple linear projector connects the two modules, enabling the LLM to accept visual tokens of CLIP as input. Meanwhile, visual instruction tuning allows the model to handle vision-language tasks. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement. LLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM. Video-LLaVA [32]. On the basis of image understanding, Video-LLaVA extends this capability to video comprehension. It unifies representations of images and videos through alignment before projection."
                        ],
                        "paper": {
                            "corpus_id": 274437586,
                            "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs",
                            "authors": [
                                {
                                    "authorId": "2118051520",
                                    "name": "Qizhe Zhang"
                                },
                                {
                                    "authorId": "2292408664",
                                    "name": "Aosong Cheng"
                                },
                                {
                                    "authorId": "2331417542",
                                    "name": "Ming Lu"
                                },
                                {
                                    "authorId": "2275104296",
                                    "name": "Renrui Zhang"
                                },
                                {
                                    "authorId": "2333364107",
                                    "name": "Zhiyong Zhuo"
                                },
                                {
                                    "authorId": "2268711797",
                                    "name": "Jiajun Cao"
                                },
                                {
                                    "authorId": "2333442704",
                                    "name": "Shaobo Guo"
                                },
                                {
                                    "authorId": "2331326229",
                                    "name": "Qi She"
                                },
                                {
                                    "authorId": "2332857566",
                                    "name": "Shanghang Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 10
                        },
                        "score": 0.78173828125
                    },
                    {
                        "id": "(Riggi et al., 2025)",
                        "snippets": [
                            "Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks."
                        ],
                        "paper": {
                            "corpus_id": 277452239,
                            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
                            "authors": [
                                {
                                    "authorId": "2292400830",
                                    "name": "S. Riggi"
                                },
                                {
                                    "authorId": "2042077694",
                                    "name": "T. Cecconello"
                                },
                                {
                                    "authorId": "2352941747",
                                    "name": "A. Pilzer"
                                },
                                {
                                    "authorId": "2352939581",
                                    "name": "S. Palazzo"
                                },
                                {
                                    "authorId": "2299008238",
                                    "name": "N. Gupta"
                                },
                                {
                                    "authorId": "2298907506",
                                    "name": "A. Hopkins"
                                },
                                {
                                    "authorId": "2258840598",
                                    "name": "C. Trigilio"
                                },
                                {
                                    "authorId": "2349648144",
                                    "name": "G. Umana"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.8369140625
                    },
                    {
                        "id": "(Elgendy et al., 2024)",
                        "snippets": [
                            "In our work, we focus on fine-tuning two distinct models that have demonstrated a robust understanding of temporal data through video processing within the VLM framework for question-answering and captioning. The first model, LLaVA-NeXT [20], was introduced in early 2024, offering improved reasoning and world knowledge compared to other large models. It exhibits data efficiency comparable to SOTA models such as LLaVA-1.5 (Liu et al., 2023), while delivering higher image resolution and enhanced visual conversation capabilities. Shortly after the release of LLaVA-NeXT, a video variant was introduced, named LLaVA-NeXT-Video, which has demonstrated strong performance in zero-shot video tasks."
                        ],
                        "paper": {
                            "corpus_id": 273638057,
                            "title": "GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing",
                            "authors": [
                                {
                                    "authorId": "2305201677",
                                    "name": "Hosam Elgendy"
                                },
                                {
                                    "authorId": "2265656556",
                                    "name": "Ahmed Sharshar"
                                },
                                {
                                    "authorId": "2292596478",
                                    "name": "Ahmed Aboeitta"
                                },
                                {
                                    "authorId": "2292603520",
                                    "name": "Yasser Ashraf"
                                },
                                {
                                    "authorId": "2327863134",
                                    "name": "Mohsen Guizani"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.6220703125
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously."
                        ],
                        "paper": {
                            "corpus_id": 275405668,
                            "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding",
                            "authors": [
                                {
                                    "authorId": "2313875208",
                                    "name": "Jia-Xin Zhao"
                                },
                                {
                                    "authorId": "2342467513",
                                    "name": "Boyuan Sun"
                                },
                                {
                                    "authorId": "2339423925",
                                    "name": "Xiang Chen"
                                },
                                {
                                    "authorId": "2339268195",
                                    "name": "Xihan Wei"
                                },
                                {
                                    "authorId": "2339266488",
                                    "name": "Qibin Hou"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.51123046875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "LLaVA-OneVision",
                "tldr": "LLaVA-OneVision extended the LLaVA family's capabilities by creating a unified model that can simultaneously handle multiple visual modalities including single images, multiple images, videos, and audio. This advancement addressed previous performance limitations across diverse visual scenarios while maintaining the core strengths of the LLaVA architecture. (3 sources)",
                "text": "\nLLaVA-OneVision, introduced by Li et al. in 2024, represents a significant expansion of the LLaVA model family by creating a unified architecture capable of handling multiple modalities simultaneously <Paper corpusId=\"275405668\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>. While earlier LLaVA models were primarily focused on single image processing, with LLaVA-NeXT extending capabilities to video data, LLaVA-OneVision broadened the scope considerably by enabling concurrent processing of single images, multiple images, videos, and even audio inputs within a single model framework <Paper corpusId=\"275405668\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>.\n\nThe primary motivation behind LLaVA-OneVision was to address the performance limitations previous models faced when managing diverse visual scenarios simultaneously <Paper corpusId=\"273821149\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper>. This unified approach allowed the model to maintain consistent performance across different input types without the need for separate specialized models for each modality. LLaVA-OneVision built upon the architectural innovations introduced in LLaVA-NeXT, including the AnyRes technique for high-resolution image processing, while further expanding the high-quality instruction datasets to encompass multi-modal training examples <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>.\n\nThe development of LLaVA-OneVision represents part of a broader trend in the LLaVA ecosystem toward more versatile and comprehensive multimodal capabilities. Other variants like LLaVA-Interactive expanded functionality with features such as image chatting, segmentation, and generation capabilities, while MoE-LLaVA explored sparse LVLM architecture based on Mixture of Experts to address performance degradation in multimodal sparse learning <Paper corpusId=\"273821149\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper>. Together, these advancements have significantly enhanced the practical utility of LLaVA models across increasingly diverse application scenarios.",
                "citations": [
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously."
                        ],
                        "paper": {
                            "corpus_id": 275405668,
                            "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding",
                            "authors": [
                                {
                                    "authorId": "2313875208",
                                    "name": "Jia-Xin Zhao"
                                },
                                {
                                    "authorId": "2342467513",
                                    "name": "Boyuan Sun"
                                },
                                {
                                    "authorId": "2339423925",
                                    "name": "Xiang Chen"
                                },
                                {
                                    "authorId": "2339268195",
                                    "name": "Xihan Wei"
                                },
                                {
                                    "authorId": "2339266488",
                                    "name": "Qibin Hou"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.51123046875
                    },
                    {
                        "id": "(Riggi et al., 2025)",
                        "snippets": [
                            "Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks."
                        ],
                        "paper": {
                            "corpus_id": 277452239,
                            "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks",
                            "authors": [
                                {
                                    "authorId": "2292400830",
                                    "name": "S. Riggi"
                                },
                                {
                                    "authorId": "2042077694",
                                    "name": "T. Cecconello"
                                },
                                {
                                    "authorId": "2352941747",
                                    "name": "A. Pilzer"
                                },
                                {
                                    "authorId": "2352939581",
                                    "name": "S. Palazzo"
                                },
                                {
                                    "authorId": "2299008238",
                                    "name": "N. Gupta"
                                },
                                {
                                    "authorId": "2298907506",
                                    "name": "A. Hopkins"
                                },
                                {
                                    "authorId": "2258840598",
                                    "name": "C. Trigilio"
                                },
                                {
                                    "authorId": "2349648144",
                                    "name": "G. Umana"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.8369140625
                    },
                    {
                        "id": "(Dai et al., 2024)",
                        "snippets": [
                            "Liu at el. introduced an end-to-end trained large vision-language assistant (LLaVA [51]) on instruction-following data for general purpose visual and language understanding, which gained widespread attention upon release. Subsequent research has further enhanced LLaVA's performance. For instance, LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios. LLaVA-Interactive [53] serves as a comprehensive demonstration platform, incorporating features such as image chatting, segmentation, and generation and editing capabilities, significantly expanding LLaVA's original functionalities. MoE-LLaVA [54], a sparse LVLM architecture based on Mixture of Experts (MoE), was developed to tackle performance degradation in multimodal sparse learning. MG-LLaVA [55] enhanced the model's visual processing capabilities by introducing multi-granularity visual streams, allowing it to handle features at various resolutions and object centers."
                        ],
                        "paper": {
                            "corpus_id": 273821149,
                            "title": "HumanVLM: Foundation for Human-Scene Vision-Language Model",
                            "authors": [
                                {
                                    "authorId": "2082462168",
                                    "name": "Dawei Dai"
                                },
                                {
                                    "authorId": "2329189750",
                                    "name": "Xu Long"
                                },
                                {
                                    "authorId": "2136494548",
                                    "name": "Yutang Li"
                                },
                                {
                                    "authorId": "2310835404",
                                    "name": "Yuanhui Zhang"
                                },
                                {
                                    "authorId": "2147222435",
                                    "name": "Shuy Xia"
                                }
                            ],
                            "year": 2024,
                            "venue": "Information Fusion",
                            "n_citations": 2
                        },
                        "score": 0.8046875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Video Variants of LLaVA",
                "tldr": "Building on image-based capabilities, the LLaVA model family expanded to handle video data through specialized variants. Notable developments include Video-LLaVA, which unified image and video representations through alignment before projection, and LLaVA-NeXT-Video, which leveraged the advanced reasoning capabilities of LLaVA-NeXT for zero-shot video understanding. (2 sources)",
                "text": "\nAs the LLaVA family of models demonstrated impressive capabilities in processing static images, researchers turned their attention to extending these capabilities to video understanding. Video-LLaVA marked an important advancement in this direction by successfully unifying the representations of images and videos through alignment before projection <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This approach enabled the model to effectively transition from static image comprehension to temporal visual understanding, bridging a significant gap in multimodal AI capabilities.\n\nFollowing the release of LLaVA-NeXT (LLaVA-1.6) with its enhanced reasoning and world knowledge capabilities, a specialized video variant called LLaVA-NeXT-Video was introduced in early 2024 <Paper corpusId=\"273638057\" paperTitle=\"(Elgendy et al., 2024)\" isShortName></Paper>. This model maintained the data efficiency comparable to LLaVA-1.5 while delivering enhanced visual conversation capabilities specifically tailored for video content <Paper corpusId=\"273638057\" paperTitle=\"(Elgendy et al., 2024)\" isShortName></Paper>. LLaVA-NeXT-Video quickly demonstrated strong performance in zero-shot video tasks, showing the model's ability to understand and reason about temporal visual information without explicit training on specific video scenarios <Paper corpusId=\"273638057\" paperTitle=\"(Elgendy et al., 2024)\" isShortName></Paper>. \n\nThe progression from image-only models to video-capable variants represents a natural evolution in the LLaVA model family's development, creating a foundation for more comprehensive multimodal understanding that would later be consolidated in unified approaches like LLaVA-OneVision.",
                "citations": [
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "LLaVA-1.5 (Liu et al., 2023). LLaVA is one of the most widely used open-source vision-language models, and its simple design, low tuning cost, and outstanding performance make it a cornerstone in the field of multi-modal models. Specifically, LLaVA employs a pre-trained CLIP as the visual encoder and Vicuna as the text decoder. A simple linear projector connects the two modules, enabling the LLM to accept visual tokens of CLIP as input. Meanwhile, visual instruction tuning allows the model to handle vision-language tasks. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement. LLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM. Video-LLaVA [32]. On the basis of image understanding, Video-LLaVA extends this capability to video comprehension. It unifies representations of images and videos through alignment before projection."
                        ],
                        "paper": {
                            "corpus_id": 274437586,
                            "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs",
                            "authors": [
                                {
                                    "authorId": "2118051520",
                                    "name": "Qizhe Zhang"
                                },
                                {
                                    "authorId": "2292408664",
                                    "name": "Aosong Cheng"
                                },
                                {
                                    "authorId": "2331417542",
                                    "name": "Ming Lu"
                                },
                                {
                                    "authorId": "2275104296",
                                    "name": "Renrui Zhang"
                                },
                                {
                                    "authorId": "2333364107",
                                    "name": "Zhiyong Zhuo"
                                },
                                {
                                    "authorId": "2268711797",
                                    "name": "Jiajun Cao"
                                },
                                {
                                    "authorId": "2333442704",
                                    "name": "Shaobo Guo"
                                },
                                {
                                    "authorId": "2331326229",
                                    "name": "Qi She"
                                },
                                {
                                    "authorId": "2332857566",
                                    "name": "Shanghang Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 10
                        },
                        "score": 0.78173828125
                    },
                    {
                        "id": "(Elgendy et al., 2024)",
                        "snippets": [
                            "In our work, we focus on fine-tuning two distinct models that have demonstrated a robust understanding of temporal data through video processing within the VLM framework for question-answering and captioning. The first model, LLaVA-NeXT [20], was introduced in early 2024, offering improved reasoning and world knowledge compared to other large models. It exhibits data efficiency comparable to SOTA models such as LLaVA-1.5 (Liu et al., 2023), while delivering higher image resolution and enhanced visual conversation capabilities. Shortly after the release of LLaVA-NeXT, a video variant was introduced, named LLaVA-NeXT-Video, which has demonstrated strong performance in zero-shot video tasks."
                        ],
                        "paper": {
                            "corpus_id": 273638057,
                            "title": "GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing",
                            "authors": [
                                {
                                    "authorId": "2305201677",
                                    "name": "Hosam Elgendy"
                                },
                                {
                                    "authorId": "2265656556",
                                    "name": "Ahmed Sharshar"
                                },
                                {
                                    "authorId": "2292596478",
                                    "name": "Ahmed Aboeitta"
                                },
                                {
                                    "authorId": "2292603520",
                                    "name": "Yasser Ashraf"
                                },
                                {
                                    "authorId": "2327863134",
                                    "name": "Mohsen Guizani"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.6220703125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Other LLaVA Variants",
                "tldr": "The LLaVA model family has expanded to include numerous specialized variants addressing different use cases and technical challenges. These include domain-specific models like LLaVA-Med for biomedical applications, architecture innovations like MoE-LLaVA using Mixture of Experts, and enhanced interaction capabilities as seen in LLaVA-Interactive. (2 sources)",
                "text": "\n- **LLaVA-Med**: A specialized biomedical variant initialized with LLaVA-0 and further trained on biomedical figure-caption pairs from PubMed Central, adapting the model's capabilities to healthcare and medical imaging applications. <Paper corpusId=\"266374969\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>\n\n- **MoE-LLaVA**: Developed as a sparse LVLM (Large Vision-Language Model) architecture based on Mixture of Experts (MoE), this variant specifically addresses performance degradation issues in multimodal sparse learning, offering improved efficiency. <Paper corpusId=\"273821149\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper>\n\n- **LLaVA-Interactive**: This variant significantly expanded the original LLaVA functionalities by incorporating interactive features such as image chatting, segmentation, and generation and editing capabilities, serving as a comprehensive demonstration platform for multimodal AI interaction. <Paper corpusId=\"273821149\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper>\n\n- **MG-LLaVA** (Multi-Granularity LLaVA): Enhanced the model's visual processing capabilities by introducing multi-granularity visual streams, enabling the model to handle features at various resolutions and object centers for more comprehensive visual understanding. <Paper corpusId=\"273821149\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Wu et al., 2023)",
                        "snippets": [
                            "LLaVA-0: LLaVA-Med is initialized with LLaVA-0 and then continuously trained with a comprehensive dataset of biomedical figure-caption pairs sourced from PubMed Central.\n\nLLaVA-1.5: LLaVA-1.5 is a general domain VLM that uses the LLaMA2 model, which has a significant improvement in language understanding when compared with LLaMA, as the backbone LLM (18,20). There are two significant improvements besides the change of the backbone LLM. Firstly, the addition of an MLP vision-language connector enhanced the system's capabilities. Secondly, the integration of academic task-oriented data further enhanced its performance and effectiveness. LLaVA-1.5 is available in 2 model sizes, 7B and 13B models, and we used both models."
                        ],
                        "paper": {
                            "corpus_id": 266374969,
                            "title": "Exploring Multimodal Large Language Models for Radiology Report Error-checking",
                            "authors": [
                                {
                                    "authorId": "2182815661",
                                    "name": "Jinge Wu"
                                },
                                {
                                    "authorId": "2275715280",
                                    "name": "Yunsoo Kim"
                                },
                                {
                                    "authorId": "2275354702",
                                    "name": "Eva C. Keller"
                                },
                                {
                                    "authorId": "2275129334",
                                    "name": "Jamie Chow"
                                },
                                {
                                    "authorId": "2275239607",
                                    "name": "Adam P. Levine"
                                },
                                {
                                    "authorId": "48573031",
                                    "name": "N. Pontikos"
                                },
                                {
                                    "authorId": "2275359351",
                                    "name": "Zina Ibrahim"
                                },
                                {
                                    "authorId": "2257359718",
                                    "name": "Paul Taylor"
                                },
                                {
                                    "authorId": "2276324181",
                                    "name": "Michelle C. Williams"
                                },
                                {
                                    "authorId": "2241781737",
                                    "name": "Honghan Wu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.56103515625
                    },
                    {
                        "id": "(Dai et al., 2024)",
                        "snippets": [
                            "Liu at el. introduced an end-to-end trained large vision-language assistant (LLaVA [51]) on instruction-following data for general purpose visual and language understanding, which gained widespread attention upon release. Subsequent research has further enhanced LLaVA's performance. For instance, LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios. LLaVA-Interactive [53] serves as a comprehensive demonstration platform, incorporating features such as image chatting, segmentation, and generation and editing capabilities, significantly expanding LLaVA's original functionalities. MoE-LLaVA [54], a sparse LVLM architecture based on Mixture of Experts (MoE), was developed to tackle performance degradation in multimodal sparse learning. MG-LLaVA [55] enhanced the model's visual processing capabilities by introducing multi-granularity visual streams, allowing it to handle features at various resolutions and object centers."
                        ],
                        "paper": {
                            "corpus_id": 273821149,
                            "title": "HumanVLM: Foundation for Human-Scene Vision-Language Model",
                            "authors": [
                                {
                                    "authorId": "2082462168",
                                    "name": "Dawei Dai"
                                },
                                {
                                    "authorId": "2329189750",
                                    "name": "Xu Long"
                                },
                                {
                                    "authorId": "2136494548",
                                    "name": "Yutang Li"
                                },
                                {
                                    "authorId": "2310835404",
                                    "name": "Yuanhui Zhang"
                                },
                                {
                                    "authorId": "2147222435",
                                    "name": "Shuy Xia"
                                }
                            ],
                            "year": 2024,
                            "venue": "Information Fusion",
                            "n_citations": 2
                        },
                        "score": 0.8046875
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.144237
    }
}