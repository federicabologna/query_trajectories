{
    "query": "classifier free guidance beyond conditional diffusion models in image generation NLP tasks",
    "user_id": "lib_user",
    "task_id": "c86c3c27-f2e3-4733-a242-8717c43b971a",
    "timestamp": "2025-06-24T00:14:45.153790",
    "n_retrieval": 256,
    "n_retrieved": 263,
    "n_candidates": 47,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.57288,
    "decomposed_query": {
        "rewritten_query": "Classifier free guidance beyond conditional diffusion models in image generation and NLP tasks.",
        "keyword_query": "classifier free guidance conditional diffusion models image generation NLP",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009603,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Text-to-image Diffusion Models in Generative AI: A Survey",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 151,
            "citation_count": 280,
            "influential_citation_count": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2303.07909",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.07909, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "48934876",
                    "name": "Chenshuang Zhang"
                },
                {
                    "authorId": "31044159",
                    "name": "Chaoning Zhang"
                },
                {
                    "authorId": "50495602",
                    "name": "Mengchun Zhang"
                },
                {
                    "authorId": "145017151",
                    "name": "In-So Kweon"
                }
            ],
            "abstract": "This survey reviews the progress of diffusion models in generating images from text, ~\\textit{i.e.} text-to-image diffusion models. As a self-contained work, this survey starts with a brief introduction of how diffusion models work for image synthesis, followed by the background for text-conditioned image synthesis. Based on that, we present an organized review of pioneering methods and their improvements on text-to-image generation. We further summarize applications beyond image generation, such as text-guided generation for various modalities like videos, and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.",
            "corpus_id": 257505012,
            "sentences": [
                {
                    "corpus_id": "257505012",
                    "title": "Text-to-image Diffusion Models in Generative AI: A Survey",
                    "text": "Labels improve image synthesis. Early works on generative adversarial models (GAN) have shown that class labels improve the image synthesis quality [35], [36], [37], [38], [39]. As a pioneering work, Conditional GAN [35] feeds the class label as an additional input layer to the model. Moreover, [40] applies class-conditional normalization statistics in image generation. In addition, AC-GAN [38] explicitly adds an auxiliary classifier loss. In other words, labels can help improve the GAN image synthesis quality by providing a conditional input or guiding the image synthesis via an auxiliary classifier. Following these success practices, [41] introduces class-conditional normalization and an auxiliary classifier into diffusion models. In order to distinguish whether the label information is added as a conditional input or an auxiliary loss with gradients, we follow [41] to define the conditional diffusion model and guided diffusion model as follows. \n\nConditional diffusion model: A conditional diffusion model learns from additional information (e.g., class and text) by taking them as model input. \n\nGuided diffusion model: During the training of a guided diffusion model, the class-induced gradients (e.g. through an auxiliary classfier) are involved in the sampling process. \n\nClassifier-free guidance. Different from classifierguided diffusion model [41] that exploits an additional classfier, it is found in [42] that the guidance can be obtained by the generative model itself without a classifier, termed as classifier-free guidance. Specifically, classifier-free guidance jointly trains a single model with the unconditional score estimator \u03b8 (x) and the conditional \u03b8 (x, c), where c denotes the class label. A null token \u2205 is placed as the class label in the unconditional part, i.e., \u03b8 (x) = \u03b8 (x, \u2205). Experimental results in [42] show that classifier-free guidance achieves a trade-off between quality and diversity similar to that achieved by classifier guidance. Without resorting to a classifier, classifier-free diffusion facilitates more modalities, e.g., text in text-to-image, as guidance.",
                    "score": 0.7167706557785839,
                    "section_title": "Guidance in diffusion-based image synthesis",
                    "char_start_offset": 9118,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 31
                        },
                        {
                            "start": 32,
                            "end": 177
                        },
                        {
                            "start": 178,
                            "end": 285
                        },
                        {
                            "start": 286,
                            "end": 372
                        },
                        {
                            "start": 373,
                            "end": 443
                        },
                        {
                            "start": 444,
                            "end": 608
                        },
                        {
                            "start": 609,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 961
                        },
                        {
                            "start": 964,
                            "end": 1111
                        },
                        {
                            "start": 1114,
                            "end": 1290
                        },
                        {
                            "start": 1293,
                            "end": 1318
                        },
                        {
                            "start": 1319,
                            "end": 1553
                        },
                        {
                            "start": 1554,
                            "end": 1730
                        },
                        {
                            "start": 1731,
                            "end": 1825
                        },
                        {
                            "start": 1826,
                            "end": 1989
                        },
                        {
                            "start": 1990,
                            "end": 2121
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 154,
                            "end": 158,
                            "matchedPaperCorpusId": "1099052"
                        },
                        {
                            "start": 160,
                            "end": 164,
                            "matchedPaperCorpusId": "2023211"
                        },
                        {
                            "start": 172,
                            "end": 176,
                            "matchedPaperCorpusId": "52889459"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98046875
                }
            ],
            "relevance_judgement": 0.98046875,
            "relevance_judgment_input_expanded": "# Title: Text-to-image Diffusion Models in Generative AI: A Survey\n# Venue: arXiv.org\n# Authors: Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, In-So Kweon\n## Abstract\nThis survey reviews the progress of diffusion models in generating images from text, ~\\textit{i.e.} text-to-image diffusion models. As a self-contained work, this survey starts with a brief introduction of how diffusion models work for image synthesis, followed by the background for text-conditioned image synthesis. Based on that, we present an organized review of pioneering methods and their improvements on text-to-image generation. We further summarize applications beyond image generation, such as text-guided generation for various modalities like videos, and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.\n## Guidance in diffusion-based image synthesis\nLabels improve image synthesis. Early works on generative adversarial models (GAN) have shown that class labels improve the image synthesis quality [35], [36], [37], [38], [39]. As a pioneering work, Conditional GAN [35] feeds the class label as an additional input layer to the model. Moreover, [40] applies class-conditional normalization statistics in image generation. In addition, AC-GAN [38] explicitly adds an auxiliary classifier loss. In other words, labels can help improve the GAN image synthesis quality by providing a conditional input or guiding the image synthesis via an auxiliary classifier. Following these success practices, [41] introduces class-conditional normalization and an auxiliary classifier into diffusion models. In order to distinguish whether the label information is added as a conditional input or an auxiliary loss with gradients, we follow [41] to define the conditional diffusion model and guided diffusion model as follows. \n\nConditional diffusion model: A conditional diffusion model learns from additional information (e.g., class and text) by taking them as model input. \n\nGuided diffusion model: During the training of a guided diffusion model, the class-induced gradients (e.g. through an auxiliary classfier) are involved in the sampling process. \n\nClassifier-free guidance. Different from classifierguided diffusion model [41] that exploits an additional classfier, it is found in [42] that the guidance can be obtained by the generative model itself without a classifier, termed as classifier-free guidance. Specifically, classifier-free guidance jointly trains a single model with the unconditional score estimator \u03b8 (x) and the conditional \u03b8 (x, c), where c denotes the class label. A null token \u2205 is placed as the class label in the unconditional part, i.e., \u03b8 (x) = \u03b8 (x, \u2205). Experimental results in [42] show that classifier-free guidance achieves a trade-off between quality and diversity similar to that achieved by classifier guidance. Without resorting to a classifier, classifier-free diffusion facilitates more modalities, e.g., text in text-to-image, as guidance.",
            "reference_string": "[257505012 | Zhang et al. | 2023 | Citations: 280]"
        },
        {
            "title": "Segmentation-Free Guidance for Text-to-Image Diffusion Models",
            "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "year": 2024,
            "reference_count": 29,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2407.04800",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.04800, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2075053",
                    "name": "K. Azarian"
                },
                {
                    "authorId": "49950690",
                    "name": "Debasmit Das"
                },
                {
                    "authorId": "2293594635",
                    "name": "Qiqi Hou"
                },
                {
                    "authorId": "2253777162",
                    "name": "F. Porikli"
                }
            ],
            "abstract": "We introduce segmentation-free guidance, a novel method designed for text-to-image diffusion models like Stable Diffusion. Our method does not require retraining of the diffusion model. At no additional compute cost, it uses the diffusion model itself as an implied segmentation network, hence named segmentation-free guidance, to dynamically adjust the negative prompt for each patch of the generated image, based on the patch\u2019s relevance to concepts in the prompt. We evaluate segmentation-free guidance both objectively, using FID, CLIP, IS, and PickScore, and subjectively, through human evaluators. For the subjective evaluation, we also propose a methodology for subsampling the prompts in a dataset like MS COCO-30K to keep the number of human evaluations manageable while ensuring that the selected subset is both representative in terms of content and fair in terms of model performance. The results demonstrate the superiority of our segmentation-free guidance to the widely used classifier-free method. Human evaluators preferred segmentation-free guidance over classifier-free 60% to 19%, with 18% of occasions showing a strong preference. Additionally, PickScore win-rate, a recently proposed metric mimicking human preference, also indicates a preference for our method over classifier-free.",
            "corpus_id": 271051241,
            "sentences": [
                {
                    "corpus_id": "271051241",
                    "title": "Segmentation-Free Guidance for Text-to-Image Diffusion Models",
                    "text": "Diffusion models are powerful generative models for creating visual content from textual prompts.Their success stems from extensive training data and their ability to handle various modalities and signals, enabling diverse applications such as content editing, inpainting, and personalization.\n\nControlling a diffusion model can be achieved primarily in two ways -conditioning and guidance.When a diffusion model is conditioned, it is typically trained to accept a particular form of additional conditioning input, such as a text prompt, image edges, segmentation map, and class labels.However, adapting the model to a different condition often necessitates retraining from scratch.This reliance on expensive retraining poses challenges for end-users seeking to adopt and employ conditioning techniques to control diffusion models.\n\nAn alternative way to control a diffusion model is through a guidance mechanism.Unlike conditioning techniques, this approach does not rely on an external conditioning signal.Instead, it associates a guidance function with the diffusion model to fulfill a specific target criterion, which could be as simple as minimizing the CLIP distance between the generated image and the provided text description.When sampling an image, the reverse process iterations are steered in the direction of the guidance function's gradient, resulting in constrained image generation.\n\nWhen comparing control techniques for diffusion models, guidance emerges as a more versatile approach.It treats the diffusion network as a foundational model which can accommodate different use cases.An earlier method in this domain involved classifier guidance [8], where an explicit classifier functioned as the guidance mechanism.This method utilized the classifier's gradients to drive the image generation process.However, classifier guidance has transitioned to classifier-free guidance [12], eliminating the need for an explicit classifier.In classifier-free guidance approaches, the network is trained to adapt class-label information and conditioning signals without relying on a fixed network architecture.\n\nIn this paper, we propose enhancing image generation quality beyond classifier-free guidance by introducing a novel and universal segmentation-free guidance approach.This methodology aims to improve image quality of diffusion models without necessitating costly retraining, architectural changes, or additional computing during inference.\n\nImage generation using classifier-free guidance involves two forward passes of the diffusion network per iteration: one that uses conditional information and one that does not.",
                    "score": 0.6843670956688026,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 97
                        },
                        {
                            "start": 97,
                            "end": 293
                        },
                        {
                            "start": 295,
                            "end": 390
                        },
                        {
                            "start": 390,
                            "end": 586
                        },
                        {
                            "start": 586,
                            "end": 682
                        },
                        {
                            "start": 682,
                            "end": 831
                        },
                        {
                            "start": 833,
                            "end": 913
                        },
                        {
                            "start": 913,
                            "end": 1008
                        },
                        {
                            "start": 1008,
                            "end": 1235
                        },
                        {
                            "start": 1235,
                            "end": 1398
                        },
                        {
                            "start": 1400,
                            "end": 1502
                        },
                        {
                            "start": 1502,
                            "end": 1600
                        },
                        {
                            "start": 1600,
                            "end": 1733
                        },
                        {
                            "start": 1733,
                            "end": 1819
                        },
                        {
                            "start": 1819,
                            "end": 1947
                        },
                        {
                            "start": 1947,
                            "end": 2116
                        },
                        {
                            "start": 2118,
                            "end": 2284
                        },
                        {
                            "start": 2284,
                            "end": 2456
                        },
                        {
                            "start": 2458,
                            "end": 2634
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1893,
                            "end": 1897,
                            "matchedPaperCorpusId": "249145348"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97900390625
                },
                {
                    "corpus_id": "271051241",
                    "title": "Segmentation-Free Guidance for Text-to-Image Diffusion Models",
                    "text": "Image generation using classifier-free guidance involves two forward passes of the diffusion network per iteration: one that uses conditional information and one that does not.The conditional information generally involves a (positive) text prompt describing different objects of interest in the generated image.For instance, in Fig. 1-a, the positive prompt is \"a cute Maltese white dog next to a cat.\"The forward pass without conditional information is usually carried out by an empty (negative) prompt (i.e., \"\").However, it is possible to employ non-empty negative prompts.This type of guidance allows the objects present in the positive, but not the negative, prompt to become more prominent.Nonetheless, the issue with having such negative prompts is that it interacts with the generated image globally.\n\nOur objective is to dynamically adjust the negative prompt for each image patch.We examine attention maps within the diffusion model, specifically where it interacts with the text prompt embeddings.For each patch of the attention map, we aim to find the object in the positive prompt with the highest correlation.Subsequently, this selected object is excluded from the negative prompt interacting with that specific patch.Accordingly, the forward pass of the diffusion model carries out as if each patch cross-attends dynamically with a different negative prompt.Furthermore, the corresponding attention weight is adjusted to account for self-attention interactions.Since this proposed method of guidance does not involve any segmentation network as a guidance function, we term this method as segmentationfree guidance.Our method realizes local interaction between prompt embedding and feature patches while dynamically adjusting the negative prompts; thus, it produces better image generation quality, as shown in Fig. 1.\n\nOur contributions can be summarized as follows: \u2022 We introduce a novel mechanism named segmentationfree guidance that effectively adjusts the negative prompt for each patch of the generated image based on the category of the patch.\u2022 We also propose an efficient subjective evaluation methodology that involves sub-sampling of prompts dataset for assessment.The chosen subset of prompts ensures the representation of dataset diversity while maintaining fairness in terms of model performance.\u2022 Finally, we perform extensive evaluation on the MS-COCO datasets on which we show both qualitative and quantitative improvement.",
                    "score": 0.6287030751998899,
                    "section_title": "Introduction",
                    "char_start_offset": 2473,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 176,
                            "end": 312
                        },
                        {
                            "start": 312,
                            "end": 403
                        },
                        {
                            "start": 403,
                            "end": 516
                        },
                        {
                            "start": 516,
                            "end": 577
                        },
                        {
                            "start": 577,
                            "end": 697
                        },
                        {
                            "start": 697,
                            "end": 809
                        },
                        {
                            "start": 811,
                            "end": 891
                        },
                        {
                            "start": 891,
                            "end": 1009
                        },
                        {
                            "start": 1009,
                            "end": 1124
                        },
                        {
                            "start": 1124,
                            "end": 1233
                        },
                        {
                            "start": 1233,
                            "end": 1374
                        },
                        {
                            "start": 1374,
                            "end": 1477
                        },
                        {
                            "start": 1477,
                            "end": 1631
                        },
                        {
                            "start": 1631,
                            "end": 1834
                        },
                        {
                            "start": 1836,
                            "end": 2067
                        },
                        {
                            "start": 2067,
                            "end": 2193
                        },
                        {
                            "start": 2193,
                            "end": 2327
                        },
                        {
                            "start": 2327,
                            "end": 2457
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9482421875
                },
                {
                    "corpus_id": "271051241",
                    "title": "Segmentation-Free Guidance for Text-to-Image Diffusion Models",
                    "text": "Our proposed work falls into the scope of controlled image generation using diffusion models.Controlled image generation can be broadly classified into conditional generation and guided generation.These are discussed as follows.\n\nConditional Generation These category of works generally require training diffusion models from scratch where conditional input can be of the form of prompts [2,12,19,27,29].One of the most popular works [12] proposed use of classifier-free guidance with class labels as prompts.In this work, the diffusion model is trained such that the output is a linear combination between that of conditional and unconditional outputs.The authors of [2] trained a diffusion model, where it is enforced to solve linear inverse problems.This is realized through a guidance function known as linear degradation operator.[19] used classifier-free guidance but extended it to descriptive phrases as prompts.Furthermore, the network was trained to enforce similarity between CLIP [21] representations of images and text.However, the major disadvantage of conditional generation methods is that the diffusion models need to be retrained and hence it is computationally intensive.\n\nGuided Generation In this category, the diffusion model is kept frozen without any re-training.However, the sampling process for image generation is modified using gradients from a guidance function.There are prior works that studied guided image generation using various constraints and guidance functions [6-9, 14, 18, 28].The most popular method in this category is classifier guidance [8].\n\nIn this method, a classifier is trained to distinguish images of different scales.The classifier is used as a guidance function, the gradients of which are used in the sampling process.Alternative methods include [28], where the guidance function is a linear operator.Since gradients of the linear operator are used, components of the images were generated in the null space of the linear operator.However, the use of null space does not naturally extend to non-linear guidance functions.In [6], the authors did an elaborate analyses of multiple simple non-linear guidance functions, e.g.non-linear blurring.The gradient of the non-linear function was calculated on expected denoised images and the sampling process was modified.",
                    "score": 0.7210439772878736,
                    "section_title": "Related Work",
                    "char_start_offset": 4946,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 93
                        },
                        {
                            "start": 93,
                            "end": 197
                        },
                        {
                            "start": 197,
                            "end": 228
                        },
                        {
                            "start": 230,
                            "end": 404
                        },
                        {
                            "start": 404,
                            "end": 509
                        },
                        {
                            "start": 509,
                            "end": 653
                        },
                        {
                            "start": 653,
                            "end": 753
                        },
                        {
                            "start": 753,
                            "end": 835
                        },
                        {
                            "start": 835,
                            "end": 920
                        },
                        {
                            "start": 920,
                            "end": 1032
                        },
                        {
                            "start": 1032,
                            "end": 1190
                        },
                        {
                            "start": 1192,
                            "end": 1287
                        },
                        {
                            "start": 1287,
                            "end": 1391
                        },
                        {
                            "start": 1391,
                            "end": 1517
                        },
                        {
                            "start": 1517,
                            "end": 1585
                        },
                        {
                            "start": 1587,
                            "end": 1669
                        },
                        {
                            "start": 1669,
                            "end": 1772
                        },
                        {
                            "start": 1772,
                            "end": 1855
                        },
                        {
                            "start": 1855,
                            "end": 1985
                        },
                        {
                            "start": 1985,
                            "end": 2075
                        },
                        {
                            "start": 2075,
                            "end": 2175
                        },
                        {
                            "start": 2175,
                            "end": 2195
                        },
                        {
                            "start": 2195,
                            "end": 2316
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 391,
                            "end": 394,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 394,
                            "end": 397,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 400,
                            "end": 403,
                            "matchedPaperCorpusId": "244908890"
                        },
                        {
                            "start": 434,
                            "end": 438,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 835,
                            "end": 839,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 992,
                            "end": 996,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 1800,
                            "end": 1804,
                            "matchedPaperCorpusId": "254125609"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9208984375
                }
            ],
            "relevance_judgement": 0.97900390625,
            "relevance_judgment_input_expanded": "# Title: Segmentation-Free Guidance for Text-to-Image Diffusion Models\n# Venue: 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\n# Authors: K. Azarian, Debasmit Das, Qiqi Hou, F. Porikli\n## Abstract\nWe introduce segmentation-free guidance, a novel method designed for text-to-image diffusion models like Stable Diffusion. Our method does not require retraining of the diffusion model. At no additional compute cost, it uses the diffusion model itself as an implied segmentation network, hence named segmentation-free guidance, to dynamically adjust the negative prompt for each patch of the generated image, based on the patch\u2019s relevance to concepts in the prompt. We evaluate segmentation-free guidance both objectively, using FID, CLIP, IS, and PickScore, and subjectively, through human evaluators. For the subjective evaluation, we also propose a methodology for subsampling the prompts in a dataset like MS COCO-30K to keep the number of human evaluations manageable while ensuring that the selected subset is both representative in terms of content and fair in terms of model performance. The results demonstrate the superiority of our segmentation-free guidance to the widely used classifier-free method. Human evaluators preferred segmentation-free guidance over classifier-free 60% to 19%, with 18% of occasions showing a strong preference. Additionally, PickScore win-rate, a recently proposed metric mimicking human preference, also indicates a preference for our method over classifier-free.\n## Introduction\nDiffusion models are powerful generative models for creating visual content from textual prompts.Their success stems from extensive training data and their ability to handle various modalities and signals, enabling diverse applications such as content editing, inpainting, and personalization.\n\nControlling a diffusion model can be achieved primarily in two ways -conditioning and guidance.When a diffusion model is conditioned, it is typically trained to accept a particular form of additional conditioning input, such as a text prompt, image edges, segmentation map, and class labels.However, adapting the model to a different condition often necessitates retraining from scratch.This reliance on expensive retraining poses challenges for end-users seeking to adopt and employ conditioning techniques to control diffusion models.\n\nAn alternative way to control a diffusion model is through a guidance mechanism.Unlike conditioning techniques, this approach does not rely on an external conditioning signal.Instead, it associates a guidance function with the diffusion model to fulfill a specific target criterion, which could be as simple as minimizing the CLIP distance between the generated image and the provided text description.When sampling an image, the reverse process iterations are steered in the direction of the guidance function's gradient, resulting in constrained image generation.\n\nWhen comparing control techniques for diffusion models, guidance emerges as a more versatile approach.It treats the diffusion network as a foundational model which can accommodate different use cases.An earlier method in this domain involved classifier guidance [8], where an explicit classifier functioned as the guidance mechanism.This method utilized the classifier's gradients to drive the image generation process.However, classifier guidance has transitioned to classifier-free guidance [12], eliminating the need for an explicit classifier.In classifier-free guidance approaches, the network is trained to adapt class-label information and conditioning signals without relying on a fixed network architecture.\n\nIn this paper, we propose enhancing image generation quality beyond classifier-free guidance by introducing a novel and universal segmentation-free guidance approach.This methodology aims to improve image quality of diffusion models without necessitating costly retraining, architectural changes, or additional computing during inference.\n\nImage generation using classifier-free guidance involves two forward passes of the diffusion network per iteration: one that uses conditional information and one that does not.\n...\nImage generation using classifier-free guidance involves two forward passes of the diffusion network per iteration: one that uses conditional information and one that does not.The conditional information generally involves a (positive) text prompt describing different objects of interest in the generated image.For instance, in Fig. 1-a, the positive prompt is \"a cute Maltese white dog next to a cat.\"The forward pass without conditional information is usually carried out by an empty (negative) prompt (i.e., \"\").However, it is possible to employ non-empty negative prompts.This type of guidance allows the objects present in the positive, but not the negative, prompt to become more prominent.Nonetheless, the issue with having such negative prompts is that it interacts with the generated image globally.\n\nOur objective is to dynamically adjust the negative prompt for each image patch.We examine attention maps within the diffusion model, specifically where it interacts with the text prompt embeddings.For each patch of the attention map, we aim to find the object in the positive prompt with the highest correlation.Subsequently, this selected object is excluded from the negative prompt interacting with that specific patch.Accordingly, the forward pass of the diffusion model carries out as if each patch cross-attends dynamically with a different negative prompt.Furthermore, the corresponding attention weight is adjusted to account for self-attention interactions.Since this proposed method of guidance does not involve any segmentation network as a guidance function, we term this method as segmentationfree guidance.Our method realizes local interaction between prompt embedding and feature patches while dynamically adjusting the negative prompts; thus, it produces better image generation quality, as shown in Fig. 1.\n\nOur contributions can be summarized as follows: \u2022 We introduce a novel mechanism named segmentationfree guidance that effectively adjusts the negative prompt for each patch of the generated image based on the category of the patch.\u2022 We also propose an efficient subjective evaluation methodology that involves sub-sampling of prompts dataset for assessment.The chosen subset of prompts ensures the representation of dataset diversity while maintaining fairness in terms of model performance.\u2022 Finally, we perform extensive evaluation on the MS-COCO datasets on which we show both qualitative and quantitative improvement.\n\n## Related Work\nOur proposed work falls into the scope of controlled image generation using diffusion models.Controlled image generation can be broadly classified into conditional generation and guided generation.These are discussed as follows.\n\nConditional Generation These category of works generally require training diffusion models from scratch where conditional input can be of the form of prompts [2,12,19,27,29].One of the most popular works [12] proposed use of classifier-free guidance with class labels as prompts.In this work, the diffusion model is trained such that the output is a linear combination between that of conditional and unconditional outputs.The authors of [2] trained a diffusion model, where it is enforced to solve linear inverse problems.This is realized through a guidance function known as linear degradation operator.[19] used classifier-free guidance but extended it to descriptive phrases as prompts.Furthermore, the network was trained to enforce similarity between CLIP [21] representations of images and text.However, the major disadvantage of conditional generation methods is that the diffusion models need to be retrained and hence it is computationally intensive.\n\nGuided Generation In this category, the diffusion model is kept frozen without any re-training.However, the sampling process for image generation is modified using gradients from a guidance function.There are prior works that studied guided image generation using various constraints and guidance functions [6-9, 14, 18, 28].The most popular method in this category is classifier guidance [8].\n\nIn this method, a classifier is trained to distinguish images of different scales.The classifier is used as a guidance function, the gradients of which are used in the sampling process.Alternative methods include [28], where the guidance function is a linear operator.Since gradients of the linear operator are used, components of the images were generated in the null space of the linear operator.However, the use of null space does not naturally extend to non-linear guidance functions.In [6], the authors did an elaborate analyses of multiple simple non-linear guidance functions, e.g.non-linear blurring.The gradient of the non-linear function was calculated on expected denoised images and the sampling process was modified.",
            "reference_string": "[271051241 | Azarian et al. | 2024 | Citations: 0]"
        },
        {
            "title": "TCFG: Tangential Damping Classifier-free Guidance",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 33,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.18137, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2182293854",
                    "name": "Mingi Kwon"
                },
                {
                    "authorId": "2352073121",
                    "name": "Shin seong Kim"
                },
                {
                    "authorId": "2351806103",
                    "name": "Jaeseok Jeong. Yi Ting Hsiao"
                },
                {
                    "authorId": "2253393666",
                    "name": "Youngjung Uh"
                }
            ],
            "abstract": "Diffusion models have achieved remarkable success in text-to-image synthesis, largely attributed to the use of classifier-free guidance (CFG), which enables high-quality, condition-aligned image generation. CFG combines the conditional score (e.g., text-conditioned) with the unconditional score to control the output. However, the unconditional score is in charge of estimating the transition between manifolds of adjacent timesteps from $x_t$ to $x_{t-1}$, which may inadvertently interfere with the trajectory toward the specific condition. In this work, we introduce a novel approach that leverages a geometric perspective on the unconditional score to enhance CFG performance when conditional scores are available. Specifically, we propose a method that filters the singular vectors of both conditional and unconditional scores using singular value decomposition. This filtering process aligns the unconditional score with the conditional score, thereby refining the sampling trajectory to stay closer to the manifold. Our approach improves image quality with negligible additional computation. We provide deeper insights into the score function behavior in diffusion models and present a practical technique for achieving more accurate and contextually coherent image synthesis.",
            "corpus_id": 277271753,
            "sentences": [
                {
                    "corpus_id": "277271753",
                    "title": "TCFG: Tangential Damping Classifier-free Guidance",
                    "text": "Diffusion models have achieved remarkable success in text-to-image synthesis, largely attributed to the use of classifier-free guidance (CFG), which enables high-quality, condition-aligned image generation. CFG combines the conditional score (e.g., text-conditioned) with the unconditional score to control the output. However, the unconditional score is in charge of estimating the transition between manifolds of adjacent timesteps from $x_t$ to $x_{t-1}$, which may inadvertently interfere with the trajectory toward the specific condition. In this work, we introduce a novel approach that leverages a geometric perspective on the unconditional score to enhance CFG performance when conditional scores are available. Specifically, we propose a method that filters the singular vectors of both conditional and unconditional scores using singular value decomposition. This filtering process aligns the unconditional score with the conditional score, thereby refining the sampling trajectory to stay closer to the manifold. Our approach improves image quality with negligible additional computation. We provide deeper insights into the score function behavior in diffusion models and present a practical technique for achieving more accurate and contextually coherent image synthesis.",
                    "score": 0.5599970775115582,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9677734375
                },
                {
                    "corpus_id": "277271753",
                    "title": "TCFG: Tangential Damping Classifier-free Guidance",
                    "text": "Diffusion models [12,31] have shown remarkable progress in image generation [19,27,30]. In particular, the emergence of classifier-free guidance [6,11] (CFG) has attracted significant attention because it allows us to provide desired guidance by leveraging the conditional estimated score directly within the diffusion model. \n\nThe classifier-free guidance fundamentally computes the * Equal contribution manifold by the given condition. Fig. 1 (a) conceptually illustrates the potential issue that arises when the manifold of the unconditional score differs from that of the conditional score. In this paper, we show that this misalignment can be resolved with a simple algorithm, which significantly reduces the tendency of CFG to generate off-manifold samples, as illustrated in Fig. 1 (b). \n\nOur approach is based on the following insights. First, the score predicted by the diffusion model estimates the intrinsic dimension of the data manifold [32]. Additionally, this intrinsic dimension can be captured by the tangent space of the target manifold [3,9]. Instead of directly estimating the intrinsic dimension, we focus on utilizing the tangential component inherent in the unconditional score during classifier-free guidance. By reducing its misalignment with the conditional score, we enhance the alignment and ultimately improve the quality of the generated outputs. \n\nSpecifically, we push the score s\u03b8 toward the normal direction of the conditional manifold by eliminating the values of column vectors with small singular values using the orthogonal matrix V obtained through the singular value decomposition of the conditional and unconditional scores. \n\nIn this paper, we propose a novel sampling method that leverages the unconditional score within CFG. To support our approach, we first lay out the theoretical foundation in section Sec. 2 and Sec. 3, discussing the manifold hypothesis and its connection to diffusion models. In Sec. 4, we provide a comprehensive explanation of our proposed method. This is followed by a detailed analysis using a toy example in Sec. 5, and we demonstrate the practical applicability of our method on real-world text-to-image models in Sec. 6.",
                    "score": 0.6473808358632955,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 87
                        },
                        {
                            "start": 88,
                            "end": 325
                        },
                        {
                            "start": 328,
                            "end": 437
                        },
                        {
                            "start": 438,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 793
                        },
                        {
                            "start": 796,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 955
                        },
                        {
                            "start": 956,
                            "end": 1061
                        },
                        {
                            "start": 1062,
                            "end": 1233
                        },
                        {
                            "start": 1234,
                            "end": 1376
                        },
                        {
                            "start": 1379,
                            "end": 1665
                        },
                        {
                            "start": 1668,
                            "end": 1768
                        },
                        {
                            "start": 1769,
                            "end": 1942
                        },
                        {
                            "start": 1943,
                            "end": 2016
                        },
                        {
                            "start": 2017,
                            "end": 2194
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 17,
                            "end": 21,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 80,
                            "end": 83,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 145,
                            "end": 148,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 1055,
                            "end": 1058,
                            "matchedPaperCorpusId": "393948"
                        },
                        {
                            "start": 1058,
                            "end": 1060,
                            "matchedPaperCorpusId": "50258911"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93408203125
                }
            ],
            "relevance_judgement": 0.9677734375,
            "relevance_judgment_input_expanded": "# Title: TCFG: Tangential Damping Classifier-free Guidance\n# Venue: arXiv.org\n# Authors: Mingi Kwon, Shin seong Kim, Jaeseok Jeong. Yi Ting Hsiao, Youngjung Uh\n## Abstract\nDiffusion models have achieved remarkable success in text-to-image synthesis, largely attributed to the use of classifier-free guidance (CFG), which enables high-quality, condition-aligned image generation. CFG combines the conditional score (e.g., text-conditioned) with the unconditional score to control the output. However, the unconditional score is in charge of estimating the transition between manifolds of adjacent timesteps from $x_t$ to $x_{t-1}$, which may inadvertently interfere with the trajectory toward the specific condition. In this work, we introduce a novel approach that leverages a geometric perspective on the unconditional score to enhance CFG performance when conditional scores are available. Specifically, we propose a method that filters the singular vectors of both conditional and unconditional scores using singular value decomposition. This filtering process aligns the unconditional score with the conditional score, thereby refining the sampling trajectory to stay closer to the manifold. Our approach improves image quality with negligible additional computation. We provide deeper insights into the score function behavior in diffusion models and present a practical technique for achieving more accurate and contextually coherent image synthesis.\n## Introduction\nDiffusion models [12,31] have shown remarkable progress in image generation [19,27,30]. In particular, the emergence of classifier-free guidance [6,11] (CFG) has attracted significant attention because it allows us to provide desired guidance by leveraging the conditional estimated score directly within the diffusion model. \n\nThe classifier-free guidance fundamentally computes the * Equal contribution manifold by the given condition. Fig. 1 (a) conceptually illustrates the potential issue that arises when the manifold of the unconditional score differs from that of the conditional score. In this paper, we show that this misalignment can be resolved with a simple algorithm, which significantly reduces the tendency of CFG to generate off-manifold samples, as illustrated in Fig. 1 (b). \n\nOur approach is based on the following insights. First, the score predicted by the diffusion model estimates the intrinsic dimension of the data manifold [32]. Additionally, this intrinsic dimension can be captured by the tangent space of the target manifold [3,9]. Instead of directly estimating the intrinsic dimension, we focus on utilizing the tangential component inherent in the unconditional score during classifier-free guidance. By reducing its misalignment with the conditional score, we enhance the alignment and ultimately improve the quality of the generated outputs. \n\nSpecifically, we push the score s\u03b8 toward the normal direction of the conditional manifold by eliminating the values of column vectors with small singular values using the orthogonal matrix V obtained through the singular value decomposition of the conditional and unconditional scores. \n\nIn this paper, we propose a novel sampling method that leverages the unconditional score within CFG. To support our approach, we first lay out the theoretical foundation in section Sec. 2 and Sec. 3, discussing the manifold hypothesis and its connection to diffusion models. In Sec. 4, we provide a comprehensive explanation of our proposed method. This is followed by a detailed analysis using a toy example in Sec. 5, and we demonstrate the practical applicability of our method on real-world text-to-image models in Sec. 6.",
            "reference_string": "[277271753 | Kwon et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Guided Flows for Generative Modeling and Decision Making",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 59,
            "citation_count": 46,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.13443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2166847",
                    "name": "Qinqing Zheng"
                },
                {
                    "authorId": "2267723599",
                    "name": "Matt Le"
                },
                {
                    "authorId": "2219927868",
                    "name": "Neta Shaul"
                },
                {
                    "authorId": "3232072",
                    "name": "Y. Lipman"
                },
                {
                    "authorId": "2267723293",
                    "name": "Aditya Grover"
                },
                {
                    "authorId": "2253976277",
                    "name": "Ricky T. Q. Chen"
                }
            ],
            "abstract": "Classifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields. We explore the usage of \\emph{Guided Flows} for a variety of downstream applications. We show that Guided Flows significantly improves the sample quality in conditional image generation and zero-shot text-to-speech synthesis, boasting state-of-the-art performance. Notably, we are the first to apply flow models for plan generation in the offline reinforcement learning setting, showcasing a 10x speedup in computation compared to diffusion models while maintaining comparable performance.",
            "corpus_id": 265351587,
            "sentences": [
                {
                    "corpus_id": "265351587",
                    "title": "Guided Flows for Generative Modeling and Decision Making",
                    "text": "Classifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields. We explore the usage of \\emph{Guided Flows} for a variety of downstream applications. We show that Guided Flows significantly improves the sample quality in conditional image generation and zero-shot text-to-speech synthesis, boasting state-of-the-art performance. Notably, we are the first to apply flow models for plan generation in the offline reinforcement learning setting, showcasing a 10x speedup in computation compared to diffusion models while maintaining comparable performance.",
                    "score": 0.600349247054058,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96533203125
                }
            ],
            "relevance_judgement": 0.96533203125,
            "relevance_judgment_input_expanded": "# Title: Guided Flows for Generative Modeling and Decision Making\n# Venue: arXiv.org\n# Authors: Qinqing Zheng, Matt Le, Neta Shaul, Y. Lipman, Aditya Grover, Ricky T. Q. Chen\n## Abstract\nClassifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields. We explore the usage of \\emph{Guided Flows} for a variety of downstream applications. We show that Guided Flows significantly improves the sample quality in conditional image generation and zero-shot text-to-speech synthesis, boasting state-of-the-art performance. Notably, we are the first to apply flow models for plan generation in the offline reinforcement learning setting, showcasing a 10x speedup in computation compared to diffusion models while maintaining comparable performance.\n",
            "reference_string": "[265351587 | Zheng et al. | 2023 | Citations: 46]"
        },
        {
            "title": "Classifier-Free Diffusion Guidance",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 25,
            "citation_count": 3970,
            "influential_citation_count": 582,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2207.12598",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.12598, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2126278",
                    "name": "Jonathan Ho"
                }
            ],
            "abstract": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
            "corpus_id": 249145348,
            "sentences": [
                {
                    "corpus_id": "249145348",
                    "title": "Classifier-Free Diffusion Guidance",
                    "text": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
                    "score": 0.699038438696144,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9599609375
                }
            ],
            "relevance_judgement": 0.9599609375,
            "relevance_judgment_input_expanded": "# Title: Classifier-Free Diffusion Guidance\n# Venue: arXiv.org\n# Authors: Jonathan Ho\n## Abstract\nClassifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.\n",
            "reference_string": "[249145348 | Ho | 2022 | Citations: 3970]"
        },
        {
            "title": "FineDiffusion: Scaling up Diffusion Models for Fine-grained Image Generation with 10, 000 Classes",
            "venue": "Applied intelligence (Boston)",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.18331, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2288865816",
                    "name": "Ziying Pan"
                },
                {
                    "authorId": "2288886271",
                    "name": "Kun Wang"
                },
                {
                    "authorId": "2243959855",
                    "name": "Gang Li"
                },
                {
                    "authorId": "2242676809",
                    "name": "Feihong He"
                },
                {
                    "authorId": "2288032594",
                    "name": "Xiwang Li"
                },
                {
                    "authorId": "2289803819",
                    "name": "Yongxuan Lai"
                }
            ],
            "abstract": "The class-conditional image generation based on diffusion models is renowned for generating high-quality and diverse images. However, most prior efforts focus on generating images for general categories, e.g., 1000 classes in ImageNet-1k. A more challenging task, large-scale fine-grained image generation, remains the boundary to explore. In this work, we present a parameter-efficient strategy, called FineDiffusion, to fine-tune large pre-trained diffusion models scaling to large-scale fine-grained image generation with 10,000 categories. FineDiffusion significantly accelerates training and reduces storage overhead by only fine-tuning tiered class embedder, bias terms, and normalization layers' parameters. To further improve the image generation quality of fine-grained categories, we propose a novel sampling method for fine-grained image generation, which utilizes superclass-conditioned guidance, specifically tailored for fine-grained categories, to replace the conventional classifier-free guidance sampling. Compared to full fine-tuning, FineDiffusion achieves a remarkable 1.56x training speed-up and requires storing merely 1.77% of the total model parameters, while achieving state-of-the-art FID of 9.776 on image generation of 10,000 classes. Extensive qualitative and quantitative experiments demonstrate the superiority of our method compared to other parameter-efficient fine-tuning methods. The code and more generated results are available at our project website: https://finediffusion.github.io/.",
            "corpus_id": 268041325,
            "sentences": [
                {
                    "corpus_id": "268041325",
                    "title": "FineDiffusion: Scaling up Diffusion Models for Fine-grained Image Generation with 10, 000 Classes",
                    "text": "Classifier-free guidance has supplanted classifier guided sampling to achieve a trade-off between the diversity and quality of sampling. While reducing training complexity, this approach secures high-quality image generation. Currently, classifier-free guidance is widely utilized in various diffusion models. We attempt to introduce hierarchical category label information because knowing which superclass each category belongs to is helpful for capturing intricate details and enhancing the overall quality of the generated images. Therefore, we introduce a fine-grained classifier-free guidance sampling method, yielding improved results under specific conditions. Integrated with TieredEmbedder, this approach leverages superclass conditional information to enhance control over generated images. Conventional classifierfree guidance involves training both conditional and unconditional generative models. In our approach, a superclass conditional generative model is introduced to replace the unconditional model. During training, we use the same neural network to parameterize both the conditional model and the superclass conditional model, leveraging a score estimator \u03f5 \u03b8 for optimization guidance. Subclass labels c are probabilistically replaced with their corresponding superclass labels c p to achieve the learning of superclass embeddings. During sampling, trained superclass and subclass embeddings are input into the DiT network to obtain predictive noise. Our method combines conditional scores estimation \u03f5 \u03b8 (x t | c) and superclass conditional scores estimation \u03f5 \u03b8 (x t | c p ) to guide specific category image generation, with the specific formula provided below: \n\nwhere \u03c9 is the guidance scale, and \u03b5\u03b8 is the modified score. \n\nBy thoroughly exploiting the distributional characteristics of superclasses and subclasses, our sampling method achieves a finer control than classifier-free guidance. We can capture commonalities among subclasses within the same superclass and simultaneously capture distinct distributions between different superclasses, thereby making the learned distributions of each category more closely approximate real-world situations.",
                    "score": 0.603919316282848,
                    "section_title": "Fine-grained Classifier-free Guidance",
                    "char_start_offset": 13406,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 136
                        },
                        {
                            "start": 137,
                            "end": 225
                        },
                        {
                            "start": 226,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 800
                        },
                        {
                            "start": 801,
                            "end": 909
                        },
                        {
                            "start": 910,
                            "end": 1018
                        },
                        {
                            "start": 1019,
                            "end": 1207
                        },
                        {
                            "start": 1208,
                            "end": 1353
                        },
                        {
                            "start": 1354,
                            "end": 1472
                        },
                        {
                            "start": 1473,
                            "end": 1685
                        },
                        {
                            "start": 1688,
                            "end": 1748
                        },
                        {
                            "start": 1751,
                            "end": 1918
                        },
                        {
                            "start": 1919,
                            "end": 2179
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.958984375
                }
            ],
            "relevance_judgement": 0.958984375,
            "relevance_judgment_input_expanded": "# Title: FineDiffusion: Scaling up Diffusion Models for Fine-grained Image Generation with 10, 000 Classes\n# Venue: Applied intelligence (Boston)\n# Authors: Ziying Pan, Kun Wang, Gang Li, Feihong He, Xiwang Li, Yongxuan Lai\n## Abstract\nThe class-conditional image generation based on diffusion models is renowned for generating high-quality and diverse images. However, most prior efforts focus on generating images for general categories, e.g., 1000 classes in ImageNet-1k. A more challenging task, large-scale fine-grained image generation, remains the boundary to explore. In this work, we present a parameter-efficient strategy, called FineDiffusion, to fine-tune large pre-trained diffusion models scaling to large-scale fine-grained image generation with 10,000 categories. FineDiffusion significantly accelerates training and reduces storage overhead by only fine-tuning tiered class embedder, bias terms, and normalization layers' parameters. To further improve the image generation quality of fine-grained categories, we propose a novel sampling method for fine-grained image generation, which utilizes superclass-conditioned guidance, specifically tailored for fine-grained categories, to replace the conventional classifier-free guidance sampling. Compared to full fine-tuning, FineDiffusion achieves a remarkable 1.56x training speed-up and requires storing merely 1.77% of the total model parameters, while achieving state-of-the-art FID of 9.776 on image generation of 10,000 classes. Extensive qualitative and quantitative experiments demonstrate the superiority of our method compared to other parameter-efficient fine-tuning methods. The code and more generated results are available at our project website: https://finediffusion.github.io/.\n## Fine-grained Classifier-free Guidance\nClassifier-free guidance has supplanted classifier guided sampling to achieve a trade-off between the diversity and quality of sampling. While reducing training complexity, this approach secures high-quality image generation. Currently, classifier-free guidance is widely utilized in various diffusion models. We attempt to introduce hierarchical category label information because knowing which superclass each category belongs to is helpful for capturing intricate details and enhancing the overall quality of the generated images. Therefore, we introduce a fine-grained classifier-free guidance sampling method, yielding improved results under specific conditions. Integrated with TieredEmbedder, this approach leverages superclass conditional information to enhance control over generated images. Conventional classifierfree guidance involves training both conditional and unconditional generative models. In our approach, a superclass conditional generative model is introduced to replace the unconditional model. During training, we use the same neural network to parameterize both the conditional model and the superclass conditional model, leveraging a score estimator \u03f5 \u03b8 for optimization guidance. Subclass labels c are probabilistically replaced with their corresponding superclass labels c p to achieve the learning of superclass embeddings. During sampling, trained superclass and subclass embeddings are input into the DiT network to obtain predictive noise. Our method combines conditional scores estimation \u03f5 \u03b8 (x t | c) and superclass conditional scores estimation \u03f5 \u03b8 (x t | c p ) to guide specific category image generation, with the specific formula provided below: \n\nwhere \u03c9 is the guidance scale, and \u03b5\u03b8 is the modified score. \n\nBy thoroughly exploiting the distributional characteristics of superclasses and subclasses, our sampling method achieves a finer control than classifier-free guidance. We can capture commonalities among subclasses within the same superclass and simultaneously capture distinct distributions between different superclasses, thereby making the learned distributions of each category more closely approximate real-world situations.",
            "reference_string": "[268041325 | Pan et al. | 2024 | Citations: 1]"
        },
        {
            "title": "ProtoDiffusion: Classifier-Free Diffusion Guidance with Prototype Learning",
            "venue": "Asian Conference on Machine Learning",
            "year": 2023,
            "reference_count": 35,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.01924",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.01924, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51222946",
                    "name": "Gulcin Baykal"
                },
                {
                    "authorId": "2213299635",
                    "name": "Halil Faruk Karagoz"
                },
                {
                    "authorId": "2092548838",
                    "name": "T. Binhuraib"
                },
                {
                    "authorId": "2256988285",
                    "name": "Gozde Unal"
                }
            ],
            "abstract": "Diffusion models are generative models that have shown significant advantages compared to other generative models in terms of higher generation quality and more stable training. However, the computational need for training diffusion models is considerably increased. In this work, we incorporate prototype learning into diffusion models to achieve high generation quality faster than the original diffusion model. Instead of randomly initialized class embeddings, we use separately learned class prototypes as the conditioning information to guide the diffusion process. We observe that our method, called ProtoDiffusion, achieves better performance in the early stages of training compared to the baseline method, signifying that using the learned prototypes shortens the training time. We demonstrate the performance of ProtoDiffusion using various datasets and experimental settings, achieving the best performance in shorter times across all settings.",
            "corpus_id": 259341599,
            "sentences": [
                {
                    "corpus_id": "259341599",
                    "title": "ProtoDiffusion: Classifier-Free Diffusion Guidance with Prototype Learning",
                    "text": "where \u03b8 is the function approximator to predict the value added to obtain x t . The simplified training objective is derived as in (Ho et al. (2020)): \n\nWhile the DDPM synthesizes unconditional images, guided diffusion models are also available for conditional image generation. Dhariwal and Nichol (2021) propose classifier guidance where the class conditional parameters \u00b5 \u03b8 (x t |y) and \u03a3 \u03b8 (x t |y) are perturbed by the gradients of a classifier p \u03c6 (y|x t ) that predicts the target class y. The perturbed mean with the guidance scale s is derived as: \n\nEven though the classifier guidance improves the quality of the images, there are a few problems of the classifier guidance. As the denoising process starts with highly noised input and proceeds with noisy images at most of the time steps, the classifier should be robust to noise. While obtaining such a classifier is challenging, predicting a class label does not require using the most of the information in the data. Therefore, taking the gradients of such a classifier might mislead the generation direction. Ho and Salimans (2021) propose classifier-free guidance method that does not require a separate classifier. The conditioning information y is periodically utilized while it is dropped out at the remaining time. Therefore, a single model can be used for both unconditional and conditional generation. Ho and Salimans (2021) derive that the unconditional \u03b8 (x t , t) and conditional \u03b8 (x t , t, y) estimations can be used to represent the gradients of the classifier as: \n\nEquation 7 implies that an implicit classifier can eliminate the need for an explicit classifier, and Ho and Salimans (2021) report better results with the classifier-free guidance compared to the explicit classifier guidance. \n\nIn our work, we enhance the classifier-free guidance method with prototype learning in order to improve the performance while decreasing the training time.",
                    "score": 0.5532888231504142,
                    "section_title": "Diffusion Models",
                    "char_start_offset": 11243,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 79
                        },
                        {
                            "start": 80,
                            "end": 150
                        },
                        {
                            "start": 153,
                            "end": 278
                        },
                        {
                            "start": 279,
                            "end": 496
                        },
                        {
                            "start": 497,
                            "end": 556
                        },
                        {
                            "start": 559,
                            "end": 683
                        },
                        {
                            "start": 684,
                            "end": 840
                        },
                        {
                            "start": 841,
                            "end": 979
                        },
                        {
                            "start": 980,
                            "end": 1072
                        },
                        {
                            "start": 1073,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1283
                        },
                        {
                            "start": 1284,
                            "end": 1372
                        },
                        {
                            "start": 1373,
                            "end": 1541
                        },
                        {
                            "start": 1544,
                            "end": 1770
                        },
                        {
                            "start": 1773,
                            "end": 1928
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 131,
                            "end": 148,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 279,
                            "end": 305,
                            "matchedPaperCorpusId": "234357997"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95849609375
                }
            ],
            "relevance_judgement": 0.95849609375,
            "relevance_judgment_input_expanded": "# Title: ProtoDiffusion: Classifier-Free Diffusion Guidance with Prototype Learning\n# Venue: Asian Conference on Machine Learning\n# Authors: Gulcin Baykal, Halil Faruk Karagoz, T. Binhuraib, Gozde Unal\n## Abstract\nDiffusion models are generative models that have shown significant advantages compared to other generative models in terms of higher generation quality and more stable training. However, the computational need for training diffusion models is considerably increased. In this work, we incorporate prototype learning into diffusion models to achieve high generation quality faster than the original diffusion model. Instead of randomly initialized class embeddings, we use separately learned class prototypes as the conditioning information to guide the diffusion process. We observe that our method, called ProtoDiffusion, achieves better performance in the early stages of training compared to the baseline method, signifying that using the learned prototypes shortens the training time. We demonstrate the performance of ProtoDiffusion using various datasets and experimental settings, achieving the best performance in shorter times across all settings.\n## Diffusion Models\nwhere \u03b8 is the function approximator to predict the value added to obtain x t . The simplified training objective is derived as in (Ho et al. (2020)): \n\nWhile the DDPM synthesizes unconditional images, guided diffusion models are also available for conditional image generation. Dhariwal and Nichol (2021) propose classifier guidance where the class conditional parameters \u00b5 \u03b8 (x t |y) and \u03a3 \u03b8 (x t |y) are perturbed by the gradients of a classifier p \u03c6 (y|x t ) that predicts the target class y. The perturbed mean with the guidance scale s is derived as: \n\nEven though the classifier guidance improves the quality of the images, there are a few problems of the classifier guidance. As the denoising process starts with highly noised input and proceeds with noisy images at most of the time steps, the classifier should be robust to noise. While obtaining such a classifier is challenging, predicting a class label does not require using the most of the information in the data. Therefore, taking the gradients of such a classifier might mislead the generation direction. Ho and Salimans (2021) propose classifier-free guidance method that does not require a separate classifier. The conditioning information y is periodically utilized while it is dropped out at the remaining time. Therefore, a single model can be used for both unconditional and conditional generation. Ho and Salimans (2021) derive that the unconditional \u03b8 (x t , t) and conditional \u03b8 (x t , t, y) estimations can be used to represent the gradients of the classifier as: \n\nEquation 7 implies that an implicit classifier can eliminate the need for an explicit classifier, and Ho and Salimans (2021) report better results with the classifier-free guidance compared to the explicit classifier guidance. \n\nIn our work, we enhance the classifier-free guidance method with prototype learning in order to improve the performance while decreasing the training time.",
            "reference_string": "[259341599 | Baykal et al. | 2023 | Citations: 3]"
        },
        {
            "title": "Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 58,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2270764731",
                    "name": "C. Wu"
                },
                {
                    "authorId": "2239102325",
                    "name": "Fernando De la Torre"
                }
            ],
            "abstract": "Text-to-image diffusion models have achieved remarkable performance in image synthesis, while the text interface does not always provide fine-grained control over certain image factors. For instance, changing a single token in the text can have unintended effects on the image. This paper shows a simple modification of classifier-free guidance can help disentangle image factors in text-to-image models. The key idea of our method, Contrastive Guidance, is to characterize an intended factor with two prompts that differ in minimal tokens: the positive prompt describes the image to be synthesized, and the baseline prompt serves as a\"baseline\"that disentangles other factors. Contrastive Guidance is a general method we illustrate whose benefits in three scenarios: (1) to guide domain-specific diffusion models trained on an object class, (2) to gain continuous, rig-like controls for text-to-image generation, and (3) to improve the performance of zero-shot image editors.",
            "corpus_id": 267770589,
            "sentences": [
                {
                    "corpus_id": "267770589",
                    "title": "Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models",
                    "text": "Text-to-image diffusion model: \n\nRecent years have witnessed unprecedented progress in text-to-image synthesis, driven by large generative models such as diffusion models [6,17,50,51] (e.g., GLIDE [38], DALL\u2022E 2 [44], Imagen [47], and Stable Diffusion [45]) and VQ Transformers [9] (e.g., DALL\u2022E [43], CogView [7,8], and Parti [57]). This paper studies how to extract disentangled image factors from text-to-image diffusion models. \n\nGuidance for diffusion models. Guidance methods modify the output distribution of pre-trained diffusion models, based on additional inputs such as class labels [6], text [38], and corrupted images [24,32]. The first guidance method is classifier guidance [6], for which a class classifier is finetuned on noisy images. Similarly, CLIP guidance [30,38] finetunes a CLIP model [42] to support text input. To avoid finetuning classifiers or CLIP, classifier-free guidance (CFG) [16] jointly trains a conditional and an unconditional diffusion model and combines their score estimates, and CFG has become the default for text-to-image tasks [38,45]. To compose multiple texts, composable diffusion [29] combines score estimates with different text inputs. Besides user-specified conditions, several works showed that even guidance based on model outputs [3] or representations [18] can improve the quality of images. In this paper, we explore how to disentangle image factors with texts to gain fine-grained control. \n\nImage editing with diffusion models. Recent works have shown that diffusion models are capable of unpaired imageto-image translation [4,35,52,55]. A more recent trend of works have explored zero-shot image editing with text-to-image diffusion models [14,25,55]. One of the applications of our Contrastive Guidance is to improve the intended edit of some of these zero-shot image editors. \n\nGuidance for other generative models. Guidance has also been widely studied for GANs [12] and autoregressive language models (LMs).",
                    "score": 0.6021384675171845,
                    "section_title": "Related Work",
                    "char_start_offset": 3897,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 30
                        },
                        {
                            "start": 33,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 431
                        },
                        {
                            "start": 434,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 639
                        },
                        {
                            "start": 640,
                            "end": 752
                        },
                        {
                            "start": 753,
                            "end": 836
                        },
                        {
                            "start": 837,
                            "end": 1079
                        },
                        {
                            "start": 1080,
                            "end": 1185
                        },
                        {
                            "start": 1186,
                            "end": 1346
                        },
                        {
                            "start": 1347,
                            "end": 1446
                        },
                        {
                            "start": 1449,
                            "end": 1485
                        },
                        {
                            "start": 1486,
                            "end": 1595
                        },
                        {
                            "start": 1596,
                            "end": 1710
                        },
                        {
                            "start": 1711,
                            "end": 1836
                        },
                        {
                            "start": 1839,
                            "end": 1876
                        },
                        {
                            "start": 1877,
                            "end": 1970
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 171,
                            "end": 174,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 174,
                            "end": 177,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 177,
                            "end": 180,
                            "matchedPaperCorpusId": "196470871"
                        },
                        {
                            "start": 180,
                            "end": 182,
                            "matchedPaperCorpusId": "227209335"
                        },
                        {
                            "start": 197,
                            "end": 201,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 212,
                            "end": 216,
                            "matchedPaperCorpusId": "248097655"
                        },
                        {
                            "start": 225,
                            "end": 229,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 252,
                            "end": 256,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 278,
                            "end": 281,
                            "matchedPaperCorpusId": "229297973"
                        },
                        {
                            "start": 296,
                            "end": 300,
                            "matchedPaperCorpusId": "232035663"
                        },
                        {
                            "start": 310,
                            "end": 313,
                            "matchedPaperCorpusId": "235212350"
                        },
                        {
                            "start": 313,
                            "end": 315,
                            "matchedPaperCorpusId": "248476190"
                        },
                        {
                            "start": 327,
                            "end": 331,
                            "matchedPaperCorpusId": "249926846"
                        },
                        {
                            "start": 594,
                            "end": 597,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 604,
                            "end": 608,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 631,
                            "end": 635,
                            "matchedPaperCorpusId": "246411364"
                        },
                        {
                            "start": 635,
                            "end": 638,
                            "matchedPaperCorpusId": "246240274"
                        },
                        {
                            "start": 689,
                            "end": 692,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 778,
                            "end": 782,
                            "matchedPaperCorpusId": "245117331"
                        },
                        {
                            "start": 782,
                            "end": 785,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 809,
                            "end": 813,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 909,
                            "end": 913,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1071,
                            "end": 1075,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 1075,
                            "end": 1078,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1128,
                            "end": 1132,
                            "matchedPaperCorpusId": "249375227"
                        },
                        {
                            "start": 1284,
                            "end": 1287,
                            "matchedPaperCorpusId": "251402961"
                        },
                        {
                            "start": 1307,
                            "end": 1311,
                            "matchedPaperCorpusId": "252683688"
                        },
                        {
                            "start": 1582,
                            "end": 1585,
                            "matchedPaperCorpusId": "236950721"
                        },
                        {
                            "start": 1585,
                            "end": 1588,
                            "matchedPaperCorpusId": "245704504"
                        },
                        {
                            "start": 1588,
                            "end": 1591,
                            "matchedPaperCorpusId": "247476275"
                        },
                        {
                            "start": 1591,
                            "end": 1594,
                            "matchedPaperCorpusId": "252815928"
                        },
                        {
                            "start": 1699,
                            "end": 1703,
                            "matchedPaperCorpusId": "251252882"
                        },
                        {
                            "start": 1703,
                            "end": 1706,
                            "matchedPaperCorpusId": "252918469"
                        },
                        {
                            "start": 1706,
                            "end": 1709,
                            "matchedPaperCorpusId": "252815928"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95654296875
                }
            ],
            "relevance_judgement": 0.95654296875,
            "relevance_judgment_input_expanded": "# Title: Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models\n# Venue: arXiv.org\n# Authors: C. Wu, Fernando De la Torre\n## Abstract\nText-to-image diffusion models have achieved remarkable performance in image synthesis, while the text interface does not always provide fine-grained control over certain image factors. For instance, changing a single token in the text can have unintended effects on the image. This paper shows a simple modification of classifier-free guidance can help disentangle image factors in text-to-image models. The key idea of our method, Contrastive Guidance, is to characterize an intended factor with two prompts that differ in minimal tokens: the positive prompt describes the image to be synthesized, and the baseline prompt serves as a\"baseline\"that disentangles other factors. Contrastive Guidance is a general method we illustrate whose benefits in three scenarios: (1) to guide domain-specific diffusion models trained on an object class, (2) to gain continuous, rig-like controls for text-to-image generation, and (3) to improve the performance of zero-shot image editors.\n## Related Work\nText-to-image diffusion model: \n\nRecent years have witnessed unprecedented progress in text-to-image synthesis, driven by large generative models such as diffusion models [6,17,50,51] (e.g., GLIDE [38], DALL\u2022E 2 [44], Imagen [47], and Stable Diffusion [45]) and VQ Transformers [9] (e.g., DALL\u2022E [43], CogView [7,8], and Parti [57]). This paper studies how to extract disentangled image factors from text-to-image diffusion models. \n\nGuidance for diffusion models. Guidance methods modify the output distribution of pre-trained diffusion models, based on additional inputs such as class labels [6], text [38], and corrupted images [24,32]. The first guidance method is classifier guidance [6], for which a class classifier is finetuned on noisy images. Similarly, CLIP guidance [30,38] finetunes a CLIP model [42] to support text input. To avoid finetuning classifiers or CLIP, classifier-free guidance (CFG) [16] jointly trains a conditional and an unconditional diffusion model and combines their score estimates, and CFG has become the default for text-to-image tasks [38,45]. To compose multiple texts, composable diffusion [29] combines score estimates with different text inputs. Besides user-specified conditions, several works showed that even guidance based on model outputs [3] or representations [18] can improve the quality of images. In this paper, we explore how to disentangle image factors with texts to gain fine-grained control. \n\nImage editing with diffusion models. Recent works have shown that diffusion models are capable of unpaired imageto-image translation [4,35,52,55]. A more recent trend of works have explored zero-shot image editing with text-to-image diffusion models [14,25,55]. One of the applications of our Contrastive Guidance is to improve the intended edit of some of these zero-shot image editors. \n\nGuidance for other generative models. Guidance has also been widely studied for GANs [12] and autoregressive language models (LMs).",
            "reference_string": "[267770589 | Wu et al. | 2024 | Citations: 2]"
        },
        {
            "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
            "venue": "International Conference on Machine Learning",
            "year": 2021,
            "reference_count": 51,
            "citation_count": 3629,
            "influential_citation_count": 296,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.10741, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "38967461",
                    "name": "Alex Nichol"
                },
                {
                    "authorId": "6515819",
                    "name": "Prafulla Dhariwal"
                },
                {
                    "authorId": "1992922591",
                    "name": "A. Ramesh"
                },
                {
                    "authorId": "67311962",
                    "name": "Pranav Shyam"
                },
                {
                    "authorId": "2051714782",
                    "name": "Pamela Mishkin"
                },
                {
                    "authorId": "39593364",
                    "name": "Bob McGrew"
                },
                {
                    "authorId": "1701686",
                    "name": "I. Sutskever"
                },
                {
                    "authorId": "2108828435",
                    "name": "Mark Chen"
                }
            ],
            "abstract": "Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.",
            "corpus_id": 245335086,
            "sentences": [
                {
                    "corpus_id": "245335086",
                    "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
                    "text": "Ho & Salimans (2021) achieved similar results without a separately trained classifier through the use of classifier-free guidance, a form of guidance that interpolates between predictions from a diffusion model with and without labels. \n\nMotivated by the ability of guided diffusion models to generate photorealistic samples and the ability of text-to-image models to handle free-form prompts, we apply guided diffusion to the problem of text-conditional image synthesis. First, we train a 3.5 billion parameter diffusion model that uses a text encoder to condition on natural language descriptions. Next, we compare two techniques for guiding diffusion models towards text prompts: CLIP guidance and classifier-free guidance. Using human and automated evaluations, we find that classifier-free guidance yields higherquality images. \n\nWe find that samples from our model generated with classifier-free guidance are both photorealistic and reflect a wide breadth of world knowledge. When evaluated by human judges, our samples are preferred to those from DALL-E (Ramesh et al., 2021) 87% of the time when evaluated for photorealism, and 69% of the time when evaluated for caption similarity. \n\n\"a hedgehog using a calculator\" \n\n\"a corgi wearing a red bowtie and a purple party hat\" \n\n\"robots meditating in a vipassana retreat\" \"a fall landscape with a small cottage next to a lake\" \"a surrealist dream-like oil painting by salvador dal\u00ed of a cat playing checkers\" \"a professional photo of a sunset behind the grand canyon\" \n\n\"a high-quality oil painting of a psychedelic hamster dragon\" \n\n\"an illustration of albert einstein wearing a superhero costume\" \n\n\"a boat in the canals of venice\" \"a painting of a fox in the style of starry night\" \"a red cube on top of a blue cube\" \"a stained glass window of a panda eating bamboo\" \"a crayon drawing of a space elevator\" \"a futuristic city in synthwave style\" \"a pixel art corgi pizza\" \"a fog rolling into new york\" \n\nFigure 1. Selected samples from GLIDE using classifier-free guidance. We observe that our model can produce photorealistic images with shadows and reflections, can compose multiple concepts in the correct way, and can produce artistic renderings of novel concepts.",
                    "score": 0.7803091933829204,
                    "section_title": "Introduction",
                    "char_start_offset": 1847,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 235
                        },
                        {
                            "start": 238,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 599
                        },
                        {
                            "start": 600,
                            "end": 726
                        },
                        {
                            "start": 727,
                            "end": 832
                        },
                        {
                            "start": 835,
                            "end": 981
                        },
                        {
                            "start": 982,
                            "end": 1190
                        },
                        {
                            "start": 1193,
                            "end": 1224
                        },
                        {
                            "start": 1227,
                            "end": 1280
                        },
                        {
                            "start": 1283,
                            "end": 1521
                        },
                        {
                            "start": 1524,
                            "end": 1585
                        },
                        {
                            "start": 1588,
                            "end": 1652
                        },
                        {
                            "start": 1655,
                            "end": 1957
                        },
                        {
                            "start": 1960,
                            "end": 1969
                        },
                        {
                            "start": 1970,
                            "end": 2029
                        },
                        {
                            "start": 2030,
                            "end": 2224
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9560546875
                }
            ],
            "relevance_judgement": 0.9560546875,
            "relevance_judgment_input_expanded": "# Title: GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models\n# Venue: International Conference on Machine Learning\n# Authors: Alex Nichol, Prafulla Dhariwal, A. Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, I. Sutskever, Mark Chen\n## Abstract\nDiffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.\n## Introduction\nHo & Salimans (2021) achieved similar results without a separately trained classifier through the use of classifier-free guidance, a form of guidance that interpolates between predictions from a diffusion model with and without labels. \n\nMotivated by the ability of guided diffusion models to generate photorealistic samples and the ability of text-to-image models to handle free-form prompts, we apply guided diffusion to the problem of text-conditional image synthesis. First, we train a 3.5 billion parameter diffusion model that uses a text encoder to condition on natural language descriptions. Next, we compare two techniques for guiding diffusion models towards text prompts: CLIP guidance and classifier-free guidance. Using human and automated evaluations, we find that classifier-free guidance yields higherquality images. \n\nWe find that samples from our model generated with classifier-free guidance are both photorealistic and reflect a wide breadth of world knowledge. When evaluated by human judges, our samples are preferred to those from DALL-E (Ramesh et al., 2021) 87% of the time when evaluated for photorealism, and 69% of the time when evaluated for caption similarity. \n\n\"a hedgehog using a calculator\" \n\n\"a corgi wearing a red bowtie and a purple party hat\" \n\n\"robots meditating in a vipassana retreat\" \"a fall landscape with a small cottage next to a lake\" \"a surrealist dream-like oil painting by salvador dal\u00ed of a cat playing checkers\" \"a professional photo of a sunset behind the grand canyon\" \n\n\"a high-quality oil painting of a psychedelic hamster dragon\" \n\n\"an illustration of albert einstein wearing a superhero costume\" \n\n\"a boat in the canals of venice\" \"a painting of a fox in the style of starry night\" \"a red cube on top of a blue cube\" \"a stained glass window of a panda eating bamboo\" \"a crayon drawing of a space elevator\" \"a futuristic city in synthwave style\" \"a pixel art corgi pizza\" \"a fog rolling into new york\" \n\nFigure 1. Selected samples from GLIDE using classifier-free guidance. We observe that our model can produce photorealistic images with shadows and reflections, can compose multiple concepts in the correct way, and can produce artistic renderings of novel concepts.",
            "reference_string": "[245335086 | Nichol et al. | 2021 | Citations: 3629]"
        },
        {
            "title": "A Review on Generative AI For Text-To-Image and Image-To-Image Generation and Implications To Scientific Images",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 37,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.21151, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267907323",
                    "name": "Zineb Sordo"
                },
                {
                    "authorId": "2271183137",
                    "name": "Eric Chagnon"
                },
                {
                    "authorId": "2276224795",
                    "name": "Daniela Ushizima"
                }
            ],
            "abstract": "This review surveys the state-of-the-art in text-to-image and image-to-image generation within the scope of generative AI. We provide a comparative analysis of three prominent architectures: Variational Autoencoders, Generative Adversarial Networks and Diffusion Models. For each, we elucidate core concepts, architectural innovations, and practical strengths and limitations, particularly for scientific image understanding. Finally, we discuss critical open challenges and potential future research directions in this rapidly evolving field.",
            "corpus_id": 276725462,
            "sentences": [
                {
                    "corpus_id": "276725462",
                    "title": "A Review on Generative AI For Text-To-Image and Image-To-Image Generation and Implications To Scientific Images",
                    "text": "Similarly to the CGAN, an important extension of the diffusion model is the Guided diffusion model that includes conditional image generation in the network. In that scenario, the model adds conditioning information y at each diffusion step: \n\nUsing Bayes rule with some computations and more importantly by adding the guidance scalar term s, we can show that guided diffusion models aim to learn \u2207logp \u03b8 (x t |y) such that: \n\nIt was also shown in [34] and [3] that a classifier guidance model defined by f \u03a6 (y|x t , t) can guide the diffusion towards the target class y by training f \u03a6 (y|x t , t) on a noisy image x t to predict class y. To do so, we build a classconditional diffusion model with mean \u00b5(x t |y) and variance \u03a3 \u03b8 (x t |y) and perturb the mean by the gradients of logf \u03a6 (y|x t , t) of class y, resulting in: \n\nFigure 6: Algorithm of classifier guided diffusion. Source: [3] Classifier Free-Guidance \n\nClassifier-free guidance, proposed by Ho et al. [12], allows for enhanced control in diffusion models by eliminating the need for separate classifiers. Instead of relying on a separate classifier, which increases training complexity and introduces bias potential, classifier-free guidance trains the diffusion model to directly learn and combine conditional and unconditional distributions during inference, streamlining the process. In other words, the authors train a conditional diffusion model \u03f5 \u03b8 (x t |y) and an unconditional model \u03f5 \u03b8 (x t |y = 0) as a single neural network as follows: \n\nThis approach is advantageous compared to the previous one as it trains a single model to guide the diffusion process and can take different types of conditional data such as text embeddings. We will see that many models rely on classifier free-guidance especially when training on multi-modal data.",
                    "score": 0.6340169319233215,
                    "section_title": "Conditional Image Generation with Guided Diffusion Classifier Guidance",
                    "char_start_offset": 17745,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 157
                        },
                        {
                            "start": 158,
                            "end": 241
                        },
                        {
                            "start": 244,
                            "end": 424
                        },
                        {
                            "start": 427,
                            "end": 640
                        },
                        {
                            "start": 641,
                            "end": 826
                        },
                        {
                            "start": 829,
                            "end": 880
                        },
                        {
                            "start": 881,
                            "end": 917
                        },
                        {
                            "start": 920,
                            "end": 1071
                        },
                        {
                            "start": 1072,
                            "end": 1353
                        },
                        {
                            "start": 1354,
                            "end": 1513
                        },
                        {
                            "start": 1516,
                            "end": 1707
                        },
                        {
                            "start": 1708,
                            "end": 1815
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 448,
                            "end": 452,
                            "matchedPaperCorpusId": "14888175"
                        },
                        {
                            "start": 457,
                            "end": 460,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 889,
                            "end": 892,
                            "matchedPaperCorpusId": "234357997"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9560546875
                }
            ],
            "relevance_judgement": 0.9560546875,
            "relevance_judgment_input_expanded": "# Title: A Review on Generative AI For Text-To-Image and Image-To-Image Generation and Implications To Scientific Images\n# Venue: arXiv.org\n# Authors: Zineb Sordo, Eric Chagnon, Daniela Ushizima\n## Abstract\nThis review surveys the state-of-the-art in text-to-image and image-to-image generation within the scope of generative AI. We provide a comparative analysis of three prominent architectures: Variational Autoencoders, Generative Adversarial Networks and Diffusion Models. For each, we elucidate core concepts, architectural innovations, and practical strengths and limitations, particularly for scientific image understanding. Finally, we discuss critical open challenges and potential future research directions in this rapidly evolving field.\n## Conditional Image Generation with Guided Diffusion Classifier Guidance\nSimilarly to the CGAN, an important extension of the diffusion model is the Guided diffusion model that includes conditional image generation in the network. In that scenario, the model adds conditioning information y at each diffusion step: \n\nUsing Bayes rule with some computations and more importantly by adding the guidance scalar term s, we can show that guided diffusion models aim to learn \u2207logp \u03b8 (x t |y) such that: \n\nIt was also shown in [34] and [3] that a classifier guidance model defined by f \u03a6 (y|x t , t) can guide the diffusion towards the target class y by training f \u03a6 (y|x t , t) on a noisy image x t to predict class y. To do so, we build a classconditional diffusion model with mean \u00b5(x t |y) and variance \u03a3 \u03b8 (x t |y) and perturb the mean by the gradients of logf \u03a6 (y|x t , t) of class y, resulting in: \n\nFigure 6: Algorithm of classifier guided diffusion. Source: [3] Classifier Free-Guidance \n\nClassifier-free guidance, proposed by Ho et al. [12], allows for enhanced control in diffusion models by eliminating the need for separate classifiers. Instead of relying on a separate classifier, which increases training complexity and introduces bias potential, classifier-free guidance trains the diffusion model to directly learn and combine conditional and unconditional distributions during inference, streamlining the process. In other words, the authors train a conditional diffusion model \u03f5 \u03b8 (x t |y) and an unconditional model \u03f5 \u03b8 (x t |y = 0) as a single neural network as follows: \n\nThis approach is advantageous compared to the previous one as it trains a single model to guide the diffusion process and can take different types of conditional data such as text embeddings. We will see that many models rely on classifier free-guidance especially when training on multi-modal data.",
            "reference_string": "[276725462 | Sordo et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2022,
            "reference_count": 115,
            "citation_count": 1133,
            "influential_citation_count": 96,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2206.10789",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.10789, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2338016295",
                    "name": "Jiahui Yu"
                },
                {
                    "authorId": "2145139570",
                    "name": "Yuanzhong Xu"
                },
                {
                    "authorId": "23978705",
                    "name": "Jing Yu Koh"
                },
                {
                    "authorId": "1821711",
                    "name": "Thang Luong"
                },
                {
                    "authorId": "1396954703",
                    "name": "Gunjan Baid"
                },
                {
                    "authorId": "2331539",
                    "name": "Zirui Wang"
                },
                {
                    "authorId": "2053781980",
                    "name": "Vijay Vasudevan"
                },
                {
                    "authorId": "31702389",
                    "name": "Alexander Ku"
                },
                {
                    "authorId": "2118771180",
                    "name": "Yinfei Yang"
                },
                {
                    "authorId": "143990191",
                    "name": "Burcu Karagol Ayan"
                },
                {
                    "authorId": "2044655623",
                    "name": "Ben Hutchinson"
                },
                {
                    "authorId": "143911112",
                    "name": "Wei Han"
                },
                {
                    "authorId": "27456119",
                    "name": "Zarana Parekh"
                },
                {
                    "authorId": "2158973314",
                    "name": "Xin Li"
                },
                {
                    "authorId": null,
                    "name": "Han Zhang"
                },
                {
                    "authorId": "1387994164",
                    "name": "Jason Baldridge"
                },
                {
                    "authorId": "48607963",
                    "name": "Yonghui Wu"
                }
            ],
            "abstract": "We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.",
            "corpus_id": 249926846,
            "sentences": [
                {
                    "corpus_id": "249926846",
                    "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
                    "text": "Classifier-free guidance [37] (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts. \n\nWith batch-sampled images per text prompt, contrastive reranking is used in DALL-E [2] which produces image-text alignment scores after the generation. We apply contrastive reranking in our work and find it is complementary to classifier-free guidance. Compared with the 512 images used in DALL-E [2], we sample just 16 images per text prompt for the experiments reported in this paper. We rerank each output set based on the alignment score of image and text embedding of a Contrastive Captioners model (CoCa) [25]. A CoCa base-size model (Table 1 in [25]) is trained on the same dataset with details in Section 4.1.",
                    "score": 0.8135096275324569,
                    "section_title": "Classifier-Free Guidance and Reranking",
                    "char_start_offset": 15352,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 549
                        },
                        {
                            "start": 550,
                            "end": 691
                        },
                        {
                            "start": 694,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 986
                        },
                        {
                            "start": 989,
                            "end": 1133
                        },
                        {
                            "start": 1134,
                            "end": 1230
                        },
                        {
                            "start": 1231,
                            "end": 1381
                        },
                        {
                            "start": 1382,
                            "end": 1535
                        },
                        {
                            "start": 1538,
                            "end": 1689
                        },
                        {
                            "start": 1690,
                            "end": 1790
                        },
                        {
                            "start": 1791,
                            "end": 1924
                        },
                        {
                            "start": 1925,
                            "end": 2054
                        },
                        {
                            "start": 2055,
                            "end": 2155
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 25,
                            "end": 29,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1621,
                            "end": 1624,
                            "matchedPaperCorpusId": "232035663"
                        },
                        {
                            "start": 1835,
                            "end": 1838,
                            "matchedPaperCorpusId": "232035663"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95263671875
                }
            ],
            "relevance_judgement": 0.95263671875,
            "relevance_judgment_input_expanded": "# Title: Scaling Autoregressive Models for Content-Rich Text-to-Image Generation\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu\n## Abstract\nWe present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.\n## Classifier-Free Guidance and Reranking\nClassifier-free guidance [37] (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts. \n\nWith batch-sampled images per text prompt, contrastive reranking is used in DALL-E [2] which produces image-text alignment scores after the generation. We apply contrastive reranking in our work and find it is complementary to classifier-free guidance. Compared with the 512 images used in DALL-E [2], we sample just 16 images per text prompt for the experiments reported in this paper. We rerank each output set based on the alignment score of image and text embedding of a Contrastive Captioners model (CoCa) [25]. A CoCa base-size model (Table 1 in [25]) is trained on the same dataset with details in Section 4.1.",
            "reference_string": "[249926846 | Yu et al. | 2022 | Citations: 1133]"
        },
        {
            "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 66,
            "citation_count": 6915,
            "influential_citation_count": 495,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2204.06125",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.06125, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1992922591",
                    "name": "A. Ramesh"
                },
                {
                    "authorId": "6515819",
                    "name": "Prafulla Dhariwal"
                },
                {
                    "authorId": "38967461",
                    "name": "Alex Nichol"
                },
                {
                    "authorId": "30414789",
                    "name": "Casey Chu"
                },
                {
                    "authorId": "2108828435",
                    "name": "Mark Chen"
                }
            ],
            "abstract": "Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.",
            "corpus_id": 248097655,
            "sentences": [
                {
                    "corpus_id": "248097655",
                    "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
                    "text": "[19,36,32,18] guide GANs using gradients from a CLIP model. For diffusion models, Dhariwal and Nichol [11] introduced classifier guidance as a way to use gradients from a classifier trained on noised images to steer the model towards higher quality generations. Nichol et al. [35] train a CLIP model on noised images and guide a text-conditional diffusion model, while Crowson, Crowson [7,8] use an unnoised CLIP model to guide unconditional or class-conditional diffusion models. Ho and Salimans [24] introduced classifier-free guidance and showed that one can perform guidance implictly from the predictions of the model with and without the conditioning information, thus removing the need for a classifier. Nichol et al. [35] showed classifier-free guidance works more favorably than CLIP guidance for text conditional image generation. \n\nSeveral previous works have trained generative image models that are directly conditioned on CLIP embeddings. Zhou et al. [61] condition GAN models on randomly perturbed CLIP image embeddings, finding that these models can generalize to CLIP text embeddings to produce text-conditional images. Crowson [9] trained diffusion models conditioned on CLIP text embeddings, allowing for direct text-conditional image generation. Wang et al. [54] train an autoregressive generative model conditioned on CLIP image embeddings, finding that it generalizes to CLIP text embeddings well enough to allow for text-conditional image synthesis. \n\nBordes et al. [3] train diffusion models conditioned on image representations from contrastive models. While the diffusion models themselves cannot generate images unconditionally, the authors experimented with a simple approach for two-stage image generation by employing Kernel Density Estimation to sample image representations. By feeding these generated representations to the diffusion model, they can generate images end-to-end in a way similar to our proposed technique. However, our work differs from this in two ways: first, we use multimodal contrastive representations rather than image-only representations; second, we employ much more powerful generative models for the first stage of the generation hierarchy, and these generative models are conditioned on text.",
                    "score": 0.668053059089506,
                    "section_title": "Related Work",
                    "char_start_offset": 25619,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 59
                        },
                        {
                            "start": 60,
                            "end": 261
                        },
                        {
                            "start": 262,
                            "end": 480
                        },
                        {
                            "start": 481,
                            "end": 710
                        },
                        {
                            "start": 711,
                            "end": 840
                        },
                        {
                            "start": 843,
                            "end": 952
                        },
                        {
                            "start": 953,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1265
                        },
                        {
                            "start": 1266,
                            "end": 1472
                        },
                        {
                            "start": 1475,
                            "end": 1577
                        },
                        {
                            "start": 1578,
                            "end": 1806
                        },
                        {
                            "start": 1807,
                            "end": 1953
                        },
                        {
                            "start": 1954,
                            "end": 2252
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 497,
                            "end": 501,
                            "matchedPaperCorpusId": "249145348"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95263671875
                }
            ],
            "relevance_judgement": 0.95263671875,
            "relevance_judgment_input_expanded": "# Title: Hierarchical Text-Conditional Image Generation with CLIP Latents\n# Venue: arXiv.org\n# Authors: A. Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen\n## Abstract\nContrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\n## Related Work\n[19,36,32,18] guide GANs using gradients from a CLIP model. For diffusion models, Dhariwal and Nichol [11] introduced classifier guidance as a way to use gradients from a classifier trained on noised images to steer the model towards higher quality generations. Nichol et al. [35] train a CLIP model on noised images and guide a text-conditional diffusion model, while Crowson, Crowson [7,8] use an unnoised CLIP model to guide unconditional or class-conditional diffusion models. Ho and Salimans [24] introduced classifier-free guidance and showed that one can perform guidance implictly from the predictions of the model with and without the conditioning information, thus removing the need for a classifier. Nichol et al. [35] showed classifier-free guidance works more favorably than CLIP guidance for text conditional image generation. \n\nSeveral previous works have trained generative image models that are directly conditioned on CLIP embeddings. Zhou et al. [61] condition GAN models on randomly perturbed CLIP image embeddings, finding that these models can generalize to CLIP text embeddings to produce text-conditional images. Crowson [9] trained diffusion models conditioned on CLIP text embeddings, allowing for direct text-conditional image generation. Wang et al. [54] train an autoregressive generative model conditioned on CLIP image embeddings, finding that it generalizes to CLIP text embeddings well enough to allow for text-conditional image synthesis. \n\nBordes et al. [3] train diffusion models conditioned on image representations from contrastive models. While the diffusion models themselves cannot generate images unconditionally, the authors experimented with a simple approach for two-stage image generation by employing Kernel Density Estimation to sample image representations. By feeding these generated representations to the diffusion model, they can generate images end-to-end in a way similar to our proposed technique. However, our work differs from this in two ways: first, we use multimodal contrastive representations rather than image-only representations; second, we employ much more powerful generative models for the first stage of the generation hierarchy, and these generative models are conditioned on text.",
            "reference_string": "[248097655 | Ramesh et al. | 2022 | Citations: 6915]"
        },
        {
            "title": "Implementing and Experimenting with Diffusion Models for Text-to-Image Generation",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 95,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2209.10948",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.10948, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1431226552",
                    "name": "Robin Zbinden"
                }
            ],
            "abstract": "Taking advantage of the many recent advances in deep learning, text-to-image generative models currently have the merit of attracting the general public attention. Two of these models, DALL-E 2 and Imagen, have demonstrated that highly photorealistic images could be generated from a simple textual description of an image. Based on a novel approach for image generation called diffusion models, text-to-image models enable the production of many different types of high resolution images, where human imagination is the only limit. However, these models require exceptionally large amounts of computational resources to train, as well as handling huge datasets collected from the internet. In addition, neither the codebase nor the models have been released. It consequently prevents the AI community from experimenting with these cutting-edge models, making the reproduction of their results complicated, if not impossible. In this thesis, we aim to contribute by firstly reviewing the different approaches and techniques used by these models, and then by proposing our own implementation of a text-to-image model. Highly based on DALL-E 2, we introduce several slight modifications to tackle the high computational cost induced. We thus have the opportunity to experiment in order to understand what these models are capable of, especially in a low resource regime. In particular, we provide additional and analyses deeper than the ones performed by the authors of DALL-E 2, including ablation studies. Besides, diffusion models use so-called guidance methods to help the generating process. We introduce a new guidance method which can be used in conjunction with other guidance methods to improve image quality. Finally, the images generated by our model are of reasonably good quality, without having to sustain the significant training costs of state-of-the-art text-toimage models.",
            "corpus_id": 252438737,
            "sentences": [
                {
                    "corpus_id": "252438737",
                    "title": "Implementing and Experimenting with Diffusion Models for Text-to-Image Generation",
                    "text": "Depending on a separate model is inconvenient and it complicates the training pipeline. Moreover, for classifier guidance, the number of classes is limited, preventing us from conditioning on complex information to generate more elaborated image compositions. Thus,  proposed classifier-free guidance, which only relies on a single diffusion model. Classifierfree guidance considers a conditional diffusion model \u03b8 (x t , t|y) that can be made unconditional by replacing occasionally during training the condition y by an empty condition \u2205, e.g., by setting the caption to an empty string. The model \u03b8 (x t , t|\u2205) can then be used to generate unconditional images. Classifier-free guidance therefore consists in updating the model output using a linear combination between \u03b8 (x t , t|\u2205) and \u03b8 (x t , t|y) in the following way:\n\nwith s \u2265 1 being the guidance scale. This update can be understood as an attempt to move further in the direction of the conditional model, while moving away from the unconditional model. It is performed at each diffusion step when sampling and Ho and Salimans (2021) have shown that it improves sample quality.",
                    "score": 0.6546996849742563,
                    "section_title": "Classifier-free guidance",
                    "char_start_offset": 30387,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95166015625
                }
            ],
            "relevance_judgement": 0.95166015625,
            "relevance_judgment_input_expanded": "# Title: Implementing and Experimenting with Diffusion Models for Text-to-Image Generation\n# Venue: arXiv.org\n# Authors: Robin Zbinden\n## Abstract\nTaking advantage of the many recent advances in deep learning, text-to-image generative models currently have the merit of attracting the general public attention. Two of these models, DALL-E 2 and Imagen, have demonstrated that highly photorealistic images could be generated from a simple textual description of an image. Based on a novel approach for image generation called diffusion models, text-to-image models enable the production of many different types of high resolution images, where human imagination is the only limit. However, these models require exceptionally large amounts of computational resources to train, as well as handling huge datasets collected from the internet. In addition, neither the codebase nor the models have been released. It consequently prevents the AI community from experimenting with these cutting-edge models, making the reproduction of their results complicated, if not impossible. In this thesis, we aim to contribute by firstly reviewing the different approaches and techniques used by these models, and then by proposing our own implementation of a text-to-image model. Highly based on DALL-E 2, we introduce several slight modifications to tackle the high computational cost induced. We thus have the opportunity to experiment in order to understand what these models are capable of, especially in a low resource regime. In particular, we provide additional and analyses deeper than the ones performed by the authors of DALL-E 2, including ablation studies. Besides, diffusion models use so-called guidance methods to help the generating process. We introduce a new guidance method which can be used in conjunction with other guidance methods to improve image quality. Finally, the images generated by our model are of reasonably good quality, without having to sustain the significant training costs of state-of-the-art text-toimage models.\n## Classifier-free guidance\nDepending on a separate model is inconvenient and it complicates the training pipeline. Moreover, for classifier guidance, the number of classes is limited, preventing us from conditioning on complex information to generate more elaborated image compositions. Thus,  proposed classifier-free guidance, which only relies on a single diffusion model. Classifierfree guidance considers a conditional diffusion model \u03b8 (x t , t|y) that can be made unconditional by replacing occasionally during training the condition y by an empty condition \u2205, e.g., by setting the caption to an empty string. The model \u03b8 (x t , t|\u2205) can then be used to generate unconditional images. Classifier-free guidance therefore consists in updating the model output using a linear combination between \u03b8 (x t , t|\u2205) and \u03b8 (x t , t|y) in the following way:\n\nwith s \u2265 1 being the guidance scale. This update can be understood as an attempt to move further in the direction of the conditional model, while moving away from the unconditional model. It is performed at each diffusion step when sampling and Ho and Salimans (2021) have shown that it improves sample quality.",
            "reference_string": "[252438737 | Zbinden | 2022 | Citations: 3]"
        },
        {
            "title": "LAW-Diffusion: Complex Scene Generation by Diffusion with Layouts",
            "venue": "IEEE International Conference on Computer Vision",
            "year": 2023,
            "reference_count": 49,
            "citation_count": 15,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.06713, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118583061",
                    "name": "Binbin Yang"
                },
                {
                    "authorId": "2157838841",
                    "name": "Yinzheng Luo"
                },
                {
                    "authorId": "49865638",
                    "name": "Ziliang Chen"
                },
                {
                    "authorId": "2749191",
                    "name": "Guangrun Wang"
                },
                {
                    "authorId": "40250403",
                    "name": "Xiaodan Liang"
                },
                {
                    "authorId": "2148303324",
                    "name": "Liang Lin"
                }
            ],
            "abstract": "Thanks to the rapid development of diffusion models, unprecedented progress has been witnessed in image synthesis. Prior works mostly rely on pre-trained linguistic models, but a text is often too abstract to properly specify all the spatial properties of an image, e.g., the layout configuration of a scene, leading to the sub-optimal results of complex scene generation. In this paper, we achieve accurate complex scene generation by proposing a semantically controllable Layout-AWare diffusion model, termed LAW-Diffusion. Distinct from the previous Layout-to-Image generation (L2I) methods that primarily explore category-aware relationships, LAW-Diffusion introduces a spatial dependency parser to encode the location-aware semantic coherence across objects as a layout embedding and produces a scene with perceptually harmonious object styles and contextual relations. To be specific, we delicately instantiate each object\u2019s regional semantics as an object region map and leverage a location-aware cross-object attention module to capture the spatial dependencies among those disentangled representations. We further propose an adaptive guidance schedule for our layout guidance to mitigate the trade-off between the regional semantic alignment and the texture fidelity of generated objects. Moreover, LAW-Diffusion allows for instance reconfiguration while maintaining the other regions in a synthesized image by introducing a layout-aware latent grafting mechanism to recompose its local regional semantics. To better verify the plausibility of generated scenes, we propose a new evaluation metric for the L2I task, dubbed Scene Relation Score (SRS) to measure how the images preserve the rational and harmonious relations among contextual objects. Comprehensive experiments on COCO-Stuff and Visual-Genome demonstrate that our LAW-Diffusion yields the state-of-the-art generative performance, especially with coherent object relations.",
            "corpus_id": 260886956,
            "sentences": [
                {
                    "corpus_id": "260886956",
                    "title": "LAW-Diffusion: Complex Scene Generation by Diffusion with Layouts",
                    "text": "Conditional Diffusion Models Classifier-guidance [4] provides a way for diffusion model to achieve conditional generation by using the gradient of a separately trained classifier p(y|x t ) during sampling. As a more efficient technique, classifier-free guidance [14, 24] replaces the noise estimator by a combination of conditional and unconditional model, without requirement of p(y|x t ): \n\nwhere y is the class label or text embedding from language model [24], \u03c9 \u2265 1 denotes the guidance scale and trivially increasing \u03c9 will amplify the effect of conditional input. \n\nWith the help of large-scale pre-trained CLIP [25] and other language models [31], diffusion models produce impressive results on text-to-image generation. However, their performance of complex scene generation are always unsatisfactory because the text embeddings from the linguistic models can not accurately capture the spatial properties, e.g., objects' locations, sizes and their implicit spatial associations. Distinct from text prompts, we focus on the task of generating complex scene images from the structured layout configurations (L2I) and further propose a diffusion modelbased method with flexibility and compositionality.",
                    "score": 0.745175143603323,
                    "section_title": "Preliminaries",
                    "char_start_offset": 11621,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 205
                        },
                        {
                            "start": 206,
                            "end": 390
                        },
                        {
                            "start": 393,
                            "end": 569
                        },
                        {
                            "start": 572,
                            "end": 727
                        },
                        {
                            "start": 728,
                            "end": 987
                        },
                        {
                            "start": 988,
                            "end": 1208
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 49,
                            "end": 52,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 618,
                            "end": 622,
                            "matchedPaperCorpusId": "231591445"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.951171875
                }
            ],
            "relevance_judgement": 0.951171875,
            "relevance_judgment_input_expanded": "# Title: LAW-Diffusion: Complex Scene Generation by Diffusion with Layouts\n# Venue: IEEE International Conference on Computer Vision\n# Authors: Binbin Yang, Yinzheng Luo, Ziliang Chen, Guangrun Wang, Xiaodan Liang, Liang Lin\n## Abstract\nThanks to the rapid development of diffusion models, unprecedented progress has been witnessed in image synthesis. Prior works mostly rely on pre-trained linguistic models, but a text is often too abstract to properly specify all the spatial properties of an image, e.g., the layout configuration of a scene, leading to the sub-optimal results of complex scene generation. In this paper, we achieve accurate complex scene generation by proposing a semantically controllable Layout-AWare diffusion model, termed LAW-Diffusion. Distinct from the previous Layout-to-Image generation (L2I) methods that primarily explore category-aware relationships, LAW-Diffusion introduces a spatial dependency parser to encode the location-aware semantic coherence across objects as a layout embedding and produces a scene with perceptually harmonious object styles and contextual relations. To be specific, we delicately instantiate each object\u2019s regional semantics as an object region map and leverage a location-aware cross-object attention module to capture the spatial dependencies among those disentangled representations. We further propose an adaptive guidance schedule for our layout guidance to mitigate the trade-off between the regional semantic alignment and the texture fidelity of generated objects. Moreover, LAW-Diffusion allows for instance reconfiguration while maintaining the other regions in a synthesized image by introducing a layout-aware latent grafting mechanism to recompose its local regional semantics. To better verify the plausibility of generated scenes, we propose a new evaluation metric for the L2I task, dubbed Scene Relation Score (SRS) to measure how the images preserve the rational and harmonious relations among contextual objects. Comprehensive experiments on COCO-Stuff and Visual-Genome demonstrate that our LAW-Diffusion yields the state-of-the-art generative performance, especially with coherent object relations.\n## Preliminaries\nConditional Diffusion Models Classifier-guidance [4] provides a way for diffusion model to achieve conditional generation by using the gradient of a separately trained classifier p(y|x t ) during sampling. As a more efficient technique, classifier-free guidance [14, 24] replaces the noise estimator by a combination of conditional and unconditional model, without requirement of p(y|x t ): \n\nwhere y is the class label or text embedding from language model [24], \u03c9 \u2265 1 denotes the guidance scale and trivially increasing \u03c9 will amplify the effect of conditional input. \n\nWith the help of large-scale pre-trained CLIP [25] and other language models [31], diffusion models produce impressive results on text-to-image generation. However, their performance of complex scene generation are always unsatisfactory because the text embeddings from the linguistic models can not accurately capture the spatial properties, e.g., objects' locations, sizes and their implicit spatial associations. Distinct from text prompts, we focus on the task of generating complex scene images from the structured layout configurations (L2I) and further propose a diffusion modelbased method with flexibility and compositionality.",
            "reference_string": "[260886956 | Yang et al. | 2023 | Citations: 15]"
        },
        {
            "title": "Diffusion Model-Based Image Editing: A Survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2024,
            "reference_count": 329,
            "citation_count": 102,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2402.17525",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.17525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2249841594",
                    "name": "Yi Huang"
                },
                {
                    "authorId": "2194958029",
                    "name": "Jiancheng Huang"
                },
                {
                    "authorId": "2247959941",
                    "name": "Yifan Liu"
                },
                {
                    "authorId": "2267694301",
                    "name": "Mingfu Yan"
                },
                {
                    "authorId": "2154657214",
                    "name": "Jiaxi Lv"
                },
                {
                    "authorId": "2267504760",
                    "name": "Jianzhuang Liu"
                },
                {
                    "authorId": "2273646978",
                    "name": "Wei Xiong"
                },
                {
                    "authorId": "2274091009",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2247480051",
                    "name": "Shifeng Chen"
                },
                {
                    "authorId": "2288206453",
                    "name": "Liangliang Cao"
                }
            ],
            "abstract": "Denoising diffusion models have emerged as a powerful tool for various image generation and editing tasks, facilitating the synthesis of visual content in an unconditional or input-conditional manner. The core idea behind them is learning to reverse the process of gradually adding noise to images, allowing them to generate high-quality samples from a complex distribution. In this survey, we provide an exhaustive overview of existing methods using diffusion models for image editing, covering both theoretical and practical aspects in the field. We delve into a thorough analysis and categorization of these works from multiple perspectives, including learning strategies, user-input conditions, and the array of specific editing tasks that can be accomplished. In addition, we pay special attention to image inpainting and outpainting, and explore both earlier traditional context-driven and current multimodal conditional methods, offering a comprehensive analysis of their methodologies. To further evaluate the performance of text-guided image editing algorithms, we propose a systematic benchmark, EditEval, featuring an innovative metric, LMM Score. Finally, we address current limitations and envision some potential directions for future research.",
            "corpus_id": 268033671,
            "sentences": [
                {
                    "corpus_id": "268033671",
                    "title": "Diffusion Model-Based Image Editing: A Survey",
                    "text": "Class-Conditioned Image Generation. Early efforts [35], [36], [88]- [91] usually incorporate the class-induced gradients via an additional pretrained classifier during sampling. However, Ho et al. [32] introduce the classifier-free guidance, which does not rely on an external classifier and allows for more versatile conditions, e.g., text, as guidance. Text-to-Image (T2I) Generation. GLIDE [38] is the first work that uses text to guide image generation directly from the highdimensional pixel level, replacing the label in class-conditioned diffusion models. Similarly, Imagen [31] uses a cascaded framework to generate high-resolution images more efficiently in pixel space. A different line of research first projects the image into a lower-dimensional space and then applies diffusion models in this latent space. Representative works include Stable Diffusion (SD) [30], VQ-diffusion [92], and DALL-E 2 [29]. Following these pioneering studies, a large number of works [41], [93]- [98] are proposed, advancing this field over the past two years. Additional Conditions. Beyond text, more specific conditions are also used to achieve higher fidelity and more precise control in image synthesis. GLIGEN [99] inserts a gated self-attention layer between the original self-attention and cross-attention layers in each block of a pretrained T2I diffusion model for generating images conditioned on grounding boxes. Make-A-Scene [100] and SpaText [101] use segmentation masks to guide the generation process. In addition to segmentation maps, ControlNet [102] can also incorporate other types of input, such as depth maps, normal maps, canny edges, pose, and sketches as conditions. Other methods [103]- [107] integrate diverse conditional inputs and include additional layers, enhancing the generative process controlled by these conditions. Personalized Image Generation. Closely related to image editing within conditional image generation is the task of creating personalized images. This task focuses on generating images that maintain a certain identity, typically guided by a few reference images of the same subject. Two early approaches are Textual Inversion [108] and DreamBooth [109].",
                    "score": 0.5554425891038743,
                    "section_title": "Conditional Image Generation",
                    "char_start_offset": 13969,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 35
                        },
                        {
                            "start": 36,
                            "end": 177
                        },
                        {
                            "start": 178,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 386
                        },
                        {
                            "start": 387,
                            "end": 562
                        },
                        {
                            "start": 563,
                            "end": 679
                        },
                        {
                            "start": 680,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 915
                        },
                        {
                            "start": 916,
                            "end": 1052
                        },
                        {
                            "start": 1053,
                            "end": 1075
                        },
                        {
                            "start": 1076,
                            "end": 1199
                        },
                        {
                            "start": 1200,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1508
                        },
                        {
                            "start": 1509,
                            "end": 1682
                        },
                        {
                            "start": 1683,
                            "end": 1842
                        },
                        {
                            "start": 1843,
                            "end": 1873
                        },
                        {
                            "start": 1874,
                            "end": 1987
                        },
                        {
                            "start": 1988,
                            "end": 2124
                        },
                        {
                            "start": 2125,
                            "end": 2195
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 50,
                            "end": 54,
                            "matchedPaperCorpusId": "235619773"
                        },
                        {
                            "start": 56,
                            "end": 60,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 62,
                            "end": 66,
                            "matchedPaperCorpusId": "247763065"
                        },
                        {
                            "start": 68,
                            "end": 72,
                            "matchedPaperCorpusId": "246442182"
                        },
                        {
                            "start": 197,
                            "end": 201,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 393,
                            "end": 397,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 891,
                            "end": 895,
                            "matchedPaperCorpusId": "244714856"
                        },
                        {
                            "start": 976,
                            "end": 980,
                            "matchedPaperCorpusId": "252762155"
                        },
                        {
                            "start": 988,
                            "end": 992,
                            "matchedPaperCorpusId": "258108187"
                        },
                        {
                            "start": 1207,
                            "end": 1211,
                            "matchedPaperCorpusId": "255942528"
                        },
                        {
                            "start": 1429,
                            "end": 1434,
                            "matchedPaperCorpusId": "247628171"
                        },
                        {
                            "start": 1447,
                            "end": 1452,
                            "matchedPaperCorpusId": "254018089"
                        },
                        {
                            "start": 1554,
                            "end": 1559,
                            "matchedPaperCorpusId": "256827727"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95068359375
                }
            ],
            "relevance_judgement": 0.95068359375,
            "relevance_judgment_input_expanded": "# Title: Diffusion Model-Based Image Editing: A Survey\n# Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence\n# Authors: Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, Liangliang Cao\n## Abstract\nDenoising diffusion models have emerged as a powerful tool for various image generation and editing tasks, facilitating the synthesis of visual content in an unconditional or input-conditional manner. The core idea behind them is learning to reverse the process of gradually adding noise to images, allowing them to generate high-quality samples from a complex distribution. In this survey, we provide an exhaustive overview of existing methods using diffusion models for image editing, covering both theoretical and practical aspects in the field. We delve into a thorough analysis and categorization of these works from multiple perspectives, including learning strategies, user-input conditions, and the array of specific editing tasks that can be accomplished. In addition, we pay special attention to image inpainting and outpainting, and explore both earlier traditional context-driven and current multimodal conditional methods, offering a comprehensive analysis of their methodologies. To further evaluate the performance of text-guided image editing algorithms, we propose a systematic benchmark, EditEval, featuring an innovative metric, LMM Score. Finally, we address current limitations and envision some potential directions for future research.\n## Conditional Image Generation\nClass-Conditioned Image Generation. Early efforts [35], [36], [88]- [91] usually incorporate the class-induced gradients via an additional pretrained classifier during sampling. However, Ho et al. [32] introduce the classifier-free guidance, which does not rely on an external classifier and allows for more versatile conditions, e.g., text, as guidance. Text-to-Image (T2I) Generation. GLIDE [38] is the first work that uses text to guide image generation directly from the highdimensional pixel level, replacing the label in class-conditioned diffusion models. Similarly, Imagen [31] uses a cascaded framework to generate high-resolution images more efficiently in pixel space. A different line of research first projects the image into a lower-dimensional space and then applies diffusion models in this latent space. Representative works include Stable Diffusion (SD) [30], VQ-diffusion [92], and DALL-E 2 [29]. Following these pioneering studies, a large number of works [41], [93]- [98] are proposed, advancing this field over the past two years. Additional Conditions. Beyond text, more specific conditions are also used to achieve higher fidelity and more precise control in image synthesis. GLIGEN [99] inserts a gated self-attention layer between the original self-attention and cross-attention layers in each block of a pretrained T2I diffusion model for generating images conditioned on grounding boxes. Make-A-Scene [100] and SpaText [101] use segmentation masks to guide the generation process. In addition to segmentation maps, ControlNet [102] can also incorporate other types of input, such as depth maps, normal maps, canny edges, pose, and sketches as conditions. Other methods [103]- [107] integrate diverse conditional inputs and include additional layers, enhancing the generative process controlled by these conditions. Personalized Image Generation. Closely related to image editing within conditional image generation is the task of creating personalized images. This task focuses on generating images that maintain a certain identity, typically guided by a few reference images of the same subject. Two early approaches are Textual Inversion [108] and DreamBooth [109].",
            "reference_string": "[268033671 | Huang et al. | 2024 | Citations: 102]"
        },
        {
            "title": "Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models",
            "venue": "",
            "year": 2025,
            "reference_count": 69,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.14399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2291140533",
                    "name": "Tian Xia"
                },
                {
                    "authorId": "2291143736",
                    "name": "Fabio De Sousa Ribeiro"
                },
                {
                    "authorId": "2182427359",
                    "name": "Rajat Rasal"
                },
                {
                    "authorId": "35982249",
                    "name": "A. Kori"
                },
                {
                    "authorId": "2330247364",
                    "name": "Raghav Mehta"
                },
                {
                    "authorId": "2254307038",
                    "name": "Ben Glocker"
                }
            ],
            "abstract": "Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.",
            "corpus_id": 279410887,
            "sentences": [],
            "relevance_judgement": 0.95068359375,
            "relevance_judgment_input_expanded": "# Title: Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models\n# Venue: \n# Authors: Tian Xia, Fabio De Sousa Ribeiro, Rajat Rasal, A. Kori, Raghav Mehta, Ben Glocker\n## Abstract\nCounterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.\n",
            "reference_string": "[279410887 | Xia et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Creativity and Machine Learning: A Survey",
            "venue": "ACM Computing Surveys",
            "year": 2021,
            "reference_count": 344,
            "citation_count": 42,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3664595",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.02726, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2067291198",
                    "name": "Giorgio Franceschelli"
                },
                {
                    "authorId": "1806767",
                    "name": "Mirco Musolesi"
                }
            ],
            "abstract": "There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.",
            "corpus_id": 233168627,
            "sentences": [
                {
                    "corpus_id": "233168627",
                    "title": "Creativity and Machine Learning: A Survey",
                    "text": "In order to generate higher-quality images and to allow text-to-image generation, a variety of effective methods for conditioning have been proposed. A possibility is to use classifier guidance [63]: the diffusion score (i.e., the added noise) includes the gradient of the log-likelihood of an auxiliary classifier model. An alternative is classifier-free guidance [117]: to avoid learning an additional model, a single neural network is used to parameterize two diffusion models, one conditional and one unconditional; the two models are then jointly trained by randomly setting the class for the unconditional model. \n\nFinally, the sampling is performed using a linear combination of conditional and unconditional score estimates. Guided Language to Image Diffusion for Generation and Editing (GLIDE) [198] demonstrates how classifier-free guidance can be effectively used to generate text-conditional images. In addition, it shows how diffusion models can be used for image editing by fine-tuning in order to reconstruct masked regions. Performance improvement can be obtained by means of a cascade of multiple diffusion models performing conditioning augmentation [116]. Notably, the diffusion model can operate on latent vectors instead of real images. Stable Diffusion [232] employs a diffusion model in the latent space of a pre-trained autoencoder. Similarly, DALL-E 2 [221] generates images by conditioning with image representations. At first, it learns a prior diffusion model to generate possible CLIP image embeddings from a given text caption, i.e., conditioned by its CLIP text embedding. Then, a diffusion decoder produces images conditioned by the image embedding. The generation quality can be further improved by means of generated captions for the images in the training set [19]. Imagen [235] uses instead a cascaded diffusion decoder, together with a frozen language model as a text encoder to increase the quality of output. \n\nAlthough the approach is particularly suitable for images, applications to other data sources have been developed as well. DiffWave [152] and WaveGrad [45] use diffusion models to generate audio. They overcome the continuousdiscrete dichotomy by working on waveform.",
                    "score": 0.5596967591201804,
                    "section_title": "Diffusion Models",
                    "char_start_offset": 46859,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 618
                        },
                        {
                            "start": 621,
                            "end": 732
                        },
                        {
                            "start": 733,
                            "end": 911
                        },
                        {
                            "start": 912,
                            "end": 1039
                        },
                        {
                            "start": 1040,
                            "end": 1174
                        },
                        {
                            "start": 1175,
                            "end": 1257
                        },
                        {
                            "start": 1258,
                            "end": 1356
                        },
                        {
                            "start": 1357,
                            "end": 1443
                        },
                        {
                            "start": 1444,
                            "end": 1603
                        },
                        {
                            "start": 1604,
                            "end": 1681
                        },
                        {
                            "start": 1682,
                            "end": 1800
                        },
                        {
                            "start": 1801,
                            "end": 1947
                        },
                        {
                            "start": 1950,
                            "end": 2072
                        },
                        {
                            "start": 2073,
                            "end": 2145
                        },
                        {
                            "start": 2146,
                            "end": 2216
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 194,
                            "end": 198,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 365,
                            "end": 370,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 803,
                            "end": 808,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 1168,
                            "end": 1173,
                            "matchedPaperCorpusId": "235619773"
                        },
                        {
                            "start": 1275,
                            "end": 1280,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 1808,
                            "end": 1813,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 2082,
                            "end": 2087,
                            "matchedPaperCorpusId": "221818900"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9501953125
                }
            ],
            "relevance_judgement": 0.9501953125,
            "relevance_judgment_input_expanded": "# Title: Creativity and Machine Learning: A Survey\n# Venue: ACM Computing Surveys\n# Authors: Giorgio Franceschelli, Mirco Musolesi\n## Abstract\nThere is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.\n## Diffusion Models\nIn order to generate higher-quality images and to allow text-to-image generation, a variety of effective methods for conditioning have been proposed. A possibility is to use classifier guidance [63]: the diffusion score (i.e., the added noise) includes the gradient of the log-likelihood of an auxiliary classifier model. An alternative is classifier-free guidance [117]: to avoid learning an additional model, a single neural network is used to parameterize two diffusion models, one conditional and one unconditional; the two models are then jointly trained by randomly setting the class for the unconditional model. \n\nFinally, the sampling is performed using a linear combination of conditional and unconditional score estimates. Guided Language to Image Diffusion for Generation and Editing (GLIDE) [198] demonstrates how classifier-free guidance can be effectively used to generate text-conditional images. In addition, it shows how diffusion models can be used for image editing by fine-tuning in order to reconstruct masked regions. Performance improvement can be obtained by means of a cascade of multiple diffusion models performing conditioning augmentation [116]. Notably, the diffusion model can operate on latent vectors instead of real images. Stable Diffusion [232] employs a diffusion model in the latent space of a pre-trained autoencoder. Similarly, DALL-E 2 [221] generates images by conditioning with image representations. At first, it learns a prior diffusion model to generate possible CLIP image embeddings from a given text caption, i.e., conditioned by its CLIP text embedding. Then, a diffusion decoder produces images conditioned by the image embedding. The generation quality can be further improved by means of generated captions for the images in the training set [19]. Imagen [235] uses instead a cascaded diffusion decoder, together with a frozen language model as a text encoder to increase the quality of output. \n\nAlthough the approach is particularly suitable for images, applications to other data sources have been developed as well. DiffWave [152] and WaveGrad [45] use diffusion models to generate audio. They overcome the continuousdiscrete dichotomy by working on waveform.",
            "reference_string": "[233168627 | Franceschelli et al. | 2021 | Citations: 42]"
        },
        {
            "title": "Compressed Image Generation with Denoising Diffusion Codebook Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 94,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.01189, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51228065",
                    "name": "Guy Ohayon"
                },
                {
                    "authorId": "2245392181",
                    "name": "Hila Manor"
                },
                {
                    "authorId": "1880407",
                    "name": "T. Michaeli"
                },
                {
                    "authorId": "2266840963",
                    "name": "Michael Elad"
                }
            ],
            "abstract": "We present a novel generative approach based on Denoising Diffusion Models (DDMs), which produces high-quality image samples along with their losslessly compressed bit-stream representations. This is obtained by replacing the standard Gaussian noise sampling in the reverse diffusion with a selection of noise samples from pre-defined codebooks of fixed iid Gaussian vectors. Surprisingly, we find that our method, termed Denoising Diffusion Codebook Model (DDCM), retains sample quality and diversity of standard DDMs, even for extremely small codebooks. We leverage DDCM and pick the noises from the codebooks that best match a given image, converting our generative model into a highly effective lossy image codec achieving state-of-the-art perceptual image compression results. More generally, by setting other noise selections rules, we extend our compression method to any conditional image generation task (e.g., image restoration), where the generated images are produced jointly with their condensed bit-stream representations. Our work is accompanied by a mathematical interpretation of the proposed compressed conditional generation schemes, establishing a connection with score-based approximations of posterior samplers for the tasks considered.",
            "corpus_id": 276094842,
            "sentences": [
                {
                    "corpus_id": "276094842",
                    "title": "Compressed Image Generation with Denoising Diffusion Codebook Models",
                    "text": "The task of text-conditional image generation can be solved using a conditional diffusion model, which, theoretically speaking, learns to sample from the posterior distribution p 0 (x 0 |y). In practice, however, using a conditional model directly typically yields low fidelity to the inputs. To address this limitation, CG can be used to improve this fidelity at the expense of sample quality and diversity (Dhariwal & Nichol, 2021). Classifier-Free Guidance (CFG) is used more often in practice, as it achieves the same tradeoff by mixing the conditional and unconditional scores during sampling (Ho & Salimans, 2021), thus eliminating the need for a classifier. Particularly, assuming we have access to both the conditional score s i (x i , y) := \u2207 xi log p i (x i |y) and the unconditional one s i (x i ), CFG proposes to modify the conditional score by \n\nwhere w, the CFG scale, is a hyper-parameter controlling the tradeoff between sample quality and diversity. \n\nHere, we introduce a new CFG method that allows generating compressed conditional samples using any pair of conditional and unconditional diffusion models, while controlling the tradeoff between generation quality and the fidelity to the inputs. \n\nNote that optimizing Eq. ( 51) is roughly equivalent to optimizing L P when x i is high dimensional (see App. C.2). As in App. C.5, we promote sample diversity by choosing k i from a randomly sampled subset of K < K indices at each step during the generation. We coin our method Compressed CFG (CCFG). \n\nWe implement our method using SD 2.1 trained on 768 \u00d7 768 images, adopting a DDPM noise schedule with T = 1000 diffusion steps, K = 64 fixed vectors in each codebook and K \u2208 {2, 3, 4, 6, 9}. We compare against the same diffusion model with standard DDPM sampling, using T = 1000 steps and w \u2208 {2, 5, 8, 11}.",
                    "score": 0.5747896722599236,
                    "section_title": "C.6. Compressed Classifier-Free Guidance",
                    "char_start_offset": 38692,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 190
                        },
                        {
                            "start": 191,
                            "end": 292
                        },
                        {
                            "start": 293,
                            "end": 434
                        },
                        {
                            "start": 435,
                            "end": 664
                        },
                        {
                            "start": 665,
                            "end": 857
                        },
                        {
                            "start": 860,
                            "end": 967
                        },
                        {
                            "start": 970,
                            "end": 1215
                        },
                        {
                            "start": 1218,
                            "end": 1333
                        },
                        {
                            "start": 1334,
                            "end": 1344
                        },
                        {
                            "start": 1345,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1519
                        },
                        {
                            "start": 1522,
                            "end": 1712
                        },
                        {
                            "start": 1713,
                            "end": 1829
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 408,
                            "end": 433,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 598,
                            "end": 619,
                            "matchedPaperCorpusId": "249145348"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94775390625
                }
            ],
            "relevance_judgement": 0.94775390625,
            "relevance_judgment_input_expanded": "# Title: Compressed Image Generation with Denoising Diffusion Codebook Models\n# Venue: arXiv.org\n# Authors: Guy Ohayon, Hila Manor, T. Michaeli, Michael Elad\n## Abstract\nWe present a novel generative approach based on Denoising Diffusion Models (DDMs), which produces high-quality image samples along with their losslessly compressed bit-stream representations. This is obtained by replacing the standard Gaussian noise sampling in the reverse diffusion with a selection of noise samples from pre-defined codebooks of fixed iid Gaussian vectors. Surprisingly, we find that our method, termed Denoising Diffusion Codebook Model (DDCM), retains sample quality and diversity of standard DDMs, even for extremely small codebooks. We leverage DDCM and pick the noises from the codebooks that best match a given image, converting our generative model into a highly effective lossy image codec achieving state-of-the-art perceptual image compression results. More generally, by setting other noise selections rules, we extend our compression method to any conditional image generation task (e.g., image restoration), where the generated images are produced jointly with their condensed bit-stream representations. Our work is accompanied by a mathematical interpretation of the proposed compressed conditional generation schemes, establishing a connection with score-based approximations of posterior samplers for the tasks considered.\n## C.6. Compressed Classifier-Free Guidance\nThe task of text-conditional image generation can be solved using a conditional diffusion model, which, theoretically speaking, learns to sample from the posterior distribution p 0 (x 0 |y). In practice, however, using a conditional model directly typically yields low fidelity to the inputs. To address this limitation, CG can be used to improve this fidelity at the expense of sample quality and diversity (Dhariwal & Nichol, 2021). Classifier-Free Guidance (CFG) is used more often in practice, as it achieves the same tradeoff by mixing the conditional and unconditional scores during sampling (Ho & Salimans, 2021), thus eliminating the need for a classifier. Particularly, assuming we have access to both the conditional score s i (x i , y) := \u2207 xi log p i (x i |y) and the unconditional one s i (x i ), CFG proposes to modify the conditional score by \n\nwhere w, the CFG scale, is a hyper-parameter controlling the tradeoff between sample quality and diversity. \n\nHere, we introduce a new CFG method that allows generating compressed conditional samples using any pair of conditional and unconditional diffusion models, while controlling the tradeoff between generation quality and the fidelity to the inputs. \n\nNote that optimizing Eq. ( 51) is roughly equivalent to optimizing L P when x i is high dimensional (see App. C.2). As in App. C.5, we promote sample diversity by choosing k i from a randomly sampled subset of K < K indices at each step during the generation. We coin our method Compressed CFG (CCFG). \n\nWe implement our method using SD 2.1 trained on 768 \u00d7 768 images, adopting a DDPM noise schedule with T = 1000 diffusion steps, K = 64 fixed vectors in each codebook and K \u2208 {2, 3, 4, 6, 9}. We compare against the same diffusion model with standard DDPM sampling, using T = 1000 steps and w \u2208 {2, 5, 8, 11}.",
            "reference_string": "[276094842 | Ohayon et al. | 2025 | Citations: 0]"
        },
        {
            "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 54,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.02687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275329323",
                    "name": "Viet Nguyen"
                },
                {
                    "authorId": "2333424276",
                    "name": "Anh Aengus Nguyen"
                },
                {
                    "authorId": "2276606039",
                    "name": "T. Dao"
                },
                {
                    "authorId": "2261741144",
                    "name": "Khoi Nguyen"
                },
                {
                    "authorId": "2269462407",
                    "name": "Cuong Pham"
                },
                {
                    "authorId": "2275127531",
                    "name": "Toan Tran"
                },
                {
                    "authorId": "2327046351",
                    "name": "Anh Tran"
                }
            ],
            "abstract": "Recent approaches have yielded promising results in distilling multi-step text-to-image diffusion models into one-step ones. The state-of-the-art efficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the teacher model's performance with limited resources. However, our study reveals its instability when handling different diffusion model backbones due to using a fixed guidance scale within the Variational Score Distillation (VSD) loss. Another weakness of the existing one-step diffusion models is the missing support for negative prompt guidance, which is crucial in practical image generation. This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images. Our experimental results show that our proposed methods significantly improve baseline models across various metrics. Remarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art benchmark for one-step diffusion models.",
            "corpus_id": 274446026,
            "sentences": [
                {
                    "corpus_id": "274446026",
                    "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
                    "text": "Classifier-free Guidance (CFG) [15] is an inference technique designed to enhance the quality of generated images by blending the predictions from both a conditional and an unconditional model. At each sampling step, CFG adjusts the denoiser's output using a control parameter \u03ba > 1, allowing for controlled guidance that aligns more closely with the desired conditions: \n\nNegative Prompts provide enhanced control by suppressing unwanted features in the generated content. Instead of producing an unconditional output, the model generates an output conditioned on the negative prompt y \u2212 , as follows: \n\nVariational Score Distillation (VSD) is a powerful framework that utilizes pretrained diffusion-based text-to-image models to enhance text-based generation across both 3D and 2D domains. Originally proposed for text-to-3D tasks [48], VSD leverages diffusion-based score matching to align Neural Radiance Fields (NeRFs) with text prompt, enabling the generation of intricate 3D objects [48]. This approach extends effectively to 2D image synthesis as well, where VSD enables rapid, high-quality text-to-image generation in a single step with models such as SwiftBrush and DMD [7,33,54,55], sidestepping the computational burden of multi-step diffusion methods. The central aim of VSD is to ensures that the renderings of a differentiable generator align with the probability density of plausible images as guided by the 2D diffusion model. To accomplish this, VSD employs a two-teacher approach that uses a fixed pretrained diffusion model \u03f5 \u03c8 and an adaptive LoRA-based teacher \u03f5 \u03d5 . While training, the student model f \u03b8 produces 2D images x0 = f \u03b8 (z, y) using an input noise z \u223c N (0, I) and a text prompt y. Noisy images xt = \u03b1 t x0 + \u03c3 t \u03f5 is then fed into both teacher models. The LoRA teacher model \u03f5 \u03d5 aligns with the student distribution by minimizing a denoising L2 loss on singlestep samples. This arrangement supports robust and adaptive guidance suitable for a variety of generative architectures.",
                    "score": 0.5623419720899366,
                    "section_title": "Background",
                    "char_start_offset": 9869,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 193
                        },
                        {
                            "start": 194,
                            "end": 370
                        },
                        {
                            "start": 373,
                            "end": 473
                        },
                        {
                            "start": 474,
                            "end": 602
                        },
                        {
                            "start": 605,
                            "end": 791
                        },
                        {
                            "start": 792,
                            "end": 995
                        },
                        {
                            "start": 996,
                            "end": 1264
                        },
                        {
                            "start": 1265,
                            "end": 1443
                        },
                        {
                            "start": 1444,
                            "end": 1588
                        },
                        {
                            "start": 1589,
                            "end": 1716
                        },
                        {
                            "start": 1717,
                            "end": 1787
                        },
                        {
                            "start": 1788,
                            "end": 1908
                        },
                        {
                            "start": 1909,
                            "end": 2015
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 833,
                            "end": 837,
                            "matchedPaperCorpusId": "258887357"
                        },
                        {
                            "start": 990,
                            "end": 994,
                            "matchedPaperCorpusId": "258887357"
                        },
                        {
                            "start": 1180,
                            "end": 1183,
                            "matchedPaperCorpusId": "271956822"
                        },
                        {
                            "start": 1186,
                            "end": 1189,
                            "matchedPaperCorpusId": "269982921"
                        },
                        {
                            "start": 1189,
                            "end": 1192,
                            "matchedPaperCorpusId": "265506768"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94482421875
                }
            ],
            "relevance_judgement": 0.94482421875,
            "relevance_judgment_input_expanded": "# Title: SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance\n# Venue: arXiv.org\n# Authors: Viet Nguyen, Anh Aengus Nguyen, T. Dao, Khoi Nguyen, Cuong Pham, Toan Tran, Anh Tran\n## Abstract\nRecent approaches have yielded promising results in distilling multi-step text-to-image diffusion models into one-step ones. The state-of-the-art efficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the teacher model's performance with limited resources. However, our study reveals its instability when handling different diffusion model backbones due to using a fixed guidance scale within the Variational Score Distillation (VSD) loss. Another weakness of the existing one-step diffusion models is the missing support for negative prompt guidance, which is crucial in practical image generation. This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images. Our experimental results show that our proposed methods significantly improve baseline models across various metrics. Remarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art benchmark for one-step diffusion models.\n## Background\nClassifier-free Guidance (CFG) [15] is an inference technique designed to enhance the quality of generated images by blending the predictions from both a conditional and an unconditional model. At each sampling step, CFG adjusts the denoiser's output using a control parameter \u03ba > 1, allowing for controlled guidance that aligns more closely with the desired conditions: \n\nNegative Prompts provide enhanced control by suppressing unwanted features in the generated content. Instead of producing an unconditional output, the model generates an output conditioned on the negative prompt y \u2212 , as follows: \n\nVariational Score Distillation (VSD) is a powerful framework that utilizes pretrained diffusion-based text-to-image models to enhance text-based generation across both 3D and 2D domains. Originally proposed for text-to-3D tasks [48], VSD leverages diffusion-based score matching to align Neural Radiance Fields (NeRFs) with text prompt, enabling the generation of intricate 3D objects [48]. This approach extends effectively to 2D image synthesis as well, where VSD enables rapid, high-quality text-to-image generation in a single step with models such as SwiftBrush and DMD [7,33,54,55], sidestepping the computational burden of multi-step diffusion methods. The central aim of VSD is to ensures that the renderings of a differentiable generator align with the probability density of plausible images as guided by the 2D diffusion model. To accomplish this, VSD employs a two-teacher approach that uses a fixed pretrained diffusion model \u03f5 \u03c8 and an adaptive LoRA-based teacher \u03f5 \u03d5 . While training, the student model f \u03b8 produces 2D images x0 = f \u03b8 (z, y) using an input noise z \u223c N (0, I) and a text prompt y. Noisy images xt = \u03b1 t x0 + \u03c3 t \u03f5 is then fed into both teacher models. The LoRA teacher model \u03f5 \u03d5 aligns with the student distribution by minimizing a denoising L2 loss on singlestep samples. This arrangement supports robust and adaptive guidance suitable for a variety of generative architectures.",
            "reference_string": "[274446026 | Nguyen et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Style-A-Video: Agile Diffusion for Arbitrary Text-Based Video Style Transfer",
            "venue": "IEEE Signal Processing Letters",
            "year": 2023,
            "reference_count": 79,
            "citation_count": 17,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.05464",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.05464, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2186281333",
                    "name": "Nisha Huang"
                },
                {
                    "authorId": "2108078624",
                    "name": "Yu-xin Zhang"
                },
                {
                    "authorId": "40441149",
                    "name": "Weiming Dong"
                }
            ],
            "abstract": "Large-scale text-to-video diffusion models have shown outstanding capabilities. However, their direct application to video stylization is hindered by the limited availability of text-to-video datasets and computational resources. Moreover, meeting content preservation standards for style transfer tasks is challenging due to the stochastic and destructive nature of the noise addition process. This letter introduces a succinct video stylization approach, named Style-A-Video, which leverages a generative pre-trained transformer and an image latent diffusion model for text-controlled video stylization. We improve the guidance conditions in the denoising process to maintain a balance between artistic expression and structural preservation. Additionally, by integrating sampling optimization and temporal consistency modules, we address inter-frame flickering and prevent additional artifacts. Comprehensive experimental results demonstrate superior content preservation and stylistic performance while minimizing resource consumption.",
            "corpus_id": 258564566,
            "sentences": [
                {
                    "corpus_id": "258564566",
                    "title": "Style-A-Video: Agile Diffusion for Arbitrary Text-Based Video Style Transfer",
                    "text": "Classifier-free guidance [17] is a method for trading off the quality and diversity of samples generated by a diffusion model. It is commonly used in class-conditional and text-conditional image generation to improve the visual quality of generated images and to make sampled images better correspond with their conditioning. Classifier-free guidance effectively shifts probability mass toward \n\nGuidance for conditions. For our task, the scoring network \u03b5  (  ,   ,  T ) has three conditions: the input image   , text prompt  T , and self-attention map   . We find it beneficial to leverage classifier-free guidance: \n\nconcerning each condition. Liu et al. [32] demonstrate that a conditional diffusion model can compose score estimates from multiple different conditioning values. We introduce three guidance scales,   ,  T , and   , which can be adjusted to trade off how strongly the generated samples correspond with the conditions. Our modified score estimate is as follows:",
                    "score": 0.8165465773768726,
                    "section_title": "Condition Guidance",
                    "char_start_offset": 15784,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 126
                        },
                        {
                            "start": 127,
                            "end": 325
                        },
                        {
                            "start": 326,
                            "end": 393
                        },
                        {
                            "start": 396,
                            "end": 420
                        },
                        {
                            "start": 421,
                            "end": 557
                        },
                        {
                            "start": 558,
                            "end": 617
                        },
                        {
                            "start": 620,
                            "end": 646
                        },
                        {
                            "start": 647,
                            "end": 782
                        },
                        {
                            "start": 783,
                            "end": 937
                        },
                        {
                            "start": 938,
                            "end": 980
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 658,
                            "end": 662,
                            "matchedPaperCorpusId": "249375227"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94091796875
                }
            ],
            "relevance_judgement": 0.94091796875,
            "relevance_judgment_input_expanded": "# Title: Style-A-Video: Agile Diffusion for Arbitrary Text-Based Video Style Transfer\n# Venue: IEEE Signal Processing Letters\n# Authors: Nisha Huang, Yu-xin Zhang, Weiming Dong\n## Abstract\nLarge-scale text-to-video diffusion models have shown outstanding capabilities. However, their direct application to video stylization is hindered by the limited availability of text-to-video datasets and computational resources. Moreover, meeting content preservation standards for style transfer tasks is challenging due to the stochastic and destructive nature of the noise addition process. This letter introduces a succinct video stylization approach, named Style-A-Video, which leverages a generative pre-trained transformer and an image latent diffusion model for text-controlled video stylization. We improve the guidance conditions in the denoising process to maintain a balance between artistic expression and structural preservation. Additionally, by integrating sampling optimization and temporal consistency modules, we address inter-frame flickering and prevent additional artifacts. Comprehensive experimental results demonstrate superior content preservation and stylistic performance while minimizing resource consumption.\n## Condition Guidance\nClassifier-free guidance [17] is a method for trading off the quality and diversity of samples generated by a diffusion model. It is commonly used in class-conditional and text-conditional image generation to improve the visual quality of generated images and to make sampled images better correspond with their conditioning. Classifier-free guidance effectively shifts probability mass toward \n\nGuidance for conditions. For our task, the scoring network \u03b5  (  ,   ,  T ) has three conditions: the input image   , text prompt  T , and self-attention map   . We find it beneficial to leverage classifier-free guidance: \n\nconcerning each condition. Liu et al. [32] demonstrate that a conditional diffusion model can compose score estimates from multiple different conditioning values. We introduce three guidance scales,   ,  T , and   , which can be adjusted to trade off how strongly the generated samples correspond with the conditions. Our modified score estimate is as follows:",
            "reference_string": "[258564566 | Huang et al. | 2023 | Citations: 17]"
        },
        {
            "title": "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2022,
            "reference_count": 60,
            "citation_count": 309,
            "influential_citation_count": 106,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2211.05105",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.05105, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "40896023",
                    "name": "P. Schramowski"
                },
                {
                    "authorId": "2166299958",
                    "name": "Manuel Brack"
                },
                {
                    "authorId": "2905059",
                    "name": "Bjorn Deiseroth"
                },
                {
                    "authorId": "2066493115",
                    "name": "K. Kersting"
                }
            ],
            "abstract": "Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse effect on overall image quality or text alignment.11Code available at https://huggingface.co/docs/diffusers/api/pipelines/stable.diffusion.safe",
            "corpus_id": 253420366,
            "sentences": [
                {
                    "corpus_id": "253420366",
                    "title": "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models",
                    "text": "We introduce safety guidance for latent diffusion models to reduce the inappropriate degeneration of DMs. Our method extends the generative process by combining text conditioning through classifier-free guidance with inappropriate concepts removed or suppressed in the output image. Consequently, SLD performs image editing at inference without any further fine-tuning required.\n\nDiffusion models iteratively denoise a Gaussian distributed variable to produce samples of a learned data distribution. Intuitively, image generation starts from random noise , and the model predicts an estimate of this noise\u02dc \u03b8 to be subtracted from the initial values. This results in a high-fidelity image x without any noise. Since this is an extremely hard problem, multiple steps are applied, each subtracting a small amount ( t ) of the predictive noise, approximating . For text-to-image generation, the model's -prediction is conditioned on a text prompt p and results in an image faithful to that prompt. The training objective of a diffusion modelx \u03b8 can be written as\n\nwhere (x, c p ) is conditioned on text prompt p, t is drawn from a uniform distribution t \u223c U([0, 1]), sampled from a Gaussian \u223c N (0, I), and w t , \u03c9 t , \u03b1 t influence image fidelity depending on t. Consequently, the DM is trained to denoise z t := x + to yield x with the squared error as loss. At inference, the DM is sampled using the model's prediction of x = (z t \u2212\u00af \u03b8 ), with\u00af \u03b8 as described below.\n\nClassifier-free guidance [17] is a conditioning method using a purely generational diffusion model, eliminating the need for an additional pre-trained classifier. The approach randomly drops the text conditioning c p with a fixed probability during training, resulting in a joint model for unconditional and conditional objectives. During inference the score estimates for the x-prediction are adjusted so that:\n\nwith guidance scale s g which is typically chosen as s g \u2208 (0, 20] and \u03b8 defining the noise estimate with parameters \u03b8. Intuitively, the unconditioned -prediction \u03b8 (z t ) is pushed in the direction of the conditioned \u03b8 (z t , c p ) to yield an image faithful to prompt p. Lastly, s g determines the magnitude of the influence",
                    "score": 0.6556133785561612,
                    "section_title": "Safe Latent Diffusion (SLD)",
                    "char_start_offset": 11423,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94091796875
                }
            ],
            "relevance_judgement": 0.94091796875,
            "relevance_judgment_input_expanded": "# Title: Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models\n# Venue: Computer Vision and Pattern Recognition\n# Authors: P. Schramowski, Manuel Brack, Bjorn Deiseroth, K. Kersting\n## Abstract\nText-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse effect on overall image quality or text alignment.11Code available at https://huggingface.co/docs/diffusers/api/pipelines/stable.diffusion.safe\n## Safe Latent Diffusion (SLD)\nWe introduce safety guidance for latent diffusion models to reduce the inappropriate degeneration of DMs. Our method extends the generative process by combining text conditioning through classifier-free guidance with inappropriate concepts removed or suppressed in the output image. Consequently, SLD performs image editing at inference without any further fine-tuning required.\n\nDiffusion models iteratively denoise a Gaussian distributed variable to produce samples of a learned data distribution. Intuitively, image generation starts from random noise , and the model predicts an estimate of this noise\u02dc \u03b8 to be subtracted from the initial values. This results in a high-fidelity image x without any noise. Since this is an extremely hard problem, multiple steps are applied, each subtracting a small amount ( t ) of the predictive noise, approximating . For text-to-image generation, the model's -prediction is conditioned on a text prompt p and results in an image faithful to that prompt. The training objective of a diffusion modelx \u03b8 can be written as\n\nwhere (x, c p ) is conditioned on text prompt p, t is drawn from a uniform distribution t \u223c U([0, 1]), sampled from a Gaussian \u223c N (0, I), and w t , \u03c9 t , \u03b1 t influence image fidelity depending on t. Consequently, the DM is trained to denoise z t := x + to yield x with the squared error as loss. At inference, the DM is sampled using the model's prediction of x = (z t \u2212\u00af \u03b8 ), with\u00af \u03b8 as described below.\n\nClassifier-free guidance [17] is a conditioning method using a purely generational diffusion model, eliminating the need for an additional pre-trained classifier. The approach randomly drops the text conditioning c p with a fixed probability during training, resulting in a joint model for unconditional and conditional objectives. During inference the score estimates for the x-prediction are adjusted so that:\n\nwith guidance scale s g which is typically chosen as s g \u2208 (0, 20] and \u03b8 defining the noise estimate with parameters \u03b8. Intuitively, the unconditioned -prediction \u03b8 (z t ) is pushed in the direction of the conditioned \u03b8 (z t , c p ) to yield an image faithful to prompt p. Lastly, s g determines the magnitude of the influence",
            "reference_string": "[253420366 | Schramowski et al. | 2022 | Citations: 309]"
        },
        {
            "title": "SPAD: Spatially Aware Multi-View Diffusers",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2024,
            "reference_count": 110,
            "citation_count": 38,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2402.05235",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.05235, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66536530",
                    "name": "Yash Kant"
                },
                {
                    "authorId": "2253894469",
                    "name": "Ziyi Wu"
                },
                {
                    "authorId": "2261673978",
                    "name": "Michael Vasilkovsky"
                },
                {
                    "authorId": "2279023722",
                    "name": "Guocheng Qian"
                },
                {
                    "authorId": "2258296012",
                    "name": "Jian Ren"
                },
                {
                    "authorId": "134642679",
                    "name": "R. A. Guler"
                },
                {
                    "authorId": "2279742224",
                    "name": "Bernard Ghanem"
                },
                {
                    "authorId": "145582202",
                    "name": "S. Tulyakov"
                },
                {
                    "authorId": "2262216913",
                    "name": "Igor Gilitschenski"
                },
                {
                    "authorId": "10753214",
                    "name": "Aliaksandr Siarohin"
                }
            ],
            "abstract": "We present SPAD, a novel approach for creating con-sistent multi-view images from text prompts or single images. To enable multi-view generation, we repurpose a pre-trained 2D diffusion model by extending its self-attention layers with cross-view interactions, and fine-tune it on a high quality subset of Objaverse. We find that a naive extension of the self-attention proposed in prior work (e.g., MV-Dream) leads to content copying between views. Therefore, we explicitly constrain the cross-view attention based on epipolar geometry. To further enhance 3D consistency, we utilize Pl\u00fccker coordinates derived from camera rays and inject them as positional encoding. This enables SPAD to reason over spatial proximity in 3D well. Compared to concurrent works that can only generate views at fixed azimuth and elevation (e.g., MVDream, SyncDreamer), SPAD offers full camera control and achieves state-of-the-art results in novel view synthesis on unseen objects from the Objaverse and Google Scanned Objects datasets. Finally, we demon-strate that text-to-3D generation using SPAD prevents the multi-face Janus issue.",
            "corpus_id": 267547881,
            "sentences": [
                {
                    "corpus_id": "267547881",
                    "title": "SPAD: Spatially Aware Multi-View Diffusers",
                    "text": "Classifier-free diffusion guidance [33] is a technique used to balance the quality and diversity of images produced by diffusion models. This method is particularly effective in class-conditional and text-conditional image generation, enhancing both the visual quality of images and their alignment with given conditions. Inspired by [5] we explore the integration of classifier-free guidance with Epipolar Attention and Pl\u00fccker Embedding. Implementing classifierfree guidance involves simultaneous training of the diffusion model for both conditional and unconditional denoising tasks. During inference, these models' score estimates are merged. We have four different types of conditioning injected into our system: Outcome: As shown in Fig. 13, we find that classifier-free guidance beyond text conditioning does not provide additional benefits, and rather leads to over-saturated generations. This also aligns with our observations on MVDream.",
                    "score": 0.7284085479189819,
                    "section_title": "B.4. Classifier-free Guidance",
                    "char_start_offset": 34445,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 136
                        },
                        {
                            "start": 137,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 586
                        },
                        {
                            "start": 587,
                            "end": 646
                        },
                        {
                            "start": 647,
                            "end": 896
                        },
                        {
                            "start": 897,
                            "end": 947
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 35,
                            "end": 39,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 334,
                            "end": 337,
                            "matchedPaperCorpusId": "253581213"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9384765625
                }
            ],
            "relevance_judgement": 0.9384765625,
            "relevance_judgment_input_expanded": "# Title: SPAD: Spatially Aware Multi-View Diffusers\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Yash Kant, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, R. A. Guler, Bernard Ghanem, S. Tulyakov, Igor Gilitschenski, Aliaksandr Siarohin\n## Abstract\nWe present SPAD, a novel approach for creating con-sistent multi-view images from text prompts or single images. To enable multi-view generation, we repurpose a pre-trained 2D diffusion model by extending its self-attention layers with cross-view interactions, and fine-tune it on a high quality subset of Objaverse. We find that a naive extension of the self-attention proposed in prior work (e.g., MV-Dream) leads to content copying between views. Therefore, we explicitly constrain the cross-view attention based on epipolar geometry. To further enhance 3D consistency, we utilize Pl\u00fccker coordinates derived from camera rays and inject them as positional encoding. This enables SPAD to reason over spatial proximity in 3D well. Compared to concurrent works that can only generate views at fixed azimuth and elevation (e.g., MVDream, SyncDreamer), SPAD offers full camera control and achieves state-of-the-art results in novel view synthesis on unseen objects from the Objaverse and Google Scanned Objects datasets. Finally, we demon-strate that text-to-3D generation using SPAD prevents the multi-face Janus issue.\n## B.4. Classifier-free Guidance\nClassifier-free diffusion guidance [33] is a technique used to balance the quality and diversity of images produced by diffusion models. This method is particularly effective in class-conditional and text-conditional image generation, enhancing both the visual quality of images and their alignment with given conditions. Inspired by [5] we explore the integration of classifier-free guidance with Epipolar Attention and Pl\u00fccker Embedding. Implementing classifierfree guidance involves simultaneous training of the diffusion model for both conditional and unconditional denoising tasks. During inference, these models' score estimates are merged. We have four different types of conditioning injected into our system: Outcome: As shown in Fig. 13, we find that classifier-free guidance beyond text conditioning does not provide additional benefits, and rather leads to over-saturated generations. This also aligns with our observations on MVDream.",
            "reference_string": "[267547881 | Kant et al. | 2024 | Citations: 38]"
        },
        {
            "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 14,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.02687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261742393",
                    "name": "Seyedmorteza Sadat"
                },
                {
                    "authorId": "2204861903",
                    "name": "Manuel Kansy"
                },
                {
                    "authorId": "1466533438",
                    "name": "Otmar Hilliges"
                },
                {
                    "authorId": "145848224",
                    "name": "Romann M. Weber"
                }
            ],
            "abstract": "Classifier-free guidance (CFG) has become the standard method for enhancing the quality of conditional diffusion models. However, employing CFG requires either training an unconditional model alongside the main diffusion model or modifying the training procedure by periodically inserting a null condition. There is also no clear extension of CFG to unconditional models. In this paper, we revisit the core principles of CFG and introduce a new method, independent condition guidance (ICG), which provides the benefits of CFG without the need for any special training procedures. Our approach streamlines the training process of conditional diffusion models and can also be applied during inference on any pre-trained conditional model. Additionally, by leveraging the time-step information encoded in all diffusion networks, we propose an extension of CFG, called time-step guidance (TSG), which can be applied to any diffusion model, including unconditional ones. Our guidance techniques are easy to implement and have the same sampling cost as CFG. Through extensive experiments, we demonstrate that ICG matches the performance of standard CFG across various conditional diffusion models. Moreover, we show that TSG improves generation quality in a manner similar to CFG, without relying on any conditional information.",
            "corpus_id": 270923987,
            "sentences": [
                {
                    "corpus_id": "270923987",
                    "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models",
                    "text": "Diffusion models have recently emerged as the main methodology behind many successful generative models [39,14,9,33,42,43].At the core of such models lies a diffusion process that gradually adds noise to the data until data points are indistinguishable from pure noise.At inference, a denoiser is used to gradually remove noise from samples until we reach a generated data point.While the theory suggests that standard sampling from diffusion models should yield high-quality images, this does not generally hold in practice, and guidance methods are often required to increase the quality of generations, albeit at the expense of less diversity [9,13,35].Classifier guidance [9] was the first method to introduce this concept by utilizing the gradient of a classifier trained on noisy images to increase the class-likelihood of generated samples.Later, classifier-free guidance (CFG) [13] was proposed, allowing the diffusion model to simulate the same behavior as classifier guidance without using an explicit classifier.Since then, CFG has been applied to other conditional generation tasks, such as text-to-image synthesis [28] and text-to-3D generation [31].\n\nIn addition to CFG's trading diversity for quality, it has two practical limitations.First, it requires the underlying model to be trained in a specific way to also learn the unconditional score function, typically by replacing the conditioning vector with a null vector with probability p (usually p \u2208 [0.1.0.2]).This additional step hinders training efficiency, as the model needs to be trained on two different tasks.Replacing the condition might also not be straightforward when the model is multimodal and uses different conditioning signals such as text, images, and audio at the same time, or when the null vector (which is usually the zero vector in practice) has a specific meaning.Second, it is not clear how to extend the benefits of classifier-free guidance beyond conditional models to unconditional generation.\n\nIn this paper, we analyze the methodology behind classifier-free guidance and show theoretically that similar behavior can be achieved without additional training of an unconditional model.The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score.",
                    "score": 0.7748687727355899,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 123
                        },
                        {
                            "start": 123,
                            "end": 269
                        },
                        {
                            "start": 269,
                            "end": 379
                        },
                        {
                            "start": 379,
                            "end": 656
                        },
                        {
                            "start": 656,
                            "end": 847
                        },
                        {
                            "start": 847,
                            "end": 1023
                        },
                        {
                            "start": 1023,
                            "end": 1163
                        },
                        {
                            "start": 1165,
                            "end": 1250
                        },
                        {
                            "start": 1250,
                            "end": 1479
                        },
                        {
                            "start": 1479,
                            "end": 1585
                        },
                        {
                            "start": 1585,
                            "end": 1856
                        },
                        {
                            "start": 1856,
                            "end": 1989
                        },
                        {
                            "start": 1991,
                            "end": 2180
                        },
                        {
                            "start": 2180,
                            "end": 2341
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 108,
                            "end": 111,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 111,
                            "end": 113,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 113,
                            "end": 116,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 116,
                            "end": 119,
                            "matchedPaperCorpusId": "196470871"
                        },
                        {
                            "start": 119,
                            "end": 122,
                            "matchedPaperCorpusId": "227209335"
                        },
                        {
                            "start": 646,
                            "end": 649,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 652,
                            "end": 655,
                            "matchedPaperCorpusId": "264490969"
                        },
                        {
                            "start": 676,
                            "end": 679,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 1127,
                            "end": 1131,
                            "matchedPaperCorpusId": "245335086"
                        },
                        {
                            "start": 1158,
                            "end": 1162,
                            "matchedPaperCorpusId": "252596091"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93798828125
                }
            ],
            "relevance_judgement": 0.93798828125,
            "relevance_judgment_input_expanded": "# Title: No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models\n# Venue: International Conference on Learning Representations\n# Authors: Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, Romann M. Weber\n## Abstract\nClassifier-free guidance (CFG) has become the standard method for enhancing the quality of conditional diffusion models. However, employing CFG requires either training an unconditional model alongside the main diffusion model or modifying the training procedure by periodically inserting a null condition. There is also no clear extension of CFG to unconditional models. In this paper, we revisit the core principles of CFG and introduce a new method, independent condition guidance (ICG), which provides the benefits of CFG without the need for any special training procedures. Our approach streamlines the training process of conditional diffusion models and can also be applied during inference on any pre-trained conditional model. Additionally, by leveraging the time-step information encoded in all diffusion networks, we propose an extension of CFG, called time-step guidance (TSG), which can be applied to any diffusion model, including unconditional ones. Our guidance techniques are easy to implement and have the same sampling cost as CFG. Through extensive experiments, we demonstrate that ICG matches the performance of standard CFG across various conditional diffusion models. Moreover, we show that TSG improves generation quality in a manner similar to CFG, without relying on any conditional information.\n## Introduction\nDiffusion models have recently emerged as the main methodology behind many successful generative models [39,14,9,33,42,43].At the core of such models lies a diffusion process that gradually adds noise to the data until data points are indistinguishable from pure noise.At inference, a denoiser is used to gradually remove noise from samples until we reach a generated data point.While the theory suggests that standard sampling from diffusion models should yield high-quality images, this does not generally hold in practice, and guidance methods are often required to increase the quality of generations, albeit at the expense of less diversity [9,13,35].Classifier guidance [9] was the first method to introduce this concept by utilizing the gradient of a classifier trained on noisy images to increase the class-likelihood of generated samples.Later, classifier-free guidance (CFG) [13] was proposed, allowing the diffusion model to simulate the same behavior as classifier guidance without using an explicit classifier.Since then, CFG has been applied to other conditional generation tasks, such as text-to-image synthesis [28] and text-to-3D generation [31].\n\nIn addition to CFG's trading diversity for quality, it has two practical limitations.First, it requires the underlying model to be trained in a specific way to also learn the unconditional score function, typically by replacing the conditioning vector with a null vector with probability p (usually p \u2208 [0.1.0.2]).This additional step hinders training efficiency, as the model needs to be trained on two different tasks.Replacing the condition might also not be straightforward when the model is multimodal and uses different conditioning signals such as text, images, and audio at the same time, or when the null vector (which is usually the zero vector in practice) has a specific meaning.Second, it is not clear how to extend the benefits of classifier-free guidance beyond conditional models to unconditional generation.\n\nIn this paper, we analyze the methodology behind classifier-free guidance and show theoretically that similar behavior can be achieved without additional training of an unconditional model.The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score.",
            "reference_string": "[270923987 | Sadat et al. | 2024 | Citations: 14]"
        },
        {
            "title": "On Mechanistic Knowledge Localization in Text-to-Image Generative Models",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 18,
            "citation_count": 15,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.01008, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2114710333",
                    "name": "Samyadeep Basu"
                },
                {
                    "authorId": "2204576892",
                    "name": "Keivan Rezaei"
                },
                {
                    "authorId": "1962835975",
                    "name": "Priyatham Kattakinda"
                },
                {
                    "authorId": "2317012495",
                    "name": "Ryan A. Rossi"
                },
                {
                    "authorId": "2299942084",
                    "name": "Cherry Zhao"
                },
                {
                    "authorId": "2061209811",
                    "name": "V. Morariu"
                },
                {
                    "authorId": "1977256",
                    "name": "Varun Manjunatha"
                },
                {
                    "authorId": "34389431",
                    "name": "S. Feizi"
                }
            ],
            "abstract": "Identifying layers within text-to-image models which control visual attributes can facilitate efficient model editing through closed-form updates. Recent work, leveraging causal tracing show that early Stable-Diffusion variants confine knowledge primarily to the first layer of the CLIP text-encoder, while it diffuses throughout the UNet.Extending this framework, we observe that for recent models (e.g., SD-XL, DeepFloyd), causal tracing fails in pinpointing localized knowledge, highlighting challenges in model editing. To address this issue, we introduce the concept of Mechanistic Localization in text-to-image models, where knowledge about various visual attributes (e.g.,\"style\",\"objects\",\"facts\") can be mechanistically localized to a small fraction of layers in the UNet, thus facilitating efficient model editing. We localize knowledge using our method LocoGen which measures the direct effect of intermediate layers to output generation by performing interventions in the cross-attention layers of the UNet. We then employ LocoEdit, a fast closed-form editing method across popular open-source text-to-image models (including the latest SD-XL)and explore the possibilities of neuron-level model editing. Using Mechanistic Localization, our work offers a better view of successes and failures in localization-based text-to-image model editing. Code will be available at https://github.com/samyadeepbasu/LocoGen.",
            "corpus_id": 269502576,
            "sentences": [
                {
                    "corpus_id": "269502576",
                    "title": "On Mechanistic Knowledge Localization in Text-to-Image Generative Models",
                    "text": "During the inference process, the regulation of image generation involves the utilization of classifier-free guidance, as outlined in Ho & Salimans (2021) which incorporates scores from both the conditional and unconditional diffusion models at each time-step.Specifically, the classifier-free guidance is applied at each time-step to combine the conditional (\u03f5 \u03b8 (z t , c, t)) and unconditional score estimates (\u03f5 \u03b8 (z t , t)).\n\nThe result is a combined score denoted as \u03b5(z t , c, t).\n\n\u03b5(z t , c, t) = \u03f5 \u03b8 (z t , c, t) + \u03b1 (\u03f5 \u03b8 (z t , c, t) \u2212 \u03f5 \u03b8 (z t , t)) , \u2200t \u2208 [T, 1] .\n\n(1) This combined score is used to update the latent z t using DDIM sampling (Song et al., 2020) at each time-step to obtain the final latent code z 0 .We term the model \u03f5 \u03b8 (z t , c, t) as the Clean Model and the final image generated as I clean .We note that text is incorporated in the process of generation using cross-attention layers denoted by {C l } M l=1 within \u03f5 \u03b8 (z t , c, t) \u2200t \u2208 [T, 1].These layers include key and value matrices -{W K l , W V l } M l=1 that take text-embedding c of the input prompt and guide the generation toward the text prompt.Generally, the text-embedding c is same across all these layers.However, in order to localize and find control points for different visual attributes, we replace the original text-embedding c with a target prompt embedding c \u2032 across a small subset of the cross-attention layers and measure its direct effect on the generated image.",
                    "score": 0.5533552017733315,
                    "section_title": "Knowledge Control in Cross-Attention Layers",
                    "char_start_offset": 8797,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 260
                        },
                        {
                            "start": 260,
                            "end": 428
                        },
                        {
                            "start": 430,
                            "end": 486
                        },
                        {
                            "start": 488,
                            "end": 575
                        },
                        {
                            "start": 577,
                            "end": 729
                        },
                        {
                            "start": 729,
                            "end": 825
                        },
                        {
                            "start": 825,
                            "end": 977
                        },
                        {
                            "start": 977,
                            "end": 1140
                        },
                        {
                            "start": 1140,
                            "end": 1204
                        },
                        {
                            "start": 1204,
                            "end": 1472
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 134,
                            "end": 154,
                            "matchedPaperCorpusId": "249145348"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93798828125
                }
            ],
            "relevance_judgement": 0.93798828125,
            "relevance_judgment_input_expanded": "# Title: On Mechanistic Knowledge Localization in Text-to-Image Generative Models\n# Venue: International Conference on Machine Learning\n# Authors: Samyadeep Basu, Keivan Rezaei, Priyatham Kattakinda, Ryan A. Rossi, Cherry Zhao, V. Morariu, Varun Manjunatha, S. Feizi\n## Abstract\nIdentifying layers within text-to-image models which control visual attributes can facilitate efficient model editing through closed-form updates. Recent work, leveraging causal tracing show that early Stable-Diffusion variants confine knowledge primarily to the first layer of the CLIP text-encoder, while it diffuses throughout the UNet.Extending this framework, we observe that for recent models (e.g., SD-XL, DeepFloyd), causal tracing fails in pinpointing localized knowledge, highlighting challenges in model editing. To address this issue, we introduce the concept of Mechanistic Localization in text-to-image models, where knowledge about various visual attributes (e.g.,\"style\",\"objects\",\"facts\") can be mechanistically localized to a small fraction of layers in the UNet, thus facilitating efficient model editing. We localize knowledge using our method LocoGen which measures the direct effect of intermediate layers to output generation by performing interventions in the cross-attention layers of the UNet. We then employ LocoEdit, a fast closed-form editing method across popular open-source text-to-image models (including the latest SD-XL)and explore the possibilities of neuron-level model editing. Using Mechanistic Localization, our work offers a better view of successes and failures in localization-based text-to-image model editing. Code will be available at https://github.com/samyadeepbasu/LocoGen.\n## Knowledge Control in Cross-Attention Layers\nDuring the inference process, the regulation of image generation involves the utilization of classifier-free guidance, as outlined in Ho & Salimans (2021) which incorporates scores from both the conditional and unconditional diffusion models at each time-step.Specifically, the classifier-free guidance is applied at each time-step to combine the conditional (\u03f5 \u03b8 (z t , c, t)) and unconditional score estimates (\u03f5 \u03b8 (z t , t)).\n\nThe result is a combined score denoted as \u03b5(z t , c, t).\n\n\u03b5(z t , c, t) = \u03f5 \u03b8 (z t , c, t) + \u03b1 (\u03f5 \u03b8 (z t , c, t) \u2212 \u03f5 \u03b8 (z t , t)) , \u2200t \u2208 [T, 1] .\n\n(1) This combined score is used to update the latent z t using DDIM sampling (Song et al., 2020) at each time-step to obtain the final latent code z 0 .We term the model \u03f5 \u03b8 (z t , c, t) as the Clean Model and the final image generated as I clean .We note that text is incorporated in the process of generation using cross-attention layers denoted by {C l } M l=1 within \u03f5 \u03b8 (z t , c, t) \u2200t \u2208 [T, 1].These layers include key and value matrices -{W K l , W V l } M l=1 that take text-embedding c of the input prompt and guide the generation toward the text prompt.Generally, the text-embedding c is same across all these layers.However, in order to localize and find control points for different visual attributes, we replace the original text-embedding c with a target prompt embedding c \u2032 across a small subset of the cross-attention layers and measure its direct effect on the generated image.",
            "reference_string": "[269502576 | Basu et al. | 2024 | Citations: 15]"
        },
        {
            "title": "VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 80,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.12327, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293439758",
                    "name": "Xianwei Zhuang"
                },
                {
                    "authorId": "2306871988",
                    "name": "Yuxin Xie"
                },
                {
                    "authorId": "2276775533",
                    "name": "Yufan Deng"
                },
                {
                    "authorId": "2307892931",
                    "name": "Liming Liang"
                },
                {
                    "authorId": "2341529875",
                    "name": "Jinghan Ru"
                },
                {
                    "authorId": "2342470685",
                    "name": "Yuguo Yin"
                },
                {
                    "authorId": "2260859476",
                    "name": "Yuexian Zou"
                }
            ],
            "abstract": "We present VARGPT, a novel multimodal large language model (MLLM) that unifies visual understanding and generation within a single autoregressive framework. VARGPT employs a next-token prediction paradigm for visual understanding and a next-scale prediction paradigm for visual autoregressive generation. VARGPT innovatively extends the LLaVA architecture, achieving efficient scale-wise autoregressive visual generation within MLLMs while seamlessly accommodating mixed-modal input and output within a single model framework. Our VARGPT undergoes a three-stage unified training process on specially curated datasets, comprising a pre-training phase and two mixed visual instruction-tuning phases. The unified training strategy are designed to achieve alignment between visual and textual features, enhance instruction following for both understanding and generation, and improve visual generation quality, respectively. Despite its LLAVA-based architecture for multimodel understanding, VARGPT significantly outperforms LLaVA-1.5 across various vision-centric benchmarks, such as visual question-answering and reasoning tasks. Notably, VARGPT naturally supports capabilities in autoregressive visual generation and instruction-to-image synthesis, showcasing its versatility in both visual understanding and generation tasks. Project page is at: \\url{https://vargpt-1.github.io/}",
            "corpus_id": 275787953,
            "sentences": [
                {
                    "corpus_id": "275787953",
                    "title": "VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model",
                    "text": "Classifier-free guidance (CFG) significantly enhances the capability of generative diffusion models to produce samples of exceptionally high fidelity. This approach integrates conditional generative models with the distribution estimation of unconditional models trained concurrently, thereby improving the overall quality of generation. Inspired by DALL-E 2 [68], VAR [84] and VAR-CLIP [100], we employ Gaussian noise features as conditional input to simulate unconditional generation. Subsequently, we derive the final distribution of image token outputs by subtracting the probability of uncon- ditional generation from the logits distribution of conditional generation. Specifically, assuming that the image features obtained through LLMs are denoted as H i , we derive the translated visual generation features through a mapping layer as H g = Projector(H i ). Subsequently, we employ the Classifier-Free Guidance (CFG) strategy to obtain the final features of the visual generation tokens, which can be expressed as: \n\nwhere H e represents randomly initialized Gaussian noise, serving as the unconditional feature, and \u03bb denotes the scale hyperparameter for the CFG strategy. \n\nThis formulation allows for the integration of both conditional and unconditional information in the generation process, potentially leading to more controlled and diverse visual outputs.",
                    "score": 0.598168971336009,
                    "section_title": "Classifier-free Guidance.",
                    "char_start_offset": 35256,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 337
                        },
                        {
                            "start": 338,
                            "end": 486
                        },
                        {
                            "start": 487,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 865
                        },
                        {
                            "start": 866,
                            "end": 1022
                        },
                        {
                            "start": 1025,
                            "end": 1181
                        },
                        {
                            "start": 1184,
                            "end": 1371
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9375
                }
            ],
            "relevance_judgement": 0.9375,
            "relevance_judgment_input_expanded": "# Title: VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model\n# Venue: arXiv.org\n# Authors: Xianwei Zhuang, Yuxin Xie, Yufan Deng, Liming Liang, Jinghan Ru, Yuguo Yin, Yuexian Zou\n## Abstract\nWe present VARGPT, a novel multimodal large language model (MLLM) that unifies visual understanding and generation within a single autoregressive framework. VARGPT employs a next-token prediction paradigm for visual understanding and a next-scale prediction paradigm for visual autoregressive generation. VARGPT innovatively extends the LLaVA architecture, achieving efficient scale-wise autoregressive visual generation within MLLMs while seamlessly accommodating mixed-modal input and output within a single model framework. Our VARGPT undergoes a three-stage unified training process on specially curated datasets, comprising a pre-training phase and two mixed visual instruction-tuning phases. The unified training strategy are designed to achieve alignment between visual and textual features, enhance instruction following for both understanding and generation, and improve visual generation quality, respectively. Despite its LLAVA-based architecture for multimodel understanding, VARGPT significantly outperforms LLaVA-1.5 across various vision-centric benchmarks, such as visual question-answering and reasoning tasks. Notably, VARGPT naturally supports capabilities in autoregressive visual generation and instruction-to-image synthesis, showcasing its versatility in both visual understanding and generation tasks. Project page is at: \\url{https://vargpt-1.github.io/}\n## Classifier-free Guidance.\nClassifier-free guidance (CFG) significantly enhances the capability of generative diffusion models to produce samples of exceptionally high fidelity. This approach integrates conditional generative models with the distribution estimation of unconditional models trained concurrently, thereby improving the overall quality of generation. Inspired by DALL-E 2 [68], VAR [84] and VAR-CLIP [100], we employ Gaussian noise features as conditional input to simulate unconditional generation. Subsequently, we derive the final distribution of image token outputs by subtracting the probability of uncon- ditional generation from the logits distribution of conditional generation. Specifically, assuming that the image features obtained through LLMs are denoted as H i , we derive the translated visual generation features through a mapping layer as H g = Projector(H i ). Subsequently, we employ the Classifier-Free Guidance (CFG) strategy to obtain the final features of the visual generation tokens, which can be expressed as: \n\nwhere H e represents randomly initialized Gaussian noise, serving as the unconditional feature, and \u03bb denotes the scale hyperparameter for the CFG strategy. \n\nThis formulation allows for the integration of both conditional and unconditional information in the generation process, potentially leading to more controlled and diverse visual outputs.",
            "reference_string": "[275787953 | Zhuang et al. | 2025 | Citations: 11]"
        },
        {
            "title": "Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning",
            "venue": "ACM Multimedia",
            "year": 2023,
            "reference_count": 46,
            "citation_count": 77,
            "influential_citation_count": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.04175",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.04175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2167971040",
                    "name": "Shengfang Zhai"
                },
                {
                    "authorId": "3431029",
                    "name": "Yinpeng Dong"
                },
                {
                    "authorId": "40238834",
                    "name": "Qingni Shen"
                },
                {
                    "authorId": "40417352",
                    "name": "Shih-Chieh Pu"
                },
                {
                    "authorId": "2181664",
                    "name": "Yuejian Fang"
                },
                {
                    "authorId": "2093561216",
                    "name": "Hang Su"
                }
            ],
            "abstract": "With the help of conditioning mechanisms, the state-of-the-art diffusion models have achieved tremendous success in guided image generation, particularly in text-to-image synthesis. To gain a better understanding of the training process and potential risks of text-to-image synthesis, we perform a systematic investigation of backdoor attack on text-to-image diffusion models and propose BadT2I, a general multimodal backdoor attack framework that tampers with image synthesis in diverse semantic levels. Specifically, we perform backdoor attacks on three levels of the vision semantics: Pixel-Backdoor, Object-Backdoor and Style-Backdoor. By utilizing a regularization loss, our methods efficiently inject backdoors into a large-scale text-to-image diffusion model while preserving its utility with benign inputs. We conduct empirical experiments on Stable Diffusion, the widely-used text-to-image diffusion model, demonstrating that the large-scale diffusion model can be easily backdoored within a few fine-tuning steps. We conduct additional experiments to explore the impact of different types of textual triggers, as well as the backdoor persistence during further training, providing insights for the development of backdoor defense methods. Besides, our investigation may contribute to the copyright protection of text-to-image models in the future. Our Code: https://github.com/sf-zhai/BadT2I.",
            "corpus_id": 258557506,
            "sentences": [
                {
                    "corpus_id": "258557506",
                    "title": "Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning",
                    "text": "Diffusion models are initially used for unconditional image synthesis [15,24,26,38] and show its ability in generating diverse, high-quality samples. In order to control the generation of diffusion models, Dhariwal et al. [10] firstly propose a conditional image synthesis method utilizing classifier guidance. Subsequent works [18,23] use CLIP, which contains multi-modal information of text and images, to achieve text-guided image synthesis. Ho and Salimans [16] propose classifier-free guidance, which incorporates the conditional mechanism into the diffusion process to achieve conditional image synthesis without external classifiers. Nichol et al. [25] firstly train a conditional diffusion model utilizing classifier-free guidance on large datasets of image-text pairs, achieving great success in text-to-image synthesis. Following that, some representative studies [2,4,20,30,32,34] of text-to-image diffusion models have been proposed, based on the conditioning mechanism. Our experiments are based on Stable Diffusion [32], which we will introduce in detail later, because of its wide applications.",
                    "score": 0.629932737247815,
                    "section_title": "RELATED WORK 2.1 Diffusion Models",
                    "char_start_offset": 7365,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 149
                        },
                        {
                            "start": 150,
                            "end": 310
                        },
                        {
                            "start": 311,
                            "end": 444
                        },
                        {
                            "start": 445,
                            "end": 640
                        },
                        {
                            "start": 641,
                            "end": 829
                        },
                        {
                            "start": 830,
                            "end": 982
                        },
                        {
                            "start": 983,
                            "end": 1109
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 70,
                            "end": 74,
                            "matchedPaperCorpusId": "219955663"
                        },
                        {
                            "start": 77,
                            "end": 80,
                            "matchedPaperCorpusId": "231979499"
                        },
                        {
                            "start": 222,
                            "end": 226,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 328,
                            "end": 332,
                            "matchedPaperCorpusId": "244909410"
                        },
                        {
                            "start": 332,
                            "end": 335,
                            "matchedPaperCorpusId": "245117331"
                        },
                        {
                            "start": 885,
                            "end": 888,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 888,
                            "end": 891,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 1029,
                            "end": 1033,
                            "matchedPaperCorpusId": "245335280"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9345703125
                }
            ],
            "relevance_judgement": 0.9345703125,
            "relevance_judgment_input_expanded": "# Title: Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning\n# Venue: ACM Multimedia\n# Authors: Shengfang Zhai, Yinpeng Dong, Qingni Shen, Shih-Chieh Pu, Yuejian Fang, Hang Su\n## Abstract\nWith the help of conditioning mechanisms, the state-of-the-art diffusion models have achieved tremendous success in guided image generation, particularly in text-to-image synthesis. To gain a better understanding of the training process and potential risks of text-to-image synthesis, we perform a systematic investigation of backdoor attack on text-to-image diffusion models and propose BadT2I, a general multimodal backdoor attack framework that tampers with image synthesis in diverse semantic levels. Specifically, we perform backdoor attacks on three levels of the vision semantics: Pixel-Backdoor, Object-Backdoor and Style-Backdoor. By utilizing a regularization loss, our methods efficiently inject backdoors into a large-scale text-to-image diffusion model while preserving its utility with benign inputs. We conduct empirical experiments on Stable Diffusion, the widely-used text-to-image diffusion model, demonstrating that the large-scale diffusion model can be easily backdoored within a few fine-tuning steps. We conduct additional experiments to explore the impact of different types of textual triggers, as well as the backdoor persistence during further training, providing insights for the development of backdoor defense methods. Besides, our investigation may contribute to the copyright protection of text-to-image models in the future. Our Code: https://github.com/sf-zhai/BadT2I.\n## RELATED WORK 2.1 Diffusion Models\nDiffusion models are initially used for unconditional image synthesis [15,24,26,38] and show its ability in generating diverse, high-quality samples. In order to control the generation of diffusion models, Dhariwal et al. [10] firstly propose a conditional image synthesis method utilizing classifier guidance. Subsequent works [18,23] use CLIP, which contains multi-modal information of text and images, to achieve text-guided image synthesis. Ho and Salimans [16] propose classifier-free guidance, which incorporates the conditional mechanism into the diffusion process to achieve conditional image synthesis without external classifiers. Nichol et al. [25] firstly train a conditional diffusion model utilizing classifier-free guidance on large datasets of image-text pairs, achieving great success in text-to-image synthesis. Following that, some representative studies [2,4,20,30,32,34] of text-to-image diffusion models have been proposed, based on the conditioning mechanism. Our experiments are based on Stable Diffusion [32], which we will introduce in detail later, because of its wide applications.",
            "reference_string": "[258557506 | Zhai et al. | 2023 | Citations: 77]"
        },
        {
            "title": "Entropy Rectifying Guidance for Diffusion and Flow Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.13987, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2329186126",
                    "name": "Tariq Berrada Ifriqi"
                },
                {
                    "authorId": "1456285042",
                    "name": "Adriana Romero-Soriano"
                },
                {
                    "authorId": "3325894",
                    "name": "M. Drozdzal"
                },
                {
                    "authorId": "2281637540",
                    "name": "Jakob Verbeek"
                },
                {
                    "authorId": "72492981",
                    "name": "Alahari Karteek"
                }
            ],
            "abstract": "Guidance techniques are commonly used in diffusion and flow models to improve image quality and consistency for conditional generative tasks such as class-conditional and text-to-image generation. In particular, classifier-free guidance (CFG) -- the most widely adopted guidance technique -- contrasts conditional and unconditional predictions to improve the generated images. This results, however, in trade-offs across quality, diversity and consistency, improving some at the expense of others. While recent work has shown that it is possible to disentangle these factors to some extent, such methods come with an overhead of requiring an additional (weaker) model, or require more forward passes per sampling step. In this paper, we propose Entropy Rectifying Guidance (ERG), a simple and effective guidance mechanism based on inference-time changes in the attention mechanism of state-of-the-art diffusion transformer architectures, which allows for simultaneous improvements over image quality, diversity and prompt consistency. ERG is more general than CFG and similar guidance techniques, as it extends to unconditional sampling. ERG results in significant improvements in various generation tasks such as text-to-image, class-conditional and unconditional image generation. We also show that ERG can be seamlessly combined with other recent guidance methods such as CADS and APG, further boosting generation performance.",
            "corpus_id": 277955619,
            "sentences": [
                {
                    "corpus_id": "277955619",
                    "title": "Entropy Rectifying Guidance for Diffusion and Flow Models",
                    "text": "Guidance techniques are commonly used in diffusion and flow models to improve image quality and consistency for conditional generative tasks such as class-conditional and text-to-image generation. In particular, classifier-free guidance (CFG) -- the most widely adopted guidance technique -- contrasts conditional and unconditional predictions to improve the generated images. This results, however, in trade-offs across quality, diversity and consistency, improving some at the expense of others. While recent work has shown that it is possible to disentangle these factors to some extent, such methods come with an overhead of requiring an additional (weaker) model, or require more forward passes per sampling step. In this paper, we propose Entropy Rectifying Guidance (ERG), a simple and effective guidance mechanism based on inference-time changes in the attention mechanism of state-of-the-art diffusion transformer architectures, which allows for simultaneous improvements over image quality, diversity and prompt consistency. ERG is more general than CFG and similar guidance techniques, as it extends to unconditional sampling. ERG results in significant improvements in various generation tasks such as text-to-image, class-conditional and unconditional image generation. We also show that ERG can be seamlessly combined with other recent guidance methods such as CADS and APG, further boosting generation performance.",
                    "score": 0.6354293699441798,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93359375
                }
            ],
            "relevance_judgement": 0.93359375,
            "relevance_judgment_input_expanded": "# Title: Entropy Rectifying Guidance for Diffusion and Flow Models\n# Venue: arXiv.org\n# Authors: Tariq Berrada Ifriqi, Adriana Romero-Soriano, M. Drozdzal, Jakob Verbeek, Alahari Karteek\n## Abstract\nGuidance techniques are commonly used in diffusion and flow models to improve image quality and consistency for conditional generative tasks such as class-conditional and text-to-image generation. In particular, classifier-free guidance (CFG) -- the most widely adopted guidance technique -- contrasts conditional and unconditional predictions to improve the generated images. This results, however, in trade-offs across quality, diversity and consistency, improving some at the expense of others. While recent work has shown that it is possible to disentangle these factors to some extent, such methods come with an overhead of requiring an additional (weaker) model, or require more forward passes per sampling step. In this paper, we propose Entropy Rectifying Guidance (ERG), a simple and effective guidance mechanism based on inference-time changes in the attention mechanism of state-of-the-art diffusion transformer architectures, which allows for simultaneous improvements over image quality, diversity and prompt consistency. ERG is more general than CFG and similar guidance techniques, as it extends to unconditional sampling. ERG results in significant improvements in various generation tasks such as text-to-image, class-conditional and unconditional image generation. We also show that ERG can be seamlessly combined with other recent guidance methods such as CADS and APG, further boosting generation performance.\n",
            "reference_string": "[277955619 | Ifriqi et al. | 2025 | Citations: 0]"
        },
        {
            "title": "RecDreamer: Consistent Text-to-3D Generation via Uniform Score Distillation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 65,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12640, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2220800052",
                    "name": "Chenxi Zheng"
                },
                {
                    "authorId": "2294183727",
                    "name": "Yihong Lin"
                },
                {
                    "authorId": "2220596660",
                    "name": "Bangzhen Liu"
                },
                {
                    "authorId": "2281155649",
                    "name": "Xuemiao Xu"
                },
                {
                    "authorId": "2273558537",
                    "name": "Yongwei Nie"
                },
                {
                    "authorId": "2257314718",
                    "name": "Shengfeng He"
                }
            ],
            "abstract": "Current text-to-3D generation methods based on score distillation often suffer from geometric inconsistencies, leading to repeated patterns across different poses of 3D assets. This issue, known as the Multi-Face Janus problem, arises because existing methods struggle to maintain consistency across varying poses and are biased toward a canonical pose. While recent work has improved pose control and approximation, these efforts are still limited by this inherent bias, which skews the guidance during generation. To address this, we propose a solution called RecDreamer, which reshapes the underlying data distribution to achieve a more consistent pose representation. The core idea behind our method is to rectify the prior distribution, ensuring that pose variation is uniformly distributed rather than biased toward a canonical form. By modifying the prescribed distribution through an auxiliary function, we can reconstruct the density of the distribution to ensure compliance with specific marginal constraints. In particular, we ensure that the marginal distribution of poses follows a uniform distribution, thereby eliminating the biases introduced by the prior knowledge. We incorporate this rectified data distribution into existing score distillation algorithms, a process we refer to as uniform score distillation. To efficiently compute the posterior distribution required for the auxiliary function, RecDreamer introduces a training-free classifier that estimates pose categories in a plug-and-play manner. Additionally, we utilize various approximation techniques for noisy states, significantly improving system performance. Our experimental results demonstrate that RecDreamer effectively mitigates the Multi-Face Janus problem, leading to more consistent 3D asset generation across different poses.",
            "corpus_id": 276422090,
            "sentences": [
                {
                    "corpus_id": "276422090",
                    "title": "RecDreamer: Consistent Text-to-3D Generation via Uniform Score Distillation",
                    "text": "A major challenge in text-to-image generation using diffusion models (Yu et al., 2024;Liu et al., 2024;Xu et al., 2024) is guiding the generative process to reflect the input text accurately. A widely adopted solution is classifier-free guidance (CFG, Ho & Salimans (2022)), which eliminates the need for external classifiers by training a unified model for both unconditional and conditional image generation. During inference, CFG achieves conditional generation by interpolating between the conditional and unconditional scores, effectively guiding the model to match the input text. This method has shown significant success in various text-to-image tasks (Balaji et al., 2022;Nichol et al., 2021;Ramesh et al., 2022). Models like DALL\u2022E 2 and Stable Diffusion (Rombach et al., 2022) have demonstrated exceptional capabilities in producing diverse and complex images, with promising extensions into text-to-3D generation. \n\nA.2 TEXT-TO-3D GENERATION \n\nRecent advances in text-to-3D generation can be broadly divided into two main approaches. The first approach focuses on directly learning 3D asset distributions from large-scale datasets such as Objaverse (Deitke et al., 2023). Notable models within this category include GET3D (Gao et al., 2022), Point-E (Nichol et al., 2022), Shap-E (Jun & Nichol, 2023), CLAY (Zhang et al., 2024c), and MeshGPT (Siddiqui et al., 2024), all of which leverage extensive 3D data to generate accurate 3D models. \n\nThe second approach relies on 2D priors (Jiang et al., 2023) for generating 3D models. Techniques like score distillation are foundational here, as exemplified by DreamFusion/SJC (Poole et al., 2022;Wang et al., 2023a) and ProlificDreamer (Wang et al., 2024b). \n\nBuilding on these baselines, researchers continue to improve visual quality.",
                    "score": 0.5739854460958809,
                    "section_title": "A.1 DIFFUSION MODELS FOR TEXT-TO-IMAGE GENERATION",
                    "char_start_offset": 23503,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 191
                        },
                        {
                            "start": 192,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 586
                        },
                        {
                            "start": 587,
                            "end": 722
                        },
                        {
                            "start": 723,
                            "end": 925
                        },
                        {
                            "start": 928,
                            "end": 953
                        },
                        {
                            "start": 956,
                            "end": 1045
                        },
                        {
                            "start": 1046,
                            "end": 1183
                        },
                        {
                            "start": 1184,
                            "end": 1450
                        },
                        {
                            "start": 1453,
                            "end": 1539
                        },
                        {
                            "start": 1540,
                            "end": 1713
                        },
                        {
                            "start": 1716,
                            "end": 1792
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 69,
                            "end": 86,
                            "matchedPaperCorpusId": "272722848"
                        },
                        {
                            "start": 86,
                            "end": 103,
                            "matchedPaperCorpusId": "268819764"
                        },
                        {
                            "start": 103,
                            "end": 119,
                            "matchedPaperCorpusId": "269625321"
                        },
                        {
                            "start": 1161,
                            "end": 1182,
                            "matchedPaperCorpusId": "254685588"
                        },
                        {
                            "start": 1234,
                            "end": 1252,
                            "matchedPaperCorpusId": "252438648"
                        },
                        {
                            "start": 1319,
                            "end": 1340,
                            "matchedPaperCorpusId": "270619933"
                        },
                        {
                            "start": 1354,
                            "end": 1377,
                            "matchedPaperCorpusId": "265457242"
                        },
                        {
                            "start": 1493,
                            "end": 1513,
                            "matchedPaperCorpusId": "265020797"
                        },
                        {
                            "start": 1652,
                            "end": 1671,
                            "matchedPaperCorpusId": "254125253"
                        },
                        {
                            "start": 1692,
                            "end": 1712,
                            "matchedPaperCorpusId": "258887357"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93359375
                }
            ],
            "relevance_judgement": 0.93359375,
            "relevance_judgment_input_expanded": "# Title: RecDreamer: Consistent Text-to-3D Generation via Uniform Score Distillation\n# Venue: arXiv.org\n# Authors: Chenxi Zheng, Yihong Lin, Bangzhen Liu, Xuemiao Xu, Yongwei Nie, Shengfeng He\n## Abstract\nCurrent text-to-3D generation methods based on score distillation often suffer from geometric inconsistencies, leading to repeated patterns across different poses of 3D assets. This issue, known as the Multi-Face Janus problem, arises because existing methods struggle to maintain consistency across varying poses and are biased toward a canonical pose. While recent work has improved pose control and approximation, these efforts are still limited by this inherent bias, which skews the guidance during generation. To address this, we propose a solution called RecDreamer, which reshapes the underlying data distribution to achieve a more consistent pose representation. The core idea behind our method is to rectify the prior distribution, ensuring that pose variation is uniformly distributed rather than biased toward a canonical form. By modifying the prescribed distribution through an auxiliary function, we can reconstruct the density of the distribution to ensure compliance with specific marginal constraints. In particular, we ensure that the marginal distribution of poses follows a uniform distribution, thereby eliminating the biases introduced by the prior knowledge. We incorporate this rectified data distribution into existing score distillation algorithms, a process we refer to as uniform score distillation. To efficiently compute the posterior distribution required for the auxiliary function, RecDreamer introduces a training-free classifier that estimates pose categories in a plug-and-play manner. Additionally, we utilize various approximation techniques for noisy states, significantly improving system performance. Our experimental results demonstrate that RecDreamer effectively mitigates the Multi-Face Janus problem, leading to more consistent 3D asset generation across different poses.\n## A.1 DIFFUSION MODELS FOR TEXT-TO-IMAGE GENERATION\nA major challenge in text-to-image generation using diffusion models (Yu et al., 2024;Liu et al., 2024;Xu et al., 2024) is guiding the generative process to reflect the input text accurately. A widely adopted solution is classifier-free guidance (CFG, Ho & Salimans (2022)), which eliminates the need for external classifiers by training a unified model for both unconditional and conditional image generation. During inference, CFG achieves conditional generation by interpolating between the conditional and unconditional scores, effectively guiding the model to match the input text. This method has shown significant success in various text-to-image tasks (Balaji et al., 2022;Nichol et al., 2021;Ramesh et al., 2022). Models like DALL\u2022E 2 and Stable Diffusion (Rombach et al., 2022) have demonstrated exceptional capabilities in producing diverse and complex images, with promising extensions into text-to-3D generation. \n\nA.2 TEXT-TO-3D GENERATION \n\nRecent advances in text-to-3D generation can be broadly divided into two main approaches. The first approach focuses on directly learning 3D asset distributions from large-scale datasets such as Objaverse (Deitke et al., 2023). Notable models within this category include GET3D (Gao et al., 2022), Point-E (Nichol et al., 2022), Shap-E (Jun & Nichol, 2023), CLAY (Zhang et al., 2024c), and MeshGPT (Siddiqui et al., 2024), all of which leverage extensive 3D data to generate accurate 3D models. \n\nThe second approach relies on 2D priors (Jiang et al., 2023) for generating 3D models. Techniques like score distillation are foundational here, as exemplified by DreamFusion/SJC (Poole et al., 2022;Wang et al., 2023a) and ProlificDreamer (Wang et al., 2024b). \n\nBuilding on these baselines, researchers continue to improve visual quality.",
            "reference_string": "[276422090 | Zheng et al. | 2025 | Citations: 3]"
        },
        {
            "title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 90,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.06861, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2354333652",
                    "name": "Diljeet Jagpal"
                },
                {
                    "authorId": "2355422362",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "145460361",
                    "name": "Vinay P. Namboodiri"
                }
            ],
            "abstract": "Zero-shot, training-free, image-based text-to-video generation is an emerging area that aims to generate videos using existing image-based diffusion models. Current methods in this space require specific architectural changes to image generation models, which limit their adaptability and scalability. In contrast to such methods, we provide a model-agnostic approach. We use intersections in diffusion trajectories, working only with the latent values. We could not obtain localized frame-wise coherence and diversity using only the intersection of trajectories. Thus, we instead use a grid-based approach. An in-context trained LLM is used to generate coherent frame-wise prompts; another is used to identify differences between frames. Based on these, we obtain a CLIP-based attention mask that controls the timing of switching the prompts for each grid cell. Earlier switching results in higher variance, while later switching results in more coherence. Therefore, our approach can ensure appropriate control between coherence and variance for the frames. Our approach results in state-of-the-art performance while being more flexible when working with diverse image-generation models. The empirical analysis using quantitative metrics and user studies confirms our model's superior temporal consistency, visual fidelity and user satisfaction, thus providing a novel way to obtain training-free, image-based text-to-video generation.",
            "corpus_id": 277633776,
            "sentences": [
                {
                    "corpus_id": "277633776",
                    "title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation",
                    "text": "Classifier-free guidance [16] enables conditioning diffusion models on text without relying on an external classifier. \n\nDuring training, the model alternates between a specific condition y (e.g., text) and a null condition \u2205, learning both the conditional s \u03b8 (x t , t | y) and unconditional s \u03b8 (x t , t | \u2205) score functions. The final conditional score function at sampling is given by: \n\nwhere \u03b3 controls the influence of the text condition. Adjusting the guidance scale, \u03b3, allows the diffusion process to produce images that align closely with the text prompt.",
                    "score": 0.6125524835617523,
                    "section_title": "Classifier-Free Guidance",
                    "char_start_offset": 6918,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 121,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 389
                        },
                        {
                            "start": 392,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 566
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 25,
                            "end": 29,
                            "matchedPaperCorpusId": "249145348"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9326171875
                }
            ],
            "relevance_judgement": 0.9326171875,
            "relevance_judgment_input_expanded": "# Title: EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation\n# Venue: arXiv.org\n# Authors: Diljeet Jagpal, Xi Chen, Vinay P. Namboodiri\n## Abstract\nZero-shot, training-free, image-based text-to-video generation is an emerging area that aims to generate videos using existing image-based diffusion models. Current methods in this space require specific architectural changes to image generation models, which limit their adaptability and scalability. In contrast to such methods, we provide a model-agnostic approach. We use intersections in diffusion trajectories, working only with the latent values. We could not obtain localized frame-wise coherence and diversity using only the intersection of trajectories. Thus, we instead use a grid-based approach. An in-context trained LLM is used to generate coherent frame-wise prompts; another is used to identify differences between frames. Based on these, we obtain a CLIP-based attention mask that controls the timing of switching the prompts for each grid cell. Earlier switching results in higher variance, while later switching results in more coherence. Therefore, our approach can ensure appropriate control between coherence and variance for the frames. Our approach results in state-of-the-art performance while being more flexible when working with diverse image-generation models. The empirical analysis using quantitative metrics and user studies confirms our model's superior temporal consistency, visual fidelity and user satisfaction, thus providing a novel way to obtain training-free, image-based text-to-video generation.\n## Classifier-Free Guidance\nClassifier-free guidance [16] enables conditioning diffusion models on text without relying on an external classifier. \n\nDuring training, the model alternates between a specific condition y (e.g., text) and a null condition \u2205, learning both the conditional s \u03b8 (x t , t | y) and unconditional s \u03b8 (x t , t | \u2205) score functions. The final conditional score function at sampling is given by: \n\nwhere \u03b3 controls the influence of the text condition. Adjusting the guidance scale, \u03b3, allows the diffusion process to produce images that align closely with the text prompt.",
            "reference_string": "[277633776 | Jagpal et al. | 2025 | Citations: 0]"
        },
        {
            "title": "IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 51,
            "citation_count": 807,
            "influential_citation_count": 185,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.06721",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.06721, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2145877255",
                    "name": "Hu Ye"
                },
                {
                    "authorId": "2157209270",
                    "name": "Jun Zhang"
                },
                {
                    "authorId": "150301258",
                    "name": "Siyi Liu"
                },
                {
                    "authorId": null,
                    "name": "Xiao Han"
                },
                {
                    "authorId": "2150081263",
                    "name": "Wei Yang"
                }
            ],
            "abstract": "Recent years have witnessed the strong power of large text-to-image diffusion models for the impressive generative capability to create high-fidelity images. However, it is very tricky to generate desired images using only text prompt as it often involves complex prompt engineering. An alternative to text prompt is image prompt, as the saying goes:\"an image is worth a thousand words\". Although existing methods of direct fine-tuning from pretrained models are effective, they require large computing resources and are not compatible with other base models, text prompt, and structural controls. In this paper, we present IP-Adapter, an effective and lightweight adapter to achieve image prompt capability for the pretrained text-to-image diffusion models. The key design of our IP-Adapter is decoupled cross-attention mechanism that separates cross-attention layers for text features and image features. Despite the simplicity of our method, an IP-Adapter with only 22M parameters can achieve comparable or even better performance to a fully fine-tuned image prompt model. As we freeze the pretrained diffusion model, the proposed IP-Adapter can be generalized not only to other custom models fine-tuned from the same base model, but also to controllable generation using existing controllable tools. With the benefit of the decoupled cross-attention strategy, the image prompt can also work well with the text prompt to achieve multimodal image generation. The project page is available at \\url{https://ip-adapter.github.io}.",
            "corpus_id": 260886966,
            "sentences": [
                {
                    "corpus_id": "260886966",
                    "title": "IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models",
                    "text": "Diffusion models are a class of generative models that comprise two processes: a diffusion process (also known as the forward process), which gradually adds Gaussian noise to the data using a fixed Markov chain of T steps, and a denoising process that generates samples from Gaussian noise with a learnable model. Diffusion models can also be conditioned on other inputs, such as text in the case of text-to-image diffusion models. Typically, the training objective of a diffusion model, denoted as \u03f5 \u03b8 , which predicts noise, is defined as a simplified variant of the variational bound: \n\nwhere x 0 represents the real data with an additional condition c, t \u2208 [0, T ] denotes the time step of diffusion process, x t = \u03b1 t x 0 + \u03c3 t \u03f5 is the noisy data at t step, and \u03b1 t , \u03c3 t are predefined functions of t that determine the diffusion process. \n\nOnce the model \u03f5 \u03b8 is trained, images can be generated from random noise in an iterative manner. Generally, fast samplers such as DDIM [21], PNDM [36] and DPM-Solver [37,38], are adopted in the inference stage to accelerate the generation process. \n\nFor the conditional diffusion models, classifier guidance [23] is a straightforward technique used to balance image fidelity and sample diversity by utilizing gradients from a separately trained classifier. To eliminate the need for training a classifier independently, classifier-free guidance [39] is often employed as an alternative method. In this approach, the conditional and unconditional diffusion models are jointly trained by randomly dropping c during training. In the sampling stage, the predicted noise is calculated based on the prediction of both the conditional model \u03f5 \u03b8 (x t , c, t) and unconditional model \u03f5 \u03b8 (x t , t): \n\nhere, w, often named guidance scale or guidance weight, is a scalar value that adjusts the alignment with condition c. \n\nFor text-to-image diffusion models, classifier-free guidance plays a crucial role in enhancing the image-text alignment of generated samples. \n\nIn our study, we utilize the open-source SD model as our example base model to implement the IP-Adapter.",
                    "score": 0.5822503013704129,
                    "section_title": "Prelimiaries",
                    "char_start_offset": 11963,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 313
                        },
                        {
                            "start": 314,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 587
                        },
                        {
                            "start": 590,
                            "end": 845
                        },
                        {
                            "start": 848,
                            "end": 944
                        },
                        {
                            "start": 945,
                            "end": 1095
                        },
                        {
                            "start": 1098,
                            "end": 1304
                        },
                        {
                            "start": 1305,
                            "end": 1441
                        },
                        {
                            "start": 1442,
                            "end": 1570
                        },
                        {
                            "start": 1571,
                            "end": 1737
                        },
                        {
                            "start": 1740,
                            "end": 1858
                        },
                        {
                            "start": 1861,
                            "end": 2002
                        },
                        {
                            "start": 2005,
                            "end": 2109
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1014,
                            "end": 1018,
                            "matchedPaperCorpusId": "249282317"
                        },
                        {
                            "start": 1156,
                            "end": 1160,
                            "matchedPaperCorpusId": "234357997"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93212890625
                }
            ],
            "relevance_judgement": 0.93212890625,
            "relevance_judgment_input_expanded": "# Title: IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models\n# Venue: arXiv.org\n# Authors: Hu Ye, Jun Zhang, Siyi Liu, Xiao Han, Wei Yang\n## Abstract\nRecent years have witnessed the strong power of large text-to-image diffusion models for the impressive generative capability to create high-fidelity images. However, it is very tricky to generate desired images using only text prompt as it often involves complex prompt engineering. An alternative to text prompt is image prompt, as the saying goes:\"an image is worth a thousand words\". Although existing methods of direct fine-tuning from pretrained models are effective, they require large computing resources and are not compatible with other base models, text prompt, and structural controls. In this paper, we present IP-Adapter, an effective and lightweight adapter to achieve image prompt capability for the pretrained text-to-image diffusion models. The key design of our IP-Adapter is decoupled cross-attention mechanism that separates cross-attention layers for text features and image features. Despite the simplicity of our method, an IP-Adapter with only 22M parameters can achieve comparable or even better performance to a fully fine-tuned image prompt model. As we freeze the pretrained diffusion model, the proposed IP-Adapter can be generalized not only to other custom models fine-tuned from the same base model, but also to controllable generation using existing controllable tools. With the benefit of the decoupled cross-attention strategy, the image prompt can also work well with the text prompt to achieve multimodal image generation. The project page is available at \\url{https://ip-adapter.github.io}.\n## Prelimiaries\nDiffusion models are a class of generative models that comprise two processes: a diffusion process (also known as the forward process), which gradually adds Gaussian noise to the data using a fixed Markov chain of T steps, and a denoising process that generates samples from Gaussian noise with a learnable model. Diffusion models can also be conditioned on other inputs, such as text in the case of text-to-image diffusion models. Typically, the training objective of a diffusion model, denoted as \u03f5 \u03b8 , which predicts noise, is defined as a simplified variant of the variational bound: \n\nwhere x 0 represents the real data with an additional condition c, t \u2208 [0, T ] denotes the time step of diffusion process, x t = \u03b1 t x 0 + \u03c3 t \u03f5 is the noisy data at t step, and \u03b1 t , \u03c3 t are predefined functions of t that determine the diffusion process. \n\nOnce the model \u03f5 \u03b8 is trained, images can be generated from random noise in an iterative manner. Generally, fast samplers such as DDIM [21], PNDM [36] and DPM-Solver [37,38], are adopted in the inference stage to accelerate the generation process. \n\nFor the conditional diffusion models, classifier guidance [23] is a straightforward technique used to balance image fidelity and sample diversity by utilizing gradients from a separately trained classifier. To eliminate the need for training a classifier independently, classifier-free guidance [39] is often employed as an alternative method. In this approach, the conditional and unconditional diffusion models are jointly trained by randomly dropping c during training. In the sampling stage, the predicted noise is calculated based on the prediction of both the conditional model \u03f5 \u03b8 (x t , c, t) and unconditional model \u03f5 \u03b8 (x t , t): \n\nhere, w, often named guidance scale or guidance weight, is a scalar value that adjusts the alignment with condition c. \n\nFor text-to-image diffusion models, classifier-free guidance plays a crucial role in enhancing the image-text alignment of generated samples. \n\nIn our study, we utilize the open-source SD model as our example base model to implement the IP-Adapter.",
            "reference_string": "[260886966 | Ye et al. | 2023 | Citations: 807]"
        },
        {
            "title": "Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 85,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.13389, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2335870109",
                    "name": "Massimiliano Viola"
                },
                {
                    "authorId": "2335870504",
                    "name": "Kevin Qu"
                },
                {
                    "authorId": "2031912818",
                    "name": "Nando Metzger"
                },
                {
                    "authorId": "34926212",
                    "name": "Bingxin Ke"
                },
                {
                    "authorId": "2078701909",
                    "name": "Alexander Becker"
                },
                {
                    "authorId": "2243003715",
                    "name": "Konrad Schindler"
                },
                {
                    "authorId": "4366091",
                    "name": "Anton Obukhov"
                }
            ],
            "abstract": "Depth completion upgrades sparse depth measurements into dense depth maps guided by a conventional image. Existing methods for this highly ill-posed task operate in tightly constrained settings and tend to struggle when applied to images outside the training domain or when the available depth measurements are sparse, irregularly distributed, or of varying density. Inspired by recent advances in monocular depth estimation, we reframe depth completion as an image-conditional depth map generation guided by sparse measurements. Our method, Marigold-DC, builds on a pretrained latent diffusion model for monocular depth estimation and injects the depth observations as test-time guidance via an optimization scheme that runs in tandem with the iterative inference of denoising diffusion. The method exhibits excellent zero-shot generalization across a diverse range of environments and handles even extremely sparse guidance effectively. Our results suggest that contemporary monocular depth priors greatly robustify depth completion: it may be better to view the task as recovering dense depth from (dense) image pixels, guided by sparse depth; rather than as inpainting (sparse) depth, guided by an image. Project website: https://MarigoldDepthCompletion.github.io/",
            "corpus_id": 274823034,
            "sentences": [
                {
                    "corpus_id": "274823034",
                    "title": "Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion",
                    "text": "To allow fine-grained control over the output, guidancebased diffusion [13] incorporates external supervision alongside the original conditioning, using a guidance function that measures whether certain criteria are met. In guided image generation, classifier guidance [14] enables class-conditional outputs from a pretrained, unconditional diffusion model, via gradients from a classifier trained on ImageNet [58] im-ages at different noise scales. Similarly, gradients from a CLIP model [51] trained on noisy images can guide generation toward a user-defined text caption [47]. An alternative, classifier-free guidance [26,47], achieves similar control without training a separate classifier, by parameterizing both conditional and unconditional diffusion models within the same network. The approach is further extended to handle general nonlinear inverse problems [9], using gradients calculated on the expected denoised images. Guidance is commonly framed from a score-based perspective on denoising diffusion [66,67], where an unconditional model approximates the time-dependent score function of the log data distribution. Finally, a variety of universal constraints, such as segmentation masks, image style, and object location, have been applied with SD under a single framework [1], fully exploiting the flexibility and control of diffusion-based image generation.",
                    "score": 0.6874599976078084,
                    "section_title": "Diffusion Guidance",
                    "char_start_offset": 8163,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 220
                        },
                        {
                            "start": 221,
                            "end": 449
                        },
                        {
                            "start": 450,
                            "end": 579
                        },
                        {
                            "start": 580,
                            "end": 789
                        },
                        {
                            "start": 790,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 1129
                        },
                        {
                            "start": 1130,
                            "end": 1374
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 410,
                            "end": 414,
                            "matchedPaperCorpusId": "2930547"
                        },
                        {
                            "start": 621,
                            "end": 625,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 868,
                            "end": 871,
                            "matchedPaperCorpusId": "252596252"
                        },
                        {
                            "start": 1019,
                            "end": 1022,
                            "matchedPaperCorpusId": "227209335"
                        },
                        {
                            "start": 1288,
                            "end": 1291,
                            "matchedPaperCorpusId": "256846836"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93115234375
                }
            ],
            "relevance_judgement": 0.93115234375,
            "relevance_judgment_input_expanded": "# Title: Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion\n# Venue: arXiv.org\n# Authors: Massimiliano Viola, Kevin Qu, Nando Metzger, Bingxin Ke, Alexander Becker, Konrad Schindler, Anton Obukhov\n## Abstract\nDepth completion upgrades sparse depth measurements into dense depth maps guided by a conventional image. Existing methods for this highly ill-posed task operate in tightly constrained settings and tend to struggle when applied to images outside the training domain or when the available depth measurements are sparse, irregularly distributed, or of varying density. Inspired by recent advances in monocular depth estimation, we reframe depth completion as an image-conditional depth map generation guided by sparse measurements. Our method, Marigold-DC, builds on a pretrained latent diffusion model for monocular depth estimation and injects the depth observations as test-time guidance via an optimization scheme that runs in tandem with the iterative inference of denoising diffusion. The method exhibits excellent zero-shot generalization across a diverse range of environments and handles even extremely sparse guidance effectively. Our results suggest that contemporary monocular depth priors greatly robustify depth completion: it may be better to view the task as recovering dense depth from (dense) image pixels, guided by sparse depth; rather than as inpainting (sparse) depth, guided by an image. Project website: https://MarigoldDepthCompletion.github.io/\n## Diffusion Guidance\nTo allow fine-grained control over the output, guidancebased diffusion [13] incorporates external supervision alongside the original conditioning, using a guidance function that measures whether certain criteria are met. In guided image generation, classifier guidance [14] enables class-conditional outputs from a pretrained, unconditional diffusion model, via gradients from a classifier trained on ImageNet [58] im-ages at different noise scales. Similarly, gradients from a CLIP model [51] trained on noisy images can guide generation toward a user-defined text caption [47]. An alternative, classifier-free guidance [26,47], achieves similar control without training a separate classifier, by parameterizing both conditional and unconditional diffusion models within the same network. The approach is further extended to handle general nonlinear inverse problems [9], using gradients calculated on the expected denoised images. Guidance is commonly framed from a score-based perspective on denoising diffusion [66,67], where an unconditional model approximates the time-dependent score function of the log data distribution. Finally, a variety of universal constraints, such as segmentation masks, image style, and object location, have been applied with SD under a single framework [1], fully exploiting the flexibility and control of diffusion-based image generation.",
            "reference_string": "[274823034 | Viola et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Active Generation for Image Classification",
            "venue": "European Conference on Computer Vision",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.06517, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2265957484",
                    "name": "Tao Huang"
                },
                {
                    "authorId": "2290839014",
                    "name": "Jiaqi Liu"
                },
                {
                    "authorId": "2111867716",
                    "name": "Shan You"
                },
                {
                    "authorId": "2155590441",
                    "name": "Chang Xu"
                }
            ],
            "abstract": "Recently, the growing capabilities of deep generative models have underscored their potential in enhancing image classification accuracy. However, existing methods often demand the generation of a disproportionately large number of images compared to the original dataset, while having only marginal improvements in accuracy. This computationally expensive and time-consuming process hampers the practicality of such approaches. In this paper, we propose to address the efficiency of image generation by focusing on the specific needs and characteristics of the model. With a central tenet of active learning, our method, named ActGen, takes a training-aware approach to image generation. It aims to create images akin to the challenging or misclassified samples encountered by the current model and incorporates these generated images into the training set to augment model performance. ActGen introduces an attentive image guidance technique, using real images as guides during the denoising process of a diffusion model. The model's attention on class prompt is leveraged to ensure the preservation of similar foreground object while diversifying the background. Furthermore, we introduce a gradient-based generation guidance method, which employs two losses to generate more challenging samples and prevent the generated images from being too similar to previously generated ones. Experimental results on the CIFAR and ImageNet datasets demonstrate that our method achieves better performance with a significantly reduced number of generated images. Code is available at https://github.com/hunto/ActGen.",
            "corpus_id": 268351323,
            "sentences": [
                {
                    "corpus_id": "268351323",
                    "title": "Active Generation for Image Classification",
                    "text": "Text-to-image diffusion models are generative models designed to create realistic images from textual descriptions.These models employ diffusion processes to iteratively generate images based on the semantic information provided in the  input text.The objective is to capture the essence of the textual description and translate it into visually coherent images.\n\nText-to-image diffusion models incorporate additional text conditional variables c in the noise prediction model to predict the conditional noise \u03f5 \u03b8 (x t , c, t) and guide the generation.A classifier-free guidance technique [22] is typically adopted, enabling the utilization of text-conditioned guidance during training without the need for a classifier.The denoising model is trained to handle both conditioned input, where a text prompt is provided, and unconditioned input, where the prompt is replaced with \u2205.This allows for the representation of the guidance direction as the translation from the conditioned input \u03f5 \u03b8 (x t , c, t) to the unconditioned input \u03f5 \u03b8 (x t , \u2205, t).The guidance is performed with a guidance scale s by\n\nIn this paper, we directly use the public text-to-image diffusion models [28,35] trained on large-scale text-image pairs as our generative models, with our innovative guidance methods introduced in inference process.",
                    "score": 0.5926716682772457,
                    "section_title": "Text-conditioned guidance",
                    "char_start_offset": 10717,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 115
                        },
                        {
                            "start": 115,
                            "end": 248
                        },
                        {
                            "start": 248,
                            "end": 362
                        },
                        {
                            "start": 364,
                            "end": 552
                        },
                        {
                            "start": 552,
                            "end": 720
                        },
                        {
                            "start": 720,
                            "end": 879
                        },
                        {
                            "start": 879,
                            "end": 1047
                        },
                        {
                            "start": 1047,
                            "end": 1099
                        },
                        {
                            "start": 1101,
                            "end": 1317
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 589,
                            "end": 593,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1178,
                            "end": 1181,
                            "matchedPaperCorpusId": "245335280"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93115234375
                }
            ],
            "relevance_judgement": 0.93115234375,
            "relevance_judgment_input_expanded": "# Title: Active Generation for Image Classification\n# Venue: European Conference on Computer Vision\n# Authors: Tao Huang, Jiaqi Liu, Shan You, Chang Xu\n## Abstract\nRecently, the growing capabilities of deep generative models have underscored their potential in enhancing image classification accuracy. However, existing methods often demand the generation of a disproportionately large number of images compared to the original dataset, while having only marginal improvements in accuracy. This computationally expensive and time-consuming process hampers the practicality of such approaches. In this paper, we propose to address the efficiency of image generation by focusing on the specific needs and characteristics of the model. With a central tenet of active learning, our method, named ActGen, takes a training-aware approach to image generation. It aims to create images akin to the challenging or misclassified samples encountered by the current model and incorporates these generated images into the training set to augment model performance. ActGen introduces an attentive image guidance technique, using real images as guides during the denoising process of a diffusion model. The model's attention on class prompt is leveraged to ensure the preservation of similar foreground object while diversifying the background. Furthermore, we introduce a gradient-based generation guidance method, which employs two losses to generate more challenging samples and prevent the generated images from being too similar to previously generated ones. Experimental results on the CIFAR and ImageNet datasets demonstrate that our method achieves better performance with a significantly reduced number of generated images. Code is available at https://github.com/hunto/ActGen.\n## Text-conditioned guidance\nText-to-image diffusion models are generative models designed to create realistic images from textual descriptions.These models employ diffusion processes to iteratively generate images based on the semantic information provided in the  input text.The objective is to capture the essence of the textual description and translate it into visually coherent images.\n\nText-to-image diffusion models incorporate additional text conditional variables c in the noise prediction model to predict the conditional noise \u03f5 \u03b8 (x t , c, t) and guide the generation.A classifier-free guidance technique [22] is typically adopted, enabling the utilization of text-conditioned guidance during training without the need for a classifier.The denoising model is trained to handle both conditioned input, where a text prompt is provided, and unconditioned input, where the prompt is replaced with \u2205.This allows for the representation of the guidance direction as the translation from the conditioned input \u03f5 \u03b8 (x t , c, t) to the unconditioned input \u03f5 \u03b8 (x t , \u2205, t).The guidance is performed with a guidance scale s by\n\nIn this paper, we directly use the public text-to-image diffusion models [28,35] trained on large-scale text-image pairs as our generative models, with our innovative guidance methods introduced in inference process.",
            "reference_string": "[268351323 | Huang et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the World's Ugliness?",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 16,
            "citation_count": 14,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.18398",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.18398, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2166299958",
                    "name": "Manuel Brack"
                },
                {
                    "authorId": "2055616945",
                    "name": "Felix Friedrich"
                },
                {
                    "authorId": "40896023",
                    "name": "P. Schramowski"
                },
                {
                    "authorId": "2066493115",
                    "name": "K. Kersting"
                }
            ],
            "abstract": "Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also reproduce inappropriate human behavior. Specifically, we demonstrate inappropriate degeneration on a large-scale for various generative text-to-image models, thus motivating the need for monitoring and moderating them at deployment. To this end, we evaluate mitigation strategies at inference to suppress the generation of inappropriate content. Our findings show that we can use models' representations of the world's ugliness to align them with human preferences.",
            "corpus_id": 258967543,
            "sentences": [
                {
                    "corpus_id": "258967543",
                    "title": "Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the World's Ugliness?",
                    "text": "Classifier Free Guidance. Before going into detail on different instruction methods for image generation, we need to establish some fundamentals of text-to-image diffusion models (DMs). Intuitively, image generation starts from random noise \u03f5, and the model predicts an estimate of this noise \u03b5\u03b8 to be subtracted from the initial values. This results in a high-fidelity image x without any noise. Since this is a complex problem, multiple steps are applied, each subtracting a small amount (\u03f5 t ) of the predictive noise, approximating \u03f5. For text-to-image generation, the model's \u03f5-prediction is conditioned on a text prompt p and results in an image faithful to that prompt. To that end, DMs employ classifier-free guidance (Ho & Salimans, 2022), a conditioning method using a purely generational diffusion model, eliminating the need for an additional pre-trained classifier. \n\nThe noise estimate \u03b5\u03b8 uses an unconditioned \u03f5-prediction \u03f5 \u03b8 (z t ) which is pushed in the direction of the conditioned estimate \u03f5 \u03b8 (z t , c p ) to yield an image faithful to prompt p. \n\nInstructing Text-to-Image Models on Safety. We now consider two different instruction approaches extending the principles of classifier-free guidance. Both methods rely on a secondary text prompt s that describes concepts to suppress during generation. First, negative prompting replaces the unconditioned \u03f5-prediction \u03f5 \u03b8 (z t ) with one conditioned on s: \u03f5 \u03b8 (z t , c s ), thus moving away from the inappropriate concepts. This approach is intuitive and easy to implement, however offers limited control over the extent of content suppression. Additionally, we use Semantic Guidance (SEGA) (Brack et al., 2023) which is a powerful method for image manipulation based on additional text prompts. SEGA adds an additional guidance term to \u03b5\u03b8 that allows us to steer the generation away from s, while keeping changes to the original image minimal.",
                    "score": 0.5828973428977694,
                    "section_title": "Instructing Models on the World's Ugliness",
                    "char_start_offset": 4676,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 25
                        },
                        {
                            "start": 26,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 337
                        },
                        {
                            "start": 338,
                            "end": 396
                        },
                        {
                            "start": 397,
                            "end": 538
                        },
                        {
                            "start": 539,
                            "end": 676
                        },
                        {
                            "start": 677,
                            "end": 878
                        },
                        {
                            "start": 881,
                            "end": 1066
                        },
                        {
                            "start": 1069,
                            "end": 1112
                        },
                        {
                            "start": 1113,
                            "end": 1219
                        },
                        {
                            "start": 1220,
                            "end": 1321
                        },
                        {
                            "start": 1322,
                            "end": 1493
                        },
                        {
                            "start": 1494,
                            "end": 1614
                        },
                        {
                            "start": 1615,
                            "end": 1765
                        },
                        {
                            "start": 1766,
                            "end": 1914
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93115234375
                }
            ],
            "relevance_judgement": 0.93115234375,
            "relevance_judgment_input_expanded": "# Title: Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the World's Ugliness?\n# Venue: arXiv.org\n# Authors: Manuel Brack, Felix Friedrich, P. Schramowski, K. Kersting\n## Abstract\nText-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also reproduce inappropriate human behavior. Specifically, we demonstrate inappropriate degeneration on a large-scale for various generative text-to-image models, thus motivating the need for monitoring and moderating them at deployment. To this end, we evaluate mitigation strategies at inference to suppress the generation of inappropriate content. Our findings show that we can use models' representations of the world's ugliness to align them with human preferences.\n## Instructing Models on the World's Ugliness\nClassifier Free Guidance. Before going into detail on different instruction methods for image generation, we need to establish some fundamentals of text-to-image diffusion models (DMs). Intuitively, image generation starts from random noise \u03f5, and the model predicts an estimate of this noise \u03b5\u03b8 to be subtracted from the initial values. This results in a high-fidelity image x without any noise. Since this is a complex problem, multiple steps are applied, each subtracting a small amount (\u03f5 t ) of the predictive noise, approximating \u03f5. For text-to-image generation, the model's \u03f5-prediction is conditioned on a text prompt p and results in an image faithful to that prompt. To that end, DMs employ classifier-free guidance (Ho & Salimans, 2022), a conditioning method using a purely generational diffusion model, eliminating the need for an additional pre-trained classifier. \n\nThe noise estimate \u03b5\u03b8 uses an unconditioned \u03f5-prediction \u03f5 \u03b8 (z t ) which is pushed in the direction of the conditioned estimate \u03f5 \u03b8 (z t , c p ) to yield an image faithful to prompt p. \n\nInstructing Text-to-Image Models on Safety. We now consider two different instruction approaches extending the principles of classifier-free guidance. Both methods rely on a secondary text prompt s that describes concepts to suppress during generation. First, negative prompting replaces the unconditioned \u03f5-prediction \u03f5 \u03b8 (z t ) with one conditioned on s: \u03f5 \u03b8 (z t , c s ), thus moving away from the inappropriate concepts. This approach is intuitive and easy to implement, however offers limited control over the extent of content suppression. Additionally, we use Semantic Guidance (SEGA) (Brack et al., 2023) which is a powerful method for image manipulation based on additional text prompts. SEGA adds an additional guidance term to \u03b5\u03b8 that allows us to steer the generation away from s, while keeping changes to the original image minimal.",
            "reference_string": "[258967543 | Brack et al. | 2023 | Citations: 14]"
        },
        {
            "title": "Text Guided Image Editing with Automatic Concept Locating and Forgetting",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.19708, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268721096",
                    "name": "Jia Li"
                },
                {
                    "authorId": "2153121378",
                    "name": "Lijie Hu"
                },
                {
                    "authorId": "2304013931",
                    "name": "Zhixian He"
                },
                {
                    "authorId": "2253808698",
                    "name": "Jingfeng Zhang"
                },
                {
                    "authorId": "2268675026",
                    "name": "Tianhang Zheng"
                },
                {
                    "authorId": "2268815109",
                    "name": "Di Wang"
                }
            ],
            "abstract": "With the advancement of image-to-image diffusion models guided by text, significant progress has been made in image editing. However, a persistent challenge remains in seamlessly incorporating objects into images based on textual instructions, without relying on extra user-provided guidance. Text and images are inherently distinct modalities, bringing out difficulties in fully capturing the semantic intent conveyed through language and accurately translating that into the desired visual modifications. Therefore, text-guided image editing models often produce generations with residual object attributes that do not fully align with human expectations. To address this challenge, the models should comprehend the image content effectively away from a disconnect between the provided textual editing prompts and the actual modifications made to the image. In our paper, we propose a novel method called Locate and Forget (LaF), which effectively locates potential target concepts in the image for modification by comparing the syntactic trees of the target prompt and scene descriptions in the input image, intending to forget their existence clues in the generated image. Compared to the baselines, our method demonstrates its superiority in text-guided image editing tasks both qualitatively and quantitatively.",
            "corpus_id": 270123253,
            "sentences": [
                {
                    "corpus_id": "270123253",
                    "title": "Text Guided Image Editing with Automatic Concept Locating and Forgetting",
                    "text": "The primary goal of our method is to forget specific concepts from diffusion models using only the model's existing knowledge in the inference stage, without requiring any additional external data.Classifier-free diffusion guidance is commonly employed in class-conditional and text-conditional image generation tasks to enhance the visual quality of the generated images and to ensure that the sampled outputs better correspond to their respective conditioning factors.Typically, diffusion models use a guidance signal, where the model is conditioned on the desired concept or attribute to generate samples aligned with that concept (Gandikota et al., 2024).\n\nOur approach employs negative guidance to allow the diffusion model to gradually forget a specified concept in the original image while applying positive guidance for learning text prompts.Combining with Eq 1 and Eq 2, we compute the following score estimate using Classifier-Free Guidance during inference:\n\nwhere c p and c n are input prompt and forgetting concepts, w and \u03b7 are weights for controlling the balance between the positive and negative guidance signals in the final CFG score estimate.A higher value of eta will give more weight to the negative guidance, allowing tuning the level of concept forgetting during the diffusion process.",
                    "score": 0.7054797680422977,
                    "section_title": "Forgetting Concept for Conditionings",
                    "char_start_offset": 14821,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 197
                        },
                        {
                            "start": 197,
                            "end": 470
                        },
                        {
                            "start": 470,
                            "end": 659
                        },
                        {
                            "start": 661,
                            "end": 850
                        },
                        {
                            "start": 850,
                            "end": 968
                        },
                        {
                            "start": 970,
                            "end": 1161
                        },
                        {
                            "start": 1161,
                            "end": 1308
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 634,
                            "end": 658,
                            "matchedPaperCorpusId": "261276613"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93017578125
                }
            ],
            "relevance_judgement": 0.93017578125,
            "relevance_judgment_input_expanded": "# Title: Text Guided Image Editing with Automatic Concept Locating and Forgetting\n# Venue: arXiv.org\n# Authors: Jia Li, Lijie Hu, Zhixian He, Jingfeng Zhang, Tianhang Zheng, Di Wang\n## Abstract\nWith the advancement of image-to-image diffusion models guided by text, significant progress has been made in image editing. However, a persistent challenge remains in seamlessly incorporating objects into images based on textual instructions, without relying on extra user-provided guidance. Text and images are inherently distinct modalities, bringing out difficulties in fully capturing the semantic intent conveyed through language and accurately translating that into the desired visual modifications. Therefore, text-guided image editing models often produce generations with residual object attributes that do not fully align with human expectations. To address this challenge, the models should comprehend the image content effectively away from a disconnect between the provided textual editing prompts and the actual modifications made to the image. In our paper, we propose a novel method called Locate and Forget (LaF), which effectively locates potential target concepts in the image for modification by comparing the syntactic trees of the target prompt and scene descriptions in the input image, intending to forget their existence clues in the generated image. Compared to the baselines, our method demonstrates its superiority in text-guided image editing tasks both qualitatively and quantitatively.\n## Forgetting Concept for Conditionings\nThe primary goal of our method is to forget specific concepts from diffusion models using only the model's existing knowledge in the inference stage, without requiring any additional external data.Classifier-free diffusion guidance is commonly employed in class-conditional and text-conditional image generation tasks to enhance the visual quality of the generated images and to ensure that the sampled outputs better correspond to their respective conditioning factors.Typically, diffusion models use a guidance signal, where the model is conditioned on the desired concept or attribute to generate samples aligned with that concept (Gandikota et al., 2024).\n\nOur approach employs negative guidance to allow the diffusion model to gradually forget a specified concept in the original image while applying positive guidance for learning text prompts.Combining with Eq 1 and Eq 2, we compute the following score estimate using Classifier-Free Guidance during inference:\n\nwhere c p and c n are input prompt and forgetting concepts, w and \u03b7 are weights for controlling the balance between the positive and negative guidance signals in the final CFG score estimate.A higher value of eta will give more weight to the negative guidance, allowing tuning the level of concept forgetting during the diffusion process.",
            "reference_string": "[270123253 | Li et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models",
            "venue": "IEEE International Conference on Computer Vision",
            "year": 2023,
            "reference_count": 65,
            "citation_count": 62,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.04441, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "23677993",
                    "name": "Wenkai Dong"
                },
                {
                    "authorId": "2216675591",
                    "name": "Song Xue"
                },
                {
                    "authorId": "2067781481",
                    "name": "Xiaoyue Duan"
                },
                {
                    "authorId": "1488666685",
                    "name": "Shumin Han"
                }
            ],
            "abstract": "Recently large-scale language-image models (e.g., text-guided diffusion models) have considerably improved the image generation capabilities to generate photorealistic images in various domains. Based on this success, current image editing methods use texts to achieve intuitive and versatile modification of images. To edit a real image using diffusion models, one must first invert the image to a noisy latent from which an edited image is sampled with a target text prompt. However, most methods lack one of the following: user-friendliness (e.g., additional masks or precise descriptions of the input image are required), generalization to larger domains, or high fidelity to the input image. In this paper, we design an accurate and quick inversion technique, Prompt Tuning Inversion, for text-driven image editing. Specifically, our proposed editing method consists of a reconstruction stage and an editing stage. In the first stage, we encode the information of the input image into a learnable conditional embedding via Prompt Tuning Inversion. In the second stage, we apply classifier-free guidance to sample the edited image, where the conditional embedding is calculated by linearly interpolating between the target embedding and the optimized one obtained in the first stage. This technique ensures a superior trade-off between editability and high fidelity to the input image of our method. For example, we can change the color of a specific object while preserving its original shape and background under the guidance of only a target text prompt. Extensive experiments on ImageNet demonstrate the superior editing performance of our method compared to the state-of-the-art baselines.",
            "corpus_id": 258556958,
            "sentences": [
                {
                    "corpus_id": "258556958",
                    "title": "Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models",
                    "text": "Our editing method is built upon text-guided diffusion models. In Stable Diffusion, the text P is fed into a pre-trained CLIP [34] text encoder \u03c4 \u03b8 to obtain its corresponding embedding and the underlying UNet model is augmented with the cross attention mechanism, which is effective for generating visual contents conditioned on the text P. One of the key challenges in this kind of generation models is the amplification of the effect induced by the conditional text. To this end, the classifier-free guidance technique is proposed, where the prediction for each step is a combination of conditional and unconditional predictions. Formally, let c = \u03c4 \u03b8 (P) be the conditional embedding vector and \u2205 = \u03c4 \u03b8 (\"\") be the unconditional one, the classifierfree guidance prediction is calculated by: \n\nwhere \u03c9 is the guidance scale parameter.",
                    "score": 0.5679428144730864,
                    "section_title": "Background and preliminaries",
                    "char_start_offset": 13763,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 62
                        },
                        {
                            "start": 63,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 632
                        },
                        {
                            "start": 633,
                            "end": 794
                        },
                        {
                            "start": 797,
                            "end": 837
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 126,
                            "end": 130,
                            "matchedPaperCorpusId": "231591445"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93017578125
                }
            ],
            "relevance_judgement": 0.93017578125,
            "relevance_judgment_input_expanded": "# Title: Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models\n# Venue: IEEE International Conference on Computer Vision\n# Authors: Wenkai Dong, Song Xue, Xiaoyue Duan, Shumin Han\n## Abstract\nRecently large-scale language-image models (e.g., text-guided diffusion models) have considerably improved the image generation capabilities to generate photorealistic images in various domains. Based on this success, current image editing methods use texts to achieve intuitive and versatile modification of images. To edit a real image using diffusion models, one must first invert the image to a noisy latent from which an edited image is sampled with a target text prompt. However, most methods lack one of the following: user-friendliness (e.g., additional masks or precise descriptions of the input image are required), generalization to larger domains, or high fidelity to the input image. In this paper, we design an accurate and quick inversion technique, Prompt Tuning Inversion, for text-driven image editing. Specifically, our proposed editing method consists of a reconstruction stage and an editing stage. In the first stage, we encode the information of the input image into a learnable conditional embedding via Prompt Tuning Inversion. In the second stage, we apply classifier-free guidance to sample the edited image, where the conditional embedding is calculated by linearly interpolating between the target embedding and the optimized one obtained in the first stage. This technique ensures a superior trade-off between editability and high fidelity to the input image of our method. For example, we can change the color of a specific object while preserving its original shape and background under the guidance of only a target text prompt. Extensive experiments on ImageNet demonstrate the superior editing performance of our method compared to the state-of-the-art baselines.\n## Background and preliminaries\nOur editing method is built upon text-guided diffusion models. In Stable Diffusion, the text P is fed into a pre-trained CLIP [34] text encoder \u03c4 \u03b8 to obtain its corresponding embedding and the underlying UNet model is augmented with the cross attention mechanism, which is effective for generating visual contents conditioned on the text P. One of the key challenges in this kind of generation models is the amplification of the effect induced by the conditional text. To this end, the classifier-free guidance technique is proposed, where the prediction for each step is a combination of conditional and unconditional predictions. Formally, let c = \u03c4 \u03b8 (P) be the conditional embedding vector and \u2205 = \u03c4 \u03b8 (\"\") be the unconditional one, the classifierfree guidance prediction is calculated by: \n\nwhere \u03c9 is the guidance scale parameter.",
            "reference_string": "[258556958 | Dong et al. | 2023 | Citations: 62]"
        },
        {
            "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2022,
            "reference_count": 74,
            "citation_count": 1833,
            "influential_citation_count": 324,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2211.09800",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.09800, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2679394",
                    "name": "Tim Brooks"
                },
                {
                    "authorId": "2248172435",
                    "name": "Aleksander Holynski"
                },
                {
                    "authorId": "1763086",
                    "name": "Alexei A. Efros"
                }
            ],
            "abstract": "We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models\u2014a language model (GPT-3) and a text-to-image model (Stable Diffusion)\u2014to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.",
            "corpus_id": 253581213,
            "sentences": [
                {
                    "corpus_id": "253581213",
                    "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
                    "text": "Classifier-free diffusion guidance [20] is a method for trading off the quality and diversity of samples generated by a diffusion model. It is commonly used in class-conditional and text-conditional image generation to improve the visual quality of generated images and to make sampled images better correspond with their conditioning. Classifierfree guidance effectively shifts probability mass toward data where an implicit classifier p \u03b8 (c|z t ) assigns high likelihood to the conditioning c. Our model is therefore capable of conditional or unconditional denoising with respect to both or either conditional inputs. We introduce two guidance scales, s I and s T , which can be adjusted to trade off how strongly the generated samples correspond with the input image and how strongly they correspond with the edit instruction. Our modified score estimate is as follows: \n\nIn Figure 4, we show the effects of these two parameters on generated samples. See Appendix B for details of our classifier-free guidance formulation.",
                    "score": 0.739921017679735,
                    "section_title": "Classifier-free Guidance for Two Conditionings",
                    "char_start_offset": 14318,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 136
                        },
                        {
                            "start": 137,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 496
                        },
                        {
                            "start": 497,
                            "end": 620
                        },
                        {
                            "start": 621,
                            "end": 830
                        },
                        {
                            "start": 831,
                            "end": 873
                        },
                        {
                            "start": 876,
                            "end": 954
                        },
                        {
                            "start": 955,
                            "end": 1026
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9296875
                }
            ],
            "relevance_judgement": 0.9296875,
            "relevance_judgment_input_expanded": "# Title: InstructPix2Pix: Learning to Follow Image Editing Instructions\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Tim Brooks, Aleksander Holynski, Alexei A. Efros\n## Abstract\nWe propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models\u2014a language model (GPT-3) and a text-to-image model (Stable Diffusion)\u2014to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions.\n## Classifier-free Guidance for Two Conditionings\nClassifier-free diffusion guidance [20] is a method for trading off the quality and diversity of samples generated by a diffusion model. It is commonly used in class-conditional and text-conditional image generation to improve the visual quality of generated images and to make sampled images better correspond with their conditioning. Classifierfree guidance effectively shifts probability mass toward data where an implicit classifier p \u03b8 (c|z t ) assigns high likelihood to the conditioning c. Our model is therefore capable of conditional or unconditional denoising with respect to both or either conditional inputs. We introduce two guidance scales, s I and s T , which can be adjusted to trade off how strongly the generated samples correspond with the input image and how strongly they correspond with the edit instruction. Our modified score estimate is as follows: \n\nIn Figure 4, we show the effects of these two parameters on generated samples. See Appendix B for details of our classifier-free guidance formulation.",
            "reference_string": "[253581213 | Brooks et al. | 2022 | Citations: 1833]"
        },
        {
            "title": "Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation",
            "venue": "2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2409.10494",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.10494, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2321408469",
                    "name": "Noah Buchanan"
                },
                {
                    "authorId": "2268404815",
                    "name": "Susan Gauch"
                },
                {
                    "authorId": "2308097654",
                    "name": "Quan Mai"
                }
            ],
            "abstract": "This paper presents a diffusion-based recommender system that incorporates classifier-free guidance. Most current recommender systems provide recommendations using conventional methods such as collaborative or content-based filtering. Diffusion is a new approach to generative AI that improves on previous generative AI approaches such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). We incorporate diffusion in a recommender system that mirrors the sequence users take when browsing and rating items. Although a few current recommender systems incorporate diffusion, they do not incorporate classifier-free guidance, a new innovation in diffusion models as a whole. In this paper, we present a diffusion recommender system that augments the underlying recommender system model for improved performance and also incorporates classifier-free guidance. Our findings show improvements over state-of-the-art recommender systems for most metrics for several recommendation tasks on a variety of datasets. In particular, our approach demonstrates the potential to provide better recommendations when data is sparse.",
            "corpus_id": 272690217,
            "sentences": [
                {
                    "corpus_id": "272690217",
                    "title": "Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation",
                    "text": "Classifier guidance [6] modifies the diffusion score, specifically the gradient of the log probability density function, which is more tractable to learn than directly modeling the data distribution. This approach employs a classifier to guide the generation process by increasing the probability of data that the classifier assigns a high likelihood to the correct label. As demonstrated by [19], data that is well-classified tends to exhibit high perceptual quality, contributing to superior image generation outcomes. \n\nDespite its effectiveness in balancing precision and recall, classifier guidance depends on the gradients of an image classifier, limiting the variability of generated images. This dependency raises the question of whether it is possible to achieve comparable or superior guidance without relying on a classifier. \n\nClassifier-free guidance, introduced by [7], addresses this issue by eliminating the need for a dedicated classifier. Instead, it involves training an unconditional denoising diffusion model, parameterized by a score estimator, alongside a conditional denoising diffusion model, parameterized by a conditional score estimator. These are parameterized using a single neural network. For the unconditional model, a null token is used for the class identifier. The models are jointly trained, with the conditioning randomly set to a null token based on a hyperparameter probability p-uncond. This methodology not only simplifies the model architecture but also enhances guidance capabilities without the dependence on an external classifier.",
                    "score": 0.6094638345305057,
                    "section_title": "D. Classifier-Free Guidance",
                    "char_start_offset": 13994,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 199
                        },
                        {
                            "start": 200,
                            "end": 372
                        },
                        {
                            "start": 373,
                            "end": 520
                        },
                        {
                            "start": 523,
                            "end": 698
                        },
                        {
                            "start": 699,
                            "end": 836
                        },
                        {
                            "start": 839,
                            "end": 956
                        },
                        {
                            "start": 957,
                            "end": 1165
                        },
                        {
                            "start": 1166,
                            "end": 1220
                        },
                        {
                            "start": 1221,
                            "end": 1296
                        },
                        {
                            "start": 1297,
                            "end": 1427
                        },
                        {
                            "start": 1428,
                            "end": 1577
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 20,
                            "end": 23,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 392,
                            "end": 396,
                            "matchedPaperCorpusId": "1687220"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9296875
                }
            ],
            "relevance_judgement": 0.9296875,
            "relevance_judgment_input_expanded": "# Title: Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation\n# Venue: 2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)\n# Authors: Noah Buchanan, Susan Gauch, Quan Mai\n## Abstract\nThis paper presents a diffusion-based recommender system that incorporates classifier-free guidance. Most current recommender systems provide recommendations using conventional methods such as collaborative or content-based filtering. Diffusion is a new approach to generative AI that improves on previous generative AI approaches such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). We incorporate diffusion in a recommender system that mirrors the sequence users take when browsing and rating items. Although a few current recommender systems incorporate diffusion, they do not incorporate classifier-free guidance, a new innovation in diffusion models as a whole. In this paper, we present a diffusion recommender system that augments the underlying recommender system model for improved performance and also incorporates classifier-free guidance. Our findings show improvements over state-of-the-art recommender systems for most metrics for several recommendation tasks on a variety of datasets. In particular, our approach demonstrates the potential to provide better recommendations when data is sparse.\n## D. Classifier-Free Guidance\nClassifier guidance [6] modifies the diffusion score, specifically the gradient of the log probability density function, which is more tractable to learn than directly modeling the data distribution. This approach employs a classifier to guide the generation process by increasing the probability of data that the classifier assigns a high likelihood to the correct label. As demonstrated by [19], data that is well-classified tends to exhibit high perceptual quality, contributing to superior image generation outcomes. \n\nDespite its effectiveness in balancing precision and recall, classifier guidance depends on the gradients of an image classifier, limiting the variability of generated images. This dependency raises the question of whether it is possible to achieve comparable or superior guidance without relying on a classifier. \n\nClassifier-free guidance, introduced by [7], addresses this issue by eliminating the need for a dedicated classifier. Instead, it involves training an unconditional denoising diffusion model, parameterized by a score estimator, alongside a conditional denoising diffusion model, parameterized by a conditional score estimator. These are parameterized using a single neural network. For the unconditional model, a null token is used for the class identifier. The models are jointly trained, with the conditioning randomly set to a null token based on a hyperparameter probability p-uncond. This methodology not only simplifies the model architecture but also enhances guidance capabilities without the dependence on an external classifier.",
            "reference_string": "[272690217 | Buchanan et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 128,
            "influential_citation_count": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.04968",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.04968, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66139883",
                    "name": "Mohammadreza Armandpour"
                },
                {
                    "authorId": "145321315",
                    "name": "A. Sadeghian"
                },
                {
                    "authorId": "8158616",
                    "name": "Huangjie Zheng"
                },
                {
                    "authorId": "145759966",
                    "name": "Amir Sadeghian"
                },
                {
                    "authorId": "2152175923",
                    "name": "Mingyuan Zhou"
                }
            ],
            "abstract": "Although text-to-image diffusion models have made significant strides in generating images from text, they are sometimes more inclined to generate images like the data on which the model was trained rather than the provided text. This limitation has hindered their usage in both 2D and 3D applications. To address this problem, we explored the use of negative prompts but found that the current implementation fails to produce desired results, particularly when there is an overlap between the main and negative prompts. To overcome this issue, we propose Perp-Neg, a new algorithm that leverages the geometrical properties of the score space to address the shortcomings of the current negative prompts algorithm. Perp-Neg does not require any training or fine-tuning of the model. Moreover, we experimentally demonstrate that Perp-Neg provides greater flexibility in generating images by enabling users to edit out unwanted concepts from the initially generated images in 2D cases. Furthermore, to extend the application of Perp-Neg to 3D, we conducted a thorough exploration of how Perp-Neg can be used in 2D to condition the diffusion model to generate desired views, rather than being biased toward the canonical views. Finally, we applied our 2D intuition to integrate Perp-Neg with the state-of-the-art text-to-3D (DreamFusion) method, effectively addressing its Janus (multi-head) problem. Our project page is available at https://Perp-Neg.github.io/",
            "corpus_id": 258059755,
            "sentences": [
                {
                    "corpus_id": "258059755",
                    "title": "Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond",
                    "text": "To generate photo-realistic images given text prompts, the diffusion models can further take advantage of classifier guidance [9] or classifier-free guidance [15] to improve the image quality. Especially, in the context of text-to-image generation, classifier-free guidance is more widely used, which is usually expressed as a linear interpolation between the conditional and unconditional prediction \u02c6 \u03b8 (x t , t, c) = (1 + \u03c4 ) \u03b8 (x t , t, c) \u2212 \u03c4 \u03b8 (x t , t) at each timestep t with a guidance scale parameter \u03c4 . \n\nWhen the prompt becomes complex, the model may fail to understand some key elements in the query prompt and create undesired images. To handle complex textual information, [21] proposes composing diffusion models to factorize the text prompts into a set of text prompts, i.e., c={c 1 , ...c n }, and model the conditional distribution as When c 1 and c 2 are conditional independent given x, the ratio R(c 1 , c 2 ) = p \u03b8 (c1,c2|x) p \u03b8 (c1|x)p \u03b8 (c2|x) = 1 and this term can be ignored. However, in practice, the input text prompts can barely be independent when we need to specify the desired attributes of the image, such as style, content, and their relations. When c 1 and c 2 have an overlap in their semantics, simply fusing the concepts could be harmful and result in undesired results, especially in the case of concept negation, as shown in Figure 2. In the second row of images, we can clearly observe the key concepts requested in the main text prompt (respectively \"armchair\", \"sunglasses\", \"crown\", and \"horse\") are removed when those concepts appear in the negative prompts. This important observation motivates us to rethink the concept composing process and propose the use of a perpendicular gradient in the sampling, which is described in the following section.",
                    "score": 0.5706729000809201,
                    "section_title": "Preliminary",
                    "char_start_offset": 5810,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 192
                        },
                        {
                            "start": 193,
                            "end": 514
                        },
                        {
                            "start": 517,
                            "end": 649
                        },
                        {
                            "start": 650,
                            "end": 1003
                        },
                        {
                            "start": 1004,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1605
                        },
                        {
                            "start": 1606,
                            "end": 1796
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 126,
                            "end": 129,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 689,
                            "end": 693,
                            "matchedPaperCorpusId": "249375227"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9296875
                }
            ],
            "relevance_judgement": 0.9296875,
            "relevance_judgment_input_expanded": "# Title: Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond\n# Venue: arXiv.org\n# Authors: Mohammadreza Armandpour, A. Sadeghian, Huangjie Zheng, Amir Sadeghian, Mingyuan Zhou\n## Abstract\nAlthough text-to-image diffusion models have made significant strides in generating images from text, they are sometimes more inclined to generate images like the data on which the model was trained rather than the provided text. This limitation has hindered their usage in both 2D and 3D applications. To address this problem, we explored the use of negative prompts but found that the current implementation fails to produce desired results, particularly when there is an overlap between the main and negative prompts. To overcome this issue, we propose Perp-Neg, a new algorithm that leverages the geometrical properties of the score space to address the shortcomings of the current negative prompts algorithm. Perp-Neg does not require any training or fine-tuning of the model. Moreover, we experimentally demonstrate that Perp-Neg provides greater flexibility in generating images by enabling users to edit out unwanted concepts from the initially generated images in 2D cases. Furthermore, to extend the application of Perp-Neg to 3D, we conducted a thorough exploration of how Perp-Neg can be used in 2D to condition the diffusion model to generate desired views, rather than being biased toward the canonical views. Finally, we applied our 2D intuition to integrate Perp-Neg with the state-of-the-art text-to-3D (DreamFusion) method, effectively addressing its Janus (multi-head) problem. Our project page is available at https://Perp-Neg.github.io/\n## Preliminary\nTo generate photo-realistic images given text prompts, the diffusion models can further take advantage of classifier guidance [9] or classifier-free guidance [15] to improve the image quality. Especially, in the context of text-to-image generation, classifier-free guidance is more widely used, which is usually expressed as a linear interpolation between the conditional and unconditional prediction \u02c6 \u03b8 (x t , t, c) = (1 + \u03c4 ) \u03b8 (x t , t, c) \u2212 \u03c4 \u03b8 (x t , t) at each timestep t with a guidance scale parameter \u03c4 . \n\nWhen the prompt becomes complex, the model may fail to understand some key elements in the query prompt and create undesired images. To handle complex textual information, [21] proposes composing diffusion models to factorize the text prompts into a set of text prompts, i.e., c={c 1 , ...c n }, and model the conditional distribution as When c 1 and c 2 are conditional independent given x, the ratio R(c 1 , c 2 ) = p \u03b8 (c1,c2|x) p \u03b8 (c1|x)p \u03b8 (c2|x) = 1 and this term can be ignored. However, in practice, the input text prompts can barely be independent when we need to specify the desired attributes of the image, such as style, content, and their relations. When c 1 and c 2 have an overlap in their semantics, simply fusing the concepts could be harmful and result in undesired results, especially in the case of concept negation, as shown in Figure 2. In the second row of images, we can clearly observe the key concepts requested in the main text prompt (respectively \"armchair\", \"sunglasses\", \"crown\", and \"horse\") are removed when those concepts appear in the negative prompts. This important observation motivates us to rethink the concept composing process and propose the use of a perpendicular gradient in the sampling, which is described in the following section.",
            "reference_string": "[258059755 | Armandpour et al. | 2023 | Citations: 128]"
        },
        {
            "title": "CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 35,
            "influential_citation_count": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.08070, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2110872233",
                    "name": "Hyungjin Chung"
                },
                {
                    "authorId": "2109216792",
                    "name": "Jeongsol Kim"
                },
                {
                    "authorId": "153118937",
                    "name": "Geon Yeong Park"
                },
                {
                    "authorId": "2268758810",
                    "name": "Hyelin Nam"
                },
                {
                    "authorId": "2254155658",
                    "name": "Jong Chul Ye"
                }
            ],
            "abstract": "Classifier-free guidance (CFG) is a fundamental tool in modern diffusion models for text-guided generation. Although effective, CFG has notable drawbacks. For instance, DDIM with CFG lacks invertibility, complicating image editing; furthermore, high guidance scales, essential for high-quality outputs, frequently result in issues like mode collapse. Contrary to the widespread belief that these are inherent limitations of diffusion models, this paper reveals that the problems actually stem from the off-manifold phenomenon associated with CFG, rather than the diffusion models themselves. More specifically, inspired by the recent advancements of diffusion model-based inverse problem solvers (DIS), we reformulate text-guidance as an inverse problem with a text-conditioned score matching loss and develop CFG++, a novel approach that tackles the off-manifold challenges inherent in traditional CFG. CFG++ features a surprisingly simple fix to CFG, yet it offers significant improvements, including better sample quality for text-to-image generation, invertibility, smaller guidance scales, reduced mode collapse, etc. Furthermore, CFG++ enables seamless interpolation between unconditional and conditional sampling at lower guidance scales, consistently outperforming traditional CFG at all scales. Moreover, CFG++ can be easily integrated into high-order diffusion solvers and naturally extends to distilled diffusion models. Experimental results confirm that our method significantly enhances performance in text-to-image generation, DDIM inversion, editing, and solving inverse problems, suggesting a wide-ranging impact and potential applications in various fields that utilize text guidance. Project Page: https://cfgpp-diffusion.github.io/.",
            "corpus_id": 270391454,
            "sentences": [
                {
                    "corpus_id": "270391454",
                    "title": "CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models",
                    "text": "Classifier-free guidance (CFG) (Ho & Salimans, 2021) forms the key basis of modern text-guided generation with diffusion models (Dhariwal & Nichol, 2021;Rombach et al., 2022). Nowadays, it is common practice to train a diffusion model with large-scale paired text-image data (Schuhmann et al., 2022), so that sampling (i.e. generating) a signal (e.g. image, video) from a diffusion model can either be done unconditionally from p \u03b8 (x|\u2205) \u2261 p \u03b8 (x), or conditionally from p \u03b8 (x|c), where c is the text conditioning. Once trained, it seems natural that one would acquire samples from the conditional distribution by simply solving the probability-flow ODE or SDE sampling (Song et al., 2021b;a;Karras et al., 2022) with the conditional score function. In practice, however, it is observed that the conditioning signal is insufficient when used naively. To emphasize the guidance, one uses the guidance scale \u03c9 > 1, where the direction can be defined by the direction from the unconditional score to the conditional score (Ho & Salimans, 2021). \n\nIn modern text-to-image (T2I) diffusion models, the guidance scale \u03c9 is typically set within the range of [5.0, 30], referred to as the moderately high range of CFG guidance (Chen et al., 2024;Podell et al., 2023). The insufficiency in guidance also holds for classifier guidance (Dhariwal & Nichol, 2021;Song et al., 2021b) so that a scale of 10 was used. While using a high guidance scale yields higher-quality images with better alignment to the condition, it is also prone to mode collapse, reduces sample diversity, and yields an inevitable accumulation of errors during the sampling process.",
                    "score": 0.5843126307797828,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 323
                        },
                        {
                            "start": 324,
                            "end": 350
                        },
                        {
                            "start": 351,
                            "end": 515
                        },
                        {
                            "start": 516,
                            "end": 750
                        },
                        {
                            "start": 751,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 1042
                        },
                        {
                            "start": 1045,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1401
                        },
                        {
                            "start": 1402,
                            "end": 1642
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 31,
                            "end": 52,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 128,
                            "end": 153,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 153,
                            "end": 174,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 275,
                            "end": 299,
                            "matchedPaperCorpusId": "252917726"
                        },
                        {
                            "start": 671,
                            "end": 691,
                            "matchedPaperCorpusId": "227209335"
                        },
                        {
                            "start": 693,
                            "end": 713,
                            "matchedPaperCorpusId": "249240415"
                        },
                        {
                            "start": 1020,
                            "end": 1041,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1219,
                            "end": 1238,
                            "matchedPaperCorpusId": "263334265"
                        },
                        {
                            "start": 1325,
                            "end": 1350,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 1350,
                            "end": 1368,
                            "matchedPaperCorpusId": "227209335"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92919921875
                }
            ],
            "relevance_judgement": 0.92919921875,
            "relevance_judgment_input_expanded": "# Title: CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models\n# Venue: arXiv.org\n# Authors: Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, Jong Chul Ye\n## Abstract\nClassifier-free guidance (CFG) is a fundamental tool in modern diffusion models for text-guided generation. Although effective, CFG has notable drawbacks. For instance, DDIM with CFG lacks invertibility, complicating image editing; furthermore, high guidance scales, essential for high-quality outputs, frequently result in issues like mode collapse. Contrary to the widespread belief that these are inherent limitations of diffusion models, this paper reveals that the problems actually stem from the off-manifold phenomenon associated with CFG, rather than the diffusion models themselves. More specifically, inspired by the recent advancements of diffusion model-based inverse problem solvers (DIS), we reformulate text-guidance as an inverse problem with a text-conditioned score matching loss and develop CFG++, a novel approach that tackles the off-manifold challenges inherent in traditional CFG. CFG++ features a surprisingly simple fix to CFG, yet it offers significant improvements, including better sample quality for text-to-image generation, invertibility, smaller guidance scales, reduced mode collapse, etc. Furthermore, CFG++ enables seamless interpolation between unconditional and conditional sampling at lower guidance scales, consistently outperforming traditional CFG at all scales. Moreover, CFG++ can be easily integrated into high-order diffusion solvers and naturally extends to distilled diffusion models. Experimental results confirm that our method significantly enhances performance in text-to-image generation, DDIM inversion, editing, and solving inverse problems, suggesting a wide-ranging impact and potential applications in various fields that utilize text guidance. Project Page: https://cfgpp-diffusion.github.io/.\n## INTRODUCTION\nClassifier-free guidance (CFG) (Ho & Salimans, 2021) forms the key basis of modern text-guided generation with diffusion models (Dhariwal & Nichol, 2021;Rombach et al., 2022). Nowadays, it is common practice to train a diffusion model with large-scale paired text-image data (Schuhmann et al., 2022), so that sampling (i.e. generating) a signal (e.g. image, video) from a diffusion model can either be done unconditionally from p \u03b8 (x|\u2205) \u2261 p \u03b8 (x), or conditionally from p \u03b8 (x|c), where c is the text conditioning. Once trained, it seems natural that one would acquire samples from the conditional distribution by simply solving the probability-flow ODE or SDE sampling (Song et al., 2021b;a;Karras et al., 2022) with the conditional score function. In practice, however, it is observed that the conditioning signal is insufficient when used naively. To emphasize the guidance, one uses the guidance scale \u03c9 > 1, where the direction can be defined by the direction from the unconditional score to the conditional score (Ho & Salimans, 2021). \n\nIn modern text-to-image (T2I) diffusion models, the guidance scale \u03c9 is typically set within the range of [5.0, 30], referred to as the moderately high range of CFG guidance (Chen et al., 2024;Podell et al., 2023). The insufficiency in guidance also holds for classifier guidance (Dhariwal & Nichol, 2021;Song et al., 2021b) so that a scale of 10 was used. While using a high guidance scale yields higher-quality images with better alignment to the condition, it is also prone to mode collapse, reduces sample diversity, and yields an inevitable accumulation of errors during the sampling process.",
            "reference_string": "[270391454 | Chung et al. | 2024 | Citations: 35]"
        },
        {
            "title": "SINE: SINgle Image Editing with Text-to-Image Diffusion Models",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2022,
            "reference_count": 66,
            "citation_count": 160,
            "influential_citation_count": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2212.04489",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.04489, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2128662401",
                    "name": "Zhixing Zhang"
                },
                {
                    "authorId": "3471102",
                    "name": "Ligong Han"
                },
                {
                    "authorId": "2461629",
                    "name": "Arna Ghosh"
                },
                {
                    "authorId": "1711560",
                    "name": "Dimitris N. Metaxas"
                },
                {
                    "authorId": "2111473627",
                    "name": "Jian Ren"
                }
            ],
            "abstract": "Recent works on diffusion models have demonstrated a strong capability for conditioning image generation, e.g., text-guided image synthesis. Such success inspires many efforts trying to use large-scale pre-trained diffusion models for tackling a challenging problem-real image editing. Works conducted in this area learn a unique textual token corresponding to several images containing the same object. However, under many circumstances, only one image is available, such as the painting of the Girl with a Pearl Earring. Using existing works on fine-tuning the pre-trained diffusion models with a single image causes severe overfitting issues. The information leakage from the pre-trained diffusion models makes editing can not keep the same content as the given image while creating new features depicted by the language guidance. This work aims to address the problem of single-image editing. We propose a novel model-based guidance built upon the classifier-free guidance so that the knowledge from the model trained on a single image can be distilled into the pre-trained diffusion model, enabling content creation even with one given image. Additionally, we propose a patch-based fine-tuning that can effectively help the model generate images of arbitrary resolution. We provide extensive experiments to validate the design choices of our approach and show promising editing capabilities, including changing style, content addition, and object manipulation. Our code is made publicly available here.",
            "corpus_id": 254408758,
            "sentences": [
                {
                    "corpus_id": "254408758",
                    "title": "SINE: SINgle Image Editing with Text-to-Image Diffusion Models",
                    "text": "With the above-presented LDMs, we introduce our approach, inspired by classifier-free guidance, to overcome overfitting when fine-tuning LDMs with one image. Classifier-free guidance [15] is a technique widely adopted by prior text-to-image diffusion models [32,35]. A single diffusion model is trained using conditional and unconditional objectives by randomly dropping the condition during training. When sampling, a linear combination of the During sampling, we give additional language guidance about the target domain to edit the image. Also, we sample a noisy latent code zT with the dimension corresponding to the desired output resolution. Language conditioning for \u03f5 \u03b8 and c are given by pre-trained language encoder \u03c4 \u03b8 with the target language guidance. While for the fine-tuned diffusion model, \u03b5\u03b8 , in addition to the language conditioning \u0109, we also input the positional embedding for the whole image. We employ a linear combination between the score calculated by each model for the first K steps and inference only on pre-trained \u03f5 \u03b8 after. \n\nconditional and unconditional score estimation is used: \n\nwhere \u03f5 \u03b8 (z t , c) and \u03f5 \u03b8 (z t ) are the conditional and unconditional \u03f5-predictions, c is the conditioning vector generated by \u03c4 \u03b8 , and w is the weight for the guidance. The predication is performed using the Tweedie's formula [7], namely, z t \u2212 \u221a 1 \u2212 \u1fb1t \u03b5\u03b8 / \u221a \u1fb1t , where \u1fb1t is a function of t that affects the sampling quality. \n\nSince we only have one image as the training data, e.g., painting of Mona Lisa, and one corresponding text descrip-tor of that image, the diffusion model suffers from overfitting, and severe language drifts after fine-tuning [33]. As a result, the fine-tuned model fails to synthesize images containing features from other language guidance. The overfitting issue might be due to only one repeated prompt used during fine-tuning, making other text prompts no longer accurate enough to control editing (see examples in Fig. 6). Model-based classifier-free guidance.",
                    "score": 0.5522403638268103,
                    "section_title": "Model-Based Classifier-Free Guidance",
                    "char_start_offset": 10673,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 157
                        },
                        {
                            "start": 158,
                            "end": 266
                        },
                        {
                            "start": 267,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 541
                        },
                        {
                            "start": 542,
                            "end": 647
                        },
                        {
                            "start": 648,
                            "end": 764
                        },
                        {
                            "start": 765,
                            "end": 915
                        },
                        {
                            "start": 916,
                            "end": 1056
                        },
                        {
                            "start": 1059,
                            "end": 1114
                        },
                        {
                            "start": 1117,
                            "end": 1290
                        },
                        {
                            "start": 1291,
                            "end": 1450
                        },
                        {
                            "start": 1453,
                            "end": 1683
                        },
                        {
                            "start": 1684,
                            "end": 1794
                        },
                        {
                            "start": 1795,
                            "end": 1979
                        },
                        {
                            "start": 1980,
                            "end": 2017
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 183,
                            "end": 187,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 258,
                            "end": 262,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 262,
                            "end": 265,
                            "matchedPaperCorpusId": "248986576"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9267578125
                }
            ],
            "relevance_judgement": 0.9267578125,
            "relevance_judgment_input_expanded": "# Title: SINE: SINgle Image Editing with Text-to-Image Diffusion Models\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Zhixing Zhang, Ligong Han, Arna Ghosh, Dimitris N. Metaxas, Jian Ren\n## Abstract\nRecent works on diffusion models have demonstrated a strong capability for conditioning image generation, e.g., text-guided image synthesis. Such success inspires many efforts trying to use large-scale pre-trained diffusion models for tackling a challenging problem-real image editing. Works conducted in this area learn a unique textual token corresponding to several images containing the same object. However, under many circumstances, only one image is available, such as the painting of the Girl with a Pearl Earring. Using existing works on fine-tuning the pre-trained diffusion models with a single image causes severe overfitting issues. The information leakage from the pre-trained diffusion models makes editing can not keep the same content as the given image while creating new features depicted by the language guidance. This work aims to address the problem of single-image editing. We propose a novel model-based guidance built upon the classifier-free guidance so that the knowledge from the model trained on a single image can be distilled into the pre-trained diffusion model, enabling content creation even with one given image. Additionally, we propose a patch-based fine-tuning that can effectively help the model generate images of arbitrary resolution. We provide extensive experiments to validate the design choices of our approach and show promising editing capabilities, including changing style, content addition, and object manipulation. Our code is made publicly available here.\n## Model-Based Classifier-Free Guidance\nWith the above-presented LDMs, we introduce our approach, inspired by classifier-free guidance, to overcome overfitting when fine-tuning LDMs with one image. Classifier-free guidance [15] is a technique widely adopted by prior text-to-image diffusion models [32,35]. A single diffusion model is trained using conditional and unconditional objectives by randomly dropping the condition during training. When sampling, a linear combination of the During sampling, we give additional language guidance about the target domain to edit the image. Also, we sample a noisy latent code zT with the dimension corresponding to the desired output resolution. Language conditioning for \u03f5 \u03b8 and c are given by pre-trained language encoder \u03c4 \u03b8 with the target language guidance. While for the fine-tuned diffusion model, \u03b5\u03b8 , in addition to the language conditioning \u0109, we also input the positional embedding for the whole image. We employ a linear combination between the score calculated by each model for the first K steps and inference only on pre-trained \u03f5 \u03b8 after. \n\nconditional and unconditional score estimation is used: \n\nwhere \u03f5 \u03b8 (z t , c) and \u03f5 \u03b8 (z t ) are the conditional and unconditional \u03f5-predictions, c is the conditioning vector generated by \u03c4 \u03b8 , and w is the weight for the guidance. The predication is performed using the Tweedie's formula [7], namely, z t \u2212 \u221a 1 \u2212 \u1fb1t \u03b5\u03b8 / \u221a \u1fb1t , where \u1fb1t is a function of t that affects the sampling quality. \n\nSince we only have one image as the training data, e.g., painting of Mona Lisa, and one corresponding text descrip-tor of that image, the diffusion model suffers from overfitting, and severe language drifts after fine-tuning [33]. As a result, the fine-tuned model fails to synthesize images containing features from other language guidance. The overfitting issue might be due to only one repeated prompt used during fine-tuning, making other text prompts no longer accurate enough to control editing (see examples in Fig. 6). Model-based classifier-free guidance.",
            "reference_string": "[254408758 | Zhang et al. | 2022 | Citations: 160]"
        },
        {
            "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking",
            "venue": "",
            "year": 2025,
            "reference_count": 43,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.20199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2363590342",
                    "name": "Pengxiang Li"
                },
                {
                    "authorId": "2362879323",
                    "name": "Shilin Yan"
                },
                {
                    "authorId": "2362728158",
                    "name": "Joey Tsai"
                },
                {
                    "authorId": "2291314199",
                    "name": "Renrui Zhang"
                },
                {
                    "authorId": "2363570661",
                    "name": "Ruichuan An"
                },
                {
                    "authorId": "145490494",
                    "name": "Ziyu Guo"
                },
                {
                    "authorId": "2303650253",
                    "name": "Xiaowei Gao"
                }
            ],
            "abstract": "Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel method that tailors the unconditional input by leveraging the model's instantaneous predictive confidence. At each step of an iterative (masked) diffusion language model, A-CFG identifies tokens in the currently generated sequence for which the model exhibits low confidence. These tokens are temporarily re-masked to create a dynamic, localized unconditional input. This focuses CFG's corrective influence precisely on areas of ambiguity, leading to more effective guidance. We integrate A-CFG into a state-of-the-art masked diffusion language model and demonstrate its efficacy. Experiments on diverse language generation benchmarks show that A-CFG yields substantial improvements over standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work highlights the benefit of dynamically adapting guidance mechanisms to model uncertainty in iterative generation.",
            "corpus_id": 278910862,
            "sentences": [],
            "relevance_judgement": 0.9267578125,
            "relevance_judgment_input_expanded": "# Title: Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking\n# Venue: \n# Authors: Pengxiang Li, Shilin Yan, Joey Tsai, Renrui Zhang, Ruichuan An, Ziyu Guo, Xiaowei Gao\n## Abstract\nClassifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel method that tailors the unconditional input by leveraging the model's instantaneous predictive confidence. At each step of an iterative (masked) diffusion language model, A-CFG identifies tokens in the currently generated sequence for which the model exhibits low confidence. These tokens are temporarily re-masked to create a dynamic, localized unconditional input. This focuses CFG's corrective influence precisely on areas of ambiguity, leading to more effective guidance. We integrate A-CFG into a state-of-the-art masked diffusion language model and demonstrate its efficacy. Experiments on diverse language generation benchmarks show that A-CFG yields substantial improvements over standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work highlights the benefit of dynamically adapting guidance mechanisms to model uncertainty in iterative generation.\n",
            "reference_string": "[278910862 | Li et al. | 2025 | Citations: 1]"
        },
        {
            "title": "ReactFace: Online Multiple Appropriate Facial Reaction Generation in Dyadic Interactions.",
            "venue": "IEEE Transactions on Visualization and Computer Graphics",
            "year": 2023,
            "reference_count": 93,
            "citation_count": 4,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.15748",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.15748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153561173",
                    "name": "Cheng Luo"
                },
                {
                    "authorId": "2275025172",
                    "name": "Siyang Song"
                },
                {
                    "authorId": "34181727",
                    "name": "Weicheng Xie"
                },
                {
                    "authorId": "73772115",
                    "name": "Micol Spitale"
                },
                {
                    "authorId": "2325909107",
                    "name": "Zongyuan Ge"
                },
                {
                    "authorId": "2121272943",
                    "name": "Linlin Shen"
                },
                {
                    "authorId": "2256439823",
                    "name": "Hatice Gunes"
                }
            ],
            "abstract": "In dyadic interaction, predicting the listener's facial reactions is challenging as different reactions could be appropriate in response to the same speaker's behaviour. Previous approaches predominantly treated this task as an interpolation or fitting problem, emphasizing deterministic outcomes but ignoring the diversity and uncertainty of human facial reactions. Furthermore, these methods often failed to model short-range and long-range dependencies within the interaction context, leading to issues in the synchrony and appropriateness of the generated facial reactions. To address these limitations, this paper reformulates the task as an extrapolation or prediction problem, and proposes an novel framework (called ReactFace) to generate multiple different but appropriate facial reactions from a speaker behaviour rather than merely replicating the corresponding listener facial behaviours. Our ReactFace generates multiple different but appropriate photo-realistic human facial reactions by: (i) learning an appropriate facial reaction distribution representing multiple different but appropriate facial reactions; and (ii) synchronizing the generated facial reactions with the speaker verbal and non-verbal behaviours at each time stamp, resulting in realistic 2D facial reaction sequences. Experimental results demonstrate the effectiveness of our approach in generating multiple diverse, synchronized, and appropriate facial reactions from each speaker's behaviour. The quality of the generated facial reactions is intimately tied to the speaker's speech and facial expressions, achieved through our novel speaker-listener interaction modules.",
            "corpus_id": 273811150,
            "sentences": [
                {
                    "corpus_id": "273811150",
                    "title": "ReactFace: Online Multiple Appropriate Facial Reaction Generation in Dyadic Interactions.",
                    "text": "Recent techniques for conditional generation have been developed to incorporate various modalities, ensuring that synthesized results align with the instructions provided by these modality signals [66]. Early approaches, such as conditional variational autoencoders (CVAEs) [66] and conditional generative adversarial networks [66], leverage class labels to distinguish image domains and guide generated image samples to possess domain-specific properties. For example, CycleGAN was proposed to translate images from a source domain to a target domain, while the StarGAN series [18], [67] further succeeded in translating images from a source domain to multiple target domains. Some techniques embed discrete class labels as conditional signals in the generation process, either by directly concatenating class labels with the input [18], [66] or by using conditional normalization techniques [67], [68]. Building on these successful practices, conditional diffusion models incorporate class information (e.g., class labels or text) into normalization layers and guide the generation process using classifier gradients. Further research has shown that guidance can be derived from the generative model itself without a classifier, a method known as classifier-free guidance. This advancement unlocks various forms of conditional signals, leading to conditional generative models [69], [70] that control synthesized samples with multiple modalities, such as text, edge maps, human pose skeletons, segmentation maps, depth, and normals.",
                    "score": 0.6450198836676306,
                    "section_title": "Conditional Generative Models",
                    "char_start_offset": 12660,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 202
                        },
                        {
                            "start": 203,
                            "end": 456
                        },
                        {
                            "start": 457,
                            "end": 677
                        },
                        {
                            "start": 678,
                            "end": 904
                        },
                        {
                            "start": 905,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1274
                        },
                        {
                            "start": 1275,
                            "end": 1534
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 197,
                            "end": 201,
                            "matchedPaperCorpusId": "13936837"
                        },
                        {
                            "start": 274,
                            "end": 278,
                            "matchedPaperCorpusId": "13936837"
                        },
                        {
                            "start": 327,
                            "end": 331,
                            "matchedPaperCorpusId": "13936837"
                        },
                        {
                            "start": 578,
                            "end": 582,
                            "matchedPaperCorpusId": "9417016"
                        },
                        {
                            "start": 584,
                            "end": 588,
                            "matchedPaperCorpusId": "208617800"
                        },
                        {
                            "start": 833,
                            "end": 837,
                            "matchedPaperCorpusId": "9417016"
                        },
                        {
                            "start": 839,
                            "end": 843,
                            "matchedPaperCorpusId": "13936837"
                        },
                        {
                            "start": 893,
                            "end": 897,
                            "matchedPaperCorpusId": "208617800"
                        },
                        {
                            "start": 899,
                            "end": 903,
                            "matchedPaperCorpusId": "54482423"
                        },
                        {
                            "start": 1379,
                            "end": 1383,
                            "matchedPaperCorpusId": "256827727"
                        },
                        {
                            "start": 1385,
                            "end": 1389,
                            "matchedPaperCorpusId": "245335280"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92578125
                }
            ],
            "relevance_judgement": 0.92578125,
            "relevance_judgment_input_expanded": "# Title: ReactFace: Online Multiple Appropriate Facial Reaction Generation in Dyadic Interactions.\n# Venue: IEEE Transactions on Visualization and Computer Graphics\n# Authors: Cheng Luo, Siyang Song, Weicheng Xie, Micol Spitale, Zongyuan Ge, Linlin Shen, Hatice Gunes\n## Abstract\nIn dyadic interaction, predicting the listener's facial reactions is challenging as different reactions could be appropriate in response to the same speaker's behaviour. Previous approaches predominantly treated this task as an interpolation or fitting problem, emphasizing deterministic outcomes but ignoring the diversity and uncertainty of human facial reactions. Furthermore, these methods often failed to model short-range and long-range dependencies within the interaction context, leading to issues in the synchrony and appropriateness of the generated facial reactions. To address these limitations, this paper reformulates the task as an extrapolation or prediction problem, and proposes an novel framework (called ReactFace) to generate multiple different but appropriate facial reactions from a speaker behaviour rather than merely replicating the corresponding listener facial behaviours. Our ReactFace generates multiple different but appropriate photo-realistic human facial reactions by: (i) learning an appropriate facial reaction distribution representing multiple different but appropriate facial reactions; and (ii) synchronizing the generated facial reactions with the speaker verbal and non-verbal behaviours at each time stamp, resulting in realistic 2D facial reaction sequences. Experimental results demonstrate the effectiveness of our approach in generating multiple diverse, synchronized, and appropriate facial reactions from each speaker's behaviour. The quality of the generated facial reactions is intimately tied to the speaker's speech and facial expressions, achieved through our novel speaker-listener interaction modules.\n## Conditional Generative Models\nRecent techniques for conditional generation have been developed to incorporate various modalities, ensuring that synthesized results align with the instructions provided by these modality signals [66]. Early approaches, such as conditional variational autoencoders (CVAEs) [66] and conditional generative adversarial networks [66], leverage class labels to distinguish image domains and guide generated image samples to possess domain-specific properties. For example, CycleGAN was proposed to translate images from a source domain to a target domain, while the StarGAN series [18], [67] further succeeded in translating images from a source domain to multiple target domains. Some techniques embed discrete class labels as conditional signals in the generation process, either by directly concatenating class labels with the input [18], [66] or by using conditional normalization techniques [67], [68]. Building on these successful practices, conditional diffusion models incorporate class information (e.g., class labels or text) into normalization layers and guide the generation process using classifier gradients. Further research has shown that guidance can be derived from the generative model itself without a classifier, a method known as classifier-free guidance. This advancement unlocks various forms of conditional signals, leading to conditional generative models [69], [70] that control synthesized samples with multiple modalities, such as text, edge maps, human pose skeletons, segmentation maps, depth, and normals.",
            "reference_string": "[273811150 | Luo et al. | 2023 | Citations: 4]"
        },
        {
            "title": "Superscopes: Amplifying Internal Feature Representations for Language Model Interpretation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 43,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.02078, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2348485713",
                    "name": "Jonathan Jacobi"
                },
                {
                    "authorId": "2333352",
                    "name": "Gal Niv"
                }
            ],
            "abstract": "Understanding and interpreting the internal representations of large language models (LLMs) remains an open challenge. Patchscopes introduced a method for probing internal activations by patching them into new prompts, prompting models to self-explain their hidden representations. We introduce Superscopes, a technique that systematically amplifies superposed features in MLP outputs (multilayer perceptron) and hidden states before patching them into new contexts. Inspired by the\"features as directions\"perspective and the Classifier-Free Guidance (CFG) approach from diffusion models, Superscopes amplifies weak but meaningful features, enabling the interpretation of internal representations that previous methods failed to explain-all without requiring additional training. This approach provides new insights into how LLMs build context and represent complex concepts, further advancing mechanistic interpretability.",
            "corpus_id": 276774646,
            "sentences": [
                {
                    "corpus_id": "276774646",
                    "title": "Superscopes: Amplifying Internal Feature Representations for Language Model Interpretation",
                    "text": "In diffusion models, Classifier-Free Guidance (CFG) (Ho et al. (2022)) enhances the alignment of generated outputs with a given condition (such as a text prompt). It is widely used in diffusion-based models (Chen et al. (2024)). \n\nCFG operates by amplifying the direction that represents the condition, where the difference between conditional and unconditional predictions defines this direction in the model's latent space. By amplifying this directional shift, the model is effectively steered toward outputs that better align with the given condition. Reapplying this shift further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance. \n\nThis technique is particularly useful in text-to-image generation models like Stable Diffusion and GLIDE, as it improves adherence to prompts without requiring an external classifier.",
                    "score": 0.6806003657354234,
                    "section_title": "Classifier-Free Guidance",
                    "char_start_offset": 10843,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 162
                        },
                        {
                            "start": 163,
                            "end": 228
                        },
                        {
                            "start": 231,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 555
                        },
                        {
                            "start": 556,
                            "end": 678
                        },
                        {
                            "start": 681,
                            "end": 864
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 207,
                            "end": 226,
                            "matchedPaperCorpusId": "269043390"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92529296875
                }
            ],
            "relevance_judgement": 0.92529296875,
            "relevance_judgment_input_expanded": "# Title: Superscopes: Amplifying Internal Feature Representations for Language Model Interpretation\n# Venue: arXiv.org\n# Authors: Jonathan Jacobi, Gal Niv\n## Abstract\nUnderstanding and interpreting the internal representations of large language models (LLMs) remains an open challenge. Patchscopes introduced a method for probing internal activations by patching them into new prompts, prompting models to self-explain their hidden representations. We introduce Superscopes, a technique that systematically amplifies superposed features in MLP outputs (multilayer perceptron) and hidden states before patching them into new contexts. Inspired by the\"features as directions\"perspective and the Classifier-Free Guidance (CFG) approach from diffusion models, Superscopes amplifies weak but meaningful features, enabling the interpretation of internal representations that previous methods failed to explain-all without requiring additional training. This approach provides new insights into how LLMs build context and represent complex concepts, further advancing mechanistic interpretability.\n## Classifier-Free Guidance\nIn diffusion models, Classifier-Free Guidance (CFG) (Ho et al. (2022)) enhances the alignment of generated outputs with a given condition (such as a text prompt). It is widely used in diffusion-based models (Chen et al. (2024)). \n\nCFG operates by amplifying the direction that represents the condition, where the difference between conditional and unconditional predictions defines this direction in the model's latent space. By amplifying this directional shift, the model is effectively steered toward outputs that better align with the given condition. Reapplying this shift further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance. \n\nThis technique is particularly useful in text-to-image generation models like Stable Diffusion and GLIDE, as it improves adherence to prompts without requiring an external classifier.",
            "reference_string": "[276774646 | Jacobi et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Edit Everything: A Text-Guided Generative System for Images Editing",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 21,
            "citation_count": 32,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.14006",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.14006, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2146701638",
                    "name": "Defeng Xie"
                },
                {
                    "authorId": "2213709086",
                    "name": "Ruichen Wang"
                },
                {
                    "authorId": "2146393919",
                    "name": "Jiancang Ma"
                },
                {
                    "authorId": "2141809694",
                    "name": "Chen Chen"
                },
                {
                    "authorId": "2130373",
                    "name": "H. Lu"
                },
                {
                    "authorId": "144041880",
                    "name": "D. Yang"
                },
                {
                    "authorId": "2212609873",
                    "name": "Fobo Shi"
                },
                {
                    "authorId": "2117690698",
                    "name": "Xiaodong Lin"
                }
            ],
            "abstract": "We introduce a new generative system called Edit Everything, which can take image and text inputs and produce image outputs. Edit Everything allows users to edit images using simple text instructions. Our system designs prompts to guide the visual module in generating requested images. Experiments demonstrate that Edit Everything facilitates the implementation of the visual aspects of Stable Diffusion with the use of Segment Anything model and CLIP. Our system is publicly available at https://github.com/DefengXie/Edit_Everything.",
            "corpus_id": 258352755,
            "sentences": [
                {
                    "corpus_id": "258352755",
                    "title": "Edit Everything: A Text-Guided Generative System for Images Editing",
                    "text": "\"While drawing I discover what I really want to say.\" -Dario Fo Visualization, including images, paintings, shots, illustrations, and photographs, can usually be described with text. However, creating these images often requires specialized skills and a significant time investment [18]. Consequently, a generative system has the potential to produce realistic images based on natural language, allowing humans to efficiently generate a wide range of visual content. Additionally, this system offers an unprecedented opportunity for continuous improvement and precise control over image editing, making it a crucial tool for real-world applications. \n\nRecently, diffusion models have shown promising performances in generating high-quality realistic images [17,14,15,12]. In particular, a text-guided method significantly improves the diversity and fidelity [10]. To address photorealism in the conditional setting, [2] proposes a classifier to guide diffusion models, allowing them to generate realistic images toward a classifier's label. [5] shows that guidance can be indeed performed by a pure generative model without a classifier, named classifierfree guidance. Classifier-free guidance combines conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that with classifier guidance. Inspired by the ability of guided diffusion models with text, [10] first implements CLIP to guide diffusion models towards text prompt [11]. However, compared to the quality of generated images, Finally, guided by the target prompt, Stable Diffusion (SD) generates the replacement object for the mask segment. This process is seamless and efficient, resulting in high-quality image editing. \n\nCLIP guidance cannot compete with classifier-free guidance. Another promising solution is that [20] designs a neural network structure (ControlNet) to control diffusion models and support conditional inputs. Diffusion models can be augmented by ControlNets to generate edge maps, segmentation maps, key points, etc. \n\nInspired by the remarkable performance of ControlNet and CLIP guidance in significantly enhancing the image quality, we leverage Segment Anything model (SAM) and CLIP to guide diffusion models [11,8]. In our work, we create a text-guided generative system called Editing Everything, combining SAM, CLIP and Stable Diffusion (SD) [14].",
                    "score": 0.5993302353474657,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 287
                        },
                        {
                            "start": 288,
                            "end": 466
                        },
                        {
                            "start": 467,
                            "end": 649
                        },
                        {
                            "start": 652,
                            "end": 771
                        },
                        {
                            "start": 772,
                            "end": 863
                        },
                        {
                            "start": 864,
                            "end": 1040
                        },
                        {
                            "start": 1041,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1349
                        },
                        {
                            "start": 1350,
                            "end": 1490
                        },
                        {
                            "start": 1491,
                            "end": 1659
                        },
                        {
                            "start": 1660,
                            "end": 1740
                        },
                        {
                            "start": 1743,
                            "end": 1802
                        },
                        {
                            "start": 1803,
                            "end": 1950
                        },
                        {
                            "start": 1951,
                            "end": 2058
                        },
                        {
                            "start": 2061,
                            "end": 2261
                        },
                        {
                            "start": 2262,
                            "end": 2395
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 282,
                            "end": 286,
                            "matchedPaperCorpusId": "1246802"
                        },
                        {
                            "start": 757,
                            "end": 761,
                            "matchedPaperCorpusId": "14888175"
                        },
                        {
                            "start": 761,
                            "end": 764,
                            "matchedPaperCorpusId": "245335280"
                        },
                        {
                            "start": 764,
                            "end": 767,
                            "matchedPaperCorpusId": "248986576"
                        },
                        {
                            "start": 767,
                            "end": 770,
                            "matchedPaperCorpusId": "232035663"
                        },
                        {
                            "start": 916,
                            "end": 919,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 1485,
                            "end": 1489,
                            "matchedPaperCorpusId": "231591445"
                        },
                        {
                            "start": 2254,
                            "end": 2258,
                            "matchedPaperCorpusId": "231591445"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9248046875
                }
            ],
            "relevance_judgement": 0.9248046875,
            "relevance_judgment_input_expanded": "# Title: Edit Everything: A Text-Guided Generative System for Images Editing\n# Venue: arXiv.org\n# Authors: Defeng Xie, Ruichen Wang, Jiancang Ma, Chen Chen, H. Lu, D. Yang, Fobo Shi, Xiaodong Lin\n## Abstract\nWe introduce a new generative system called Edit Everything, which can take image and text inputs and produce image outputs. Edit Everything allows users to edit images using simple text instructions. Our system designs prompts to guide the visual module in generating requested images. Experiments demonstrate that Edit Everything facilitates the implementation of the visual aspects of Stable Diffusion with the use of Segment Anything model and CLIP. Our system is publicly available at https://github.com/DefengXie/Edit_Everything.\n## Introduction\n\"While drawing I discover what I really want to say.\" -Dario Fo Visualization, including images, paintings, shots, illustrations, and photographs, can usually be described with text. However, creating these images often requires specialized skills and a significant time investment [18]. Consequently, a generative system has the potential to produce realistic images based on natural language, allowing humans to efficiently generate a wide range of visual content. Additionally, this system offers an unprecedented opportunity for continuous improvement and precise control over image editing, making it a crucial tool for real-world applications. \n\nRecently, diffusion models have shown promising performances in generating high-quality realistic images [17,14,15,12]. In particular, a text-guided method significantly improves the diversity and fidelity [10]. To address photorealism in the conditional setting, [2] proposes a classifier to guide diffusion models, allowing them to generate realistic images toward a classifier's label. [5] shows that guidance can be indeed performed by a pure generative model without a classifier, named classifierfree guidance. Classifier-free guidance combines conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that with classifier guidance. Inspired by the ability of guided diffusion models with text, [10] first implements CLIP to guide diffusion models towards text prompt [11]. However, compared to the quality of generated images, Finally, guided by the target prompt, Stable Diffusion (SD) generates the replacement object for the mask segment. This process is seamless and efficient, resulting in high-quality image editing. \n\nCLIP guidance cannot compete with classifier-free guidance. Another promising solution is that [20] designs a neural network structure (ControlNet) to control diffusion models and support conditional inputs. Diffusion models can be augmented by ControlNets to generate edge maps, segmentation maps, key points, etc. \n\nInspired by the remarkable performance of ControlNet and CLIP guidance in significantly enhancing the image quality, we leverage Segment Anything model (SAM) and CLIP to guide diffusion models [11,8]. In our work, we create a text-guided generative system called Editing Everything, combining SAM, CLIP and Stable Diffusion (SD) [14].",
            "reference_string": "[258352755 | Xie et al. | 2023 | Citations: 32]"
        },
        {
            "title": "Denoising Diffusion Models for Plug-and-Play Image Restoration",
            "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "year": 2023,
            "reference_count": 62,
            "citation_count": 219,
            "influential_citation_count": 33,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.08995",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.08995, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "Yuanzhi Zhu"
                },
                {
                    "authorId": "144110274",
                    "name": "K. Zhang"
                },
                {
                    "authorId": "145270228",
                    "name": "Jingyun Liang"
                },
                {
                    "authorId": "32879676",
                    "name": "Jiezhang Cao"
                },
                {
                    "authorId": "1766554",
                    "name": "B. Wen"
                },
                {
                    "authorId": "1732855",
                    "name": "R. Timofte"
                },
                {
                    "authorId": "1681236",
                    "name": "L. Gool"
                }
            ],
            "abstract": "Plug-and-play Image Restoration (IR) has been widely recognized as a flexible and interpretable method for solving various inverse problems by utilizing any off-the-shelf denoiser as the implicit image prior. However, most existing methods focus on discriminative Gaussian denoisers. Although diffusion models have shown impressive performance for high-quality image synthesis, their potential to serve as a generative denoiser prior to the plug-and-play IR methods remains to be further explored. While several other attempts have been made to adopt diffusion models for image restoration, they either fail to achieve satisfactory results or typically require an unacceptable number of Neural Function Evaluations (NFEs) during inference. This paper proposes DiffPIR, which integrates the traditional plug-and-play method into the diffusion sampling framework. Compared to plug-and-play IR methods that rely on discriminative Gaussian denoisers, DiffPIR is expected to inherit the generative ability of diffusion models. Experimental results on three representative IR tasks, including super-resolution, image deblurring, and inpainting, demonstrate that DiffPIR achieves state-of-the-art performance on both the FFHQ and ImageNet datasets in terms of reconstruction faithfulness and perceptual quality with no more than 100 NFEs. The source code is available at https://github.com/yuanzhi-zhu/DiffPIR",
            "corpus_id": 258714952,
            "sentences": [
                {
                    "corpus_id": "258714952",
                    "title": "Denoising Diffusion Models for Plug-and-Play Image Restoration",
                    "text": "For conditional generation tasks given the condition y, the goal is to sample images from the posterior distribution p(x|y). In the work of Song et al. [53], (3) can be rewritten as follows for conditional generation with the help of Bayes' theorem \n\nwhere the posterior is divided into p t (x) and p t (y|x). In this way, the unconditional pre-trained diffusion models can be used for conditional generation with an additional classifier. \n\nHo et al. [25] introduced the classifier-free diffusion guidance with s \u03b8 (x, t, y) = \u2207 x log p t (x|y) the imageconditional diffusion models. With the same idea, Saharia et al. [48,49] trained image-conditional diffusion models for SR and image-to-image translation in concurrent work. Nichol et al. [40] proposed to use text-guided diffusion models to generate photo-realistic images with classifierfree guidance. The hyperparameter \u03bb in (1) can be interpreted as the guidance scale in classifier-free diffusion models. \n\nWhile the above methods need to train a diffusion model from scratch, conditional generation can also be done with unconditional pre-trained diffusion models [7,8,32,38]. Given (9), we can first update with one unconditional reverse diffusion step and then incorporate the conditional information.",
                    "score": 0.7893906347632174,
                    "section_title": "Conditional Diffusion Models",
                    "char_start_offset": 9389,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 124
                        },
                        {
                            "start": 125,
                            "end": 248
                        },
                        {
                            "start": 251,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 439
                        },
                        {
                            "start": 442,
                            "end": 584
                        },
                        {
                            "start": 585,
                            "end": 728
                        },
                        {
                            "start": 729,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 963
                        },
                        {
                            "start": 966,
                            "end": 1136
                        },
                        {
                            "start": 1137,
                            "end": 1263
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 620,
                            "end": 624,
                            "matchedPaperCorpusId": "243938678"
                        },
                        {
                            "start": 1132,
                            "end": 1135,
                            "matchedPaperCorpusId": "246240274"
                        },
                        {
                            "start": 1143,
                            "end": 1146,
                            "matchedPaperCorpusId": "245117332"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9228515625
                }
            ],
            "relevance_judgement": 0.9228515625,
            "relevance_judgment_input_expanded": "# Title: Denoising Diffusion Models for Plug-and-Play Image Restoration\n# Venue: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\n# Authors: Yuanzhi Zhu, K. Zhang, Jingyun Liang, Jiezhang Cao, B. Wen, R. Timofte, L. Gool\n## Abstract\nPlug-and-play Image Restoration (IR) has been widely recognized as a flexible and interpretable method for solving various inverse problems by utilizing any off-the-shelf denoiser as the implicit image prior. However, most existing methods focus on discriminative Gaussian denoisers. Although diffusion models have shown impressive performance for high-quality image synthesis, their potential to serve as a generative denoiser prior to the plug-and-play IR methods remains to be further explored. While several other attempts have been made to adopt diffusion models for image restoration, they either fail to achieve satisfactory results or typically require an unacceptable number of Neural Function Evaluations (NFEs) during inference. This paper proposes DiffPIR, which integrates the traditional plug-and-play method into the diffusion sampling framework. Compared to plug-and-play IR methods that rely on discriminative Gaussian denoisers, DiffPIR is expected to inherit the generative ability of diffusion models. Experimental results on three representative IR tasks, including super-resolution, image deblurring, and inpainting, demonstrate that DiffPIR achieves state-of-the-art performance on both the FFHQ and ImageNet datasets in terms of reconstruction faithfulness and perceptual quality with no more than 100 NFEs. The source code is available at https://github.com/yuanzhi-zhu/DiffPIR\n## Conditional Diffusion Models\nFor conditional generation tasks given the condition y, the goal is to sample images from the posterior distribution p(x|y). In the work of Song et al. [53], (3) can be rewritten as follows for conditional generation with the help of Bayes' theorem \n\nwhere the posterior is divided into p t (x) and p t (y|x). In this way, the unconditional pre-trained diffusion models can be used for conditional generation with an additional classifier. \n\nHo et al. [25] introduced the classifier-free diffusion guidance with s \u03b8 (x, t, y) = \u2207 x log p t (x|y) the imageconditional diffusion models. With the same idea, Saharia et al. [48,49] trained image-conditional diffusion models for SR and image-to-image translation in concurrent work. Nichol et al. [40] proposed to use text-guided diffusion models to generate photo-realistic images with classifierfree guidance. The hyperparameter \u03bb in (1) can be interpreted as the guidance scale in classifier-free diffusion models. \n\nWhile the above methods need to train a diffusion model from scratch, conditional generation can also be done with unconditional pre-trained diffusion models [7,8,32,38]. Given (9), we can first update with one unconditional reverse diffusion step and then incorporate the conditional information.",
            "reference_string": "[258714952 | Zhu et al. | 2023 | Citations: 219]"
        },
        {
            "title": "Predicated Diffusion: Predicate Logic-Based Attention Guidance for Text-to-Image Diffusion Models",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2023,
            "reference_count": 45,
            "citation_count": 8,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2311.16117",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.16117, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268492302",
                    "name": "Kota Sueyoshi"
                },
                {
                    "authorId": "2268495650",
                    "name": "Takashi Matsubara"
                }
            ],
            "abstract": "Diffusion models have achieved remarkable success in generating high-quality, diverse, and creative images. However, in text-based image generation, they often struggle to accurately capture the intended meaning of the text. For instance, a specified object might not be generated, or an adjective might incorrectly alter unintended objects. Moreover, we found that relationships indicating possession between objects are frequently overlooked. Despite the diversity of users' intentions in text, existing methods often focus on only some aspects of these intentions. In this paper, we propose Predicated Diffusion, a unified framework designed to more effectively express users' intentions. It represents the intended meaning as propositions using predicate logic and treats the pixels in attention maps as fuzzy predicates. This approach provides a differentiable loss function that offers guidance for the image generation process to better fulfill the propositions. Comparative evaluations with existing methods demonstrated that Predicated Diffusion excels in generating images faithful to various text prompts, while maintaining high image quality, as validated by human evaluators and pretrained image-text models.",
            "corpus_id": 265466084,
            "sentences": [
                {
                    "corpus_id": "265466084",
                    "title": "Predicated Diffusion: Predicate Logic-Based Attention Guidance for Text-to-Image Diffusion Models",
                    "text": "Training-Free Guidance Even when a diffusion model is designed without condition c, it can reproduce the conditional probability p(x|c) without retraining. This is because, from a diffusion model p(x) and a separate classifier p(c|x) for class label c, one can obtain the gradient of the conditional log-probability, \u2207 x log p(x|c) = \u2207 x log p(c|x) + \u2207 x log p(x). Although grounded in probability theory, what it practically offers is additional guidance \u2207 x log p(c|x) for updating images, which is generalized as classifier guidance [3]. \n\nConsider the reverse process modeled as a Gaussian distribution p( \n\nwhere the parameters are determined by neural networks \u00b5 \u03b8 and \u03a3 \u03b8 . A guidance method can be defined as a method that introduces an adjustment g(x t , t) to the update in the reverse process as \n\nWhen the diffusion model is conditioned on c, the difference between conditional and unconditional updates serves as classifier-free guidance, which can adjust the fidelity of the generated image to condition c [12]. Liu et al. [18] proposed Composable Diffusion, inspired by energy-based models [5]. It generates an image conditioned on two concepts, c 0 and c 1 , by summing their respective conditional updates. It negates or removes a concept c n from generated images by subtracting the update conditioned on c n , termed as a negative prompt. Some studies developed guidance using annotations, such as bounding boxes [19,20,40] and segmentation masks [23]. While effective in intentionally controlling image layout, guidance methods based on annotations sometimes limit the diversity of generated images. \n\nAttention Guidance Other previous studies developed guidance methods using the attention maps of the crossattention mechanism, termed as attention guidance. High pixel intensity in an attention map A w suggests the presence of the corresponding object or concept w at that pixel. Attend-and-Excite enhances the intensity of at least one pixel in the attention map A w to ensure the existence of the corresponding object w (that is, address missing objects) [2].",
                    "score": 0.5539278102987983,
                    "section_title": "Related Work",
                    "char_start_offset": 6145,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 155
                        },
                        {
                            "start": 156,
                            "end": 364
                        },
                        {
                            "start": 365,
                            "end": 540
                        },
                        {
                            "start": 543,
                            "end": 609
                        },
                        {
                            "start": 612,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 806
                        },
                        {
                            "start": 809,
                            "end": 1025
                        },
                        {
                            "start": 1026,
                            "end": 1109
                        },
                        {
                            "start": 1110,
                            "end": 1223
                        },
                        {
                            "start": 1224,
                            "end": 1357
                        },
                        {
                            "start": 1358,
                            "end": 1471
                        },
                        {
                            "start": 1472,
                            "end": 1619
                        },
                        {
                            "start": 1622,
                            "end": 1778
                        },
                        {
                            "start": 1779,
                            "end": 1901
                        },
                        {
                            "start": 1902,
                            "end": 2083
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 536,
                            "end": 539,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 1020,
                            "end": 1024,
                            "matchedPaperCorpusId": "249145348"
                        },
                        {
                            "start": 1037,
                            "end": 1041,
                            "matchedPaperCorpusId": "249375227"
                        },
                        {
                            "start": 1105,
                            "end": 1108,
                            "matchedPaperCorpusId": "214223619"
                        },
                        {
                            "start": 1439,
                            "end": 1442,
                            "matchedPaperCorpusId": "259991581"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9189453125
                }
            ],
            "relevance_judgement": 0.9189453125,
            "relevance_judgment_input_expanded": "# Title: Predicated Diffusion: Predicate Logic-Based Attention Guidance for Text-to-Image Diffusion Models\n# Venue: Computer Vision and Pattern Recognition\n# Authors: Kota Sueyoshi, Takashi Matsubara\n## Abstract\nDiffusion models have achieved remarkable success in generating high-quality, diverse, and creative images. However, in text-based image generation, they often struggle to accurately capture the intended meaning of the text. For instance, a specified object might not be generated, or an adjective might incorrectly alter unintended objects. Moreover, we found that relationships indicating possession between objects are frequently overlooked. Despite the diversity of users' intentions in text, existing methods often focus on only some aspects of these intentions. In this paper, we propose Predicated Diffusion, a unified framework designed to more effectively express users' intentions. It represents the intended meaning as propositions using predicate logic and treats the pixels in attention maps as fuzzy predicates. This approach provides a differentiable loss function that offers guidance for the image generation process to better fulfill the propositions. Comparative evaluations with existing methods demonstrated that Predicated Diffusion excels in generating images faithful to various text prompts, while maintaining high image quality, as validated by human evaluators and pretrained image-text models.\n## Related Work\nTraining-Free Guidance Even when a diffusion model is designed without condition c, it can reproduce the conditional probability p(x|c) without retraining. This is because, from a diffusion model p(x) and a separate classifier p(c|x) for class label c, one can obtain the gradient of the conditional log-probability, \u2207 x log p(x|c) = \u2207 x log p(c|x) + \u2207 x log p(x). Although grounded in probability theory, what it practically offers is additional guidance \u2207 x log p(c|x) for updating images, which is generalized as classifier guidance [3]. \n\nConsider the reverse process modeled as a Gaussian distribution p( \n\nwhere the parameters are determined by neural networks \u00b5 \u03b8 and \u03a3 \u03b8 . A guidance method can be defined as a method that introduces an adjustment g(x t , t) to the update in the reverse process as \n\nWhen the diffusion model is conditioned on c, the difference between conditional and unconditional updates serves as classifier-free guidance, which can adjust the fidelity of the generated image to condition c [12]. Liu et al. [18] proposed Composable Diffusion, inspired by energy-based models [5]. It generates an image conditioned on two concepts, c 0 and c 1 , by summing their respective conditional updates. It negates or removes a concept c n from generated images by subtracting the update conditioned on c n , termed as a negative prompt. Some studies developed guidance using annotations, such as bounding boxes [19,20,40] and segmentation masks [23]. While effective in intentionally controlling image layout, guidance methods based on annotations sometimes limit the diversity of generated images. \n\nAttention Guidance Other previous studies developed guidance methods using the attention maps of the crossattention mechanism, termed as attention guidance. High pixel intensity in an attention map A w suggests the presence of the corresponding object or concept w at that pixel. Attend-and-Excite enhances the intensity of at least one pixel in the attention map A w to ensure the existence of the corresponding object w (that is, address missing objects) [2].",
            "reference_string": "[265466084 | Sueyoshi et al. | 2023 | Citations: 8]"
        },
        {
            "title": "Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator",
            "venue": "ACM Multimedia",
            "year": 2023,
            "reference_count": 45,
            "citation_count": 10,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.06710",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.06710, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46509200",
                    "name": "Jing Zhao"
                },
                {
                    "authorId": "28331771",
                    "name": "Heliang Zheng"
                },
                {
                    "authorId": "2518211",
                    "name": "Chaoyue Wang"
                },
                {
                    "authorId": "2156125124",
                    "name": "Long Lan"
                },
                {
                    "authorId": "3441469",
                    "name": "Wanrong Huang"
                },
                {
                    "authorId": "2120811655",
                    "name": "Wenjing Yang"
                }
            ],
            "abstract": "Classifier-free guidance is an effective sampling technique in diffusion models that has been widely adopted. The main idea is to extrapolate the model in the direction of text guidance and away from null-text guidance. In this paper, we demonstrate that null-text guidance in diffusion models is secretly a cartoon-style creator, i.e., the generated images can be efficiently transformed into cartoons by simply perturbing the null-text guidance. Specifically, we proposed two disturbance methods, i.e., Rollback disturbance (Back-D) and Image disturbance (Image-D), to construct misalignment between the noisy images used for predicting null-text guidance and text guidance (subsequently referred to as null-text noisy image and text noisy imageb respectively) in the sampling process. Back-D achieves cartoonization by altering the noisb level of the null-text noisy image via replacing xt with xl + \u0394 t. Image-D, alternatively, produces high-fidelity, diverse cartoons by defining xt as a clean input image, which further improves the incorporation of finer image details. Through comprehensive experiments, we delved into the principle of noise disturbing for null-text and uncovered that the efficacy of disturbance depends on the correlation between the null-text noisy image and the source image. Moreover, the proposed methods, which can generate cartoon images and cartoonize specific ones, are training-free and easily integrated as a plug-and-play component in any classifier-free guided diffusion model. The project page is available at https://nulltextforcartoon.github.io/.",
            "corpus_id": 258615416,
            "sentences": [
                {
                    "corpus_id": "258615416",
                    "title": "Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator",
                    "text": "Classifier-free guidance [13] is a powerful sampling technique as it directs the model towards text guidance and away from null-text guidance by introducing a null-text guidance term. Compared to the previous study, classifier guidance [6], which utilizes a separate classifier to trade off Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) via truncation or low-temperature sampling, classifierfree guidance can be easily implemented and applied. Specifically, classifier-free guidance trains an unconditional denoising diffusion model together with the conditional model and updates the prediction of noise by increasing the distance between target noise and null-text noise. The utilization of classifier-free guidance has greatly enhanced the caliber of generated images and has become ubiquitous in subsequent studies [7, 8, 10, 11, 15-17, 20, 25, 26, 33, 39]. Based on classifier-free guidance, stable diffusion [23] has facilitated the training of diffusion models on restricted computational resources, whilst preserving their quality and adaptability by deploying them within the potent pretrained autoencoder's latent space. This has resulted in marking new milestones in the realm of image inpainting and class-conditional image synthesis whilst exhibiting exceptionally competitive performance across multiple tasks. Considering this, our study endeavors to further explore the potency of classifierfree guidance in conjunction with stable diffusion acting as the cornerstone of our investigation.",
                    "score": 0.6150969249390097,
                    "section_title": "RELATED WORKS 2.1 Classifier-free guidance",
                    "char_start_offset": 4609,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 183
                        },
                        {
                            "start": 184,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 685
                        },
                        {
                            "start": 686,
                            "end": 873
                        },
                        {
                            "start": 874,
                            "end": 1142
                        },
                        {
                            "start": 1143,
                            "end": 1336
                        },
                        {
                            "start": 1337,
                            "end": 1517
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 236,
                            "end": 239,
                            "matchedPaperCorpusId": "234357997"
                        },
                        {
                            "start": 926,
                            "end": 930,
                            "matchedPaperCorpusId": "245335280"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91796875
                }
            ],
            "relevance_judgement": 0.91796875,
            "relevance_judgment_input_expanded": "# Title: Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator\n# Venue: ACM Multimedia\n# Authors: Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wanrong Huang, Wenjing Yang\n## Abstract\nClassifier-free guidance is an effective sampling technique in diffusion models that has been widely adopted. The main idea is to extrapolate the model in the direction of text guidance and away from null-text guidance. In this paper, we demonstrate that null-text guidance in diffusion models is secretly a cartoon-style creator, i.e., the generated images can be efficiently transformed into cartoons by simply perturbing the null-text guidance. Specifically, we proposed two disturbance methods, i.e., Rollback disturbance (Back-D) and Image disturbance (Image-D), to construct misalignment between the noisy images used for predicting null-text guidance and text guidance (subsequently referred to as null-text noisy image and text noisy imageb respectively) in the sampling process. Back-D achieves cartoonization by altering the noisb level of the null-text noisy image via replacing xt with xl + \u0394 t. Image-D, alternatively, produces high-fidelity, diverse cartoons by defining xt as a clean input image, which further improves the incorporation of finer image details. Through comprehensive experiments, we delved into the principle of noise disturbing for null-text and uncovered that the efficacy of disturbance depends on the correlation between the null-text noisy image and the source image. Moreover, the proposed methods, which can generate cartoon images and cartoonize specific ones, are training-free and easily integrated as a plug-and-play component in any classifier-free guided diffusion model. The project page is available at https://nulltextforcartoon.github.io/.\n## RELATED WORKS 2.1 Classifier-free guidance\nClassifier-free guidance [13] is a powerful sampling technique as it directs the model towards text guidance and away from null-text guidance by introducing a null-text guidance term. Compared to the previous study, classifier guidance [6], which utilizes a separate classifier to trade off Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) via truncation or low-temperature sampling, classifierfree guidance can be easily implemented and applied. Specifically, classifier-free guidance trains an unconditional denoising diffusion model together with the conditional model and updates the prediction of noise by increasing the distance between target noise and null-text noise. The utilization of classifier-free guidance has greatly enhanced the caliber of generated images and has become ubiquitous in subsequent studies [7, 8, 10, 11, 15-17, 20, 25, 26, 33, 39]. Based on classifier-free guidance, stable diffusion [23] has facilitated the training of diffusion models on restricted computational resources, whilst preserving their quality and adaptability by deploying them within the potent pretrained autoencoder's latent space. This has resulted in marking new milestones in the realm of image inpainting and class-conditional image synthesis whilst exhibiting exceptionally competitive performance across multiple tasks. Considering this, our study endeavors to further explore the potency of classifierfree guidance in conjunction with stable diffusion acting as the cornerstone of our investigation.",
            "reference_string": "[258615416 | Zhao et al. | 2023 | Citations: 10]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "258564566",
            "title": "Style-A-Video: Agile Diffusion for Arbitrary Text-Based Video Style Transfer",
            "text": "Classifier-free guidance [17] is a method for trading off the quality and diversity of samples generated by a diffusion model. It is commonly used in class-conditional and text-conditional image generation to improve the visual quality of generated images and to make sampled images better correspond with their conditioning. Classifier-free guidance effectively shifts probability mass toward \n\nGuidance for conditions. For our task, the scoring network \u03b5  (  ,   ,  T ) has three conditions: the input image   , text prompt  T , and self-attention map   . We find it beneficial to leverage classifier-free guidance: \n\nconcerning each condition. Liu et al. [32] demonstrate that a conditional diffusion model can compose score estimates from multiple different conditioning values. We introduce three guidance scales,   ,  T , and   , which can be adjusted to trade off how strongly the generated samples correspond with the conditions. Our modified score estimate is as follows:",
            "score": 0.8165465773768726,
            "section_title": "Condition Guidance",
            "char_start_offset": 15784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 980
                }
            ],
            "ref_mentions": [
                {
                    "start": 658,
                    "end": 662,
                    "matchedPaperCorpusId": "249375227"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94091796875
        },
        {
            "corpus_id": "271212594",
            "title": "VividDreamer: Invariant Score Distillation For Hyper-Realistic Text-to-3D Generation",
            "text": "Recently, significant progress has been made in text-to-image generation [22,40,41,59,62].Among these, diffusion models [11,18,30,45,47] are particularly notable for their outstanding performance.These models, once trained, are capable of transforming Gaussian noise into high-definition, realistic samples through multiple iterations.To enable conditional image generation, Dhariwal et al .[8] introduces classifier guidance.This approach requires training an additional noisy image classifier, thereby needing extra gradient guidance during sampling, and limiting generated image variety.To address these issues, Ho et al .proposes classifier-free guidance [12], eliminating the need for an explicit classifier.This method trains the diffusion model in both conditioned and unconditioned modes, it modifies the model by Eq. 1:\n\nwhere \u2205 is an empty text prompt representing the unconditional case, and w is a guidance scale, used to control the influence of conditional information.Building upon this methodology, the introduction of models such as GLIDE [34], Imagen [42], and Stable Diffusion [41] have propelled diffusion models to excel in the text-to-image generation domain.Recently, the influence of these models has also extended to text-to-3D generation.",
            "score": 0.8156415658551491,
            "section_title": "Text to image Generation",
            "char_start_offset": 4338,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 90,
                    "end": 196
                },
                {
                    "start": 196,
                    "end": 335
                },
                {
                    "start": 335,
                    "end": 391
                },
                {
                    "start": 391,
                    "end": 426
                },
                {
                    "start": 426,
                    "end": 590
                },
                {
                    "start": 590,
                    "end": 625
                },
                {
                    "start": 625,
                    "end": 713
                },
                {
                    "start": 713,
                    "end": 828
                },
                {
                    "start": 830,
                    "end": 983
                },
                {
                    "start": 983,
                    "end": 1181
                },
                {
                    "start": 1181,
                    "end": 1264
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 83,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 83,
                    "end": 86,
                    "matchedPaperCorpusId": "267547419"
                },
                {
                    "start": 86,
                    "end": 89,
                    "matchedPaperCorpusId": "258960308"
                },
                {
                    "start": 120,
                    "end": 124,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 124,
                    "end": 127,
                    "matchedPaperCorpusId": "249240415"
                },
                {
                    "start": 127,
                    "end": 130,
                    "matchedPaperCorpusId": "249282317"
                },
                {
                    "start": 391,
                    "end": 394,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1069,
                    "end": 1073,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1096,
                    "end": 1100,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.916015625
        },
        {
            "corpus_id": "249926846",
            "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
            "text": "Classifier-free guidance [37] (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts. \n\nWith batch-sampled images per text prompt, contrastive reranking is used in DALL-E [2] which produces image-text alignment scores after the generation. We apply contrastive reranking in our work and find it is complementary to classifier-free guidance. Compared with the 512 images used in DALL-E [2], we sample just 16 images per text prompt for the experiments reported in this paper. We rerank each output set based on the alignment score of image and text embedding of a Contrastive Captioners model (CoCa) [25]. A CoCa base-size model (Table 1 in [25]) is trained on the same dataset with details in Section 4.1.",
            "score": 0.8135096275324569,
            "section_title": "Classifier-Free Guidance and Reranking",
            "char_start_offset": 15352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2155
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1621,
                    "end": 1624,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1835,
                    "end": 1838,
                    "matchedPaperCorpusId": "232035663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95263671875
        },
        {
            "corpus_id": "258714952",
            "title": "Denoising Diffusion Models for Plug-and-Play Image Restoration",
            "text": "For conditional generation tasks given the condition y, the goal is to sample images from the posterior distribution p(x|y). In the work of Song et al. [53], (3) can be rewritten as follows for conditional generation with the help of Bayes' theorem \n\nwhere the posterior is divided into p t (x) and p t (y|x). In this way, the unconditional pre-trained diffusion models can be used for conditional generation with an additional classifier. \n\nHo et al. [25] introduced the classifier-free diffusion guidance with s \u03b8 (x, t, y) = \u2207 x log p t (x|y) the imageconditional diffusion models. With the same idea, Saharia et al. [48,49] trained image-conditional diffusion models for SR and image-to-image translation in concurrent work. Nichol et al. [40] proposed to use text-guided diffusion models to generate photo-realistic images with classifierfree guidance. The hyperparameter \u03bb in (1) can be interpreted as the guidance scale in classifier-free diffusion models. \n\nWhile the above methods need to train a diffusion model from scratch, conditional generation can also be done with unconditional pre-trained diffusion models [7,8,32,38]. Given (9), we can first update with one unconditional reverse diffusion step and then incorporate the conditional information.",
            "score": 0.7893906347632174,
            "section_title": "Conditional Diffusion Models",
            "char_start_offset": 9389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 963
                },
                {
                    "start": 966,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1263
                }
            ],
            "ref_mentions": [
                {
                    "start": 620,
                    "end": 624,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 1132,
                    "end": 1135,
                    "matchedPaperCorpusId": "246240274"
                },
                {
                    "start": 1143,
                    "end": 1146,
                    "matchedPaperCorpusId": "245117332"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9228515625
        },
        {
            "corpus_id": "245335086",
            "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
            "text": "Ho & Salimans (2021) achieved similar results without a separately trained classifier through the use of classifier-free guidance, a form of guidance that interpolates between predictions from a diffusion model with and without labels. \n\nMotivated by the ability of guided diffusion models to generate photorealistic samples and the ability of text-to-image models to handle free-form prompts, we apply guided diffusion to the problem of text-conditional image synthesis. First, we train a 3.5 billion parameter diffusion model that uses a text encoder to condition on natural language descriptions. Next, we compare two techniques for guiding diffusion models towards text prompts: CLIP guidance and classifier-free guidance. Using human and automated evaluations, we find that classifier-free guidance yields higherquality images. \n\nWe find that samples from our model generated with classifier-free guidance are both photorealistic and reflect a wide breadth of world knowledge. When evaluated by human judges, our samples are preferred to those from DALL-E (Ramesh et al., 2021) 87% of the time when evaluated for photorealism, and 69% of the time when evaluated for caption similarity. \n\n\"a hedgehog using a calculator\" \n\n\"a corgi wearing a red bowtie and a purple party hat\" \n\n\"robots meditating in a vipassana retreat\" \"a fall landscape with a small cottage next to a lake\" \"a surrealist dream-like oil painting by salvador dal\u00ed of a cat playing checkers\" \"a professional photo of a sunset behind the grand canyon\" \n\n\"a high-quality oil painting of a psychedelic hamster dragon\" \n\n\"an illustration of albert einstein wearing a superhero costume\" \n\n\"a boat in the canals of venice\" \"a painting of a fox in the style of starry night\" \"a red cube on top of a blue cube\" \"a stained glass window of a panda eating bamboo\" \"a crayon drawing of a space elevator\" \"a futuristic city in synthwave style\" \"a pixel art corgi pizza\" \"a fog rolling into new york\" \n\nFigure 1. Selected samples from GLIDE using classifier-free guidance. We observe that our model can produce photorealistic images with shadows and reflections, can compose multiple concepts in the correct way, and can produce artistic renderings of novel concepts.",
            "score": 0.7803091933829204,
            "section_title": "Introduction",
            "char_start_offset": 1847,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 238,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1280
                },
                {
                    "start": 1283,
                    "end": 1521
                },
                {
                    "start": 1524,
                    "end": 1585
                },
                {
                    "start": 1588,
                    "end": 1652
                },
                {
                    "start": 1655,
                    "end": 1957
                },
                {
                    "start": 1960,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2224
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9560546875
        },
        {
            "corpus_id": "270923987",
            "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models",
            "text": "Diffusion models have recently emerged as the main methodology behind many successful generative models [39,14,9,33,42,43].At the core of such models lies a diffusion process that gradually adds noise to the data until data points are indistinguishable from pure noise.At inference, a denoiser is used to gradually remove noise from samples until we reach a generated data point.While the theory suggests that standard sampling from diffusion models should yield high-quality images, this does not generally hold in practice, and guidance methods are often required to increase the quality of generations, albeit at the expense of less diversity [9,13,35].Classifier guidance [9] was the first method to introduce this concept by utilizing the gradient of a classifier trained on noisy images to increase the class-likelihood of generated samples.Later, classifier-free guidance (CFG) [13] was proposed, allowing the diffusion model to simulate the same behavior as classifier guidance without using an explicit classifier.Since then, CFG has been applied to other conditional generation tasks, such as text-to-image synthesis [28] and text-to-3D generation [31].\n\nIn addition to CFG's trading diversity for quality, it has two practical limitations.First, it requires the underlying model to be trained in a specific way to also learn the unconditional score function, typically by replacing the conditioning vector with a null vector with probability p (usually p \u2208 [0.1.0.2]).This additional step hinders training efficiency, as the model needs to be trained on two different tasks.Replacing the condition might also not be straightforward when the model is multimodal and uses different conditioning signals such as text, images, and audio at the same time, or when the null vector (which is usually the zero vector in practice) has a specific meaning.Second, it is not clear how to extend the benefits of classifier-free guidance beyond conditional models to unconditional generation.\n\nIn this paper, we analyze the methodology behind classifier-free guidance and show theoretically that similar behavior can be achieved without additional training of an unconditional model.The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score.",
            "score": 0.7748687727355899,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 123,
                    "end": 269
                },
                {
                    "start": 269,
                    "end": 379
                },
                {
                    "start": 379,
                    "end": 656
                },
                {
                    "start": 656,
                    "end": 847
                },
                {
                    "start": 847,
                    "end": 1023
                },
                {
                    "start": 1023,
                    "end": 1163
                },
                {
                    "start": 1165,
                    "end": 1250
                },
                {
                    "start": 1250,
                    "end": 1479
                },
                {
                    "start": 1479,
                    "end": 1585
                },
                {
                    "start": 1585,
                    "end": 1856
                },
                {
                    "start": 1856,
                    "end": 1989
                },
                {
                    "start": 1991,
                    "end": 2180
                },
                {
                    "start": 2180,
                    "end": 2341
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 111,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 111,
                    "end": 113,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 113,
                    "end": 116,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 116,
                    "end": 119,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 119,
                    "end": 122,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 646,
                    "end": 649,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 652,
                    "end": 655,
                    "matchedPaperCorpusId": "264490969"
                },
                {
                    "start": 676,
                    "end": 679,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1127,
                    "end": 1131,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 1158,
                    "end": 1162,
                    "matchedPaperCorpusId": "252596091"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93798828125
        },
        {
            "corpus_id": "272398236",
            "title": "Exploring Low-Dimensional Subspaces in Diffusion Models for Controllable Image Editing",
            "text": "Models & Classifier-Free Guidance. So far, our discussion has only focused on unconditional diffusion models. Moreover, our approach can be generalized from unconditional diffusion models to T2I diffusion models [38,4,48,19], where the latter enables controllable image generation x 0 guided by a text prompt c. In more detail, when training T2I diffusion models, we optimize a conditional denoising function \u03f5 \u03b8 (x t , t, c). For sampling, we employ a technique called classifier-free guidance [41], which substitutes the unconditional denoiser \u03f5 \u03b8 (x t , t) in Equation ( 1) with its conditional counterpart \u03b5\u03b8 (x t , t, c) that can be described as follows: \n\nHere, \u2205 denotes the empty prompt and \u03b7 > 0 denotes the strength for the classifier-free guidance.",
            "score": 0.7620938525900778,
            "section_title": "Text-to-image (T2I) Diffusion",
            "char_start_offset": 9191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 659
                },
                {
                    "start": 662,
                    "end": 759
                }
            ],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 218,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 218,
                    "end": 221,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 221,
                    "end": 224,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 495,
                    "end": 499,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "260886956",
            "title": "LAW-Diffusion: Complex Scene Generation by Diffusion with Layouts",
            "text": "Conditional Diffusion Models Classifier-guidance [4] provides a way for diffusion model to achieve conditional generation by using the gradient of a separately trained classifier p(y|x t ) during sampling. As a more efficient technique, classifier-free guidance [14, 24] replaces the noise estimator by a combination of conditional and unconditional model, without requirement of p(y|x t ): \n\nwhere y is the class label or text embedding from language model [24], \u03c9 \u2265 1 denotes the guidance scale and trivially increasing \u03c9 will amplify the effect of conditional input. \n\nWith the help of large-scale pre-trained CLIP [25] and other language models [31], diffusion models produce impressive results on text-to-image generation. However, their performance of complex scene generation are always unsatisfactory because the text embeddings from the linguistic models can not accurately capture the spatial properties, e.g., objects' locations, sizes and their implicit spatial associations. Distinct from text prompts, we focus on the task of generating complex scene images from the structured layout configurations (L2I) and further propose a diffusion modelbased method with flexibility and compositionality.",
            "score": 0.745175143603323,
            "section_title": "Preliminaries",
            "char_start_offset": 11621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1208
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 52,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 618,
                    "end": 622,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.951171875
        },
        {
            "corpus_id": "253581213",
            "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
            "text": "Classifier-free diffusion guidance [20] is a method for trading off the quality and diversity of samples generated by a diffusion model. It is commonly used in class-conditional and text-conditional image generation to improve the visual quality of generated images and to make sampled images better correspond with their conditioning. Classifierfree guidance effectively shifts probability mass toward data where an implicit classifier p \u03b8 (c|z t ) assigns high likelihood to the conditioning c. Our model is therefore capable of conditional or unconditional denoising with respect to both or either conditional inputs. We introduce two guidance scales, s I and s T , which can be adjusted to trade off how strongly the generated samples correspond with the input image and how strongly they correspond with the edit instruction. Our modified score estimate is as follows: \n\nIn Figure 4, we show the effects of these two parameters on generated samples. See Appendix B for details of our classifier-free guidance formulation.",
            "score": 0.739921017679735,
            "section_title": "Classifier-free Guidance for Two Conditionings",
            "char_start_offset": 14318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9296875
        },
        {
            "corpus_id": "267547881",
            "title": "SPAD: Spatially Aware Multi-View Diffusers",
            "text": "Classifier-free diffusion guidance [33] is a technique used to balance the quality and diversity of images produced by diffusion models. This method is particularly effective in class-conditional and text-conditional image generation, enhancing both the visual quality of images and their alignment with given conditions. Inspired by [5] we explore the integration of classifier-free guidance with Epipolar Attention and Pl\u00fccker Embedding. Implementing classifierfree guidance involves simultaneous training of the diffusion model for both conditional and unconditional denoising tasks. During inference, these models' score estimates are merged. We have four different types of conditioning injected into our system: Outcome: As shown in Fig. 13, we find that classifier-free guidance beyond text conditioning does not provide additional benefits, and rather leads to over-saturated generations. This also aligns with our observations on MVDream.",
            "score": 0.7284085479189819,
            "section_title": "B.4. Classifier-free Guidance",
            "char_start_offset": 34445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 947
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 39,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 334,
                    "end": 337,
                    "matchedPaperCorpusId": "253581213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "271709598",
            "title": "ProCreate, Don't Reproduce! Propulsive Energy Diffusion for Creative Generation",
            "text": "In this section, we review related areas of work in guided diffusion models, few-shot image generation, sample diversity, and data replication issues. Guided Diffusion Models. Conditioning provides a powerful and flexible way to guide diffusion model generation using text [35,40], images [20,29,38], videos [18,22], layout [53], or even scene graphs [11,50] and point clouds [33,51]. Recent works have explored classifier-free guidance and classifier guidance as two methods for class-or text-conditional image generation [8,21]. Classifier-free guidance requires training a new DDPM that accepts an additional conditioning input, but achieves strong performance for conditional image generation tasks, such as conditioning on class [8], prompts or regions of the original image [31], among others. On the other hand, classifier guidance avoids re-training by guiding a pre-trained model using a classifier to guide the sampling process. Recent work applied classifier guidance across a variety of conditioning goals without training new classifiers [3]. DOODL [48] improves the performance of off-the-shell classifier guidance by performing backpropagation through the entire diffusion inference process to optimize the initial noise. Few-Shot Image Generation. Diffusion models can be adapted to a few-shot setting [15], including for customization and personalization [14,36]. Giannone et al . [15] developed a method for few-shot adaptation of diffusion models, but they focused on CIFAR-100 [24], which only contains 32 \u00d7 32 images. In contrast, we collect our own few-shot generation dataset with 512 \u00d7 512 images, where we choose each class to be practical for designers and for the images of each class to contain more conceptual similarities (e.g., Burberry design) than simply belonging to the same semantic category (e.g., horse). Standard fine-tuning, Dreambooth [36], and Textual Inversion [14] all allow for customizing diffusion models. The first two methods require fine-tuning an entire diffusion model to produce images with a consistent subject, which makes it prone to overfitting.",
            "score": 0.7239686062006154,
            "section_title": "Related Work",
            "char_start_offset": 4266,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 293,
                    "matchedPaperCorpusId": "235619773"
                },
                {
                    "start": 380,
                    "end": 383,
                    "matchedPaperCorpusId": "252872881"
                },
                {
                    "start": 523,
                    "end": 526,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 734,
                    "end": 737,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84326171875
        },
        {
            "corpus_id": "266362385",
            "title": "Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint Method",
            "text": "Diffusion models are powerful generative models that exhibit impressive performances across different modality generation, including image [5,11,12], video [22,34,39] and audio generation [16]. Guided sampling, including classifier guidance [5] and classifier-free guidance [11], has been widely used in diffusion models to realize controllable generation, such as text-to-image generation [29], imageto-image generation [24,28], and ControlNet [37]. Guided sampling controls the outputs of generative models by conditioning on various types of signals, such as descriptive text, class labels, and images. \n\nA line of guidance methods involves task-specific training of diffusion models using paired data, i.e., targets and conditions. For instance, classifier guidance [5] combines the score estimation of diffusion models with the gradients of the image classifiers to direct the generation process to produce images corresponding to a particular class. In this way, several image classifiers need to be trained on the noisy states of intermediate generation steps of diffusion models. Alternatively, classifier-free guidance [11] directly trains a new score estimator with conditions and uses a linear combination of conditional and unconditional score estimators for sampling. Although this line of methods can effectively guide diffusion models to generate data satisfying certain properties, they are not sufficiently flexible to adapt to any type of guiding due to the cost of training and the feasibility of collecting paired data. \n\nTo this end, another line of training-free guidance methods has been explored [2,14,36]. In training-free guided sampling, at a certain sampling step t, the guidance function is usually constructed as the gradients of the loss function obtained by the off-the-shelf pre-trained models, such as face-ID detection or aesthetic evaluation models. More specifically, the guidance gradients are computed based on the one-step approximation of denoised images from the noisy samples at certain steps t. Then, gradients are added to corresponding sampling steps as guidance to direct the generation process to the desired results. This line of methods offers greater flexibility by allowing the diffusion models to adapt to a broad spectrum of guidance.",
            "score": 0.7224955166367588,
            "section_title": "Introduction",
            "char_start_offset": 1371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 605
                },
                {
                    "start": 608,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1539
                },
                {
                    "start": 1542,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 142,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 142,
                    "end": 145,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 188,
                    "end": 192,
                    "matchedPaperCorpusId": "256390486"
                },
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 274,
                    "end": 278,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 390,
                    "end": 394,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 421,
                    "end": 425,
                    "matchedPaperCorpusId": "256616002"
                },
                {
                    "start": 425,
                    "end": 428,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 445,
                    "end": 449,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 770,
                    "end": 773,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1128,
                    "end": 1132,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1620,
                    "end": 1623,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 1626,
                    "end": 1629,
                    "matchedPaperCorpusId": "257622962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91064453125
        },
        {
            "corpus_id": "271051241",
            "title": "Segmentation-Free Guidance for Text-to-Image Diffusion Models",
            "text": "Our proposed work falls into the scope of controlled image generation using diffusion models.Controlled image generation can be broadly classified into conditional generation and guided generation.These are discussed as follows.\n\nConditional Generation These category of works generally require training diffusion models from scratch where conditional input can be of the form of prompts [2,12,19,27,29].One of the most popular works [12] proposed use of classifier-free guidance with class labels as prompts.In this work, the diffusion model is trained such that the output is a linear combination between that of conditional and unconditional outputs.The authors of [2] trained a diffusion model, where it is enforced to solve linear inverse problems.This is realized through a guidance function known as linear degradation operator.[19] used classifier-free guidance but extended it to descriptive phrases as prompts.Furthermore, the network was trained to enforce similarity between CLIP [21] representations of images and text.However, the major disadvantage of conditional generation methods is that the diffusion models need to be retrained and hence it is computationally intensive.\n\nGuided Generation In this category, the diffusion model is kept frozen without any re-training.However, the sampling process for image generation is modified using gradients from a guidance function.There are prior works that studied guided image generation using various constraints and guidance functions [6-9, 14, 18, 28].The most popular method in this category is classifier guidance [8].\n\nIn this method, a classifier is trained to distinguish images of different scales.The classifier is used as a guidance function, the gradients of which are used in the sampling process.Alternative methods include [28], where the guidance function is a linear operator.Since gradients of the linear operator are used, components of the images were generated in the null space of the linear operator.However, the use of null space does not naturally extend to non-linear guidance functions.In [6], the authors did an elaborate analyses of multiple simple non-linear guidance functions, e.g.non-linear blurring.The gradient of the non-linear function was calculated on expected denoised images and the sampling process was modified.",
            "score": 0.7210439772878736,
            "section_title": "Related Work",
            "char_start_offset": 4946,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 228
                },
                {
                    "start": 230,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 509
                },
                {
                    "start": 509,
                    "end": 653
                },
                {
                    "start": 653,
                    "end": 753
                },
                {
                    "start": 753,
                    "end": 835
                },
                {
                    "start": 835,
                    "end": 920
                },
                {
                    "start": 920,
                    "end": 1032
                },
                {
                    "start": 1032,
                    "end": 1190
                },
                {
                    "start": 1192,
                    "end": 1287
                },
                {
                    "start": 1287,
                    "end": 1391
                },
                {
                    "start": 1391,
                    "end": 1517
                },
                {
                    "start": 1517,
                    "end": 1585
                },
                {
                    "start": 1587,
                    "end": 1669
                },
                {
                    "start": 1669,
                    "end": 1772
                },
                {
                    "start": 1772,
                    "end": 1855
                },
                {
                    "start": 1855,
                    "end": 1985
                },
                {
                    "start": 1985,
                    "end": 2075
                },
                {
                    "start": 2075,
                    "end": 2175
                },
                {
                    "start": 2175,
                    "end": 2195
                },
                {
                    "start": 2195,
                    "end": 2316
                }
            ],
            "ref_mentions": [
                {
                    "start": 391,
                    "end": 394,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 394,
                    "end": 397,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 400,
                    "end": 403,
                    "matchedPaperCorpusId": "244908890"
                },
                {
                    "start": 434,
                    "end": 438,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 835,
                    "end": 839,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 992,
                    "end": 996,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1800,
                    "end": 1804,
                    "matchedPaperCorpusId": "254125609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9208984375
        },
        {
            "corpus_id": "274233770",
            "title": "AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea",
            "text": "As discussed in Section G.2, we apply classifier-free guidance with respect to three conditionings: the input image c I , the text instruction c T and the visual prompt with task embedding c V . We introduce separate guidance scales s I , s T and s V that enable separately trading off the strength of each conditioning. \n\nWhen ignoring c V , we can have the modified score estimate as InstructPix2Pix [5]: \n\nBelow is the modified score estimate for our model with classifier-free guidance on three conditions (copied from Equation 8): \n\nOur generative model learns P (z|c I , c T ), the probability distribution of image latents z = E(x) conditioned on an input image c I , a text instruction c T and the visual prompt with task embedding c V . We arrive at our particular classifier-free guidance formulation by expressing the conditional probability as follows: \n\nDiffusion models estimate the score [26] of the data distribution, i.e., the derivative of the log probability. Taking the logarithm gives us the following expression: \n\nTaking the derivative and rearranging we attain: \n\nThis corresponds with the terms in our classifier-free guidance formulation in Equation 8.",
            "score": 0.7186702599119789,
            "section_title": "G.3. Classifier-free Guidance Details",
            "char_start_offset": 48136,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 320
                },
                {
                    "start": 323,
                    "end": 406
                },
                {
                    "start": 409,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1178
                }
            ],
            "ref_mentions": [
                {
                    "start": 402,
                    "end": 405,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 903,
                    "end": 907,
                    "matchedPaperCorpusId": "1152227"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84716796875
        },
        {
            "corpus_id": "257505012",
            "title": "Text-to-image Diffusion Models in Generative AI: A Survey",
            "text": "Labels improve image synthesis. Early works on generative adversarial models (GAN) have shown that class labels improve the image synthesis quality [35], [36], [37], [38], [39]. As a pioneering work, Conditional GAN [35] feeds the class label as an additional input layer to the model. Moreover, [40] applies class-conditional normalization statistics in image generation. In addition, AC-GAN [38] explicitly adds an auxiliary classifier loss. In other words, labels can help improve the GAN image synthesis quality by providing a conditional input or guiding the image synthesis via an auxiliary classifier. Following these success practices, [41] introduces class-conditional normalization and an auxiliary classifier into diffusion models. In order to distinguish whether the label information is added as a conditional input or an auxiliary loss with gradients, we follow [41] to define the conditional diffusion model and guided diffusion model as follows. \n\nConditional diffusion model: A conditional diffusion model learns from additional information (e.g., class and text) by taking them as model input. \n\nGuided diffusion model: During the training of a guided diffusion model, the class-induced gradients (e.g. through an auxiliary classfier) are involved in the sampling process. \n\nClassifier-free guidance. Different from classifierguided diffusion model [41] that exploits an additional classfier, it is found in [42] that the guidance can be obtained by the generative model itself without a classifier, termed as classifier-free guidance. Specifically, classifier-free guidance jointly trains a single model with the unconditional score estimator \u03b8 (x) and the conditional \u03b8 (x, c), where c denotes the class label. A null token \u2205 is placed as the class label in the unconditional part, i.e., \u03b8 (x) = \u03b8 (x, \u2205). Experimental results in [42] show that classifier-free guidance achieves a trade-off between quality and diversity similar to that achieved by classifier guidance. Without resorting to a classifier, classifier-free diffusion facilitates more modalities, e.g., text in text-to-image, as guidance.",
            "score": 0.7167706557785839,
            "section_title": "Guidance in diffusion-based image synthesis",
            "char_start_offset": 9118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1111
                },
                {
                    "start": 1114,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1989
                },
                {
                    "start": 1990,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "1099052"
                },
                {
                    "start": 160,
                    "end": 164,
                    "matchedPaperCorpusId": "2023211"
                },
                {
                    "start": 172,
                    "end": 176,
                    "matchedPaperCorpusId": "52889459"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "276647345",
            "title": "Language-Informed Hyperspectral Image Synthesis for Imbalanced-Small Sample Classification via Semi-Supervised Conditional Diffusion Model",
            "text": "where \u03f5 t \u223c N (0, I). Following classifier-free guidance diffusion [83], we set \u03b5\u03b8 = (1 + \u03c9)\u03f5 \u03b8 (z t , t, c) \u2212 \u03c9\u03f5 \u03b8 (z t , t, c = \u2298). c = \u2298 is done by randomly dropping out c during training and replacing it with a learned \"null\" embedding \u2298. Generally, training with classifier-free guidance requires two models: an unconditional generation model and a conditional generation model. However, these two models can be unified into a single model by probabilistically omitting the language condition during training. During inference, the final result can be achieved by linear extrapolation between the conditional and unconditional generations. This allows for adjustment of the generation effect to balance the fidelity and diversity of the generated samples by changing the guidance coefficient \u03c9.",
            "score": 0.7098064619458538,
            "section_title": "1) Forward Diffusion Process:",
            "char_start_offset": 22391,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 799
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8837890625
        },
        {
            "corpus_id": "266375120",
            "title": "Conditional Image Generation with Pretrained Generative Model",
            "text": "In previous work, [4] employed a classifier for image generation conditioned on specific class c, but this necessitated training the classifier on noisy images obtained during the reverse diffusion process. Subsequently, the research community proposed classifier-free guidance methods; however, these methods required training class conditional diffusion models [9]. [2] provided a general comprehensive framework for guidance that is compatible with various user input modalities and doesn't require training of any component. This universal framework offers a flexible framework for guiding the generation process without the need for any training. As per our knowledge, past works have put emphasis only on the quality of guided images and ignored the computational overhead placed by the addition of guidance mechanism. \n\nHere we follow the categorization in [2] dividing prior works into two categories, conditional image generation and guided image generation. The essential difference between these two categories lies in that the first category requires training new diffusion models, and the latter does not need to train new diffusion models which largely reduces the time complexity.",
            "score": 0.7077758987258391,
            "section_title": "Related Work",
            "char_start_offset": 5090,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 824
                },
                {
                    "start": 827,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1195
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "268536735",
            "title": "Diffusion Model for Data-Driven Black-Box Optimization",
            "text": "Diffusion models successfully incorporate diverse guidance in practical applications.For image generation, guiding the backward diffusion process towards higher log probabilities predicted by a classifier (which can be viewed as the reward signal) leads to improved sample quality, where the classifier can either be externally trained, i.e., classifier guidance [Dhariwal and Nichol, 2021] or implicitly specified by a conditioned diffusion model, i.e., classifier-free guidance [Ho and Salimans, 2022].Classifier-free guidance has become a standard technique in the state-of-the-art text-to-image diffusion models [Rombach et al., 2022, Ramesh et al., 2022, Balaji et al., 2022].Other types of guidance are also explored in Nichol et al. [2021], Graikos et al. [2022], Bansal et al. [2023].Similar ideas have been explored in sequence modeling problems.In offline reinforcement learning, Decision Diffuser [Ajay et al., 2023] is a diffusion model trained on offline trajectories and can be conditioned to generate new trajectories with high returns, satisfying certain safety constraints, or composing skills.For discrete generations, Diffusion LM [Li et al., 2022] manages to train diffusion models on discrete text space with an additional embedding layer and a rounding step.The authors further show that gradients of any classifier can be incorporated to control and guide the text generation.These appealing empirical performance raises condensed curiosity of their theoretical underpinnings.",
            "score": 0.7074807071960799,
            "section_title": "Guided Diffusion Models",
            "char_start_offset": 10336,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 85,
                    "end": 504
                },
                {
                    "start": 504,
                    "end": 681
                },
                {
                    "start": 681,
                    "end": 792
                },
                {
                    "start": 792,
                    "end": 855
                },
                {
                    "start": 855,
                    "end": 1111
                },
                {
                    "start": 1111,
                    "end": 1280
                },
                {
                    "start": 1280,
                    "end": 1399
                },
                {
                    "start": 1399,
                    "end": 1499
                }
            ],
            "ref_mentions": [
                {
                    "start": 616,
                    "end": 637,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 771,
                    "end": 791,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 908,
                    "end": 927,
                    "matchedPaperCorpusId": "254044710"
                },
                {
                    "start": 1150,
                    "end": 1167,
                    "matchedPaperCorpusId": "249192356"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "270123253",
            "title": "Text Guided Image Editing with Automatic Concept Locating and Forgetting",
            "text": "The primary goal of our method is to forget specific concepts from diffusion models using only the model's existing knowledge in the inference stage, without requiring any additional external data.Classifier-free diffusion guidance is commonly employed in class-conditional and text-conditional image generation tasks to enhance the visual quality of the generated images and to ensure that the sampled outputs better correspond to their respective conditioning factors.Typically, diffusion models use a guidance signal, where the model is conditioned on the desired concept or attribute to generate samples aligned with that concept (Gandikota et al., 2024).\n\nOur approach employs negative guidance to allow the diffusion model to gradually forget a specified concept in the original image while applying positive guidance for learning text prompts.Combining with Eq 1 and Eq 2, we compute the following score estimate using Classifier-Free Guidance during inference:\n\nwhere c p and c n are input prompt and forgetting concepts, w and \u03b7 are weights for controlling the balance between the positive and negative guidance signals in the final CFG score estimate.A higher value of eta will give more weight to the negative guidance, allowing tuning the level of concept forgetting during the diffusion process.",
            "score": 0.7054797680422977,
            "section_title": "Forgetting Concept for Conditionings",
            "char_start_offset": 14821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 470
                },
                {
                    "start": 470,
                    "end": 659
                },
                {
                    "start": 661,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 968
                },
                {
                    "start": 970,
                    "end": 1161
                },
                {
                    "start": 1161,
                    "end": 1308
                }
            ],
            "ref_mentions": [
                {
                    "start": 634,
                    "end": 658,
                    "matchedPaperCorpusId": "261276613"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "249209915",
            "title": "Guided-TTS 2: A Diffusion Model for High-quality Adaptive Text-to-Speech with Untranscribed Data",
            "text": "Recently, classifier guidance [10,43] and classifier-free guidance [14] are proposed for guiding the diffusion model. The discretized reverse process for each method can be obtained by replacing the unconditional score s(X t ) in Eq. 2 with the modified conditional score \u015d(X t |y), which is defined differently depending on the approach. \n\nClassifier Guidance Song et al. [43] show that it is possible to generate conditional samples using unconditional diffusion model by leveraging the separately trained classifier. Given the unconditional score and the gradient of the log-density of the classifier \u2207 Xt log p t (y|X t ), the conditional score s(X t |y) can be estimated by adding the unconditional score and the classifier gradient. Conditional samples can then be generated by setting the modified conditional score equal to the conditional score as in Eq. 4. \n\nClassifier guidance can also be utilized to improve conditional diffusion models. Dhariwal and Nichol [10] provide an additional classifier gradient to the conditional score, and the modified conditional score accordingly is as follows: \n\n(5) Dhariwal and Nichol [10] show that sample quality can be improved at the expense of diversity if more classifier gradient is provided by increasing the gradient scale \u03b3. By adjusting the gradient scale for the optimal value, they achieve state-of-the-art performance in conditional image generation. [35] shows powerful performance in text-to-image generation task using CLIP and classifier-free guidance.",
            "score": 0.7033766769425945,
            "section_title": "Diffusion Guidance Methods",
            "char_start_offset": 4719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 338
                },
                {
                    "start": 341,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 866
                },
                {
                    "start": 869,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1517
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89501953125
        },
        {
            "corpus_id": "261030400",
            "title": "DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability",
            "text": "Nichol et al. [37] propose to learn the variances of the reverse diffusion process to achieve higher sampling efficiency with fewer forward passes. Dhariwal et al. propose classifier-free guidance to circumvent this problem by jointly training a conditional and an unconditional model and combine the resulting conditional and unconditional scores to achieve the same effect as classifier guidance. Recently, diffusion models are applied to text-toimage generation and achieve appealing generation results [36,19,43,46,45]. Some methods [56,1] make an attempt to unify the generation and discriminative tasks by directly adding a classification head into the UNet structure while our DiffDis formulate the discriminative problem into the powerful diffusion process.",
            "score": 0.7005846155527361,
            "section_title": "Related Work",
            "char_start_offset": 7317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 765
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 18,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 510,
                    "end": 513,
                    "matchedPaperCorpusId": "235619773"
                },
                {
                    "start": 519,
                    "end": 522,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 541,
                    "end": 543,
                    "matchedPaperCorpusId": "244908617"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74365234375
        },
        {
            "corpus_id": "249145348",
            "title": "Classifier-Free Diffusion Guidance",
            "text": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
            "score": 0.699038438696144,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "269982915",
            "title": "CoLay: Controllable Layout Generation through Multi-conditional Latent Diffusion",
            "text": "Conditional diffusion models [14] have demonstrated high effectiveness for controllable generation in various settings, such as image-to-image translation [33], text-to-image generation [32,35] and text-to-video generation [13].By treating any conditional information as an extra input to the model, conditional diffusion models can be trained with the same training objective as their unconditional counterparts.In addition, classifier-free guidance [11] is an effective technique to further improve the sample quality of conditional diffusion models, where a conditional and an unconditional model are jointly estimated, and the model prediction is modified to be a linear combination of two model outputs.\n\nConditional diffusion models have also been generalized to multi-conditional settings.For example, [6,7,27] study the problem of composing models trained with different single conditions to enable multi-conditional generation.However, these methods either require additional MCMC sampling steps leading to slow generation, or strong assumption that the multiple conditions are conditionally independent at all noise levels.For the conditions we study in this paper, such assumption does not hold.Therefore, we directly feed the model with a random subset of the multiple conditions during training, such that during inference, the model can generate samples given an arbitrary subset of conditions.",
            "score": 0.6938959293921527,
            "section_title": "Conditional Diffusion Models",
            "char_start_offset": 6818,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 413
                },
                {
                    "start": 413,
                    "end": 708
                },
                {
                    "start": 710,
                    "end": 796
                },
                {
                    "start": 796,
                    "end": 936
                },
                {
                    "start": 936,
                    "end": 1133
                },
                {
                    "start": 1133,
                    "end": 1206
                },
                {
                    "start": 1206,
                    "end": 1408
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 33,
                    "matchedPaperCorpusId": "235619773"
                },
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 190,
                    "end": 193,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 809,
                    "end": 812,
                    "matchedPaperCorpusId": "257078922"
                },
                {
                    "start": 812,
                    "end": 814,
                    "matchedPaperCorpusId": "252568193"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8076171875
        },
        {
            "corpus_id": "269283056",
            "title": "Analysis of Classifier-Free Guidance Weight Schedulers",
            "text": "Diffusion models have demonstrated prominent generative capabilities in various domains e.g. images (Ho et al., 2020), videos (Luo et al., 2023), acoustic signals (Kang et al., 2023b), or 3D avatars (Chen et al., 2023). Conditional generation with diffusion (e.g. text-conditioned image generation) has been explored in numerous works (Saharia et al., 2022;Ruiz et al., 2023;Balaji et al., 2022), and is achieved in its simplest form by adding an extra condition input to the model (Nichol & Dhariwal, 2021). To increase the influence of the condition on the generation process, Classifier Guidance (Dhariwal & Nichol, 2021) proposes to linearly combine the gradients of a separately trained image classifier with those of a diffusion model. Alternatively, Classifier-Free Guidance (CFG) (Ho & Salimans, 2021) simultaneously trains conditional and unconditional models, and exploits a Bayesian implicit classifier to condition the generation without an external classifier. \n\nIn both cases, a weighting parameter \u03c9 controls the importance of the generative and guidance terms and is directly applied at all timesteps. Varying \u03c9 is a trade-off between fidelity and condition reliance, as an increase in condition reliance often results in a decline in both fidelity and diversity. In some recent literature, the concept of dynamic guidance instead of constant one has been mentioned: MUSE (Chang et al., 2023) observed that a linearly increasing guidance weight could enhance performance and potentially increase diversity. This approach has been adopted in subsequent works, such as in Stable Video Diffusion (Blattmann et al., 2023), and further mentioned in Gao et al. (2023) through an exhaustive search for a parameterized cosine-based curve (pcs4) that performs very well on a specific pair of model and task. Intriguingly, despite the recent appearance of this topic in the literature, none of the referenced studies has conducted any empirical experiments or analyses to substantiate the use of a guidance weight scheduler.",
            "score": 0.693242497136197,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 2030
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 117,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 126,
                    "end": 144,
                    "matchedPaperCorpusId": "257532642"
                },
                {
                    "start": 163,
                    "end": 183,
                    "matchedPaperCorpusId": "253581601"
                },
                {
                    "start": 199,
                    "end": 218,
                    "matchedPaperCorpusId": "254408910"
                },
                {
                    "start": 335,
                    "end": 357,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 357,
                    "end": 375,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 788,
                    "end": 809,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1388,
                    "end": 1408,
                    "matchedPaperCorpusId": "255372955"
                },
                {
                    "start": 1660,
                    "end": 1677,
                    "matchedPaperCorpusId": "257767316"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.875
        },
        {
            "corpus_id": "261557399",
            "title": "Hierarchical Masked 3D Diffusion Model for Video Outpainting",
            "text": "Classifier-free Guidance. Classifier-free guidance [20] has been proven to be effective in diffusion models. Classifier-free guidance improves the results of conditional generation, where the implicit classifier   ( |  ) assigns high probability to the conditioning . In our case, we have two conditional inputs. One is the context information of the video  1 , and the other is the global video clip  2 . We jointly train the unconditional and conditional models by randomly setting  1 and  2 to a fixed null value \u2205 with probabilities  1 and  2 . At inference time, we follow Brooks' [3] approach for two conditional inputs and use the following linear combination of the conditional and unconditional score estimates: \n\nwhere  1 and  2 are the guidance scales. The guidance scales control whether the generated video relies more on the context of the video or on the global frames of the video.",
            "score": 0.6905129669464625,
            "section_title": "3.2.3",
            "char_start_offset": 16857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 897
                }
            ],
            "ref_mentions": [
                {
                    "start": 586,
                    "end": 589,
                    "matchedPaperCorpusId": "253581213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6845703125
        },
        {
            "corpus_id": "274823034",
            "title": "Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion",
            "text": "To allow fine-grained control over the output, guidancebased diffusion [13] incorporates external supervision alongside the original conditioning, using a guidance function that measures whether certain criteria are met. In guided image generation, classifier guidance [14] enables class-conditional outputs from a pretrained, unconditional diffusion model, via gradients from a classifier trained on ImageNet [58] im-ages at different noise scales. Similarly, gradients from a CLIP model [51] trained on noisy images can guide generation toward a user-defined text caption [47]. An alternative, classifier-free guidance [26,47], achieves similar control without training a separate classifier, by parameterizing both conditional and unconditional diffusion models within the same network. The approach is further extended to handle general nonlinear inverse problems [9], using gradients calculated on the expected denoised images. Guidance is commonly framed from a score-based perspective on denoising diffusion [66,67], where an unconditional model approximates the time-dependent score function of the log data distribution. Finally, a variety of universal constraints, such as segmentation masks, image style, and object location, have been applied with SD under a single framework [1], fully exploiting the flexibility and control of diffusion-based image generation.",
            "score": 0.6874599976078084,
            "section_title": "Diffusion Guidance",
            "char_start_offset": 8163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1374
                }
            ],
            "ref_mentions": [
                {
                    "start": 410,
                    "end": 414,
                    "matchedPaperCorpusId": "2930547"
                },
                {
                    "start": 621,
                    "end": 625,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 868,
                    "end": 871,
                    "matchedPaperCorpusId": "252596252"
                },
                {
                    "start": 1019,
                    "end": 1022,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 1288,
                    "end": 1291,
                    "matchedPaperCorpusId": "256846836"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93115234375
        },
        {
            "corpus_id": "263909429",
            "title": "OmniControl: Control Any Joint at Any Time for Human Motion Generation",
            "text": "Recently, the diffusion-based generative model has gained significant attention due to their impressive performance in image generation (Rombach et al., 2022). Diffusion models are well-suited for controlling and conditioning. Typically, there are several methods for conditional generation. Imputation and inpainting (Choi et al., 2021;Chung et al., 2022) fill in missing parts of data with observed data such that the filled-in content is visually consistent with the surrounding area. However, it is difficult when the observed data is in a different space compared to the filling part, e.g., generating images from semantic maps. Classifier guidance (Chung et al., 2022;Dhariwal & Nichol, 2021) exploits training a separate classifier to improve the conditional diffusion generation model. Classifierfree guidance (Ho & Salimans, 2021) jointly trains conditional and unconditional diffusion models and combines them to attain a trade-off between sample quality and diversity. GLIGEN (Li et al., 2023b) adds a trainable gated self-attention layer at each transformer block to absorb new grounding input. ControlNet (Zhang et al., 2023b)",
            "score": 0.6869431125004333,
            "section_title": "CONTROLLABLE DIFFUSION-BASED GENERATIVE MODEL IN IMAGE GENERATION",
            "char_start_offset": 8082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1139
                }
            ],
            "ref_mentions": [
                {
                    "start": 136,
                    "end": 158,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 318,
                    "end": 337,
                    "matchedPaperCorpusId": "236950721"
                },
                {
                    "start": 818,
                    "end": 839,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 987,
                    "end": 1005,
                    "matchedPaperCorpusId": "255942528"
                },
                {
                    "start": 1118,
                    "end": 1139,
                    "matchedPaperCorpusId": "256827727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.748046875
        },
        {
            "corpus_id": "260900063",
            "title": "DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models",
            "text": "Our method is based on three significant techniques: classifier-guidance [42], classifier-free guidance [16], and DDIM inversion [40]. The first two pertain to label conditioning methods in diffusion, whereas the last one is associated with image conditioning. We provide a concise overview of these techniques. \n\nConditional Diffusion Models. As a member of generative models, diffusion models generate images (x 0 ) through a multi-step denoising (reverse) process starting from Gaussian noise (x T ). This process was first formulated as a Markov process by Ho et al. [15] with the following forward (diffusion) process: \n\nwhere \n\nand the decreasing sequence \u03b1 1:T \u2208 (0, 1] T is the transition coefficient. After refactoring the process to be non-Markov, Song et al. [40] proposed a skip-step sampling strategy to speedup the generation, as in Eq. ( 3), where t \u2208 [1, ..., T ], \u03f5 t \u223c N (0, I) is the standard Gaussian noise independent of x t , and \u03f5 \n\n\u03b8 is the estimated noise by the model \u03b8 at timestep t. The sampling process can be performed on any sub-sequence t \u2208 \u03c4 \u2282 [1, ..., T ]. \n\nUnder this formulation, there are two ways to apply the label semantic condition y to the generation process: classifier guidance and classifier-free guidance. For classifier guidance [42,4], the condition-guided noise prediction \u03b5(x t ) is given by (we omit \u03b8 and t in \u03f5(\u2022)): \n\nwhere log p \u03d5 is given by a classifier trained on noisy data x t , and s is to adjust the guidance scale (i.e., strength of the guidance). For classifier-free guidance [16,33], a conditional diffusion model \u03b5(t) \u03b8 (x t , y) is trained. The training objective is the same as vanilla diffusion models, but \u03f5 changes to \u03b5 during inference as follows (we omit \u03b8 and t): \n\nwhere \u03c9 is to adjust the guidance scale. Both classifier guidance and classifier-free guidance are qualified for conditional generation. \n\nThe Inversion Problem of Diffusion Models.",
            "score": 0.685288468754676,
            "section_title": "Preliminaries",
            "char_start_offset": 8732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 311
                },
                {
                    "start": 314,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1090
                },
                {
                    "start": 1093,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1369
                },
                {
                    "start": 1372,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1737
                },
                {
                    "start": 1740,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1876
                },
                {
                    "start": 1879,
                    "end": 1921
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 77,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 104,
                    "end": 108,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 129,
                    "end": 133,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 571,
                    "end": 575,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 770,
                    "end": 774,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 1277,
                    "end": 1281,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 1281,
                    "end": 1283,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1540,
                    "end": 1544,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1544,
                    "end": 1547,
                    "matchedPaperCorpusId": "245335086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "271051241",
            "title": "Segmentation-Free Guidance for Text-to-Image Diffusion Models",
            "text": "Diffusion models are powerful generative models for creating visual content from textual prompts.Their success stems from extensive training data and their ability to handle various modalities and signals, enabling diverse applications such as content editing, inpainting, and personalization.\n\nControlling a diffusion model can be achieved primarily in two ways -conditioning and guidance.When a diffusion model is conditioned, it is typically trained to accept a particular form of additional conditioning input, such as a text prompt, image edges, segmentation map, and class labels.However, adapting the model to a different condition often necessitates retraining from scratch.This reliance on expensive retraining poses challenges for end-users seeking to adopt and employ conditioning techniques to control diffusion models.\n\nAn alternative way to control a diffusion model is through a guidance mechanism.Unlike conditioning techniques, this approach does not rely on an external conditioning signal.Instead, it associates a guidance function with the diffusion model to fulfill a specific target criterion, which could be as simple as minimizing the CLIP distance between the generated image and the provided text description.When sampling an image, the reverse process iterations are steered in the direction of the guidance function's gradient, resulting in constrained image generation.\n\nWhen comparing control techniques for diffusion models, guidance emerges as a more versatile approach.It treats the diffusion network as a foundational model which can accommodate different use cases.An earlier method in this domain involved classifier guidance [8], where an explicit classifier functioned as the guidance mechanism.This method utilized the classifier's gradients to drive the image generation process.However, classifier guidance has transitioned to classifier-free guidance [12], eliminating the need for an explicit classifier.In classifier-free guidance approaches, the network is trained to adapt class-label information and conditioning signals without relying on a fixed network architecture.\n\nIn this paper, we propose enhancing image generation quality beyond classifier-free guidance by introducing a novel and universal segmentation-free guidance approach.This methodology aims to improve image quality of diffusion models without necessitating costly retraining, architectural changes, or additional computing during inference.\n\nImage generation using classifier-free guidance involves two forward passes of the diffusion network per iteration: one that uses conditional information and one that does not.",
            "score": 0.6843670956688026,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 293
                },
                {
                    "start": 295,
                    "end": 390
                },
                {
                    "start": 390,
                    "end": 586
                },
                {
                    "start": 586,
                    "end": 682
                },
                {
                    "start": 682,
                    "end": 831
                },
                {
                    "start": 833,
                    "end": 913
                },
                {
                    "start": 913,
                    "end": 1008
                },
                {
                    "start": 1008,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1398
                },
                {
                    "start": 1400,
                    "end": 1502
                },
                {
                    "start": 1502,
                    "end": 1600
                },
                {
                    "start": 1600,
                    "end": 1733
                },
                {
                    "start": 1733,
                    "end": 1819
                },
                {
                    "start": 1819,
                    "end": 1947
                },
                {
                    "start": 1947,
                    "end": 2116
                },
                {
                    "start": 2118,
                    "end": 2284
                },
                {
                    "start": 2284,
                    "end": 2456
                },
                {
                    "start": 2458,
                    "end": 2634
                }
            ],
            "ref_mentions": [
                {
                    "start": 1893,
                    "end": 1897,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97900390625
        },
        {
            "corpus_id": "252846258",
            "title": "Self-Guided Diffusion Models",
            "text": "Diffusion models have recently enabled tremendous advancements in many computer vision fields related to image synthesis, but counterintuitively this often comes with the cost of requiring large annotated datasets [49,55]. For example, the image fidelity of samples from diffusion models can be spectacularly enhanced by conditioning on class labels [17]. Classifier guidance goes a step further and offers control over the alignment with the class label, by using the classifier gradient to guide the image generation [17]. Classifier-free guidance [28] replaces the dedicated classifier with a diffusion model trained by randomly dropping the condition during training. This has proven a fruitful line Self-guided diffusion framework. Our method can leverage large and diverse image datasets without any annotations for training guided diffusion models. Starting from a dataset without ground-truth annotations, we apply a self-supervised feature extractor to create self-annotations. Using these, we train diffusion models with either self-labeled, self-boxed, or self-segmented guidance that enable controlled generation and improved image fidelity. \n\nof research for several other condition modalities, such as text [50,55], image layout [53], visual neighbors [3], and image features [20]. However, all these conditioning and guidance methods require ground-truth annotations. In many domains, this is an unrealistic and too costly assumption. For example, medical images require domain experts to annotate very high-resolution data, which is infeasible to do exhaustively [45]. In this paper, we propose to remove the necessity of ground-truth annotation for guided diffusion models. \n\nWe are inspired by progress in self-supervised learning [11,13], which encodes images into semantically meaningful latent vectors without using any label information. It usually does so by solving a pretext task [2,21,24,69] on imagelevel to remove the necessity of labels. This annotation-free paradigm enables the representation learning to upscale to larger and more diverse image datasets [19].",
            "score": 0.6820990016976312,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1153
                },
                {
                    "start": 1156,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1690
                },
                {
                    "start": 1693,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 550,
                    "end": 554,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1221,
                    "end": 1225,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 1243,
                    "end": 1247,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1579,
                    "end": 1583,
                    "matchedPaperCorpusId": "237266800"
                },
                {
                    "start": 1749,
                    "end": 1753,
                    "matchedPaperCorpusId": "233444273"
                },
                {
                    "start": 1753,
                    "end": 1756,
                    "matchedPaperCorpusId": "219721239"
                },
                {
                    "start": 1905,
                    "end": 1908,
                    "matchedPaperCorpusId": "207930156"
                },
                {
                    "start": 1908,
                    "end": 1911,
                    "matchedPaperCorpusId": "4009713"
                },
                {
                    "start": 1911,
                    "end": 1914,
                    "matchedPaperCorpusId": "207930212"
                },
                {
                    "start": 1914,
                    "end": 1917,
                    "matchedPaperCorpusId": "9658690"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88232421875
        },
        {
            "corpus_id": "276774646",
            "title": "Superscopes: Amplifying Internal Feature Representations for Language Model Interpretation",
            "text": "In diffusion models, Classifier-Free Guidance (CFG) (Ho et al. (2022)) enhances the alignment of generated outputs with a given condition (such as a text prompt). It is widely used in diffusion-based models (Chen et al. (2024)). \n\nCFG operates by amplifying the direction that represents the condition, where the difference between conditional and unconditional predictions defines this direction in the model's latent space. By amplifying this directional shift, the model is effectively steered toward outputs that better align with the given condition. Reapplying this shift further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance. \n\nThis technique is particularly useful in text-to-image generation models like Stable Diffusion and GLIDE, as it improves adherence to prompts without requiring an external classifier.",
            "score": 0.6806003657354234,
            "section_title": "Classifier-Free Guidance",
            "char_start_offset": 10843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 228
                },
                {
                    "start": 231,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 678
                },
                {
                    "start": 681,
                    "end": 864
                }
            ],
            "ref_mentions": [
                {
                    "start": 207,
                    "end": 226,
                    "matchedPaperCorpusId": "269043390"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92529296875
        },
        {
            "corpus_id": "276961040",
            "title": "Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective",
            "text": "Conditional generation, e.g., class-to-image, text-to-image, or image-to-video, is omnipresent as it provides a compelling way to control the output. Ideally, conditional generation results are both diverse and of high-fidelity. Namely, the generative models' outputs align with the conditioning information perfectly and diligently follow the training data diversity. However, there is a trade-off between high-fidelity and diversity: without constraining diversity there are always possibilities to sample from areas on the data distribution manifold that are not well-trained. Thus, trading diversity for fidelity is a long-standing problem and the community has developed various approaches, e.g., the truncation trick for generative adversarial nets (GANs) [7,28], low-temperature sampling for probabilistic models [2], or temperature control in large language models [1,19]. \n\nMore recently, to trade diversity and fidelity in denois-ing diffusion models [25,35,53,58], several techniques have been developed [16,26,31], from which classifierfree guidance [24] emerged as the de-facto standard. For instance, classifier-free guidance, especially at sufficient scale, has been found to be critical for high-quality textto-image [50] and text-to-3D [47] generation. Despite its popularity, we think a solid understanding of classifier-free guidance is missing. Recently, several efforts provide insights by studying classifier-free guidance from a theoretical perspective [6,12,61] showing that sampling from classifier-free guidance is not the same as sampling from a sharpened distribution. \n\nInstead of solely focusing on classifier-free guidance as done in the works mentioned above, we trace back to the root of classifier-free guidance, i.e., classifier guidance [16]. It is classifier guidance that decomposes the conditional generation into a combination of an unconditional generation and a classifier prediction. Classifier-free guidance directly mimics this decomposition, replacing the classifier by randomly dropping conditioning information during training [24]. This connection motivates us to carefully study classifier guidance's derivation and its behavior.",
            "score": 0.6800385457414532,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 762,
                    "end": 765,
                    "matchedPaperCorpusId": "52889459"
                },
                {
                    "start": 765,
                    "end": 768,
                    "matchedPaperCorpusId": "54482423"
                },
                {
                    "start": 820,
                    "end": 823,
                    "matchedPaperCorpusId": "12174018"
                },
                {
                    "start": 876,
                    "end": 879,
                    "matchedPaperCorpusId": "271571434"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 965,
                    "end": 968,
                    "matchedPaperCorpusId": "235694314"
                },
                {
                    "start": 968,
                    "end": 971,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 971,
                    "end": 974,
                    "matchedPaperCorpusId": "5560643"
                },
                {
                    "start": 1015,
                    "end": 1019,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1019,
                    "end": 1022,
                    "matchedPaperCorpusId": "252683688"
                },
                {
                    "start": 1022,
                    "end": 1025,
                    "matchedPaperCorpusId": "254096299"
                },
                {
                    "start": 1062,
                    "end": 1066,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1233,
                    "end": 1237,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1253,
                    "end": 1257,
                    "matchedPaperCorpusId": "252596091"
                },
                {
                    "start": 1476,
                    "end": 1479,
                    "matchedPaperCorpusId": "271903235"
                },
                {
                    "start": 1479,
                    "end": 1482,
                    "matchedPaperCorpusId": "272770713"
                },
                {
                    "start": 1482,
                    "end": 1485,
                    "matchedPaperCorpusId": "273549917"
                },
                {
                    "start": 1773,
                    "end": 1777,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 2075,
                    "end": 2079,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87109375
        },
        {
            "corpus_id": "263674911",
            "title": "Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis",
            "text": "In concurrent work to ours that also explores zero-shot conditional generation using diffusion models, [3] used a text to image model [34] and a two-step forward and backward universal guidance process, but it works well only after heavy optimization on network-based inverse problems such as semantic generation and identity generation. Another concurrent work [42] explored the usage of unconditional diffusion models for linear inverse problems using a pseudo-inverse model. In contrast to all these prior works, our steered diffusion algorithm generalizes well to both image-to-image translation tasks and high-level label-based generation tasks.",
            "score": 0.6739854481535199,
            "section_title": "Concurrent Work",
            "char_start_offset": 8535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 650
                }
            ],
            "ref_mentions": [
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "259298715"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.243408203125
        },
        {
            "corpus_id": "253098360",
            "title": "Conditional Diffusion with Less Explicit Guidance via Model Predictive Control",
            "text": "Diffusion models are a class of generative models that have achieved remarkable sample quality, particularly for text-to-image generation (1), where diffusion has been guided using classifier guidance or classifier-free guidance to sample images x \u223c p(x|c) for a conditioning variable c (e.g., text) (2; 3). Controlling generative models is important for applications such as text generation and drug discovery, where multiple distinct conditional variables c 1 , c 2 , ...c n can be important: e.g., drug activity and permeability (4). \n\nFor each new conditioning information source c of interest, classifier guidance and classifier-free guidance require training a new explicit guidance model over all diffusion time steps t \u2208 [0, ..., T ] (often, T =100 to 1,000), and sample using the explicit guide at each generative time step (often, 25-100) (2; 3). Here, we explore whether conditional sampling is achievable without explicit guidance at every generative step, and if it is achievable with very few steps. This line of inquiry may make it easier to condition on new variables by reducing the training burden of new explicit guidance models. \n\nRejection sampling and Langevin \"churning\" have been explored for image editing, inpainting, and conditional sampling on new variables without training a new model over diffusion time steps, but lack general applicability (5; 1; 6; 7; 8; 9; 10): churning appears limited to \"local\" edits, while rejection sampling is inefficient for rare events. Separately, scheduler advances have reduced sampling steps from 100-1000 to 25-50 while retaining high sample quality (11; 12). This work aims to be generally applicable and synergistic with scheduler improvements. Diffusion models. Diffusion models are trained on noise-corrupted data, and learn an iterative denoising process to generate samples. We give a non-precise introduction following (13), and refer interested readers to (11) for a precise description. A diffusion model x\u03b8 is trained to optimize:",
            "score": 0.6707709428455972,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 536
                },
                {
                    "start": 539,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2005
                }
            ],
            "ref_mentions": [
                {
                    "start": 532,
                    "end": 535,
                    "matchedPaperCorpusId": "247628012"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "248097655",
            "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
            "text": "[19,36,32,18] guide GANs using gradients from a CLIP model. For diffusion models, Dhariwal and Nichol [11] introduced classifier guidance as a way to use gradients from a classifier trained on noised images to steer the model towards higher quality generations. Nichol et al. [35] train a CLIP model on noised images and guide a text-conditional diffusion model, while Crowson, Crowson [7,8] use an unnoised CLIP model to guide unconditional or class-conditional diffusion models. Ho and Salimans [24] introduced classifier-free guidance and showed that one can perform guidance implictly from the predictions of the model with and without the conditioning information, thus removing the need for a classifier. Nichol et al. [35] showed classifier-free guidance works more favorably than CLIP guidance for text conditional image generation. \n\nSeveral previous works have trained generative image models that are directly conditioned on CLIP embeddings. Zhou et al. [61] condition GAN models on randomly perturbed CLIP image embeddings, finding that these models can generalize to CLIP text embeddings to produce text-conditional images. Crowson [9] trained diffusion models conditioned on CLIP text embeddings, allowing for direct text-conditional image generation. Wang et al. [54] train an autoregressive generative model conditioned on CLIP image embeddings, finding that it generalizes to CLIP text embeddings well enough to allow for text-conditional image synthesis. \n\nBordes et al. [3] train diffusion models conditioned on image representations from contrastive models. While the diffusion models themselves cannot generate images unconditionally, the authors experimented with a simple approach for two-stage image generation by employing Kernel Density Estimation to sample image representations. By feeding these generated representations to the diffusion model, they can generate images end-to-end in a way similar to our proposed technique. However, our work differs from this in two ways: first, we use multimodal contrastive representations rather than image-only representations; second, we employ much more powerful generative models for the first stage of the generation hierarchy, and these generative models are conditioned on text.",
            "score": 0.668053059089506,
            "section_title": "Related Work",
            "char_start_offset": 25619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1472
                },
                {
                    "start": 1475,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2252
                }
            ],
            "ref_mentions": [
                {
                    "start": 497,
                    "end": 501,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95263671875
        },
        {
            "corpus_id": "270562252",
            "title": "Adding Conditional Control to Diffusion Models with Reinforcement Learning",
            "text": "Diffusion models have emerged as effective generative models for capturing intricate distributions (Sohl-Dickstein et al., 2015;Ho et al., 2020). Their capabilities are further enhanced by building conditional diffusion models p(x|c). For instance, in text-to-image generative models like DALL-E (Ramesh et al., 2021) and Stable Diffusion (Rombach et al., 2022), c \u2208 C is a prompt, and x \u2208 X is the image generated according to this prompt. While diffusion models trained on extensive datasets have shown remarkable success, additional controls often need to be incorporated during downstream fine-tuning when treating these powerful models as pre-trained diffusion models. \n\nIn this work, our goal is to incorporate new conditional controls into pre-trained diffusion models. Specifically, given access to a large pre-trained model capable of modeling p(x|c) trained on extensive datasets, we aim to condition it on an additional random variable y \u2208 Y, thereby creating a generative model p(x|c, y). To accomplish this, we utilize the pre-trained model and an offline dataset consisting guidance, our method uses the offline dataset in a sample-efficient manner and enables leveraging the conditional independence assumption, which significantly simplifies the construction of the offline dataset. Additionally, we establish a close connection to classifier guidance (Dhariwal and Nichol, 2021;Song et al., 2020), showing that it provides an alternative method for obtaining the aforementioned soft-optimal policies (in ideal cases where there are no statistical/model-misspecification errors in the algorithms). Despite this connection, our algorithm addresses common challenges in classifier guidance, such as the need to learn classifiers at multiple noise scales in standard classifier guidance and the use of fundamental approximations in some variants to avoid learning these noisy classifiers (Chung et al., 2022;Song et al., 2022). Experimentally, we validate the superiority of CTRL over baselines in both single-task and multi-task conditional image generation, such as generating highly aesthetic yet compressible images, where existing methods often struggle. Table 1 summarizes the main features of the proposed algorithm compared to existing methods.",
            "score": 0.6668255299871321,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2265
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 128,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 128,
                    "end": 144,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 339,
                    "end": 361,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1368,
                    "end": 1395,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1921,
                    "end": 1939,
                    "matchedPaperCorpusId": "259298715"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64599609375
        },
        {
            "corpus_id": "266693789",
            "title": "Diffusion Model with Perceptual Loss",
            "text": "Guidance methods alter the model prediction and guide the sample toward desired regions during the generation process. Classifier Guidance [7] adds classifier gradients to the predicted score to guide the sample generation to maximize the classification. It can turn an unconditional diffusion model conditional. However, it is not evident why applying classifier guidance on an already conditional diffusion model can significantly improve sample quality. Previous research has attributed it to low-temperature sampling [15,24]. Classifier-Free Guidance (CFG) [15] uses Bayes' rule and finds that the diffusion model itself can be inverted as an implicit classifier. Specifically, the model is queried both conditionally and unconditionally at every inference step and the difference is amplified toward the conditional direction. Both methods only work for conditional generation and entangle sample quality with conditional alignment [24]. Self-Supervised Guidance [20] uses self-supervised networks to generate synthetic clustering labels for unconditional data. This allows unconditional data to use CFG for improving quality. Guidance-Free Training [4] shows that CFG can be applied at training. This bakes the guided flow into the model and saves computation during inference. More recently, Discriminator Guidance [25] proposes to train a discriminator network to classify real and generated samples and use it as guidance during diffusion generation. Self-Attention Guidance [18] finds that the self-attention map of the diffusion model can be exploited to enhance quality. Autoguidance [24] finds a smaller or less-trained model can be used as negative guidance to improve quality. Although these methods are effective in improving quality, they also present issues such as increased complexity and reduced diversity [15,24], etc. On the other hand, our research aims to explore the underlying issue: why diffusion models without guidance fail in the first place. \n\nThe loss objective of diffusion models has been studied by prior works. Loss weighting is found to influence perceptual quality and likelihood evaluation [6,13,50] but still cannot produce good samples without guidance. Multiscale loss [19] is proposed to improve high-resolution generation. Smoothness penalty [11] is proposed to enforce smoother latent traversal. l1 distance is explored for colorization and in-painting tasks [41].",
            "score": 0.6583198984947735,
            "section_title": "Related Work",
            "char_start_offset": 3312,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1973
                },
                {
                    "start": 1976,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2195
                },
                {
                    "start": 2196,
                    "end": 2267
                },
                {
                    "start": 2268,
                    "end": 2341
                },
                {
                    "start": 2342,
                    "end": 2410
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 142,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 521,
                    "end": 525,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 561,
                    "end": 565,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1322,
                    "end": 1326,
                    "matchedPaperCorpusId": "254096299"
                },
                {
                    "start": 1484,
                    "end": 1488,
                    "matchedPaperCorpusId": "252683688"
                },
                {
                    "start": 1827,
                    "end": 1831,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 2130,
                    "end": 2133,
                    "matchedPaperCorpusId": "247922317"
                },
                {
                    "start": 2133,
                    "end": 2136,
                    "matchedPaperCorpusId": "257557255"
                },
                {
                    "start": 2136,
                    "end": 2139,
                    "matchedPaperCorpusId": "235352469"
                },
                {
                    "start": 2212,
                    "end": 2216,
                    "matchedPaperCorpusId": "256274516"
                },
                {
                    "start": 2287,
                    "end": 2291,
                    "matchedPaperCorpusId": "266054322"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.720703125
        },
        {
            "corpus_id": "253420366",
            "title": "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models",
            "text": "We introduce safety guidance for latent diffusion models to reduce the inappropriate degeneration of DMs. Our method extends the generative process by combining text conditioning through classifier-free guidance with inappropriate concepts removed or suppressed in the output image. Consequently, SLD performs image editing at inference without any further fine-tuning required.\n\nDiffusion models iteratively denoise a Gaussian distributed variable to produce samples of a learned data distribution. Intuitively, image generation starts from random noise , and the model predicts an estimate of this noise\u02dc \u03b8 to be subtracted from the initial values. This results in a high-fidelity image x without any noise. Since this is an extremely hard problem, multiple steps are applied, each subtracting a small amount ( t ) of the predictive noise, approximating . For text-to-image generation, the model's -prediction is conditioned on a text prompt p and results in an image faithful to that prompt. The training objective of a diffusion modelx \u03b8 can be written as\n\nwhere (x, c p ) is conditioned on text prompt p, t is drawn from a uniform distribution t \u223c U([0, 1]), sampled from a Gaussian \u223c N (0, I), and w t , \u03c9 t , \u03b1 t influence image fidelity depending on t. Consequently, the DM is trained to denoise z t := x + to yield x with the squared error as loss. At inference, the DM is sampled using the model's prediction of x = (z t \u2212\u00af \u03b8 ), with\u00af \u03b8 as described below.\n\nClassifier-free guidance [17] is a conditioning method using a purely generational diffusion model, eliminating the need for an additional pre-trained classifier. The approach randomly drops the text conditioning c p with a fixed probability during training, resulting in a joint model for unconditional and conditional objectives. During inference the score estimates for the x-prediction are adjusted so that:\n\nwith guidance scale s g which is typically chosen as s g \u2208 (0, 20] and \u03b8 defining the noise estimate with parameters \u03b8. Intuitively, the unconditioned -prediction \u03b8 (z t ) is pushed in the direction of the conditioned \u03b8 (z t , c p ) to yield an image faithful to prompt p. Lastly, s g determines the magnitude of the influence",
            "score": 0.6556133785561612,
            "section_title": "Safe Latent Diffusion (SLD)",
            "char_start_offset": 11423,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94091796875
        },
        {
            "corpus_id": "267312219",
            "title": "Spatial-Aware Latent Initialization for Controllable Image Generation",
            "text": "where z t+1 denotes noisy latent at timestep t + 1. Note that the added noise \u03f5 \u03b8 (z t , t, C) in timestep t is related to the noisy latent z t , thus establishing the strong relationship between the last latent z T and the original image latent z 0 . Classifier-Free Guidance. Instead of using a well-trained classifier for guidance as in [4], we use classifier-free guidance [10] to guide image generation via interpolation between conditional and unconditional sampling. Specifically, the classifier-free guidance sampling is expressed as: \n\nwhere \u2205 denotes the unconditional textual embedding, and w denotes the guidance parameter that balances the conditional and unconditional sampling.",
            "score": 0.6554107187783318,
            "section_title": "Preliminaries",
            "char_start_offset": 14927,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 692
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.646484375
        },
        {
            "corpus_id": "252438737",
            "title": "Implementing and Experimenting with Diffusion Models for Text-to-Image Generation",
            "text": "Depending on a separate model is inconvenient and it complicates the training pipeline. Moreover, for classifier guidance, the number of classes is limited, preventing us from conditioning on complex information to generate more elaborated image compositions. Thus,  proposed classifier-free guidance, which only relies on a single diffusion model. Classifierfree guidance considers a conditional diffusion model \u03b8 (x t , t|y) that can be made unconditional by replacing occasionally during training the condition y by an empty condition \u2205, e.g., by setting the caption to an empty string. The model \u03b8 (x t , t|\u2205) can then be used to generate unconditional images. Classifier-free guidance therefore consists in updating the model output using a linear combination between \u03b8 (x t , t|\u2205) and \u03b8 (x t , t|y) in the following way:\n\nwith s \u2265 1 being the guidance scale. This update can be understood as an attempt to move further in the direction of the conditional model, while moving away from the unconditional model. It is performed at each diffusion step when sampling and Ho and Salimans (2021) have shown that it improves sample quality.",
            "score": 0.6546996849742563,
            "section_title": "Classifier-free guidance",
            "char_start_offset": 30387,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95166015625
        },
        {
            "corpus_id": "258309719",
            "title": "Exploring Compositional Visual Generation with Latent Classifier Guidance",
            "text": "Diffusion probabilistic models have achieved enormous success in the field of image generation and manipulation. In this paper, we explore a novel paradigm of using the diffusion model and classifier guidance in the latent semantic space for compositional visual tasks. Specifically, we train latent diffusion models and auxiliary latent classifiers to facilitate non-linear navigation of latent representation generation for any pre-trained generative model with a semantic latent space. We demonstrate that such conditional generation achieved by latent classifier guidance provably maximizes a lower bound of the conditional log probability during training. To maintain the original semantics during manipulation, we introduce a new guidance term, which we show is crucial for achieving compositionality. With additional assumptions, we show that the non-linear manipulation reduces to a simple latent arithmetic approach. We show that this paradigm based on latent classifier guidance is agnostic to pre-trained generative models, and present competitive results for both image generation and sequential manipulation of real and synthetic images. Our findings suggest that latent classifier guidance is a promising approach that merits further exploration, even in the presence of other strong competing methods.",
            "score": 0.6511016876395666,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80859375
        },
        {
            "corpus_id": "254564168",
            "title": "Towards Practical Plug-and-Play Diffusion Models",
            "text": "Diffusion models Diffusion models [8,18,26,38,49] and score-based models [51,53] are families of the generative model that generate samples from a given distribution by gradually removing noise. Unlike other likelihood-based methods such as VAEs [27] or flow-based models [9,10], diffusion models have shown superior generation capabilities comparable to GANs [3,12,23]. Although diffusion models suffer from slow generation, previous works such as DDIM [50], A-DDIM [2], PNDM [33], and DEIS [55] have achieved significant acceleration in the generation process.\n\nFor conditional generation in diffusion models, classifier guidance [8,53] and classifier-free guidance [19] are widely applied to various tasks [17,25,29,37,44]. Classifier guidance uses gradients of the external classifier, whereas classifier-free guidance interpolates between predictions from a diffusion model with and without labels. However, for classifier-free guidance, diffusion models should be learned as labeled data because it requires the prediction of labels. In this paper, we focus on the classifier guidance that freezes the unconditional diffusion model and guides it with the external model to conduct various conditional generations without labeled data in plug-and-play manner. Plug-and-play generation Following [36], we use the term plug-and-play to refer to the capability of generating images at test time based on a condition given by a replaceable condition network without training it and generative model jointly. There have been various attempts for plug-and-play conditional generation in both image generation [11,22,24,36,52] and text generation [6,34,48], by binding constraints to the unconditional models, such as GAN [12], VAE [27]. These methods allow the single unconditional generative model to perform various tasks by changing the constraint model.\n\nMost similar work to ours, Graikos et al. [13] attempted plug-and-play on diffusion models for various tasks by directly optimizing latent images with the off-the-shelf model. However, it fails to generate meaningful images in complex distribution as ImageNet. Contrary to this, our method successfully guidance",
            "score": 0.6507236728937498,
            "section_title": "Related Work",
            "char_start_offset": 4046,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 34,
                    "end": 37,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 37,
                    "end": 40,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 43,
                    "end": 46,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 46,
                    "end": 49,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 73,
                    "end": 77,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 366,
                    "end": 369,
                    "matchedPaperCorpusId": "54482423"
                },
                {
                    "start": 632,
                    "end": 635,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 713,
                    "end": 716,
                    "matchedPaperCorpusId": "246430592"
                },
                {
                    "start": 722,
                    "end": 725,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1608,
                    "end": 1612,
                    "matchedPaperCorpusId": "36549760"
                },
                {
                    "start": 1612,
                    "end": 1615,
                    "matchedPaperCorpusId": "240225810"
                },
                {
                    "start": 1615,
                    "end": 1618,
                    "matchedPaperCorpusId": "235253954"
                },
                {
                    "start": 1621,
                    "end": 1624,
                    "matchedPaperCorpusId": "244130146"
                },
                {
                    "start": 1645,
                    "end": 1648,
                    "matchedPaperCorpusId": "208617790"
                },
                {
                    "start": 1648,
                    "end": 1651,
                    "matchedPaperCorpusId": "222178257"
                },
                {
                    "start": 1651,
                    "end": 1654,
                    "matchedPaperCorpusId": "211069137"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79150390625
        },
        {
            "corpus_id": "245502474",
            "title": "Multimodal Image Synthesis and Editing: The Generative AI Era",
            "text": "A downside of guidance function method lies in the requirement of an additional guidance model which leads to a complicated training pipeline. Recently, Ho et al. [27] achieve compelling results without a separately guidance model by using a form of guidance that interpolates between predictions from a diffusion model with and without labels. GLIDE [180] compares CLIP-guided diffusion model and conditional diffusion model on text-to-image synthesis task, and concludes that training conditional diffusion model yields better generation performance.",
            "score": 0.6497681301460148,
            "section_title": "Diffusion-based Methods",
            "char_start_offset": 28919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 552
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.361572265625
        },
        {
            "corpus_id": "260887414",
            "title": "Precipitation nowcasting with generative diffusion models",
            "text": "Generation often requires a way to control how samples are created to influence the final output. This process is commonly referred to as conditioned or guided diffusion. Numerous approaches have been devised to integrate image and/or text embeddings into the diffusion process, allowing for guided generation. In the mathematical context, \"guidance\" entails conditioning a prior data distribution, represented by p(x), with a particular constraint, like a class label or an image/text embedding. This conditioning leads to the formation of a conditional distribution, denoted as p(x|y). \n\nTo convert a diffusion model p \u03b8 into a conditional diffusion model, we can introduce conditioning information y at each diffusion step as follows: \n\nThere are typically two approaches to learning this distribution, one based on an auxiliary classifier (similar, in spirit, to AC-GANs [48]), and a second one that is classifier-free. \n\nThe idea behind classifier guidance is the following. Our aim is to learn the gradient of the logarithm of the conditional density p \u03b8 (x t |y). By applying Bayes' rule, we can express it as: \n\nSince the gradient operator only applies to x t , we can eliminate the term p \u03b8 (y); after simplification we get: \n\nHere s is a scalar term used to modulate the strength of the guidance term. \n\nAs described in [37], we can use a classifier f \u03d5 (y|x t , t)) to guide the diffusion during generation. This technique involves training a classifier f \u03d5 (y|x t , t) on a noisy image x t to predict its class y. The gradient \u2207 x log f \u03d5 (y|x t ) can then be utilized to guide the diffusion sampling process towards the conditioning information y by modifying the noise prediction. We shall not use this technique, particularly suited for discrete labels, so further details are omitted. \n\nThe theory of condition diffusion without relying on an independent classifier has been investigated in [49]. The approach consists in training a conditional diffusion model \u03f5 \u03b8 (x t , t, y) along with an unconditional model \u03f5 \u03b8 (x t , t, 0).",
            "score": 0.6491139379794651,
            "section_title": "Conditioning",
            "char_start_offset": 11554,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1233
                },
                {
                    "start": 1236,
                    "end": 1311
                },
                {
                    "start": 1314,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1800
                },
                {
                    "start": 1803,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2045
                }
            ],
            "ref_mentions": [
                {
                    "start": 875,
                    "end": 879,
                    "matchedPaperCorpusId": "1099052"
                },
                {
                    "start": 1330,
                    "end": 1334,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86279296875
        },
        {
            "corpus_id": "277271753",
            "title": "TCFG: Tangential Damping Classifier-free Guidance",
            "text": "Diffusion models [12,31] have shown remarkable progress in image generation [19,27,30]. In particular, the emergence of classifier-free guidance [6,11] (CFG) has attracted significant attention because it allows us to provide desired guidance by leveraging the conditional estimated score directly within the diffusion model. \n\nThe classifier-free guidance fundamentally computes the * Equal contribution manifold by the given condition. Fig. 1 (a) conceptually illustrates the potential issue that arises when the manifold of the unconditional score differs from that of the conditional score. In this paper, we show that this misalignment can be resolved with a simple algorithm, which significantly reduces the tendency of CFG to generate off-manifold samples, as illustrated in Fig. 1 (b). \n\nOur approach is based on the following insights. First, the score predicted by the diffusion model estimates the intrinsic dimension of the data manifold [32]. Additionally, this intrinsic dimension can be captured by the tangent space of the target manifold [3,9]. Instead of directly estimating the intrinsic dimension, we focus on utilizing the tangential component inherent in the unconditional score during classifier-free guidance. By reducing its misalignment with the conditional score, we enhance the alignment and ultimately improve the quality of the generated outputs. \n\nSpecifically, we push the score s\u03b8 toward the normal direction of the conditional manifold by eliminating the values of column vectors with small singular values using the orthogonal matrix V obtained through the singular value decomposition of the conditional and unconditional scores. \n\nIn this paper, we propose a novel sampling method that leverages the unconditional score within CFG. To support our approach, we first lay out the theoretical foundation in section Sec. 2 and Sec. 3, discussing the manifold hypothesis and its connection to diffusion models. In Sec. 4, we provide a comprehensive explanation of our proposed method. This is followed by a detailed analysis using a toy example in Sec. 5, and we demonstrate the practical applicability of our method on real-world text-to-image models in Sec. 6.",
            "score": 0.6473808358632955,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1376
                },
                {
                    "start": 1379,
                    "end": 1665
                },
                {
                    "start": 1668,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2194
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 21,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 80,
                    "end": 83,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 145,
                    "end": 148,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1055,
                    "end": 1058,
                    "matchedPaperCorpusId": "393948"
                },
                {
                    "start": 1058,
                    "end": 1060,
                    "matchedPaperCorpusId": "50258911"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93408203125
        },
        {
            "corpus_id": "256846836",
            "title": "Universal Guidance for Diffusion Models",
            "text": "In this paper, we focus on controlled image generation with various constraints. Consider a differentiable guidance function f , for example a CLIP feature extractor or a segmentation network. When applied to an image, we obtain a vector c = f (x). We also consider a function \u2113(\u00b7, \u00b7) that measures the closeness of two vectors c and c \u2032 . Given a particular choice of c, which we call a prompt, the corresponding constraint (based on c, \u2113, and f ) is formalized as \u2113(c, f (z)) \u2248 0, and we aim to generate a sample z from the image distribution satisfying the constraint. In plain words, we want to generate an in-distribution image that matches the prompt.\n\nPrior work that studied controlled generative diffusion mainly falls into two categories. We refer to the first category as conditional image generation, and the second category as guided image generation. Next, we discuss the characteristics of each category and better situate our work among existing methods.\n\nConditional Image Generation. Methods from this category require training new diffusion models that accept the prompt as an additional input [2,9,18,27,29]. For example, [9] proposed classifier-free guidance using class labels as prompts, and trained a diffusion model by linear interpolation between unconditional and conditional outputs of the denoising networks. [2] studied the case where the guidance function is a known linear degradation operator, and trained a conditional model to solve linear inverse problems. [18] further extended classifier-free guidance to text-conditional image generation with descriptive phrases as prompts, and trained a diffusion model to enforce the similarity between the CLIP [20] representations of the generated images and the text prompts. These methods are successful across different types of constraints, however the requirement to retrain the diffusion model makes them computationally intensive.\n\nGuided Image Generation. Works in this category employed a frozen pre-trained diffusion model as a foundation model, but modify the sampling method to guide the image generation with feedback from the guidance function. Our method falls into this category. Prior work that studied guided image generation did so with a variety of restrictions and external guidance functions [3,4,6,7,12,16,28]. For example, [6] proposed classifier guidance, where they trained a classifier",
            "score": 0.6473517440978295,
            "section_title": "Controlled Image Generation",
            "char_start_offset": 5681,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1124,
                    "end": 1127,
                    "matchedPaperCorpusId": "244908890"
                },
                {
                    "start": 1687,
                    "end": 1691,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91162109375
        },
        {
            "corpus_id": "264172506",
            "title": "Elucidating The Design Space of Classifier-Guided Diffusion Generation",
            "text": "In this work, we elucidate the design space of off-the-shelf classifier guidance in diffusion generation. Using our training-free and accessible designs, off-the-shelf classifiers can effectively guide conditional diffusion, achieving state-of-the-art performance in ImageNet 128x128. Our approach is applicable to various diffusion models such as DDPM, EDM, and DiT, as well as text-to-image scenarios. We believe our work contributes significantly to the investigation of the ideal guidance method for diffusion models that may greatly benefit the booming AIGC industry. \n\nThere are multiple directions to extend this work. First, we primarily investigated classifier guidance in diffusion generation while there are more sophisticated discriminative models, e.g., detection models and visual Question Answering models, as well as other types of generative methods, e.g., masked image modeling [Chang et al., 2022[Chang et al., , 2023] ] and autoregressive models [Esser et al., 2021, Yu et al., 2022, 2023]. It would be interesting to explore other types of guided generation. Second, we only considered image generative models, and extending to language models would also be a promising direction. We believe that our proposed designs and calibration methodology hold potential for diverse modalities and we leave this for future work. To replicate the DiT off-the-shelf classifier guided sampling in Table 9: classifier-free scale s = 1.5, Softplus \u03b2 = 6, joint logit temperature \u03c4 1 = 1.1, marginal logit temperature \u03c4 2 = 0.5, classifier guidance schedule added sine factor \u03b3 t = 0.2,",
            "score": 0.6466040987011096,
            "section_title": "Discussion",
            "char_start_offset": 24646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1591
                }
            ],
            "ref_mentions": [
                {
                    "start": 966,
                    "end": 985,
                    "matchedPaperCorpusId": "229297973"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84423828125
        },
        {
            "corpus_id": "273811150",
            "title": "ReactFace: Online Multiple Appropriate Facial Reaction Generation in Dyadic Interactions.",
            "text": "Recent techniques for conditional generation have been developed to incorporate various modalities, ensuring that synthesized results align with the instructions provided by these modality signals [66]. Early approaches, such as conditional variational autoencoders (CVAEs) [66] and conditional generative adversarial networks [66], leverage class labels to distinguish image domains and guide generated image samples to possess domain-specific properties. For example, CycleGAN was proposed to translate images from a source domain to a target domain, while the StarGAN series [18], [67] further succeeded in translating images from a source domain to multiple target domains. Some techniques embed discrete class labels as conditional signals in the generation process, either by directly concatenating class labels with the input [18], [66] or by using conditional normalization techniques [67], [68]. Building on these successful practices, conditional diffusion models incorporate class information (e.g., class labels or text) into normalization layers and guide the generation process using classifier gradients. Further research has shown that guidance can be derived from the generative model itself without a classifier, a method known as classifier-free guidance. This advancement unlocks various forms of conditional signals, leading to conditional generative models [69], [70] that control synthesized samples with multiple modalities, such as text, edge maps, human pose skeletons, segmentation maps, depth, and normals.",
            "score": 0.6450198836676306,
            "section_title": "Conditional Generative Models",
            "char_start_offset": 12660,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1534
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "13936837"
                },
                {
                    "start": 274,
                    "end": 278,
                    "matchedPaperCorpusId": "13936837"
                },
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "13936837"
                },
                {
                    "start": 578,
                    "end": 582,
                    "matchedPaperCorpusId": "9417016"
                },
                {
                    "start": 584,
                    "end": 588,
                    "matchedPaperCorpusId": "208617800"
                },
                {
                    "start": 833,
                    "end": 837,
                    "matchedPaperCorpusId": "9417016"
                },
                {
                    "start": 839,
                    "end": 843,
                    "matchedPaperCorpusId": "13936837"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "208617800"
                },
                {
                    "start": 899,
                    "end": 903,
                    "matchedPaperCorpusId": "54482423"
                },
                {
                    "start": 1379,
                    "end": 1383,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 1385,
                    "end": 1389,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92578125
        },
        {
            "corpus_id": "265457008",
            "title": "One More Step: A Versatile Plug-and-Play Module for Rectifying Diffusion Schedule Flaws and Enhancing Low-Frequency Controls",
            "text": "Diffusion models [9,37] have significantly advanced the field of text-to-image synthesis [1,12,13,25,29,30]. These models often operate within the latent space to optimize computational efficiency [30] or initially generate lowresolution images that are subsequently enhanced through super-resolution techniques [1,29]. Recent developments in fast sampling methods have notably decreased the diffusion model's generation steps from hundreds to just a few [15,[20][21][22]36]. Moreover, incorporating classifier guidance during the sampling phase significantly improves the quality of the results [4]. While classifier-free guidance is commonly used [8], exploring other guidance types also presents promising avenues for advancements in this domain [6,40].",
            "score": 0.6414498697004937,
            "section_title": "A. Related Works",
            "char_start_offset": 23634,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 756
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 20,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 92,
                    "end": 95,
                    "matchedPaperCorpusId": "244896176"
                },
                {
                    "start": 104,
                    "end": 107,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 596,
                    "end": 599,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.359130859375
        },
        {
            "corpus_id": "268041325",
            "title": "FineDiffusion: Scaling up Diffusion Models for Fine-grained Image Generation with 10, 000 Classes",
            "text": "Diffusion models [Sohl-Dickstein et al., 2015;Ramesh et al., 2022;Saharia et al., 2022;Li et al., 2022;He et al., 2024] [Song et al., 2020;Ho and Salimans, 2022;Ho and Salimans, 2022]. Classifier-free guidance [Ho and Salimans, 2022] proposes a diffusion guidance method without additional classifiers by jointly training conditional and unconditional diffusion models. For its impressive image generation results, numbers of models have adopted classifier-free guidance, making significant strides in the image generation field [Nichol et al., 2021;Ramesh et al., 2022;Gafni et al., 2022;Mokady et al., 2023]. However, classifierfree guidance inadequately considers the correlations among different classes in fine-grained fields. In light of this, we explore a fine-grained classifier-free guidance approach that effectively utilizes the label hierarchy structure to improve finegrained image generation. Additionally, recent works [Bao et al., 2023;Peebles and Xie, 2023] have explored the integration of Transformer architectures into diffusion models. \n\nDiT models [Peebles and Xie, 2023] have garnered significant attention due to their superior scalability and simplicity, inspiring numerous subsequent works [Xie et al., 2023;Mo et al., 2024;He et al., 2023;Lu et al., 2024]. We follow the DiT [Peebles and Xie, 2023] paradigm and further fine-tune the pre-trained models for fine-grained image generation.",
            "score": 0.6404482096065875,
            "section_title": "Diffusion Models",
            "char_start_offset": 5480,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1414
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 46,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 66,
                    "end": 87,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 103,
                    "end": 119,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 139,
                    "end": 161,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 161,
                    "end": 183,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 210,
                    "end": 233,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 570,
                    "end": 589,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 589,
                    "end": 609,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 1234,
                    "end": 1250,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 1250,
                    "end": 1266,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 1266,
                    "end": 1282,
                    "matchedPaperCorpusId": "245335086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87548828125
        },
        {
            "corpus_id": "269149144",
            "title": "Magic Clothing: Controllable Garment-Driven Image Synthesis",
            "text": "Classifier-free guidance [16] is a method that helps the diffusion models to attain a trade-off between sample quality and diversity by jointly training a conditional and an unconditional diffusion model.The implementation during training is relatively simple by setting the conditional control c = \u2205 with some probability.During inference, the score estimate \u03b5 (z  , c) linearly combine the conditional and unconditional predictions as:\n\nwhere  \u2a7e 1 represents the strength of conditional controls.As for our garment-driven image synthesis task, two variant conditional controls should be considered: detailed garment features   and text prompts   .If we consider them as independent controls, we can naively modify the independent classifier-free guidance in [25] for our use.Specifically, garment features c  and text prompts c  can be independently set to \u2205 with some probability during training.Then at inference time, we introduce the garment guidance scale   and text guidance scale   to adjust the strengths of conditional controls from the garment and text prompts, respectively.The score estimate with independent classifier-free guidance is calculated as:\n\n\u03b5\n\nDespite its simplicity, fusing two denoising scores as done in Equation 5 could lead to undesired results since this method ignores the fact that our two conditional controls may have overlapping semantics.Drawing inspiration from [3], we leverage joint classifier-free guidance to balance these two conditional controls.Here joint indicates that we set the drop rate of garment features and text prompts according to a joint distribution.More precisely, we randomly set 5% of training samples with c  = \u2205  , 5% with c  = \u2205  and another 5% with both c  = \u2205  and c  = \u2205  .Our joint classifier-free guidance score estimate is:\n\nIn Figure 3, we show the effects of these two parameters on generated samples with the random seed fixed.It is noticeable that the garment in the character become more similar to the input garment with a larger   and a larger   will make the generated result follow the text prompt more precisely.Since a large gap between   and   may distort the garment details, we empirically set   = 7.5 and   = 2.5 in our experiments.",
            "score": 0.6402261971974108,
            "section_title": "Joint Classifier-free Guidance",
            "char_start_offset": 11306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 204,
                    "end": 323
                },
                {
                    "start": 323,
                    "end": 437
                },
                {
                    "start": 439,
                    "end": 498
                },
                {
                    "start": 498,
                    "end": 649
                },
                {
                    "start": 649,
                    "end": 777
                },
                {
                    "start": 777,
                    "end": 899
                },
                {
                    "start": 899,
                    "end": 1087
                },
                {
                    "start": 1087,
                    "end": 1165
                },
                {
                    "start": 1167,
                    "end": 1168
                },
                {
                    "start": 1170,
                    "end": 1376
                },
                {
                    "start": 1376,
                    "end": 1491
                },
                {
                    "start": 1491,
                    "end": 1609
                },
                {
                    "start": 1609,
                    "end": 1741
                },
                {
                    "start": 1741,
                    "end": 1794
                },
                {
                    "start": 1796,
                    "end": 1901
                },
                {
                    "start": 1901,
                    "end": 2093
                },
                {
                    "start": 2093,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 760,
                    "end": 764,
                    "matchedPaperCorpusId": "249375227"
                },
                {
                    "start": 1401,
                    "end": 1404,
                    "matchedPaperCorpusId": "253581213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9111328125
        },
        {
            "corpus_id": "273662408",
            "title": "Dual Conditional Diffusion Models for Sequential Recommendation",
            "text": "Diffusion models, derived from non-equilibrium thermodynamics, have achieved notable success in areas like computer vision, sequence modeling, and audio processing [35]. Two primary paradigms exist: unconditional models and conditional models. Unconditional models, such as DDPM [9], aim to push the performance boundaries of generative models. Conditional models incorporate guidance from labels to control generated samples. SGGM [36] conditions on self-produced labels, while You et al. [38] demonstrate how few labels and pseudo-training enhance both large-scale diffusion models and semi-supervised learners. Dhariwal [3] uses classifier guidance to improve sample quality, and Ho and Salimans [11] combine conditional and unconditional scores to balance sample quality and diversity. Multi-modal guidance further enriches the diffusion process, as seen in DiffuSeq [5] for NLP tasks and SDEdit [22] for image translation, with latent diffusion models (LDM) [27] offering flexible, unified semantics. unCLIP [25] and ConPreDiff [34] integrate CLIP latents in text-to-image generation.",
            "score": 0.6393806585326525,
            "section_title": "Related Work 5.1 Diffusion Models",
            "char_start_offset": 33383,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1089
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "252070859"
                },
                {
                    "start": 279,
                    "end": 282,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 490,
                    "end": 494,
                    "matchedPaperCorpusId": "257050467"
                },
                {
                    "start": 623,
                    "end": 626,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 871,
                    "end": 874,
                    "matchedPaperCorpusId": "252917661"
                },
                {
                    "start": 900,
                    "end": 904,
                    "matchedPaperCorpusId": "245704504"
                },
                {
                    "start": 963,
                    "end": 967,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1033,
                    "end": 1037,
                    "matchedPaperCorpusId": "266755829"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44921875
        },
        {
            "corpus_id": "269283056",
            "title": "Analysis of Classifier-Free Guidance Weight Schedulers",
            "text": "Generative and Diffusion Models. Before the advent of diffusion models, several generative models were developed to create new data that mimics a given dataset, either unconditionally or with conditional guidance. Notable achievements include Variational AutoEncoders (VAEs) (Kingma & Welling, 2014) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), which have recorded significant progress in various generative tasks (Brock et al., 2018;Kang et al., 2023a;Dufour et al., 2022;Donahue et al., 2018). Recently, diffusion models have demonstrated a remarkable capacity to produce high-quality and diverse samples. They have achieved state-of-the-art results in several generation tasks, notably in image synthesis (Song et al., 2020;Ho et al., 2020), text-to-image applications (Dhariwal & Nichol, 2021;Rombach et al., 2022;Podell et al., 2023;Pernias et al., 2023) and text-to-motion (Chen et al., 2023). \n\nGuidance in Diffusion and Text-to-Image. Making generative models controllable and capable of producing user-aligned outputs requires making the generation conditional on a given input. Conditioned diffusion models have been vastly explored (Saharia et al., 2022;Ruiz et al., 2023;Balaji et al., 2022). The condition is achieved in its simplest form by adding extra input, typically with residual connections (Nichol & Dhariwal, 2021). To reinforce the model's fidelity to specific conditions, two main approaches prevail: Classifier Guidance (CG) (Dhariwal & Nichol, 2021), which involves training an image classifier externally, and Classifier-Free Guidance (CFG) (Ho & Salimans, 2021), that relies on an implicit classifier through joint training of conditional and unconditional models (using dropout on the condition). \n\nParticularly, CFG has catalyzed advancements in text-conditional generation, a domain where training a noisy text classifier is less convenient and performs worse.",
            "score": 0.6385916351556165,
            "section_title": "Related Work",
            "char_start_offset": 3885,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1750
                },
                {
                    "start": 1753,
                    "end": 1916
                }
            ],
            "ref_mentions": [
                {
                    "start": 439,
                    "end": 459,
                    "matchedPaperCorpusId": "52889459"
                },
                {
                    "start": 459,
                    "end": 478,
                    "matchedPaperCorpusId": "257427461"
                },
                {
                    "start": 478,
                    "end": 498,
                    "matchedPaperCorpusId": "252780703"
                },
                {
                    "start": 498,
                    "end": 519,
                    "matchedPaperCorpusId": "52890982"
                },
                {
                    "start": 733,
                    "end": 752,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 752,
                    "end": 768,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 822,
                    "end": 843,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 904,
                    "end": 923,
                    "matchedPaperCorpusId": "254408910"
                },
                {
                    "start": 1168,
                    "end": 1190,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1190,
                    "end": 1208,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 1593,
                    "end": 1614,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "256846836",
            "title": "Universal Guidance for Diffusion Models",
            "text": "In this section, we present results testing our proposed universal guidance algorithm against a wide variety of guidance functions. Specifically, we experiment with Stable Diffusion [22], a diffusion model that is able to perform text-conditional generation by accepting text prompt as additional input, and experiment with a purely unconditional diffusion model trained on ImageNet [5], where we use pretrained model provided by OpenAI [6]. We note that Stable Diffusion, while being a text-conditional generative model, can also perform unconditional image generation by simply using an empty string for the text prompt. We first present the experiment on Stable Diffusion for different guidance functions in Sec. 4.1, and present the results on ImageNet diffusion model in Sec. 4.2.",
            "score": 0.6385587007804321,
            "section_title": "Experiments",
            "char_start_offset": 16116,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 186,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 383,
                    "end": 386,
                    "matchedPaperCorpusId": "57246310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1771240234375
        },
        {
            "corpus_id": "259316918",
            "title": "LEDITS: Real Image Editing with DDPM Inversion and Semantic Guidance",
            "text": "The concept of Semantic Guidance [7] was introduced to enhance fine grained control over the generation process of text guided diffusion models. SEGA extends principles introduced in classifier-free guidance by exclusively interacting with the concepts already present in the model's latent space. The calculation takes place within the ongoing diffusion iteration and is designed to impact the diffusion process across multiple directions. More specifically, SEGA uses multiple textual descriptions e i , representing the given target concepts of the generated image, in addition to the text prompt p.",
            "score": 0.6379845133228526,
            "section_title": "Semantic Guidance",
            "char_start_offset": 4886,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 602
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73828125
        },
        {
            "corpus_id": "252438737",
            "title": "Implementing and Experimenting with Diffusion Models for Text-to-Image Generation",
            "text": "When we consider image generation, we often want to specify the content of the synthetic images. Therefore, generative models have been easily adapted to take into consideration extra information to have control on the generation. While early models were only capable of producing samples of a specific class label, alignDRAW (Mansimov et al., 2016) demonstrated that it was possible to consider captions describing the contents of the images to produce scene compositions unseen in the dataset. However, the sample quality was low and most of the images were blurry. It is only when DALL-E  came out that generative models were able to produce realistic outputs given an image caption. We now show how to take into account conditional information in the context of diffusion models.\n\nDiffusion models have two distinct ways to integrate conditional information and we first need to explain the difference between the two. Conditional generative models try to learn the probability distribution p(x|y), e.g., generating an image x belonging to the class y or matching a caption y. Applied to diffusion models, it simply consists of learning the conditional model \u03b8 (x t , t|y) instead of the unconditional model \u03b8 (x t , t). Thus, the model directly takes y as input to condition the generation of x during both training and sampling. How y is concretely incorporated into the model is explained in subsection 4.2.1.\n\nOn the other hand, guidance methods don't change the model structure, and are only used during sampling. Guidance slightly modifies the output of the model at each diffusion step to help the generative process to go in the desired direction. This small update often takes the form of a gradient ascent step and the desired direction can again be a condition y such as a class label or a caption.  exhibited that guidance could greatly improve sample quality. Finally, guidance methods and conditional generative models are complementary and can therefore be used jointly. Some guidance approaches such as classifier-free guidance even require a conditional model. We now review the different guidance methods usey by diffusion models.",
            "score": 0.6369286013924299,
            "section_title": "Conditioning and guidance",
            "char_start_offset": 27142,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 349,
                    "matchedPaperCorpusId": "9996719"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82568359375
        },
        {
            "corpus_id": "245117331",
            "title": "More Control for Free! Image Synthesis with Semantic Diffusion Guidance",
            "text": "Our approach demonstrates better controllability as different types of image guidance are proposed where users can decide how much semantic, structural, or style information to preserve by using different types and scales of guidance, while not requiring to re-train the unconditional diffusion model. \n\nDiffusion Models Diffusion models are a new type of generative models consisting of a forward process (signal to noise) and a reverse process (noise to signal). The denoising diffusion probabilistic model (DDPM) [47,19] is a latent variable model where a denoising autoencoder gradually transforms Gaussian noise into signal. Score-based generative model [48,49,50] trains a neural network to predict the score function which is used to draw samples via Langevin Dynamics. Diffusion models have demonstrated comparable or superior image quality compared to GANs while exhibiting better mode coverage and training stability. They have also been explored for conditional generation such as class-conditional generation, image-guided synthesis, and super-resolution [50,10,8,34]. Concurrent work [2] explored text-guided image editing with diffusion models. Dhariwal et al. [10] proposed classifier guidance for class-conditional image synthesis with diffusion models. Based on the guidance algorithm proposed in [10], we further explore whether diffusion models can be semantically guided by text or image, or both to synthesize images.",
            "score": 0.636693577453007,
            "section_title": "Related Work",
            "char_start_offset": 7236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 304,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1438
                }
            ],
            "ref_mentions": [
                {
                    "start": 516,
                    "end": 520,
                    "matchedPaperCorpusId": "14888175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75537109375
        },
        {
            "corpus_id": "254408516",
            "title": "SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation",
            "text": "Being able to randomly sample shapes provides limited ability for interaction. Therefore, learning of a conditional distribution is essential for user applications. Importantly, multiple forms of conditional inputs are desirable such that the model can account for various kinds of scenarios. Thanks to the flexible conditional mechanism provided by a latent diffusion model [34], we can incorporate multiple conditional input modalities at once with task-specific encoders E \u03d5 and a cross-attention module. To further allow for more flexibility in controlling the distribution, we adopt classifier-free guidance for conditional generation. The objective function reads as follows: \n\n(3) where E \u03d5i (c i ) is the task-specific encoder for the i th modality, D is a dropout operation enabling classifier-free guidance, and F is a feature aggregation function. In this work, F refers to a simple concatenation. \n\nAt inference time, given conditions from multiple modalities, we perform classifier-free guidance as follows \n\nwhere s i denotes the weight of conditions from the i th modality and \u2205 denotes a condition filled with zeros. Intuitively, modalities with larger weights play more important roles in guiding the conditional generation. \n\nIn this work, we study SDFusion combined with three conditional modalities applied separately or jointly. For shape completion, given a partial observation of a shape, we perform blended diffusion similar to [4]. For single-view 3D reconstruction, we adopt CLIP [32] as the image encoder. For text-guided 3D generation, we adopt BERT [12] as the text encoder. The encoded features are then used to modulate the diffusion process with cross-attention.",
            "score": 0.636208349642316,
            "section_title": "Learning the Conditional Distribution",
            "char_start_offset": 10793,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 908
                },
                {
                    "start": 911,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1694
                }
            ],
            "ref_mentions": [
                {
                    "start": 375,
                    "end": 379,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1452,
                    "end": 1455,
                    "matchedPaperCorpusId": "244714366"
                },
                {
                    "start": 1506,
                    "end": 1510,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1578,
                    "end": 1582,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.912109375
        },
        {
            "corpus_id": "265281572",
            "title": "Enhancing Object Coherence in Layout-to-Image Synthesis",
            "text": "Diffusion models [4,12,24,29,44] are emerging as promising generative models, setting new benchmarks in image generation across various domains [6,8,37,44], including class-conditional [7,45], text-to-image [16,29,30], and image-to-image translation tasks [14,15,21,25,31]. Notably, ADM-G [7] explores the classifier guidance, using the classifier's gradient on noisy images during sampling. Further, Ho et al. [11] introduce a classifier-free approach by interpolating between predictions of a diffusion model Self-similarity Coherence Attention \n\nSS(i,j)",
            "score": 0.6361906215815321,
            "section_title": "Diffusion Model",
            "char_start_offset": 10245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 556
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 23,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 23,
                    "end": 26,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 26,
                    "end": 29,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 144,
                    "end": 147,
                    "matchedPaperCorpusId": "257952298"
                },
                {
                    "start": 185,
                    "end": 188,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 188,
                    "end": 191,
                    "matchedPaperCorpusId": "249953799"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 256,
                    "end": 260,
                    "matchedPaperCorpusId": "257833658"
                },
                {
                    "start": 260,
                    "end": 263,
                    "matchedPaperCorpusId": "246411364"
                },
                {
                    "start": 263,
                    "end": 266,
                    "matchedPaperCorpusId": "246240274"
                },
                {
                    "start": 266,
                    "end": 269,
                    "matchedPaperCorpusId": "251953464"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.654296875
        },
        {
            "corpus_id": "264172506",
            "title": "Elucidating The Design Space of Classifier-Guided Diffusion Generation",
            "text": "Guidance in conditional diffusion generation is of great importance for sample quality and controllability. However, existing guidance schemes are to be desired. On one hand, mainstream methods such as classifier guidance and classifier-free guidance both require extra training with labeled data, which is time-consuming and unable to adapt to new conditions. On the other hand, training-free methods such as universal guidance, though more flexible, have yet to demonstrate comparable performance. In this work, through a comprehensive investigation into the design space, we show that it is possible to achieve significant performance improvements over existing guidance schemes by leveraging off-the-shelf classifiers in a training-free fashion, enjoying the best of both worlds. Employing calibration as a general guideline, we propose several pre-conditioning techniques to better exploit pretrained off-the-shelf classifiers for guiding diffusion generation. Extensive experiments on ImageNet validate our proposed method, showing that state-of-the-art diffusion models (DDPM, EDM, DiT) can be further improved (up to 20%) using off-the-shelf classifiers with barely any extra computational cost. With the proliferation of publicly available pretrained classifiers, our proposed approach has great potential and can be readily scaled up to text-to-image generation tasks. The code is available at https://github.com/AlexMaOLS/EluCD/tree/main.",
            "score": 0.6361691463959346,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.875
        },
        {
            "corpus_id": "258309719",
            "title": "Exploring Compositional Visual Generation with Latent Classifier Guidance",
            "text": "Conditional generation with diffusion models relies on perturbing unconditional generation with user-specified guidance terms, namely classifier guidance [9,39,42] and classifier-free guidance [15]. Although classifier-free guidance performs competitively in image space and is sometimes more favorable than classifier guidance [15,23], we argue that using classifier guidance in latent diffusion models has its unique advantages. Regarding the underperformance of classifier guidance in the image space, one popular suspicion is that image classifiers tend to learn shortcuts from suspicious correlations. For example, a deep neural network classifier on the attribute \"old\" can be misguided by \"white hair\" and ignore its holistic features. This problem is alleviated in a compact, even disentangled latent space, if the semantic directions of 'old' and 'white hair' are orthogonal. Also, deep image classifiers are typically vulnerable to adversarial attacks, while latent classifiers with much few parameters suffer less from this problem. Another benefit is that classifiers are usually easier to train than diffusion models used in classifier-free guidance. Finally, when the classifiers are linear, classifier guidance resembles linear arithmetic methods, as we will show in Section 3.4.\n\nThe goal of conditional generation is to model the conditional distribution p(z|y) where y is the conditions or attributes. By Bayes rules p(z t |y) = p(zt)p(y|zt) /p(y), the score of the conditional probability \u2207 zt log p(z t |y) can be factorized as the unconditional score \u2207 zt log p(z t ) and the gradient flow \u2207 zt log p(y|z t ). Therefore, one simply needs an unconditional latent diffusion model and a latent classifier to model the conditional score, known as classifier guidance. In practice, the classifier guidance term is usually scaled by a factor \u03b1, such that \u2207 zt log p(z t |y) = \u2207 zt log p(z t ) + \u03b1\u2207 zt log p(y|z t ). The factor \u03b1 serves as a temperature parameter which adds another layer of controllability to the sharpness of the",
            "score": 0.6360099812171949,
            "section_title": "Conditional and Compositional Generation",
            "char_start_offset": 9897,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 157,
                    "end": 160,
                    "matchedPaperCorpusId": "14888175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.783203125
        },
        {
            "corpus_id": "277955619",
            "title": "Entropy Rectifying Guidance for Diffusion and Flow Models",
            "text": "Guidance techniques are commonly used in diffusion and flow models to improve image quality and consistency for conditional generative tasks such as class-conditional and text-to-image generation. In particular, classifier-free guidance (CFG) -- the most widely adopted guidance technique -- contrasts conditional and unconditional predictions to improve the generated images. This results, however, in trade-offs across quality, diversity and consistency, improving some at the expense of others. While recent work has shown that it is possible to disentangle these factors to some extent, such methods come with an overhead of requiring an additional (weaker) model, or require more forward passes per sampling step. In this paper, we propose Entropy Rectifying Guidance (ERG), a simple and effective guidance mechanism based on inference-time changes in the attention mechanism of state-of-the-art diffusion transformer architectures, which allows for simultaneous improvements over image quality, diversity and prompt consistency. ERG is more general than CFG and similar guidance techniques, as it extends to unconditional sampling. ERG results in significant improvements in various generation tasks such as text-to-image, class-conditional and unconditional image generation. We also show that ERG can be seamlessly combined with other recent guidance methods such as CADS and APG, further boosting generation performance.",
            "score": 0.6354293699441798,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "269302533",
            "title": "Gradient Guidance for Diffusion Models: An Optimization Perspective",
            "text": "To summarize the related work, we first give an overview of empirical studies relevant to our objective. We then discuss the theory of diffusion models, to which our main contribution is focused. Other related topics, such as direct latent optimization in diffusion models and a detailed review of sampling and statistical theory of diffusion models, are deferred to Appendix A. \n\nClassifier Guidance and Training-free Guidance. [21] introduced classifier-based guidance, steering pre-trained diffusion models towards a particular class during inference. This method offers flexibility by avoiding task-specific fine-tuning, but still requires training a time-dependent classifier. Training-free guidance methods [15,54,67,4,30,47,26] eliminate the need for a timedependent classifier, using only off-the-shelf loss guidance during inference. [15,54,30,26] is a line of works solving inverse problems on image and [67,47] aims for guided/conditional image generation. Though not originally developed for solving optimization problems, [15,67] both propose a similar guidance to ours: taking gradient on the predicted clean data x 0 with respect to corrupted x t . Differently, our paper presents the first rigorous theoretical analysis of this gradient-based guidance approach. Furthermore, we propose an algorithm that iteratively applies the guidance as a module to the local linearization of the optimization objective, demonstrating provable convergence guarantees. \n\nFine-tuning of Diffusion Models. Several methods for fine-tuning diffusion models to optimize downstream reward functions include RL-based fine-tuning [7,24] and direct backpropagation to rewards [17,49,65,60]. However, these approaches often suffer from high computational costs and catastrophic forgetting in pre-trained models. Our guidance method is training-free and applied during the inference phase, eliminating the need to fine-tune diffusion models. \n\nTheory of Diffusion Models. Current theory works primarily focus on unconditional diffusion models. Several studies demonstrate that the distributions generated by diffusion models closely approximate the true data distribution, provided the score function is accurately estimated [19,2,8,36,12,37,14,13,6]. For conditional diffusion models, [68,25] establish sample complexity bounds for learning generic conditional distributions.",
            "score": 0.6351354121712963,
            "section_title": "Related Works",
            "char_start_offset": 5844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1469
                },
                {
                    "start": 1472,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1931
                },
                {
                    "start": 1934,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2366
                }
            ],
            "ref_mentions": [
                {
                    "start": 717,
                    "end": 720,
                    "matchedPaperCorpusId": "260957043"
                },
                {
                    "start": 720,
                    "end": 723,
                    "matchedPaperCorpusId": "257622962"
                },
                {
                    "start": 723,
                    "end": 725,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 731,
                    "end": 734,
                    "matchedPaperCorpusId": "266573790"
                },
                {
                    "start": 847,
                    "end": 850,
                    "matchedPaperCorpusId": "260957043"
                },
                {
                    "start": 853,
                    "end": 856,
                    "matchedPaperCorpusId": "266573790"
                },
                {
                    "start": 914,
                    "end": 918,
                    "matchedPaperCorpusId": "257622962"
                },
                {
                    "start": 1039,
                    "end": 1042,
                    "matchedPaperCorpusId": "257622962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6220703125
        },
        {
            "corpus_id": "276725462",
            "title": "A Review on Generative AI For Text-To-Image and Image-To-Image Generation and Implications To Scientific Images",
            "text": "Similarly to the CGAN, an important extension of the diffusion model is the Guided diffusion model that includes conditional image generation in the network. In that scenario, the model adds conditioning information y at each diffusion step: \n\nUsing Bayes rule with some computations and more importantly by adding the guidance scalar term s, we can show that guided diffusion models aim to learn \u2207logp \u03b8 (x t |y) such that: \n\nIt was also shown in [34] and [3] that a classifier guidance model defined by f \u03a6 (y|x t , t) can guide the diffusion towards the target class y by training f \u03a6 (y|x t , t) on a noisy image x t to predict class y. To do so, we build a classconditional diffusion model with mean \u00b5(x t |y) and variance \u03a3 \u03b8 (x t |y) and perturb the mean by the gradients of logf \u03a6 (y|x t , t) of class y, resulting in: \n\nFigure 6: Algorithm of classifier guided diffusion. Source: [3] Classifier Free-Guidance \n\nClassifier-free guidance, proposed by Ho et al. [12], allows for enhanced control in diffusion models by eliminating the need for separate classifiers. Instead of relying on a separate classifier, which increases training complexity and introduces bias potential, classifier-free guidance trains the diffusion model to directly learn and combine conditional and unconditional distributions during inference, streamlining the process. In other words, the authors train a conditional diffusion model \u03f5 \u03b8 (x t |y) and an unconditional model \u03f5 \u03b8 (x t |y = 0) as a single neural network as follows: \n\nThis approach is advantageous compared to the previous one as it trains a single model to guide the diffusion process and can take different types of conditional data such as text embeddings. We will see that many models rely on classifier free-guidance especially when training on multi-modal data.",
            "score": 0.6340169319233215,
            "section_title": "Conditional Image Generation with Guided Diffusion Classifier Guidance",
            "char_start_offset": 17745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 241
                },
                {
                    "start": 244,
                    "end": 424
                },
                {
                    "start": 427,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 917
                },
                {
                    "start": 920,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1815
                }
            ],
            "ref_mentions": [
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 457,
                    "end": 460,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 889,
                    "end": 892,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9560546875
        },
        {
            "corpus_id": "259982947",
            "title": "Can Diffusion Model Conditionally Generate Astrophysical Images?",
            "text": "Classifier-free guidance is introduced (Ho & Salimans 2021) for text-to-image generation and also used in popular models like Stable Diffusion (Rombach et al. 2021) and Imagen (Saharia et al. 2022). It is shown to achieve a better trade-off between sample quality and diversity. During training time, the label is discarded randomly with a probability (0.28 in this work). So the networks learn both the unconditional and conditional generative models. During the sampling time, the noise prediction \u03b5  is a combination of the unconditional prediction   (x  ) and conditional prediction   (x  |c) with a guidance scale  \n\nwhere  = 0 leads to a standard conditional generative model, and  = \u22121 leads to a standard unconditional generative model. Note that the above formula can be rewritten by \u03b5  (x  |c) =   (x  ) +  \u2032 (  (x  |c) \u2212   (x  )) (Nichol et al. 2022), where  \u2032 = 1 leads to a standard conditional generative model. From this form, the guidance scale can be interpreted by guiding the unconditional model to the direction where the output image meets the input condition (Dieleman 2022). When  \u2032 > 1, or equivalently,  > 0 in Equation 3, the diffusion models tend to conditionally generate higher-quality natural images, but at costs to image diversity.",
            "score": 0.6337383201176588,
            "section_title": "Classifier-free guidance",
            "char_start_offset": 7479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1263
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8271484375
        },
        {
            "corpus_id": "258480331",
            "title": "Shap-E: Generating Conditional 3D Implicit Functions",
            "text": "Dhariwal and Nichol [9] first showed this effect using image-space gradients from a noise-aware classifier, and Ho and Salimans [21] later proposed classifier-free guidance to remove the need for a separate classifier. To utilize classifier-free guidance, we train our diffusion model to condition on some information y (e.g. a conditioning image or textual description), but randomly drop this signal during training to enable the model to make unconditional predictions. \n\nDuring sampling, we then adjust our model prediction as follows: \n\nwhere s is a guidance scale. When s = 0 or s = 1, this is equivalent to regular unconditional or conditional sampling, respectively. Setting s > 1 typically produces more coherent but less diverse samples. We employ this technique for all of our models, finding (as expected) that guidance is necessary to obtain the best results.",
            "score": 0.6320645137257972,
            "section_title": "Diffusion Models",
            "char_start_offset": 10899,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 539
                },
                {
                    "start": 542,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 872
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 132,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.888671875
        },
        {
            "corpus_id": "270391454",
            "title": "CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models",
            "text": "Classifier-free guidance (CFG) is a fundamental tool in modern diffusion models for text-guided generation. Although effective, CFG has notable drawbacks. For instance, DDIM with CFG lacks invertibility, complicating image editing; furthermore, high guidance scales, essential for high-quality outputs, frequently result in issues like mode collapse. Contrary to the widespread belief that these are inherent limitations of diffusion models, this paper reveals that the problems actually stem from the off-manifold phenomenon associated with CFG, rather than the diffusion models themselves. More specifically, inspired by the recent advancements of diffusion model-based inverse problem solvers (DIS), we reformulate text-guidance as an inverse problem with a text-conditioned score matching loss and develop CFG++, a novel approach that tackles the off-manifold challenges inherent in traditional CFG. CFG++ features a surprisingly simple fix to CFG, yet it offers significant improvements, including better sample quality for text-to-image generation, invertibility, smaller guidance scales, reduced mode collapse, etc. Furthermore, CFG++ enables seamless interpolation between unconditional and conditional sampling at lower guidance scales, consistently outperforming traditional CFG at all scales. Moreover, CFG++ can be easily integrated into high-order diffusion solvers and naturally extends to distilled diffusion models. Experimental results confirm that our method significantly enhances performance in text-to-image generation, DDIM inversion, editing, and solving inverse problems, suggesting a wide-ranging impact and potential applications in various fields that utilize text guidance. Project Page: https://cfgpp-diffusion.github.io/.",
            "score": 0.6312829467975142,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91748046875
        },
        {
            "corpus_id": "270521423",
            "title": "ControlVAR: Exploring Controllable Visual Autoregressive Modeling",
            "text": "Inspired by the classifier-free guidance Ho & Salimans (2022) from diffusion models, we empirically find a similar form of guidance that can be used for autoregressive sample conditional images based on teacher forcing. In this section, we aim to analyze the spirit of classifier-free guidance (CFG) and analogy it to our teacher-forcing guidance (TFG). In the diffusion models, \u2207 I log p(I| * ) is represented by the logits outputted by the diffusion-UNet. In this way, during inference, the classifier-free guidance can be calculated as \n\nwhere \u03b3 C , \u03b3 c , \u03b3 ct are the guidance scales that are used to adjust the amplitude to apply the conditional guidance. \u2205 denotes leveraging a special empty token to replace the original token to disable the additional conditional information Ho & Salimans (2022).",
            "score": 0.6310772985074364,
            "section_title": "B DISCUSSION OF THE TEACHER FORCING GUIDANCE",
            "char_start_offset": 17011,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 805
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59912109375
        },
        {
            "corpus_id": "258557506",
            "title": "Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning",
            "text": "Diffusion models are initially used for unconditional image synthesis [15,24,26,38] and show its ability in generating diverse, high-quality samples. In order to control the generation of diffusion models, Dhariwal et al. [10] firstly propose a conditional image synthesis method utilizing classifier guidance. Subsequent works [18,23] use CLIP, which contains multi-modal information of text and images, to achieve text-guided image synthesis. Ho and Salimans [16] propose classifier-free guidance, which incorporates the conditional mechanism into the diffusion process to achieve conditional image synthesis without external classifiers. Nichol et al. [25] firstly train a conditional diffusion model utilizing classifier-free guidance on large datasets of image-text pairs, achieving great success in text-to-image synthesis. Following that, some representative studies [2,4,20,30,32,34] of text-to-image diffusion models have been proposed, based on the conditioning mechanism. Our experiments are based on Stable Diffusion [32], which we will introduce in detail later, because of its wide applications.",
            "score": 0.629932737247815,
            "section_title": "RELATED WORK 2.1 Diffusion Models",
            "char_start_offset": 7365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1109
                }
            ],
            "ref_mentions": [
                {
                    "start": 70,
                    "end": 74,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 77,
                    "end": 80,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 222,
                    "end": 226,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 328,
                    "end": 332,
                    "matchedPaperCorpusId": "244909410"
                },
                {
                    "start": 332,
                    "end": 335,
                    "matchedPaperCorpusId": "245117331"
                },
                {
                    "start": 885,
                    "end": 888,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 888,
                    "end": 891,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1029,
                    "end": 1033,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9345703125
        },
        {
            "corpus_id": "276741036",
            "title": "DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes",
            "text": "Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) could improve the performance of conditional generative models without relying on an external classifier. Specifically, during training, the model simultaneously learns both conditional generation p(x|c) and unconditional generation p(x), and guidance during sampling is provided by the following equation: \n\nwhere xt (c) is the result conditioned on c, xt (\u2205) is the unconditioned result, and w is a weight parameter controlling the strength of the conditional guidance. By adjusting w, an appropriate balance between the accuracy and diversity of the generated scenes can be achieved.",
            "score": 0.6295838710604886,
            "section_title": "A.4 CLASSIFIER-FREE GUIDANCE",
            "char_start_offset": 21593,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 359
                },
                {
                    "start": 362,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 639
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7490234375
        },
        {
            "corpus_id": "266693678",
            "title": "Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale",
            "text": "The diffusion model (Sohl-Dickstein et al., 2015;Song & Ermon, 2019;Song et al., 2020b) is a family of generative models that produce high-quality samples utilizing the diffusion processes of molecules. Denoising diffusion probabilistic model (DDPM) (Ho et al., 2020) is one of the most popular diffusion models whose diffusion process can be viewed as a sequence of denoising steps. It is the core of several large text-to-image models that have been widely adopted in AI art production, such as the latent diffusion model (stable diffusion) (Rombach et al., 2021) and DALL\u2022E (Betker et al., 2023). DDPMs control the generation of samples by learning conditional distributions that are specified by sam-ple contexts. However, such conditional DDPMs has weak control over context hence do not work well to generate desired content (Luo, 2022). \n\nGuidance techniques, notably classifier guidance (Song et al., 2020b;Dhariwal & Nichol, 2021) and classifierfree guidance (Ho, 2022), provide enhanced control at the cost of sample diversity. Classifier guidance, requiring an additional classifier, faces implementation challenges in non-classification tasks like text-to-image generation. Classifier-free guidance circumvents this by linearly combining conditional and unconditional DDPM weighted by a guidance scale parameter. However, large guidance scale often leads to overly saturated and unnatural images (Ho, 2022). Though techniques like dynamic thresholding (Saharia et al., 2022) can handle color issues by clipping outof-range pixel values, a systematic solution for general sampling tasks, including latent space diffusion (Rombach et al., 2021) and those beyond image generation, remains absent. This paper proposes characteristic guidance as a systematic solution to the large guidance scale issue by addressing the non-linearity neglicted in classifier-free guidance.",
            "score": 0.6291559344696824,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 68,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 543,
                    "end": 565,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1632,
                    "end": 1654,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90087890625
        },
        {
            "corpus_id": "260126018",
            "title": "AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models",
            "text": "Diffusion models have shown great generation quality and diversity in the image synthesis task since Ho et al. [?] proposed a probabilistic diffusion model for image generation that greatly improved the performance of diffusion models. Diffusion models for conditional image generation are extensively developed for more usable and flexible image synthesis. Dhariwal & Nichol [?] proposed a conditional diffusion model that adopted classifier-guidance for incorporating label information into the diffusion model. They separately trained an additional classifier and utilized the gradient of the classifier for conditional image generation. Jonathan Ho & Tim Salimans [?] performed the conditional guidance without an extra classifier to a diffusion model. They trained a conditional diffusion model together with a standard diffusion model. During sampling, they adopted the combination of these two models for image generation. Their idea is motivated by an implicit classifier with the Bayes rule. Followed by [?, ?]'s works, many research [?,?,?,?] have been proposed to achieve state-of-the-art performance on image generation, image inpainting, and text-to-image generation tasks. Despite utilizing diffusion models for image generation has been widely discussed, none of these works have discovered the adversarial examples generation method with the diffusion model. Also, it is a new challenge to defend against the adversarial examples generated by the diffusion model.",
            "score": 0.6288879124774235,
            "section_title": "D.1 Conditional Diffusion Model for Image Generation",
            "char_start_offset": 27087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1479
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74462890625
        },
        {
            "corpus_id": "271051241",
            "title": "Segmentation-Free Guidance for Text-to-Image Diffusion Models",
            "text": "Image generation using classifier-free guidance involves two forward passes of the diffusion network per iteration: one that uses conditional information and one that does not.The conditional information generally involves a (positive) text prompt describing different objects of interest in the generated image.For instance, in Fig. 1-a, the positive prompt is \"a cute Maltese white dog next to a cat.\"The forward pass without conditional information is usually carried out by an empty (negative) prompt (i.e., \"\").However, it is possible to employ non-empty negative prompts.This type of guidance allows the objects present in the positive, but not the negative, prompt to become more prominent.Nonetheless, the issue with having such negative prompts is that it interacts with the generated image globally.\n\nOur objective is to dynamically adjust the negative prompt for each image patch.We examine attention maps within the diffusion model, specifically where it interacts with the text prompt embeddings.For each patch of the attention map, we aim to find the object in the positive prompt with the highest correlation.Subsequently, this selected object is excluded from the negative prompt interacting with that specific patch.Accordingly, the forward pass of the diffusion model carries out as if each patch cross-attends dynamically with a different negative prompt.Furthermore, the corresponding attention weight is adjusted to account for self-attention interactions.Since this proposed method of guidance does not involve any segmentation network as a guidance function, we term this method as segmentationfree guidance.Our method realizes local interaction between prompt embedding and feature patches while dynamically adjusting the negative prompts; thus, it produces better image generation quality, as shown in Fig. 1.\n\nOur contributions can be summarized as follows: \u2022 We introduce a novel mechanism named segmentationfree guidance that effectively adjusts the negative prompt for each patch of the generated image based on the category of the patch.\u2022 We also propose an efficient subjective evaluation methodology that involves sub-sampling of prompts dataset for assessment.The chosen subset of prompts ensures the representation of dataset diversity while maintaining fairness in terms of model performance.\u2022 Finally, we perform extensive evaluation on the MS-COCO datasets on which we show both qualitative and quantitative improvement.",
            "score": 0.6287030751998899,
            "section_title": "Introduction",
            "char_start_offset": 2473,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 176,
                    "end": 312
                },
                {
                    "start": 312,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 516
                },
                {
                    "start": 516,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 809
                },
                {
                    "start": 811,
                    "end": 891
                },
                {
                    "start": 891,
                    "end": 1009
                },
                {
                    "start": 1009,
                    "end": 1124
                },
                {
                    "start": 1124,
                    "end": 1233
                },
                {
                    "start": 1233,
                    "end": 1374
                },
                {
                    "start": 1374,
                    "end": 1477
                },
                {
                    "start": 1477,
                    "end": 1631
                },
                {
                    "start": 1631,
                    "end": 1834
                },
                {
                    "start": 1836,
                    "end": 2067
                },
                {
                    "start": 2067,
                    "end": 2193
                },
                {
                    "start": 2193,
                    "end": 2327
                },
                {
                    "start": 2327,
                    "end": 2457
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "272593086",
            "title": "Training-Free Guidance for Discrete Diffusion Models for Molecular Generation",
            "text": "Diffusion models are a powerful method for generating data from a given distribution. To enhance their utility, extensive research has focused on developing techniques for guiding the output rather than relying solely on unconditional generation. These guidance methods have significantly evolved since the introduction of classifier guidance [1], which established diffusion models as the state-of-the-art for image generation, surpassing GANs. Classifier guidance introduced a framework allowing the gradients from a classifier to influence the generation process. The main limitation of this method is that it requires the classifier to perform well on the data at all timesteps t. This necessitates training a guidance model with a specific noise scheduler to augment the training data across all timesteps. \n\nClassifier-free guidance addresses this issue by training a diffusion model that can condition on specific attributes, allowing for both unconditional and conditional generation [2]. During sampling, classifier-free guidance combines the outputs of a single model, both when conditioned on specific attributes and when unconditioned, effectively guiding the generation process toward the desired attributes, similar to how a Bayes classifier influences predictions. However, the main drawback of this method is that the attributes for guidance must be fixed beforehand during training. \n\nA new flexible approach for guiding the sampling of diffusion models is training-free guidance, which allows guidance models to be paired with a diffusion model without requiring the guidance model to be trained on noisy data generated by the noise scheduler [3]- [6]. This enables the creation of foundation diffusion models that can be combined with guidance models in a plug-and-play manner. It also simplifies benchmarking and building on other researchers' work, as separate guidance models are unnecessary and can be easily shared. These improvements have made diffusion models the preferred architecture in many domains where control and human feedback are essential. Moreover, unlike autoregressive models, diffusion models learn the joint data distribution directly and do not rely on chaining conditional distributions. Their iterative nature also provides unique opportunities to guide the generation process in ways that autoregressive or purely conditional models cannot replicate. \n\nClassifier guidance [7] and classifier-free guidance [8] have previously been implemented for graph generation using a discrete diffusion model.",
            "score": 0.6253227417098398,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 811
                },
                {
                    "start": 814,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2231
                },
                {
                    "start": 2232,
                    "end": 2396
                },
                {
                    "start": 2399,
                    "end": 2543
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 346,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 992,
                    "end": 995,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1661,
                    "end": 1664,
                    "matchedPaperCorpusId": "256846836"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91162109375
        },
        {
            "corpus_id": "259924894",
            "title": "Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement",
            "text": "Guided Diffusions. For image generations, guiding the backward diffusion process towards higher log probabilities predicted by a classifier (which can be viewed as the reward signal) leads to improved sample quality, where the classifier can either be separated trained, i.e., classifier-guided [11] or implicitly specified by the conditioned diffusion models, i.e., classifier-free [18]. Classifier-free guidance has become a standard technique in the state-of-the-art text-to-image diffusion models [39,38,4]. Other types of guidance are also explored [33,14].",
            "score": 0.6246717230626745,
            "section_title": "Related Work",
            "char_start_offset": 2761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 562
                }
            ],
            "ref_mentions": [
                {
                    "start": 295,
                    "end": 299,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 501,
                    "end": 505,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8955078125
        },
        {
            "corpus_id": "254564528",
            "title": "The Stable Artist: Steering Semantics in Diffusion Latent Space",
            "text": "The first step towards the Stable Artist is guided diffusion. Specifically, diffusion models iteratively denoise a Gaussian distributed variable to produce samples of a learned data distribution. For text-to-image generation, the model is conditioned on a text prompt p and guided towards an image, faithful to that prompt. The training objective of a diffusion model x\u03b8 , can be written as \n\nwhere (x, c p ) is conditioned on text prompt p, t is drawn from a uniform distribution t \u223c U([0, 1]), \u03f5 sampled from a Gaussian \u03f5 \u223c N (0, I), and w t , \u03c9 t , \u03b1 t influence image fidelity depending on t. Consequently, the DM is trained to denoise z t := x + \u03f5 to yield x with the squared error as a loss. At inference, the DM is sampled using the model's prediction of x = (z t \u2212 \u03b5\u03b8 ), with \u03b5\u03b8 as described below. \n\nClassifier-free guidance [5] is a conditioning method using a purely generative diffusion model, eliminating the need for an additional pre-trained classifier. The approach randomly drops the text conditioning c p with a fixed probability during training, resulting in a joint model for unconditional and conditional objectives. During inference the score estimates for the x-prediction are adjusted so that: \n\nwith guidance scale s g and \u03f5 \u03b8 defining the noise estimate with parameters \u03b8. Intuitively, the unconditioned \u03f5prediction is pushed in the direction of the conditioned one, with the s g determining the extent of the adjustment.",
            "score": 0.6245816575204086,
            "section_title": "Guided Diffusion",
            "char_start_offset": 4241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1447
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89013671875
        },
        {
            "corpus_id": "268248478",
            "title": "Theoretical Insights for Diffusion Guidance: A Case Study for Gaussian Mixture Models",
            "text": "To uncover the unreasonable power of these guided approaches and better assist practice, this paper takes the first step towards this goal in the context of diffusion models. Diffusion models, which convert noise into new data instances by learning to reverse a Markov diffusion process, have become a cornerstone in contemporary generative modeling (Ho et al., 2020;Song et al., 2020b;Yang et al., 2023). Compared to alternative generative models, such as variational autoencoder or generative adversarial network, diffusion models are known to be more stable, and generate high-quality samples based on learning the gradient of the log-density function (also known as the score function). When data is multi-modal, namely, it potentially comes from multiple classes, a natural question is how to make use of these class labels for conditional synthesis. Towards this direction, Dhariwal and Nichol (2021) put forward the idea of classifier guidancean approach to enhance the sample quality with the aid of an extra trained classifier. The classifier guidance approach combines an unconditional diffusion model's score estimate with the gradient of the log probability of a classifier. Subsequently, Ho and Salimans (2022) presented the so-called classifier-free guidance, which \n\nFigure 1: The effect of guidance on a three-component GMM in R 2 . Each component has weight 1/3 and identity covariance, and the component centers are ( \u221a 3/2, 1/2), (\u2212 \u221a 3/2, 1/2) and (0, \u22121). The leftmost panel displays the unguided density. We increase the guidance strength from left to right. This plot imitates Figures 2 of Ho and Salimans (2022). \n\ninstead mixes the score estimates of an unconditional diffusion model with that of a conditional diffusion model jointly trained over the data and the label. For both guidance methods, adjusting the mixing weights of the unconditional score estimate and the other component controls the trade-off between the Fr\u00e9chet Inception Distance (FID) and the Inception Score (IS) in the context of image synthesis.",
            "score": 0.6235022633263212,
            "section_title": "Training with guidance for diffusion models",
            "char_start_offset": 1407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1636
                },
                {
                    "start": 1639,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 2044
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 367,
                    "matchedPaperCorpusId": "219955663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.853515625
        },
        {
            "corpus_id": "256390486",
            "title": "AudioLDM: Text-to-Audio Generation with Latent Diffusion Models",
            "text": "After classifier guidance (Song et al., 2021;Nichol & Dhariwal, 2021), classifier-free guidance (Ho & Salimans, 2021;Nichol et al., 2021) (CFG) has been the state-of-the-art technique for guiding diffusion models. During training, we randomly discard our condition E x with a fixed probability, e.g., 10% to train both the conditional LDMs \u03f5 \u03b8 (z n , n, E x ) and the unconditional LDMs \u03f5 \u03b8 (z n , n). In generation, we use text embedding E y as condition and perform sampling with a modified noise estimation \u03b5\u03b8 (z n , n, E y ):",
            "score": 0.6199760013935004,
            "section_title": "Conditioning Augmentation",
            "char_start_offset": 14397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 529
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 45,
                    "matchedPaperCorpusId": "227209335"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56640625
        },
        {
            "corpus_id": "278394336",
            "title": "Guide your favorite protein sequence generative model",
            "text": "Guidance is a useful technique that enables conditional generation with diffusion and flow models. Originally developed for continuous state-space models, guidance allows steering the generative process toward samples with desired properties without requiring conditional retraining [13][14][15]. In continuous state-space diffusion models, the generation process involves sampling from a reverse stochastic differential equation (SDE) that gradually transforms noise into structured data. To condition this process on a desired property y, it has been shown that one can leverage Bayes' theorem to obtain the conditional score function: \n\nThis formulation, termed classifier guidance, combines the unconditional score \u2207 xt log p t (x t ) with the score of a predictor model \u2207 xt log p t (y|x t ). The strength of guidance can be controlled by introducing a parameter \u03b3: \n\nThis approach provides several advantages: (ii) it enables conditioning an unconditional model without retraining, (ii) the guidance strength \u03b3 can be adjusted at inference time, and (iii) it allows composition of multiple guidance signals. Ho and Salimans [42] later introduced classifier-free guidance, which achieves similar results without an explicit classifier by combining a conditionally trained model and an unconditional model, which in practice are typically trained jointly. Classifier-free guidance has been shown to improve sample quality [42] compared to using only the conditionally trained model, and recent work on autoguidance showed further improved sample quality by combining a trained model with a more inferior version of itself [43]. \n\nWhile guidance, either classifier-based, classifier-free or autoguidance, has proven extremely valuable for continuous state-space models, extending these concepts to discrete state-spaces poses unique challenges, since the score functions are not defined and these formulations cannot be directly applied. We sidestep the need for a score function by instead returning to Bayes' theorem, which, in the context of diffusion on discrete state-spaces, dictates that we must effectively compute \n\nwhere x t+dt and x t are separated by a infinitesimal time step dt, and y is the desired property we wish to condition on.",
            "score": 0.6189361904558177,
            "section_title": "Background on diffusion guidance",
            "char_start_offset": 19814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1631
                },
                {
                    "start": 1634,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2125
                },
                {
                    "start": 2128,
                    "end": 2250
                }
            ],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 287,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 287,
                    "end": 291,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 291,
                    "end": 295,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 1130,
                    "end": 1134,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1426,
                    "end": 1430,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1626,
                    "end": 1630,
                    "matchedPaperCorpusId": "270226598"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68994140625
        },
        {
            "corpus_id": "276249479",
            "title": "History-Guided Video Diffusion",
            "text": "Classifier-free guidance (CFG) (Ho & Salimans, 2022) is a crucial technique for improving sample quality in diffusion models. CFG jointly trains conditional and unconditional models s \u03b8 (x, c, k) \u2248 \u2207 log p k (x k |c) and s \u03b8 (x, \u2205, k) \u2248 \u2207 log p k (x k ) by randomly dropping out the conditioning c. During sampling, the true conditional score \u2207 log p k (x k |c) is replaced with the weighted score \n\nwhere \u03c9 \u2265 1 is the guidance scale that pushes the sample towards the conditioning. In VDMs, CFG is predominantly used for text guidance (Ho et al., 2022b;Wang et al., 2023). For frame conditioning, \"first frame\" guidance is commonplace in image-to-video models (Blattmann et al., 2023a;  Yang et al., 2024), or \"fixed set of few frames\" (Blattmann et al., 2023b;Gupta et al., 2023;Watson et al., 2024), likewise in multi-view diffusion models (Gao et al., 2024). \n\nOur work generalizes CFG by enabling guidance with a variable number of conditioning frames and later extends beyond the conventional approach of subtracting an unconditioned score -similar to prior works in compositional generative models (Du & Kaelbling, 2024;Liu et al., 2022;Du et al., 2023), we compose score from multiple conditioning to combine their behaviors. Additionally, we eliminate the reliance on binary-dropout training, the default mechanism for enabling CFG, which we empirically show performs sub-optimally when extended to history guidance. Diffusion Forcing. Traditionally, diffusion models are trained using uniform noise levels across all tokens. Diffusion Forcing (DF) (Chen et al., 2024) proposes training sequence diffusion models with independently varied noise levels per frame. Although DF provides theoretical and empirical support for this approach, their work focuses on causal, state-space models. CausVid (Yin et al., 2024) builds on DF by scaling it to a causal transformer, creating an autoregressive video foundation model.",
            "score": 0.6188548368531862,
            "section_title": "Preliminaries and Related Work",
            "char_start_offset": 6172,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1925
                }
            ],
            "ref_mentions": [
                {
                    "start": 737,
                    "end": 762,
                    "matchedPaperCorpusId": "258187553"
                },
                {
                    "start": 1127,
                    "end": 1144,
                    "matchedPaperCorpusId": "249375227"
                },
                {
                    "start": 1144,
                    "end": 1160,
                    "matchedPaperCorpusId": "257078922"
                },
                {
                    "start": 1558,
                    "end": 1577,
                    "matchedPaperCorpusId": "270869622"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83837890625
        },
        {
            "corpus_id": "266374764",
            "title": "Adaptive Guidance: Training-free Acceleration of Conditional Diffusion Models",
            "text": "This paper presents a comprehensive study on the role of Classifier-Free Guidance (CFG) in text-conditioned diffusion models from the perspective of inference efficiency. In particular, we relax the default choice of applying CFG in all diffusion steps and instead search for efficient guidance policies. We formulate the discovery of such policies in the differentiable Neural Architecture Search framework. Our findings suggest that the denoising steps proposed by CFG become increasingly aligned with simple conditional steps, which renders the extra neural network evaluation of CFG redundant, especially in the second half of the denoising process. Building upon this insight, we propose\"Adaptive Guidance\"(AG), an efficient variant of CFG, that adaptively omits network evaluations when the denoising process displays convergence. Our experiments demonstrate that AG preserves CFG's image quality while reducing computation by 25%. Thus, AG constitutes a plug-and-play alternative to Guidance Distillation, achieving 50% of the speed-ups of the latter while being training-free and retaining the capacity to handle negative prompts. Finally, we uncover further redundancies of CFG in the first half of the diffusion process, showing that entire neural function evaluations can be replaced by simple affine transformations of past score estimates. This method, termed LinearAG, offers even cheaper inference at the cost of deviating from the baseline model. Our findings provide insights into the efficiency of the conditional denoising process that contribute to more practical and swift deployment of text-conditioned diffusion models.",
            "score": 0.6176096835067953,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89599609375
        },
        {
            "corpus_id": "268667035",
            "title": "DreamFlow: High-Quality Text-to-3D Generation by Approximating Probability Flow",
            "text": "Text-to-image diffusion models.Text-to-image diffusion models (Nichol et al., 2021;Saharia et al., 2022;Rombach et al., 2022) are conditional generative models that are trained with text embeddings.They utilize classifier-free guidance (CFG) (Ho & Salimans, 2022), which learns both conditional and unconditional models, and guide the sampling by interpolating the predictions with guidance scale \u03c9: D \u03c9 (x; \u03c3, y) = (1 + \u03c9)D(x; \u03c3, y) \u2212 \u03c9D(x; \u03c3), where y is a text prompt.Empirically, \u03c9 > 0 controls the tradeoff between sample fidelity and diversity.CFG scale is important in text-to-3D generation, as for the convergence of 3D optimization (Poole et al., 2022;Wang et al., 2023b).",
            "score": 0.6174653856889771,
            "section_title": "INTRODUCTION",
            "char_start_offset": 6882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 31,
                    "end": 198
                },
                {
                    "start": 198,
                    "end": 471
                },
                {
                    "start": 471,
                    "end": 550
                },
                {
                    "start": 550,
                    "end": 681
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 104,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 104,
                    "end": 125,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.861328125
        },
        {
            "corpus_id": "271719885",
            "title": "Diffusion Model Meets Non-Exemplar Class-Incremental Learning and Beyond",
            "text": "They have shown impressive performance and note that our method falls into this category. Diffusion Model. Diffusion models draw inspiration from non-equilibrium thermodynamics, and they have shown exceptional performance in various image-related tasks, including unconditional and class-conditional generation [15], image-to-image translation [56] and text-to-image synthesis [54]. DDPM [25] constructs a Markov chain of discrete steps to progressively add random noise for the input and then learn to reverse the diffusion process, thereby enabling the generation of desired data samples from the noise. It exhibits slow sampling speed and DDIM [62] can be used to accelerate the denoising process. Conditional generation is primarily achieved through two approaches: classifier guidance [15,37] and classifier-free guidance [26]. The former involves initially training an unconditional diffusion model, followed by a classifier to guide generation process. The latter incorporates conditions directly during training. Here we adopt the simple yet effective classifier-free guidance approach for class-conditional generation. \n\nSelf-Supervised Learning. Learning with self-supervision has proven to be effective to learn general representations. The recent studies begin with artificially designed pretext tasks, e.g., prediction rotations [19], patch permutation [47] and image colorization [33]. More recently, contrastive learning has achieved great success, and the core idea is to contrast positive pairs against negative pairs. In practice, this methodology benefits from a large number of negative samples [71,63,23,11], resulting in the need for a memory bank [23] or a large batch size [11]. BYOL [22] predicts the output of one view directly based on another view, obviating the necessity for a large batch size. SimSiam [12] introduces the stop-gradient operation, which further eliminates the need for a momentum encoder, appearing simpler yet equally effective. In CIL, some works explore improving the quality of the learned representations by SSL and prove its effectiveness. [69] attempts to learn class-agnostic knowledge and multi-perspective knowledge by SSL. PASS [81] employs a rotation-based proxy task to boost the performance, which can be viewed as a combination of supervised and self-supervised learning.",
            "score": 0.6164863621524683,
            "section_title": "Related Work",
            "char_start_offset": 7951,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1127
                },
                {
                    "start": 1130,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2180
                },
                {
                    "start": 2181,
                    "end": 2333
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 315,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 344,
                    "end": 348,
                    "matchedPaperCorpusId": "233241040"
                },
                {
                    "start": 377,
                    "end": 381,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 388,
                    "end": 392,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 790,
                    "end": 794,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 794,
                    "end": 797,
                    "matchedPaperCorpusId": "245117331"
                },
                {
                    "start": 1366,
                    "end": 1370,
                    "matchedPaperCorpusId": "187547"
                },
                {
                    "start": 1394,
                    "end": 1398,
                    "matchedPaperCorpusId": "7023610"
                },
                {
                    "start": 1615,
                    "end": 1619,
                    "matchedPaperCorpusId": "4591284"
                },
                {
                    "start": 1619,
                    "end": 1622,
                    "matchedPaperCorpusId": "189762205"
                },
                {
                    "start": 1622,
                    "end": 1625,
                    "matchedPaperCorpusId": "207930212"
                },
                {
                    "start": 1625,
                    "end": 1628,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 1670,
                    "end": 1674,
                    "matchedPaperCorpusId": "207930212"
                },
                {
                    "start": 1697,
                    "end": 1701,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 1708,
                    "end": 1712,
                    "matchedPaperCorpusId": "219687798"
                },
                {
                    "start": 1833,
                    "end": 1837,
                    "matchedPaperCorpusId": "227118869"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.853515625
        },
        {
            "corpus_id": "271600562",
            "title": "Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy Curvature of Attention",
            "text": "Classifier-free guidance (CFG) [14], first proposed as a replacement for classifier guidance (CG) [8] is controlled by a scale parameter. The higher we set classifier-free guidance, the more we get faithful, high-quality images. However, it requires external labels, such as text [30] or class [8] labels, making it impossible to apply to unconditional diffusion models. Also, it requires specific traning procedure with label dropping and it is known that high CFG causes saturation [42]. \n\nFigure 4: Conditional generation using ControlNet [51] and SEG. Table 1: Quantitative comparison of SEG with vanilla SDXL [35], SAG [17], and PAG [1] for unconditional generation.",
            "score": 0.6164607641083202,
            "section_title": "Discussion on related work",
            "char_start_offset": 15881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 671
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 101,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 294,
                    "end": 297,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 484,
                    "end": 488,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 542,
                    "end": 546,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 624,
                    "end": 628,
                    "matchedPaperCorpusId": "252683688"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81103515625
        },
        {
            "corpus_id": "256416107",
            "title": "Meta-Learning via Classifier(-free) Diffusion Guidance",
            "text": "To improve the quality of sampled networks, the classifier guidance technique presented in Section 4.2 can be also combined together with diffusion models. The gradient of an auxiliary classifier (or CLIP encoder) can be added during sampling to induce an effect similar to GAN truncation (Brock et al., 2018), producing samples that are less diverse but of higher quality. \n\nThe classifier-free guidance technique (Ho & Salimans, 2021;Nichol et al., 2022) allows us to leverage a conditional diffusion model to obtain the same effect as above, without the auxiliary classifier. To do so, we train the conditional network \u03c8 (z t , t, e i ) to also model the unconditional case \u03c8 (z t , t). One way of doing this is with conditioning dropout, simply dropping the conditional input e i for a certain percentage of training samples, inputting zeros instead. \n\nWe can then sample at each diffusion iteration with \n\nFor \u03b3 = 0, this recovers the unconditional diffusion model, while for \u03b3 = 1 it recovers the standard task-conditional model. For \u03b3 > 1, we instead obtain the classifier-free guidance effect, which we show results in the sampling of latent vectors \u1e91i corresponding to higher-performing taskconditional network weights h(\u1e91 i , \u03c8) = \u0174i . We point to a more in-depth discussion on classifier-free guidance and its connection to score matching in Appendix A.1.",
            "score": 0.6160860215456911,
            "section_title": "Classifier-Free Guidance for Meta-Learning",
            "char_start_offset": 20015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 854
                },
                {
                    "start": 857,
                    "end": 908
                },
                {
                    "start": 911,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1366
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.908203125
        },
        {
            "corpus_id": "248006185",
            "title": "Video Diffusion Models",
            "text": "In the conditional generation setting, the data x is equipped with a conditioning signal c, which may represent a class label, text caption, or other type of conditioning. To train a diffusion model to fit p(x|c), the only modification that needs to be made is to provide c to the model as x\u03b8 (z t , c). \n\nImprovements to sample quality can be obtained in this setting by using classifier-free guidance [20]. This method samples using adjusted model predictions \u02dc \u03b8 , constructed via \n\nwhere w is the guidance strength, \n\nis the regular conditional model prediction, and \u03b8 (z t ) is a prediction from an unconditional model jointly trained with the conditional model (if c consists of embedding vectors, unconditional modeling can be represented as c = 0). For w > 0 this adjustment has the effect of over-emphasizing the effect of conditioning on the signal c, which tends to produce samples of lower diversity but higher quality compared to sampling from the regular conditional model [20]. The method can be interpreted as a way to guide the samples towards areas where an implicit classifier p(c|z t ) has high likelihood, and is an adaptation of the explicit classifier guidance method proposed by [16].",
            "score": 0.6159075084551731,
            "section_title": "Background",
            "char_start_offset": 4036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 303
                },
                {
                    "start": 306,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 483
                },
                {
                    "start": 486,
                    "end": 519
                },
                {
                    "start": 522,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1208
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 407,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 987,
                    "end": 991,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1203,
                    "end": 1207,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8681640625
        },
        {
            "corpus_id": "268063857",
            "title": "Diffusion Language Models Are Versatile Protein Learners",
            "text": "During training, we freeze the parameters of the modality encoder and DPLM, and only update the parameters of the adapter via supervised fine-tuning on the given paired data (x, c). We then obtain a conditional DPLM for p \u03b8 (x|E \u03d5 (c)) making the full potentials of both DPLM and the modality expert E \u03d5 (c). In \u00a7D.5, we also develop classifier-free guidance for such adapter-tuned DPLM as an immediately available booster for cross-modal conditional generation without intricate condition dropout during training. Case III: Plug-and-play controllable generation with discrete classifier guidance (Fig. 1C-3). Directly building a conditional model is prohibitive in most cases due to data scarcity. Thus, incorporating classifier guidance into continuous diffusion models (Dhariwal & Nichol, 2021a) proves particularly useful. This integration with pretrained classifiers enables steering generation towards desired preferences. However, continuous classifier guidance requires valid definition of \u2207 x log p \u03b8 (x), or \"score\" (Song & Ermon, 2019), which does not exist for discrete diffusion. Inspired by continuous diffusion classifier guid-ance and DiGress on guided graph diffusion (Vignac et al., 2022), here we introduce classifier-guided conditional generation for discrete diffusion LMs. Concretely, we want to sample from the conditional distribution of q(x (t\u22121) |x (t) , y) \u221d q(x (t\u22121) |x (t) )q(y|x (t\u22121) ), which is approximated by p \u03b8 (x (t\u22121) |x (t) )p \u03d5 (y|x (t\u22121) ) where p \u03d5 (y|x (t\u22121) ) is a discriminative guidance model (classifier or regressor w.r.t. user's desired properties). However, p \u03d5 (y|x (t\u22121) ) cannot be factorized as a product over all positions, prohibiting evaluation of all possible values of x (t\u22121) .",
            "score": 0.615654684742144,
            "section_title": "Conditioning",
            "char_start_offset": 17770,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1738
                }
            ],
            "ref_mentions": [
                {
                    "start": 772,
                    "end": 798,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1026,
                    "end": 1046,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 1185,
                    "end": 1206,
                    "matchedPaperCorpusId": "252595881"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6015625
        },
        {
            "corpus_id": "258615416",
            "title": "Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator",
            "text": "Classifier-free guidance [13] is a powerful sampling technique as it directs the model towards text guidance and away from null-text guidance by introducing a null-text guidance term. Compared to the previous study, classifier guidance [6], which utilizes a separate classifier to trade off Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) via truncation or low-temperature sampling, classifierfree guidance can be easily implemented and applied. Specifically, classifier-free guidance trains an unconditional denoising diffusion model together with the conditional model and updates the prediction of noise by increasing the distance between target noise and null-text noise. The utilization of classifier-free guidance has greatly enhanced the caliber of generated images and has become ubiquitous in subsequent studies [7, 8, 10, 11, 15-17, 20, 25, 26, 33, 39]. Based on classifier-free guidance, stable diffusion [23] has facilitated the training of diffusion models on restricted computational resources, whilst preserving their quality and adaptability by deploying them within the potent pretrained autoencoder's latent space. This has resulted in marking new milestones in the realm of image inpainting and class-conditional image synthesis whilst exhibiting exceptionally competitive performance across multiple tasks. Considering this, our study endeavors to further explore the potency of classifierfree guidance in conjunction with stable diffusion acting as the cornerstone of our investigation.",
            "score": 0.6150969249390097,
            "section_title": "RELATED WORKS 2.1 Classifier-free guidance",
            "char_start_offset": 4609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1517
                }
            ],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 926,
                    "end": 930,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91796875
        },
        {
            "corpus_id": "273404136",
            "title": "Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion",
            "text": "We first identify the difficult samples where the model struggles to extract helpful features for target classification. The difficulty estimation can be task-specific. For instance, in long-tail classification with scarce data, the difficulty of each sample depends on whether it belongs to tail classes. For tasks with low-quality data, we can utilize the loss or confidence on the ground-truth class to measure the difficulty. These samples are marked as \"hard samples\" within the training set (see Fig. 1), to highlight their role in the model's learning process. Synthetic Data Generation with Image Guidance Classifier-free guidance was initially introduced by Ho & Salimans (2022), to integrate conditional information into the image denoising process of diffusion models without the need for a classifier. It has been adopted by several Text-to-Image generation models such as Stable Diffusion (SD) (Rombach et al., 2022). Given the original image's latent representation z real , the denoising (backward diffusion) process can start from any step t with initial z t defined as \n\nImage Guidance : High \u2192 Low =0 =0.9",
            "score": 0.6128006532807329,
            "section_title": "Synthetic-to-Real Data Generation Hard Sample Identification",
            "char_start_offset": 8240,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1123
                }
            ],
            "ref_mentions": [
                {
                    "start": 907,
                    "end": 929,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8134765625
        },
        {
            "corpus_id": "277633776",
            "title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation",
            "text": "Classifier-free guidance [16] enables conditioning diffusion models on text without relying on an external classifier. \n\nDuring training, the model alternates between a specific condition y (e.g., text) and a null condition \u2205, learning both the conditional s \u03b8 (x t , t | y) and unconditional s \u03b8 (x t , t | \u2205) score functions. The final conditional score function at sampling is given by: \n\nwhere \u03b3 controls the influence of the text condition. Adjusting the guidance scale, \u03b3, allows the diffusion process to produce images that align closely with the text prompt.",
            "score": 0.6125524835617523,
            "section_title": "Classifier-Free Guidance",
            "char_start_offset": 6918,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 389
                },
                {
                    "start": 392,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 566
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "266174689",
            "title": "DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models",
            "text": "Classifier-free guidance (Ho and Salimans, 2022) serves to trade off mode coverage and sample fidelity in training conditional diffusion models. As an alternative method to classifier guidance (Dhariwal and Nichol, 2021) which incorporates gradients of image classifiers into the score estimate of a diffusion model, the classifier-free guidance mechanism jointly trains a conditional and an unconditional diffusion model, and combines the resulting conditional and unconditional score estimates. Specifically, an unconditional model p \u03b8 (z) parameterized through a score estimator \u03f5 \u03b8 (z \u03bb ) is trained together with the conditional diffusion model p \u03b8 (z|c) parameterized through \u03f5 \u03b8 (z \u03bb , c). The conditioning signal in the unconditional model is discarded by randomly setting the class identifier c to a null token \u2205 with some probability p uncond which is a major hyperparameter of this guidance mechanism. The sampling could then be formulated using the linear combination of the conditional and unconditional score estimates, where w controls the strength or weight of the guidance:",
            "score": 0.6116227689346856,
            "section_title": "Classifier-free guidance",
            "char_start_offset": 7707,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1090
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89208984375
        },
        {
            "corpus_id": "265506378",
            "title": "Motion-Conditioned Image Animation for Video Editing",
            "text": "Conditional latent diffusion models. Diffusion models learn to generate samples from a training distribution by reversing a gradual noising process. At the sampling time starting from a Gaussian noise, the model generates less noisy samples in T time-steps where each time-step, t, corresponds with a specific noise level [7]. In latent diffusion models for each input image x, this noising and denoising process is applied on the latent space, z = E(x), of a pretrained variational autoencoder with encoder E, resulting in more efficient training and sampling steps. In a text-conditional latent diffusion model, text features are extracted from a pre-trained language model, and then fed into the latent diffusion U-Net blocks via cross-attention modules. In addition to the text conditioning, the image or video generation models can be also conditioned on an additional input image by concatenating its features with the noisy latent features at each time-step, z t , and adding extra input channels to the first convolutional layer of the U-Net [3,12,43]. The network will be trained to predict the noise added to the noisy latent features given image and text conditioning inputs, respectively. \n\nClassifier free guidance. Classifier-free guidance was proposed in [14] and is widely used to improve the fidelity and diversity of the generated samples and their correspondence with the conditioning input in a diffusion model. During training, the diffusion model is trained jointly in a conditional and unconditional setting where the conditioning input is set to NULL with a specific frequency. At inference, the generated samples are guided to be more faithful to the conditioning input while being further away from the NULL input with a guidance scale s >= 1.",
            "score": 0.6108152504653699,
            "section_title": "Background",
            "char_start_offset": 9419,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1200
                },
                {
                    "start": 1203,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1769
                }
            ],
            "ref_mentions": [
                {
                    "start": 322,
                    "end": 325,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1050,
                    "end": 1053,
                    "matchedPaperCorpusId": "253581213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83447265625
        },
        {
            "corpus_id": "260091522",
            "title": "Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting",
            "text": "Classifier guidance repurposes unconditionally-trained image diffusion models for class-conditional image generation [10]. The key idea constitutes decomposing the class-conditional score function using the Bayes rule, \n\nand employing an auxiliary classifier to estimate \u2207 x t log p(c|x t ). Specifically, the following modified reverse diffusion process (Eq. 3) allows sampling from the class-conditional distribution, \n\nwhere s is a scale parameter controlling the strength of the guidance.",
            "score": 0.6101536863972703,
            "section_title": "Diffusion Guidance",
            "char_start_offset": 5829,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 218
                },
                {
                    "start": 221,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 419
                },
                {
                    "start": 422,
                    "end": 492
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 121,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.487060546875
        },
        {
            "corpus_id": "277066244",
            "title": "Z-Magic: Zero-shot Multiple Attributes Guided Image Creator",
            "text": "Recent advances in Denoising Diffusion Probabilistic Models (DDPMs) [18] have demonstrated exceptional capabili-In high-dimensional spaces, two random vectors are almost always nearly orthogonal [39]. ties in various image synthesis tasks, including image manipulation and conditional generation. Classifier-Guidance [13] introduces an additional classifier to guide the generation toward specific categories, while classifier-free guidance models [17] combine conditional and unconditional models to enhance generation quality. \n\nSeveral landmark architectures have further shaped this field. GLIDE [31] employs a pre-trained CLIP model [35] for text-guided image synthesis, and Stable Diffusion [38] improves computational efficiency by performing textconditioned denoising in latent space. ControlNet [48] incorporates a parallel U-Net architecture to enable diverse visual conditions, including landmarks, edge maps, and skeletal structures. Similarly, T2I-Adapter [30] proposes a lightweight adapter network to integrate various visual control signals. \n\nDespite their impressive performance, these approaches demand substantial computational resources for training. In contrast, our work explores a training-free paradigm for controlling diffusion models, offering greater flexibility and broader applicability.",
            "score": 0.6099534204713393,
            "section_title": "Conditional Diffusion Model",
            "char_start_offset": 5080,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1057
                },
                {
                    "start": 1060,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1317
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 72,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 317,
                    "end": 321,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 600,
                    "end": 604,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 638,
                    "end": 642,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 697,
                    "end": 701,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 804,
                    "end": 808,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 969,
                    "end": 973,
                    "matchedPaperCorpusId": "256900833"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7978515625
        },
        {
            "corpus_id": "272690217",
            "title": "Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation",
            "text": "Classifier guidance [6] modifies the diffusion score, specifically the gradient of the log probability density function, which is more tractable to learn than directly modeling the data distribution. This approach employs a classifier to guide the generation process by increasing the probability of data that the classifier assigns a high likelihood to the correct label. As demonstrated by [19], data that is well-classified tends to exhibit high perceptual quality, contributing to superior image generation outcomes. \n\nDespite its effectiveness in balancing precision and recall, classifier guidance depends on the gradients of an image classifier, limiting the variability of generated images. This dependency raises the question of whether it is possible to achieve comparable or superior guidance without relying on a classifier. \n\nClassifier-free guidance, introduced by [7], addresses this issue by eliminating the need for a dedicated classifier. Instead, it involves training an unconditional denoising diffusion model, parameterized by a score estimator, alongside a conditional denoising diffusion model, parameterized by a conditional score estimator. These are parameterized using a single neural network. For the unconditional model, a null token is used for the class identifier. The models are jointly trained, with the conditioning randomly set to a null token based on a hyperparameter probability p-uncond. This methodology not only simplifies the model architecture but also enhances guidance capabilities without the dependence on an external classifier.",
            "score": 0.6094638345305057,
            "section_title": "D. Classifier-Free Guidance",
            "char_start_offset": 13994,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 520
                },
                {
                    "start": 523,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1577
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 23,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 392,
                    "end": 396,
                    "matchedPaperCorpusId": "1687220"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9296875
        },
        {
            "corpus_id": "276421312",
            "title": "Diffusion Models without Classifier-free Guidance",
            "text": "Classifier-free guidance (CFG) (Ho & Salimans, 2021) is a widely adopted technique in conditional diffusion models to enhance generation performance and alignment to conditions. It provides an explicit control of the focus on conditioning variables and avoids to sample within the \"low temperature\" regions with low quality. \n\nThe key design of CFG is to combine the posterior probability and utilize Bayes' rule during inference time. To facilitate this, it is required to train both conditional and unconditional diffusion models. In particular, CFG trains the models to predict \n\nwhere is an additional empty class introduced in common practices. During training, the model switches between the two modes with a ratio \u03bb. \n\nFor inference, the model combines the conditional and unconditional scores and guides the denoising process as \n\nFigure 2: We use a grid 2D distribution with two classes, marked with orange and gray regions, as example and train diffusion models on it. We plot the generated samples, trajectories, and probability density function (PDF) of conditional, unconditional, CFG-guided model, and our approach. \n\n(a) The first row indicates that although CFG improves quality by eliminating outliers, the samples concentrate in the center of data distributions, resulting the loss of diversity. In contrast, our method yields less outliers than the conditional model and a better coverage of data than CFG. \n\n(b) In the second row, the trajectories of CFG show sharp turns at the beginning, e.g. samples inside the red box, while our method directly drives the samples to the closet data distributions. \n\n(c) The PDF plots of the last row also suggest that our method predicts more symmetric contours than CFG, balancing both quality and diversity. \n\nwhere w is the guidance scale that controls the focus on conditional scores and the trade-off between generation performance and sampling diversity. CFG has become an widely adopted protocol in most of diffusion models for tasks, such as image generation and video generation.",
            "score": 0.6089926556163329,
            "section_title": "Classifier-Free Guidance",
            "char_start_offset": 5406,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 324
                },
                {
                    "start": 327,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1767
                },
                {
                    "start": 1770,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 51,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86376953125
        },
        {
            "corpus_id": "273142392",
            "title": "Opportunities and challenges of diffusion models for generative AI",
            "text": "Diffusion models achieve state-of-the-art performance in image and audio generation [8 -11 ,25 ] and are one of the fundamental building blocks of image and audio synthesis systems. \n\nDiffusion models' performance is appraised of high-fidelity sample generation and allows versatile guidance to control the generation. The simplest example of generation under guidance is to generate images of certain categories, such as cats or dogs. Such categorical information is taken as a conditional signal and fed into conditional diffusion models. In more detail, we train conditional diffusion models using a labeled data set consisting of sample pairs (x i , y i ) , where y i is the label of an image x i . The training is to estimate a conditional score function using the data set, modeling the correspondence between x and y . In this way, conditional diffusion models are learning the conditional distribution P (x = image | y = given label ) and allow sampling from the distribution. \n\nIn text-to-image synthesis systems, the conditional information is an input text prompt, which can be a sentence consisting of objects or more abstract requirements, e.g. aesthetic quality. To generate images aligned with prompts, conditional diffusion models are trained with a massive annotated data set encompassing image and text summary pairs denoted as (x i , y i ) . The text y i wi l l be transformed into a word embedding and taken as input to a conditional diffusion model. Similar to the generation of images in certain categories, conditional diffusion models for text-to-image synthesis learn the conditional distribution P (x = image | y = text prompt ) and allow sampling from it. For instance, Nichol et al. [48 ] implemented the classifier-free guidance method (see a detailed description in the subsection entitled 'Learning the conditional score') for text-conditioned image generation, which outperforms some mature image synthesis systems such as DALL-E. In more sophisticated synthesis systems, some fine-tuning steps are implemented to further enable abstract prompt conditioning and improve the quality of generated images. For example, Yang et al. [49 ] utilized language models to guide the text-to-image generation of diffusion models under complex prompts with multiple objects, attributes and relationships.",
            "score": 0.6089186061068173,
            "section_title": "Vision and audio generation",
            "char_start_offset": 16467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 184,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2323
                }
            ],
            "ref_mentions": [
                {
                    "start": 1711,
                    "end": 1716,
                    "matchedPaperCorpusId": "245335086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8916015625
        },
        {
            "corpus_id": "257687400",
            "title": "Explore the Power of Synthetic Data on Few-shot Object Detection",
            "text": "Recently, diffusion models [16,37] have become a promising generative modeling framework, achieving stateof-the-art performance on image generation tasks [5,17]. DALL.E [32] proposes a two-stage model: a prior that generates a CLIP [31] image embedding given a text caption and a decoder that generates an image based on the image embedding. Imagen [36] is a text-to-image diffusion model based on the power of language models and diffusion models. Using cross-attention layers, Stable Diffusion [35] uses diffusion models to synthesize high-resolution images for general conditioning inputs. GLIDE [27] studies diffusion models for the text-conditional image synthesis by comparing two guidance strategies: CLIP and classifier-free guidance. The synthetic images generated from GLIDE are used for zero-shot, few-shot, and regular image recognition tasks [15], showing the power of synthetic data on image recognition. However, [15] does not consider the influence of the number of synthetic data defined by G in the (K+G)shot setting. By comparing Stable Diffusion to GLIDE in Table 9, we notice that the quality of synthetic data from Stable Diffusion is superior to that of GLIDE. Therefore, we use Stable Diffusion in our experiments.",
            "score": 0.6082776712474585,
            "section_title": "Text-to-Image Generators",
            "char_start_offset": 8648,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 27,
                    "end": 31,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 31,
                    "end": 34,
                    "matchedPaperCorpusId": "219708245"
                },
                {
                    "start": 154,
                    "end": 157,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 157,
                    "end": 160,
                    "matchedPaperCorpusId": "235619773"
                },
                {
                    "start": 232,
                    "end": 236,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 349,
                    "end": 353,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 496,
                    "end": 500,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.705078125
        },
        {
            "corpus_id": "265466381",
            "title": "HumanRef: Single Image to 3D Human Generation via Reference-Guided Diffusion",
            "text": "Diffusion models [19,47] are latent-variable generative models that have garnered significant attention due to their impressive generation results. It consists of a forward process that slowly removes structure from data by adding noise and a reverse process or generative model that slowly adds structure from noise. To improve the performance of diffusion models, denoising diffusion implicit models [48] propose to use non-Markovian diffusion processes to reduce the generation steps and [12] proposes classifier guidance to improve the sample quality using a classifier to trade off diversity for fidelity. While [18] introduces classifier-free guidance by mixing the score estimates of a conditional diffusion model and a jointly trained unconditional diffusion model. Benefited from the scalability of the diffusion models and large-scale aligned image-text datasets, text-to-image has made great progress such as Glide [33], DALL-E 2 [42], Imagen [44] and StableDiffusion [43]. These pretrained diffusion models have been used as a diffusion prior to promote the development of many other tasks like image editing and 3D generation.",
            "score": 0.6081915110672024,
            "section_title": "Diffusion Models",
            "char_start_offset": 7367,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1139
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 21,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 21,
                    "end": 24,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 954,
                    "end": 958,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 979,
                    "end": 983,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7021484375
        },
        {
            "corpus_id": "277628017",
            "title": "Electronic Structure Guided Inverse Design Using Generative Models",
            "text": "In contrast to classifier-based guidance, classifier-free guidance [34] integrates the conditional generation directly into the diffusion model by training a single neural network for both conditional and unconditional models. Instead of learning the unconditional score function \u2207 St log p t (S t ) independently, classifier-free guidance learns the unconditional and conditional distributions jointly: \n\nwhere p uncond is some probability for unconditional generation, set as a hyper-parameter. During sampling, the mixed score function becomes (1 + \u03c9)\u2207 xt log p t (x t , y) \u2212 \u03c9\u2207 xt log p t (x t , \u2205), where \u03c9 is the guidance strength. In practice, we project y into the same space as the node representations, and combine them through element-wise addition. In our experiments, we let p uncond = 0.2 and \u03c9 = 1.",
            "score": 0.6077220946681539,
            "section_title": "CLASSIFIER-FREE GUIDANCE",
            "char_start_offset": 7202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 813
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.763671875
        },
        {
            "corpus_id": "277313374",
            "title": "Unpaired Object-Level SAR-to-Optical Image Translation for Aircraft with Keypoints-Guided Diffusion Models",
            "text": "a) Diffusion Model: Diffusion models learn a data distribution p(x 0 ) by modeling a forward process that gradually adds noise to data and a reverse process that denoises the noisy data. The forward process is defined as: \n\nwhere \u03b2 t \u2208 (0, 1) is a variance schedule, and x t represents the noisy version of data x 0 at time step t. The forward process allows direct sampling of x t from x 0 using: \n\nwhere \u1fb1t = t s=1 (1 \u2212 \u03b2 s ). The reverse process is defined as: \n\nIn noise prediction parameterization, the model \u03f5 \u03b8 (x t , t) predicts the added noise \u03f5, and the training objective is: \n\nwhere \n\nb) Conditional Diffusion Model: To incorporate conditions c (e.g., images, class labels or keypoint annotations) into the generation process, conditional diffusion models modify the reverse process: \n\nUnder noise prediction parameterization, the loss function becomes: \n\nThis enables the model to leverage semantic or structural information (c) during training and generation. c) Classifier-Guided Diffusion Model: Classifier-guided diffusion improves the conditional diffusion process by using a pretrained classifier p \u03d5 (c|x t ) to compute the gradient of the log-probability of the condition: \n\nThis guidance modifies the reverse process: \n\nClassifier guidance enhances fidelity and consistency of generated images but introduces additional computational cost for evaluating the classifier during sampling. \n\nd) Classifier-Free Guidance Diffusion Model: Classifierfree guidance eliminates the need for a separate classifier by training the diffusion model to jointly learn conditional \u03f5 \u03b8 (x t , t, c) and unconditional \u03f5 \u03b8 (x t , t) denoising objectives. The guided reverse process is then expressed as: \n\nwhere w is the guidance scale that controls the trade-off between fidelity (conditional consistency) and diversity (unconditional generative quality). \n\nThe corresponding training objective remains: \n\nClassifier-free guidance provides an efficient alternative while maintaining high-fidelity conditional generation, making it suitable for object-level unpaired translation tasks with highgeneration possibility. By combining these components, the framework effectively bridges the domain gap between SAR and OPT images, generating high-quality OPT images with accurate structural and semantic alignment.",
            "score": 0.6074199859653853,
            "section_title": "B. Preliminaries",
            "char_start_offset": 19532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 221
                },
                {
                    "start": 224,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1407
                },
                {
                    "start": 1410,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1705
                },
                {
                    "start": 1708,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 1906
                },
                {
                    "start": 1909,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2311
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91259765625
        },
        {
            "corpus_id": "275906852",
            "title": "TFG-Flow: Training-free Guidance in Multimodal Generative Flow",
            "text": "Recent advancements in generative foundation models have demonstrated their increasing power across a wide range of domains (Reid et al., 2024;Achiam et al., 2023;Abramson et al., 2024). In particular, diffusion-based foundation models, such as Stable Diffusion (Esser et al., 2024) and SORA (Brooks et al., 2024) have achieved significant success, catalyzing a new wave of applications in areas such as art and science. As these models become more prevalent, a critical question arises: how can we steer these foundation models to achieve specific properties during inference time? One promising direction is using classifier-based guidance (Dhariwal & Nichol, 2021) or classifierfree guidance (Ho & Salimans, 2022), which typically necessitate training a specialized model for each conditioning signal (e.g., a noise-conditional classifier or a text-conditional denoiser). This resource-intensive and time-consuming process greatly limits their applicability. Recently, there has been growing interest in training-free guidance for diffusion models, which allows users to steer the generation process using an off-the-shelf differentiable target predictor without requiring additional model training (Ye et al., 2024). A target predictor can be any classifier, loss, or energy function used to score the quality of the generated samples. Training-free guidance offers a flexible and efficient means of customizing generation, holding the potential to transform the field of generative AI. Despite significant advances in generative models, most existing training-free guidance techniques are tailored to diffusion models that operate on continuous data, such as images. However, extending generative models to jointly address both discrete and continuous data-referred to as multimodal data (Campbell et al., 2024)-remains a critical challenge for broader applications in scientific fields (Wang et al., 2023). One key reason this expansion is essential is that many real-world problems involve multimodal data, such as molecular design, where both discrete elements (e.g., atom types) and continuous attributes (e.g., 3D coordinates) must be modeled together.",
            "score": 0.6070886745524399,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2162
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 185,
                    "matchedPaperCorpusId": "269633210"
                },
                {
                    "start": 262,
                    "end": 282,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 642,
                    "end": 667,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1892,
                    "end": 1911,
                    "matchedPaperCorpusId": "260384616"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7421875
        },
        {
            "corpus_id": "264172506",
            "title": "Elucidating The Design Space of Classifier-Guided Diffusion Generation",
            "text": "Diffusion probabilistic model (DPM) [Sohl-Dickstein et al., 2015, Ho et al., 2020, Song et al., 2020b] is a powerful generative model that employs a forward diffusion process to gradually add noise to data and generate new data from noise through a reversed process. DPM's exceptional sample quality and scalability have significantly contributed to the success of Artificial Intelligence Generated Content (AIGC) in various domains, including images [Saharia et al., 2022, Ramesh et al., 2022, 2021, Rombach et al., 2022], videos [Ho et al., 2022b, Singer et al., 2022, Ho et al., 2022a, Molad et al., 2023], and 3D objects [Poole et al., 2022, Lin et al., 2023, Wang et al., 2023]. \n\nConditional generation is one of the core tasks of AIGC. With the diffusion formulation, condition injection, especially the classical class condition, becomes more transparent as it can be modeled as an extra term during the reverse process. To align with the diffusion process, Dhariwal and Nichol [2021] proposed classifier guidance (CG) to train a time/noise-dependent classifier and demonstrated significant quality improvement over the unguided baseline. Ho and Salimans [2022] later proposed classifier-free guidance (CFG) to implicitly implement the classifier gradient with the score function difference and achieved superior performance in the classical class-conditional image generation. However, both CG and CFG require extra training with labeled data, which is not only time-consuming but also practically cumbersome, especially when adapting to new conditions. To reduce computational costs, training-free guidance methods have been proposed [Bansal et al., 2023] that take advantage of pretrained discriminative models. \n\nDespite the improved flexibility, training-free guidance has not demonstrated convincing performance compared to CG & CFG in formal quantitative evaluation of guiding diffusion generation. There seems to be an irreconcilable trade-off between performance and flexibility and the current guidance schemes are still to be desired.",
            "score": 0.6068026750426453,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2053
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 81,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 499,
                    "end": 522,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 548,
                    "end": 569,
                    "matchedPaperCorpusId": "252595919"
                },
                {
                    "start": 644,
                    "end": 662,
                    "matchedPaperCorpusId": "253708074"
                },
                {
                    "start": 966,
                    "end": 992,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1644,
                    "end": 1665,
                    "matchedPaperCorpusId": "256846836"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75439453125
        },
        {
            "corpus_id": "253734769",
            "title": "EDGE: Editable Dance Generation From Music",
            "text": "Diffusion models [19,54] are a class of deep generative models which learn a data distribution by reversing a scheduled noising process. In the past few years, diffusion models have been shown to be a promising avenue for generative modeling, exceeding the state-of-the-art in generative tasks [18,24,50]. Much like previous generative approaches like VAE [28] and GAN [12], diffusion models are also capable of conditional generation. Dhariwal et al. [6] introduced classifier guidance for image generation, where the output of a diffusion model may be \"steered\" towards a target, such as a class label, using the gradients of a differentiable auxiliary model. Saharia et al. [49] proposed to use direct concatenation of conditions for Pix2Pix-like tasks, akin to Conditional GAN and CVAE [38,55], and Ho et al. [20] demonstrated that classifier-free guidance can achieve state-of-the-art results while allowing more explicit control over the diversity-fidelity tradeoff.\n\nMost recently, diffusion-based methods have demonstrated strong performance in generating motions conditioned on text [26,58,63]. While the tasks of text-to-motion and music-conditioned dance generation share high-level similarities, the dance generation task suffers more challenging computational scaling (see Sec. 3) and, due to its specialized nature, much lower data availability.",
            "score": 0.6055808948103554,
            "section_title": "Generative Diffusion Models",
            "char_start_offset": 5198,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 21,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 21,
                    "end": 24,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 301,
                    "end": 304,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 452,
                    "end": 455,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 794,
                    "end": 797,
                    "matchedPaperCorpusId": "13936837"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.681640625
        },
        {
            "corpus_id": "272832564",
            "title": "TFG: Unified Training-Free Guidance for Diffusion Models",
            "text": "Recent advancements in generative models, particularly diffusion models [61,21,62,66], have demonstrated remarkable effectiveness across vision [65,48,52], small molecules [74,73,24], proteins [1,72], audio [35,29], 3D objects [40,41], and many more. Diffusion models estimate the gradient of log density (i.e., Stein score, [67]) of the data distribution [65] via denoising learning objectives, and can generate new samples via an iterative denoising process. With impressive scalability to billions of data [58], future diffusion models have the potential to serve as foundational generative models across a wide range of applications. Consequently, the problem of conditional generation based on these models, i.e., tailoring outputs to satisfy user-defined criteria such as labels, attributes, energies, and spatial-temporal information, is becoming increasingly important [63,2]. \n\nConditional generation methods like classifier-based guidance [66,7] and classifier-free guidance [23] typically require training a specialized model for each conditioning signal (e.g., a noise-conditional classifier or a text-conditional denoiser). This resource-intensive and time-consuming process greatly limits their applicability. In contrast, training-free guidance aims to generate samples that align with certain targets specified through an off-the-shelf differentiable target predictor without involving any additional training. Here, a target predictor can be any classifier, loss function, probability function, or energy function used to score the quality of the generated samples. \n\nIn classifier-based guidance [66,7], where a noise-conditional classifier is specifically trained to predict the target property on both clean and noisy samples, incorporating guidance in the diffusion Figure 1: (a) Illustration of the unified search space of our proposed TFG, where the height (color) stands for performance. Existing algorithms search along sub-manifolds, while TFG results in improved guidance thanks to its extended search space. (b) The label accuracy (higher the better) and Fr\u00e9chet inception distance (FID, lower the better) of different methods for the label guidance task on CIFAR10 [30], averaged across ten labels.",
            "score": 0.6044279865959691,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 884
                },
                {
                    "start": 887,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1582
                },
                {
                    "start": 1585,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2227
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 76,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 148,
                    "end": 151,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 151,
                    "end": 154,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 176,
                    "end": 179,
                    "matchedPaperCorpusId": "258436871"
                },
                {
                    "start": 179,
                    "end": 182,
                    "matchedPaperCorpusId": "247839510"
                },
                {
                    "start": 193,
                    "end": 196,
                    "matchedPaperCorpusId": "269633210"
                },
                {
                    "start": 196,
                    "end": 199,
                    "matchedPaperCorpusId": "271161349"
                },
                {
                    "start": 227,
                    "end": 231,
                    "matchedPaperCorpusId": "232092778"
                },
                {
                    "start": 325,
                    "end": 329,
                    "matchedPaperCorpusId": "53492374"
                },
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 509,
                    "end": 513,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 877,
                    "end": 881,
                    "matchedPaperCorpusId": "260957043"
                },
                {
                    "start": 881,
                    "end": 883,
                    "matchedPaperCorpusId": "256846836"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7978515625
        },
        {
            "corpus_id": "268041325",
            "title": "FineDiffusion: Scaling up Diffusion Models for Fine-grained Image Generation with 10, 000 Classes",
            "text": "Classifier-free guidance has supplanted classifier guided sampling to achieve a trade-off between the diversity and quality of sampling. While reducing training complexity, this approach secures high-quality image generation. Currently, classifier-free guidance is widely utilized in various diffusion models. We attempt to introduce hierarchical category label information because knowing which superclass each category belongs to is helpful for capturing intricate details and enhancing the overall quality of the generated images. Therefore, we introduce a fine-grained classifier-free guidance sampling method, yielding improved results under specific conditions. Integrated with TieredEmbedder, this approach leverages superclass conditional information to enhance control over generated images. Conventional classifierfree guidance involves training both conditional and unconditional generative models. In our approach, a superclass conditional generative model is introduced to replace the unconditional model. During training, we use the same neural network to parameterize both the conditional model and the superclass conditional model, leveraging a score estimator \u03f5 \u03b8 for optimization guidance. Subclass labels c are probabilistically replaced with their corresponding superclass labels c p to achieve the learning of superclass embeddings. During sampling, trained superclass and subclass embeddings are input into the DiT network to obtain predictive noise. Our method combines conditional scores estimation \u03f5 \u03b8 (x t | c) and superclass conditional scores estimation \u03f5 \u03b8 (x t | c p ) to guide specific category image generation, with the specific formula provided below: \n\nwhere \u03c9 is the guidance scale, and \u03b5\u03b8 is the modified score. \n\nBy thoroughly exploiting the distributional characteristics of superclasses and subclasses, our sampling method achieves a finer control than classifier-free guidance. We can capture commonalities among subclasses within the same superclass and simultaneously capture distinct distributions between different superclasses, thereby making the learned distributions of each category more closely approximate real-world situations.",
            "score": 0.603919316282848,
            "section_title": "Fine-grained Classifier-free Guidance",
            "char_start_offset": 13406,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1685
                },
                {
                    "start": 1688,
                    "end": 1748
                },
                {
                    "start": 1751,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2179
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.958984375
        },
        {
            "corpus_id": "272987840",
            "title": "Unified Gradient-Based Machine Unlearning with Remain Geometry Enhancement",
            "text": "Conditional Diffusion Models and Classifier-free guidance. Diffusion models [40] have gained prominence in the field of generative modeling, particularly for their effectiveness in generating high-quality images. A sample x T is sampled from a Gaussian distribution and gradually denoised for T time steps, finally recovering a clean sample x 0 . Conditional Diffusion Model [40] is a variant that conditions the generation process on additional information c such as class labels, text descriptions, or other modalities. In practice, the conditional diffusion model is trained to predict the noise \u03f5 \u03b8 (x t |c) to form the denoising process p \u03b8 (x t\u22121 |x t , c), where x t is a noisy version of the input x. The corresponding objective of the conditional diffusion model is typically formulated as: \n\nwith t uniformly sampled from {1, . . . , T }. In this setting, classifier-free guidance [41] is proposed to encourage the sampling procedure to find x with high log p(c|x). Then diffusion process is given by \u03b5\u03b8 (x t |c) = (1 \u2212 w)\u03f5 \u03b8 (x t |\u2205) + w\u03f5 \u03b8 (x t |c), where \u03b5\u03b8 (x t |c) represents the noise estimation obtained by the conditional diffusion model given c, w \u2208 [0, 1] is the guidance weight, \u2205 denotes the 'null' condition. The generation process initiates with Gaussian noise xT \u223c N (0, 1) and repeats denoising the data by \u03b5\u03b8 (x t |c) to obtain xt\u22121 until t = 0, producing the authentic data conditioned on c. \n\nLatent Diffusion Models. Direct training of conditional diffusion models in high-resolution pixel space is often computationally prohibitive. Latent diffusion models (LDMs) [42,58] address this challenge with an image compression approach. Specifically, an autoencoder is trained using perceptual loss and a patch-based adversarial objective to master the process of perceptual image compression.",
            "score": 0.6025247896236489,
            "section_title": "B.3 Preliminary on Diffusion Model",
            "char_start_offset": 35596,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 59,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1419
                },
                {
                    "start": 1422,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1818
                }
            ],
            "ref_mentions": [
                {
                    "start": 76,
                    "end": 80,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 375,
                    "end": 379,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 891,
                    "end": 895,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1595,
                    "end": 1599,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1599,
                    "end": 1602,
                    "matchedPaperCorpusId": "254854389"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.912109375
        },
        {
            "corpus_id": "267770589",
            "title": "Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models",
            "text": "Text-to-image diffusion model: \n\nRecent years have witnessed unprecedented progress in text-to-image synthesis, driven by large generative models such as diffusion models [6,17,50,51] (e.g., GLIDE [38], DALL\u2022E 2 [44], Imagen [47], and Stable Diffusion [45]) and VQ Transformers [9] (e.g., DALL\u2022E [43], CogView [7,8], and Parti [57]). This paper studies how to extract disentangled image factors from text-to-image diffusion models. \n\nGuidance for diffusion models. Guidance methods modify the output distribution of pre-trained diffusion models, based on additional inputs such as class labels [6], text [38], and corrupted images [24,32]. The first guidance method is classifier guidance [6], for which a class classifier is finetuned on noisy images. Similarly, CLIP guidance [30,38] finetunes a CLIP model [42] to support text input. To avoid finetuning classifiers or CLIP, classifier-free guidance (CFG) [16] jointly trains a conditional and an unconditional diffusion model and combines their score estimates, and CFG has become the default for text-to-image tasks [38,45]. To compose multiple texts, composable diffusion [29] combines score estimates with different text inputs. Besides user-specified conditions, several works showed that even guidance based on model outputs [3] or representations [18] can improve the quality of images. In this paper, we explore how to disentangle image factors with texts to gain fine-grained control. \n\nImage editing with diffusion models. Recent works have shown that diffusion models are capable of unpaired imageto-image translation [4,35,52,55]. A more recent trend of works have explored zero-shot image editing with text-to-image diffusion models [14,25,55]. One of the applications of our Contrastive Guidance is to improve the intended edit of some of these zero-shot image editors. \n\nGuidance for other generative models. Guidance has also been widely studied for GANs [12] and autoregressive language models (LMs).",
            "score": 0.6021384675171845,
            "section_title": "Related Work",
            "char_start_offset": 3897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 33,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1446
                },
                {
                    "start": 1449,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 174,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 174,
                    "end": 177,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 177,
                    "end": 180,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 180,
                    "end": 182,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "248097655"
                },
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 252,
                    "end": 256,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 278,
                    "end": 281,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 296,
                    "end": 300,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 310,
                    "end": 313,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 313,
                    "end": 315,
                    "matchedPaperCorpusId": "248476190"
                },
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "249926846"
                },
                {
                    "start": 594,
                    "end": 597,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 604,
                    "end": 608,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "246411364"
                },
                {
                    "start": 635,
                    "end": 638,
                    "matchedPaperCorpusId": "246240274"
                },
                {
                    "start": 689,
                    "end": 692,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "245117331"
                },
                {
                    "start": 782,
                    "end": 785,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 809,
                    "end": 813,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 909,
                    "end": 913,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1071,
                    "end": 1075,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 1075,
                    "end": 1078,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1128,
                    "end": 1132,
                    "matchedPaperCorpusId": "249375227"
                },
                {
                    "start": 1284,
                    "end": 1287,
                    "matchedPaperCorpusId": "251402961"
                },
                {
                    "start": 1307,
                    "end": 1311,
                    "matchedPaperCorpusId": "252683688"
                },
                {
                    "start": 1582,
                    "end": 1585,
                    "matchedPaperCorpusId": "236950721"
                },
                {
                    "start": 1585,
                    "end": 1588,
                    "matchedPaperCorpusId": "245704504"
                },
                {
                    "start": 1588,
                    "end": 1591,
                    "matchedPaperCorpusId": "247476275"
                },
                {
                    "start": 1591,
                    "end": 1594,
                    "matchedPaperCorpusId": "252815928"
                },
                {
                    "start": 1699,
                    "end": 1703,
                    "matchedPaperCorpusId": "251252882"
                },
                {
                    "start": 1703,
                    "end": 1706,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 1706,
                    "end": 1709,
                    "matchedPaperCorpusId": "252815928"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95654296875
        },
        {
            "corpus_id": "258999759",
            "title": "StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners",
            "text": "We synthesize images by prompting Stable Diffusion with text from large scale image-text datasets, such as CC12M [9] and RedCaps [16]. Surprisingly, our investigation reveals that when the classifier-free guidance scale is properly configured for Stable Diffusion, it is able to synthesize images on which training self-supervised methods can perform at par with or better than training on real images of the same sample size. Inspired by the idea of contrastive self-supervised learning, which promotes intra-image invariance, we develop a representation learning approach that promotes intra-caption invariance. We achieve this by treating the multiple images generated from the same text prompt as positives for each other and use them in a multi-positive contrastive loss (see Figure 1). Despite training with solely synthetic images, this approach, called StableRep, even outperforms state-of-the-art methods such as CLIP [58] using the same text set, but with corresponding real images, on various representation evaluation benchmarks. \n\nIntuitively, one reason that synthetic data can be better than real data is because we are able to achieve a greater degree of control in the sampling, such as via the guidance scale in Stable Diffusion, or via text prompts and latent noise variables. Furthermore, generative models have the potential to generalize beyond their training data and therefore provide a richer (synthetic) training set than the corresponding real data alone. Our key contributions are:  via classifier-free guidance [32], which linearly combines conditional score estimate \u03f5(t, z \u03bb ) and unconditional estimate \u03f5(z \u03bb ) with the guidance scale w at each step \u03bb: \n\nThe Stable Diffusion model G sd relies on text sources to generate images. Instead of collecting a corpus of captions from scratch, we use the text part of existing uncurated image-text pair datasets, such as CC3M [71] and CC12M [9]. Formally, given an image caption dataset {t i } N i=1 , we generate one image per caption, forming a synthetic image dataset of the same size.",
            "score": 0.6019885117014028,
            "section_title": "Introduction",
            "char_start_offset": 1983,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1684
                },
                {
                    "start": 1687,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2063
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 116,
                    "matchedPaperCorpusId": "231951742"
                },
                {
                    "start": 927,
                    "end": 931,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1901,
                    "end": 1905,
                    "matchedPaperCorpusId": "51876975"
                },
                {
                    "start": 1916,
                    "end": 1919,
                    "matchedPaperCorpusId": "231951742"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88623046875
        },
        {
            "corpus_id": "270370952",
            "title": "Cometh: A continuous-time discrete-state graph diffusion model",
            "text": "In the conditional generation setting, one wants to generate samples satisfying a specific property y, to which we refer as the conditioner. For example, in text-to-image diffusion models, the conditioner consists of a textual description specifying the image the model is intended to generate. The most straightforward way to perform conditional generation for diffusion models is to inject the conditioner into the network-therefore modeling p \u03b8 (z (t\u22121) | z (t) , y)-hoping that the model will take it into account. However, the network might ignore y, and several efficient approaches to conditional generation for diffusion models were consequently developed. \n\nThe approach leveraged by Vignac et al. (2022) to perform conditional generation is classifier-guidance. It relies on a trained unconditional diffusion model and a regressor, or classifier, depending on the conditioner, trained to predict the conditioner given noisy inputs. As mentioned in Ho and Salimans (2021), it has the disadvantage of complicating the training pipeline, as a pre-trained classifier cannot be used during inference. \n\nTo avoid training a classifier to guide the sampling process, classifier-free guidance has been proposed in Ho and Salimans (2021) and then adapted for discrete data in Tang et al. (2022). A classifier-free conditional diffusion model jointly trains a conditional and unconditional model through conditional dropout. That is, the conditioner is randomly dropped with probability p uncond during training, in which the conditioner is set to a null vector. However, Tang et al. (2022) showed that learning the null conditioner jointly with the model's parameters is more efficient. \n\nAt the sampling stage, the next state is sampled through \n\nwhere s is the guidance strength. We refer to Tang et al. (2022) for deriving the above expression for the sampling process. \n\nLet us now explain how we apply classifier-free guidance in our setting. Denoting Rt,\u03b8 (G, G | y), the conditional reverse rate can be written as \n\nand similarly for edges.",
            "score": 0.6013599874086615,
            "section_title": "A.5 CLASSIFIER-FREE GUIDANCE",
            "char_start_offset": 43768,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1105
                },
                {
                    "start": 1108,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1746
                },
                {
                    "start": 1749,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1873
                },
                {
                    "start": 1876,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2021
                },
                {
                    "start": 2024,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 693,
                    "end": 713,
                    "matchedPaperCorpusId": "252595881"
                },
                {
                    "start": 958,
                    "end": 980,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1216,
                    "end": 1238,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90087890625
        },
        {
            "corpus_id": "267027841",
            "title": "Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis",
            "text": "\u2022 We introduce soft guidance, a technique that allows for the localization of global conditions without requiring explicit structural cues, thereby providing a unique mechanism for imposing global semantics onto specific image regions. \u2022 By combining these two propositions, we present COMPOSE AND CONQUER (CnC), a framework that augments text-conditional diffusion models with enhanced control over three-dimensional object placement and injection of global semantics onto localized regions. \n\nConditional Diffusion Models. Diffusion models (DMs) (Sohl-Dickstein et al., 2015;Ho et al., 2020) are generative latent variable models that are trained to reverse a forward process that gradually transforms a target data distribution into a known prior. Proving highly effective in its ability to generate samples in an unconditional manner, many following works (Dhariwal & Nichol, 2021;Ho et al., 2022;Nichol et al., 2021;Rombach et al., 2022;Ramesh et al., 2022) formulate the diffusion process to take in a specific condition to generate corresponding images. Out of said models, Rombach et al. (2022) proposes LDM, a latent text-conditional DM that utilizes an autoencoder, effectively reducing the computational complexity of generation while achieving high-fidelity results. \n\nLDMs, more commonly known as Stable Diffusion, is one of the most potent diffusion models open to the research community. LDMs utilize a twofold approach, where an encoder maps x to its latent representation z, and proceeds denoising z in a much lower, memory-friendly dimension. Once fully denoised, a decoder maps z to the original image dimension, effectively generating a sample. \n\nBeyond Text Conditions. While text-conditional DMs enable creatives to use free-form prompts, text as the sole condition has limitations. Namely, text-conditional DMs struggles with localizing objects and certain semantic concepts with text alone, because text prompts of large web-scale datasets used to train said models (Schuhmann et al., 2021) do not provide explicit localized descriptions and/or semantic information. Addressing this limitation, many works have introduced methods to incorporate additional conditional signals to the models while preserving its powerful prior, e.g. freezing the model while training an additional module.",
            "score": 0.6005219939848698,
            "section_title": "INTRODUCTION",
            "char_start_offset": 5500,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 492
                },
                {
                    "start": 495,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1278
                },
                {
                    "start": 1281,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2255
                },
                {
                    "start": 2256,
                    "end": 2311
                }
            ],
            "ref_mentions": [
                {
                    "start": 548,
                    "end": 577,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 885,
                    "end": 901,
                    "matchedPaperCorpusId": "235619773"
                },
                {
                    "start": 921,
                    "end": 942,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1081,
                    "end": 1102,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.330810546875
        },
        {
            "corpus_id": "265351587",
            "title": "Guided Flows for Generative Modeling and Decision Making",
            "text": "Classifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields. We explore the usage of \\emph{Guided Flows} for a variety of downstream applications. We show that Guided Flows significantly improves the sample quality in conditional image generation and zero-shot text-to-speech synthesis, boasting state-of-the-art performance. Notably, we are the first to apply flow models for plan generation in the offline reinforcement learning setting, showcasing a 10x speedup in computation compared to diffusion models while maintaining comparable performance.",
            "score": 0.600349247054058,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96533203125
        },
        {
            "corpus_id": "269033464",
            "title": "Tuning-Free Adaptive Style Incorporation for Structure-Consistent Text-Driven Style Transfer",
            "text": "The Diffusion Model [40] was initially employed to mitigate Gaussian noise applied continuously to training images and can be conceived as a series of denoising autoencoders.The advent of Denoising Diffusion Probabilistic Models (DDPM) [21] propelled the utilization of diffusion models for image generation into the mainstream.Subsequent advancements, exemplified by Denoising Diffusion Implicit Models (DDIM) [41], substantially accelerated sampling speeds while effecting directed generation from random noise to samples, facilitating subsequent research in image editing.Dhariwal and Nichol first introduced Classifier Guidance Diffusion [11], substantiating the superiority of diffusion models over GANs in image generation tasks.Classifier-Free Diffusion Guidance [22] then achieved a balance between sample quality and diversity during image generation.Building upon prior work, GLIDE [34] initiated using text as a conditional guide for image generation in a classifier-free manner.DALLE-2 [35],\n\nbuilding on the foundation laid by GLIDE, incorporated CLIP as a guiding factor, bringing diffusion models into the public eye.Subsequently, the field of image generation based on diffusion models experienced a surge, with projects such as Imagen [37] and Stable Diffusion [36] emerging, prompting the gradual application of diffusion models in a broader range of tasks, including object detection [7], style transfer [46], image editing [5,8,29] and semantic segmentation [51].With the release of ControlNet [52], the potential of diffusion models in various other domains is being further explored [10].",
            "score": 0.6002739267558035,
            "section_title": "Diffusion Models",
            "char_start_offset": 4341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 174,
                    "end": 328
                },
                {
                    "start": 328,
                    "end": 575
                },
                {
                    "start": 575,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 860
                },
                {
                    "start": 860,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1003
                },
                {
                    "start": 1005,
                    "end": 1132
                },
                {
                    "start": 1132,
                    "end": 1483
                },
                {
                    "start": 1483,
                    "end": 1610
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 24,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 236,
                    "end": 240,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 411,
                    "end": 415,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 642,
                    "end": 646,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 770,
                    "end": 774,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 892,
                    "end": 896,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 1252,
                    "end": 1256,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1278,
                    "end": 1282,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1403,
                    "end": 1406,
                    "matchedPaperCorpusId": "253581633"
                },
                {
                    "start": 1423,
                    "end": 1427,
                    "matchedPaperCorpusId": "260900064"
                },
                {
                    "start": 1443,
                    "end": 1446,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 1446,
                    "end": 1448,
                    "matchedPaperCorpusId": "260203189"
                },
                {
                    "start": 1448,
                    "end": 1451,
                    "matchedPaperCorpusId": "258865473"
                },
                {
                    "start": 1478,
                    "end": 1482,
                    "matchedPaperCorpusId": "257557537"
                },
                {
                    "start": 1514,
                    "end": 1518,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 1605,
                    "end": 1609,
                    "matchedPaperCorpusId": "252199918"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8203125
        },
        {
            "corpus_id": "256846465",
            "title": "Text-Guided Scene Sketch-to-Photo Synthesis",
            "text": "Denoising diffusion probabilistic models (DDPM) have shown state-of-the-art results in image generation [19]. Despite the great performance of diffusion models in generative modeling, diffusion models in pixel space consume an extensive amount of computational power. Therefore, latent diffusion models (LDM) were proposed that worked in the latent space of a pre-trained variational autoencoder [7]. The success of LDM can be seen in the current trending text-toimage model, known as Stable Diffusion. During sampling, classifier-free guidance [20] is often used to trade off between sample quality and diversity. The output of the model with classifier-free guidance is \n\nwhere \u03b8 is a learned network that predicts the noise, z t is a noisy latent, c is a given condition, s > 1 is a guidance scale and \u2205 is a null condition. In this paper, we also deploy classifier-free guidance for sketch-to-photo synthesis.",
            "score": 0.5996886409560653,
            "section_title": "Latent Diffusion Models",
            "char_start_offset": 4403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 671
                },
                {
                    "start": 674,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 913
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 108,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 396,
                    "end": 399,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.916015625
        },
        {
            "corpus_id": "258352755",
            "title": "Edit Everything: A Text-Guided Generative System for Images Editing",
            "text": "\"While drawing I discover what I really want to say.\" -Dario Fo Visualization, including images, paintings, shots, illustrations, and photographs, can usually be described with text. However, creating these images often requires specialized skills and a significant time investment [18]. Consequently, a generative system has the potential to produce realistic images based on natural language, allowing humans to efficiently generate a wide range of visual content. Additionally, this system offers an unprecedented opportunity for continuous improvement and precise control over image editing, making it a crucial tool for real-world applications. \n\nRecently, diffusion models have shown promising performances in generating high-quality realistic images [17,14,15,12]. In particular, a text-guided method significantly improves the diversity and fidelity [10]. To address photorealism in the conditional setting, [2] proposes a classifier to guide diffusion models, allowing them to generate realistic images toward a classifier's label. [5] shows that guidance can be indeed performed by a pure generative model without a classifier, named classifierfree guidance. Classifier-free guidance combines conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that with classifier guidance. Inspired by the ability of guided diffusion models with text, [10] first implements CLIP to guide diffusion models towards text prompt [11]. However, compared to the quality of generated images, Finally, guided by the target prompt, Stable Diffusion (SD) generates the replacement object for the mask segment. This process is seamless and efficient, resulting in high-quality image editing. \n\nCLIP guidance cannot compete with classifier-free guidance. Another promising solution is that [20] designs a neural network structure (ControlNet) to control diffusion models and support conditional inputs. Diffusion models can be augmented by ControlNets to generate edge maps, segmentation maps, key points, etc. \n\nInspired by the remarkable performance of ControlNet and CLIP guidance in significantly enhancing the image quality, we leverage Segment Anything model (SAM) and CLIP to guide diffusion models [11,8]. In our work, we create a text-guided generative system called Editing Everything, combining SAM, CLIP and Stable Diffusion (SD) [14].",
            "score": 0.5993302353474657,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 649
                },
                {
                    "start": 652,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1740
                },
                {
                    "start": 1743,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2058
                },
                {
                    "start": 2061,
                    "end": 2261
                },
                {
                    "start": 2262,
                    "end": 2395
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 286,
                    "matchedPaperCorpusId": "1246802"
                },
                {
                    "start": 757,
                    "end": 761,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 761,
                    "end": 764,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 764,
                    "end": 767,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 767,
                    "end": 770,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 916,
                    "end": 919,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1485,
                    "end": 1489,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 2254,
                    "end": 2258,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9248046875
        },
        {
            "corpus_id": "245335086",
            "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
            "text": "Images, such as illustrations, paintings, and photographs, can often be easily described using text, but can require specialized skills and hours of labor to create. Therefore, a tool capable of generating realistic images from natural language can empower humans to create rich and diverse visual content with unprecedented ease. The ability to edit images using natural language further allows for iterative refinement and fine-grained control, both of which are critical for real world applications. \n\nRecent text-conditional image models are capable of synthesizing images from free-form text prompts, and can compose unrelated objects in semantically plausible ways (Xu et al., 2017;Zhu et al., 2019;Tao et al., 2020;Ramesh et al., 2021;Zhang et al., 2021). However, they are not yet able to generate photorealistic images that capture all aspects of their corresponding text prompts. \n\nOn the other hand, unconditional image models can synthesize photorealistic images (Brock et al., 2018;Karras et al., 2019a;b;Razavi et al., 2019), sometimes with enough fidelity that humans can't distinguish them from real images (Zhou et al., 2019). Within this line of research, diffusion models (Sohl-Dickstein et al., 2015;Song & Ermon, 2020b) have emerged as a promising family of generative models, achieving state-of-the-art sample quality on a number of image generation benchmarks (Ho et al., 2020;Dhariwal & Nichol, 2021;Ho et al., 2021). \n\nTo achieve photorealism in the class-conditional setting, Dhariwal & Nichol (2021) augmented diffusion models with classifier guidance, a technique which allows diffusion models to condition on a classifier's labels. The classifier is first trained on noised images, and during the diffusion sampling process, gradients from the classifier are used to guide the sample towards the label. Ho & Salimans (2021) achieved similar results without a separately trained classifier through the use of classifier-free guidance, a form of guidance that interpolates between predictions from a diffusion model with and without labels.",
            "score": 0.5992570152422669,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 889
                },
                {
                    "start": 892,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2067
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90087890625
        },
        {
            "corpus_id": "258987376",
            "title": "MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL",
            "text": "Recently, there have been incredible advances in the field of conditional content generation with the strong generation capabilities of conditioned diffusion models. Conditional diffusion model pushes the state-of-the-art on text-to-image generation tasks such as DALL-E (Ramesh et al., 2022) and ImageGen (Saharia et al., 2022). The technique of conditioning can divide into two fashions: classifier-guided (Nichol & Dhariwal, 2021) and classifier-free (Ho & Salimans, 2022). \n\nThe former improves sample quality while reducing diversity in conditional diffusion models using gradients from a pre-trained classifier p \u03d5 (y|x k ) during sampling. The latter is an alternate technique that avoids this pre-trained classifier by instead jointly training a single diffusion model on conditional \u03f5 \u03b8 (x k , y, k) and unconditional \u03f5 \u03b8 (x k , k) noise model via randomly dropping conditional label y. \n\nIn fact, the aforementioned Diffuser (Janner et al., 2022b) can also be considered as a classifier-guided conditional diffusion model, where the pre-trained reward model is another form of classifier for evaluating the sample quality. \n\nOur designed MetaDiffuser builds upon the Diffuser and additionally incorporates classifier-free manner, by injecting the context as label y into the conditional noise model \u03f5 \u03b8 (x k , y, k), achieving more precise conditional generation. \n\nThe details about the relationship between two different conditional fashions can be found in Appendix A.",
            "score": 0.5983719600061675,
            "section_title": "Conditional Diffusion Model",
            "char_start_offset": 8301,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 476
                },
                {
                    "start": 479,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 1132
                },
                {
                    "start": 1135,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1481
                }
            ],
            "ref_mentions": [
                {
                    "start": 408,
                    "end": 433,
                    "matchedPaperCorpusId": "231979499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8583984375
        },
        {
            "corpus_id": "268253681",
            "title": "GUIDE: Guidance-based Incremental Learning with Diffusion Models",
            "text": "Besides conditioning, controlling diffusion model outputs can be achieved by modifying the process of sampling that incorporates additional signals from the guidance function. Classifier guidance introduced by Dhariwal and Nichol [8] employ a trained classifier model to steer the generation process via gradients of a specified loss function, typically assessing the alignment of generated images with specific classes. This concept is extended by Bansal et al. [2] to include any off-the-shelf model in guidance and further applied by Augustin et al. [1] for explaining the decisions of the classifier with generated counterfactual examples. Contrasting with these methods, Epstein et al. [10] introduce self-guidance based on the internal representations of the diffusion model, while Ho and Salimans [17] introduce classifier-free guidance, achieving results akin to classifier-based approaches by joint training of unconditional and conditional diffusion models.",
            "score": 0.5982819243262388,
            "section_title": "Guided image generation in diffusion models",
            "char_start_offset": 6060,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 967
                }
            ],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 233,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "256846836"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73583984375
        },
        {
            "corpus_id": "275787953",
            "title": "VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model",
            "text": "Classifier-free guidance (CFG) significantly enhances the capability of generative diffusion models to produce samples of exceptionally high fidelity. This approach integrates conditional generative models with the distribution estimation of unconditional models trained concurrently, thereby improving the overall quality of generation. Inspired by DALL-E 2 [68], VAR [84] and VAR-CLIP [100], we employ Gaussian noise features as conditional input to simulate unconditional generation. Subsequently, we derive the final distribution of image token outputs by subtracting the probability of uncon- ditional generation from the logits distribution of conditional generation. Specifically, assuming that the image features obtained through LLMs are denoted as H i , we derive the translated visual generation features through a mapping layer as H g = Projector(H i ). Subsequently, we employ the Classifier-Free Guidance (CFG) strategy to obtain the final features of the visual generation tokens, which can be expressed as: \n\nwhere H e represents randomly initialized Gaussian noise, serving as the unconditional feature, and \u03bb denotes the scale hyperparameter for the CFG strategy. \n\nThis formulation allows for the integration of both conditional and unconditional information in the generation process, potentially leading to more controlled and diverse visual outputs.",
            "score": 0.598168971336009,
            "section_title": "Classifier-free Guidance.",
            "char_start_offset": 35256,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1371
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9375
        },
        {
            "corpus_id": "259108885",
            "title": "Anomaly Detection in Satellite Videos using Diffusion Models",
            "text": "To improve the quality of generated images, a classifier is utilized to provide information to the diffusion model about the desired target distribution. This classifier, as described in [14], takes the form of f \u03d5 (y|x t , t) where x t represents the noisy image. By using gradients in the form of \u2207x log f \u03d5(y|x t ), the diffusion sampling process is guided towards the target image by modifying the noise prediction of the original model. To achieve this, we recall the following equation [34]: \n\nUsing this, we can write the score function for the joint distribution q(x t , y) as follows: \n\nFrom this, we can derive the new classifier-guided predictor in the form: \n\n) where \u03b5\u03b8 is the modified noise prediction. \n\nFree Guidance Diffusion Contrasting with guided diffusion, free guidance diffusion does not rely on an external classifier. Instead, the contents of the image itself guide the diffusion process, which is aided by a diffusion term in the generative model. The training process involves using both a conditional model p \u03b8 (x|y) and an unconditional denoising diffusion model p \u03b8 (x) [35]. An implicit classifier is used for training, where conditioning information is periodically discarded at random to allow the model to generate images unconditionally. The gradient for the implicit classifier can be derived from the conditional and unconditional score estimators using the following equation:",
            "score": 0.5980379566695674,
            "section_title": "Guided Diffusion",
            "char_start_offset": 12620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 669
                },
                {
                    "start": 672,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1414
                }
            ],
            "ref_mentions": [
                {
                    "start": 187,
                    "end": 191,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 492,
                    "end": 496,
                    "matchedPaperCorpusId": "196470871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73583984375
        },
        {
            "corpus_id": "259088588",
            "title": "LipVoicer: Generating Speech from Silent Videos Guided by Lip Reading",
            "text": "One key feature in many diffusion models is the use of guidance for conditional generation. Guidance, both with or without a classifier, enables us to \"guide\" our iterative inference process to generate outputs that are more faithful to our conditioning information, e.g., in text-to-image, it helps enforce that the generated images match the prompt text. \n\nAssume we wish to sample from q(x t |c), x t is our sample at the current iteration, c is some context, and p(c|x t ) is a pre-trained classifier. Our goal is to generate x t\u22121 . The idea of classifier guidance (Dhariwal & Nichol, 2021) is to use the classifier to guide the diffusion process to generate outputs that have the right context c. Specifically, if the diffusion model returns \u03f5 \u03b8 (x t , t), the classifier guidance alters the noise term that will be used for the update to \n\nwhere \u03c9 1 is a hyperparameter that controls the level of guidance. \n\nIn a later work (Ho & Salimans, 2021), a classifier-free guidance that removes the dependence on an existing classifier is proposed. In classifier-free guidance, we make two noise predictions, one with the conditioning context information, \u03f5 \u03b8 (x t , c, t), and one without it -\u03f5 \u03b8 (x t , t). We then use \u03b5 = \u03f5 \u03b8 (x t , c, t) + \u03c9 2 (\u03f5 \u03b8 (x t , c, t) \u2212 \u03f5 \u03b8 (x t , t)) where the hyperparameter \u03c9 2 controls the guidance strength. This allows us to enhance the update directions that correspond to the context c.",
            "score": 0.5977402271924305,
            "section_title": "GUIDANCE",
            "char_start_offset": 5177,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 356
                },
                {
                    "start": 359,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1425
                }
            ],
            "ref_mentions": [
                {
                    "start": 932,
                    "end": 953,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86083984375
        },
        {
            "corpus_id": "270562252",
            "title": "Adding Conditional Control to Diffusion Models with Reinforcement Learning",
            "text": "For instance, with the Stable Diffusion pre-trained model, where X is an image and C is a text prompt, additional attributes like Y 1 (compressibility) and Y 2 (color) depend only on the image, not the prompt. Thus, we can leverage the conditional independence of Y 1 and Y 2 from C given X to simplify the implementation of CTRL . The effectiveness of CTRL in this context is further validated experimentally in Section 6.2. \n\nCan classifier-free guidance leverage conditional independence? The applicability of conditional independence in classifier-free guidance, which directly models p \u03b3 (\u2022|c, y), is uncertain. For instance, when Y \u22a5 C|X as in Example 1, our method only requires (x, y) pairs, while classifier-free guidance typically needs (c, x, y) triplets. when Y 1 \u22a5 Y 2 |C, X as in Example 2, our approach utilizes triplets (c, x, y 1 ) and (c, x, y 2 ). However, as far as we are concerned, quadruples (c, x, y 1 , y 2 ) are necessary for classifier-free guidance, and acquiring such data at scale could pose a bottleneck.",
            "score": 0.5972435967797293,
            "section_title": "LEVERAGING CONDITIONAL INDEPENDENCE, COMPOSITIONALLY VIA CTRL",
            "char_start_offset": 20099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1035
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7744140625
        },
        {
            "corpus_id": "277043967",
            "title": "Understanding the Quality-Diversity Trade-off in Diffusion Language Models",
            "text": "Classifier-free guidance (CFG) [10], is a method for guiding the diffusion process without requiring a separate classifier. For a conditional diffusion model \u01770 = f \u03b8 (y t , t, x), classifier-free guidance perturbs the predicted value as follows2 : \n\nfor some guidance scale s \u2265 0. Larger values of s shift the prediction y 0 further from the unconditional prediction f \u03b8 (y t , t, \u2205) in the direction of the conditional prediction, causing the condition to have increased effect. \n\nCFG has seen wide usage in the domain of images, and is understood to trade off diversity for fidelity [17,21]. While classifiers have been used to guide the diffusion process in text generation [14], to the best of my knowledge classifierfree guidance has not yet been explored.",
            "score": 0.5944327940061834,
            "section_title": "Classifier-free Guidance",
            "char_start_offset": 5636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 762
                }
            ],
            "ref_mentions": [
                {
                    "start": 586,
                    "end": 590,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 590,
                    "end": 593,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9013671875
        },
        {
            "corpus_id": "254564168",
            "title": "Towards Practical Plug-and-Play Diffusion Models",
            "text": "Diffusion-based generative models have achieved remarkable success in image generation. Their guidance formulation allows an external model to plug-and-play control the generation process for various tasks without finetuning the diffusion model. However, the direct use of publicly available off-the-shelf models for guidance fails due to their poor performance on noisy inputs. For that, the existing practice is to fine-tune the guidance models with labeled data corrupted with noises. In this paper, we argue that this practice has limitations in two aspects: (1) performing on inputs with extremely various noises is too hard for a single guidance model; (2) collecting labeled datasets hinders scaling up for various tasks. To tackle the limitations, we propose a novel strategy that leverages multiple experts where each expert is specialized in a particular noise range and guides the reverse process of the diffusion at its corresponding timesteps. However, as it is infeasible to manage multiple networks and utilize labeled data, we present a practical guidance framework termed Practical Plug-And-Play (PPAP), which leverages parameter-efficient fine-tuning and data-free knowledge transfer. We exhaustively conduct ImageNet class conditional generation experiments to show that our method can successfully guide diffusion with small trainable parameters and no labeled data. Finally, we show that image classifiers, depth estimators, and semantic segmentation models can guide publicly available GLIDE through our framework in a plug-and-play manner. Our code is available at https://github.com/riiid/PPAP.",
            "score": 0.5942671876035194,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339599609375
        },
        {
            "corpus_id": "265608773",
            "title": "Readout Guidance: Learning Control from Diffusion Features",
            "text": "Conditional Diffusion Models. It has become increasingly popular to fine-tune text-to-image diffusion models to condition generation on signals beyond text, including camera pose [35], a reference identity [51], a reference image [6,32,38], a depth map [3], and more [23]. Due to the high cost of training diffusion models, many methods propose to keep the base model frozen and instead train an additional network that takes in the control signal and modulates the intermediate diffusion features accordingly. These types of models, including ControlNets [70], Adapters [40], and LoRAs [22], require less compute to train, fewer training samples, and can be combined in various ways because they build on the same frozen base model. Our method is orthogonal and complementary to this kind of adapter tuning, since our method aims to guide the sampling process based on the diffusion model's features, rather than modulate it, and can therefore be applied to any base model with any additional adapter. Our approach is more similar to classifier guidance, which is shown in Dhariwal & Nichol [13] to not only en-able conditional generation from unconditional models, but also reinforce the capabilities of a conditional model. In our experiments, we similarly demonstrate that our method can be applied to these adapter-based models [40,70] to further improve their control capabilities. Sampling-Time Guidance. Because diffusion models synthesize images through an iterative sampling process rather than a single forward pass, one can guide the sampling process in a particular direction without modifying the base model. Such guidance was first achieved with gradients from an ImageNet [12] classifier trained on noisy images from the diffusion forward process [13]. Follow-up works explored alternative guidance approaches including the difference of score estimates between a conditional and unconditional model [21], off-the-shelf models that operate on the predicted clean image [4,60], and hand-designed training-free functions that operate on diffusion features [10,15,33,43]. In our work, we focus on learning this guidance function from diffusion features.",
            "score": 0.5939042770783193,
            "section_title": "Related Work",
            "char_start_offset": 3038,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2166
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 183,
                    "matchedPaperCorpusId": "257631738"
                },
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 230,
                    "end": 233,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 556,
                    "end": 560,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 1092,
                    "end": 1096,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1337,
                    "end": 1340,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 1688,
                    "end": 1692,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 1763,
                    "end": 1767,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1916,
                    "end": 1920,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1985,
                    "end": 1988,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 1988,
                    "end": 1991,
                    "matchedPaperCorpusId": "257757144"
                },
                {
                    "start": 2074,
                    "end": 2077,
                    "matchedPaperCorpusId": "258999106"
                },
                {
                    "start": 2080,
                    "end": 2083,
                    "matchedPaperCorpusId": "254125549"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.830078125
        },
        {
            "corpus_id": "273655150",
            "title": "Shallow Diffuse: Robust and Invisible Watermarking through Low-Dimensional Subspaces in Diffusion Models",
            "text": "The diffusion model can be generalized from unconditional to T2I [4,5], where the latter enables controllable image generation x 0 guided by a text prompt c. In more detail, when training T2I diffusion models, we optimize a conditional denoising function \u03f5 \u03b8 (x t , t, c). For sampling, we employ a technique called classifier-free guidance (CFG) [44], which substitutes the unconditional denoiser \u03f5 \u03b8 (x t , t) in Equation (1) with its conditional counterpart \u03b5\u03b8 (x t , t, c) that can be described as \u03b5\u03b8",
            "score": 0.5935731003622892,
            "section_title": "Text-to-image (T2I) diffusion models & classifier-free guidance (CFG).",
            "char_start_offset": 7300,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 504
                }
            ],
            "ref_mentions": [
                {
                    "start": 65,
                    "end": 68,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 68,
                    "end": 70,
                    "matchedPaperCorpusId": "268247980"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6904296875
        },
        {
            "corpus_id": "258427128",
            "title": "Class-Balancing Diffusion Models",
            "text": "Diffusion models Diffusion models are recently proposed generative models [44] based on non-equilibrium thermodynamics. Conditional diffusion models [7] encode label information into the generation process and improve largely the generation performance. The guidance structure proposed in [7] makes it possible to control the generation process through an external module. Based on a similar intuition, researchers arrive to realize diverse functions, such as guided adversarial purification [35], few-shot generation [9] and so on [38,43]. The drawback of classifier guidance (noted as CG) [7] lies in its requirement of training another auxiliary classifier. To address the issue, classifier-free guidance (noted as CFG) [12] proposed a mechanism that uses the generator itself to express the class guidance information. CFG is proved to be a resource efficient method and achieves outstanding performance on large models [33]. Moreover, CFG only requires to add one line in training, which can be easily transplanted on different models.",
            "score": 0.5933328044888355,
            "section_title": "Related Works",
            "char_start_offset": 3899,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1040
                }
            ],
            "ref_mentions": [
                {
                    "start": 532,
                    "end": 536,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 536,
                    "end": 539,
                    "matchedPaperCorpusId": "247839278"
                },
                {
                    "start": 924,
                    "end": 928,
                    "matchedPaperCorpusId": "245335086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6748046875
        },
        {
            "corpus_id": "270562574",
            "title": "Extracting Training Data from Unconditional Diffusion Models",
            "text": "Diffusion Probabilistic Models DPMs [38] have achieved state-of-the-art performance in image and video generation, as exemplified by models such as Stable Diffusion [33], DALL-E 3 [4], Sora [5], Runway [33] and Imagen [35]. These models excel on various benchmarks [15]. DPMs can be interpreted from two perspectives: 1) score matching [42], where model learns the gradient of data distribution [43], and (2) denoising diffusion [21], where Gaussian noise is added to clean images over multiple time steps, and the model is trained to reverse this process. For conditional sampling, [15] introduced classifier guidance to steer the denoising process, while [22] proposed classifier-free guidance, enabling conditional generation without explicit classifiers.",
            "score": 0.5930936706462225,
            "section_title": "Related Work",
            "char_start_offset": 3344,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 758
                }
            ],
            "ref_mentions": [
                {
                    "start": 36,
                    "end": 40,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 218,
                    "end": 222,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 265,
                    "end": 269,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 583,
                    "end": 587,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6005859375
        },
        {
            "corpus_id": "272709333",
            "title": "InverseMeetInsert: Robust Real Image Editing via Geometric Accumulation Inversion in Guided Diffusion Models",
            "text": "Essentially, this process operates inversely, transitioning from z 0 to z T rather than the typical z T to z 0 pathway. Here, z 0 represents the encoded form of the provided real image I 0 . A straightforward inversion method for DDIM sampling was proposed in references [13,35], relying on the idea that the ODE process can be inverted using very small steps: \n\nClassifier-free Guidance. Classifier-free guidance [9] addresses the challenge of amplification of the effect induced by the conditioned text in text-guided generation. This technique involves making predictions both conditionally and unconditionally. These predictions are then combined. Formally, let \u2205 = T (\"\") represent the embedding of an empty text and let \u03c9 denote the guidance scale parameter. The classifier-free guidance prediction is then defined as \n\nFor example, in Stable Diffusion, the default value for \u03c9 is 7.5.",
            "score": 0.5929927328082465,
            "section_title": "Background and Preliminaries",
            "char_start_offset": 12097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 360
                },
                {
                    "start": 363,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 891
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 275,
                    "matchedPaperCorpusId": "249240415"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6015625
        },
        {
            "corpus_id": "273098412",
            "title": "Plug-and-Play Controllable Generation for Discrete Masked Models",
            "text": "Conditional generation based on guidance. As an important task in controllable generation, conditional generation aims to generate a random variable X \u223c p(x) (e.g., image) given another random variable Y known as the condition (e.g., the text description of an image). A popular approach is guidance: for example, in continuous diffusion model, the classifier guidance (Dhariwal & Nichol, 2021) learns a classifier p(y|x) that predicts the condition Y given a noisy sample of X and leverages its information to generate X conditional on Y = y. Classifier-free guidance (Ho & Salimans, 2022) trains a score model that approximates both the conditional and unconditional score functions, Preprint using a combination of them for conditional generation. These approaches can be extended to the discrete diffusion model (see Nisonoff et al. (2024)). \n\nControllable generation for discrete generative models. The study of controllable generation is an emerging area in language modeling (see, e.g., Zhang et al. (2023) for a review). A notable work, Dathathri et al. (2020), proposed applying gradient updates to the key-value cache in transformers, a task-agnostic approach but requiring fine-tuning during inference. For diffusion models, a recent work Li et al. (2024) introduced soft value-based decoding, a derivative-free algorithm that requires pre-sampled trajectories x 0 , x 1 , . . . , x T of discrete diffusion model to estimate a conditional expectation. This method does not exploit the special properties of the masking process. To the best of our knowledge, there are no fine-tuning-free samplers for controllable generation in discrete masked models.",
            "score": 0.5928987831485348,
            "section_title": "RELATED WORK",
            "char_start_offset": 16791,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1662
                }
            ],
            "ref_mentions": [
                {
                    "start": 369,
                    "end": 393,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 994,
                    "end": 1013,
                    "matchedPaperCorpusId": "245986550"
                },
                {
                    "start": 1045,
                    "end": 1068,
                    "matchedPaperCorpusId": "208617790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "266573122",
            "title": "SVGDreamer: Text Guided SVG Generation with Diffusion Model",
            "text": "Denoising diffusion probabilistic models (DDPMs) [8,31,33,35], particularly those conditioned on text, have shown promising results in text-to-image synthesis. For example, Classifier-Free Guidance (CFG) [7] has improved visual quality and is widely used in large-scale text conditional diffusion model frameworks, including GLIDE [20], Stable Diffusion [26], DALL\u2022E 2 [24], Imagen [27] and DeepFloyd IF [37]. The progress achieved by text-to-image diffusion models [20,24,26,27]  opment of a series of text-guided tasks, such as text-to-3D [22]. In this work, we employ Stable Diffusion model to provide supervision for text-to-SVG generation.",
            "score": 0.5928041039245983,
            "section_title": "Text-to-Image Diffusion Model",
            "char_start_offset": 7297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 644
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 52,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 52,
                    "end": 55,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 55,
                    "end": 58,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 58,
                    "end": 61,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 331,
                    "end": 335,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 354,
                    "end": 358,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 382,
                    "end": 386,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 466,
                    "end": 470,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 473,
                    "end": 476,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 476,
                    "end": 479,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 541,
                    "end": 545,
                    "matchedPaperCorpusId": "252596091"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "268351323",
            "title": "Active Generation for Image Classification",
            "text": "Text-to-image diffusion models are generative models designed to create realistic images from textual descriptions.These models employ diffusion processes to iteratively generate images based on the semantic information provided in the  input text.The objective is to capture the essence of the textual description and translate it into visually coherent images.\n\nText-to-image diffusion models incorporate additional text conditional variables c in the noise prediction model to predict the conditional noise \u03f5 \u03b8 (x t , c, t) and guide the generation.A classifier-free guidance technique [22] is typically adopted, enabling the utilization of text-conditioned guidance during training without the need for a classifier.The denoising model is trained to handle both conditioned input, where a text prompt is provided, and unconditioned input, where the prompt is replaced with \u2205.This allows for the representation of the guidance direction as the translation from the conditioned input \u03f5 \u03b8 (x t , c, t) to the unconditioned input \u03f5 \u03b8 (x t , \u2205, t).The guidance is performed with a guidance scale s by\n\nIn this paper, we directly use the public text-to-image diffusion models [28,35] trained on large-scale text-image pairs as our generative models, with our innovative guidance methods introduced in inference process.",
            "score": 0.5926716682772457,
            "section_title": "Text-conditioned guidance",
            "char_start_offset": 10717,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 115,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 362
                },
                {
                    "start": 364,
                    "end": 552
                },
                {
                    "start": 552,
                    "end": 720
                },
                {
                    "start": 720,
                    "end": 879
                },
                {
                    "start": 879,
                    "end": 1047
                },
                {
                    "start": 1047,
                    "end": 1099
                },
                {
                    "start": 1101,
                    "end": 1317
                }
            ],
            "ref_mentions": [
                {
                    "start": 589,
                    "end": 593,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1178,
                    "end": 1181,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93115234375
        },
        {
            "corpus_id": "257078922",
            "title": "Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC",
            "text": "It may be convenient to train a model of p(x) where x is, say, the distribution of all images, but in practice we often want to generate samples from p(x|y) where y is some attribute, label, or feature. This can be accomplished within the framework of diffusion models by introducing a learned predictive model p \u03b8 (y|x; t), i.e a time-conditional model of the distribution of some feature y given x. We can then exploit Bayes' rule to notice that (for \u03bb = 1), \u2207 x log p \u03b8 (x|y; t) = \u2207 x log p \u03b8 (x; t) + \u03bb\u2207 x log p \u03b8 (y|x; t). (6) \n\nIn practice, when using the right side of Equation 6 for sampling, it is beneficial to increase the 'guidance scale' \u03bb to be > 1 (Dhariwal & Nichol, 2021). Thus, we can repurpose the unconditional diffusion model and turn it into a conditional model. \n\nIf instead of a classifier, we have a both an unconditional diffusion model \u2207 x log p \u03b8 (x; t) and a conditional diffusion model \u2207 x log p \u03b8 (x|y; t), we can again utilize Bayes' rule to derive an implicit predictive model's gradients \u2207 x log p \u03b8 (y|x; t) = \u2207 x log p \u03b8 (x|y; t) \u2212 \u2207 x log p \u03b8 (x; t) (7) which can be used to replace the explicit model in Equation 6, giving what is known as classifier-free guidance (Ho & Salimans, 2022). This method has led to incredible performance, but comes at a cost to modularity. This contrasts with the classifier-guidance setting, where we only need to train a single (costly) generative model. We can then attach any predictive model we would like to for conditioning. This is beneficial as it is often much easier and cheaper to train predictive models than a flexible generative model. In the classifier-free setting, we must know exactly which y we would like to condition on, and incorporate these labels into model training.",
            "score": 0.5923772038227259,
            "section_title": "Controllable Generation",
            "char_start_offset": 8186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 531
                },
                {
                    "start": 534,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 784
                },
                {
                    "start": 787,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1760
                }
            ],
            "ref_mentions": [
                {
                    "start": 663,
                    "end": 688,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80517578125
        },
        {
            "corpus_id": "272022133",
            "title": "Synthetic data generation by diffusion models",
            "text": "GANs) when scaling to higher resolutions or more complex data types. Many efforts have been devoted to improving the sample efficiency, including the training-free ODE solvers [10 ] and the disti l lation methods [11 ] with some extra training. In many generation tasks, e.g. textto-image generation [7 ], we would have some input (e.g. text prompts). One key technique for such applications is classifier-free guidance [12 ], which trains two weight-sharing models \u03b8 (x t , t, y ) and \u03b8 (x t , t, \u2205 ) for the noise prediction models, where y denotes the input (e.g. text prompt) and \u03b8 (x t , t, y ) is the conditional model. Here, we use \u2205 as a special 'empty' token for the unconditional model. In a practical implementation, Ho and Salimans [12 ] chose to randomly set y to the unconditional identifier \u2205 with some prespecified probability. Classifier-free guidance then combines these two models as \u02c6 \u03b8 (x t , t, y ) := (1 + s ) \u03b8 (x t , t, y ) \u2212 s \u03b8 (x t , t, \u2205 ) to trade off the text-image alignment and the sample diversity. The hyperparameter s is known as a 'guidance scale' , where a larger s usually improves the text-image alignment, but reduces the sample diversity. By choosing a proper guidance scale, pre-trained largescale text-to-image diffusion models can generate images with comparable quality to human artists. \n\nBesides images, diffusion models (often with proper extension of the guidance) have been adopted for generating high-quality data across various domains, including speech, three-dimensional (3D) contents, human motions, videos and molecules.",
            "score": 0.5908319504236778,
            "section_title": "Jun Zhu",
            "char_start_offset": 3457,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1577
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 181,
                    "matchedPaperCorpusId": "249282317"
                },
                {
                    "start": 213,
                    "end": 218,
                    "matchedPaperCorpusId": "257280191"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8134765625
        },
        {
            "corpus_id": "278394368",
            "title": "Diffusion Model Quantization: A Review",
            "text": "Text-conditional Guided Image Generation. We evaluate the performance of text-guided image generation using Stable Diffusion v1-4 on the MS-COCO 512x512 [91] dataset. The PLMS [29] sampler is employed with 50 sampling steps, and the classifier-free guidance (cfg) is fixed at the default value of 7.5 in Stable Diffusion, serving as a trade-off between sample quality and diversity. \n\nFor each evaluation round, we generate 30,000 images and compute the FID, sFID, and CLIP score [143] as quantitative evaluation metrics.",
            "score": 0.5903114365830637,
            "section_title": "Experimental Setup",
            "char_start_offset": 69059,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 521
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 157,
                    "matchedPaperCorpusId": "14113767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7138671875
        },
        {
            "corpus_id": "265498527",
            "title": "DiG-IN: Diffusion Guidance for Investigating Networks - Uncovering Classifier Differences, Neuron Visualisations, and Visual Counterfactual Explanations",
            "text": "Classifier-free guidance (CFG) [36] was introduced as an alternative to classifier guidance [9,45,56,80]. [18] already used a class-conditional denoising model \u03f5 \u03b8 (x t , t, y) that was given the target class as additional input. The class label y was thereby integrated into the model via adaptive group normalization layers. They introduced classifier guidance to enforce the generation of the correct target class by strengthening the influence of y on the output of the generative process. Classifier-free guidance is an alternative that also strengthens the impact of the conditioning signal in combination with a conditional denoising model \u03f5 \u03b8 (x t , t, y) without the requirement of an external classifier. \n\nIn the following, we will first introduce cross-attention (XA) conditioning that is used by Stable Diffusion [64] to condition the denoising model \u03f5 \u03b8 not only on class labels but also other modalities such as text prompts or depth maps. Then we will introduce classifier-free Guidance as a solution to strengthen the impact of the conditioning signal.",
            "score": 0.5902050636743851,
            "section_title": "A.3. Classifier-Free Guidance and Cross-Attention Conditioning",
            "char_start_offset": 35114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1069
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 101,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 101,
                    "end": 104,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 826,
                    "end": 830,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "277272356",
            "title": "Guidance Free Image Editing via Explicit Conditioning",
            "text": "Despite the success of diffusion models, faster training and more diverse and higher quality sampling was shown to depend on selecting data, noise or velocity parametrizations (Salimans & Ho, 2022). Conditional Flow Matching (CFM) (Lipman et al., 2023;Tong et al., 2024;Lipman et al., 2024;Esser et al., 2024) provides an alternative natural parametrization for diffusion, namely via optimal transport, that yields SOTA models. More broadly, general diffusion and flow models can be recast in a unified framework, viewed under Evidence Lower BOund (ELBO) objectives (Kingma & Gao, 2023). \n\nCFG and selected alternatives Classifier Free Guidance (CFG) (Ho & Salimans, 2022;Zheng et al., 2023) is a key pillar of SOTA diffusion and flow systems, defining a conditional framework to image generation. Great focus is on its theoretical underpinning (Bradley & Nakkiran, 2024) and candidate alternatives such as Independent Condition Guidance (ICG) and Time Step Guidance (TSG) (Sadat et al., 2024). Our work provides a new avenue of research for conditional image generation with a new Explicit Conditioning (EC) mechanism that is on par with CFG in terms of sample diversity and quality. \n\nSingle pass distillation. Previous work (Meng et al., 2023) can distill CFG from three passes to one pass, sensibly reducing latency, while matching CFG on diversity and quality generation. More recently (Ahn et al., 2024) proposed to extend the distillation-based line of methods to learn a mapping from normal Gaussian to a guidance-free Gaussian distribution. Their work shares similarities with us in terms of exploring the sampling distribution, however, they still rely on guidance-dependent teachers to learn such a mapping. Our approach achieves the same goal without the painstaking of distillation. \n\nImage editing.",
            "score": 0.5898871155195515,
            "section_title": "Related Work",
            "char_start_offset": 5845,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1795
                },
                {
                    "start": 1798,
                    "end": 1812
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 270,
                    "matchedPaperCorpusId": "259847293"
                },
                {
                    "start": 270,
                    "end": 290,
                    "matchedPaperCorpusId": "274598274"
                },
                {
                    "start": 290,
                    "end": 309,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 845,
                    "end": 871,
                    "matchedPaperCorpusId": "271903235"
                },
                {
                    "start": 1227,
                    "end": 1246,
                    "matchedPaperCorpusId": "252762155"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.759765625
        },
        {
            "corpus_id": "253761291",
            "title": "Person Image Synthesis via Denoising Diffusion Model",
            "text": "Recently, Ren et al. [18] propose a framework NTED based on neural texture extraction and distribution operation, which achieves superior results. \n\nDiffusion Models: The existing GAN-based approaches attempt to directly transfer the style of the source image to a given target pose, which requires the architecture to model complex transformation of pose. In this work, we present a diffusion-based framework named PIDM that breaks the pose transformation process into several conditional denoising diffusion steps, in which each step is relatively simple to model. Diffusion models [6] are recently proposed generative models that can synthesize high-quality images. After success in unconditional generation, these models are extended to work in conditional generation settings, demonstrating competitive or even better performance than GANs. For class-conditioned generation, Dhariwal et al. [2] introduce classifier-guided diffusion, which is later adapted by GLIDE [16] to enable conditioning over CLIP textual representations. Recently, Ho et al. [7] propose a Classifier-Free Guidance approach that enables conditioning without requiring pretraining of the classifiers. In this work, we develop the first diffusion-based approach for poseguided person synthesis task. We also introduce disentangled classifier-free guidance to tightly align the output im-age style and pose with the source image appearance and target pose, respectively.",
            "score": 0.5895907173815285,
            "section_title": "Related Work",
            "char_start_offset": 6158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 149,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1445
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 25,
                    "matchedPaperCorpusId": "248157334"
                },
                {
                    "start": 584,
                    "end": 587,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 896,
                    "end": 899,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83642578125
        },
        {
            "corpus_id": "268819999",
            "title": "Towards Memorization-Free Diffusion Models",
            "text": "Classifier guidance (CG) and classifier-free guidance (CFG) are methods used in diffusion models to steer image generation towards higher likelihood outcomes as determined by an explicit or implicit classifier p \u03d5 py|x t q.In the scorebased framework [38], CG and CFG involve learning the gradient of the log probability for the conditional model, \u2207 xt log p \u03b8 px t |yq, rather than the score of unconditional model, \u2207 xt log p \u03b8 px t q.The conditional score can be easily derived using Bayes' rule as the sum of the unconditional score and the gradient of the log classifier probability:\n\n\u2207 xt log p \u03b8 px t |yq \" \u2207 xt log p \u03b8 px t q `\u2207xt log p \u03d5 py|x t q (5) Classifier Guidance (CG) [7] involves training an explicit classifier p \u03d5 py|x t q on perturbed images x t and then employing its gradients \u2207 xt log p \u03d5 py|x t q to direct the diffusion sampling process towards a class label y.Inserting Eq. (4) into Eq.( 5), [7] shows a new epsilon prediction \u03b5 corresponds to the score presented in Eq. ( 5):\n\nClassifier-Free Guidance (CFG) [11] eliminates the need of an explicit classifier for computing Eq. ( 5).It requires concurrent training on conditional and unconditional objectives.At inference, the epsilon prediction is linearly directed towards the conditional prediction and away from the unconditional, and s 0 \u0105 1 controls the degree of adjustment: \u03b5 \u00d0 \u03f5 \u03b8 px t q `s0 \u00a8p\u03f5 \u03b8 px t , yq \u00b4\u03f5\u03b8 px t qq.",
            "score": 0.589408533258795,
            "section_title": "Guidance in Diffusion Models",
            "char_start_offset": 10351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 223,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 588
                },
                {
                    "start": 590,
                    "end": 887
                },
                {
                    "start": 887,
                    "end": 913
                },
                {
                    "start": 913,
                    "end": 1003
                },
                {
                    "start": 1005,
                    "end": 1110
                },
                {
                    "start": 1110,
                    "end": 1186
                },
                {
                    "start": 1186,
                    "end": 1406
                }
            ],
            "ref_mentions": [
                {
                    "start": 685,
                    "end": 688,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 919,
                    "end": 922,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7275390625
        },
        {
            "corpus_id": "265302625",
            "title": "SEGA: Instructing Text-to-Image Models using Semantic Guidance",
            "text": "The first step towards SEGA is guided diffusion. Specifically, diffusion models (DM) iteratively denoise a Gaussian distributed variable to produce samples of a learned data distribution. For text-toimage generation, the model is conditioned on a text prompt p and guided toward an image faithful to that prompt. The training objective of a DM x\u03b8 can be written as \n\nwhere (x, c p ) is conditioned on text prompt p, t is drawn from a uniform distribution t \u223c U([0, 1]), \u03f5 sampled from a Gaussian \u03f5 \u223c N (0, I), and w t , \u03c9 t , \u03b1 t influence the image fidelity depending on t. Consequently, the DM is trained to denoise z t := x + \u03f5 yielding x with the squared error loss. At inference, the DM is sampled using the prediction of x = (z t \u2212 \u03b5\u03b8 ), with \u03b5\u03b8 as described below. \n\nClassifier-free guidance [10] is a conditioning method using a purely generative diffusion model, eliminating the need for an additional pre-trained classifier. During training, the text conditioning c p drops randomly with a fixed probability, resulting in a joint model for unconditional and conditional objectives. During inference, the score estimates for the x-prediction are adjusted so that: \n\nwith guidance scale s g and \u03f5 \u03b8 defining the noise estimate with parameters \u03b8. Intuitively, the unconditioned \u03f5-prediction is pushed in the direction of the conditioned one, with s g determining the extent of the adjustment.",
            "score": 0.5885417828256144,
            "section_title": "Guided Diffusion",
            "char_start_offset": 7683,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 49,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 364
                },
                {
                    "start": 367,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 771
                },
                {
                    "start": 774,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1399
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.900390625
        },
        {
            "corpus_id": "274514554",
            "title": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
            "text": "Classifier-free guidance (CFG) [35] significantly improves the sample quality and motion stability of text-conditioned diffusion models. However, it increases computational cost and inference latency since it requires extra output for the unconditional input at each inference step. Especially for the large video model and high-resolution video generation, the inference burden is extremely expensive when generating text-conditional and text-unconditional videos, simultaneously. To solve this limitation, we distill the combined output for unconditional and conditional inputs into a single student model [60]. Specifically, the student model is conditioned on a guidance scale and shares the same structures and hyper-parameters as the teacher model. We initialize the student model with the same parameters as the teacher model and train with the guidance scale randomly sampled from 1 to 8. We experimentally find that text-guidance distillation approximatively brings 1.9x acceleration.",
            "score": 0.5882255564932078,
            "section_title": "Text-guidance Distillation",
            "char_start_offset": 30235,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 993
                }
            ],
            "ref_mentions": [
                {
                    "start": 608,
                    "end": 612,
                    "matchedPaperCorpusId": "252762155"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7880859375
        },
        {
            "corpus_id": "270226598",
            "title": "Guiding a Diffusion Model with a Bad Version of Itself",
            "text": "The primary axes of interest in image-generating diffusion models are image quality, the amount of variation in the results, and how well the results align with a given condition, e.g., a class label or a text prompt. The popular classifier-free guidance approach uses an unconditional model to guide a conditional model, leading to simultaneously better prompt alignment and higher-quality images at the cost of reduced variation. These effects seem inherently entangled, and thus hard to control. We make the surprising observation that it is possible to obtain disentangled control over image quality without compromising the amount of variation by guiding generation using a smaller, less-trained version of the model itself rather than an unconditional model. This leads to significant improvements in ImageNet generation, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using publicly available networks. Furthermore, the method is also applicable to unconditional diffusion models, drastically improving their quality.",
            "score": 0.5878645130982508,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8349609375
        },
        {
            "corpus_id": "261075939",
            "title": "LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model",
            "text": "Diffusion models (Ho, Jain, and Abbeel 2020;Sohl-Dickstein et al. 2015) are deep generative models that use stochastic diffusion processes. Diffusion-based methods have been applied to various tasks and shown superior performance of generative diversity and facticity, including text-to-image generation (Rombach et al. 2021;Ruiz et al. 2022;Gu et al. 2022), music generation (Mittal et al. 2021;Hawthorne et al. 2022;Schneider, Jin, and Sch\u00f6lkopf 2023), seq2seq generation (Gong et al. 2023;Yuan et al. 2022) and text-to-motion generation (Ren et al. 2022;Zhang et al. 2022;Tevet et al. 2023;Zhao et al. 2023)... As for conditional generation, Dhariwal and Nichol (Dhariwal and Nichol 2021) introduced classifier-guided diffusion. The Classifier-Free Guidance approach proposed by Ho and Salimans (Ho and Salimans 2022) enables conditioning while trading-off fidelity and diversity. Stable Diffusion (Rombach et al. 2021) has also shown its ability to synthesize highquality images from text. Recently, diffusion-based methods have been studied in text-to-motion tasks (Tevet et al. 2023;Zhang et al. 2022). The text conditions are usually attributes that are easy to model, whereas music and past motions as conditions in the sequence-to-sequence problem of dance generation are much more complex.",
            "score": 0.5872428456662694,
            "section_title": "Generative Diffusion Methods",
            "char_start_offset": 11811,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1299
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 44,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 44,
                    "end": 71,
                    "matchedPaperCorpusId": "215896890"
                },
                {
                    "start": 304,
                    "end": 325,
                    "matchedPaperCorpusId": "253098430"
                },
                {
                    "start": 342,
                    "end": 357,
                    "matchedPaperCorpusId": "244714856"
                },
                {
                    "start": 492,
                    "end": 508,
                    "matchedPaperCorpusId": "254877381"
                },
                {
                    "start": 540,
                    "end": 557,
                    "matchedPaperCorpusId": "253098430"
                },
                {
                    "start": 901,
                    "end": 921,
                    "matchedPaperCorpusId": "253098430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73388671875
        },
        {
            "corpus_id": "256416107",
            "title": "Meta-Learning via Classifier(-free) Diffusion Guidance",
            "text": "We introduce meta-learning algorithms that perform zero-shot weight-space adaptation of neural network models to unseen tasks. Our methods repurpose the popular generative image synthesis techniques of natural language guidance and diffusion models to generate neural network weights adapted for tasks. We first train an unconditional generative hypernetwork model to produce neural network weights; then we train a second\"guidance\"model that, given a natural language task description, traverses the hypernetwork latent space to find high-performance task-adapted weights in a zero-shot manner. We explore two alternative approaches for latent space guidance:\"HyperCLIP\"-based classifier guidance and a conditional Hypernetwork Latent Diffusion Model (\"HyperLDM\"), which we show to benefit from the classifier-free guidance technique common in image generation. Finally, we demonstrate that our approaches outperform existing multi-task and meta-learning methods in a series of zero-shot learning experiments on our Meta-VQA dataset.",
            "score": 0.5868568837600945,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91357421875
        },
        {
            "corpus_id": "254408668",
            "title": "Diffusion Guided Domain Adaptation of Image Generators",
            "text": "Latent diffusion model. We use the publicly available latent diffusion model(LDM) StableDiffusion [46] in this paper as our guidance model. A LDM encodes images x into latent space z with an encoder E, z 0 = E(x), and the denoising process is preformed in the latent space Z. Briefly, a latent diffusion model \u03b8 can be trained on a denoising objective of the following form: \n\nwhere (x, c) are data-conditioning pairs, \u223c N (0, I), t \u223c Uniform(1, T ) and w t is a weighting term. Classifier-free guidance. In the denoising sampling process, Classifier guidance is an effective method to guide the synthesis better toward the desired direction, e.g. a class or a text prompt [4]. The method uses gradients from a pre-trained model p(c|z t ) during sampling. Classifier-free guidance (CFG) [12] is an alternative technique that avoids this pre-trained classifier. During the training of the conditional diffusion model, randomly dropping the condition c lets the model learns to generate an image even without a condition. Therefore, a well-conditioned image can be generated by pushing the synthetic results under condition c further away from the unconditioned results during the diffusion process, where \n\nHere, \u03b8 (z t |c, t) and \u03b8 (z t |\u2205, t) are conditional and unconditional -predictions. s is the guidance weight and increasing s > 1 strengthens the effect of guidance.",
            "score": 0.5864820574193842,
            "section_title": "Background",
            "char_start_offset": 6326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1373
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 102,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 673,
                    "end": 676,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8994140625
        },
        {
            "corpus_id": "270869763",
            "title": "Diffusion Models and Representation Learning: A Survey",
            "text": "To address this limitation, Classifier-free guidance (CFG) [67] eliminates the need for a pre-trained classifier.CFG works by training an unconditional diffusion model parametrized by \u03f5 \u03b8 (x t , t, \u03d5) together with a conditional model parametrized by \u03f5 \u03b8 (x t , t, c).For the unconditional model, a null input token \u03d5 is used as a conditioning signal c.The network is trained by randomly dropping out the conditioning signal with probability p uncond .Sampling is then performed using a weighted combination of conditional and unconditional score estimates:\n\nThis sampling method does not rely on the gradients of a pre-trained classifier but still requires an annotated dataset to train the conditional denoising network.Fully unconditional approaches have yet to match classifier-free guidance, though recent works using diffusion model representations for self-supervised guidance show promise [73,100].These methods do not need annotated data, allowing the use of larger unlabelled datasets.\n\nTable 1 shows the requirements of current guidance methods.While classifier and classifier-free guidance improve generation results, they require annotated training data.Self-guidance and online guidance are fully selfsupervised alternatives that achieve competitive performance without annotations.\n\nClassifier and classifier-free guidance are controlled generation methods that rely on conditional training.Trainingfree approaches modify the generation process of a pretrained model by binding multiple diffusion processes [14] or using time-independent energy functions [179].Other controlled generation methods take a variational perspective [54,119,146,164], treating controlled generation as a source point optimization problem [17].The goal is to find samples x that minimize a loss function L(x) and are likely under the model's distribution p.The optimization is formulated as min x0 L(x), where x 0 is the source noise point.The loss function L(x) can be modified for conditional sampling to generate a sample belonging to a particular class y.",
            "score": 0.5859617492017117,
            "section_title": "Diffusion Model Guidance",
            "char_start_offset": 17610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 113,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 353
                },
                {
                    "start": 353,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 557
                },
                {
                    "start": 559,
                    "end": 722
                },
                {
                    "start": 722,
                    "end": 906
                },
                {
                    "start": 906,
                    "end": 995
                },
                {
                    "start": 997,
                    "end": 1056
                },
                {
                    "start": 1056,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1296
                },
                {
                    "start": 1298,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1576
                },
                {
                    "start": 1576,
                    "end": 1736
                },
                {
                    "start": 1736,
                    "end": 1849
                },
                {
                    "start": 1849,
                    "end": 1932
                },
                {
                    "start": 1932,
                    "end": 2051
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 63,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1522,
                    "end": 1526,
                    "matchedPaperCorpusId": "256900756"
                },
                {
                    "start": 1651,
                    "end": 1655,
                    "matchedPaperCorpusId": "258418154"
                },
                {
                    "start": 1655,
                    "end": 1659,
                    "matchedPaperCorpusId": "257757144"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78662109375
        },
        {
            "corpus_id": "257102889",
            "title": "Region-Aware Diffusion for Zero-shot Text-driven Image Editing",
            "text": "Enhanced Directional Guidance. To reinforce the editing direction of the source region to follow the target text, we attempt to modify a classifier-free guidance [17] to strengthen cross-modal guidance. It is a strategy for guiding diffusion models without necessitating the training of a separate classifier model. Generally, classifier-free guidance offers two benefits. For starters, rather than relying on the knowledge of a separate (and perhaps smaller) categorization model, it allows a single model to leverage its experience while guiding. Second, it simplifies directing when conditioned on information that is difficult to predict using a classifier. \n\nIn order to provide classifier-free guidance, the tag  in a class-conditional diffusion model   (  | ) is replaced with a null tag \u2205 throughout the training process. The output of the model is further extended in the direction of   (  | ) \n\nand away from   (  | \u2205) during sampling: \n\nThe recommended guidance scale is  = 5. This equation was inspired by the classifier, \n\nThe function of the true scores is used to represent the gradient  * , \n\nThe modified prediction \u03b5 is subsequently employed to guide us toward the target text prompts  2 , as demonstrated in Algorithm 1:",
            "score": 0.5853821433888846,
            "section_title": "Manuscript submitted to ACM",
            "char_start_offset": 13072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1239
                }
            ],
            "ref_mentions": [
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "271244787",
            "title": "CoSIGN: Few-Step Guidance of ConSIstency Model to Solve General INverse Problems",
            "text": "Guiding Diffusion Models.Existing works have explored methods to guide diffusion models with class labels [7], texts [30,34,36] and images [34,54].Classifier guidance [7] and Classfier-Free Guidance (CFG) [14] pioneered class-conditioned generation.Benefiting from CFG, large text-to-image diffusion models like Latent Diffusion [34], Imagen [36] and Glide [30] enjoyed great success by leveraging pre-trained text and image encoder to inject guidance into sampling process of diffusion models.ControlNet [54] provided a fine-tuning method to adapt these pre-trained models to specific image-to-image translation tasks.In tasks like sketch-to-image and depth-to-image, ControlNet showed superior ability in enhancing semantic and structural similarity.However, its ability of enforcing measurement consistency in general inverse problems still remains under-explored.\n\nAccelerating Diffusion Models.Slow sampling speed has been limiting the real-world applications of diffusion models in generation of 3D scenes [32,49], videos [26] and speeches [51].Recent works proposing to switch from SDE [13] to ODE [48] and adopt higher-order ODE solvers [16,23,24,56] managed to accelerate sampling process to 5-10 NFEs.Unfortunately, these ODE solvers cannot be directly applied to the likelihood score \u2207 xt log p t (y|x t ) in DIS.Another line of works [25,27,38,44] explored methods to distill the sampling trajectory of pre-trained diffusion models.Although these methods efficiently accelerate unconditional image generation, how can these distillation models help accelerate inverse problem solving has not been thoroughly studied yet.",
            "score": 0.5852711694648474,
            "section_title": "Related Work",
            "char_start_offset": 7698,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 25,
                    "end": 147
                },
                {
                    "start": 147,
                    "end": 249
                },
                {
                    "start": 249,
                    "end": 494
                },
                {
                    "start": 494,
                    "end": 619
                },
                {
                    "start": 619,
                    "end": 752
                },
                {
                    "start": 752,
                    "end": 867
                },
                {
                    "start": 869,
                    "end": 899
                },
                {
                    "start": 899,
                    "end": 1051
                },
                {
                    "start": 1051,
                    "end": 1211
                },
                {
                    "start": 1211,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1444
                },
                {
                    "start": 1444,
                    "end": 1632
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 109,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 117,
                    "end": 121,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 121,
                    "end": 124,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 124,
                    "end": 127,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 139,
                    "end": 143,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 167,
                    "end": 170,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 329,
                    "end": 333,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 342,
                    "end": 346,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 1012,
                    "end": 1016,
                    "matchedPaperCorpusId": "252596091"
                },
                {
                    "start": 1028,
                    "end": 1032,
                    "matchedPaperCorpusId": "254125713"
                },
                {
                    "start": 1093,
                    "end": 1097,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 1105,
                    "end": 1109,
                    "matchedPaperCorpusId": "254125609"
                },
                {
                    "start": 1145,
                    "end": 1149,
                    "matchedPaperCorpusId": "249240415"
                },
                {
                    "start": 1149,
                    "end": 1152,
                    "matchedPaperCorpusId": "249282317"
                },
                {
                    "start": 1350,
                    "end": 1353,
                    "matchedPaperCorpusId": "252762155"
                },
                {
                    "start": 1353,
                    "end": 1356,
                    "matchedPaperCorpusId": "246442182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.708984375
        },
        {
            "corpus_id": "251881575",
            "title": "Adaptively-Realistic Image Generation from Stroke and Sketch with Diffusion Model",
            "text": "Diffusion models are flourishing in recent years as a powerful family of generative models with diversity, training stability, and easy scalability, which GANs commonly lack. Fundamentally, diffusion models fulfill the sampling from a target distribution by reversing a progressive noise diffusion process, in which the process is defined as a Markov chain of diffusion steps for adding noise to the data. In addition to providing competitive or even superior capability on unconditional image generation [8,18] in comparison to GANs, diffusion models also make significant progress on various tasks of conditional generation. Given a target class, [5] proposes a classifier-guidance mechanism that adopts a pretrained classifier to provide gradients as guidance toward generating images of the target class. More recently, classifier-free diffusion guidance [9] introduces a technique which jointly trains a conditional and an unconditional diffusion model without any pretrained classifier. Other than directly modifying the network of an unconditional diffusion model for conditional generation, ILVR [3] instead proposes to iteratively introduce the condition into the generative process via refining the intermediate latent images with a noisy reference image at each time-step during sampling. Therefore, ILVR is able to sample highquality images while controlling the amount of high-level semantics being inherited from the given reference images. As the nature of diffusion models for adopting a progressive denoising process, the generation/synthesis via sampling can start from a noisy input (similar to the intermediate stage of sampling) instead of always beginning from random noise. SDEdit [17] hence realizes stroke-based image synthesis by starting the sampling from a stroke input with noise injected, in which the generative model used in SDEdit is built upon stochastic differential equations where its mechanism is quite similar to diffusion models (e.g. sampling via iterative denoising). In this work, we exploit both the techniques of classifier-free diffusion guidance and ILVR into our diffusion-based framework of image generation for fulfilling a three-dimensional control on the synthesized images in terms of their realism and the consistency with respect to the stroke and sketch conditions.",
            "score": 0.5845148621098072,
            "section_title": "Diffusion Models",
            "char_start_offset": 6409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2321
                }
            ],
            "ref_mentions": [
                {
                    "start": 505,
                    "end": 508,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 508,
                    "end": 511,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 649,
                    "end": 652,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 859,
                    "end": 862,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1104,
                    "end": 1107,
                    "matchedPaperCorpusId": "236950721"
                },
                {
                    "start": 1704,
                    "end": 1708,
                    "matchedPaperCorpusId": "236772794"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "270391454",
            "title": "CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models",
            "text": "Classifier-free guidance (CFG) (Ho & Salimans, 2021) forms the key basis of modern text-guided generation with diffusion models (Dhariwal & Nichol, 2021;Rombach et al., 2022). Nowadays, it is common practice to train a diffusion model with large-scale paired text-image data (Schuhmann et al., 2022), so that sampling (i.e. generating) a signal (e.g. image, video) from a diffusion model can either be done unconditionally from p \u03b8 (x|\u2205) \u2261 p \u03b8 (x), or conditionally from p \u03b8 (x|c), where c is the text conditioning. Once trained, it seems natural that one would acquire samples from the conditional distribution by simply solving the probability-flow ODE or SDE sampling (Song et al., 2021b;a;Karras et al., 2022) with the conditional score function. In practice, however, it is observed that the conditioning signal is insufficient when used naively. To emphasize the guidance, one uses the guidance scale \u03c9 > 1, where the direction can be defined by the direction from the unconditional score to the conditional score (Ho & Salimans, 2021). \n\nIn modern text-to-image (T2I) diffusion models, the guidance scale \u03c9 is typically set within the range of [5.0, 30], referred to as the moderately high range of CFG guidance (Chen et al., 2024;Podell et al., 2023). The insufficiency in guidance also holds for classifier guidance (Dhariwal & Nichol, 2021;Song et al., 2021b) so that a scale of 10 was used. While using a high guidance scale yields higher-quality images with better alignment to the condition, it is also prone to mode collapse, reduces sample diversity, and yields an inevitable accumulation of errors during the sampling process.",
            "score": 0.5843126307797828,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1642
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 52,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 128,
                    "end": 153,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 153,
                    "end": 174,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 275,
                    "end": 299,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 671,
                    "end": 691,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 693,
                    "end": 713,
                    "matchedPaperCorpusId": "249240415"
                },
                {
                    "start": 1020,
                    "end": 1041,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1219,
                    "end": 1238,
                    "matchedPaperCorpusId": "263334265"
                },
                {
                    "start": 1325,
                    "end": 1350,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1350,
                    "end": 1368,
                    "matchedPaperCorpusId": "227209335"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92919921875
        },
        {
            "corpus_id": "272524606",
            "title": "DreamMapping: High-Fidelity Text-to-3D Generation via Variational Distribution Mapping",
            "text": "Conditioned on the text prompt y, diffusion models accept it as another input for the diffusion process, denoted as \u03f5 \u03c6 (xt ,t, y), with related score function \u2207x t log p(xt |y). Classifier Free Guidance (CFG) [HS21] is utilized as an implicit classifier to get the textual guidance for image generation. It has a changeable hyperparameter named CFG scale, hereafter denoted as s, and the original prediction is changed as: \n\n(4) As mentioned in Eq. ( 3), this prediction is also related to a corresponding score function using Bayes' rule: \n\n(5) the latter term can be intuitively understood as guiding xt being more in line with the text description y.",
            "score": 0.5830337116284964,
            "section_title": "Diffusion Models",
            "char_start_offset": 9631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 654
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 216,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63330078125
        },
        {
            "corpus_id": "259360783",
            "title": "Single Image LDR to HDR Conversion Using Conditional Diffusion",
            "text": "Similar to other generative frameworks, diffusion models can be made to sample conditionally given some variable of interest p \u03b8 (x0 | y) like a class label or a sentence description. Particularly in our case of generation of HDR images given a single LDR input, we want our output that is generated by starting from Gaussian noise to be conditioned on the input LDR image. [11] show that guiding the diffusion process using a separate classifier can help. \n\nIn this setup, we take a pre-trained classifier to guide the reverse diffusion process. Specifically, we push process in the direction of the gradient of the target label probability. The downside of this approach is the reliance of another guiding network. An alternative approach proposed by [6] eliminates this reliance by using special training of the diffusion model itself to guide the sampling. \n\nDuring training, the conditioning label, denoted by y, can be assigned a null label with a certain probability. At the inference stage, an artificial shift towards the conditional direction is applied to the reconstructed samples using a parameter termed the guidance scale (s) to distance them from the null label and thus enhance the effect of conditioning. This approach has been shown to yield superior sample quality based on human evaluation compared to classifier guidance, as reported in [16]. Table 3: Quantitative evaluation of our approach. The best-performing method is highlighted red and second-best performing method is indicated by blue for each metric.",
            "score": 0.5829464499338246,
            "section_title": "Conditional Diffusion",
            "char_start_offset": 6291,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1532
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65234375
        },
        {
            "corpus_id": "258967543",
            "title": "Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the World's Ugliness?",
            "text": "Classifier Free Guidance. Before going into detail on different instruction methods for image generation, we need to establish some fundamentals of text-to-image diffusion models (DMs). Intuitively, image generation starts from random noise \u03f5, and the model predicts an estimate of this noise \u03b5\u03b8 to be subtracted from the initial values. This results in a high-fidelity image x without any noise. Since this is a complex problem, multiple steps are applied, each subtracting a small amount (\u03f5 t ) of the predictive noise, approximating \u03f5. For text-to-image generation, the model's \u03f5-prediction is conditioned on a text prompt p and results in an image faithful to that prompt. To that end, DMs employ classifier-free guidance (Ho & Salimans, 2022), a conditioning method using a purely generational diffusion model, eliminating the need for an additional pre-trained classifier. \n\nThe noise estimate \u03b5\u03b8 uses an unconditioned \u03f5-prediction \u03f5 \u03b8 (z t ) which is pushed in the direction of the conditioned estimate \u03f5 \u03b8 (z t , c p ) to yield an image faithful to prompt p. \n\nInstructing Text-to-Image Models on Safety. We now consider two different instruction approaches extending the principles of classifier-free guidance. Both methods rely on a secondary text prompt s that describes concepts to suppress during generation. First, negative prompting replaces the unconditioned \u03f5-prediction \u03f5 \u03b8 (z t ) with one conditioned on s: \u03f5 \u03b8 (z t , c s ), thus moving away from the inappropriate concepts. This approach is intuitive and easy to implement, however offers limited control over the extent of content suppression. Additionally, we use Semantic Guidance (SEGA) (Brack et al., 2023) which is a powerful method for image manipulation based on additional text prompts. SEGA adds an additional guidance term to \u03b5\u03b8 that allows us to steer the generation away from s, while keeping changes to the original image minimal.",
            "score": 0.5828973428977694,
            "section_title": "Instructing Models on the World's Ugliness",
            "char_start_offset": 4676,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1914
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93115234375
        },
        {
            "corpus_id": "268680839",
            "title": "An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in Diffusion Models",
            "text": "Guided generation of data involves creating new data samples, x, based on given pair of data (x 0 , y 0 ).We can condition the score function on a label y to learn conditional generative models for class-specific data generation.This objective can be done by leveraging the Bayes' rule to decompose the conditional score function [5]:\n\nClassifier guidance.By providing a weight term \u03bb to adjust the balance between the unconditional score function and the classifier guidance during the sampling phase, we can recast Eq. 6 as follows:\n\nunconditional score function (7) where the first term can be learned by a classifier.Classifier-free guidance (CFG).We can also model the unconditional score function \u2207 xt log p(x t ) and the joint score function \u2207 xt log p(x t , y) simultaneously to substitute the classifier guidance [7] for obtaining a trade-off between the quality and diversity of samples.We replace \u03bb in Eq. 7 with 1 + \u03c9 to be consistent with the range [0, +\u221e] of the guidance scale \u03c9 proposed in the original paper:\n\nThe conditional generation in our study is achieved through classifier-free guidance from caption y to image x, while unconditional generation uses the same approach with an empty caption embedding.",
            "score": 0.5826733344954276,
            "section_title": "Guided diffusion models",
            "char_start_offset": 7396,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 106,
                    "end": 229
                },
                {
                    "start": 229,
                    "end": 334
                },
                {
                    "start": 336,
                    "end": 356
                },
                {
                    "start": 356,
                    "end": 534
                },
                {
                    "start": 536,
                    "end": 621
                },
                {
                    "start": 621,
                    "end": 652
                },
                {
                    "start": 652,
                    "end": 897
                },
                {
                    "start": 897,
                    "end": 1025
                },
                {
                    "start": 1027,
                    "end": 1225
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79736328125
        },
        {
            "corpus_id": "272832492",
            "title": "MonoFormer: One Transformer for Both Diffusion and Autoregression",
            "text": "We train the model on the JourneyDB (Sun et al., 2024a) and UltraChat (Ding et al., 2023) for text-to-image generation and text generation. Considering that the diffusion task is more difficult, the ratio of the numbers of image generation samples and text generation samples is 9 : 1. The Global batch size is 1024. \n\nClassifier-free guidance. We adopt classifier-free guidance (Ho & Salimans, 2022) for text-toimage generation. In the training stage, we randomly drop the text tokens for unconditional generation. We set the drop probability to 1/10. The final latent output is computed by combining the unconditional and conditional outputs and using a guidance factor to control the scale of the guidance.",
            "score": 0.5824509416345424,
            "section_title": "EXPERIMENTS",
            "char_start_offset": 14229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 709
                }
            ],
            "ref_mentions": [
                {
                    "start": 36,
                    "end": 55,
                    "matchedPaperCorpusId": "259316541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71630859375
        },
        {
            "corpus_id": "272832564",
            "title": "TFG: Unified Training-Free Guidance for Diffusion Models",
            "text": "Recently, training-free guidance for diffusion models has gained increasing attention and has been adopted in various applications. TFG is based on an extensive literature review over ten algorithmic papers for different purposes, including images, audio, molecules, and motions [34,8,43,32,19,13,16,15,45,38,68]. While we incorporate several key algorithms into our framework, we acknowledge that encompassing all approaches is impossible, as it would make the unification bloated and less practical. We seek to find a balance point by unifying most representative algorithms while keeping the techniques clear and easily studied. \n\nAn often discussed problem is why we care about training-free guidance, given the ever-growing community of language-based generative models such as the image generator of GPT4. In practice, there are countless conditional generation tasks where the conditions are hard to accurately convey to or represent by language encoders. For instance, it can fail to under a complex property of a molecule or generate CelebA-style faces. We give an illustrative analysis in Appendix A.",
            "score": 0.5823520053759366,
            "section_title": "Discussions and Limitations",
            "char_start_offset": 27834,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1110
                }
            ],
            "ref_mentions": [
                {
                    "start": 285,
                    "end": 288,
                    "matchedPaperCorpusId": "253157756"
                },
                {
                    "start": 291,
                    "end": 294,
                    "matchedPaperCorpusId": "261705895"
                },
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "271745991"
                },
                {
                    "start": 300,
                    "end": 303,
                    "matchedPaperCorpusId": "258108230"
                },
                {
                    "start": 303,
                    "end": 306,
                    "matchedPaperCorpusId": "263674911"
                },
                {
                    "start": 306,
                    "end": 309,
                    "matchedPaperCorpusId": "258309302"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.132080078125
        },
        {
            "corpus_id": "260886966",
            "title": "IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models",
            "text": "Diffusion models are a class of generative models that comprise two processes: a diffusion process (also known as the forward process), which gradually adds Gaussian noise to the data using a fixed Markov chain of T steps, and a denoising process that generates samples from Gaussian noise with a learnable model. Diffusion models can also be conditioned on other inputs, such as text in the case of text-to-image diffusion models. Typically, the training objective of a diffusion model, denoted as \u03f5 \u03b8 , which predicts noise, is defined as a simplified variant of the variational bound: \n\nwhere x 0 represents the real data with an additional condition c, t \u2208 [0, T ] denotes the time step of diffusion process, x t = \u03b1 t x 0 + \u03c3 t \u03f5 is the noisy data at t step, and \u03b1 t , \u03c3 t are predefined functions of t that determine the diffusion process. \n\nOnce the model \u03f5 \u03b8 is trained, images can be generated from random noise in an iterative manner. Generally, fast samplers such as DDIM [21], PNDM [36] and DPM-Solver [37,38], are adopted in the inference stage to accelerate the generation process. \n\nFor the conditional diffusion models, classifier guidance [23] is a straightforward technique used to balance image fidelity and sample diversity by utilizing gradients from a separately trained classifier. To eliminate the need for training a classifier independently, classifier-free guidance [39] is often employed as an alternative method. In this approach, the conditional and unconditional diffusion models are jointly trained by randomly dropping c during training. In the sampling stage, the predicted noise is calculated based on the prediction of both the conditional model \u03f5 \u03b8 (x t , c, t) and unconditional model \u03f5 \u03b8 (x t , t): \n\nhere, w, often named guidance scale or guidance weight, is a scalar value that adjusts the alignment with condition c. \n\nFor text-to-image diffusion models, classifier-free guidance plays a crucial role in enhancing the image-text alignment of generated samples. \n\nIn our study, we utilize the open-source SD model as our example base model to implement the IP-Adapter.",
            "score": 0.5822503013704129,
            "section_title": "Prelimiaries",
            "char_start_offset": 11963,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1737
                },
                {
                    "start": 1740,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 2002
                },
                {
                    "start": 2005,
                    "end": 2109
                }
            ],
            "ref_mentions": [
                {
                    "start": 1014,
                    "end": 1018,
                    "matchedPaperCorpusId": "249282317"
                },
                {
                    "start": 1156,
                    "end": 1160,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "276928635",
            "title": "PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference Time by Leveraging Sparsity",
            "text": "In order to generate samples following condition given by users, diffusion models are extended to conditional generative models [18,43] with additional inputs in the models: \n\nwhere x t , \u03f5 are sampled same as Eq. 1 and c denotes a specific condition that x has, in most cases the embedding of a class or text. However, since vanilla sampling often results in suboptimal performance for conditional generation, various guidance sampling methods have been extensively explored to enhance sample quality [1,7,10,18,20,21,24,46]. For clarity, let us shorten the notation as \u03f5 \u03b8 (x t , c) := \u03f5 \u03b8 (x t , t, c) and denote the unconditional model as \u03f5 \u03b8 (x t , \u2205), where \u2205 represents the null condition. Classifier-Free Guidance (CFG) adjusts the classconditioned probability relative to the unconditional one, becoming p( \n\n, resulting in an adjusted sampling process: \n\nwhere w is the guidance scale. Recently, \"weak model\" guidance has been introduced, which weakens the conditional model and computes the difference with the normal conditional output as follow: \n\nwhere s is the guidance weight, and \u03b5 represents a model that is intentionally weakened or perturbed, achieved through various heuristic methods. For instance, AG [24] uses a flawed model variant, PAG [1] replaces self-attention weights with an identity matrix, SEG [20] blurs attention weights, Time Step Gudiance (TSG) [46] perturbs timestep embeddings, and SelfGuidance [29] alters noise levels. While effective, these approaches lack a clear theoretical foundation and have limitations: 1) they require specific layer identification, 2) increase computational cost with added NFEs, and 3) are incompatible with step-distilled models. Our method overcomes all of these limitations.",
            "score": 0.5820855332566344,
            "section_title": "Guidance Sampling in Diffusion Models",
            "char_start_offset": 6334,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1058
                },
                {
                    "start": 1061,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1745
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 135,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 502,
                    "end": 505,
                    "matchedPaperCorpusId": "268692048"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "252683688"
                },
                {
                    "start": 1262,
                    "end": 1265,
                    "matchedPaperCorpusId": "268692048"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8212890625
        },
        {
            "corpus_id": "273142392",
            "title": "Opportunities and challenges of diffusion models for generative AI",
            "text": "Here, the raw image can also represent an image prompt for more diverse editing task s. Brook s et al. [51 ] adopted the classifier-free guidance method again to train an image editing diffusion model from scratch on a massive synthetic data set. Training from scratch, however, often requires heavy computational resources. It is desired to maintain a pre-trained textto-image diffusion model and efficiently adapt it to editing tasks. Works in this direction include [5 4 -56 ], w hich advocate the use of an external cross attention mechanism for aligning the generated images with prompts. In all these applications, conditional diffusion models are shown to be expressive and effective in modeling conditional distributions [10 ].",
            "score": 0.581706523194563,
            "section_title": "Vision and audio generation",
            "char_start_offset": 20680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 735
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 108,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 729,
                    "end": 734,
                    "matchedPaperCorpusId": "227209335"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62060546875
        },
        {
            "corpus_id": "259203172",
            "title": "Conditional Text Image Generation with Diffusion Models",
            "text": "Recently, a category of deep generative models, named diffusion models [59], has achieved impressive results in computer vision tasks, outperforming GAN-based methods in the diversity of generated images [15]. Inspired by the non-equilibrium thermodynamics theory [59], Ho et al. [23] learned to model the Markov transition from noise to data distribution, enabling unconditional image generation. Luhman et al. [42] focused on online handwriting and generated point sequences based on diffusion models. Prafulla et al. [15] introduced additional classifiers to provide conditions for the diffusion models. Nichol et al. [49] explored diffusion models for text-conditional image synthesis with classifier-free guidance. Ramesh et al. [53] proposed a twostage model, i.e., a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding.\n\nUnlike general image generation, text image generation requires more unique contexts and textural features at the character-level. This inspires us to use a text recognizer to obtain conditions related to text images.",
            "score": 0.5813879599100913,
            "section_title": "Diffusion Models",
            "char_start_offset": 7668,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 75,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 204,
                    "end": 208,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 264,
                    "end": 268,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 280,
                    "end": 284,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 520,
                    "end": 524,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 621,
                    "end": 625,
                    "matchedPaperCorpusId": "245335086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8349609375
        },
        {
            "corpus_id": "272832564",
            "title": "TFG: Unified Training-Free Guidance for Diffusion Models",
            "text": "Given an unconditional diffusion model and a predictor for a target property of interest (e.g., a classifier), the goal of training-free guidance is to generate samples with desirable target properties without additional training. Existing methods, though effective in various individual applications, often lack theoretical grounding and rigorous testing on extensive benchmarks. As a result, they could even fail on simple tasks, and applying them to a new problem becomes unavoidably difficult. This paper introduces a novel algorithmic framework encompassing existing methods as special cases, unifying the study of training-free guidance into the analysis of an algorithm-agnostic design space. Via theoretical and empirical investigation, we propose an efficient and effective hyper-parameter searching strategy that can be readily applied to any downstream task. We systematically benchmark across 7 diffusion models on 16 tasks with 40 targets, and improve performance by 8.5% on average. Our framework and benchmark offer a solid foundation for conditional generation in a training-free manner.",
            "score": 0.5802997194727496,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62841796875
        },
        {
            "corpus_id": "273970225",
            "title": "Unraveling the Connections between Flow Matching and Diffusion Probabilistic Models in Training-free Conditional Generation",
            "text": "Diffusion-based conditional generation tasks. Diffusion-based conditional generation tasks aim at leveraging the potential of the diffusion models [10,11,9] to generate the desired samples, which is an important challenge. Conditional generation has been widely applied in various ways. For example, the inverse problem [16,2] uses degenerated images as a condition to enhance its quality. The text-to image [17,18,19,20], text-to-video [21], and text-to-3D [22] generations use text prompts as a condition to generate the images, the videos, the 3D objects described by the text prompt, respectively. Depending on whether the condition generation model needs to be retrained, the conditional generation methods can be mainly divided into training-based and training-free methods. In this paper, we focus on the training-free methods. \n\nTraining-free methods [2,1,3,16,15] aim at leveraging pre-trained unconditional diffusion models. Since the unconditional diffusion models can be categorized into DPMs and FMs, the training-free conditional DPMs and the training-free conditional FMs are discussed. \n\nTraining-free conditional DPMs. These methods [2,1,3,23] use DPMs as the unconditional diffusion models, where DPMs model the sampling process [11,10,9] based on the score function [9]. One of the successful approaches is based on the posterior sampling proposed by DPS [2]. DPS indicated that the score function represents the correlation between marginal distribution, i.e., the target of the unconditional generation and the sampling process. Thus, DPS derived a conditional term to connect the unconditional generation and condition, leveraging this correction. Meanwhile, DPS further proposed an estimation method for the conditional term under linear inverse assumption [16]. Then, FreeDom [1] introduced the energy function to estimate the conditional term without the limitation of linear inverse assumption. MPGD [3] and DSG [7] introduced the manifold theory and sphere constraint to enhance the FreeDom, respectively. \n\nTraining-free conditional FMs.",
            "score": 0.58014532828266,
            "section_title": "Related Work",
            "char_start_offset": 4296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 834
                },
                {
                    "start": 837,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2032
                },
                {
                    "start": 2035,
                    "end": 2065
                }
            ],
            "ref_mentions": [
                {
                    "start": 324,
                    "end": 326,
                    "matchedPaperCorpusId": "252596252"
                },
                {
                    "start": 859,
                    "end": 862,
                    "matchedPaperCorpusId": "252596252"
                },
                {
                    "start": 864,
                    "end": 866,
                    "matchedPaperCorpusId": "265466093"
                },
                {
                    "start": 1150,
                    "end": 1153,
                    "matchedPaperCorpusId": "252596252"
                },
                {
                    "start": 1155,
                    "end": 1157,
                    "matchedPaperCorpusId": "265466093"
                },
                {
                    "start": 1374,
                    "end": 1377,
                    "matchedPaperCorpusId": "252596252"
                },
                {
                    "start": 1926,
                    "end": 1929,
                    "matchedPaperCorpusId": "265466093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06695556640625
        },
        {
            "corpus_id": "249240430",
            "title": "DiVAE: Photorealistic Images Synthesis with Denoising Diffusion Decoder",
            "text": "Guided Diffusion [9] find that samples from a class conditional diffusion model with a independent classifier guidance can be significantly improved. Classifier-Free Diffusion [19] propose classifier-free guidance that does not need to train a separate classifier model. \n\nOne stage and Two stage Image Synthesis Current visual generation work can be generalization into one-stage direct generation and two-stage generation [7,34,47]. Visual auto-regressive models, such as PixelCNN [41], PixelRNN [43], and Image Transformer, diffusion model ,such as DDPM [17] and Guided diffusion [9], performed visual synthesis in a \"pixel-by-pixel\" manner, optimization and inference often is with high computational cost. To mitigate the shortcomings of individual approaches, a lot of research combine the strengths of different methods. As Figure 2 shows, DALL-E [31], NUWA [45], VQ-Diffusion [14] and Latent Didffusion [33] first learn an encoder-decoder architecture, like VQ-GAN [12] and VQ-VAE [42], which can compress image to latent representation and faithfully reconstruct it back to image, in second stage, AR based model: DALL-E [31] and NUWA [45] sequentially predict image token depends on the condition, while diffusion based model, VQ-Diffusion [14] and Latent Didffusion [33] predict it with a gradual denoising process. \n\nOur work aims to propose an generality model to generate more detailed and photorealistic images to improve the reconstructing stage of multi stage image synthesis. The potential improvements of image embedding generating model is expecting for future works. \n\n\u2022 We propose DiVAE, a vae generation framework with a diffusion decoder, which can generate more photorealistic images and achieved state-of-the-art results on image reconstruction from embeddings. \n\n\u2022 We perform DiVAE with an Auto-regressive generator on Text-to-Image (T2I) synthesis tasks and generate high-quanlity and more detailed images.",
            "score": 0.5799142994731119,
            "section_title": "body",
            "char_start_offset": 1782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 270
                },
                {
                    "start": 273,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1326
                },
                {
                    "start": 1329,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1587
                },
                {
                    "start": 1590,
                    "end": 1787
                },
                {
                    "start": 1790,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 20,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 176,
                    "end": 180,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 427,
                    "end": 430,
                    "matchedPaperCorpusId": "226290226"
                },
                {
                    "start": 483,
                    "end": 487,
                    "matchedPaperCorpusId": "14989939"
                },
                {
                    "start": 498,
                    "end": 502,
                    "matchedPaperCorpusId": "8142135"
                },
                {
                    "start": 557,
                    "end": 561,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 583,
                    "end": 586,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 854,
                    "end": 858,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 973,
                    "end": 977,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 1130,
                    "end": 1134,
                    "matchedPaperCorpusId": "232035663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85302734375
        },
        {
            "corpus_id": "254564168",
            "title": "Towards Practical Plug-and-Play Diffusion Models",
            "text": "Recently, diffusion-based generative models [49] have shown great success in various domains, including image generation [14,44,45], text-to-speech [21,40], and text * Co-first autor. \u2020 Corresponding author. Figure 1. Overview of our framework. Practical Plug-And-Play (PPAP) enables the diffusion model to be guided by leveraging off-the-shelf models. Images shown below are generated by guiding the unconditional GLIDE [37] with DeepLabV3 [4], ResNet50 [15], and MiDaS [43] in a plug-and-play manner.\n\ngeneration [32]. Specifically, for image generation, recent works have shown that diffusion models are capable of generating high-quality images comparable to those generated by GANs [8,12], while not suffering from mode collapse or training instabilities [38].\n\nIn addition to these advantages, their formulation allows the external model guidance [8,49,53], which guides the generation process of diffusion models towards the desired condition. Since guided diffusion leverages external guidance models and does not require further fine-tuning of the diffusion model, it holds the potential for cheap and controllable generation in a plug-and-play manner. For example, previous approaches use an image classifier for classconditional image generation [8,53], a fashion understanding model for fashion image editing [28], and a vision-language model for text-based image generation [1,37]. From these, if the publicly available off-the-shelf model can be used for guidance, one can easily apply one diffusion to various generation tasks.\n\nFor this purpose, an existing practice is to fine-tune the external off-the-shelf model on a noisy version of the training dataset [8,12], to adapt the model on the noisy latent images encountered during the diffusion process. However, we argue that such a practice has two challenges for plugand-play generation: (1) A single guidance model is insufficient to make predictions on inputs corrupted with varying degrees of noise, namely a too difficult task; and (2) It requires a labeled training dataset, which becomes a major hurdle whenever leveraging the off-the-shelf model.\n\nIn this paper, we first investigate the",
            "score": 0.5795533605130533,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 48,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 121,
                    "end": 125,
                    "matchedPaperCorpusId": "244714856"
                },
                {
                    "start": 125,
                    "end": 128,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 152,
                    "end": 155,
                    "matchedPaperCorpusId": "234483016"
                },
                {
                    "start": 455,
                    "end": 459,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 471,
                    "end": 475,
                    "matchedPaperCorpusId": "195776274"
                },
                {
                    "start": 687,
                    "end": 690,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 760,
                    "end": 764,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 853,
                    "end": 856,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 856,
                    "end": 859,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 1257,
                    "end": 1260,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1387,
                    "end": 1390,
                    "matchedPaperCorpusId": "244714366"
                },
                {
                    "start": 1675,
                    "end": 1678,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19482421875
        },
        {
            "corpus_id": "265351810",
            "title": "AnimateAnything: Fine-Grained Open Domain Image Animation with Motion Guidance",
            "text": "The evolution of image generation research has transitioned from traditional frameworks such as Generative Adversarial Networks (GANs) [13], Variational Autoencoders (VAEs) [27], and autoregressive transformer models (ARMs) [6], to the more recent diffusion models (DMs) [15]. This shift is attributed to the stability, superior sample quality, and conditional generation capabilities of DMs. DALLE-2 [42] represents a significant advancement by integrating the CLIP model [41] for text-image feature alignment, enabling text-prompted image synthesis. GLIDE [37] introduces classifier-free guidance to refine image quality, while Imagen [45] utilizes a sequence of diffusion models for high-resolution image creation. Latent Diffusion Models (LDMs) [42] employ an autoencoder [10] to manage the diffusion process in latent space, enhancing efficiency. Subsequent models like DiTs [39] and SDXL [40] further concentrate on latent space manipulation. For conditional generation, T2I-Adapter [35] and ControlNet [63] have been developed to integrate spatial conditions such as depth maps and sketches into LDMs. Composer [22] extends this by training LDMs with multiple conditions for more precise control. Building on these developments, our work introduces motion area and motion strength guidance to provide fine-grained control of image animation.",
            "score": 0.578768930076022,
            "section_title": "Image generation with diffusion models",
            "char_start_offset": 7363,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1348
                }
            ],
            "ref_mentions": [
                {
                    "start": 224,
                    "end": 227,
                    "matchedPaperCorpusId": "219781060"
                },
                {
                    "start": 473,
                    "end": 477,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 637,
                    "end": 641,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 776,
                    "end": 780,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 880,
                    "end": 884,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 1009,
                    "end": 1013,
                    "matchedPaperCorpusId": "256827727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5791015625
        },
        {
            "corpus_id": "257427549",
            "title": "Cones: Concept Neurons in Diffusion Models for Customized Generation",
            "text": "Diffusion Models. \n\nDiffusion models (Ho et al., 2020;Dhariwal & Nichol, 2021;Rombach et al., 2022;Song et al., 2020) are parametric neural networks that learn image distributions by gradual denoising. To further explore the extensibility of diffusion models, many works have been devoted to diffusion-based conditional generation, which can be broadly classified into two categories. The first one is the approach known as classifier-guidance (Liu et al., 2023), which utilizes a classifier to promote the sampling process of the pre-trained unconditional model. Despite the low cost, the generation effect is less competitive. The second one is known as the classifier-free approach (Ho & Salimans, 2022), which directly collects a large amount of data pairs for joint optimization under the guarantee of conditional probabilistic derivation. This approach can yield stunningly detailed results but requires a huge amount of data and computation resources. Owing to advances in language (Radford et al., 2021) and cross-modal foundation models (Radford et al., 2021), much text-to-image work (Saharia et al., 2022;Ramesh et al., 2022;Nichol et al., 2021) with classifier-free techniques is beginning to emerge, facilitating explicit control on the corresponding semantics and style. \n\nHowever, the expressiveness of text is still limited, and more work wants to utilize additional, conditional information (e.g., reference image, grounding (Li et al., 2023) and sketch (Voynov et al., 2022)) to guide the global control further. \n\nText-to-Image Diffusion Model. A text-to-image diffusion model (Yu et al., 2022;Saharia et al., 2022;Rombach et al., 2022) x\u03b8 will guide this denoising procedure with a text prompt describing the image content. Typically, it is trained by denoising a noised image x t = \u03b1 t x + \u03c3 t as  (Rombach et al., 2022) as the default text-to-image diffusion model due to its state-of-the-art performance and easy availability.",
            "score": 0.5786826882826621,
            "section_title": "Preliminaries and Background",
            "char_start_offset": 4792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 20,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1284
                },
                {
                    "start": 1287,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1949
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 54,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 78,
                    "end": 99,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 444,
                    "end": 462,
                    "matchedPaperCorpusId": "245117331"
                },
                {
                    "start": 989,
                    "end": 1011,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1046,
                    "end": 1068,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1634,
                    "end": 1655,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "256358592",
            "title": "PLay: Parametrically Conditioned Layout Generation using Latent Diffusion",
            "text": "Recent advances in Diffusion Models (DMs) (Sohl-Dickstein et al., 2015) have shown promising results in image generation (Song et al., 2020;Ho et al., 2020;Dhariwal & Nichol, 2021), where classifier guidance (Dhariwal & Nichol, 2021) and classifier-free guidance (CFG) (Ho & Salimans, 2022) methods not only improve the generation quality but also enable the possibility of developing conditional diffusion models. Most recent works for textto-image generation use CFG, including GLIDE (Nichol et al., 2021), DALL\u2022E2 (Ramesh et al., 2022), Imagen (Saharia et al., 2022b), and LDMs (Rombach et al., 2022). In addition to text, LDMs explore other conditions such as bounding boxes and semantic maps. We also adopt CFG for the guideline conditions in PLay, as it does not require an extra classifier and leads to the generation of better results. \n\nApplying diffusion models outside of the image domain has also drawn attention from researchers, such as 3D-model (Lin et al., 2022), video (Ho et al., 2022), andmusic (Mittal et al., 2021) generation. (Mittal et al., 2021) converts discrete melody tokens into a continuous latent space, and trains the diffusion model in the latent space. This work inspires us to convert the discrete layout elements, composed by concatenating different types of tokens including their class and coordinates, to a continuous domain for the diffusion process. A concurrent work, (Strudel et al., 2022), applies the same idea for language generation. We also follow LDMs (Rombach et al., 2022) to add a small KLpenalty to ensure high layout reconstruction quality and avoid arbitrarily large variance in the latent space. \n\nSeveral recent works explore methods to further control the diffusion process.",
            "score": 0.5785847505186492,
            "section_title": "Diffusion Models",
            "char_start_offset": 11323,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1650
                },
                {
                    "start": 1653,
                    "end": 1731
                }
            ],
            "ref_mentions": [
                {
                    "start": 42,
                    "end": 71,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 140,
                    "end": 156,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 581,
                    "end": 603,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1500,
                    "end": 1522,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "274762839",
            "title": "Simple Guidance Mechanisms for Discrete Diffusion Models",
            "text": "Diffusion models (Sohl-Dickstein et al., 2015;Ho et al., 2020) gained widespread adoption in image generation and signal processing in part due to their high controllability via mechanisms such as classifier-based (Dhariwal & Nichol, 2021a) and classifier-free guidance (Nichol et al., 2021;Ho & Salimans, 2022). Tasks where guidance plays a key role include MRI denoising (Song & Ermon, 2019), 3D reconstruction (Poole et al., 2022;Gao et al., 2024), and conditional generation (Saharia et al., 2022;Gokaslan et al., 2024). \n\nHowever, applying controllable diffusion-based generation to tasks where the data is discrete (e.g., molecule design or text generation) presents challenges. First, standard diffusion models and their guidance mechanisms are not directly applicable, since they require taking gradients with respect to the data, and these are not defined in discrete settings. Second, popular discrete extensions of diffusion (Sahoo et al., 2024a;Shi et al., 2024) cannot perform multiple editing passes on generated tokens, hence are not ideal for controllable generation. Third, the performance of discrete diffusion models (measured by perplexity) lags behind autoregressive (AR) models, especially for classes of diffusion that are amenable to control, such as uniform noise (Austin et al., 2021;Lou et al., 2023). \n\nHere, we propose discrete diffusion models and guidance mechanisms that are effective at controllable generation and that address the above challenges. First, we provide straightforward and easy-to-implement adaptations of classifier-based and classifier-free guidance for discrete diffusion models. Second, we revisit uniform noise diffusion language models (UDLM), which undo random token perturbations and are particularly amenable to guidance, since they can repeatedly edit their samples (Austin et al., 2021) and thus correct errors. We address performance issues that plagued Relative to autoregressive models, which make local predictions one token at a time, discrete diffusion models denoise the entire sequence at every iteration, allowing for more guidable outputs.",
            "score": 0.576758575747484,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1328
                },
                {
                    "start": 1331,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 46,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 373,
                    "end": 393,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 501,
                    "end": 523,
                    "matchedPaperCorpusId": "272724289"
                },
                {
                    "start": 1289,
                    "end": 1310,
                    "matchedPaperCorpusId": "235755106"
                },
                {
                    "start": 1824,
                    "end": 1845,
                    "matchedPaperCorpusId": "235755106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71435546875
        },
        {
            "corpus_id": "269137408",
            "title": "Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models",
            "text": "To achieve conditional data generation with diffusion models, classifier-free guidance (Ho & Salimans, 2022) is a simple but effective approach. Intuitively, the classifier-free guidance method tries to let the posterior estimator \"know\" the label of the data it models. In latent space, it uses one neural network to receive z t and condition c to make estimations. We can denote the estimated noise given corresponding condition c for continuous features as \u03b5(z t , c), \n\nwhere \u03b3(z t , c) = \u03b5(z t , c) \u2212 \u03b5(z t ) is guidance of c and it is scaled by guidance weight w g . \n\nEquation ( 5) can be extended to support multivariate guidance. Especially in tabular synthesis, we want to generate samples conditioned on labels c and sensitive features s. In this case, we can let our neural network take z t , c, and s as inputs, resulting in: In the forward process, the input data point x is preprocessed into numerical part x num and categorical part x cat , and then passing through T steps of the diffusion kernel to get \n\nIn the reverse process, the posterior estimator iteratively denoises noisy input z T conditioning on an outcome c and N sensitive attributes s (1) , \u2022 \u2022 \u2022 , s (N ) . The estimated data point is x. Our model can generate fair synthetic data by leveraging sensitive guidance to ensure a balanced joint distribution of the target label and sensitive attributes. \n\nUsing the mechanism of multivariate classifier-free guidance introduced in Section 3.4, Equation (6) extends label-only conditioning in classifier-free guidance with multivariate feature-level conditioning. The term \u03b3(z t , c) is the guidance term for the target label, representing the adjustment to the noise estimate based on the label condition. The term \u03b3(z t , c, s) is the sensitive guidance term, incorporating adjustments based on the sensitive attribute s, and the label condition c here is used as a reference to restrict the influence of the sensitive guidance, ensuring fairness without distorting the primary label distribution.",
            "score": 0.5763602671292449,
            "section_title": "Multivariate Latent Guidance",
            "char_start_offset": 13534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1381
                },
                {
                    "start": 1384,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 2026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87841796875
        },
        {
            "corpus_id": "272525313",
            "title": "TextToucher: Fine-Grained Text-to-Touch Generation",
            "text": "Following the step three, the most conspicuous object is shoe. \n\nLLaVA LLaVA where \u03f5 \u03b8 (x t , t) is the predicted noise by diffusion model at time t. \n\nTo make image generation more controllable, conditional diffusion models incorporate additional inputs, such as textural descriptions (Rombach et al. 2022;Chen et al. 2023) and segmentation map (Zhang, Rao, and Agrawala 2023). Classifier-free guidance (Ho and Salimans 2022) aims to find a x that maximizes log p(c|x). With Bayes theorem, the model \u03b5\u03b8 (x t , t, c) can be modified as follows: \n\nwhere s represents the scale of the guidance and c = \u2205 indicates DPMs generate data with non-conditions. Since classifier-free guidance can significantly improve the quality of vision images, we also adopt the technique in tactile image generation.",
            "score": 0.5762010838089833,
            "section_title": "Users Users",
            "char_start_offset": 10403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 65,
                    "end": 149
                },
                {
                    "start": 152,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 544
                },
                {
                    "start": 547,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 795
                }
            ],
            "ref_mentions": [
                {
                    "start": 286,
                    "end": 307,
                    "matchedPaperCorpusId": "248097655"
                },
                {
                    "start": 307,
                    "end": 324,
                    "matchedPaperCorpusId": "263334265"
                },
                {
                    "start": 346,
                    "end": 377,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 404,
                    "end": 426,
                    "matchedPaperCorpusId": "8771709"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82568359375
        },
        {
            "corpus_id": "274965611",
            "title": "Dataset Augmentation by Mixing Visual Concepts",
            "text": "Classifier-free guidance: One of the key challenges in text-guided generation is the amplification of the effect induced by the conditioned text. To this end, Ho et al. [20] have presented the classifier-free guidance technique, where the denoising is also performed unconditionally, which is then extrapolated with the conditioned prediction. More formally, let \u2205 = \u03c8('') be the embedding of a null text and let w be the guidance scale parameter, then the classifierfree guidance prediction is defined by: \n\nwhere w = 7.5 is the default parameter for Stable Diffusion. We will now outline the procedure to create multiple embeddings by randomly mixing visual concepts from a limited set of embedded representations.",
            "score": 0.5758763397795322,
            "section_title": "Latent Diffusion Model (LDM):",
            "char_start_offset": 11376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 716
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65966796875
        },
        {
            "corpus_id": "273549917",
            "title": "Rectified Diffusion Guidance for Conditional Generation",
            "text": "DPMs and conditional generation. Diffusion probabilistic model (DPM) introduces a new scheme of generative modeling, formulated by forward diffusing and reverse denoising processes (Sohl-Dickstein et al., 2015;Ho et al., 2020;Song et al., 2020). It is trained by optimizing the variational lower bound. Benefiting from this breakthrough, DPM achieves high generation fidelity, and even beat GANs on image generation. Conditional generation (Choi et al., 2021;Huang et al., 2023) takes better advantage of intrinsic intricate knowledge of data distribution, making DPM easier to scale up and the most promising option for generative modeling. Among the literature, text-toimage generation injects the embedding of text prompts to DPM, faithfully demonstrating the text content (Podell et al., 2024;Chen et al., 2024;Esser et al., 2024). \n\nClassifier-Free Guidance. Classifier-Free Guidance (CFG) serves as the successor of Classifier Guidance (CG) (Dhariwal & Nichol, 2021), circumventing the usage of a classifier for noisy images. Both CFG and CG attempt to formulate the underlying distribution by concentrating more on condition influence, achieving better conditional fidelity. Despite great success in large-scale conditional generation, CFG faces a technical flaw that the guided distribution is not theoretically guaranteed to recover the ground-truth conditional distribution (Du et al., 2023;Karras et al., 2024a;Bradley & Nakkiran, 2024). There exists a shifting issue that the expectation of guided distribution is drifted away from the correct one. This phenomenon may harm the condition faithfulness, especially for extremely broad distribution (e.g., open-vocabulary synthesis).",
            "score": 0.575454958653109,
            "section_title": "RELATED WORK",
            "char_start_offset": 5288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1692
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 210,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 210,
                    "end": 226,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 440,
                    "end": 459,
                    "matchedPaperCorpusId": "236950721"
                },
                {
                    "start": 459,
                    "end": 478,
                    "matchedPaperCorpusId": "257038979"
                },
                {
                    "start": 947,
                    "end": 972,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1384,
                    "end": 1401,
                    "matchedPaperCorpusId": "257078922"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78466796875
        },
        {
            "corpus_id": "274234227",
            "title": "Noise Diffusion for Enhancing Semantic Faithfulness in Text-to-Image Synthesis",
            "text": "Early studies on text-to-image synthesis mainly focus on GANs [39,41,[44][45][46] and auto-regressive models [4,9,29,43]. More recently, diffusion models have outperformed these methods [8]. The concept of training a generative model by adding noise to data and learning the reverse process to restore the original data distribution is first proposed by [36]. DDPM [14] showcases the remarkable ability of diffusion models for unconditional image generation. Song et al. [38] extend this by presenting a stochastic differential equation (SDE) to model the forward and reverse processes, and derive an equivalent neural ordinary differential equation (ODE) that samples from the same distribution, enabling exact likelihood computation and improving sampling efficiency. To achieve photorealism in classconditional settings, Dhariwal and Nichol [8] enhance diffusion models with classifier guidance by training a classifier on noisy images and using its gradients to guide samples toward the target label. Ho and Salimans [13] introduce classifier-free guidance, which combines the score estimates from a jointly trained conditional and unconditional model. For generating images from free-form textual prompts, Nichol et al. [25] employ a text encoder to condition the denoising process on language descriptions, and demonstrate that text-guided diffusion models with classifier-free guidance can yield higher quality images. Meanwhile, numerous efforts have been made to address the issue of unfaithfulness in image generation using diffusion models. One research direction is to explore how to train more powerful diffusion models. Balaji et al. [2] observe the difference of the importance of the text condition in different denoising stages and propose training an ensemble of text-to-image diffusion models specialized for different synthesis stages. Ramesh et al. [30] enhance the output of diffusion models with prior CLIP image embeddings added to the timestep embedding of diffusion models and concatenated with the output of text encoder. Saharia et al. [33] enhance the alignment of textual and visual content with the language understanding of large language models (LLMs).",
            "score": 0.5754512632145746,
            "section_title": "Related Work",
            "char_start_offset": 5548,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 66,
                    "matchedPaperCorpusId": "247778989"
                },
                {
                    "start": 69,
                    "end": 73,
                    "matchedPaperCorpusId": "1277217"
                },
                {
                    "start": 73,
                    "end": 77,
                    "matchedPaperCorpusId": "231592822"
                },
                {
                    "start": 77,
                    "end": 81,
                    "matchedPaperCorpusId": "91183909"
                },
                {
                    "start": 112,
                    "end": 114,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 114,
                    "end": 117,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 354,
                    "end": 358,
                    "matchedPaperCorpusId": "14888175"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9169921875
        },
        {
            "corpus_id": "276094842",
            "title": "Compressed Image Generation with Denoising Diffusion Codebook Models",
            "text": "The task of text-conditional image generation can be solved using a conditional diffusion model, which, theoretically speaking, learns to sample from the posterior distribution p 0 (x 0 |y). In practice, however, using a conditional model directly typically yields low fidelity to the inputs. To address this limitation, CG can be used to improve this fidelity at the expense of sample quality and diversity (Dhariwal & Nichol, 2021). Classifier-Free Guidance (CFG) is used more often in practice, as it achieves the same tradeoff by mixing the conditional and unconditional scores during sampling (Ho & Salimans, 2021), thus eliminating the need for a classifier. Particularly, assuming we have access to both the conditional score s i (x i , y) := \u2207 xi log p i (x i |y) and the unconditional one s i (x i ), CFG proposes to modify the conditional score by \n\nwhere w, the CFG scale, is a hyper-parameter controlling the tradeoff between sample quality and diversity. \n\nHere, we introduce a new CFG method that allows generating compressed conditional samples using any pair of conditional and unconditional diffusion models, while controlling the tradeoff between generation quality and the fidelity to the inputs. \n\nNote that optimizing Eq. ( 51) is roughly equivalent to optimizing L P when x i is high dimensional (see App. C.2). As in App. C.5, we promote sample diversity by choosing k i from a randomly sampled subset of K < K indices at each step during the generation. We coin our method Compressed CFG (CCFG). \n\nWe implement our method using SD 2.1 trained on 768 \u00d7 768 images, adopting a DDPM noise schedule with T = 1000 diffusion steps, K = 64 fixed vectors in each codebook and K \u2208 {2, 3, 4, 6, 9}. We compare against the same diffusion model with standard DDPM sampling, using T = 1000 steps and w \u2208 {2, 5, 8, 11}.",
            "score": 0.5747896722599236,
            "section_title": "C.6. Compressed Classifier-Free Guidance",
            "char_start_offset": 38692,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1215
                },
                {
                    "start": 1218,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1519
                },
                {
                    "start": 1522,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1829
                }
            ],
            "ref_mentions": [
                {
                    "start": 408,
                    "end": 433,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 598,
                    "end": 619,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94775390625
        },
        {
            "corpus_id": "276422090",
            "title": "RecDreamer: Consistent Text-to-3D Generation via Uniform Score Distillation",
            "text": "A major challenge in text-to-image generation using diffusion models (Yu et al., 2024;Liu et al., 2024;Xu et al., 2024) is guiding the generative process to reflect the input text accurately. A widely adopted solution is classifier-free guidance (CFG, Ho & Salimans (2022)), which eliminates the need for external classifiers by training a unified model for both unconditional and conditional image generation. During inference, CFG achieves conditional generation by interpolating between the conditional and unconditional scores, effectively guiding the model to match the input text. This method has shown significant success in various text-to-image tasks (Balaji et al., 2022;Nichol et al., 2021;Ramesh et al., 2022). Models like DALL\u2022E 2 and Stable Diffusion (Rombach et al., 2022) have demonstrated exceptional capabilities in producing diverse and complex images, with promising extensions into text-to-3D generation. \n\nA.2 TEXT-TO-3D GENERATION \n\nRecent advances in text-to-3D generation can be broadly divided into two main approaches. The first approach focuses on directly learning 3D asset distributions from large-scale datasets such as Objaverse (Deitke et al., 2023). Notable models within this category include GET3D (Gao et al., 2022), Point-E (Nichol et al., 2022), Shap-E (Jun & Nichol, 2023), CLAY (Zhang et al., 2024c), and MeshGPT (Siddiqui et al., 2024), all of which leverage extensive 3D data to generate accurate 3D models. \n\nThe second approach relies on 2D priors (Jiang et al., 2023) for generating 3D models. Techniques like score distillation are foundational here, as exemplified by DreamFusion/SJC (Poole et al., 2022;Wang et al., 2023a) and ProlificDreamer (Wang et al., 2024b). \n\nBuilding on these baselines, researchers continue to improve visual quality.",
            "score": 0.5739854460958809,
            "section_title": "A.1 DIFFUSION MODELS FOR TEXT-TO-IMAGE GENERATION",
            "char_start_offset": 23503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1713
                },
                {
                    "start": 1716,
                    "end": 1792
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 86,
                    "matchedPaperCorpusId": "272722848"
                },
                {
                    "start": 86,
                    "end": 103,
                    "matchedPaperCorpusId": "268819764"
                },
                {
                    "start": 103,
                    "end": 119,
                    "matchedPaperCorpusId": "269625321"
                },
                {
                    "start": 1161,
                    "end": 1182,
                    "matchedPaperCorpusId": "254685588"
                },
                {
                    "start": 1234,
                    "end": 1252,
                    "matchedPaperCorpusId": "252438648"
                },
                {
                    "start": 1319,
                    "end": 1340,
                    "matchedPaperCorpusId": "270619933"
                },
                {
                    "start": 1354,
                    "end": 1377,
                    "matchedPaperCorpusId": "265457242"
                },
                {
                    "start": 1493,
                    "end": 1513,
                    "matchedPaperCorpusId": "265020797"
                },
                {
                    "start": 1652,
                    "end": 1671,
                    "matchedPaperCorpusId": "254125253"
                },
                {
                    "start": 1692,
                    "end": 1712,
                    "matchedPaperCorpusId": "258887357"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "272367426",
            "title": "Zero-Shot Paragraph-level Handwriting Imitation with Latent Diffusion Models",
            "text": "Another important part of diffusion models is the sampling for generating new latent representations, which, in our case, is based on additional conditioning. Recent developments have shifted towards adopting classifier-free guidance (Ho & Salimans, 2021) over its predecessor, classifier guidance (Dhariwal & Nichol, 2021). This shift is not merely a matter of preference but is substantiated by empirical evidence suggesting enhanced performance in generating conditioned outputs that closely mimic the desired attributes. Our preliminary experiments in handwriting imitation verify this trend, indicating a superior fidelity in reproducing handwriting styles when utilizing classifier-free guidance. \n\nThe basis for applying classifier-free guidance is a diffusion model which learns a conditional distribution p(x|c) and an unconditional distribution p(x) at the same time. We achieve this by replacing the conditioning information with an empty style image x empty and an empty string with a set probability p = 0.2 during training. That allows us to strengthen the conditioning information during sampling by leveraging the scaled difference between the conditional and unconditional distribution. Mathematically classifier-free guidance equates to: \n\nwhere s is the scaling parameter and c empty is modeled as a blank page for style input and an empty string as target text.",
            "score": 0.5739231616532698,
            "section_title": "Classifier-free Guidance",
            "char_start_offset": 12977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1255
                },
                {
                    "start": 1258,
                    "end": 1381
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "258832615",
            "title": "Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration",
            "text": "Recently, diffusion models have become a promising generative modeling framework, achieving stateof-the-art performance on image generation tasks [27][28][29]. GLIDE [25] studies diffusion models for the textconditional image synthesis by classifier-free guidance strategies. InstructPix2Pix [5] proposes a effective framework to edit images with human instructions, which opens up new opportunities for controllable image creation by Label word: dog.",
            "score": 0.5728973420273987,
            "section_title": "Text-to-Image Diffusion Models",
            "char_start_offset": 5128,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 451
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 154,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 154,
                    "end": 158,
                    "matchedPaperCorpusId": "248986576"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5888671875
        },
        {
            "corpus_id": "254044198",
            "title": "Investigating Prompt Engineering in Diffusion Models",
            "text": "The use of diffusion models for text guided image generation has become an important paradigm in the last year with the introduction of models such as GLIDE (Nichol et al. [2022]), DALL-E 2 (Ramesh et al. [2022]), Imagen (Saharia et al. [2022]) and Latent Diffusion (Rombach et al. [2022]). \n\nIn early work, Diffusion models were often conditioned on dataset such as Imagenet (Deng et al. [2009]). Recent models Ramesh et al. [2022], Saharia et al. [2022] have moved away from classifier guidance (Ho et al. [2020]) to guiding their output via the embeddings resulting from text input to language models such as CLIP (Radford et al. [2021]). \n\nThe Stable Diffusion model (released under a permissive license in August 2022) is a Latent Diffusion Model (Rombach et al. [2022]) trained on 512x512 images from the LAION Aesthetics dataset of 2 Billion images (a subset of the LAION-5B database Schuhmann et al. [2022]).",
            "score": 0.5727956458596668,
            "section_title": "Introduction & Related Works",
            "char_start_offset": 31,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 293,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 916
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 178,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 190,
                    "end": 211,
                    "matchedPaperCorpusId": "248097655"
                },
                {
                    "start": 266,
                    "end": 288,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 376,
                    "end": 395,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 412,
                    "end": 432,
                    "matchedPaperCorpusId": "248097655"
                },
                {
                    "start": 752,
                    "end": 774,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64501953125
        },
        {
            "corpus_id": "277321603",
            "title": "Unconditional Priors Matter! Improving Conditional Generation of Fine-Tuned Diffusion Models",
            "text": "Guidance in Diffusion Models. Classifier-Free Guidance (CFG) [28] has become the de facto guidance technique for conditional generation with diffusion models, leading to notable improvements in both condition alignment and image quality. However, recent research has highlighted some of its limitations. Kynk\u00e4\u00e4nniemi et al. [41] have shown that the specific timesteps at which CFG is applied significantly impact image diversity, and proposed to restrict CFG to certain intervals. \n\nAnother line of work [1,31] addresses the limited applicability of CFG for text-based conditions when using off-the-shelf diffusion models like Stable Diffusion [52]. These approaches introduce a guidance technique that extends to a broader range of generation tasks, including unconditional generation, inverse problems, and conditional generation with non-text conditions (e.g., depth maps [67]). Recently, Karras et al. [38] propose Autoguidance which uses the noise estimate from an under-trained version of itself, instead of unconditional noise, to resolve inherent issues of the entangled guidance for condition alignment and image quality. A more detailed discussion on autoguidance can be found in the Appendix. However, previous works have not explored how the dynamics of CFG shift when a diffusion model is fine-tuned for a specific task [7,46,64]. In this work, we address the critical issue of unconditional noise degradation that occurs during fine-tuning and pro-pose a novel solution by combining noise predictions from multiple diffusion models. \n\nMerging Diffusion Models. Aligned with the mixtureof-experts [8] and model merging [65] literature on foundation models, there is growing research on methods for merging diffusion models to enable effective composition of multiple conditions. Diffusion Soup [3] directly merges weights of different diffusion models, Mix-of-Show [25] combines the weights of LoRA adapters [32], and Max-Fusion [48] merges intermediate model features. Notably, leveraging the iterative denoising process of diffusion models, merging their noise estimates has emerged as a simple yet powerful technique for composing conditions.",
            "score": 0.5726929725454133,
            "section_title": "Related Works",
            "char_start_offset": 5639,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 29
                },
                {
                    "start": 30,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 65,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 504,
                    "end": 507,
                    "matchedPaperCorpusId": "268692048"
                },
                {
                    "start": 507,
                    "end": 510,
                    "matchedPaperCorpusId": "252683688"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 875,
                    "end": 879,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 1333,
                    "end": 1336,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 1336,
                    "end": 1339,
                    "matchedPaperCorpusId": "257631738"
                },
                {
                    "start": 1339,
                    "end": 1342,
                    "matchedPaperCorpusId": "253523371"
                },
                {
                    "start": 1807,
                    "end": 1810,
                    "matchedPaperCorpusId": "270391477"
                },
                {
                    "start": 1942,
                    "end": 1946,
                    "matchedPaperCorpusId": "269148434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89599609375
        },
        {
            "corpus_id": "278501742",
            "title": "StableMotion: Repurposing Diffusion-Based Image Priors for Motion Estimation",
            "text": "Classifier-free guidance. To control the content generated by the model and balance controllability with fidelity, classifier-free guidance (CFG) [9] incorporates conditions y as follows: \n\nCFG theoretically requires two diffusion outputs, one conditional and one unconditional. Joint training is usually applied by randomly setting the condition y to a null condition \u03d5 with some probability p y , and sampling process is a linear combination of conditional and unconditional predictions: \n\nwhere w is the parameter that balances fidelity and diversity. \n\nIn our work, we use full-condition guidance, which means p y and w is set to be zeros. \n\nLatent diffusion models. LDM [33] employs a VAE [18] to encode (E) and decode (D) images, and performs diffusion in the latent space instead of pixel space:",
            "score": 0.5726620256375633,
            "section_title": "Background",
            "char_start_offset": 9532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 187
                },
                {
                    "start": 190,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 802
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 149,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 694,
                    "end": 698,
                    "matchedPaperCorpusId": "211146177"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8115234375
        },
        {
            "corpus_id": "265033224",
            "title": "InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image",
            "text": "Classifier-free guidance trains an unconditional denoising diffusion model together with the conditional model instead of training a separate classifier (Ho & Salimans, 2021). Liu et al. (2022) show that composable conditional diffusion models can generate images containing all concepts conditioned on a set of concepts by composing score estimates. Following (Ho & Salimans, 2021;Liu et al., 2022;Brooks et al., 2023), we train a single network to parameterize the image-text conditional, the only-image conditional, and the unconditional model. We train the unconditional model simply by setting c I = \u2205, c T = \u2205 with probability p 1 , similarly, only setting c T = \u2205 with probability p 2 for the only-image-conditional model \u03f5 \u03b8 (w ot , c I , \u2205). In our paper, we set p 1 = 0.05, p 2 = 0.05 as hyperparameters. \n\nIn the inference phase, we add a little bit of noise to the latent of the input image (usually 15 steps) to obtain w ot and then use our model to perform conditional denoising. The model predicts three score estimates, the image-text conditional \u03f5 \u03b8 (w ot , c I , c T ), the only-image conditional \u03f5 \u03b8 (w ot , c I , \u2205), and the unconditional \u03f5 \u03b8 (w ot , \u2205, \u2205). c T = \u2205 indicates that the text takes an empty character. c I = \u2205 means that the concatenation w o takes zero and identity modulation takes zero. Image and text conditioning sampling can be performed as follows: \n\nwhere s I and s T are the guidance scales for alignment with the input image and the text instruction, respectively.",
            "score": 0.572417746832472,
            "section_title": "IMAGE AND TEXT CONDITIONING",
            "char_start_offset": 20084,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1389
                },
                {
                    "start": 1392,
                    "end": 1508
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 174,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 176,
                    "end": 193,
                    "matchedPaperCorpusId": "249375227"
                },
                {
                    "start": 361,
                    "end": 382,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 382,
                    "end": 399,
                    "matchedPaperCorpusId": "249375227"
                },
                {
                    "start": 399,
                    "end": 419,
                    "matchedPaperCorpusId": "253581213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85693359375
        },
        {
            "corpus_id": "270870339",
            "title": "Diffusion-BBO: Diffusion-Based Inverse Modeling for Online Black-Box Optimization",
            "text": "Forward methods build a surrogate model to approximate and optimize the black-box objective function. However, these approaches can struggle with capturing the data manifold within the design space and avoiding out-of-distribution and invalid inputs [Kumar and Levine, 2020]. Song et al. [2022], Zhang et al. [2021] proposed Likelihood-free BO using likelihood-free inference to extend BO to a broader class of models and utilities. It directly models the acquisition function without separately performing inference with a surrogate model. However, there is a risk where the acquisition function is over-confident. Our work builds upon recent progress in inverse approaches for offline BBO, which utilize diffusion modeling to better learn the data manifold within the design space [Krishnamoorthy et al., 2023, Kong et al., 2024]. But we focus solely on online BBO setting by introducing a sample-efficient inverse modeling method using conditional diffusion models. \n\nDiffusion Models. As an emerging class of generative models with strong expressiveness, diffusion models [Sohl-Dickstein et al., 2015, Song et al., 2020] have been successfully deployed across various domains including image generation [Rombach et al., 2022], reinforcement learning [Wang et al., 2022], robotics [Chi et al., 2023], etc. Notably, through the formulation of stochastic differential equations (SDEs), [Song et al., 2020] provides a unified continuoustime score-based framework for distinctive classes of diffusion models. To steer the generation toward high-quality samples with desired properties, it is important to guide the backward data-generation process using task-specific information. Hence, different types of guidance are studied in prior works [Bansal et al., 2023, Nichol et al., 2021, Zhang et al., 2023], including classifier guidance [Dhariwal and Nichol, 2021] where the classifier is trained externally, and classifier-free guidance [Ho and Salimans, 2022], in which the classifier is implicitly specified. In this work, we employ classifier-free guidance to eliminate the requirement of training a separate classifier model, thereby enabling feasible uncertainty quantification in conditional diffusion models. \n\nUncertainty Quantification.",
            "score": 0.5719680692111253,
            "section_title": "Introduction",
            "char_start_offset": 1745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2215
                },
                {
                    "start": 2218,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 274,
                    "matchedPaperCorpusId": "209515681"
                },
                {
                    "start": 276,
                    "end": 294,
                    "matchedPaperCorpusId": "250072514"
                },
                {
                    "start": 783,
                    "end": 811,
                    "matchedPaperCorpusId": "259138671"
                },
                {
                    "start": 1076,
                    "end": 1104,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 1207,
                    "end": 1229,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1742,
                    "end": 1762,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 1836,
                    "end": 1863,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77880859375
        },
        {
            "corpus_id": "258059755",
            "title": "Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond",
            "text": "To generate photo-realistic images given text prompts, the diffusion models can further take advantage of classifier guidance [9] or classifier-free guidance [15] to improve the image quality. Especially, in the context of text-to-image generation, classifier-free guidance is more widely used, which is usually expressed as a linear interpolation between the conditional and unconditional prediction \u02c6 \u03b8 (x t , t, c) = (1 + \u03c4 ) \u03b8 (x t , t, c) \u2212 \u03c4 \u03b8 (x t , t) at each timestep t with a guidance scale parameter \u03c4 . \n\nWhen the prompt becomes complex, the model may fail to understand some key elements in the query prompt and create undesired images. To handle complex textual information, [21] proposes composing diffusion models to factorize the text prompts into a set of text prompts, i.e., c={c 1 , ...c n }, and model the conditional distribution as When c 1 and c 2 are conditional independent given x, the ratio R(c 1 , c 2 ) = p \u03b8 (c1,c2|x) p \u03b8 (c1|x)p \u03b8 (c2|x) = 1 and this term can be ignored. However, in practice, the input text prompts can barely be independent when we need to specify the desired attributes of the image, such as style, content, and their relations. When c 1 and c 2 have an overlap in their semantics, simply fusing the concepts could be harmful and result in undesired results, especially in the case of concept negation, as shown in Figure 2. In the second row of images, we can clearly observe the key concepts requested in the main text prompt (respectively \"armchair\", \"sunglasses\", \"crown\", and \"horse\") are removed when those concepts appear in the negative prompts. This important observation motivates us to rethink the concept composing process and propose the use of a perpendicular gradient in the sampling, which is described in the following section.",
            "score": 0.5706729000809201,
            "section_title": "Preliminary",
            "char_start_offset": 5810,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 514
                },
                {
                    "start": 517,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1796
                }
            ],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 129,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 689,
                    "end": 693,
                    "matchedPaperCorpusId": "249375227"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9296875
        },
        {
            "corpus_id": "270199289",
            "title": "Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling",
            "text": "Classifier-free Guidance An intriguing property of conditional diffusion models is that we can easily guide the iterative sampling process for better sampling quality.For instance, Ho and Salimans [2021] introduced Classifier-free Guidance (CFG), which utilizes the diffusion model itself to perform guidance at test time.More specifically, we perform sampling using the following linear combination:\n\nwhere \u03b3 is the guidance weight, and x \u03b8 (x t ) = x \u03b8 (x t , c = \u2205) is the unconditional denoising output.\n\nDuring training, we drop the condition c with certain probability p uncond to facilitate unconditional prediction.When \u03b3 > 1, CFG takes effect and amplifies the difference between conditional and unconditional generation, leading to a global control of high-quality generation.\n\nCompared to autoregressive models, diffusion models are more flexible in adjusting sample steps, allowing for the utilization of noise schedules to learn different frequencies.Additionally, with the use of CFG, diffusion models can achieve higher quality images with much fewer parameters than autoregressive models.However, it's notable that CFG can significantly impact the diversity of the diffusion output, which motivates us to revisit the basics and combine the strengths of both.",
            "score": 0.5704567680115183,
            "section_title": "Preliminaries",
            "char_start_offset": 7014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 167,
                    "end": 322
                },
                {
                    "start": 322,
                    "end": 400
                },
                {
                    "start": 402,
                    "end": 507
                },
                {
                    "start": 509,
                    "end": 623
                },
                {
                    "start": 623,
                    "end": 786
                },
                {
                    "start": 788,
                    "end": 964
                },
                {
                    "start": 964,
                    "end": 1104
                },
                {
                    "start": 1104,
                    "end": 1274
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 203,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "268513112",
            "title": "THOR: Text to Human-Object Interaction Diffusion via Relation Intervention",
            "text": "Following [30,58], we predict denoised interactions x 0 with network G \u03b8 in each step and noise it back to x t\u22121 iteratively following the Markov chain.Denoting x0 = G \u03b8 (x t , t, c), the reverse process can be formulated as\n\nwhere \u03c3 2 is a fixed variance.\n\nClassifier-free guidance Previous diffusion models [11,33,48,58] achieve classifier-free guidance by the linear combination of unconditional generation and conditional generation.However, in the context of Text2HOI generation, object shape has to serve as an essential condition modality.This is mainly because, even within the same class, objects with different shapes possess distinguishable motion spaces.This distinction may arise from variations in size or geometric structure, impacting how objects move or interact.Consequently, the classifier-free guidance is reformulated as: where s is the guidance scale.With this modification, THOR maintains a consistent awareness of the object's shape, striking a balance between fidelity and diversity guided by the provided text prompt.\n\nIn each diffusion step t, the noisy interactions x t is linearly projected, and concatenated with the condition c.We use a transformer encoder with M layers to predict the denoised x0 with positional encoding.This builds a general Text2HOI generation model if directly go to next step.However, such a simple diffusion model tends to learn the entire distribution of input data, while lacking the nuanced awareness regarding interactions.Our model, THOR, addresses this limitation by introducing human object relation intervention as in the next subsection.",
            "score": 0.5687423762528587,
            "section_title": "Text2HOI Diffusion Framework",
            "char_start_offset": 11969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 152,
                    "end": 224
                },
                {
                    "start": 226,
                    "end": 256
                },
                {
                    "start": 258,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 546
                },
                {
                    "start": 546,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 780
                },
                {
                    "start": 780,
                    "end": 873
                },
                {
                    "start": 873,
                    "end": 1043
                },
                {
                    "start": 1045,
                    "end": 1159
                },
                {
                    "start": 1159,
                    "end": 1254
                },
                {
                    "start": 1254,
                    "end": 1330
                },
                {
                    "start": 1330,
                    "end": 1482
                },
                {
                    "start": 1482,
                    "end": 1601
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 14,
                    "matchedPaperCorpusId": "254535555"
                },
                {
                    "start": 14,
                    "end": 17,
                    "matchedPaperCorpusId": "252595883"
                },
                {
                    "start": 309,
                    "end": 313,
                    "matchedPaperCorpusId": "254408910"
                },
                {
                    "start": 319,
                    "end": 322,
                    "matchedPaperCorpusId": "252595883"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75048828125
        },
        {
            "corpus_id": "258823370",
            "title": "DiffuSIA: A Spiral Interaction Architecture for Encoder-Decoder Text Diffusion",
            "text": "Diffusion-LM (Li et al., 2022) proposes continuous diffusion on the embedding space for text generation. In the forward process, an embedding step is designed to introduce a Markov transition from discrete words w to x 0 that is parametrized by q \u03c6 (x 0 |w) = N (EMB(w), \u03c3 0 I). In the reverse process, a trainable rounding step is added and parametrized by p \u03b8 (w 1), the training objective is modified as: \n\n(2) \n\nClassifier-free Guidance Extending the guidance method proposed by Dhariwal and Nichol (2021), semantic diffusion guidance (SDG) (Liu et al., 2021) allows fine-grained and continuous control of model class, including either language or image guidance, or both. Furthermore, a classifierfree guidance method is proposed that is more effective at controlling generation (Ho and Salimans, 2022;Ramesh et al., 2022). Let unconditional denoising diffusion model p \u03b8 (x) be parameterized through a score estimator \u03b8 (x t , t) and the conditional model p \u03b8 (x|c) be parameterized through \u03b8 (x t , t, c). These two models can be learned via a single neural network. Precisely, a conditional diffusion model p \u03b8 (x|c) is trained on paired data (x, c), where the conditioning information c is discarded periodically and randomly, so that the model knows how to generate unconditionally as well, i.e. \u03b8 (x t , t) = \u03b8 (x t , t, c = \u2205). \n\nIn this paper, we focus on the sequence-tosequence text generation tasks which produce a target sequence w x = {w x 1 , ..., w x n } conditioning on the source sequence w c = {w c 1 , ..., w c m }. Different from Ho and Salimans (2022), conditional information is involved all the time and not discarded, which has been proved effective in Gong et al. (2022). Thus the training objective becomes: \n\n(3)",
            "score": 0.5685891993992758,
            "section_title": "Continuous Diffusion on Embedding Space",
            "char_start_offset": 9408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 407
                },
                {
                    "start": 410,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1738
                },
                {
                    "start": 1741,
                    "end": 1744
                }
            ],
            "ref_mentions": [
                {
                    "start": 483,
                    "end": 509,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89697265625
        },
        {
            "corpus_id": "258556958",
            "title": "Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models",
            "text": "Our editing method is built upon text-guided diffusion models. In Stable Diffusion, the text P is fed into a pre-trained CLIP [34] text encoder \u03c4 \u03b8 to obtain its corresponding embedding and the underlying UNet model is augmented with the cross attention mechanism, which is effective for generating visual contents conditioned on the text P. One of the key challenges in this kind of generation models is the amplification of the effect induced by the conditional text. To this end, the classifier-free guidance technique is proposed, where the prediction for each step is a combination of conditional and unconditional predictions. Formally, let c = \u03c4 \u03b8 (P) be the conditional embedding vector and \u2205 = \u03c4 \u03b8 (\"\") be the unconditional one, the classifierfree guidance prediction is calculated by: \n\nwhere \u03c9 is the guidance scale parameter.",
            "score": 0.5679428144730864,
            "section_title": "Background and preliminaries",
            "char_start_offset": 13763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 837
                }
            ],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 130,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93017578125
        },
        {
            "corpus_id": "251979380",
            "title": "FLAME: Free-form Language-based Motion Synthesis & Editing",
            "text": "Diffusion models (Ho, Jain, and Abbeel 2020) are recently proposed generative models that are shown to be good at synthesizing highly-complex image datasets. Compared to GANs (Goodfellow et al. 2014;Karras, Laine, and Aila 2019;Karras et al. 2020;Sauer, Schwarz, and Geiger 2022) and VAEs (Kingma and Welling 2013;Van Den Oord, Vinyals et al. 2017), they have been presenting improved quality in generating multi-modal outputs, advantageous to text-to-image generation and text-to-motion generation of our interest-there can be various modes of images/motions corresponding to a single text description. Diffusion models are originally proposed in Sohl-Dickstein et al. (2015) and developed in Ho, Jain, and Abbeel (2020) and Song, Meng, and Ermon (2020), showing high-quality image generation. After, they are extended to work on conditional generation settings, demonstrating even better performances. Class-conditional models are studied in Dhariwal and Nichol (2021), and text-conditional models are proposed by adapting the conditioning scheme for text (GLIDE; (Nichol et al. 2021)). unCLIP1 (Ramesh et al. 2022) and Imagen (Saharia et al. 2022) further show that conditioning on pre-trained high-level embedding gives improved result. Our model shares some similarity with the GLIDE model, but has crucial differences in that it has a new design to handle temporal sequences of variable length. \n\nTo achieve the best performance, several techniques have been proposed and applied to the aforementioned models. Improved DDPM (Nichol and Dhariwal 2021) suggests learning the reverse-diffusion variances. Classifier-free guidance (Ho and Salimans 2021) has been introduced to enable conditional generation without the need for a separate classifier model. We employ both of these techniques in the proposed model.",
            "score": 0.5676275817578582,
            "section_title": "Related Work Diffusion Models & Text-conditional Generation",
            "char_start_offset": 3925,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1816
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 44,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 175,
                    "end": 199,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 199,
                    "end": 228,
                    "matchedPaperCorpusId": "54482423"
                },
                {
                    "start": 228,
                    "end": 247,
                    "matchedPaperCorpusId": "209202273"
                },
                {
                    "start": 247,
                    "end": 279,
                    "matchedPaperCorpusId": "246441861"
                },
                {
                    "start": 648,
                    "end": 676,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 694,
                    "end": 721,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 944,
                    "end": 970,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1066,
                    "end": 1086,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 1530,
                    "end": 1556,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 1633,
                    "end": 1655,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69580078125
        },
        {
            "corpus_id": "254823541",
            "title": "SADM: Sequence-Aware Diffusion Model for Longitudinal Medical Image Generation",
            "text": "There are two branches of conditioning methods for diffusion models: 1) Classifier guided [5]; and 2) Classifier-free guidance [9]. However, it is often hard to define the problem setting for a classifier in the medical domain, and even the state-ofthe-art classifiers often do not have the performance suitable for classifier-guided models. Thus, we opt to use classifier-free guidance for conditioning the diffusion model. The classifier-free guided diffusion model takes the conditioning signal c as an additional input, and is defined as \n\nwhich is the weighted sum of the model with condition c and model with zero tensor \u2205, i.e., unconditional model. The guidance strength w controls the tradeoff between sample quality and diversity, i.e., the higher the guidance strength the lower the diversity. Eq. ( 4) can also be performed in -space \u02dc \u03b8 (z t , c, \n\nDuring training, we can randomly replace the conditioning signals c by a zero tensor with probability p uncond . Then, the noise-prediction loss term [8] for the reverse process conditional generative model is: \n\nwhere \u00ce \u223c Be(p uncond ) is either a zero or an identity tensor sampled from a Bernoulli distribution, z t = \u03b1 t x+\u03c3 t , and",
            "score": 0.5663822777086901,
            "section_title": "Classifier-free Guidance",
            "char_start_offset": 5389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1072
                },
                {
                    "start": 1075,
                    "end": 1198
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1012,
                    "end": 1015,
                    "matchedPaperCorpusId": "219955663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69775390625
        },
        {
            "corpus_id": "274149817",
            "title": "Decoupling Training-Free Guided Diffusion by ADMM",
            "text": "Specifically, we decouple these two components by representing the samples from the diffusion model as x and introducing an auxiliary variable z that represents samples refined by the guidance function. This formulation allows us to transform the conditional generation problem into two more tractable subproblems, connected by the constraint x = z. Within this setup, we introduce a dual variable and develop a new algorithm based on the Alternating Direction Method of Multipliers (ADMM) [16,17] for conditional generation. Our algorithm sidesteps the use of the weight hyperparameter and allows for a more natural and adaptive balancing between the diffusion model and the guidance function. Additionally, we theoretically demonstrate that the proximal operator of the diffusion term \u2212 log p(x) in the ADMM can be effectively approximated by the diffusion reverse process and further provide a rigorous proof of the convergence of our algorithm under generic assumptions regarding the distributions p(x) and p(y|z). \n\nWe evaluate our proposed framework on various conditional generation tasks in different domains (see Figure 1 for illustration). In image synthesis tasks, our method can accept both non-linear semantic parsing models and linear measurement functions as guidance conditions, and achieves superior performance in terms of both image quality and condition satisfaction. In addition, our method is capable of generalizing in motion domain and guide text-condition motion diffusion models to follow specific trajectories.",
            "score": 0.5662208835359793,
            "section_title": "Introduction",
            "char_start_offset": 3358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 1018
                },
                {
                    "start": 1021,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1537
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6591796875
        },
        {
            "corpus_id": "270869763",
            "title": "Diffusion Models and Representation Learning: A Survey",
            "text": "Recent improvements in image generation results have largely been driven by improved guidance approaches.The ability to control generation by passing user-defined conditions is an important property of generative models, and guidance describes the modulation of the strength of the conditioning signal within the model.Conditioning signals can have a wide range of modalities, ranging from class labels, to text embeddings to other images.A simple method to pass spatial conditioning signals to diffusion models is to simply concatenate the conditioning signal with the denoising targets and then pass the signal through the denoising network [12,75].Another effective approach uses cross-attention mechanisms, where a conditioning signal c is preprocessed by an encoder to an intermediate projection E(c), and then injected into the intermediate layer of the denoising network using cross-attention [76,142].These conditioning approaches alone do not leave the possibility to regulate the strength of the conditioning signal within the model.Diffusion model guidance has recently emerged as an approach to more precisely trade-off generation quality and diversity.\n\nDhariwal and Nichol [42] use classifier guidance, a compute-efficient method leveraging a pre-trained noiserobust classifier to improve sample quality.Classifier guidance is based on the observation that a pre-trained diffusion model can be conditioned using the gradients of a classifier parametrized by \u03d5 outputting p \u03d5 (c|x t , t).The gradients of the log-likelihood of this classifier \u2207 xt log p \u03d5 (c|x t , t) can be used to guide the diffusion process towards generating an image belonging to class label y.The score estimator for p(x|c) can be written as\n\n(11) By using Bayes' theorem, the noise prediction network can then be rewritten to estimate:\n\nwhere the parameter w modulates the strength of the conditioning signal.Classifier guidance is a versatile approach that increases sample quality, but it is heavily reliant on the availability of a noise-robust pre-trained classifier, which in turn relies on the availability of annotated data, which is not available in many applications.\n\nTo address this limitation, Classifier-free guidance (CFG) [67] eliminates the need for a pre-trained classifier.",
            "score": 0.5661990285037293,
            "section_title": "Diffusion Model Guidance",
            "char_start_offset": 15445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 105,
                    "end": 319
                },
                {
                    "start": 319,
                    "end": 439
                },
                {
                    "start": 439,
                    "end": 651
                },
                {
                    "start": 651,
                    "end": 909
                },
                {
                    "start": 909,
                    "end": 1043
                },
                {
                    "start": 1043,
                    "end": 1165
                },
                {
                    "start": 1167,
                    "end": 1318
                },
                {
                    "start": 1318,
                    "end": 1501
                },
                {
                    "start": 1501,
                    "end": 1679
                },
                {
                    "start": 1679,
                    "end": 1727
                },
                {
                    "start": 1729,
                    "end": 1822
                },
                {
                    "start": 1824,
                    "end": 1896
                },
                {
                    "start": 1896,
                    "end": 2163
                },
                {
                    "start": 2165,
                    "end": 2278
                }
            ],
            "ref_mentions": [
                {
                    "start": 643,
                    "end": 647,
                    "matchedPaperCorpusId": "253581703"
                },
                {
                    "start": 647,
                    "end": 650,
                    "matchedPaperCorpusId": "252846258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85107421875
        },
        {
            "corpus_id": "257757015",
            "title": "High Fidelity Image Synthesis With Deep VAEs In Latent Space",
            "text": "In addition to learning on lower dimensional representations, large-scale diffusion and autoregressive models employ classifier-free guidance (Ho & Salimans, 2022) to boost image fidelity (Nichol et al., 2021;Saharia et al., 2022b;Gafni et al., 2022;Yu et al., 2022). This technique is designed to trade off diversity for sample quality, since poor likelihood-based models tend to cover regions not present in the data distribution. Guidance pushes samples towards regions that more closely resemble a desired label by comparing conditional and unconditional likelihood functions. Luhman & Luhman (2022) extends this technique to hierarchical VAEs where means and log-variances are extrapolated towards class-conditional values and away from unconditional ones. This VAE guidance strategy considers diagonal Gaussian priors p(z i |z <i ) conditioned only on previous latents, and priors p(z i |z <i , c) conditioned on both previous latents and an auxiliary label c. The guided latent variables are drawn from p guided,w\u00b5,w\u03c3 (z i |z <i , c) defined as: \n\nfor conditional means and covariances \u00b5 c , diag \u03c3 2 c and unconditional ones \u00b5 u , diag \u03c3 2 u . The strength of the mean and variance guidance are controlled by w \u00b5 and w \u03c3 respectively, which may be chosen independently. \n\nIntuitively, mean guidance accentuates components that are present in the class-conditional value but not in the unconditional one. This helps give generated images a distinct visual structure that is characteristic of other images in the class. Variance guidance acts similarly to low-temperature sampling, adaptively adjusting the temperature based on how the ratio of class-conditional variance compares to the unconditional one. \n\nTo implement guidance in VAEs, we train a model where all priors are conditioned on a label that is replaced with a dummy label 10% of the time. At sampling time, we store running hidden states for both the conditional and unconditional generation paths.",
            "score": 0.5644347218119157,
            "section_title": "Classifier-Free Guidance",
            "char_start_offset": 12864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1969
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8134765625
        },
        {
            "corpus_id": "269293130",
            "title": "Concept Arithmetics for Circumventing Concept Inhibition in Diffusion Models",
            "text": "Diffusion Model is a type of generative model that employs a gradual denoising process to learn the distribution p(x) of the data [5,12,20,31,32].The diffusion model generates an image x 0 in T steps by iteratively predicting and removing noise starting from the initial Gaussian noise sample x T .Noise prediction is learned to optimize the score function \u2207 x log p(x).\n\nClassifier guidance [5,31,33] enables generation conditioned on some input c by adding a conditional score term \u03b3\u2207 x log p(c | x) with guidance scale \u03b3 > 1 controlling the influence of the conditional signal.p(c | x) can be an external image classifier model predicting the class label c.Classifier-free guidance [12] proposes to train the model jointly on conditional and unconditional denoising to obtain a single neural network that models both unconditional p(x) and conditional p(c | x) distributions.In this case, the total guidance can be expressed as\n\nor in terms of the learned U-Net model \u03f5 \u03b8 that predicts the noise to be removed from x t at timestep t and conditioned on prompt c 12 :\n\nLatent Diffusion Models [26] incorporate encoder E and decoder D before and after the diffusion process, respectively.Moving the gradual denoising from image pixel space to lower dimensional encoder-decoder latent space improves convergence and running speeds.",
            "score": 0.5642348481219048,
            "section_title": "Diffusion Models",
            "char_start_offset": 5926,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 146,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 370
                },
                {
                    "start": 372,
                    "end": 580
                },
                {
                    "start": 580,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 878
                },
                {
                    "start": 878,
                    "end": 930
                },
                {
                    "start": 932,
                    "end": 1068
                },
                {
                    "start": 1070,
                    "end": 1188
                },
                {
                    "start": 1188,
                    "end": 1330
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 133,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 133,
                    "end": 136,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 136,
                    "end": 139,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 139,
                    "end": 142,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 142,
                    "end": 145,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 392,
                    "end": 395,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 395,
                    "end": 398,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 685,
                    "end": 689,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 1094,
                    "end": 1098,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82177734375
        },
        {
            "corpus_id": "254409018",
            "title": "Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors",
            "text": "The diffusion model [19,49,50], which is a likelihood-based model consisting of cascading denoising autoencoders, has recently shown great success in numerous generative tasks with different modalities including image [14, 39,43,44], audio [31], video [48], and motion [51]. To name a few, DDPM [19] explored the diffusion model for unconditional image generation. GLIDE [39] introduced text-conditional diffusion model and showed that classifier free guidance has better performance than CLIP [42] guidance. DALLE-2 [43] modified GLIDE to generate semantically consistent images conditioned on a CLIP image embedding, and proposed a diffusion prior that produces the image embedding given a text caption. MDM [51] utilized a classifier-free diffusion-based generative model for text-to-motion and action-to-motion tasks, allowing motion completion and editing as well.",
            "score": 0.5636001415002478,
            "section_title": "Diffusion Generative Models",
            "char_start_offset": 8284,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 869
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 24,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 24,
                    "end": 27,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 229,
                    "end": 232,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 295,
                    "end": 299,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 494,
                    "end": 498,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84814453125
        },
        {
            "corpus_id": "266550896",
            "title": "A Two-Stage Personalized Virtual Try-On Framework With Shape Control and Texture Guidance",
            "text": "With the advent of the diffusion model, the image generation task has made a huge breakthrough. With the ability to preserve the semantic structure of data, diffusion models can achieve higher diversity and fidelity than GAN-based generative models. GLIDE [21] and DALLE-2 [22] propose to generate novel images based on the text descriptions provided by humans, which have never been seen in the dataset before. Stable Diffusion [23] generates images by iterating over \"denoised\" data in a latent representation space, enabling text generation tasks to be completed on consumer GPUs. Retrieval-Diffusion [24] introduces nearest-neighbor retrieval to improve  generation quality and diversity. Humandiffusion [10] controls the results of human image generation through text input on the basis of segmentation. \n\nMany researchers have proposed text-guided conditional image generation using diffusion models. DiffusionCLIP [25] fine-tunes images by combining pre-trained diffusion models with CLIPloss. Dreambooth [26] implements directional modification of image properties through text. Imagic [27] uses complex language input to achieve multi-attribute adjustment of images. Blended Diffusion [28] proposes a multistep mixing process that utilizes user-supplied masks for local manipulation. FICE [11] adjusts the clothing style of a human body image by converting the entered text information into style attributes. \n\nAlthough the use of text can achieve simple control over image generation and editing, text guidance still has its disadvantages of lack of detailed information and low accuracy. On the other hand, image guidance can provide more detailed and accurate information. As a result, some work begin to emerge exploring the possibility of image-guided generation. BBDM [29] and DeepPortraitDrawing [30] use sketches as guiding conditions to generate corresponding images. Many studies have been proposed for image restoration tasks. For example, MCG [31] and RePaint [32] realize image repair and restoration for image occlusion. Palette [33] proposes a unified framework for image-to-image conversion based on a conditional diffusion model for conversion tasks (i.e., coloring, fixing, uncropping, and JPEG recovery).",
            "score": 0.5635108328044148,
            "section_title": "B. Conditional image generation based on the diffusion model",
            "char_start_offset": 8028,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 808
                },
                {
                    "start": 811,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2232
                }
            ],
            "ref_mentions": [
                {
                    "start": 429,
                    "end": 433,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 604,
                    "end": 608,
                    "matchedPaperCorpusId": "248377386"
                },
                {
                    "start": 921,
                    "end": 925,
                    "matchedPaperCorpusId": "244909410"
                },
                {
                    "start": 1012,
                    "end": 1016,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 1094,
                    "end": 1098,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 1194,
                    "end": 1198,
                    "matchedPaperCorpusId": "244714366"
                },
                {
                    "start": 1783,
                    "end": 1787,
                    "matchedPaperCorpusId": "257687901"
                },
                {
                    "start": 1812,
                    "end": 1816,
                    "matchedPaperCorpusId": "248512601"
                },
                {
                    "start": 1964,
                    "end": 1968,
                    "matchedPaperCorpusId": "249282628"
                },
                {
                    "start": 1981,
                    "end": 1985,
                    "matchedPaperCorpusId": "246240274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08599853515625
        },
        {
            "corpus_id": "276618257",
            "title": "Recommendations Beyond Catalogs: Diffusion Models for Personalized Generation",
            "text": "We employ Classifier-Free Guidance (CFG) (Ho & Salimans, 2022), a widely used technique in diffusion-based generative models, to enhance user-conditioned image embedding generation. CFG modulates the sampling process by interpolating between the unconditional model output \u03b5 \u03b8 (I t e , t) and the user-rating conditional prediction \u03b5 \u03b8 (I t e , t, U, R): \n\nHere \u03c9 is the guidance scale that controls the trade-off between personalization and diversity. Higher values of guidance result in less diverse sets of images, but more attuned to their conditioning signals. We conduct extensive experiments to evaluate the influence of different guidance scaling in Section 4.5",
            "score": 0.5632911682515102,
            "section_title": "CLASSIFIER-FREE GUIDANCE",
            "char_start_offset": 13932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 354
                },
                {
                    "start": 357,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 669
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9033203125
        },
        {
            "corpus_id": "273142392",
            "title": "Opportunities and challenges of diffusion models for generative AI",
            "text": "Diffusion models, inspired by thermodynamics modeling [5 ], have emerged in recent years with ground-breaking performance, surpassing the previous state of the art, such as generative adversarial networks (GANs) [6 ] and variational autoencoders (VAEs) [7 ]. Diffusion models are widely adopted in computer vision and audio generation tasks [8 -11 ], and further utilized in text generation [12 ,13 ], sequential data modeling [14 -16 ], reinforcement learning and control [17 -20 ], as well as life science [21 -23 ]. For a more comprehensive exposition of applications, we refer readers to survey papers [3 ,24 -28 ]. \n\nThe celebrated performance of diffusion models is indispensable to numerous methodological innovations that significantly expand the scope and boost the functionality of diffusion models, enabling high-fidelity generation, efficient sampling and flexible control of the sample generation. For example, Austin et al. [29 ] and Ouyan et al. [30 ] extended diffusion models to discrete data generation, while the vanilla diffusion models target continuous data. Meanwhile, there is an active line of research aiming to expedite the sample generation speed of diffusion models (see the references in the online supplementary material). Las t but not leas t, a recent surge of research focuses on fine-tuning diffusion models towards generating samples of desired properties, such as generating images with peculiar aesthetic qualities [31 ,32 ]. These task-specific properties are often encoded as guidance to the diffusion model, consisting of conditioning and control signals to steer the sample generation. Notably, guidance allows for the creation of diverse and relevant content across a wide range of applications, which underscores the versatility and adaptability of diffusion models. We call diffusion models with guidance conditional diffusion models.",
            "score": 0.5631026511506039,
            "section_title": "Diffusion models are a new powerhouse",
            "char_start_offset": 1548,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 54,
                    "end": 58,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "1033682"
                },
                {
                    "start": 253,
                    "end": 257,
                    "matchedPaperCorpusId": "211146177"
                },
                {
                    "start": 938,
                    "end": 943,
                    "matchedPaperCorpusId": "235755106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048675537109375
        },
        {
            "corpus_id": "251643669",
            "title": "Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance",
            "text": "In this work we propose to harness the perceptually aligned gradients phenomenon by utilizing robust classifiers to guide a diffusion process. Since this phenomenon's discovery, several works have explored the generative capabilities of such classifiers. Santurkar et al. (2019) demonstrated that adversarially robust In contrast, we propose an independent classifier training scheme, which scales well to more diverse datasets with higher resolutions. Another fascinating work is (Ho & Salimans, 2021), which proposed a class-conditional synthesis without using a classifier. Instead, they offered to combine predictions of conditional and unconditional diffusion models linearly. Interestingly, a single neural network was used for both models. However, while it shows impressive performance, the proposed combination is heuristic, and requires careful hyperparameter tuning. Their work (named classifier-free guidance) follows a different research direction than ours, as we focus on enhancing classifier guidance, enabling information from outside the trained diffusion model to be incorporated into the generation process. This approach improves the generative process' modularity and flexibility, as it allows the continued definitions of more classes, without requiring further training of the base generative diffusion model. Moreover, in the case where classes are not mutually exclusive, classifier guidance allows for the generation of multiple classes in the same image at inference time (by taking the gradient for all requested classes). This is possible in classifier-free guidance only by defining a combinatorial number of class embeddings (to account for all possible class intersections).",
            "score": 0.5630529141481965,
            "section_title": "Related Work",
            "char_start_offset": 27365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1707
                }
            ],
            "ref_mentions": [
                {
                    "start": 481,
                    "end": 502,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85986328125
        },
        {
            "corpus_id": "269502261",
            "title": "Improving Subject-Driven Image Synthesis with Subject-Agnostic Guidance",
            "text": "Similar to classifier guidance [13], classifier-free guidance is designed to trade between image quality and diversity, but without the need of a classifier.It is widely adopted in existing works [46,51].\n\nDuring training, an unconditional diffusion model is jointly trained by randomly replacing the input condition c by a null condition \u03d5.Once trained, during each iteration t, a weighted sum of the conditional output and the unconditional output is computed:\n\nIn general, a larger w produces better quality, whereas a smaller w yields greater diversity.",
            "score": 0.5629786826211787,
            "section_title": "Classifier-Free Guidance",
            "char_start_offset": 7841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 157,
                    "end": 204
                },
                {
                    "start": 206,
                    "end": 341
                },
                {
                    "start": 341,
                    "end": 462
                },
                {
                    "start": 464,
                    "end": 557
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 35,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 200,
                    "end": 203,
                    "matchedPaperCorpusId": "256827727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59912109375
        },
        {
            "corpus_id": "274446026",
            "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
            "text": "Classifier-free guidance (CFG) [15] and negative prompting are key techniques for content control in image generation, from text-to-2D to text-to-3D synthesis. In multistep diffusion models, these methods enhance desired image features and suppress undesired ones, improving visual fidelity and reducing artifacts like the Janus problem in 3D tasks [1,8,20]. Score-based approaches like NFSD [20] employ negative prompting to steer generated samples toward real-image distributions, while SDS-Bridge [31] uses negative prompts to model source distributions more accurately. Despite their success, negative prompting has yet to be integrated into one-step or few-step diffusion models. To bridge this gap, we introduce Negative-Away Steer Attention (NASA), the first method to enable negative prompting in one-step and few-step diffusion models, enhancing control in faster generation settings.",
            "score": 0.5626165908174823,
            "section_title": "Sampling Guidance and Negative Prompting",
            "char_start_offset": 7351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 893
                }
            ],
            "ref_mentions": [
                {
                    "start": 352,
                    "end": 354,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74560546875
        },
        {
            "corpus_id": "274446026",
            "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
            "text": "Classifier-free Guidance (CFG) [15] is an inference technique designed to enhance the quality of generated images by blending the predictions from both a conditional and an unconditional model. At each sampling step, CFG adjusts the denoiser's output using a control parameter \u03ba > 1, allowing for controlled guidance that aligns more closely with the desired conditions: \n\nNegative Prompts provide enhanced control by suppressing unwanted features in the generated content. Instead of producing an unconditional output, the model generates an output conditioned on the negative prompt y \u2212 , as follows: \n\nVariational Score Distillation (VSD) is a powerful framework that utilizes pretrained diffusion-based text-to-image models to enhance text-based generation across both 3D and 2D domains. Originally proposed for text-to-3D tasks [48], VSD leverages diffusion-based score matching to align Neural Radiance Fields (NeRFs) with text prompt, enabling the generation of intricate 3D objects [48]. This approach extends effectively to 2D image synthesis as well, where VSD enables rapid, high-quality text-to-image generation in a single step with models such as SwiftBrush and DMD [7,33,54,55], sidestepping the computational burden of multi-step diffusion methods. The central aim of VSD is to ensures that the renderings of a differentiable generator align with the probability density of plausible images as guided by the 2D diffusion model. To accomplish this, VSD employs a two-teacher approach that uses a fixed pretrained diffusion model \u03f5 \u03c8 and an adaptive LoRA-based teacher \u03f5 \u03d5 . While training, the student model f \u03b8 produces 2D images x0 = f \u03b8 (z, y) using an input noise z \u223c N (0, I) and a text prompt y. Noisy images xt = \u03b1 t x0 + \u03c3 t \u03f5 is then fed into both teacher models. The LoRA teacher model \u03f5 \u03d5 aligns with the student distribution by minimizing a denoising L2 loss on singlestep samples. This arrangement supports robust and adaptive guidance suitable for a variety of generative architectures.",
            "score": 0.5623419720899366,
            "section_title": "Background",
            "char_start_offset": 9869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2015
                }
            ],
            "ref_mentions": [
                {
                    "start": 833,
                    "end": 837,
                    "matchedPaperCorpusId": "258887357"
                },
                {
                    "start": 990,
                    "end": 994,
                    "matchedPaperCorpusId": "258887357"
                },
                {
                    "start": 1180,
                    "end": 1183,
                    "matchedPaperCorpusId": "271956822"
                },
                {
                    "start": 1186,
                    "end": 1189,
                    "matchedPaperCorpusId": "269982921"
                },
                {
                    "start": 1189,
                    "end": 1192,
                    "matchedPaperCorpusId": "265506768"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94482421875
        },
        {
            "corpus_id": "256662738",
            "title": "GraphGUIDE: interpretable and controllable conditional graph generation with discrete Bernoulli diffusion",
            "text": "Beyond the generation of samples from q 0 (x), a major goal of generative modeling is to perform conditional generation, where we wish to draw a sample from q 0 (x) which satisfies a specific label or property. Within the diffusion-model literature, there are effectively two methods for conditional generation. \n\nClassifier-guided conditional generation was proposed in Song et al. (2021), in which an external classifier f (x t ) is trained on x t to predict some label y. Input gradients from this classifier are then used during the generative process to bias the generation of an object toward one which has the label y. While elegant in its mathematical justification (a simple invocation of Bayes' Rule), it relies on an external classifier which is trained on noisy inputs x t from across the diffusion timeline and a predefined set of labels y. This method is also only readily applied to diffusion models trained in a continuous-time and continuous-noise setting, due to its reliance on gradients. \n\nIn contrast to classifier-guided conditional generation, Ho et al. (2021) proposed an alternative method: classifier-free conditional generation. Instead of relying on an external classifier, the neural network p \u03b8 (which defines the reversediffusion/generative process) is trained with labels as an input: p \u03b8 (x t , t, y). This method for conditional generation has been exceedingly popular, and has been shown to generate state-of-the-art samples (Rombach et al., 2021;Ho et al., 2022). Unlike classifier-guided conditional generation, this method enjoys the freedom of not relying on any external classifier, and it can be applied to discrete-time and/or discrete-noise diffusion settings. \n\nUnfortunately, both methods for conditional generation suffer from some limitations. Firstly, both methods merely supply a reverse-diffusion-influencing signal to the generative process (i.e. through biasing gradients or through an auxiliary input). This acts as a soft constraint which not only is uninterpretable, but also cannot be controlled or modified manually by a human during reverse diffusion. Secondly, both methods require a predefined set of labels y (to train either an external classifier or the diffusion model itself).",
            "score": 0.5622920961292138,
            "section_title": "Conditional generation",
            "char_start_offset": 7236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 311
                },
                {
                    "start": 314,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2109
                },
                {
                    "start": 2110,
                    "end": 2241
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.472900390625
        },
        {
            "corpus_id": "277501810",
            "title": "Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis",
            "text": "Training-free works focus on modifying the original diffusion sampling process to drive the conditional image synthesis using feedback from the guidance function. For text condition, mainstream guidance frameworks are classifier and CLIP guidance. Specifically, classifier guidance [7] incorporates class information into diffusion models using gradients of a timestep-aware image classifier. CLIP guidance, such as DOODL [43], replaces classification scores with global CLIP scores, which can make generation process accommodate free-form descriptions. Regarding layout condition, current approaches often implicitly modify distributions of attention scores to achieve guidance. The underlying assumption is: spatial distribution of highresponse attention aligns perceptually with the locations of objects in synthesized images [3]. Based on this idea, BoxDiff [50] and R&B [49] impose region and boundary constraints to stored cross-attention maps. DenseDiffusion [19] and A&R [32] further leverage semantic affinity in selfattention maps to ensure more accurate object placement. With respect to drag condition, DragDiffusion [39] and DragNoise [25] employ motion supervision and point tracking steps to optimize diffusion semantic features. These steps enable features of original points to move to destination points as effectively as possible. However, aforementioned approaches are often designed to encode a particular condition form, which restricts their generalization across a wide range of conditions.",
            "score": 0.5622848007588226,
            "section_title": "Training-free Conditional Image Synthesis",
            "char_start_offset": 6948,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1514
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 285,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 422,
                    "end": 426,
                    "matchedPaperCorpusId": "257757144"
                },
                {
                    "start": 829,
                    "end": 832,
                    "matchedPaperCorpusId": "256416326"
                },
                {
                    "start": 862,
                    "end": 866,
                    "matchedPaperCorpusId": "259991581"
                },
                {
                    "start": 966,
                    "end": 970,
                    "matchedPaperCorpusId": "261101003"
                },
                {
                    "start": 979,
                    "end": 983,
                    "matchedPaperCorpusId": "259108247"
                },
                {
                    "start": 1129,
                    "end": 1133,
                    "matchedPaperCorpusId": "259252555"
                },
                {
                    "start": 1148,
                    "end": 1152,
                    "matchedPaperCorpusId": "268819764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8359375
        },
        {
            "corpus_id": "252199918",
            "title": "Diffusion Models in Vision: A Survey",
            "text": "We next showcase diffusion models that are applied to conditional image synthesis. The condition is commonly based on various source signals, in most cases some class labels being used. Some methods perform both unconditional and conditional generation, which are also discussed here.\n\n1) Denoising Diffusion Probabilistic Models: Dhariwal et al. [5] introduce few architectural changes to improve the FID of diffusion models. They also propose classifier guidance, a strategy which uses the gradients of a classifier to guide the diffusion during sampling. Bordes et al. [101] examine representations resulting from self-supervised tasks by visualizing and comparing them to the original image. They also compare representations generated from different sources. The authors implement several modifications to the U-Net architecture presented by Dhariwal et al. [5], such as adding conditional batch normalization layers, and mapping the vector representation through a fully connected layer. The method presented in [95] allows diffusion models to produce images from low-density regions of the data manifold. They use two new losses to guide the reverse process. The first loss guides the diffusion towards low-density regions, while the second enforces the diffusion to stay on the manifold. Kong et al. [89] define a bijection between the continuous diffusion steps and the noise levels. With the defined bijection, they are able to construct an approximate diffusion process which requires less steps. The method is tested using the previous DDIM [7] and DDPM [2] architectures on image generation. Pandey et al. [18] build a generator-refiner framework, where the generator is a VAE and the refiner is a DDPM conditioned by the output of the VAE. The latent space of the VAE can be used to control the content of the generated image because the DDPM only adds the details. After training the framework, the resulting DDPM is able to generalize to different noise types. Ho et al. [104] introduce Cascaded Diffusion Models (CDM), an approach to generate high-resolution images conditioned on ImageNet classes. The framework uses multiple diffusion models, where the first model from the pipeline generates low-resolution images conditioned on the image class. The subsequent models are responsible for generating images of increasingly higher resolutions, being conditioned on both the class and the",
            "score": 0.56223819475162,
            "section_title": "B. Conditional Image Generation",
            "char_start_offset": 40644,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 350,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 572,
                    "end": 577,
                    "matchedPaperCorpusId": "245329541"
                },
                {
                    "start": 863,
                    "end": 866,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1018,
                    "end": 1022,
                    "matchedPaperCorpusId": "247839278"
                },
                {
                    "start": 1308,
                    "end": 1312,
                    "matchedPaperCorpusId": "235265701"
                },
                {
                    "start": 1553,
                    "end": 1556,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 1566,
                    "end": 1569,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 1619,
                    "end": 1623,
                    "matchedPaperCorpusId": "249335390"
                },
                {
                    "start": 1987,
                    "end": 1992,
                    "matchedPaperCorpusId": "235619773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4306640625
        },
        {
            "corpus_id": "252070859",
            "title": "Diffusion Models: A Comprehensive Survey of Methods and Applications",
            "text": "Vision-language models have attracted a lot of attention recently due to the number of potential applications [183]. Text-to-Image generation is the task of generating a corresponding image from a descriptive text [62]. Blended diffusion [7] utilizes both pre-trained DDPM [54] and CLIP [183] models, and it proposes a solution for region-based image editing for general purposes, which uses natural language guidance and is applicable to real and diverse images. DiffusionCLIP [117] carries out CLIP-guided text-driven image manipulation based on full inversion capability and high-quality image generation power of recent diffusion models. It finetunes the score function in the reverse diffusion process using a CLIP loss that controls the attributes of the generated image based on the text prompts. On the other hand, unCLIP (DALLE-2) [186] proposes a two-stage approach, a prior model that can generate a CLIP-based image embedding conditioned on a text caption, and a diffusion-based decoder that can generate an image conditioned on the image embedding. Recently, Imagen [199] proposes a text-to-image diffusion model and a comprehensive benchmark for performance evaluation. It shows that Imagen performs well against the state-of-the-art approaches including VQ-GAN+CLIP [46], Latent Diffusion Models [145], and DALL-E 2 [186]. Models based on classifier guidance [54] use the gradients of an extra classifier to improve the sampling quality of a diffusion model, whereas schemes based on classifier-free guidance [89] mix the score estimates of a conditional diffusion model and a jointly trained unconditional diffusion model. Inspired by the ability of these guided diffusion models [54,89] to generate photorealistic samples and the ability of text-to-image models to handle free-form prompts, GLIDE [166] applies guided diffusion to the application of text-conditioned image synthesis as demonstrated in Figure 8. VQ-Diffusion [78] proposes a vector-quantized diffusion model for text-to-image generation, and it eliminates the unidirectional bias and avoids accumulative prediction errors.",
            "score": 0.561422316664651,
            "section_title": "Text-to-Image Generation.",
            "char_start_offset": 64794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 2105
                }
            ],
            "ref_mentions": [
                {
                    "start": 110,
                    "end": 115,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 238,
                    "end": 241,
                    "matchedPaperCorpusId": "244714366"
                },
                {
                    "start": 273,
                    "end": 277,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 287,
                    "end": 292,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 478,
                    "end": 483,
                    "matchedPaperCorpusId": "244909410"
                },
                {
                    "start": 1374,
                    "end": 1378,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.908203125
        },
        {
            "corpus_id": "277321603",
            "title": "Unconditional Priors Matter! Improving Conditional Generation of Fine-Tuned Diffusion Models",
            "text": "In recent years, diffusion models [29,56,59] have shown great success in generation tasks, becoming the de facto standard generative model across many data modalities such as images [50][51][52][53], video [4,5,30,66], and audio [13,33,44]. The success of diffusion models is not only due to their high-quality results and ease of training, but also the simplicity of adapting them into conditional diffusion models. While previous generative models such as GANs [23] and VAEs [39] require separate training for each conditional generation task, making it costly to create various conditional generative models, diffusion models introduced a considerably more effective approach: training an unconditional model (or a conditional model with simple conditions, such as text) as a base and branching out into multiple conditional models. At the core of the extendability of diffusion models in easily converting an unconditional (or less conditioned) base model into a conditional (or more conditioned) model is the Classifier-Free Guidance (CFG) [28] technique. CFG proposed to learn to predict both unconditional and conditional noises using a single neural network, without introducing another network, such as a classifier, as in the classifier-guidance [15] approach. CFG combines unconditional and conditional noise predictions to generate data conditioned on a given input. It has been widely adopted not only for training a conditional model from scratch but also for fine-tuning a base model to incorporate other conditions, by adding encoders for the conditional input. Many successful conditional generative models have been fine-tuned using CFG from a base model. For example, Zero-1-to-3 [46] and Versatile Diffusion [64] use variants of Stable Diffusion [52] (SD) as a base, with additional encoders to incorporate the input image as conditions, while Instruct-Pix2Pix [7] uses SD1.5 as a base and incorporates text editing instructions and input reference images as conditions to perform instruction-based image editing. \n\nDespite its successes and widespread usage, fine-tuning a conditional model from a base model using the CFG technique has limitations, most notably producing lower-quality results for unconditional generation.",
            "score": 0.5611215241237816,
            "section_title": "Introduction",
            "char_start_offset": 1600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 2033
                },
                {
                    "start": 2036,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 43,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 182,
                    "end": 186,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 190,
                    "end": 194,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 209,
                    "end": 211,
                    "matchedPaperCorpusId": "258187553"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "248006185"
                },
                {
                    "start": 214,
                    "end": 217,
                    "matchedPaperCorpusId": "263151295"
                },
                {
                    "start": 229,
                    "end": 233,
                    "matchedPaperCorpusId": "259108357"
                },
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "260775781"
                },
                {
                    "start": 463,
                    "end": 467,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 1045,
                    "end": 1049,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1699,
                    "end": 1703,
                    "matchedPaperCorpusId": "257631738"
                },
                {
                    "start": 1728,
                    "end": 1732,
                    "matchedPaperCorpusId": "253523371"
                },
                {
                    "start": 1766,
                    "end": 1770,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1881,
                    "end": 1884,
                    "matchedPaperCorpusId": "253581213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91455078125
        },
        {
            "corpus_id": "271903235",
            "title": "Classifier-Free Guidance is a Predictor-Corrector",
            "text": "We investigate the theoretical foundations of classifier-free guidance (CFG). CFG is the dominant method of conditional sampling for text-to-image diffusion models, yet unlike other aspects of diffusion, it remains on shaky theoretical footing. In this paper, we disprove common misconceptions, by showing that CFG interacts differently with DDPM (Ho et al., 2020) and DDIM (Song et al., 2021), and neither sampler with CFG generates the gamma-powered distribution $p(x|c)^\\gamma p(x)^{1-\\gamma}$. Then, we clarify the behavior of CFG by showing that it is a kind of predictor-corrector method (Song et al., 2020) that alternates between denoising and sharpening, which we call predictor-corrector guidance (PCG). We prove that in the SDE limit, CFG is actually equivalent to combining a DDIM predictor for the conditional distribution together with a Langevin dynamics corrector for a gamma-powered distribution (with a carefully chosen gamma). Our work thus provides a lens to theoretically understand CFG by embedding it in a broader design space of principled sampling methods.",
            "score": 0.5606202033422896,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80078125
        },
        {
            "corpus_id": "271334405",
            "title": "OutfitAnyone: Ultra-high Quality Virtual Try-On for Any Clothing and Any Person",
            "text": "In the context of original SD, classifier-free guidance [15] is a technique used to control the generation process without relying on an external classifier.This method leverages a single diffusion model trained on both conditional and unconditional data.By adjusting the scale of guidance, it can steer the generation process towards producing images that align with a given text prompt.\n\nIn our virtual Try-On framework, we identified the clothing image as the pivotal control element, underscoring its significance over textual prompts.Consequently, we have tailored the unconditional classifier guidance to utilize a blank clothing image, while the conditional guidance is informed by the actual clothing image provided.we are able to harness the guidance scale effectively, thereby delivering more precise and consistent generation outcomes.",
            "score": 0.5604870211067824,
            "section_title": "Classifier-Free Guidance",
            "char_start_offset": 8842,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 157,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 388
                },
                {
                    "start": 390,
                    "end": 539
                },
                {
                    "start": 539,
                    "end": 724
                },
                {
                    "start": 724,
                    "end": 846
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82666015625
        },
        {
            "corpus_id": "277271753",
            "title": "TCFG: Tangential Damping Classifier-free Guidance",
            "text": "Diffusion models have achieved remarkable success in text-to-image synthesis, largely attributed to the use of classifier-free guidance (CFG), which enables high-quality, condition-aligned image generation. CFG combines the conditional score (e.g., text-conditioned) with the unconditional score to control the output. However, the unconditional score is in charge of estimating the transition between manifolds of adjacent timesteps from $x_t$ to $x_{t-1}$, which may inadvertently interfere with the trajectory toward the specific condition. In this work, we introduce a novel approach that leverages a geometric perspective on the unconditional score to enhance CFG performance when conditional scores are available. Specifically, we propose a method that filters the singular vectors of both conditional and unconditional scores using singular value decomposition. This filtering process aligns the unconditional score with the conditional score, thereby refining the sampling trajectory to stay closer to the manifold. Our approach improves image quality with negligible additional computation. We provide deeper insights into the score function behavior in diffusion models and present a practical technique for achieving more accurate and contextually coherent image synthesis.",
            "score": 0.5599970775115582,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9677734375
        },
        {
            "corpus_id": "277510202",
            "title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
            "text": "Diffusion and Flow-based Models. Unlike generative ad-versarial methods [7] that rely on one-step generation, diffusion models [4] have demonstrated significantly improved performance in generating high-quality samples. Early diffusion models were primarily score-based generative models, including DDPM [10], DDIM [34], EDM [16], and Stable Diffusion [30], which focused on learning the SDEs governing the diffusion process. \n\nNext, Flow Matching [21] provides an alternative approach by directly modeling sample trajectories using ordinary differential equations (ODEs) instead of SDEs. This enables more stable and efficient generative processes by learning a continuous flow field that smoothly transports samples from a prior distribution to the target distribution. Several works, including Rectified Flow [24], SD3 [5], Lumina-Next [45], Flux [20], Vchitect-2.0 [6], Lumina-Video [23] HunyuanVideo [18], SkyReels-v1 [33], and Wan2.1 [39] have demonstrated that ODE-based methods achieve faster convergence and improved controllability in text-to-image and text-to-video generation. As a result, Flow Matching has become a compelling alternative to stochastic diffusion models, offering better interpretability and training stability. Thus, our analysis is based on Flow Matching models, which aim to provide more accurate classifier-free guidance. Guidance in Diffusion Models. Achieving better control over diffusion models remains challenging yet essential. Early approaches, such as classifier guidance (CG) [4], introduce control by incorporating classifier gradients into the sampling process. However, this method requires separately trained classifiers, making it less flexible and computationally demanding. To overcome these limitations, classifierfree guidance (CFG) [9] was proposed, enabling guidance without the need for an external classifier. Instead, CFG trains conditional and unconditional models simultaneously and interpolates between their outputs during sampling. \n\nDespite its effectiveness, CFG relies on an unbounded empirical parameter, known as the guidance scale, which determines how strongly the generated output is influenced by the conditional model.",
            "score": 0.5599950872298092,
            "section_title": "Related Work",
            "char_start_offset": 3527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1992
                },
                {
                    "start": 1995,
                    "end": 2189
                }
            ],
            "ref_mentions": [
                {
                    "start": 127,
                    "end": 130,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 325,
                    "end": 329,
                    "matchedPaperCorpusId": "249240415"
                },
                {
                    "start": 352,
                    "end": 356,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "252734897"
                },
                {
                    "start": 812,
                    "end": 816,
                    "matchedPaperCorpusId": "252111177"
                },
                {
                    "start": 822,
                    "end": 825,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 1518,
                    "end": 1521,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89111328125
        },
        {
            "corpus_id": "253735443",
            "title": "DreamArtist++: Controllable One-Shot Text-to-Image Generation via Positive-Negative Adapter",
            "text": "With the exponential evolution of generative models, the focus of research on Text-to-Image synthesis has gradually shifted from GAN to Diffusion [12,13,14,15,16,17]. Some large-scale text-to-image models [3,23,24,2] have made highly accurate and fine-grained controllable semantic generation. The recently proposed stable diffusion [2] unprecedented making high-resolution and high-quality large-scale text-to-image models become reality. These Diffusion models usually employ classifier guidance [21] or classifier-free [22] guidance to generate images with text guiding. [32,33] attempts to add spatial conditioning controls pretrained text-to-image diffusion models, while [34] employs both image and text as prompts to control the diffusion models with an additional cross attention. However, it is difficult for these methods to generate images following user-given patterns. And complex descriptions are required to generate high quality images.",
            "score": 0.559852828269957,
            "section_title": "Text-to-Image Synthesis",
            "char_start_offset": 8072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 952
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 150,
                    "end": 153,
                    "matchedPaperCorpusId": "219965481"
                },
                {
                    "start": 153,
                    "end": 156,
                    "matchedPaperCorpusId": "244799255"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "202577442"
                },
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "76661216"
                },
                {
                    "start": 208,
                    "end": 211,
                    "matchedPaperCorpusId": "235212350"
                },
                {
                    "start": 214,
                    "end": 216,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 333,
                    "end": 336,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 522,
                    "end": 526,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 574,
                    "end": 578,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 578,
                    "end": 581,
                    "matchedPaperCorpusId": "256900833"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57373046875
        },
        {
            "corpus_id": "233168627",
            "title": "Creativity and Machine Learning: A Survey",
            "text": "In order to generate higher-quality images and to allow text-to-image generation, a variety of effective methods for conditioning have been proposed. A possibility is to use classifier guidance [63]: the diffusion score (i.e., the added noise) includes the gradient of the log-likelihood of an auxiliary classifier model. An alternative is classifier-free guidance [117]: to avoid learning an additional model, a single neural network is used to parameterize two diffusion models, one conditional and one unconditional; the two models are then jointly trained by randomly setting the class for the unconditional model. \n\nFinally, the sampling is performed using a linear combination of conditional and unconditional score estimates. Guided Language to Image Diffusion for Generation and Editing (GLIDE) [198] demonstrates how classifier-free guidance can be effectively used to generate text-conditional images. In addition, it shows how diffusion models can be used for image editing by fine-tuning in order to reconstruct masked regions. Performance improvement can be obtained by means of a cascade of multiple diffusion models performing conditioning augmentation [116]. Notably, the diffusion model can operate on latent vectors instead of real images. Stable Diffusion [232] employs a diffusion model in the latent space of a pre-trained autoencoder. Similarly, DALL-E 2 [221] generates images by conditioning with image representations. At first, it learns a prior diffusion model to generate possible CLIP image embeddings from a given text caption, i.e., conditioned by its CLIP text embedding. Then, a diffusion decoder produces images conditioned by the image embedding. The generation quality can be further improved by means of generated captions for the images in the training set [19]. Imagen [235] uses instead a cascaded diffusion decoder, together with a frozen language model as a text encoder to increase the quality of output. \n\nAlthough the approach is particularly suitable for images, applications to other data sources have been developed as well. DiffWave [152] and WaveGrad [45] use diffusion models to generate audio. They overcome the continuousdiscrete dichotomy by working on waveform.",
            "score": 0.5596967591201804,
            "section_title": "Diffusion Models",
            "char_start_offset": 46859,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1947
                },
                {
                    "start": 1950,
                    "end": 2072
                },
                {
                    "start": 2073,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2216
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 365,
                    "end": 370,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 803,
                    "end": 808,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 1168,
                    "end": 1173,
                    "matchedPaperCorpusId": "235619773"
                },
                {
                    "start": 1275,
                    "end": 1280,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1808,
                    "end": 1813,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 2082,
                    "end": 2087,
                    "matchedPaperCorpusId": "221818900"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9501953125
        },
        {
            "corpus_id": "256416107",
            "title": "Meta-Learning via Classifier(-free) Diffusion Guidance",
            "text": "Several techniques have been proposed for effective conditional sampling in generative and diffusion models, such as classifier/CLIP guidance (Dhariwal & Nichol, 2021;Gal et al., 2021;Patashnik et al., 2021) and classifier-free guidance (Ho & Salimans, 2021;Crowson, 2022;Nichol et al., 2022). Diffusion models with classifier-free guidance have also been successfully applied in non-visual domains, such as audio generation (Kim et al., 2022) and robotic planning (Janner et al., 2022). \n\nZero-shot learning There exists a large literature on zeroshot learning, including both established benchmarks and well known methods (Han et al., 2021;Su et al., 2022;Gupta et al., 2021). While these zero-shot learning works consider the zero-shot performance on unseen class labels within a single classification task, our setting considers that of the zero-shot performance where test tasks themselves are unseen, thus raising the zero shot problem to the task-level.",
            "score": 0.559675931733925,
            "section_title": "Meta learning",
            "char_start_offset": 29061,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 960
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 167,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 425,
                    "end": 443,
                    "matchedPaperCorpusId": "246430592"
                },
                {
                    "start": 624,
                    "end": 642,
                    "matchedPaperCorpusId": "232417664"
                },
                {
                    "start": 642,
                    "end": 658,
                    "matchedPaperCorpusId": "250520486"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6513671875
        },
        {
            "corpus_id": "274598164",
            "title": "Self-Guidance: Boosting Flow and Diffusion Generation on Their Own",
            "text": "Over the past decade, deep generative models have achieved remarkable advancements across various applications [5, 6, 9, 14, 17, 21, 22, 25, 26, 36-38, 40, 42, 44, 45, 48]. Among them, diffusion models [12,58,60] and flow-based models [2,8,29] have notably excelled in producing high-resolution, text-driven data such as images [50,51,54], videos [4,14,67], and others [7,20,28,39,45,56,62,68], pushing the boundaries of Artificial Intelligence Generated Contents. \n\nIn simple terms, diffusion models learn a multi-step transition from a prior distribution p T (x T ) to a real data distribution p 0 (x 0 ). However, default sampling methods for diffusion and flow-based models often lead to unsatisfactory generation quality, such as broken human hands and faces, and images with bad foreground and background. To address these issues, various guidance strategies have emerged as cheap yet effective ways to guide the generation process for better generation quality. For instance, classifier-free guidance (CFG) [11] modifies the velocity of diffusion and flow-based generative models by adding a delta term between class-conditional and unconditional velocities, which pushes generated samples to have high class probabilities. \n\nThough these existing guidance have shown impressive performance improvements, they have various individual restrictions. For instance, the CFG relies on computing an additional unconditional velocity, which requires training the diffusion model under both conditional and unconditional settings, therefore, harms the modeling performances [1]. Auto-Guidance (AG) pays a significant price that requires training an additional bad-version model, which is tricky as well as requiring more memory costs. Other guidance, such as Perturbed-attention Guidance (PAG [1]), and selfattention guidance (SAG [16]), do not rely on additional training. However, as the PAG paper described, the effectiveness of PAG is highly sensitive to the selection of perturbed attention layers inside the neural network, making it less flexible to enhance models in general applications.",
            "score": 0.5588190013532138,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 206,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 209,
                    "end": 212,
                    "matchedPaperCorpusId": "235352469"
                },
                {
                    "start": 328,
                    "end": 332,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 375,
                    "end": 378,
                    "matchedPaperCorpusId": "221818900"
                },
                {
                    "start": 378,
                    "end": 381,
                    "matchedPaperCorpusId": "259108195"
                },
                {
                    "start": 390,
                    "end": 393,
                    "matchedPaperCorpusId": "259501305"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71826171875
        },
        {
            "corpus_id": "251799923",
            "title": "Understanding Diffusion Models: A Unified Perspective",
            "text": "So far, we have focused on modeling just the data distribution p(x). However, we are often also interested in learning conditional distribution p(x|y), which would enable us to explicitly control the data we generate through conditioning information y. This forms the backbone of image super-resolution models such as Cascaded Diffusion Models [18], as well as state-of-the-art image-text models such as DALL-E 2 [19] and Imagen [7]. \n\nA natural way to add conditioning information is simply alongside the timestep information, at each iteration. Recall our joint distribution from Equation 32: \n\nThen, to turn this into a conditional diffusion model, we can simply add arbitrary conditioning information y at each transition step as: \n\nFor example, y could be a text encoding in image-text generation, or a low-resolution image to perform super-resolution on. We are thus able to learn the core neural networks of a VDM as before, by predicting x\u03b8 (x t , t, y) \u2248 x 0 , \u02c6 \u03b8 (x t , t, y) \u2248 0 , or s \u03b8 (x t , t, y) \u2248 \u2207 log p(x t |y) for each desired interpretation and implementation. \n\nA caveat of this vanilla formulation is that a conditional diffusion model trained in this way may potentially learn to ignore or downplay any given conditioning information. Guidance is therefore proposed as a way to more explicitly control the amount of weight the model gives to the conditioning information, at the cost of sample diversity. The two most popular forms of guidance are known as Classifier Guidance [10,20] and Classifier-Free Guidance [21].",
            "score": 0.5586317607179405,
            "section_title": "Guidance",
            "char_start_offset": 42256,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 433
                },
                {
                    "start": 436,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1082
                },
                {
                    "start": 1085,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1544
                }
            ],
            "ref_mentions": [
                {
                    "start": 344,
                    "end": 348,
                    "matchedPaperCorpusId": "235619773"
                },
                {
                    "start": 1506,
                    "end": 1509,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1539,
                    "end": 1543,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74462890625
        },
        {
            "corpus_id": "276421312",
            "title": "Diffusion Models without Classifier-free Guidance",
            "text": "Diffusion models (Sohl-Dickstein et al., 2015;Song & Ermon, 2019;Ho et al., 2020;Song et al., 2021a;b) have become the cornerstone of many successful generative models, e.g. image generation (Dhariwal & Nichol, 2021;Nichol et al., 2022;Rombach et al., 2022;Podell et al., 2024;Chen et al., 2024) and video generation (Ho et al., 2022;Blattmann et al., 2023;Gupta et al., 2025;Polyak et al., 2024;Wang et al., 2024) tasks. However, diffusion models also struggle to generate \"low temperature\" samples (Ho & Salimans, 2021;Karras et al., 2024) due to the nature of training objectives, and techniques such as Classifier guidance (Dhariwal & Nichol, 2021) and Classifier-free guidance (CFG) (Ho & Salimans, 2021) are proposed to improve performances. \n\nDespite its advantage and ubiquity, CFG has several drawbacks (Karras et al., 2024) and poses challenges to effective implementations (Kynk\u00e4\u00e4nniemi et al., 2024)  models. One critical limitation is the simultaneous training of unconditional model apart from the main diffusion model. The unconditional model is typically implemented by randomly dropping the condition of training pairs and replacing with an manually defined empty label. The introduction of additional tasks may reduce network capabilities and lead to skewed sampling distributions (Karras et al., 2024;Kynk\u00e4\u00e4nniemi et al., 2024). Furthermore, CFG requires two forward passes per denoising step during inference, one for the conditioned and another for the unconditioned model, thereby significantly escalating the computational costs.",
            "score": 0.5585266441583258,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1552
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 46,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 46,
                    "end": 65,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 81,
                    "end": 100,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 216,
                    "end": 236,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 236,
                    "end": 257,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 257,
                    "end": 277,
                    "matchedPaperCorpusId": "259341735"
                },
                {
                    "start": 277,
                    "end": 295,
                    "matchedPaperCorpusId": "263334265"
                },
                {
                    "start": 317,
                    "end": 334,
                    "matchedPaperCorpusId": "248006185"
                },
                {
                    "start": 334,
                    "end": 357,
                    "matchedPaperCorpusId": "258187553"
                },
                {
                    "start": 357,
                    "end": 376,
                    "matchedPaperCorpusId": "266163109"
                },
                {
                    "start": 500,
                    "end": 521,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 521,
                    "end": 541,
                    "matchedPaperCorpusId": "270226598"
                },
                {
                    "start": 688,
                    "end": 709,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 812,
                    "end": 833,
                    "matchedPaperCorpusId": "270226598"
                },
                {
                    "start": 884,
                    "end": 911,
                    "matchedPaperCorpusId": "269043032"
                },
                {
                    "start": 1299,
                    "end": 1320,
                    "matchedPaperCorpusId": "270226598"
                },
                {
                    "start": 1320,
                    "end": 1346,
                    "matchedPaperCorpusId": "269043032"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6318359375
        },
        {
            "corpus_id": "269005366",
            "title": "Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance",
            "text": "Recently, text-to-image generation has witnessed rapid development and various applications [30,31,33,34,48], where visually stunning images can be created by simply typing in a text prompt.In particular, after DDPM [7,12] succeeded GANs [3,8], diffusion models [40], such as Stable Diffusion [34] and DallE-3 [2], have emerged as the new state-of-the-art family for image-generative models.\n\nThe key feature of diffusion models is to approximate the true data distribution p(x) by reversing the process of perturbing the data with noise progressively in a long iterative * the corresponding author: liuyuisanai@gmail.com Figure 1.A motivation example.The first line shows images generated by Stable Diffusion with CFG and S-CFG, where the prompt is \"a photo of an astronaut riding a horse\" and the segmentation maps are manually labeled (Ground, Sky, Horse, Astronaut).The below line shows the average norm curves of the estimated classifier score \u2207x t log p(c|xt) (solid line) and diffusion score \u2207x t log p(xt) (dashed line) in each semantic region.The Y-axis scale unit is set as the dynamic variance parameter \u03c3t for better illustrations without damaging the conclusion.chain.To incorporate the text prompt c into the final generation, it is necessary to enhance the likelihood of c given the current latent image x t at each reversed diffusion step t.Instead of training extra classifiers to model p(c|x t ) at each diffusion step t [7], classifier-free guidance (CFG) [11] has recently been proposed to estimate both the classifier score \u2207 xt log p(c|x t ) and the diffusion score \u2207 xt p(x t ) with the same neural models, such as U-net [35].In particular, an empirical CFG scale is introduced to control the strength of the text guidance on the whole image space.However, we argue that a global CFG scale results in spatial inconsistency on varying semantic strengths during the denoising process and suboptimal quality of the final image.",
            "score": 0.5581218550457647,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 190,
                    "end": 391
                },
                {
                    "start": 393,
                    "end": 631
                },
                {
                    "start": 631,
                    "end": 652
                },
                {
                    "start": 652,
                    "end": 870
                },
                {
                    "start": 870,
                    "end": 1052
                },
                {
                    "start": 1052,
                    "end": 1175
                },
                {
                    "start": 1175,
                    "end": 1181
                },
                {
                    "start": 1181,
                    "end": 1357
                },
                {
                    "start": 1357,
                    "end": 1649
                },
                {
                    "start": 1649,
                    "end": 1771
                },
                {
                    "start": 1771,
                    "end": 1947
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 96,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 99,
                    "end": 102,
                    "matchedPaperCorpusId": "1563370"
                },
                {
                    "start": 102,
                    "end": 105,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 238,
                    "end": 241,
                    "matchedPaperCorpusId": "52889459"
                },
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1475,
                    "end": 1479,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.814453125
        },
        {
            "corpus_id": "268531795",
            "title": "Understanding and Improving Training-free Loss-based Diffusion Guidance",
            "text": "Adding additional control to pretrained diffusion models has become an increasingly popular research area, with extensive applications in computer vision, reinforcement learning, and AI for science. Recently, several studies have proposed training-free loss-based guidance by using off-the-shelf networks pretrained on clean images. This approach enables zero-shot conditional generation for universal control formats, which appears to offer a free lunch in diffusion guidance. In this paper, we aim to develop a deeper understanding of training-free guidance, as well as overcome its limitations. We offer a theoretical analysis that supports training-free guidance from the perspective of optimization, distinguishing it from classifier-based (or classifier-free) guidance. To elucidate their drawbacks, we theoretically demonstrate that training-free guidance is more susceptible to adversarial gradients and exhibits slower convergence rates compared to classifier guidance. We then introduce a collection of techniques designed to overcome the limitations, accompanied by theoretical rationale and empirical evidence. Our experiments in image and motion generation confirm the efficacy of these techniques.",
            "score": 0.5579466655399394,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7470703125
        },
        {
            "corpus_id": "273098845",
            "title": "Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models",
            "text": "Diffusion models (Sohl-Dickstein et al., 2015;Ho et al., 2020;Song et al., 2021b) are a class of generative models that learn the data distribution by reversing a forward process that adds noise to the data until the samples are indistinguishable from pure noise. Although the theory suggests that simulating the backward process in diffusion models should result in correct sampling from the data distribution, unguided sampling from diffusion models often results in low-quality images that do not align well with the input condition. Accordingly, classifier-free guidance (Ho & Salimans, 2022) has been established as an essential tool in modern diffusion models for increasing the quality of generations and the alignment between the condition and the generated image, albeit at the cost of reduced diversity (Ho & Salimans, 2022;Sadat et al., 2024a). \n\nModern text-to-image models, such as Stable Diffusion (Rombach et al., 2022), generally require high guidance scales in order for the generations to have better quality and align well with the input prompt. However, high guidance scales often result in oversaturated colors and simplified image compositions (Saharia et al., 2022b;Kynk\u00e4\u00e4nniemi et al., 2024). Despite these disadvantages, high CFG scales are still used in practice due to their superior image quality compared to alternatives. \n\nIn this paper, we analyze the update rule of CFG and show that with a few modifications to how the CFG update is applied at inference, we can vastly mitigate the oversaturation and artifacts of high guidance scales. First, we show that the CFG update rule can be decomposed into two components, one that is parallel to the conditional model prediction, and one that is orthogonal to this prediction. We show that the orthogonal element is mainly responsible for improving image quality, while the parallel part primarily adds contrast and saturation to the output. To the best of our knowledge, this is the first study that disentangles these two effects in CFG. \n\nAdditionally, we establish a connection between the CFG update rule and stochastic gradient ascent.",
            "score": 0.5575151144946414,
            "section_title": "INTRODUCTION",
            "char_start_offset": 386,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2015
                },
                {
                    "start": 2018,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 62,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 62,
                    "end": 81,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 834,
                    "end": 854,
                    "matchedPaperCorpusId": "264490969"
                },
                {
                    "start": 912,
                    "end": 934,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7119140625
        },
        {
            "corpus_id": "273345753",
            "title": "Saliency Guided Optimization of Diffusion Latents",
            "text": "The Image Diffusion Model, initially proposed by Sohl-Dickstein et al. [25], has been utilized for image generation. Ho et al. [8] established a link between the diffusion model and the denoising score matching model [27] and introduced Denoising Diffusion Probabilistic Models (DDPM) as an alternative way to achieve superior image generation quality. Rombach et al. proposed the Latent Diffusion Models (LDM) [21], which execute diffusion steps in the latent space, thereby decreasing computational costs. Dhariwal et al. [3] enhanced image generation quality over GANs by identifying a superior model architecture and introducing a novel sampling technique, which is classifier guidance. However, classifier guidance necessitates training an additional classifier network. Ho et al. [9] proposed a framework that trains a conditional model and an unconditional model jointly to provide classifier-free guidance with the same effect as classifier guidance. Nichol et al. [17] proposed GLIDE, which considered CLIP [19] guidance as a guiding strategy and discussed its possibility of improving the generated image quality. Ramesh et al. [20] proposed DALL-E2 to generate images from CLIP latent embeddings. Saharia et al. [23] proposed Imagen, which improves both sample fidelity and text-image alignment by increasing the size of the text encoder. In this study, we utilize Stable Diffusion model as our foundational framework to explore diffusion latents optimization guided by saliency to enhance the quality of the generated images.",
            "score": 0.5574822976545669,
            "section_title": "Image Diffusion",
            "char_start_offset": 4111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1537
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 75,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 127,
                    "end": 130,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 217,
                    "end": 221,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 411,
                    "end": 415,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 524,
                    "end": 527,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1016,
                    "end": 1020,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1223,
                    "end": 1227,
                    "matchedPaperCorpusId": "248986576"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80419921875
        },
        {
            "corpus_id": "267938436",
            "title": "Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion",
            "text": "Controllable Image Generation. To facilitate flexible image generation capable of meeting diverse requirements, achieving controllable image generation has become a prominent focus in recent times. A prevalent approach is guidance generation. This kind of method obviates the necessity of training a diffusion model from scratch with specific conditions and instead offers direction by pre-existing specialized models or loss functions for the reverse process of the diffusion model. An early attempt in this direction was GLIDE (Nichol et al., 2021) and classifier-free guidance   (Ho & Salimans, 2022), which facilitate a diffusion model in generating images that align with textual descriptions. \n\nTo satisfy arbitrary requirements like textual descriptions, layouts, or segmentation, Universal Guidance (Bansal et al., 2023) is proposed. It leverages pre-existing models to offer iterative directions during the reverse process of a diffusion model. Specifically, to achieve control over multi-faced objects within the prompts, current methods typically involve the incorporation of prior layout information, either by utilizing the bound boxes as guidance (Lian et al., 2023) or injecting grounding information into new trainable layers (Li et al., 2023b;Chen et al., 2023b). Furthermore, it has been observed that cross-attention primarily governs the handling of object-related information. Consequently,  various approaches have been developed that involve modifying cross-attention mechanisms to ensure that the diffusion model sufficiently attends to all objects specified in the prompts (Feng et al., 2022;Kim et al., 2023b;Chefer et al., 2023). Notably, approaches that combine bounding boxes with attention control have demonstrated improved performance in this regard (Phung et al., 2023;Wang et al., 2023b). \n\nWhat sets our method apart is the central role played by a discriminative Vision-Language Model (VLM) during the image synthesis process, rather than relying on the generative model. Additionally, our approach is entirely training-free and does not necessitate the inclusion of extra information, such as layout details or object indices.",
            "score": 0.5574097426213194,
            "section_title": "Related Work",
            "char_start_offset": 23346,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1822
                },
                {
                    "start": 1825,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2163
                }
            ],
            "ref_mentions": [
                {
                    "start": 807,
                    "end": 828,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 1617,
                    "end": 1635,
                    "matchedPaperCorpusId": "261101003"
                },
                {
                    "start": 1635,
                    "end": 1655,
                    "matchedPaperCorpusId": "256416326"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84814453125
        },
        {
            "corpus_id": "266162752",
            "title": "4M: Massively Multimodal Masked Modeling",
            "text": "Chained generation. Because any generated modality can be used as a conditioning too, we can perform chained generation of several modalities, one after another, with each fully generated one being added to the conditioning of the next (see Figure 3). Performing generation in this chained manner results in each additional modality being generated in a consistent manner, as shown in Figures 9 and 10. In addition, we found that for certain generative tasks, such as caption-to-RGB, generating intermediate modalities such as CLIP tokens can further improve image fidelity (see Figure 9). \n\nClassifier-free guidance. Classifier-free guidance is crucial for improving both image fidelity and how well the generation matches the conditioning. It is most commonly used in diffusion models [50], but can be applied in token-based models as well [40,123,18]. We perform classifier-free guidance by computing a weighted combinations of the logits of a forward pass with the conditioning and one without the conditioning: logits guided = logits uncond + w (logits cond \u2212 logits uncond ) . \n\nHere, w is the guidance scale. When performing chained generation, we add each fully generated modality to the set of guided modalities. \n\nMultimodal guidance. While guidance has been shown to significantly improve image quality, it can still happen that generative models ignore parts of the input, unpredictably focus on some parts more than others, or generate undesired concepts. Negative prompting [78] is a popular way of keeping the model from generating undesired concepts. Liu et al. [68] show that performing compositional guidance on multiple conditions can further improve text-image similarity. In a similar way, we can perform compositional generation by weighting different (parts of) modalities by different continous amounts -even negatively. We can do this by computing a weighted sum of the logits of an unconditional case and the logits of each conditional case: \n\nHere, w i are the guidance scales for the different conditions. For example, this allows 4M to generate semantically or geometrically similar variants of images by weakly conditioning on their extracted segmentation, normal, or depth maps (see Figure 13).",
            "score": 0.5572493602181797,
            "section_title": "A.3 Generation procedure details",
            "char_start_offset": 37472,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1082
                },
                {
                    "start": 1085,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1221
                },
                {
                    "start": 1224,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2225
                }
            ],
            "ref_mentions": [
                {
                    "start": 842,
                    "end": 846,
                    "matchedPaperCorpusId": "247628171"
                },
                {
                    "start": 846,
                    "end": 850,
                    "matchedPaperCorpusId": "249926846"
                },
                {
                    "start": 850,
                    "end": 853,
                    "matchedPaperCorpusId": "255372955"
                },
                {
                    "start": 1578,
                    "end": 1582,
                    "matchedPaperCorpusId": "249375227"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8916015625
        },
        {
            "corpus_id": "267406481",
            "title": "Can Shape-Infused Joint Embeddings Improve Image-Conditioned 3D Diffusion?",
            "text": "To jointly train a conditional and an unconditional model, we replace the input conditioning with a learnable null token \u2205 with probability p. At inference time, we combine the conditional and unconditional predictions at each step t as: \n\nwhere c is the guidance token(s), y \u03b8 is the DDPM, x t is the input at time t of the diffusion process, and w \u2265 1 is the guidance scale. We apply classifier-free guidance with p = 0.1 on E c tokens and E i embeddings independently. \n\nInference is performed by first generating a pure noise sample, and then running 1000 backward diffusion steps. We obtain a tensor representing the output volume that we voxelize through binary thresholding. For conditional generation, we apply classifier-free guidance with a guidance scale of 1.5, as we found it produces the best results.",
            "score": 0.5565927702494694,
            "section_title": "IV. 3D DIFFUSION GENERATION PIPELINE",
            "char_start_offset": 14867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 815
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.697265625
        },
        {
            "corpus_id": "271963450",
            "title": "Alfie: Democratising RGBA Image Generation With No $$$",
            "text": "The generation process of diffusion models can be conditioned on a guidance signal. The most popular diffusion models to date are text-to-image models that can be conditioned on a natural language prompt, embedded by the text encoder of a pre-trained multimodal model (e.g. CLIP [51] or T5 [52]) in a vector e. The conditioning is achieved by performing cross-attention with e inside the noise estimation network. Note that, in this work, we use as backbone a diffusion model trained with the Classifier-Free Guidance [24] conditioning strategy, in which the noise prediction \u03b5\u03b8 is obtained by combining the conditional prediction \u03f5 \u03b8 (x t , e, t) and the unconditional prediction \u03f5 \u03b8 (x t , \u2205, t), where \u2205 is the embedding vector of the null prompt, with weight s as \u03b5\u03b8 (x t , e, t) = \u03f5 \u03b8 (x t , \u2205, t) + s(\u03f5 \u03b8 (x t , e, t) \u2212 \u03f5 \u03b8 (x t , \u2205, t)). \n\nDiffusion Transformers. Diffusion models originally featured a convolutionalattentive U-Net-like model as a noise estimator. The seminal work [48] proposes to replace the U-Net with a multi-block Diffusion Transformer model (DiT) to achieve better performance and scalability. The noise estimation Transformer takes as input a sequence of tokens obtained from squared patches of the latent vectors added to a positional embedding. At each block of the Transformer, the generation is conditioned on the timestep and the class. After the last block, the tokens are decoded and rearranged into the final image spatial dimension. Subsequent works [11,12] adapt the class-guided Diffusion Transformer to work with text guidance. In this work, we exploit the recently-proposed PixArt-\u03a3 Diffusion Transformer [11], which is directly conditioned on the textual prompt.",
            "score": 0.5562530976443253,
            "section_title": "Preliminaries",
            "char_start_offset": 10508,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1707
                }
            ],
            "ref_mentions": [
                {
                    "start": 290,
                    "end": 294,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 518,
                    "end": 522,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 989,
                    "end": 993,
                    "matchedPaperCorpusId": "254854389"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8720703125
        },
        {
            "corpus_id": "270226044",
            "title": "Dreamguider: Improved Training free Diffusion-based Conditional Generation",
            "text": "Generative modeling utilizing Denoising Diffusion Probabilistic Models (DDPMs) [38,19,14,42] has massively improved over the past few years.Multiple works have extended the use of diffusion models for text-to-image synthesis [3,34,36], 3D synthesis [32,22], video generation [18,5,45], as well as for conditioning to solve inverse problems.Moreover, like conditional generative adversarial networks (GANs) [15,2], DDPMs can be adapted to tasks based on a labels [34,14] or visual priorbased conditioning [35].However, like conditional GANs [43,33], DDPMs also need to be trained with annotated pairs of labels and instructions to obtain satisfactory results.This poses a limitation in many cases where there is a lack of paired data to train large diffusion models.Due to this reason, there has been recent interest in models that can perform conditional generation without the need for explicit training [47,6,30,16].\n\nProgressing towards this direction is prior research in plug-and-play models.First introduced in [30], the initial research on plug-and-play models [30,16] enabled conditional sampling from GANs trained with unlabeled data.For this, a pre-trained classifier [37,20] or a captioning model was used to estimate the deviation between the GAN-generated image and a given label, and based on this deviation, the GAN input noise was modulated until the generated sample satisfied the given text or class label.A similar approach that has been attempted for diffusion models to facilitate conditional sampling from unconditional diffusion models is classifier guidance [14,16], where a noise-robust classifier is trained along with the diffusion model to guide the sampling towards a particular direction.However, classifier guidance brings in the computational costs of training a classifier, which is often undesirable.Some recent works have performed conditional generation without explicit training for the condition by utilizing the implicit guidance capabilities of the diffusion model [9,47,29,4,8].",
            "score": 0.555859293606479,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 140,
                    "end": 340
                },
                {
                    "start": 340,
                    "end": 509
                },
                {
                    "start": 509,
                    "end": 658
                },
                {
                    "start": 658,
                    "end": 765
                },
                {
                    "start": 765,
                    "end": 918
                },
                {
                    "start": 920,
                    "end": 997
                },
                {
                    "start": 997,
                    "end": 1143
                },
                {
                    "start": 1143,
                    "end": 1424
                },
                {
                    "start": 1424,
                    "end": 1718
                },
                {
                    "start": 1718,
                    "end": 1834
                },
                {
                    "start": 1834,
                    "end": 2019
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 83,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 83,
                    "end": 86,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 86,
                    "end": 89,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 89,
                    "end": 92,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "233241040"
                },
                {
                    "start": 279,
                    "end": 281,
                    "matchedPaperCorpusId": "258187553"
                },
                {
                    "start": 281,
                    "end": 284,
                    "matchedPaperCorpusId": "254974187"
                },
                {
                    "start": 410,
                    "end": 412,
                    "matchedPaperCorpusId": "2057420"
                },
                {
                    "start": 466,
                    "end": 469,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 504,
                    "end": 508,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 540,
                    "end": 544,
                    "matchedPaperCorpusId": "41805341"
                },
                {
                    "start": 909,
                    "end": 911,
                    "matchedPaperCorpusId": "13997252"
                },
                {
                    "start": 911,
                    "end": 914,
                    "matchedPaperCorpusId": "2023211"
                },
                {
                    "start": 1017,
                    "end": 1021,
                    "matchedPaperCorpusId": "2023211"
                },
                {
                    "start": 1068,
                    "end": 1072,
                    "matchedPaperCorpusId": "2023211"
                },
                {
                    "start": 1182,
                    "end": 1185,
                    "matchedPaperCorpusId": "52947736"
                },
                {
                    "start": 1582,
                    "end": 1586,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63720703125
        },
        {
            "corpus_id": "265039918",
            "title": "A Survey on Generative Diffusion Models",
            "text": "Diffusion models are versatile, capable of generating data samples from both unconditional  0 and conditional  0 (|) distributions, with  as a given condition such as a class label or text linked to data  [36]. The score network   (, , ) integrates this condition during training. Various sampling algorithms, including classifier-free guidance [48] and classifier guidance [27], are designed for conditional generation. Labeled Conditions Sampling with labeled conditions guides each sampling step's gradient. It typically requires an additional classifier with a UNet Encoder architecture to generate condition gradients for specific labels, which can be text, categorical, binary, or extracted features [12,27,28,[49][50][51][52][53][54][55]. The method, first presented by [27], underpins current conditional sampling techniques. Unlabeled Conditions Unlabeled condition sampling uses self-information for guidance, often applied in a selfsupervised manner [56,57]. It is commonly used in denoising [58], paint-to-image [59], and inpainting tasks [17].",
            "score": 0.5557896234169705,
            "section_title": "Conditional Diffusion Probabilistic Models",
            "char_start_offset": 11844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1056
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 710,
                    "end": 713,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 724,
                    "end": 728,
                    "matchedPaperCorpusId": "244896176"
                },
                {
                    "start": 777,
                    "end": 781,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1024,
                    "end": 1028,
                    "matchedPaperCorpusId": "236950721"
                },
                {
                    "start": 1051,
                    "end": 1055,
                    "matchedPaperCorpusId": "235765577"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.486328125
        },
        {
            "corpus_id": "268033671",
            "title": "Diffusion Model-Based Image Editing: A Survey",
            "text": "Class-Conditioned Image Generation. Early efforts [35], [36], [88]- [91] usually incorporate the class-induced gradients via an additional pretrained classifier during sampling. However, Ho et al. [32] introduce the classifier-free guidance, which does not rely on an external classifier and allows for more versatile conditions, e.g., text, as guidance. Text-to-Image (T2I) Generation. GLIDE [38] is the first work that uses text to guide image generation directly from the highdimensional pixel level, replacing the label in class-conditioned diffusion models. Similarly, Imagen [31] uses a cascaded framework to generate high-resolution images more efficiently in pixel space. A different line of research first projects the image into a lower-dimensional space and then applies diffusion models in this latent space. Representative works include Stable Diffusion (SD) [30], VQ-diffusion [92], and DALL-E 2 [29]. Following these pioneering studies, a large number of works [41], [93]- [98] are proposed, advancing this field over the past two years. Additional Conditions. Beyond text, more specific conditions are also used to achieve higher fidelity and more precise control in image synthesis. GLIGEN [99] inserts a gated self-attention layer between the original self-attention and cross-attention layers in each block of a pretrained T2I diffusion model for generating images conditioned on grounding boxes. Make-A-Scene [100] and SpaText [101] use segmentation masks to guide the generation process. In addition to segmentation maps, ControlNet [102] can also incorporate other types of input, such as depth maps, normal maps, canny edges, pose, and sketches as conditions. Other methods [103]- [107] integrate diverse conditional inputs and include additional layers, enhancing the generative process controlled by these conditions. Personalized Image Generation. Closely related to image editing within conditional image generation is the task of creating personalized images. This task focuses on generating images that maintain a certain identity, typically guided by a few reference images of the same subject. Two early approaches are Textual Inversion [108] and DreamBooth [109].",
            "score": 0.5554425891038743,
            "section_title": "Conditional Image Generation",
            "char_start_offset": 13969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2195
                }
            ],
            "ref_mentions": [
                {
                    "start": 50,
                    "end": 54,
                    "matchedPaperCorpusId": "235619773"
                },
                {
                    "start": 56,
                    "end": 60,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 62,
                    "end": 66,
                    "matchedPaperCorpusId": "247763065"
                },
                {
                    "start": 68,
                    "end": 72,
                    "matchedPaperCorpusId": "246442182"
                },
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 393,
                    "end": 397,
                    "matchedPaperCorpusId": "245335086"
                },
                {
                    "start": 891,
                    "end": 895,
                    "matchedPaperCorpusId": "244714856"
                },
                {
                    "start": 976,
                    "end": 980,
                    "matchedPaperCorpusId": "252762155"
                },
                {
                    "start": 988,
                    "end": 992,
                    "matchedPaperCorpusId": "258108187"
                },
                {
                    "start": 1207,
                    "end": 1211,
                    "matchedPaperCorpusId": "255942528"
                },
                {
                    "start": 1429,
                    "end": 1434,
                    "matchedPaperCorpusId": "247628171"
                },
                {
                    "start": 1447,
                    "end": 1452,
                    "matchedPaperCorpusId": "254018089"
                },
                {
                    "start": 1554,
                    "end": 1559,
                    "matchedPaperCorpusId": "256827727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95068359375
        },
        {
            "corpus_id": "256416107",
            "title": "Meta-Learning via Classifier(-free) Diffusion Guidance",
            "text": "A.1. Classifier-Free Guidance \n\nWe hereby provide a rationale for the use of classifier guidance and classifier-free guidance during diffusion model sampling. \n\nAs per the \"score matching\" interpretation of diffusion models, we assume that our trained noise network approximates the score function of the true conditional latent distribution p(z|e i ) as \u03c8 (z t , t, e i ) \u2248 \u2212\u03c3 t \u2207 z t log p(z t |e i ). For classifier guidance, we can perturb our diffusion sampling by adding the gradient of the log likelihood of our CLIP encoder p \u03c8 (e i |z t ) to the diffusion score as follows \n\nWe can rewrite this as classifier guidance on the unconditional score \u2207 z t log p(z t ) with \n\nusing Bayes' rule, as log p(z t |e i ) = log p(e i |z t ) + log p(z t ) \u2212 log p(e i ), and thus \n\nFor classifier-free guidance, we aim to perform the above sampling without access to a classifier, as long we possess a conditional diffusion model \u03c8 (z t , t, e i ) that doubles as an unconditional model \u03c8 (z t , t, 0), as illustrated in Section 5.2. \n\nUsing Bayes' rule again, we can see that \n\n. If we substitute this into Eq. 12 we obtain \n\nwhich can be implemented with our conditional network as",
            "score": 0.5551358918052006,
            "section_title": "A. Appendix",
            "char_start_offset": 30651,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 4
                },
                {
                    "start": 5,
                    "end": 29
                },
                {
                    "start": 32,
                    "end": 158
                },
                {
                    "start": 161,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1071
                },
                {
                    "start": 1074,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1178
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63330078125
        },
        {
            "corpus_id": "257622962",
            "title": "FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model",
            "text": "We propose a training-free energy-guided conditional diffusion model, FreeDoM, to address a wide range of conditional generation tasks without training. Our method uses off-the-shelf pre-trained time-independent networks to approximate the time-dependent energy functions. Then, we use the gradient of the approximated energy to guide the generation process. Our method supports different diffusion models, including image and latent diffusion models. It is worth emphasizing that the applications presented in this paper are only a subset of the applications FreeDoM supports and should not be limited to these. In future work, we aim to explore even more energy functions for a broader range of tasks. \n\nDespite its merits, our FreeDoM method has some limitations: (1) The time cost of the sampling is still higher than the training-required methods because each iteration adds a derivative operation for the energy function, and the timetravel strategy introduces more sampling steps. (2) It is difficult to use the energy function to control the fine-grained structure features in the large data domain. For example, using the Canny edge maps as the conditions may result in poor guidance, even if we use the time-travel strategy. In this case, the training-required methods will provide a better alternative. (3) Eq. 12 deals with multi-condition control and assumes that the provided conditions are independent, which is not necessarily true in practice. When conditions conflict with each other, FreeDoM may produce subpar generation results.",
            "score": 0.5549556377354968,
            "section_title": "Conclusions & Limitations",
            "char_start_offset": 26803,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 703
                },
                {
                    "start": 706,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1549
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1514892578125
        },
        {
            "corpus_id": "272368110",
            "title": "Guide-and-Rescale: Self-Guidance Mechanism for Effective Tuning-Free Real Image Editing",
            "text": "Classifier guidance relies on a separate model, classifier p(y|z t ), trained on noised latents. As long as this classifier is differentiable with respect to the current latent, we can subtract score function \u2207 zt log p(y|z t ) from the diffusion model prediction to sample from the conditional distribution p(z t |y) instead of the data distribution p(z t ). \n\nTraining a separate classifier on specific data may sometimes be a resourceful task. Given a conditional diffusion model, one can rely on this model's knowledge about additional data, that it inquires with conditioning capabilities. This guidance technique is called classifier-free guidance (CFG) [8]. To determine the sampling direction, which leads to correspondence with conditioning data y, CFG compares a conditional prediction of the model with an unconditional one. In case of text-to-image models, the latter can be easily obtained by conditioning the model on the empty text \u2205 = \"\". With CFG incorporated, the diffusion model prediction, used in both sampling, Equation 1, and inversion, Equation 2, takes the form of \u03b5\u03b8 (z t , t, y): \n\nwhere w is a guidance scale, that controls to which extent additional data y influences the generation process. For SD model, the guidance scale is typically chosen as w = 7.5. It is important to note, that with w = 1 CFG sampling step equals regular conditional sampling. Self-guidance. As proposed in [4], the choice in guidance sources is not limited to either the classifier or the diffusion model itself. One can use any energy function g to guide the sampling process, as long as there exists a gradient with respect to z t . When the energy function uses outputs of internal layers of the diffusion model, the guidance process is called self-guidance. The authors of the method suggest defining energy function g on top of cross-attention maps A cross := cross attn.[\u03b5 \u03b8 (z t , t, y)] and the output of penultimate layer of the diffusion model decoder, features \u03a8 := features[\u03b5 \u03b8 (z t , t, y)].",
            "score": 0.5549421474219158,
            "section_title": "Preliminaries",
            "char_start_offset": 11895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 359
                },
                {
                    "start": 362,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2010
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "265295505",
            "title": "A Survey of Emerging Applications of Diffusion Probabilistic Models in MRI",
            "text": "DDIM [16] provided a way of conditional generation through deterministic sampling of noisy hidden variables, and the score-based SDE [24] pointed out that conditional generation can be achieved by solving a conditional reverse-time SDE and provided three examples of controllable generation, which opened up the study of conditional generation. The guided-DPMs [28] then proposed training a noisy image classifier q(y|x t ) to control the generation of samples conditioned on the category y, using the gradient \u2207 xt log q(y|x t ) with intensity \u03b3. In contrast, the author in [29] highlighted that category guidance can be achieved by introducing the condition y during the training of diffusion probabilistic models, which is an implicit way of constructing a classifier that could adopt data pairs of conditional and perturbed images. Furthermore, recent works [30,31,32] extended category conditions to encompass image, text, and multi-modal conditional generations. As another representative approach for conditional generation, the Latent Diffusion Probabilistic Models (LDMs) [33] considered constructing a pre-trained Encoder-Decoder and used DPMs to generate the hidden variables at the bottleneck, which reduced the computational complexity of DPMs and made it possible for conditional operations in the latent space.",
            "score": 0.554777946673674,
            "section_title": "Conditional DPMs",
            "char_start_offset": 16051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1325
                }
            ],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 365,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 866,
                    "end": 869,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 869,
                    "end": 872,
                    "matchedPaperCorpusId": "245117331"
                },
                {
                    "start": 1081,
                    "end": 1085,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10467529296875
        },
        {
            "corpus_id": "278368381",
            "title": "CountDiffusion: Text-to-Image Synthesis with Training-Free Counting-Guidance Diffusion",
            "text": "In 2022, Ho et al. [30] proposed Classifier-Free Diffusion Guidance, theoretically proving that the diffusion model does not require a classifier. Since then, the diffusion model has firmly established its position in the field of T2I generation. \n\nUniversal guidance: Universal guidance [31] is a guidance algorithm that augments the image sampling method of a diffusion model to include guidance from an off-the-shelf auxiliary network. It introduces a guidance loss during the backward process of the diffusion model, where the loss is back-propagated a few times at each step to achieve guided image generation. Compared to training-based algorithms, this algorithm only adds a small amount of inference time but achieves comparable results in fine-tuning and continued training. It performs well in various domains such as style transfer [32], conditional generation [9], and image editing [33].",
            "score": 0.5546532526634745,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 8181,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 246
                },
                {
                    "start": 249,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 900
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 843,
                    "end": 847,
                    "matchedPaperCorpusId": "257427673"
                },
                {
                    "start": 872,
                    "end": 875,
                    "matchedPaperCorpusId": "259991581"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "269587736",
            "title": "Defect Image Sample Generation With Diffusion Prior for Steel Surface Defect Recognition",
            "text": "Denote \u03b8 * as the optimized network parameter DATA GENERATION // Image-oriented Generation Sample x \u223c p(x), \u03f5 \u223c N (0, I)\n\ntop of the transition kernel mentioned in the previous section, diffusion models balance the fidelity and diversity of this conditional generation using classifier-free guidance [56]:\n\nwhere \u03f5 \u03b8 (z t ; t) is the prediction of noise without text guidance, and \u03c9 cf g is a scalar that adjusts the influence of the condition on the generative process.Generating images in Stable Diffusion is concluded as iterating [47], which starts from pure Gaussian z T and ends in z 0 that can be decoded to image via x = D(z 0 ).",
            "score": 0.5543454234732308,
            "section_title": "A. Preliminary",
            "char_start_offset": 18678,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 122,
                    "end": 305
                },
                {
                    "start": 307,
                    "end": 470
                },
                {
                    "start": 470,
                    "end": 637
                }
            ],
            "ref_mentions": [
                {
                    "start": 534,
                    "end": 538,
                    "matchedPaperCorpusId": "219955663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27197265625
        },
        {
            "corpus_id": "268510449",
            "title": "Denoising Task Difficulty-based Curriculum for Training Diffusion Models",
            "text": "Here, we provide experimental setups concisely. Detailed setups are presented in Appendix E. \n\nEvaluation protocols. For our comprehensive evaluation of various methods, we employed three distinct image-generation tasks: 1) Unconditional generation with the FFHQ dataset (Karras et al., 2019), 2) Class-conditional generation with CIFAR-10 ( Krizhevsky et al., 2009) and Ima-geNet (Deng et al., 2009) datasets, and 3) Text-to-Image generation with MS-COCO dataset (Lin et al., 2014). In 2) and 3) setups, we applied classifier-free guidance (Ho & Salimans, 2022). Target models. We employed three exemplary diffusion architectures for experiments: DiT (Peebles & Xie, 2022), which integrates latent diffusion models (Rombach et al., 2022) with Transformer architectures (Vaswani et al., 2017) parameterized as \u03f5-prediction, EDM (Karras et al., 2022), which focuses on pixel-level diffusion utilizing UNet-based architectures (Ronneberger et al., 2015) parameterized as F -prediction, and SiT (Ma et al., 2024) for score-and velocity-prediction. For the text-to-image generation, we incorporated a CLIP text encoder (Radford et al., 2021) as described in DTR (Park et al., 2024b).",
            "score": 0.5541721026780311,
            "section_title": "EXPERIMENTAL SETUP",
            "char_start_offset": 25395,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 92
                },
                {
                    "start": 95,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1179
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 292,
                    "matchedPaperCorpusId": "54482423"
                },
                {
                    "start": 381,
                    "end": 400,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 464,
                    "end": 482,
                    "matchedPaperCorpusId": "14113767"
                },
                {
                    "start": 828,
                    "end": 849,
                    "matchedPaperCorpusId": "249240415"
                },
                {
                    "start": 925,
                    "end": 951,
                    "matchedPaperCorpusId": "3719281"
                },
                {
                    "start": 1115,
                    "end": 1137,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1158,
                    "end": 1178,
                    "matchedPaperCorpusId": "263835004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7490234375
        },
        {
            "corpus_id": "265466084",
            "title": "Predicated Diffusion: Predicate Logic-Based Attention Guidance for Text-to-Image Diffusion Models",
            "text": "Training-Free Guidance Even when a diffusion model is designed without condition c, it can reproduce the conditional probability p(x|c) without retraining. This is because, from a diffusion model p(x) and a separate classifier p(c|x) for class label c, one can obtain the gradient of the conditional log-probability, \u2207 x log p(x|c) = \u2207 x log p(c|x) + \u2207 x log p(x). Although grounded in probability theory, what it practically offers is additional guidance \u2207 x log p(c|x) for updating images, which is generalized as classifier guidance [3]. \n\nConsider the reverse process modeled as a Gaussian distribution p( \n\nwhere the parameters are determined by neural networks \u00b5 \u03b8 and \u03a3 \u03b8 . A guidance method can be defined as a method that introduces an adjustment g(x t , t) to the update in the reverse process as \n\nWhen the diffusion model is conditioned on c, the difference between conditional and unconditional updates serves as classifier-free guidance, which can adjust the fidelity of the generated image to condition c [12]. Liu et al. [18] proposed Composable Diffusion, inspired by energy-based models [5]. It generates an image conditioned on two concepts, c 0 and c 1 , by summing their respective conditional updates. It negates or removes a concept c n from generated images by subtracting the update conditioned on c n , termed as a negative prompt. Some studies developed guidance using annotations, such as bounding boxes [19,20,40] and segmentation masks [23]. While effective in intentionally controlling image layout, guidance methods based on annotations sometimes limit the diversity of generated images. \n\nAttention Guidance Other previous studies developed guidance methods using the attention maps of the crossattention mechanism, termed as attention guidance. High pixel intensity in an attention map A w suggests the presence of the corresponding object or concept w at that pixel. Attend-and-Excite enhances the intensity of at least one pixel in the attention map A w to ensure the existence of the corresponding object w (that is, address missing objects) [2].",
            "score": 0.5539278102987983,
            "section_title": "Related Work",
            "char_start_offset": 6145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1619
                },
                {
                    "start": 1622,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 536,
                    "end": 539,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1020,
                    "end": 1024,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1037,
                    "end": 1041,
                    "matchedPaperCorpusId": "249375227"
                },
                {
                    "start": 1105,
                    "end": 1108,
                    "matchedPaperCorpusId": "214223619"
                },
                {
                    "start": 1439,
                    "end": 1442,
                    "matchedPaperCorpusId": "259991581"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9189453125
        },
        {
            "corpus_id": "277272356",
            "title": "Guidance Free Image Editing via Explicit Conditioning",
            "text": "Diffusion models (Karras et al., 2022;2024) have made great progress in image generation abilities, demonstrating impressive results for conditional generation. Currently, all state-of-the-art (SOTA) conditional diffusion systems leverage Classifier Free Guidance (CFG) (Ho & Salimans, 2022;Zheng et al., 2023) to yield diverse high-quality conditional image generation, at the cost of the increased number of the denoising passes of the learned neural network models. Specifically, the image editing task is conditioned on a given image and an instruction prompt. Applying CFG for image editing requires three denoising passes to yield plausible results, see Fig. 1 (d), (e), and (f). \n\nInformally, the formulation of CFG is based on two premises (Ho & Salimans, 2022;Zheng et al., 2023;Brooks et al., 2022). First, there exists conditional information that is not natively supported by the underlying generative modeling. Second, the score, i.e., the gradient of the log of the density of conditional generative modeling may be aided by the score of unconditional generative modeling. More formally, CFG can be seen as predictor-corrector (Bradley & Nakkiran, 2024). CFG boosts conditional image generation significantly across diffusion and flow (Lipman et al., 2023;Tong et al., 2024) models alike. This has led to a surge of research on different ways to combine not just conditional and unconditional models but more generally two or more models (Sadat et al., 2024). \n\nThe main practical drawback of CFG lies in the need for multiple conditional and unconditional denoising passes, resulting in additional computation costs. To alleviate the computational cost, a student model is distilled to mimic in one pass the original teacher sampling mechanism requiring multiple passes. CFG distillation, however, imposes a cumbersome extra training stage and is prone to lag behind the teacher's performance (Meng et al., 2023).",
            "score": 0.55345287654051,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1928
                }
            ],
            "ref_mentions": [
                {
                    "start": 1141,
                    "end": 1167,
                    "matchedPaperCorpusId": "271903235"
                },
                {
                    "start": 1270,
                    "end": 1288,
                    "matchedPaperCorpusId": "259847293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89794921875
        },
        {
            "corpus_id": "269502576",
            "title": "On Mechanistic Knowledge Localization in Text-to-Image Generative Models",
            "text": "During the inference process, the regulation of image generation involves the utilization of classifier-free guidance, as outlined in Ho & Salimans (2021) which incorporates scores from both the conditional and unconditional diffusion models at each time-step.Specifically, the classifier-free guidance is applied at each time-step to combine the conditional (\u03f5 \u03b8 (z t , c, t)) and unconditional score estimates (\u03f5 \u03b8 (z t , t)).\n\nThe result is a combined score denoted as \u03b5(z t , c, t).\n\n\u03b5(z t , c, t) = \u03f5 \u03b8 (z t , c, t) + \u03b1 (\u03f5 \u03b8 (z t , c, t) \u2212 \u03f5 \u03b8 (z t , t)) , \u2200t \u2208 [T, 1] .\n\n(1) This combined score is used to update the latent z t using DDIM sampling (Song et al., 2020) at each time-step to obtain the final latent code z 0 .We term the model \u03f5 \u03b8 (z t , c, t) as the Clean Model and the final image generated as I clean .We note that text is incorporated in the process of generation using cross-attention layers denoted by {C l } M l=1 within \u03f5 \u03b8 (z t , c, t) \u2200t \u2208 [T, 1].These layers include key and value matrices -{W K l , W V l } M l=1 that take text-embedding c of the input prompt and guide the generation toward the text prompt.Generally, the text-embedding c is same across all these layers.However, in order to localize and find control points for different visual attributes, we replace the original text-embedding c with a target prompt embedding c \u2032 across a small subset of the cross-attention layers and measure its direct effect on the generated image.",
            "score": 0.5533552017733315,
            "section_title": "Knowledge Control in Cross-Attention Layers",
            "char_start_offset": 8797,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 260,
                    "end": 428
                },
                {
                    "start": 430,
                    "end": 486
                },
                {
                    "start": 488,
                    "end": 575
                },
                {
                    "start": 577,
                    "end": 729
                },
                {
                    "start": 729,
                    "end": 825
                },
                {
                    "start": 825,
                    "end": 977
                },
                {
                    "start": 977,
                    "end": 1140
                },
                {
                    "start": 1140,
                    "end": 1204
                },
                {
                    "start": 1204,
                    "end": 1472
                }
            ],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 154,
                    "matchedPaperCorpusId": "249145348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93798828125
        },
        {
            "corpus_id": "274422874",
            "title": "Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling",
            "text": "Guidance with trained weak model Classifier-Free Guidance (CFG) [10] improves conditional generation in Diffusion Models by using an implicit unconditional model as a weak model. However, differences in tasks between the unconditional and conditional models can reduce sample diversity [6,19] and increase sampling trajectory curvature [6], leading to overshooting the data manifold and producing skewed or oversaturated images. Autoguidance [19] mitigates these issues by employing a bad version of the main model as a weak model, trained with reduced capacity and compute to ensure alignment. This alignment allows the guidance algorithm to correct errors by analyzing prediction differences. While effective, it requires additional training, which is challenging for largescale video diffusion models. \n\nGuidance with training-free weak model Another line of work avoids additional training by using selfperturbation of the main model to mimic a weak model. Self-Attention Guidance (SAG) [14] blurs high-attention regions, Perturbed Attention Guidance (PAG) [1] replaces attention maps with identity matrices, and Smoothed Energy Guidance (SEG) [13] applies Gaussian blur to the attention weights to smooth the energy landscape. These methods guide sampling toward high-quality outputs by leveraging differences in predictions from the weakened model. While effective, they are primarily designed for image diffusion models with 2D self-attention. We aim to extend this approach to video diffusion models, which require handling temporal dynamics with additional temporal or 3D spatiotemporal attention layers.",
            "score": 0.5532949340650533,
            "section_title": "Related Work",
            "char_start_offset": 5061,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 804
                },
                {
                    "start": 807,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1613
                }
            ],
            "ref_mentions": [
                {
                    "start": 991,
                    "end": 995,
                    "matchedPaperCorpusId": "252683688"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59716796875
        },
        {
            "corpus_id": "259341599",
            "title": "ProtoDiffusion: Classifier-Free Diffusion Guidance with Prototype Learning",
            "text": "where \u03b8 is the function approximator to predict the value added to obtain x t . The simplified training objective is derived as in (Ho et al. (2020)): \n\nWhile the DDPM synthesizes unconditional images, guided diffusion models are also available for conditional image generation. Dhariwal and Nichol (2021) propose classifier guidance where the class conditional parameters \u00b5 \u03b8 (x t |y) and \u03a3 \u03b8 (x t |y) are perturbed by the gradients of a classifier p \u03c6 (y|x t ) that predicts the target class y. The perturbed mean with the guidance scale s is derived as: \n\nEven though the classifier guidance improves the quality of the images, there are a few problems of the classifier guidance. As the denoising process starts with highly noised input and proceeds with noisy images at most of the time steps, the classifier should be robust to noise. While obtaining such a classifier is challenging, predicting a class label does not require using the most of the information in the data. Therefore, taking the gradients of such a classifier might mislead the generation direction. Ho and Salimans (2021) propose classifier-free guidance method that does not require a separate classifier. The conditioning information y is periodically utilized while it is dropped out at the remaining time. Therefore, a single model can be used for both unconditional and conditional generation. Ho and Salimans (2021) derive that the unconditional \u03b8 (x t , t) and conditional \u03b8 (x t , t, y) estimations can be used to represent the gradients of the classifier as: \n\nEquation 7 implies that an implicit classifier can eliminate the need for an explicit classifier, and Ho and Salimans (2021) report better results with the classifier-free guidance compared to the explicit classifier guidance. \n\nIn our work, we enhance the classifier-free guidance method with prototype learning in order to improve the performance while decreasing the training time.",
            "score": 0.5532888231504142,
            "section_title": "Diffusion Models",
            "char_start_offset": 11243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 150
                },
                {
                    "start": 153,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 556
                },
                {
                    "start": 559,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1541
                },
                {
                    "start": 1544,
                    "end": 1770
                },
                {
                    "start": 1773,
                    "end": 1928
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 148,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 279,
                    "end": 305,
                    "matchedPaperCorpusId": "234357997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95849609375
        },
        {
            "corpus_id": "267782749",
            "title": "T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching",
            "text": "Recent works have demonstrated that classifier-free guidance provides a clear improvement in generation quality. In this work, we consider the diffusion models that are trained with classifier-free guidance due to their popularity.",
            "score": 0.5528985147593463,
            "section_title": "Preliminary",
            "char_start_offset": 12554,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 231
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.252685546875
        },
        {
            "corpus_id": "276287597",
            "title": "Greed is Good: A Unifying Perspective on Guided Generation",
            "text": "Guided generation greatly extends the utility of state-of-the-art generative models by allowing the end user to exert greater control over the generative process, ultimately making the tool more useful in a wide variety of applications ranging from conditional generation, editing of samples, inverse problems &c. We focus particularly on a subset of neural differential equations that model affine probability paths, in other words, diffusion and flow models due to their widespread adoption in a large variety of practical tasks. E.g., audio (H. Liu et al. 2023;Schneider et al. 2024), images (Rombach et al. 2022;Black Forest Labs 2024), biometrics (Blasingame and C. Liu 2024c), molecules (Hoogeboom et al. 2022;Ben-Hamu et al. 2024), proteins (Watson et al. 2023;Skreta et al. 2025), &c. \n\nWe can divide the guided generation techniques into two broad categories: conditional training and training-free methods. The former of these two requires the training of the underlying diffusion/flow model on additional conditional information, either as a part of the training or at a later time as additional fine-tuning (Ho and Salimans 2021;J. Song, Meng, and Ermon 2021;Hu et al. 2022). The latter category instead makes use of some known guidance function defined on the data distribution and incorporates this information back to the model to influence the generative process. These training-free techniques can be further broken down into two sub-categories, i.e., posterior and endto-end guidance. The former class of techniques uses a simple estimation of the posterior distribution that can be easily found in diffusion models (Chung, J. Kim, et al. 2023) and some flow models (cf . Lipman, Havasi, et al. 2024, Section 4.8). This simple posterior estimate can then be fed into a guidance function to construct a gradient w.r.t. to the current timestep. We refer to this category as posterior guidance as they use this posterior estimate to perform the guidance process.",
            "score": 0.5528494170674848,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1977
                }
            ],
            "ref_mentions": [
                {
                    "start": 564,
                    "end": 586,
                    "matchedPaperCorpusId": "271923420"
                },
                {
                    "start": 595,
                    "end": 616,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 693,
                    "end": 716,
                    "matchedPaperCorpusId": "247839510"
                },
                {
                    "start": 716,
                    "end": 737,
                    "matchedPaperCorpusId": "267770268"
                },
                {
                    "start": 748,
                    "end": 768,
                    "matchedPaperCorpusId": "271161349"
                },
                {
                    "start": 768,
                    "end": 787,
                    "matchedPaperCorpusId": "274992495"
                },
                {
                    "start": 1119,
                    "end": 1141,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1141,
                    "end": 1171,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 1171,
                    "end": 1186,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1634,
                    "end": 1661,
                    "matchedPaperCorpusId": "252596252"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08465576171875
        },
        {
            "corpus_id": "259243610",
            "title": "DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability",
            "text": "Our key use case for generative models is conditional sampling with one or more constraints. When conditionally sampling a diffusion model, one can use classifier-based [29] or classifier-free [30] guidance. Classifier-based guidance uses the gradient of a classifier to bias the sampling of an unconditional diffusion model. In contrast, the classifierfree guidance doesn't require another model but assumes knowledge of all conditions at training time. \n\nWe hope to achieve compositional generality by allowing the model to work on new tasks when given new constraints. Namely, we don't assume that all potential constraint types are known when training the models, but would like to compose new constraints directly with the existing models. Thus, we opt to use classifier-based guidance as it allows us to combine the unconditional diffusion model with new classifiers after it is trained. \n\nFor classifier guidance, as shown by Dhariwal et al. [29], the denoising procedure can be approximated as \n\nwhere \n\n)) is the gradient from a classifier p \u03d5 (\u2022), that models the likelihood of sample x (t) having property y, to bias the sampling. This sampling process can be significantly more efficient than rejection sampling if one's goal is to get x (0) that has property y.",
            "score": 0.5528160415510803,
            "section_title": "B. Conditional Diffusion Sampling",
            "char_start_offset": 12180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 893
                },
                {
                    "start": 896,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1009
                },
                {
                    "start": 1012,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1274
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68017578125
        },
        {
            "corpus_id": "269043390",
            "title": "An Overview of Diffusion Models: Applications, Guided Generation, Statistical Rates and Optimization",
            "text": "For example, [60][61][62][63] extend diffusion models to discrete data generation, while the vanilla diffusion models target at continuous data.Meanwhile, there is an active line of research aiming to expedite the sample generation speed of diffusion models [64][65][66][67][68][69][70][71][72][73][74].Last but not the least, a recent surge of research focuses on fine-tuning diffusion models towards generating samples of desired properties, such as generating images with peculiar aesthetic qualities [75][76][77][78][79][80][81][82][83].These task-specific properties are often encoded as guidance to the diffusion model, consisting of conditioning and control signals to steer the sample generation.Notably, guidance allows for the creation of diverse and relevant content across a wide range of applications, which underscores the versatility and adaptability of diffusion models.We term diffusion models with guidance as conditional diffusion models.\n\nDespite the rapidly growing body of empirical advancements, theories of diffusion models fall far behind.Some recent theories view diffusion models as an unsupervised distribution learner and sampler, and thus establish their sampling convergence guarantees [84][85][86][87][88][89] and statistical distribution learning guarantees [90][91][92].Such results offer invaluable theoretical insights into the efficiency and accuracy of diffusion models for modeling complex data, with a central focus on the unconditioned diffusion models in distribution estimation.This leaves a gap between theory and practice for conditional diffusion models.In specific, a theoretical foundation to support and motivate principled methodologies for guidance design and adapting diffusion models to task-specific needs is still lacking.\n\nThis paper serves as a contemporary exposure to diffusion models for stimulating sophisticated and forward-looking study on them.We mainly focus on the following fundamental theoretical questions of diffusion models:\n\n\u2022 Can diffusion models learn data distributions accurately and efficiently?If so, what is the sample complexity, especially for structured data?\n\n\u2022 Can conditional diffusion models generate distributions aligned to guidance?If so, how can we properly design the guidance and what is the sample complexity?",
            "score": 0.5524074371543306,
            "section_title": "Introduction",
            "char_start_offset": 1626,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 303
                },
                {
                    "start": 303,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 704
                },
                {
                    "start": 704,
                    "end": 886
                },
                {
                    "start": 886,
                    "end": 957
                },
                {
                    "start": 959,
                    "end": 1064
                },
                {
                    "start": 1064,
                    "end": 1304
                },
                {
                    "start": 1304,
                    "end": 1521
                },
                {
                    "start": 1521,
                    "end": 1600
                },
                {
                    "start": 1600,
                    "end": 1777
                },
                {
                    "start": 1779,
                    "end": 1908
                },
                {
                    "start": 1908,
                    "end": 1995
                },
                {
                    "start": 1997,
                    "end": 2072
                },
                {
                    "start": 2072,
                    "end": 2141
                },
                {
                    "start": 2143,
                    "end": 2221
                },
                {
                    "start": 2221,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 17,
                    "matchedPaperCorpusId": "235262511"
                },
                {
                    "start": 17,
                    "end": 21,
                    "matchedPaperCorpusId": "235755106"
                },
                {
                    "start": 21,
                    "end": 25,
                    "matchedPaperCorpusId": "244714856"
                },
                {
                    "start": 258,
                    "end": 262,
                    "matchedPaperCorpusId": "219708245"
                },
                {
                    "start": 266,
                    "end": 270,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 274,
                    "end": 278,
                    "matchedPaperCorpusId": "249282317"
                },
                {
                    "start": 282,
                    "end": 286,
                    "matchedPaperCorpusId": "249240415"
                },
                {
                    "start": 532,
                    "end": 536,
                    "matchedPaperCorpusId": "246823323"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.041473388671875
        },
        {
            "corpus_id": "254408758",
            "title": "SINE: SINgle Image Editing with Text-to-Image Diffusion Models",
            "text": "With the above-presented LDMs, we introduce our approach, inspired by classifier-free guidance, to overcome overfitting when fine-tuning LDMs with one image. Classifier-free guidance [15] is a technique widely adopted by prior text-to-image diffusion models [32,35]. A single diffusion model is trained using conditional and unconditional objectives by randomly dropping the condition during training. When sampling, a linear combination of the During sampling, we give additional language guidance about the target domain to edit the image. Also, we sample a noisy latent code zT with the dimension corresponding to the desired output resolution. Language conditioning for \u03f5 \u03b8 and c are given by pre-trained language encoder \u03c4 \u03b8 with the target language guidance. While for the fine-tuned diffusion model, \u03b5\u03b8 , in addition to the language conditioning \u0109, we also input the positional embedding for the whole image. We employ a linear combination between the score calculated by each model for the first K steps and inference only on pre-trained \u03f5 \u03b8 after. \n\nconditional and unconditional score estimation is used: \n\nwhere \u03f5 \u03b8 (z t , c) and \u03f5 \u03b8 (z t ) are the conditional and unconditional \u03f5-predictions, c is the conditioning vector generated by \u03c4 \u03b8 , and w is the weight for the guidance. The predication is performed using the Tweedie's formula [7], namely, z t \u2212 \u221a 1 \u2212 \u1fb1t \u03b5\u03b8 / \u221a \u1fb1t , where \u1fb1t is a function of t that affects the sampling quality. \n\nSince we only have one image as the training data, e.g., painting of Mona Lisa, and one corresponding text descrip-tor of that image, the diffusion model suffers from overfitting, and severe language drifts after fine-tuning [33]. As a result, the fine-tuned model fails to synthesize images containing features from other language guidance. The overfitting issue might be due to only one repeated prompt used during fine-tuning, making other text prompts no longer accurate enough to control editing (see examples in Fig. 6). Model-based classifier-free guidance.",
            "score": 0.5522403638268103,
            "section_title": "Model-Based Classifier-Free Guidance",
            "char_start_offset": 10673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1114
                },
                {
                    "start": 1117,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 258,
                    "end": 262,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 262,
                    "end": 265,
                    "matchedPaperCorpusId": "248986576"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9267578125
        },
        {
            "corpus_id": "272987032",
            "title": "Simple and Fast Distillation of Diffusion Models",
            "text": "Stable Diffusion [41], a latent diffusion model combined with classifier-free guidance [13], has shown to be highly effective in high-resolution image generation. The classifier-free guidance extends the flexibility of the generation of diffusion models by introducing the guidance scale \u03c9. Given a conditioning information c, the noise-prediction model is rewritten as \n\nHowever, Stable Diffusion requires a large number of network parameters and sampling steps to produce a satisfying generation. Moreover, the cost of every sampling step doubles since both conditional and unconditional evaluations are involved in Eq. 9. Distilling the Stable Diffusion model into a few steps is challenging because of source-intensive requirements and the flexibility given by the guidance scale. To address this issue, existing methods either introduce an \u03c9-condition into their model [34,22,32], or simply discard the guidance scale [55,46]. \n\nHere, we propose a new strategy with the observation on the sampling trajectories generated by Stable Diffusion under different guidance scales. Following the three-dimensional projection technique proposed in [4], we visualize the sampling trajectories generated by Stable Diffusion in its latent space using DPM-Solver++(2M) starting from 20 fixed latent encodings. As shown in Figure 9, sampling trajectories projected to the three-dimensional subspace exhibit a regular boomerang shape, which is consistent with the findings in the previous work [4]. Furthermore, we observe that the sampling trajectories become more complex as the guidance scale increases, making trajectory distillation on the high guidance scale even more challenging. This observation naturally leads to our strategy: perform distillation with a guidance scale of 1 and sampling with any guidance scale. Our strategy enables accelerated training since the unconditional evaluation in Eq. 9 is eliminated.",
            "score": 0.5519025320990953,
            "section_title": "Distillation under Classifier-free Guidance",
            "char_start_offset": 18937,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 21,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 874,
                    "end": 878,
                    "matchedPaperCorpusId": "252762155"
                },
                {
                    "start": 878,
                    "end": 881,
                    "matchedPaperCorpusId": "258999690"
                },
                {
                    "start": 1144,
                    "end": 1147,
                    "matchedPaperCorpusId": "269921711"
                },
                {
                    "start": 1484,
                    "end": 1487,
                    "matchedPaperCorpusId": "269921711"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81591796875
        },
        {
            "corpus_id": "260091821",
            "title": "Diffusion Sampling with Momentum for Mitigating Divergence Artifacts",
            "text": "Guided diffusion sampling is a widely used technique for conditional sampling, such as text-to-image and class-to-image generation. There are main two approaches for guided sampling: \n\nClassifier guidance [5,2] uses a pre-trained classifier model p \u03d5 (c | x t , t) to define the conditional noise prediction model at inference time: \n\nwhere s > 0 is a \"guidance\" scale. The model can be extended to accept any guidance function, such as CLIP function [28] for text-to-image generation [29]. This approach only modifies the sampling equation at inference time and thus can be applied to a trained diffusion model without retraining. \n\nClassifier-free guidance, proposed by Ho et al. [30], trains a conditional noise model \u03f5 \u03b8 (x t , t | c) to generate data samples with the label c: \n\nwhere \u03d5 is a null label to allow for unconditional sampling. The sampling equations in both approaches can be expressed as a \"guided ODE\" of the form \n\nwhere g(x, \u03c3) represents a guidance function. To accelerate guided diffusion sampling, splitting numerical methods have been proposed, such as Lie-Trotter Splitting (LTSP) [26]. This method divides Equation 6 into two subproblems, i) dy d\u03c3 = \u03b5(y, \u03c3) and ii) dz d\u03c3 = g(z, \u03c3), but can only apply high-order numerical methods to the first equation while resorting to the Euler method for the second equation to avoid numerical instability. Higher-order splitting methods, such as Strang Splitting (STSP) [26], are also able to mitigate artifacts. However, these methods require solving the second equation twice per step, which is comparable to increasing the total sampling step to avoid artifacts. Both approaches require non-negligible computation.",
            "score": 0.5518879455011447,
            "section_title": "Guided Diffusion Sampling",
            "char_start_offset": 5986,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1684
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 208,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 208,
                    "end": 210,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 451,
                    "end": 455,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 682,
                    "end": 686,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1108,
                    "end": 1112,
                    "matchedPaperCorpusId": "256358823"
                },
                {
                    "start": 1437,
                    "end": 1441,
                    "matchedPaperCorpusId": "256358823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78662109375
        },
        {
            "corpus_id": "275920723",
            "title": "CE-SDWV: Effective and Efficient Concept Erasure for Text-to-Image Diffusion Models via a Semantic-Driven Word Vocabulary",
            "text": "Ho et al. [21] assume that \u03a3 \u03b8 is not learnable and reparameterize the \u00b5 \u03b8 (x t , t) based on Equation 6and Equation 8: \n\nThe simple loss function can be expressed as follows: To achieve more controllable image generation, conditional diffusion models incorporate additional inputs, such as text conditions [45] and segmentation map [64]. Classifier-free guidance [20] aims to find a x that maximizes log p(c|x). Using Bayes' theorem, the model \u03b5\u03b8 (x t , t, c) can be modified as follows: \u03b5\u03b8 (x t , t, c) \u221d s \u2022 \u03f5 \u03b8 (x t , t, c) + (1 \u2212 s) \u2022 \u03f5 \u03b8 (x t , t, \u2205), (11) where s represents the guidance scale and c = \u2205 denotes unconditional generation by DPMs.",
            "score": 0.5515509263798525,
            "section_title": "E. Preliminaries of Diffusion Models",
            "char_start_offset": 46998,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 122,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 652
                }
            ],
            "ref_mentions": [
                {
                    "start": 307,
                    "end": 311,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 333,
                    "end": 337,
                    "matchedPaperCorpusId": "256827727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75439453125
        },
        {
            "corpus_id": "259309051",
            "title": "Counting Guidance for High Fidelity Text-to-Image Synthesis",
            "text": "Diffusion models [3,9,33,37,38] are a new family of generative models that have significantly improved the performance of image synthesis and text-to-image generation. DDPM [9] defined diffusion as a Markov chain process by gradually adding noise, showing the potential of diffusion models for unconditional image generation. Simultaneously, [38] interpreted diffusion models as Stochastic Differential Equations, providing broader insights into their function. One of the problems with DDPM is that it depends on probabilistic sampling and requires about 1,000 steps to obtain high-fidelity results, making the sampling process very slow and computationally intensive. To alleviate this problem, DDIM [36] removed the probabilistic factor in DDPM and achieved comparable image quality to DDPM with only 50 denoising steps. \n\nBeyond unconditional image generation, recent papers on diffusion models also started to focus on conditional image generation. [3] suggested classifier guidance by calculating the gradient of a classifier to perform conditional image generation. However, this method requires a noiseaware classifier and per-step gradient calculation. To avoid this problem, [10] proposed classifier-free guidance, which removes the need for an external classifier by computing \"ten apples on the table\" \"fifty apples on the table\" each denoising step as an extrapolation, requiring one conditional and one unconditional step. Furthermore, Control-Net [47] proposed a separate control network attached to a pre-trained diffusion model to perform guidance with additional input with feasible training time. Universal Guidance [1] alleviates the problem of requiring a noise-aware classifier by instead calculating the gradient of the predicted clean data point. \n\nOne issue of diffusion models is the high inference cost because of repeated inference in pixel-space. To address this problem, Stable Diffusion [33] proposed performing the diffusion process in a low dimensional latent space instead of image space, greatly reducing the computational cost. Despite Stable Diffusion's powerful performance, there are still some remaining problems. For example, Stable Diffusion usually fails to generate multiple objects successfully (e.g., \"an apple and a lemon on the table\").",
            "score": 0.5513297779529561,
            "section_title": "Diffusion Models",
            "char_start_offset": 4285,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1770
                },
                {
                    "start": 1773,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2284
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 20,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 20,
                    "end": 22,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 22,
                    "end": 25,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 25,
                    "end": 28,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 28,
                    "end": 31,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 173,
                    "end": 176,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 342,
                    "end": 346,
                    "matchedPaperCorpusId": "227209335"
                },
                {
                    "start": 702,
                    "end": 706,
                    "matchedPaperCorpusId": "222140788"
                },
                {
                    "start": 954,
                    "end": 957,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 1185,
                    "end": 1189,
                    "matchedPaperCorpusId": "249145348"
                },
                {
                    "start": 1462,
                    "end": 1466,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 1635,
                    "end": 1638,
                    "matchedPaperCorpusId": "256846836"
                },
                {
                    "start": 1918,
                    "end": 1922,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "273877392",
            "title": "MegaPortrait: Revisiting Diffusion Control for High-fidelity Portrait Generation",
            "text": "Diffusion-models [5] generate images by denoising the noised images, and the noise predicted can be also recognized as score [18] or gradient of energy of Energy-based Models (EBMs) [8]. Under the perspective of score or energy, a classic framework of classfier-free guidance [6] is proposed to generate high-fidelity images without explicit classifier guidance by compositing the diffusion score into conditional distribution by Bayesian theorem. Safe Latent Diffusion [16] further extends this framework to eliminate the harmful information from the synthesis process, which achieves better alignment than negative prompting.",
            "score": 0.5513154979287828,
            "section_title": "EBMs and Diffusion",
            "char_start_offset": 7454,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 627
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 20,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 125,
                    "end": 129,
                    "matchedPaperCorpusId": "196470871"
                },
                {
                    "start": 182,
                    "end": 185,
                    "matchedPaperCorpusId": "249375227"
                },
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "253420366"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78955078125
        },
        {
            "corpus_id": "269005366",
            "title": "Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance",
            "text": "In this section, we introduce the technical details of Semantic-aware Classifier-Free Guidance (S-CFG).where the overview of the framework is shown in Figure 2. At each denoising step in diffusion models, the current latent image is fed into the U-net backbone to estimate both diffusion score and conditional diffusion score without or with text prompt input.With the extracted attention maps, we can derive region masks for the relatively independent semantic units.In particular, the cross-attention map is renormalized for assigning each patch to the corresponding token, while the self-attention map is used to complete the semantic regions.Then, to balance the amplification of diverse semantic information, we set adaptive CFG scales on diverse region masks and obtain the scale map to rescale their classifier scores into a uniform level.",
            "score": 0.5511853060830962,
            "section_title": "Methods",
            "char_start_offset": 12259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 103,
                    "end": 360
                },
                {
                    "start": 360,
                    "end": 468
                },
                {
                    "start": 468,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 846
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.759765625
        },
        {
            "corpus_id": "274149817",
            "title": "Decoupling Training-Free Guided Diffusion by ADMM",
            "text": "Diffusion models [22,47,50] have emerged as a potent paradigm in generative modeling, demonstrating versatility across various domains, including image synthesis [45,46], 3D object generation [30,40], natural language processing [29,33], and time-series analysis [3,42]. In many of these areas, there is an escalating interest in conditional generation, where the generative process is guided not only by the diffusion model but also by external information such as text prompts [1,21,27,28,41], segmentation maps [37,59], sketches [37,52,59], etc. In this paper, we focus on solving this task by combining an unconditional diffusion model with a differentiable guidance function that evaluates condition satisfaction in a plug-and-play fashion [8,18,48], which generates the desired samples without additional training. Specifically, our objective is to generate samples from the prior distribution p(x) that satisfy conditions y, approximating the posterior p(x|y). Using Bayes' rule, the posterior can be expressed as p(x|y) \u221d p(x)p(y|x), where p(x) is modeled by a pretrained diffusion model and p(y|x) is approximated by a differentiable loss function. This formulation naturally divides the conditional generation task into two distinct components. However, effectively integrating these components poses significant challenges due to their different objectives. For example, the model may generate diverse but condition-violating samples or produce samples that satisfy the conditions but lack diversity or quality. Existing approaches [8,18,20,48,56,58] mainly introduce a weight hyperparameter to strike a balance between the unconditional diffusion model and the guidance function. Nonetheless, determining an optimal parameter is non-trivial and highly dependent on the specific task, making it difficult to generalize across different problem settings. \n\nTo address these limitations, we propose a novel framework that fundamentally rethinks the interaction between the unconditional diffusion model and the guidance function in conditional generation. Specifically, we decouple these two components by representing the samples from the diffusion model as x and introducing an auxiliary variable z that represents samples refined by the guidance function.",
            "score": 0.5511627060709208,
            "section_title": "Introduction",
            "char_start_offset": 1293,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1864
                },
                {
                    "start": 1867,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2267
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 24,
                    "matchedPaperCorpusId": "14888175"
                },
                {
                    "start": 162,
                    "end": 166,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 166,
                    "end": 169,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 192,
                    "end": 196,
                    "matchedPaperCorpusId": "253708074"
                },
                {
                    "start": 229,
                    "end": 233,
                    "matchedPaperCorpusId": "249192356"
                },
                {
                    "start": 233,
                    "end": 236,
                    "matchedPaperCorpusId": "254854302"
                },
                {
                    "start": 266,
                    "end": 269,
                    "matchedPaperCorpusId": "231719657"
                },
                {
                    "start": 479,
                    "end": 482,
                    "matchedPaperCorpusId": "244714366"
                },
                {
                    "start": 485,
                    "end": 488,
                    "matchedPaperCorpusId": "252918469"
                },
                {
                    "start": 488,
                    "end": 491,
                    "matchedPaperCorpusId": "244909410"
                },
                {
                    "start": 514,
                    "end": 518,
                    "matchedPaperCorpusId": "256900833"
                },
                {
                    "start": 518,
                    "end": 521,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 532,
                    "end": 536,
                    "matchedPaperCorpusId": "256900833"
                },
                {
                    "start": 536,
                    "end": 539,
                    "matchedPaperCorpusId": "254018130"
                },
                {
                    "start": 539,
                    "end": 542,
                    "matchedPaperCorpusId": "256827727"
                },
                {
                    "start": 748,
                    "end": 751,
                    "matchedPaperCorpusId": "249889060"
                },
                {
                    "start": 751,
                    "end": 754,
                    "matchedPaperCorpusId": "260957043"
                },
                {
                    "start": 1546,
                    "end": 1549,
                    "matchedPaperCorpusId": "249889060"
                },
                {
                    "start": 1552,
                    "end": 1555,
                    "matchedPaperCorpusId": "260957043"
                },
                {
                    "start": 1558,
                    "end": 1561,
                    "matchedPaperCorpusId": "257622962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59423828125
        },
        {
            "corpus_id": "265498574",
            "title": "Curved Diffusion: A Generative Model with Optical Geometry Control",
            "text": "Recent advancements in image generation tasks are built on large-scale diffusion models that are trained on paired text and image data [21,23,25]. To improve the level of visual control over the generated content there were introduced a variety conditioning approaches by either explicit model conditioning [8] or gradient guidance [5,27]. A number of work perform conditioning by concatenating the input noised image with an unchangeable conditioning tensor [23,24]. More sophisticated approaches [17,29] modify a pretrained model architecture to make it conditional to some new modalities. These methods typically finetune a text-conditioned model on a set of extracted image features (e.g., depth, segmentation) with the task of reconstructing the original image from a text prompt together with these visual features.",
            "score": 0.5511108370303212,
            "section_title": "Controllable Generation with Text-to-Image Diffusion Models",
            "char_start_offset": 2914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 821
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 139,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 139,
                    "end": 142,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 142,
                    "end": 145,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 332,
                    "end": 335,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 335,
                    "end": 338,
                    "matchedPaperCorpusId": "254018130"
                },
                {
                    "start": 459,
                    "end": 463,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 502,
                    "end": 505,
                    "matchedPaperCorpusId": "256827727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061981201171875
        },
        {
            "corpus_id": "265498623",
            "title": "IG Captioner: Information Gain Captioners are Strong Zero-shot Classifiers",
            "text": "Text-to-image Generative Classifiers. The classic approach [15,38] of using generative models to perform recognition tasks is the Bayes algorithm [29]. \n\nDuring training, this algorithm models the data distribution while during inference, it provides predictions by solving a maximum likelihood estimation (MLE) problem. Recently, there have been the methods [7,19] proposed to convert the text-to-image diffusion models, Stable Diffusion [40] or Imagen [41], into a zeroshot classifier using the Bayes algorithm. \n\nThe focus of IG captioner is different with these previous arts. First, we explore the possibility of achieving a good classifier solely through the generative training, instead of how to convert a generative model into a classifier. The Stable Diffusion used by the above diffusion model classifiers [7,19] is not suitable for our goal because it uses the CLIP text encoder pretrained with contrastive loss to provide the text guidance. Second, we identify the negative impact of the text priors inherited from the pretraining data and propose methods to reduce this impact for zero-shot classification tasks. Besides, IG captioner is an image-totext generative captioner. We also perform the comparisons in Tab. 8. \n\nClassifier-Free Guidance (CFG). CFG [17] is an important approach to improving the sample quality of text-to-image generative models [11,30,37,42,50]. During training, CFG requires the model to generate images both with and without text conditions. During inference, the output image is sampled according to a linear combination of both the conditional and unconditional predictions to improve text-image alignment, which relates CFG to IG captioner from a high level. In contrast, IG captioner is an image-to-text captioner that uses the probability gain from unconditionally to conditionally generating captions with the aim of reducing the bias from text priors for zero-shot classification tasks. \n\nContrast in Text Generation.",
            "score": 0.5508298090074603,
            "section_title": "Related Work",
            "char_start_offset": 6231,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 151
                },
                {
                    "start": 154,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1232
                },
                {
                    "start": 1235,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1935
                },
                {
                    "start": 1938,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 63,
                    "matchedPaperCorpusId": "10327263"
                },
                {
                    "start": 63,
                    "end": 66,
                    "matchedPaperCorpusId": "12196480"
                },
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "296750"
                },
                {
                    "start": 439,
                    "end": 443,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "243938678"
                },
                {
                    "start": 1368,
                    "end": 1372,
                    "matchedPaperCorpusId": "247628171"
                },
                {
                    "start": 1378,
                    "end": 1381,
                    "matchedPaperCorpusId": "248986576"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6943359375
        },
        {
            "corpus_id": "270702277",
            "title": "Repairing Catastrophic-Neglect in Text-to-Image Diffusion Models via Attention-Guided Feature Enhancement",
            "text": "In recent years, the diffusion model has emerged as a more advanced and popular framework for text-toimage (T2I) generation compared to traditional nondiffusion methods like Variational Autoencoders (VAEs) (Yan et al., 2016;Mansimov et al., 2016) and Generative Adversarial Networks (GANs) (Zhu et al., 2019;Ye et al., 2021). Compared to GANs and VAEs, diffusion models achieve better results due to their stability during training and ability to progressively refine images, leading to higher quality and more detailed outputs (Ho et al., 2020;Nichol and Dhariwal, 2021) . To control the generation of diffusion models, Dhariwal and Nichol (2021) firstly propose a conditional image synthesis method utilizing classifier guidance, achieving great success in text-to-image generation. Following that, some representative studies (Bao et al., 2022;Ramesh et al., 2022b;Rombach et al., 2022;Saharia et al., 2022) of text-to-image diffusion models have been proposed, based on the conditioning mechanism. Our experiments are based on Stable Diffusion (Rombach et al., 2022) considering its wide applications.",
            "score": 0.55074337172109,
            "section_title": "Text-to-Image Diffusion Models",
            "char_start_offset": 25288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1105
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 224,
                    "matchedPaperCorpusId": "7577075"
                },
                {
                    "start": 224,
                    "end": 246,
                    "matchedPaperCorpusId": "9996719"
                },
                {
                    "start": 290,
                    "end": 308,
                    "matchedPaperCorpusId": "91183909"
                },
                {
                    "start": 308,
                    "end": 324,
                    "matchedPaperCorpusId": "235742820"
                },
                {
                    "start": 528,
                    "end": 545,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 621,
                    "end": 647,
                    "matchedPaperCorpusId": "234357997"
                },
                {
                    "start": 868,
                    "end": 889,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 889,
                    "end": 910,
                    "matchedPaperCorpusId": "248986576"
                },
                {
                    "start": 1048,
                    "end": 1070,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37939453125
        },
        {
            "corpus_id": "274762759",
            "title": "EP-CFG: Energy-Preserving Classifier-Free Guidance",
            "text": "Classifier-free guidance (CFG) is widely used in diffusion models but often introduces over-contrast and over-saturation artifacts at higher guidance strengths. We present EP-CFG (Energy-Preserving Classifier-Free Guidance), which addresses these issues by preserving the energy distribution of the conditional prediction during the guidance process. Our method simply rescales the energy of the guided output to match that of the conditional prediction at each denoising step, with an optional robust variant for improved artifact suppression. Through experiments, we show that EP-CFG maintains natural image quality and preserves details across guidance strengths while retaining CFG's semantic alignment benefits, all with minimal computational overhead.",
            "score": 0.5506380721074364,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84619140625
        },
        {
            "paperId": "cc7f37a261eaa60113716b8e969da6b57d206da6",
            "corpusId": 268532529,
            "title": "Unveil Conditional Diffusion Models with Classifier-free Guidance: A Sharp Statistical Theory",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 24,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.11968, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292286035",
                    "name": "Hengyu Fu"
                },
                {
                    "authorId": "2292209795",
                    "name": "Zhuoran Yang"
                },
                {
                    "authorId": "2258997595",
                    "name": "Mengdi Wang"
                },
                {
                    "authorId": "2258803874",
                    "name": "Minshuo Chen"
                }
            ],
            "abstract": "Conditional diffusion models serve as the foundation of modern image synthesis and find extensive application in fields like computational biology and reinforcement learning. In these applications, conditional diffusion models incorporate various conditional information, such as prompt input, to guide the sample generation towards desired properties. Despite the empirical success, theory of conditional diffusion models is largely missing. This paper bridges this gap by presenting a sharp statistical theory of distribution estimation using conditional diffusion models. Our analysis yields a sample complexity bound that adapts to the smoothness of the data distribution and matches the minimax lower bound. The key to our theoretical development lies in an approximation result for the conditional score function, which relies on a novel diffused Taylor approximation technique. Moreover, we demonstrate the utility of our statistical theory in elucidating the performance of conditional diffusion models across diverse applications, including model-based transition kernel estimation in reinforcement learning, solving inverse problems, and reward conditioned sample generation.",
            "corpus_id": "268532529",
            "text": "Conditional diffusion models serve as the foundation of modern image synthesis and find extensive application in fields like computational biology and reinforcement learning. In these applications, conditional diffusion models incorporate various conditional information, such as prompt input, to guide the sample generation towards desired properties. Despite the empirical success, theory of conditional diffusion models is largely missing. This paper bridges this gap by presenting a sharp statistical theory of distribution estimation using conditional diffusion models. Our analysis yields a sample complexity bound that adapts to the smoothness of the data distribution and matches the minimax lower bound. The key to our theoretical development lies in an approximation result for the conditional score function, which relies on a novel diffused Taylor approximation technique. Moreover, we demonstrate the utility of our statistical theory in elucidating the performance of conditional diffusion models across diverse applications, including model-based transition kernel estimation in reinforcement learning, solving inverse problems, and reward conditioned sample generation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.76318359375
        },
        {
            "paperId": "0c64aa15f46e2b2918f5850b53d3e092ac8fb40c",
            "corpusId": 279410887,
            "title": "Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models",
            "venue": "",
            "year": 2025,
            "referenceCount": 69,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.14399, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2291140533",
                    "name": "Tian Xia"
                },
                {
                    "authorId": "2291143736",
                    "name": "Fabio De Sousa Ribeiro"
                },
                {
                    "authorId": "2182427359",
                    "name": "Rajat Rasal"
                },
                {
                    "authorId": "35982249",
                    "name": "A. Kori"
                },
                {
                    "authorId": "2330247364",
                    "name": "Raghav Mehta"
                },
                {
                    "authorId": "2254307038",
                    "name": "Ben Glocker"
                }
            ],
            "abstract": "Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.",
            "corpus_id": "279410887",
            "text": "Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.95068359375
        },
        {
            "paperId": "f21edc1d2a6327e11611e90f80ab81c0f8a9e29e",
            "corpusId": 274281305,
            "title": "Classifier-Free Guidance inside the Attraction Basin May Cause Memorization",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 27,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.16738, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292386658",
                    "name": "Anubhav Jain"
                },
                {
                    "authorId": "2332360439",
                    "name": "Yuya Kobayashi"
                },
                {
                    "authorId": "47720660",
                    "name": "Takashi Shibuya"
                },
                {
                    "authorId": "51245193",
                    "name": "Yuhta Takida"
                },
                {
                    "authorId": "2271134207",
                    "name": "Nasir D. Memon"
                },
                {
                    "authorId": "2248163772",
                    "name": "Julian Togelius"
                },
                {
                    "authorId": "2253398841",
                    "name": "Yuki Mitsufuji"
                }
            ],
            "abstract": "Diffusion models are prone to exactly reproduce images from the training data. This exact reproduction of the training data is concerning as it can lead to copyright infringement and/or leakage of privacy-sensitive information. In this paper, we present a novel perspective on the memorization phenomenon and propose a simple yet effective approach to mitigate it. We argue that memorization occurs because of an attraction basin in the denoising process which steers the diffusion trajectory towards a memorized image. However, this can be mitigated by guiding the diffusion trajectory away from the attraction basin by not applying classifier-free guidance until an ideal transition point occurs from which classifier-free guidance is applied. This leads to the generation of non-memorized images that are high in image quality and well-aligned with the conditioning mechanism. To further improve on this, we present a new guidance technique, opposite guidance, that escapes the attraction basin sooner in the denoising process. We demonstrate the existence of attraction basins in various scenarios in which memorization occurs, and we show that our proposed approach successfully mitigates memorization.",
            "corpus_id": "274281305",
            "text": "Diffusion models are prone to exactly reproduce images from the training data. This exact reproduction of the training data is concerning as it can lead to copyright infringement and/or leakage of privacy-sensitive information. In this paper, we present a novel perspective on the memorization phenomenon and propose a simple yet effective approach to mitigate it. We argue that memorization occurs because of an attraction basin in the denoising process which steers the diffusion trajectory towards a memorized image. However, this can be mitigated by guiding the diffusion trajectory away from the attraction basin by not applying classifier-free guidance until an ideal transition point occurs from which classifier-free guidance is applied. This leads to the generation of non-memorized images that are high in image quality and well-aligned with the conditioning mechanism. To further improve on this, we present a new guidance technique, opposite guidance, that escapes the attraction basin sooner in the denoising process. We demonstrate the existence of attraction basins in various scenarios in which memorization occurs, and we show that our proposed approach successfully mitigates memorization.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.489013671875
        },
        {
            "paperId": "bc033fea761b973e2d119c47fc36c797e9c4f0a1",
            "corpusId": 279261040,
            "title": "How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models",
            "venue": "",
            "year": 2025,
            "referenceCount": 41,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.08351, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2288559437",
                    "name": "Huixuan Zhang"
                },
                {
                    "authorId": "2107967598",
                    "name": "Junzhe Zhang"
                },
                {
                    "authorId": "2288537941",
                    "name": "Xiaojun Wan"
                }
            ],
            "abstract": "With the rapid development of text-to-vision generation diffusion models, classifier-free guidance has emerged as the most prevalent method for conditioning. However, this approach inherently requires twice as many steps for model forwarding compared to unconditional generation, resulting in significantly higher costs. While previous study has introduced the concept of adaptive guidance, it lacks solid analysis and empirical results, making previous method unable to be applied to general diffusion models. In this work, we present another perspective of applying adaptive guidance and propose Step AG, which is a simple, universally applicable adaptive guidance strategy. Our evaluations focus on both image quality and image-text alignment. whose results indicate that restricting classifier-free guidance to the first several denoising steps is sufficient for generating high-quality, well-conditioned images, achieving an average speedup of 20% to 30%. Such improvement is consistent across different settings such as inference steps, and various models including video generation models, highlighting the superiority of our method.",
            "corpus_id": "279261040",
            "text": "With the rapid development of text-to-vision generation diffusion models, classifier-free guidance has emerged as the most prevalent method for conditioning. However, this approach inherently requires twice as many steps for model forwarding compared to unconditional generation, resulting in significantly higher costs. While previous study has introduced the concept of adaptive guidance, it lacks solid analysis and empirical results, making previous method unable to be applied to general diffusion models. In this work, we present another perspective of applying adaptive guidance and propose Step AG, which is a simple, universally applicable adaptive guidance strategy. Our evaluations focus on both image quality and image-text alignment. whose results indicate that restricting classifier-free guidance to the first several denoising steps is sufficient for generating high-quality, well-conditioned images, achieving an average speedup of 20% to 30%. Such improvement is consistent across different settings such as inference steps, and various models including video generation models, highlighting the superiority of our method.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.83349609375
        },
        {
            "paperId": "28ac2d96aa0e12e8d70583ff38faaa3d67649819",
            "corpusId": 278910862,
            "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking",
            "venue": "",
            "year": 2025,
            "referenceCount": 43,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.20199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2363590342",
                    "name": "Pengxiang Li"
                },
                {
                    "authorId": "2362879323",
                    "name": "Shilin Yan"
                },
                {
                    "authorId": "2362728158",
                    "name": "Joey Tsai"
                },
                {
                    "authorId": "2291314199",
                    "name": "Renrui Zhang"
                },
                {
                    "authorId": "2363570661",
                    "name": "Ruichuan An"
                },
                {
                    "authorId": "145490494",
                    "name": "Ziyu Guo"
                },
                {
                    "authorId": "2303650253",
                    "name": "Xiaowei Gao"
                }
            ],
            "abstract": "Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel method that tailors the unconditional input by leveraging the model's instantaneous predictive confidence. At each step of an iterative (masked) diffusion language model, A-CFG identifies tokens in the currently generated sequence for which the model exhibits low confidence. These tokens are temporarily re-masked to create a dynamic, localized unconditional input. This focuses CFG's corrective influence precisely on areas of ambiguity, leading to more effective guidance. We integrate A-CFG into a state-of-the-art masked diffusion language model and demonstrate its efficacy. Experiments on diverse language generation benchmarks show that A-CFG yields substantial improvements over standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work highlights the benefit of dynamically adapting guidance mechanisms to model uncertainty in iterative generation.",
            "corpus_id": "278910862",
            "text": "Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel method that tailors the unconditional input by leveraging the model's instantaneous predictive confidence. At each step of an iterative (masked) diffusion language model, A-CFG identifies tokens in the currently generated sequence for which the model exhibits low confidence. These tokens are temporarily re-masked to create a dynamic, localized unconditional input. This focuses CFG's corrective influence precisely on areas of ambiguity, leading to more effective guidance. We integrate A-CFG into a state-of-the-art masked diffusion language model and demonstrate its efficacy. Experiments on diverse language generation benchmarks show that A-CFG yields substantial improvements over standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work highlights the benefit of dynamically adapting guidance mechanisms to model uncertainty in iterative generation.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9267578125
        },
        {
            "paperId": "16110ff34d8419df6f08bf37645a5eac8018b1d4",
            "corpusId": 275101124,
            "title": "Prompt Conditioned Batik Pattern Generation Using LoRA Weighted Diffusion Model With Classifier-Free Guidance",
            "venue": "IEEE Access",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3523494",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3523494?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2024.3523494, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2339017112",
                    "name": "Rahmatulloh Daffa Izzuddin Wahid"
                },
                {
                    "authorId": "2289752969",
                    "name": "Novanto Yudistira"
                },
                {
                    "authorId": "2280597299",
                    "name": "Candra Dewi"
                },
                {
                    "authorId": "2339017144",
                    "name": "Irawati Nurmala Sari"
                },
                {
                    "authorId": "2082156135",
                    "name": "Dyanningrum Pradhikta"
                },
                {
                    "authorId": "2339017299",
                    "name": ".. Fatmawati"
                }
            ],
            "abstract": "Batik, a significant element of Indonesian cultural heritage, is renowned for its intricate patterns and profound philosophical meanings. While preserving traditional batik is crucial, the creation of modern patterns is equally encouraged to keep the art form vibrant and evolving. Current research primarily focuses on batik classification, leaving a gap in the exploration of generative models for batik pattern creation. This paper investigates the application of text-to-image (T2I) generative models to synthesize batik motifs, leveraging latent diffusion models (LDM), Low-Rank Adaptation (LoRA), and classifier-free guidance. Our methodology employed a dataset of 20,000 batik images. Multimodal models such as LLaVA and BLIP were utilized to generate detailed captions for these images. A pretrained LDM was subsequently fine-tuned on its denoising U-Net part, either by naively fine-tuned the entire layer or by employing using LoRA. The fine-tuning process was critical in enhancing the model\u2019s capability to generate high-quality and user-specific batik patterns. The results demonstrated that the LDM fine-tuned on the entire denoising U-Net with LLaVA-captioned images outperformed other models, achieving the lowest Fr\u00e9chet Inception Distance (FID) and highest Inception Score (IS). The thoroughness of LLaVA captions proved superior to those generated by BLIP, emphasizing the significance of detailed image descriptions in generative tasks. Notably, the model not only replicated existing batik patterns but also innovatively combined multiple motifs and even able to create entirely new designs, as verified by batik expert. This research contributes to the field of computer-assisted batik pattern generation, providing significant advantages for batik artists, manufacturers, and users by accelerating the pattern creation process and expanding the possibilities of batik art.",
            "corpus_id": "275101124",
            "text": "Batik, a significant element of Indonesian cultural heritage, is renowned for its intricate patterns and profound philosophical meanings. While preserving traditional batik is crucial, the creation of modern patterns is equally encouraged to keep the art form vibrant and evolving. Current research primarily focuses on batik classification, leaving a gap in the exploration of generative models for batik pattern creation. This paper investigates the application of text-to-image (T2I) generative models to synthesize batik motifs, leveraging latent diffusion models (LDM), Low-Rank Adaptation (LoRA), and classifier-free guidance. Our methodology employed a dataset of 20,000 batik images. Multimodal models such as LLaVA and BLIP were utilized to generate detailed captions for these images. A pretrained LDM was subsequently fine-tuned on its denoising U-Net part, either by naively fine-tuned the entire layer or by employing using LoRA. The fine-tuning process was critical in enhancing the model\u2019s capability to generate high-quality and user-specific batik patterns. The results demonstrated that the LDM fine-tuned on the entire denoising U-Net with LLaVA-captioned images outperformed other models, achieving the lowest Fr\u00e9chet Inception Distance (FID) and highest Inception Score (IS). The thoroughness of LLaVA captions proved superior to those generated by BLIP, emphasizing the significance of detailed image descriptions in generative tasks. Notably, the model not only replicated existing batik patterns but also innovatively combined multiple motifs and even able to create entirely new designs, as verified by batik expert. This research contributes to the field of computer-assisted batik pattern generation, providing significant advantages for batik artists, manufacturers, and users by accelerating the pattern creation process and expanding the possibilities of batik art.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.47265625
        },
        {
            "paperId": "498ac9b2e494601d20a3d0211c16acf2b7954a54",
            "corpusId": 252715883,
            "title": "Imagen Video: High Definition Video Generation with Diffusion Models",
            "venue": "arXiv.org",
            "year": 2022,
            "referenceCount": 51,
            "citationCount": 1541,
            "influentialCitationCount": 88,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.02303",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.02303, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2126278",
                    "name": "Jonathan Ho"
                },
                {
                    "authorId": "144333684",
                    "name": "William Chan"
                },
                {
                    "authorId": "2314850972",
                    "name": "Chitwan Saharia"
                },
                {
                    "authorId": "21040156",
                    "name": "Jay Whang"
                },
                {
                    "authorId": "9659905",
                    "name": "Ruiqi Gao"
                },
                {
                    "authorId": "2194424",
                    "name": "A. Gritsenko"
                },
                {
                    "authorId": "1726807",
                    "name": "Diederik P. Kingma"
                },
                {
                    "authorId": "16443937",
                    "name": "Ben Poole"
                },
                {
                    "authorId": "144739074",
                    "name": "Mohammad Norouzi"
                },
                {
                    "authorId": "1793739",
                    "name": "David J. Fleet"
                },
                {
                    "authorId": "2887364",
                    "name": "Tim Salimans"
                }
            ],
            "abstract": "We present Imagen Video, a text-conditional video generation system based on a cascade of video diffusion models. Given a text prompt, Imagen Video generates high definition videos using a base video generation model and a sequence of interleaved spatial and temporal video super-resolution models. We describe how we scale up the system as a high definition text-to-video model including design decisions such as the choice of fully-convolutional temporal and spatial super-resolution models at certain resolutions, and the choice of the v-parameterization of diffusion models. In addition, we confirm and transfer findings from previous work on diffusion-based image generation to the video generation setting. Finally, we apply progressive distillation to our video models with classifier-free guidance for fast, high quality sampling. We find Imagen Video not only capable of generating videos of high fidelity, but also having a high degree of controllability and world knowledge, including the ability to generate diverse videos and text animations in various artistic styles and with 3D object understanding. See https://imagen.research.google/video/ for samples.",
            "corpus_id": "252715883",
            "text": "We present Imagen Video, a text-conditional video generation system based on a cascade of video diffusion models. Given a text prompt, Imagen Video generates high definition videos using a base video generation model and a sequence of interleaved spatial and temporal video super-resolution models. We describe how we scale up the system as a high definition text-to-video model including design decisions such as the choice of fully-convolutional temporal and spatial super-resolution models at certain resolutions, and the choice of the v-parameterization of diffusion models. In addition, we confirm and transfer findings from previous work on diffusion-based image generation to the video generation setting. Finally, we apply progressive distillation to our video models with classifier-free guidance for fast, high quality sampling. We find Imagen Video not only capable of generating videos of high fidelity, but also having a high degree of controllability and world knowledge, including the ability to generate diverse videos and text animations in various artistic styles and with 3D object understanding. See https://imagen.research.google/video/ for samples.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5693359375
        },
        {
            "paperId": "e0ae0c3ade978bdcb7483c0071a7d74e6621f62c",
            "corpusId": 278910795,
            "title": "Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance",
            "venue": "",
            "year": 2025,
            "referenceCount": 44,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.21101, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2173702283",
                    "name": "Badr Moufad"
                },
                {
                    "authorId": "2056067918",
                    "name": "Yazid Janati"
                },
                {
                    "authorId": "35501788",
                    "name": "Alain Durmus"
                },
                {
                    "authorId": "2363572545",
                    "name": "Ahmed Ghorbel"
                },
                {
                    "authorId": "2237176489",
                    "name": "Eric Moulines"
                },
                {
                    "authorId": "2248195306",
                    "name": "Jimmy Olsson"
                }
            ],
            "abstract": "Classifier-Free Guidance (CFG) is a widely used technique for improving conditional diffusion models by linearly combining the outputs of conditional and unconditional denoisers. While CFG enhances visual quality and improves alignment with prompts, it often reduces sample diversity, leading to a challenging trade-off between quality and diversity. To address this issue, we make two key contributions. First, CFG generally does not correspond to a well-defined denoising diffusion model (DDM). In particular, contrary to common intuition, CFG does not yield samples from the target distribution associated with the limiting CFG score as the noise level approaches zero -- where the data distribution is tilted by a power $w \\gt 1$ of the conditional distribution. We identify the missing component: a R\\'enyi divergence term that acts as a repulsive force and is required to correct CFG and render it consistent with a proper DDM. Our analysis shows that this correction term vanishes in the low-noise limit. Second, motivated by this insight, we propose a Gibbs-like sampling procedure to draw samples from the desired tilted distribution. This method starts with an initial sample from the conditional diffusion model without CFG and iteratively refines it, preserving diversity while progressively enhancing sample quality. We evaluate our approach on both image and text-to-audio generation tasks, demonstrating substantial improvements over CFG across all considered metrics. The code is available at https://github.com/yazidjanati/cfgig",
            "corpus_id": "278910795",
            "text": "Classifier-Free Guidance (CFG) is a widely used technique for improving conditional diffusion models by linearly combining the outputs of conditional and unconditional denoisers. While CFG enhances visual quality and improves alignment with prompts, it often reduces sample diversity, leading to a challenging trade-off between quality and diversity. To address this issue, we make two key contributions. First, CFG generally does not correspond to a well-defined denoising diffusion model (DDM). In particular, contrary to common intuition, CFG does not yield samples from the target distribution associated with the limiting CFG score as the noise level approaches zero -- where the data distribution is tilted by a power $w \\gt 1$ of the conditional distribution. We identify the missing component: a R\\'enyi divergence term that acts as a repulsive force and is required to correct CFG and render it consistent with a proper DDM. Our analysis shows that this correction term vanishes in the low-noise limit. Second, motivated by this insight, we propose a Gibbs-like sampling procedure to draw samples from the desired tilted distribution. This method starts with an initial sample from the conditional diffusion model without CFG and iteratively refines it, preserving diversity while progressively enhancing sample quality. We evaluate our approach on both image and text-to-audio generation tasks, demonstrating substantial improvements over CFG across all considered metrics. The code is available at https://github.com/yazidjanati/cfgig",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.90234375
        }
    ],
    "quotes": {
        "cost": 0.27281399999999995,
        "quotes": [
            {
                "idx": 0,
                "key": "[233168627 | Franceschelli et al. | 2021 | Citations: 42]",
                "snippets": "In order to generate higher-quality images and to allow text-to-image generation, a variety of effective methods for conditioning have been proposed. A possibility is to use classifier guidance (Dhariwal et al., 2021): the diffusion score (i.e., the added noise) includes the gradient of the log-likelihood of an auxiliary classifier model. An alternative is classifier-free guidance (Ho, 2022): to avoid learning an additional model, a single neural network is used to parameterize two diffusion models, one conditional and one unconditional; the two models are then jointly trained by randomly setting the class for the unconditional model. \n\nFinally, the sampling is performed using a linear combination of conditional and unconditional score estimates. Guided Language to Image Diffusion for Generation and Editing (GLIDE) (Nichol et al., 2021) demonstrates how classifier-free guidance can be effectively used to generate text-conditional images.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[245335086 | Nichol et al. | 2021 | Citations: 3629]": "Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.",
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                },
                "metadata": [
                    {
                        "section_title": "Diffusion Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 911,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 149
                            },
                            {
                                "start": 150,
                                "end": 321
                            },
                            {
                                "start": 322,
                                "end": 618
                            },
                            {
                                "start": 621,
                                "end": 732
                            },
                            {
                                "start": 733,
                                "end": 911
                            }
                        ],
                        "ref_mentions": [
                            "234357997",
                            "249145348",
                            "245335086"
                        ],
                        "quote": "In order to generate higher-quality images and to allow text-to-image generation, a variety of effective methods for conditioning have been proposed. A possibility is to use classifier guidance (Dhariwal et al., 2021): the diffusion score (i.e., the added noise) includes the gradient of the log-likelihood of an auxiliary classifier model. An alternative is classifier-free guidance (Ho, 2022): to avoid learning an additional model, a single neural network is used to parameterize two diffusion models, one conditional and one unconditional; the two models are then jointly trained by randomly setting the class for the unconditional model. \n\nFinally, the sampling is performed using a linear combination of conditional and unconditional score estimates. Guided Language to Image Diffusion for Generation and Editing (GLIDE) (Nichol et al., 2021) demonstrates how classifier-free guidance can be effectively used to generate text-conditional images."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[245335086 | Nichol et al. | 2021 | Citations: 3629]",
                "snippets": "Ho & Salimans (2021) achieved similar results without a separately trained classifier through the use of classifier-free guidance, a form of guidance that interpolates between predictions from a diffusion model with and without labels.\n\nNext, we compare two techniques for guiding diffusion models towards text prompts: CLIP guidance and classifier-free guidance. Using human and automated evaluations, we find that classifier-free guidance yields higherquality images.\n\nWe find that samples from our model generated with classifier-free guidance are both photorealistic and reflect a wide breadth of world knowledge. When evaluated by human judges, our samples are preferred to those from DALL-E (Ramesh et al., 2021) 87% of the time when evaluated for photorealism, and 69% of the time when evaluated for caption similarity.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Ho & Salimans (2021) achieved similar results without a separately trained classifier through the use of classifier-free guidance, a form of guidance that interpolates between predictions from a diffusion model with and without labels.\n\nNext, we compare two techniques for guiding diffusion models towards text prompts: CLIP guidance and classifier-free guidance. Using human and automated evaluations, we find that classifier-free guidance yields higherquality images.\n\nWe find that samples from our model generated with classifier-free guidance are both photorealistic and reflect a wide breadth of world knowledge. When evaluated by human judges, our samples are preferred to those from DALL-E (Ramesh et al., 2021) 87% of the time when evaluated for photorealism, and 69% of the time when evaluated for caption similarity.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[248097655 | Ramesh et al. | 2022 | Citations: 6915]",
                "snippets": "Ho and Salimans (Ho, 2022) introduced classifier-free guidance and showed that one can perform guidance implictly from the predictions of the model with and without the conditioning information, thus removing the need for a classifier. Nichol et al. [35] showed classifier-free guidance works more favorably than CLIP guidance for text conditional image generation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 481,
                        "end": 840,
                        "sentence_offsets": [
                            {
                                "start": 481,
                                "end": 710
                            },
                            {
                                "start": 711,
                                "end": 840
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "Ho and Salimans (Ho, 2022) introduced classifier-free guidance and showed that one can perform guidance implictly from the predictions of the model with and without the conditioning information, thus removing the need for a classifier. Nichol et al. [35] showed classifier-free guidance works more favorably than CLIP guidance for text conditional image generation."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[249145348 | Ho | 2022 | Citations: 3970]",
                "snippets": "We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 526,
                        "end": 922,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[249926846 | Yu et al. | 2022 | Citations: 1133]",
                "snippets": "Classifier-free guidance (Ho, 2022) (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability...Classifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Classifier-Free Guidance and Reranking",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 548,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 174
                            },
                            {
                                "start": 175,
                                "end": 410
                            },
                            {
                                "start": 411,
                                "end": 549
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "Classifier-free guidance (Ho, 2022) (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability"
                    },
                    {
                        "section_title": "Classifier-Free Guidance and Reranking",
                        "pdf_hash": "",
                        "start": 989,
                        "end": 1536,
                        "sentence_offsets": [
                            {
                                "start": 989,
                                "end": 1133
                            },
                            {
                                "start": 1134,
                                "end": 1230
                            },
                            {
                                "start": 1231,
                                "end": 1381
                            },
                            {
                                "start": 1382,
                                "end": 1535
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Classifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[252438737 | Zbinden | 2022 | Citations: 3]",
                "snippets": "Depending on a separate model is inconvenient and it complicates the training pipeline. Moreover, for classifier guidance, the number of classes is limited, preventing us from conditioning on complex information to generate more elaborated image compositions. Thus,  proposed classifier-free guidance, which only relies on a single diffusion model. Classifierfree guidance considers a conditional diffusion model \u03b8 (x t , t|y) that can be made unconditional by replacing occasionally during training the condition y by an empty condition \u2205, e.g., by setting the caption to an empty string. The model \u03b8 (x t , t|\u2205) can then be used to generate unconditional images. Classifier-free guidance therefore consists in updating the model output using a linear combination between \u03b8 (x t , t|\u2205) and \u03b8 (x t , t|y) in the following way:\n\nwith s \u2265 1 being the guidance scale. This update can be understood as an attempt to move further in the direction of the conditional model, while moving away from the unconditional model. It is performed at each diffusion step when sampling and Ho and Salimans (2021) have shown that it improves sample quality.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Classifier-free guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1139,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Depending on a separate model is inconvenient and it complicates the training pipeline. Moreover, for classifier guidance, the number of classes is limited, preventing us from conditioning on complex information to generate more elaborated image compositions. Thus,  proposed classifier-free guidance, which only relies on a single diffusion model. Classifierfree guidance considers a conditional diffusion model \u03b8 (x t , t|y) that can be made unconditional by replacing occasionally during training the condition y by an empty condition \u2205, e.g., by setting the caption to an empty string. The model \u03b8 (x t , t|\u2205) can then be used to generate unconditional images. Classifier-free guidance therefore consists in updating the model output using a linear combination between \u03b8 (x t , t|\u2205) and \u03b8 (x t , t|y) in the following way:\n\nwith s \u2265 1 being the guidance scale. This update can be understood as an attempt to move further in the direction of the conditional model, while moving away from the unconditional model. It is performed at each diffusion step when sampling and Ho and Salimans (2021) have shown that it improves sample quality."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[253420366 | Schramowski et al. | 2022 | Citations: 309]",
                "snippets": "Classifier-free guidance [17] is a conditioning method using a purely generational diffusion model, eliminating the need for an additional pre-trained classifier. The approach randomly drops the text conditioning c p with a fixed probability during training, resulting in a joint model for unconditional and conditional objectives. During inference the score estimates for the x-prediction are adjusted so that:\n\nwith guidance scale s g which is typically chosen as s g \u2208 (0, 20] and \u03b8 defining the noise estimate with parameters \u03b8. Intuitively, the unconditioned -prediction \u03b8 (z t ) is pushed in the direction of the conditioned \u03b8 (z t , c p ) to yield an image faithful to prompt p. Lastly, s g determines the magnitude of the influence",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Safe Latent Diffusion (SLD)",
                        "pdf_hash": "",
                        "start": 1468,
                        "end": 2207,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Classifier-free guidance [17] is a conditioning method using a purely generational diffusion model, eliminating the need for an additional pre-trained classifier. The approach randomly drops the text conditioning c p with a fixed probability during training, resulting in a joint model for unconditional and conditional objectives. During inference the score estimates for the x-prediction are adjusted so that:\n\nwith guidance scale s g which is typically chosen as s g \u2208 (0, 20] and \u03b8 defining the noise estimate with parameters \u03b8. Intuitively, the unconditioned -prediction \u03b8 (z t ) is pushed in the direction of the conditioned \u03b8 (z t , c p ) to yield an image faithful to prompt p. Lastly, s g determines the magnitude of the influence"
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[253581213 | Brooks et al. | 2022 | Citations: 1833]",
                "snippets": "Classifier-free diffusion guidance [20] is a method for trading off the quality and diversity of samples generated by a diffusion model. It is commonly used in class-conditional and text-conditional image generation to improve the visual quality of generated images and to make sampled images better correspond with their conditioning. Classifierfree guidance effectively shifts probability mass toward data where an implicit classifier p \u03b8 (c|z t ) assigns high likelihood to the conditioning c. Our model is therefore capable of conditional or unconditional denoising with respect to both or either conditional inputs. We introduce two guidance scales, s I and s T , which can be adjusted to trade off how strongly the generated samples correspond with the input image and how strongly they correspond with the edit instruction.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Classifier-free Guidance for Two Conditionings",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 830,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 136
                            },
                            {
                                "start": 137,
                                "end": 335
                            },
                            {
                                "start": 336,
                                "end": 496
                            },
                            {
                                "start": 497,
                                "end": 620
                            },
                            {
                                "start": 621,
                                "end": 830
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Classifier-free diffusion guidance [20] is a method for trading off the quality and diversity of samples generated by a diffusion model. It is commonly used in class-conditional and text-conditional image generation to improve the visual quality of generated images and to make sampled images better correspond with their conditioning. Classifierfree guidance effectively shifts probability mass toward data where an implicit classifier p \u03b8 (c|z t ) assigns high likelihood to the conditioning c. Our model is therefore capable of conditional or unconditional denoising with respect to both or either conditional inputs. We introduce two guidance scales, s I and s T , which can be adjusted to trade off how strongly the generated samples correspond with the input image and how strongly they correspond with the edit instruction."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[254408758 | Zhang et al. | 2022 | Citations: 160]",
                "snippets": "Model-Based Classifier-Free Guidance. With the above-presented LDMs, we introduce our approach, inspired by classifier-free guidance, to overcome overfitting when fine-tuning LDMs with one image. Classifier-free guidance [15] is a technique widely adopted by prior text-to-image diffusion models [32,35]. A single diffusion model is trained using conditional and unconditional objectives by randomly dropping the condition during training. When sampling, a linear combination of the conditional and unconditional score estimation is used: where \u03f5 \u03b8 (z t , c) and \u03f5 \u03b8 (z t ) are the conditional and unconditional \u03f5-predictions, c is the conditioning vector generated by \u03c4 \u03b8 , and w is the weight for the guidance. The predication is performed using the Tweedie's formula [7], namely, z t \u2212 \u221a 1 \u2212 \u1fb1t \u03b5\u03b8 / \u221a \u1fb1t , where \u1fb1t is a function of t that affects the sampling quality.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Model-Based Classifier-Free Guidance. With the above-presented LDMs, we introduce our approach, inspired by classifier-free guidance, to overcome overfitting when fine-tuning LDMs with one image. Classifier-free guidance [15] is a technique widely adopted by prior text-to-image diffusion models [32,35]. A single diffusion model is trained using conditional and unconditional objectives by randomly dropping the condition during training. When sampling, a linear combination of the conditional and unconditional score estimation is used: where \u03f5 \u03b8 (z t , c) and \u03f5 \u03b8 (z t ) are the conditional and unconditional \u03f5-predictions, c is the conditioning vector generated by \u03c4 \u03b8 , and w is the weight for the guidance. The predication is performed using the Tweedie's formula [7], namely, z t \u2212 \u221a 1 \u2212 \u1fb1t \u03b5\u03b8 / \u221a \u1fb1t , where \u1fb1t is a function of t that affects the sampling quality.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[257505012 | Zhang et al. | 2023 | Citations: 280]",
                "snippets": "Classifier-free guidance. Different from classifierguided diffusion model [41] that exploits an additional classfier, it is found in [42] that the guidance can be obtained by the generative model itself without a classifier, termed as classifier-free guidance. Specifically, classifier-free guidance jointly trains a single model with the unconditional score estimator \u03b8 (x) and the conditional \u03b8 (x, c), where c denotes the class label. A null token \u2205 is placed as the class label in the unconditional part, i.e., \u03b8 (x) = \u03b8 (x, \u2205). Experimental results in [42] show that classifier-free guidance achieves a trade-off between quality and diversity similar to that achieved by classifier guidance. Without resorting to a classifier, classifier-free diffusion facilitates more modalities, e.g., text in text-to-image, as guidance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Guidance in diffusion-based image synthesis",
                        "pdf_hash": "",
                        "start": 1293,
                        "end": 2121,
                        "sentence_offsets": [
                            {
                                "start": 1293,
                                "end": 1318
                            },
                            {
                                "start": 1319,
                                "end": 1553
                            },
                            {
                                "start": 1554,
                                "end": 1730
                            },
                            {
                                "start": 1731,
                                "end": 1825
                            },
                            {
                                "start": 1826,
                                "end": 1989
                            },
                            {
                                "start": 1990,
                                "end": 2121
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Classifier-free guidance. Different from classifierguided diffusion model [41] that exploits an additional classfier, it is found in [42] that the guidance can be obtained by the generative model itself without a classifier, termed as classifier-free guidance. Specifically, classifier-free guidance jointly trains a single model with the unconditional score estimator \u03b8 (x) and the conditional \u03b8 (x, c), where c denotes the class label. A null token \u2205 is placed as the class label in the unconditional part, i.e., \u03b8 (x) = \u03b8 (x, \u2205). Experimental results in [42] show that classifier-free guidance achieves a trade-off between quality and diversity similar to that achieved by classifier guidance. Without resorting to a classifier, classifier-free diffusion facilitates more modalities, e.g., text in text-to-image, as guidance."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[258059755 | Armandpour et al. | 2023 | Citations: 128]",
                "snippets": "To generate photo-realistic images given text prompts, the diffusion models can further take advantage of classifier guidance [9] or classifier-free guidance [15] to improve the image quality. Especially, in the context of text-to-image generation, classifier-free guidance is more widely used, which is usually expressed as a linear interpolation between the conditional and unconditional prediction \u02c6 \u03b8 (x t , t, c) = (1 + \u03c4 ) \u03b8 (x t , t, c) \u2212 \u03c4 \u03b8 (x t , t) at each timestep t with a guidance scale parameter \u03c4.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                },
                "metadata": [
                    {
                        "section_title": "Preliminary",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 513,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 192
                            },
                            {
                                "start": 193,
                                "end": 514
                            }
                        ],
                        "ref_mentions": [
                            "234357997"
                        ],
                        "quote": "To generate photo-realistic images given text prompts, the diffusion models can further take advantage of classifier guidance [9] or classifier-free guidance [15] to improve the image quality. Especially, in the context of text-to-image generation, classifier-free guidance is more widely used, which is usually expressed as a linear interpolation between the conditional and unconditional prediction \u02c6 \u03b8 (x t , t, c) = (1 + \u03c4 ) \u03b8 (x t , t, c) \u2212 \u03c4 \u03b8 (x t , t) at each timestep t with a guidance scale parameter \u03c4."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[258352755 | Xie et al. | 2023 | Citations: 32]",
                "snippets": "In particular, a text-guided method significantly improves the diversity and fidelity [10]. To address photorealism in the conditional setting, (Dhariwal et al., 2021) proposes a classifier to guide diffusion models, allowing them to generate realistic images toward a classifier's label. [5] shows that guidance can be indeed performed by a pure generative model without a classifier, named classifierfree guidance. Classifier-free guidance combines conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that with classifier guidance. Inspired by the ability of guided diffusion models with text, [10] first implements CLIP to guide diffusion models towards text prompt (Radford et al., 2021). However, compared to the quality of generated images,...CLIP guidance cannot compete with classifier-free guidance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[231591445 | Radford et al. | 2021 | Citations: 29867]": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 772,
                        "end": 1545,
                        "sentence_offsets": [
                            {
                                "start": 772,
                                "end": 863
                            },
                            {
                                "start": 864,
                                "end": 1040
                            },
                            {
                                "start": 1041,
                                "end": 1168
                            },
                            {
                                "start": 1169,
                                "end": 1349
                            },
                            {
                                "start": 1350,
                                "end": 1490
                            },
                            {
                                "start": 1491,
                                "end": 1659
                            }
                        ],
                        "ref_mentions": [
                            "234357997",
                            "231591445"
                        ],
                        "quote": "In particular, a text-guided method significantly improves the diversity and fidelity [10]. To address photorealism in the conditional setting, (Dhariwal et al., 2021) proposes a classifier to guide diffusion models, allowing them to generate realistic images toward a classifier's label. [5] shows that guidance can be indeed performed by a pure generative model without a classifier, named classifierfree guidance. Classifier-free guidance combines conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that with classifier guidance. Inspired by the ability of guided diffusion models with text, [10] first implements CLIP to guide diffusion models towards text prompt (Radford et al., 2021). However, compared to the quality of generated images,"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1743,
                        "end": 1803,
                        "sentence_offsets": [
                            {
                                "start": 1743,
                                "end": 1802
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "CLIP guidance cannot compete with classifier-free guidance."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[258556958 | Dong et al. | 2023 | Citations: 62]",
                "snippets": "To this end, the classifier-free guidance technique is proposed, where the prediction for each step is a combination of conditional and unconditional predictions. Formally, let c = \u03c4 \u03b8 (P) be the conditional embedding vector and \u2205 = \u03c4 \u03b8 (\"\") be the unconditional one, the classifierfree guidance prediction is calculated by: \n\nwhere \u03c9 is the guidance scale parameter.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Background and preliminaries",
                        "pdf_hash": "",
                        "start": 470,
                        "end": 837,
                        "sentence_offsets": [
                            {
                                "start": 470,
                                "end": 632
                            },
                            {
                                "start": 633,
                                "end": 794
                            },
                            {
                                "start": 797,
                                "end": 837
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To this end, the classifier-free guidance technique is proposed, where the prediction for each step is a combination of conditional and unconditional predictions. Formally, let c = \u03c4 \u03b8 (P) be the conditional embedding vector and \u2205 = \u03c4 \u03b8 (\"\") be the unconditional one, the classifierfree guidance prediction is calculated by: \n\nwhere \u03c9 is the guidance scale parameter."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[258557506 | Zhai et al. | 2023 | Citations: 77]",
                "snippets": "Dhariwal et al. (Dhariwal et al., 2021) firstly propose a conditional image synthesis method utilizing classifier guidance. Subsequent works (Kim et al., 2021)(Liu et al., 2021) use CLIP, which contains multi-modal information of text and images, to achieve text-guided image synthesis. Ho and Salimans [16] propose classifier-free guidance, which incorporates the conditional mechanism into the diffusion process to achieve conditional image synthesis without external classifiers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion",
                    "[244909410 | Kim et al. | 2021 | Citations: 655]": "Recently, GAN inversion methods combined with Contrastive Language-Image Pretraining (CLIP) enables zeroshot image manipulation guided by text prompts. However, their applications to diverse real images are still difficult due to the limited GAN inversion capability. Specifically, these approaches often have difficulties in reconstructing images with novel poses, views, and highly variable contents compared to the training data, altering object identity, or producing unwanted image artifacts. To mitigate these problems and enable faithful manipulation of real images, we propose a novel method, dubbed DiffusionCLIP, that performs textdriven image manipulation using diffusion models. Based on full inversion capability and high-quality image generation power of recent diffusion models, our method performs zeroshot image manipulation successfully even between unseen domains and takes another step towards general application by manipulating images from a widely varying ImageNet dataset. Furthermore, we propose a novel noise combination method that allows straightforward multi-attribute manipulation. Extensive experiments and human evaluation confirmed robust and superior manipulation performance of our methods compared to the existing baselines. Code is available at https://github.com/gwang-kim/DiffusionCLIP.git",
                    "[245117331 | Liu et al. | 2021 | Citations: 256]": "Controllable image synthesis models allow creation of diverse images based on text instructions or guidance from a reference image. Recently, denoising diffusion probabilistic models have been shown to generate more realistic imagery than prior methods, and have been successfully demonstrated in unconditional and class-conditional settings. We investigate fine-grained, continuous control of this model class, and introduce a novel unified framework for semantic diffusion guidance, which allows either language or image guidance, or both. Guidance is injected into a pretrained unconditional diffusion model using the gradient of image-text or image matching scores, without re-training the diffusion model. We explore CLIP-based language guidance as well as both content and style-based image guidance in a unified framework. Our text-guided synthesis approach can be applied to datasets without associated text annotations. We conduct experiments on FFHQ and LSUN datasets, and show results on fine-grained text-guided image synthesis, synthesis of images related to a style or content reference image, and examples with both textual and image guidance.1"
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK 2.1 Diffusion Models",
                        "pdf_hash": "",
                        "start": 206,
                        "end": 640,
                        "sentence_offsets": [
                            {
                                "start": 150,
                                "end": 310
                            },
                            {
                                "start": 311,
                                "end": 444
                            },
                            {
                                "start": 445,
                                "end": 640
                            }
                        ],
                        "ref_mentions": [
                            "234357997",
                            "244909410",
                            "245117331"
                        ],
                        "quote": "Dhariwal et al. (Dhariwal et al., 2021) firstly propose a conditional image synthesis method utilizing classifier guidance. Subsequent works (Kim et al., 2021)(Liu et al., 2021) use CLIP, which contains multi-modal information of text and images, to achieve text-guided image synthesis. Ho and Salimans [16] propose classifier-free guidance, which incorporates the conditional mechanism into the diffusion process to achieve conditional image synthesis without external classifiers."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[258564566 | Huang et al. | 2023 | Citations: 17]",
                "snippets": "Classifier-free guidance [17] is a method for trading off the quality and diversity of samples generated by a diffusion model. It is commonly used in class-conditional and text-conditional image generation to improve the visual quality of generated images and to make sampled images better correspond with their conditioning. Classifier-free guidance effectively shifts probability mass toward...Guidance for conditions. For our task, the scoring network \u03b5  (  ,   ,  T ) has three conditions: the input image   , text prompt  T , and self-attention map   . We find it beneficial to leverage classifier-free guidance:...concerning each condition. Liu et al. (Liu et al., 2022) demonstrate that a conditional diffusion model can compose score estimates from multiple different conditioning values. We introduce three guidance scales,   ,  T , and   , which can be adjusted to trade off how strongly the generated samples correspond with the conditions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249375227 | Liu et al. | 2022 | Citations: 528]": "Large text-guided diffusion models, such as DALLE-2, are able to generate stunning photorealistic images given natural language descriptions. While such models are highly flexible, they struggle to understand the composition of certain concepts, such as confusing the attributes of different objects or relations between objects. In this paper, we propose an alternative structured approach for compositional generation using diffusion models. An image is generated by composing a set of diffusion models, with each of them modeling a certain component of the image. To do this, we interpret diffusion models as energy-based models in which the data distributions defined by the energy functions may be explicitly combined. The proposed method can generate scenes at test time that are substantially more complex than those seen in training, composing sentence descriptions, object relations, human facial attributes, and even generalizing to new combinations that are rarely seen in the real world. We further illustrate how our approach may be used to compose pre-trained text-guided diffusion models and generate photorealistic images containing all the details described in the input descriptions, including the binding of certain object attributes that have been shown difficult for DALLE-2. These results point to the effectiveness of the proposed method in promoting structured generalization for visual generation. Project page: https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/"
                },
                "metadata": [
                    {
                        "section_title": "Condition Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 394,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 126
                            },
                            {
                                "start": 127,
                                "end": 325
                            },
                            {
                                "start": 326,
                                "end": 393
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Classifier-free guidance [17] is a method for trading off the quality and diversity of samples generated by a diffusion model. It is commonly used in class-conditional and text-conditional image generation to improve the visual quality of generated images and to make sampled images better correspond with their conditioning. Classifier-free guidance effectively shifts probability mass toward"
                    },
                    {
                        "section_title": "Condition Guidance",
                        "pdf_hash": "",
                        "start": 396,
                        "end": 619,
                        "sentence_offsets": [
                            {
                                "start": 396,
                                "end": 420
                            },
                            {
                                "start": 421,
                                "end": 557
                            },
                            {
                                "start": 558,
                                "end": 617
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Guidance for conditions. For our task, the scoring network \u03b5  (  ,   ,  T ) has three conditions: the input image   , text prompt  T , and self-attention map   . We find it beneficial to leverage classifier-free guidance:"
                    },
                    {
                        "section_title": "Condition Guidance",
                        "pdf_hash": "",
                        "start": 620,
                        "end": 938,
                        "sentence_offsets": [
                            {
                                "start": 620,
                                "end": 646
                            },
                            {
                                "start": 647,
                                "end": 782
                            },
                            {
                                "start": 783,
                                "end": 937
                            }
                        ],
                        "ref_mentions": [
                            "249375227"
                        ],
                        "quote": "concerning each condition. Liu et al. (Liu et al., 2022) demonstrate that a conditional diffusion model can compose score estimates from multiple different conditioning values. We introduce three guidance scales,   ,  T , and   , which can be adjusted to trade off how strongly the generated samples correspond with the conditions."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[258615416 | Zhao et al. | 2023 | Citations: 10]",
                "snippets": "Classifier-free guidance [13] is a powerful sampling technique as it directs the model towards text guidance and away from null-text guidance by introducing a null-text guidance term. Compared to the previous study, classifier guidance (Dhariwal et al., 2021), which utilizes a separate classifier to trade off Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) via truncation or low-temperature sampling, classifierfree guidance can be easily implemented and applied. Specifically, classifier-free guidance trains an unconditional denoising diffusion model together with the conditional model and updates the prediction of noise by increasing the distance between target noise and null-text noise. The utilization of classifier-free guidance has greatly enhanced the caliber of generated images and has become ubiquitous in subsequent studies [7] 8, 10, 11, 15-17, 20, 25, 26, 33, 39].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORKS 2.1 Classifier-free guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 873,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 183
                            },
                            {
                                "start": 184,
                                "end": 455
                            },
                            {
                                "start": 456,
                                "end": 685
                            },
                            {
                                "start": 686,
                                "end": 873
                            }
                        ],
                        "ref_mentions": [
                            "234357997"
                        ],
                        "quote": "Classifier-free guidance [13] is a powerful sampling technique as it directs the model towards text guidance and away from null-text guidance by introducing a null-text guidance term. Compared to the previous study, classifier guidance (Dhariwal et al., 2021), which utilizes a separate classifier to trade off Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) via truncation or low-temperature sampling, classifierfree guidance can be easily implemented and applied. Specifically, classifier-free guidance trains an unconditional denoising diffusion model together with the conditional model and updates the prediction of noise by increasing the distance between target noise and null-text noise. The utilization of classifier-free guidance has greatly enhanced the caliber of generated images and has become ubiquitous in subsequent studies [7] 8, 10, 11, 15-17, 20, 25, 26, 33, 39]."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[258714952 | Zhu et al. | 2023 | Citations: 219]",
                "snippets": "Ho et al. [25] introduced the classifier-free diffusion guidance with s \u03b8 (x, t, y) = \u2207 x log p t (x|y) the imageconditional diffusion models. With the same idea, Saharia et al. (Saharia et al., 2021)[49] trained image-conditional diffusion models for SR and image-to-image translation in concurrent work. Nichol et al. [40] proposed to use text-guided diffusion models to generate photo-realistic images with classifierfree guidance. The hyperparameter \u03bb in (1) can be interpreted as the guidance scale in classifier-free diffusion models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[243938678 | Saharia et al. | 2021 | Citations: 1647]": "This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io/ for an overview of the results and code."
                },
                "metadata": [
                    {
                        "section_title": "Conditional Diffusion Models",
                        "pdf_hash": "",
                        "start": 442,
                        "end": 963,
                        "sentence_offsets": [
                            {
                                "start": 442,
                                "end": 584
                            },
                            {
                                "start": 585,
                                "end": 728
                            },
                            {
                                "start": 729,
                                "end": 857
                            },
                            {
                                "start": 858,
                                "end": 963
                            }
                        ],
                        "ref_mentions": [
                            "243938678"
                        ],
                        "quote": "Ho et al. [25] introduced the classifier-free diffusion guidance with s \u03b8 (x, t, y) = \u2207 x log p t (x|y) the imageconditional diffusion models. With the same idea, Saharia et al. (Saharia et al., 2021)[49] trained image-conditional diffusion models for SR and image-to-image translation in concurrent work. Nichol et al. [40] proposed to use text-guided diffusion models to generate photo-realistic images with classifierfree guidance. The hyperparameter \u03bb in (1) can be interpreted as the guidance scale in classifier-free diffusion models."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[258967543 | Brack et al. | 2023 | Citations: 14]",
                "snippets": "Classifier Free Guidance. Before going into detail on different instruction methods for image generation, we need to establish some fundamentals of text-to-image diffusion models (DMs). Intuitively, image generation starts from random noise \u03f5, and the model predicts an estimate of this noise \u03b5\u03b8 to be subtracted from the initial values. This results in a high-fidelity image x without any noise. Since this is a complex problem, multiple steps are applied, each subtracting a small amount (\u03f5 t ) of the predictive noise, approximating \u03f5. For text-to-image generation, the model's \u03f5-prediction is conditioned on a text prompt p and results in an image faithful to that prompt. To that end, DMs employ classifier-free guidance (Ho & Salimans, 2022), a conditioning method using a purely generational diffusion model, eliminating the need for an additional pre-trained classifier. \n\nThe noise estimate \u03b5\u03b8 uses an unconditioned \u03f5-prediction \u03f5 \u03b8 (z t ) which is pushed in the direction of the conditioned estimate \u03f5 \u03b8 (z t , c p ) to yield an image faithful to prompt p.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Instructing Models on the World's Ugliness",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1066,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 25
                            },
                            {
                                "start": 26,
                                "end": 185
                            },
                            {
                                "start": 186,
                                "end": 337
                            },
                            {
                                "start": 338,
                                "end": 396
                            },
                            {
                                "start": 397,
                                "end": 538
                            },
                            {
                                "start": 539,
                                "end": 676
                            },
                            {
                                "start": 677,
                                "end": 878
                            },
                            {
                                "start": 881,
                                "end": 1066
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Classifier Free Guidance. Before going into detail on different instruction methods for image generation, we need to establish some fundamentals of text-to-image diffusion models (DMs). Intuitively, image generation starts from random noise \u03f5, and the model predicts an estimate of this noise \u03b5\u03b8 to be subtracted from the initial values. This results in a high-fidelity image x without any noise. Since this is a complex problem, multiple steps are applied, each subtracting a small amount (\u03f5 t ) of the predictive noise, approximating \u03f5. For text-to-image generation, the model's \u03f5-prediction is conditioned on a text prompt p and results in an image faithful to that prompt. To that end, DMs employ classifier-free guidance (Ho & Salimans, 2022), a conditioning method using a purely generational diffusion model, eliminating the need for an additional pre-trained classifier. \n\nThe noise estimate \u03b5\u03b8 uses an unconditioned \u03f5-prediction \u03f5 \u03b8 (z t ) which is pushed in the direction of the conditioned estimate \u03f5 \u03b8 (z t , c p ) to yield an image faithful to prompt p."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[259341599 | Baykal et al. | 2023 | Citations: 3]",
                "snippets": "Ho and Salimans (2021) propose classifier-free guidance method that does not require a separate classifier. The conditioning information y is periodically utilized while it is dropped out at the remaining time. Therefore, a single model can be used for both unconditional and conditional generation. Ho and Salimans (2021) derive that the unconditional \u03b8 (x t , t) and conditional \u03b8 (x t , t, y) estimations can be used to represent the gradients of the classifier as: \n\nEquation 7 implies that an implicit classifier can eliminate the need for an explicit classifier, and Ho and Salimans (2021) report better results with the classifier-free guidance compared to the explicit classifier guidance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Diffusion Models",
                        "pdf_hash": "",
                        "start": 1073,
                        "end": 1770,
                        "sentence_offsets": [
                            {
                                "start": 1073,
                                "end": 1180
                            },
                            {
                                "start": 1181,
                                "end": 1283
                            },
                            {
                                "start": 1284,
                                "end": 1372
                            },
                            {
                                "start": 1373,
                                "end": 1541
                            },
                            {
                                "start": 1544,
                                "end": 1770
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Ho and Salimans (2021) propose classifier-free guidance method that does not require a separate classifier. The conditioning information y is periodically utilized while it is dropped out at the remaining time. Therefore, a single model can be used for both unconditional and conditional generation. Ho and Salimans (2021) derive that the unconditional \u03b8 (x t , t) and conditional \u03b8 (x t , t, y) estimations can be used to represent the gradients of the classifier as: \n\nEquation 7 implies that an implicit classifier can eliminate the need for an explicit classifier, and Ho and Salimans (2021) report better results with the classifier-free guidance compared to the explicit classifier guidance."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[260886956 | Yang et al. | 2023 | Citations: 15]",
                "snippets": "Classifier-guidance (Dhariwal et al., 2021) provides a way for diffusion model to achieve conditional generation by using the gradient of a separately trained classifier p(y|x t ) during sampling. As a more efficient technique, classifier-free guidance [14, 24] replaces the noise estimator by a combination of conditional and unconditional model, without requirement of p(y|x t ): \n\nwhere y is the class label or text embedding from language model [24], \u03c9 \u2265 1 denotes the guidance scale and trivially increasing \u03c9 will amplify the effect of conditional input.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                },
                "metadata": [
                    {
                        "section_title": "Preliminaries",
                        "pdf_hash": "",
                        "start": 29,
                        "end": 569,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 205
                            },
                            {
                                "start": 206,
                                "end": 390
                            },
                            {
                                "start": 393,
                                "end": 569
                            }
                        ],
                        "ref_mentions": [
                            "234357997"
                        ],
                        "quote": "Classifier-guidance (Dhariwal et al., 2021) provides a way for diffusion model to achieve conditional generation by using the gradient of a separately trained classifier p(y|x t ) during sampling. As a more efficient technique, classifier-free guidance [14, 24] replaces the noise estimator by a combination of conditional and unconditional model, without requirement of p(y|x t ): \n\nwhere y is the class label or text embedding from language model [24], \u03c9 \u2265 1 denotes the guidance scale and trivially increasing \u03c9 will amplify the effect of conditional input."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[260886966 | Ye et al. | 2023 | Citations: 807]",
                "snippets": "For the conditional diffusion models, classifier guidance (Dhariwal et al., 2021) is a straightforward technique used to balance image fidelity and sample diversity by utilizing gradients from a separately trained classifier. To eliminate the need for training a classifier independently, classifier-free guidance [39] is often employed as an alternative method. In this approach, the conditional and unconditional diffusion models are jointly trained by randomly dropping c during training. In the sampling stage, the predicted noise is calculated based on the prediction of both the conditional model \u03f5 \u03b8 (x t , c, t) and unconditional model \u03f5 \u03b8 (x t , t): \n\nhere, w, often named guidance scale or guidance weight, is a scalar value that adjusts the alignment with condition c. \n\nFor text-to-image diffusion models, classifier-free guidance plays a crucial role in enhancing the image-text alignment of generated samples.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                },
                "metadata": [
                    {
                        "section_title": "Prelimiaries",
                        "pdf_hash": "",
                        "start": 1098,
                        "end": 2002,
                        "sentence_offsets": [
                            {
                                "start": 1098,
                                "end": 1304
                            },
                            {
                                "start": 1305,
                                "end": 1441
                            },
                            {
                                "start": 1442,
                                "end": 1570
                            },
                            {
                                "start": 1571,
                                "end": 1737
                            },
                            {
                                "start": 1740,
                                "end": 1858
                            },
                            {
                                "start": 1861,
                                "end": 2002
                            }
                        ],
                        "ref_mentions": [
                            "234357997"
                        ],
                        "quote": "For the conditional diffusion models, classifier guidance (Dhariwal et al., 2021) is a straightforward technique used to balance image fidelity and sample diversity by utilizing gradients from a separately trained classifier. To eliminate the need for training a classifier independently, classifier-free guidance [39] is often employed as an alternative method. In this approach, the conditional and unconditional diffusion models are jointly trained by randomly dropping c during training. In the sampling stage, the predicted noise is calculated based on the prediction of both the conditional model \u03f5 \u03b8 (x t , c, t) and unconditional model \u03f5 \u03b8 (x t , t): \n\nhere, w, often named guidance scale or guidance weight, is a scalar value that adjusts the alignment with condition c. \n\nFor text-to-image diffusion models, classifier-free guidance plays a crucial role in enhancing the image-text alignment of generated samples."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[265351587 | Zheng et al. | 2023 | Citations: 46]",
                "snippets": "Classifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 481,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Classifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[265466084 | Sueyoshi et al. | 2023 | Citations: 8]",
                "snippets": "Training-Free Guidance Even when a diffusion model is designed without condition c, it can reproduce the conditional probability p(x|c) without retraining. This is because, from a diffusion model p(x) and a separate classifier p(c|x) for class label c, one can obtain the gradient of the conditional log-probability, \u2207 x log p(x|c) = \u2207 x log p(c|x) + \u2207 x log p(x). Although grounded in probability theory, what it practically offers is additional guidance \u2207 x log p(c|x) for updating images, which is generalized as classifier guidance (Dhariwal et al., 2021)...When the diffusion model is conditioned on c, the difference between conditional and unconditional updates serves as classifier-free guidance, which can adjust the fidelity of the generated image to condition c (Ho, 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 539,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 155
                            },
                            {
                                "start": 156,
                                "end": 364
                            },
                            {
                                "start": 365,
                                "end": 540
                            }
                        ],
                        "ref_mentions": [
                            "234357997"
                        ],
                        "quote": "Training-Free Guidance Even when a diffusion model is designed without condition c, it can reproduce the conditional probability p(x|c) without retraining. This is because, from a diffusion model p(x) and a separate classifier p(c|x) for class label c, one can obtain the gradient of the conditional log-probability, \u2207 x log p(x|c) = \u2207 x log p(c|x) + \u2207 x log p(x). Although grounded in probability theory, what it practically offers is additional guidance \u2207 x log p(c|x) for updating images, which is generalized as classifier guidance (Dhariwal et al., 2021)"
                    },
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 809,
                        "end": 1025,
                        "sentence_offsets": [
                            {
                                "start": 809,
                                "end": 1025
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "When the diffusion model is conditioned on c, the difference between conditional and unconditional updates serves as classifier-free guidance, which can adjust the fidelity of the generated image to condition c (Ho, 2022)."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[267547881 | Kant et al. | 2024 | Citations: 38]",
                "snippets": "Classifier-free diffusion guidance (Ho, 2022) is a technique used to balance the quality and diversity of images produced by diffusion models. This method is particularly effective in class-conditional and text-conditional image generation, enhancing both the visual quality of images and their alignment with given conditions. Inspired by (Brooks et al., 2022) we explore the integration of classifier-free guidance with Epipolar Attention and Pl\u00fccker Embedding. Implementing classifierfree guidance involves simultaneous training of the diffusion model for both conditional and unconditional denoising tasks. During inference, these models' score estimates are merged. We have four different types of conditioning injected into our system: Outcome: As shown in Fig. 13, we find that classifier-free guidance beyond text conditioning does not provide additional benefits, and rather leads to over-saturated generations. This also aligns with our observations on MVDream.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
                    "[253581213 | Brooks et al. | 2022 | Citations: 1833]": "We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models\u2014a language model (GPT-3) and a text-to-image model (Stable Diffusion)\u2014to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions."
                },
                "metadata": [
                    {
                        "section_title": "B.4. Classifier-free Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 947,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 136
                            },
                            {
                                "start": 137,
                                "end": 321
                            },
                            {
                                "start": 322,
                                "end": 439
                            },
                            {
                                "start": 440,
                                "end": 586
                            },
                            {
                                "start": 587,
                                "end": 646
                            },
                            {
                                "start": 647,
                                "end": 896
                            },
                            {
                                "start": 897,
                                "end": 947
                            }
                        ],
                        "ref_mentions": [
                            "249145348",
                            "253581213"
                        ],
                        "quote": "Classifier-free diffusion guidance (Ho, 2022) is a technique used to balance the quality and diversity of images produced by diffusion models. This method is particularly effective in class-conditional and text-conditional image generation, enhancing both the visual quality of images and their alignment with given conditions. Inspired by (Brooks et al., 2022) we explore the integration of classifier-free guidance with Epipolar Attention and Pl\u00fccker Embedding. Implementing classifierfree guidance involves simultaneous training of the diffusion model for both conditional and unconditional denoising tasks. During inference, these models' score estimates are merged. We have four different types of conditioning injected into our system: Outcome: As shown in Fig. 13, we find that classifier-free guidance beyond text conditioning does not provide additional benefits, and rather leads to over-saturated generations. This also aligns with our observations on MVDream."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[267770589 | Wu et al. | 2024 | Citations: 2]",
                "snippets": "The key idea of our method, Contrastive Guidance, is to characterize an intended factor with two prompts that differ in minimal tokens: the positive prompt describes the image to be synthesized, and the baseline prompt serves as a\"baseline\"that disentangles other factors. Contrastive Guidance is a general method we illustrate whose benefits in three scenarios: (1) to guide domain-specific diffusion models trained on an object class, (2) to gain continuous, rig-like controls for text-to-image generation, and (3) to improve the performance of zero-shot image editors.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The key idea of our method, Contrastive Guidance, is to characterize an intended factor with two prompts that differ in minimal tokens: the positive prompt describes the image to be synthesized, and the baseline prompt serves as a\"baseline\"that disentangles other factors. Contrastive Guidance is a general method we illustrate whose benefits in three scenarios: (1) to guide domain-specific diffusion models trained on an object class, (2) to gain continuous, rig-like controls for text-to-image generation, and (3) to improve the performance of zero-shot image editors.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[268033671 | Huang et al. | 2024 | Citations: 102]",
                "snippets": "Early efforts (Ho et al., 2021), (Dhariwal et al., 2021), (Chao et al., 2022)- (246442182) usually incorporate the class-induced gradients via an additional pretrained classifier during sampling. However, Ho et al. (Ho, 2022) introduce the classifier-free guidance, which does not rely on an external classifier and allows for more versatile conditions, e.g., text, as guidance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion",
                    "[235619773 | Ho et al. | 2021 | Citations: 1235]": "We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation benchmark, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256, outperforming VQ-VAE-2.",
                    "[247763065 | Chao et al. | 2022 | Citations: 42]": "Many existing conditional score-based data generation methods utilize Bayes' theorem to decompose the gradients of a log posterior density into a mixture of scores. These methods facilitate the training procedure of conditional score models, as a mixture of scores can be separately estimated using a score model and a classifier. However, our analysis indicates that the training objectives for the classifier in these methods may lead to a serious score mismatch issue, which corresponds to the situation that the estimated scores deviate from the true ones. Such an issue causes the samples to be misled by the deviated scores during the diffusion process, resulting in a degraded sampling quality. To resolve it, we formulate a novel training objective, called Denoising Likelihood Score Matching (DLSM) loss, for the classifier to match the gradients of the true log likelihood density. Our experimental evidence shows that the proposed method outperforms the previous methods on both Cifar-10 and Cifar-100 benchmarks noticeably in terms of several key evaluation metrics. We thus conclude that, by adopting DLSM, the conditional scores can be accurately modeled, and the effect of the score mismatch issue is alleviated."
                },
                "metadata": [
                    {
                        "section_title": "Conditional Image Generation",
                        "pdf_hash": "",
                        "start": 36,
                        "end": 354,
                        "sentence_offsets": [
                            {
                                "start": 36,
                                "end": 177
                            },
                            {
                                "start": 178,
                                "end": 354
                            }
                        ],
                        "ref_mentions": [
                            "235619773",
                            "234357997",
                            "247763065",
                            "246442182",
                            "249145348"
                        ],
                        "quote": "Early efforts (Ho et al., 2021), (Dhariwal et al., 2021), (Chao et al., 2022)- (246442182) usually incorporate the class-induced gradients via an additional pretrained classifier during sampling. However, Ho et al. (Ho, 2022) introduce the classifier-free guidance, which does not rely on an external classifier and allows for more versatile conditions, e.g., text, as guidance."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[268041325 | Pan et al. | 2024 | Citations: 1]",
                "snippets": "Classifier-free guidance has supplanted classifier guided sampling to achieve a trade-off between the diversity and quality of sampling. While reducing training complexity, this approach secures high-quality image generation. Currently, classifier-free guidance is widely utilized in various diffusion models. We attempt to introduce hierarchical category label information because knowing which superclass each category belongs to is helpful for capturing intricate details and enhancing the overall quality of the generated images. Therefore, we introduce a fine-grained classifier-free guidance sampling method, yielding improved results under specific conditions. Integrated with TieredEmbedder, this approach leverages superclass conditional information to enhance control over generated images. Conventional classifierfree guidance involves training both conditional and unconditional generative models. In our approach, a superclass conditional generative model is introduced to replace the unconditional model. During training, we use the same neural network to parameterize both the conditional model and the superclass conditional model, leveraging a score estimator \u03f5 \u03b8 for optimization guidance. Subclass labels c are probabilistically replaced with their corresponding superclass labels c p to achieve the learning of superclass embeddings. During sampling, trained superclass and subclass embeddings are input into the DiT network to obtain predictive noise. Our method combines conditional scores estimation \u03f5 \u03b8 (x t | c) and superclass conditional scores estimation \u03f5 \u03b8 (x t | c p ) to guide specific category image generation, with the specific formula provided below: \n\nwhere \u03c9 is the guidance scale, and \u03b5\u03b8 is the modified score. \n\nBy thoroughly exploiting the distributional characteristics of superclasses and subclasses, our sampling method achieves a finer control than classifier-free guidance. We can capture commonalities among subclasses within the same superclass and simultaneously capture distinct distributions between different superclasses, thereby making the learned distributions of each category more closely approximate real-world situations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Fine-grained Classifier-free Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2179,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 136
                            },
                            {
                                "start": 137,
                                "end": 225
                            },
                            {
                                "start": 226,
                                "end": 309
                            },
                            {
                                "start": 310,
                                "end": 533
                            },
                            {
                                "start": 534,
                                "end": 667
                            },
                            {
                                "start": 668,
                                "end": 800
                            },
                            {
                                "start": 801,
                                "end": 909
                            },
                            {
                                "start": 910,
                                "end": 1018
                            },
                            {
                                "start": 1019,
                                "end": 1207
                            },
                            {
                                "start": 1208,
                                "end": 1353
                            },
                            {
                                "start": 1354,
                                "end": 1472
                            },
                            {
                                "start": 1473,
                                "end": 1685
                            },
                            {
                                "start": 1688,
                                "end": 1748
                            },
                            {
                                "start": 1751,
                                "end": 1918
                            },
                            {
                                "start": 1919,
                                "end": 2179
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Classifier-free guidance has supplanted classifier guided sampling to achieve a trade-off between the diversity and quality of sampling. While reducing training complexity, this approach secures high-quality image generation. Currently, classifier-free guidance is widely utilized in various diffusion models. We attempt to introduce hierarchical category label information because knowing which superclass each category belongs to is helpful for capturing intricate details and enhancing the overall quality of the generated images. Therefore, we introduce a fine-grained classifier-free guidance sampling method, yielding improved results under specific conditions. Integrated with TieredEmbedder, this approach leverages superclass conditional information to enhance control over generated images. Conventional classifierfree guidance involves training both conditional and unconditional generative models. In our approach, a superclass conditional generative model is introduced to replace the unconditional model. During training, we use the same neural network to parameterize both the conditional model and the superclass conditional model, leveraging a score estimator \u03f5 \u03b8 for optimization guidance. Subclass labels c are probabilistically replaced with their corresponding superclass labels c p to achieve the learning of superclass embeddings. During sampling, trained superclass and subclass embeddings are input into the DiT network to obtain predictive noise. Our method combines conditional scores estimation \u03f5 \u03b8 (x t | c) and superclass conditional scores estimation \u03f5 \u03b8 (x t | c p ) to guide specific category image generation, with the specific formula provided below: \n\nwhere \u03c9 is the guidance scale, and \u03b5\u03b8 is the modified score. \n\nBy thoroughly exploiting the distributional characteristics of superclasses and subclasses, our sampling method achieves a finer control than classifier-free guidance. We can capture commonalities among subclasses within the same superclass and simultaneously capture distinct distributions between different superclasses, thereby making the learned distributions of each category more closely approximate real-world situations."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[268351323 | Huang et al. | 2024 | Citations: 5]",
                "snippets": "Text-to-image diffusion models incorporate additional text conditional variables c in the noise prediction model to predict the conditional noise \u03f5 \u03b8 (x t , c, t) and guide the generation.A classifier-free guidance technique (Ho, 2022) is typically adopted, enabling the utilization of text-conditioned guidance during training without the need for a classifier.The denoising model is trained to handle both conditioned input, where a text prompt is provided, and unconditioned input, where the prompt is replaced with \u2205.This allows for the representation of the guidance direction as the translation from the conditioned input \u03f5 \u03b8 (x t , c, t) to the unconditioned input \u03f5 \u03b8 (x t , \u2205, t).The guidance is performed with a guidance scale s by",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Text-conditioned guidance",
                        "pdf_hash": "",
                        "start": 364,
                        "end": 1099,
                        "sentence_offsets": [
                            {
                                "start": 364,
                                "end": 552
                            },
                            {
                                "start": 552,
                                "end": 720
                            },
                            {
                                "start": 720,
                                "end": 879
                            },
                            {
                                "start": 879,
                                "end": 1047
                            },
                            {
                                "start": 1047,
                                "end": 1099
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "Text-to-image diffusion models incorporate additional text conditional variables c in the noise prediction model to predict the conditional noise \u03f5 \u03b8 (x t , c, t) and guide the generation.A classifier-free guidance technique (Ho, 2022) is typically adopted, enabling the utilization of text-conditioned guidance during training without the need for a classifier.The denoising model is trained to handle both conditioned input, where a text prompt is provided, and unconditioned input, where the prompt is replaced with \u2205.This allows for the representation of the guidance direction as the translation from the conditioned input \u03f5 \u03b8 (x t , c, t) to the unconditioned input \u03f5 \u03b8 (x t , \u2205, t).The guidance is performed with a guidance scale s by"
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[269502576 | Basu et al. | 2024 | Citations: 15]",
                "snippets": "During the inference process, the regulation of image generation involves the utilization of classifier-free guidance, as outlined in (Ho, 2022) which incorporates scores from both the conditional and unconditional diffusion models at each time-step.Specifically, the classifier-free guidance is applied at each time-step to combine the conditional (\u03f5 \u03b8 (z t , c, t)) and unconditional score estimates (\u03f5 \u03b8 (z t , t)).\n\nThe result is a combined score denoted as \u03b5(z t , c, t).\n\n\u03b5(z t , c, t) = \u03f5 \u03b8 (z t , c, t) + \u03b1 (\u03f5 \u03b8 (z t , c, t) \u2212 \u03f5 \u03b8 (z t , t)) , \u2200t \u2208 [T, 1] .\n\n(1) This combined score is used to update the latent z t using DDIM sampling (Song et al., 2020) at each time-step to obtain the final latent code z 0 .",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Control in Cross-Attention Layers",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 729,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 260
                            },
                            {
                                "start": 260,
                                "end": 428
                            },
                            {
                                "start": 430,
                                "end": 486
                            },
                            {
                                "start": 488,
                                "end": 575
                            },
                            {
                                "start": 577,
                                "end": 729
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "During the inference process, the regulation of image generation involves the utilization of classifier-free guidance, as outlined in (Ho, 2022) which incorporates scores from both the conditional and unconditional diffusion models at each time-step.Specifically, the classifier-free guidance is applied at each time-step to combine the conditional (\u03f5 \u03b8 (z t , c, t)) and unconditional score estimates (\u03f5 \u03b8 (z t , t)).\n\nThe result is a combined score denoted as \u03b5(z t , c, t).\n\n\u03b5(z t , c, t) = \u03f5 \u03b8 (z t , c, t) + \u03b1 (\u03f5 \u03b8 (z t , c, t) \u2212 \u03f5 \u03b8 (z t , t)) , \u2200t \u2208 [T, 1] .\n\n(1) This combined score is used to update the latent z t using DDIM sampling (Song et al., 2020) at each time-step to obtain the final latent code z 0 ."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[270123253 | Li et al. | 2024 | Citations: 9]",
                "snippets": "Classifier-free diffusion guidance is commonly employed in class-conditional and text-conditional image generation tasks to enhance the visual quality of the generated images and to ensure that the sampled outputs better correspond to their respective conditioning factors.Typically, diffusion models use a guidance signal, where the model is conditioned on the desired concept or attribute to generate samples aligned with that concept (Gandikota et al., 2023).\n\nOur approach employs negative guidance to allow the diffusion model to gradually forget a specified concept in the original image while applying positive guidance for learning text prompts.Combining with Eq 1 and Eq 2, we compute the following score estimate using Classifier-Free Guidance during inference:\n\nwhere c p and c n are input prompt and forgetting concepts, w and \u03b7 are weights for controlling the balance between the positive and negative guidance signals in the final CFG score estimate.A higher value of eta will give more weight to the negative guidance, allowing tuning the level of concept forgetting during the diffusion process.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[261276613 | Gandikota et al. | 2023 | Citations: 192]": "Text-to-image models suffer from various safety issues that may limit their suitability for deployment. Previous methods have separately addressed individual issues of bias, copyright, and offensive content in text-to-image models. However, in the real world, all of these issues appear simultaneously in the same model. We present a method that tackles all issues with a single approach. Our method, Unified Concept Editing (UCE), edits the model without training using a closed-form solution, and scales seamlessly to concurrent edits on text-conditional diffusion models.We present scalable simultaneous debiasing, style erasure, and content moderation by editing text-to-image projections, and perform extensive experiments demonstrating improved efficacy and scalability over prior work. Our code is available at unified.baulab.info."
                },
                "metadata": [
                    {
                        "section_title": "Forgetting Concept for Conditionings",
                        "pdf_hash": "",
                        "start": 197,
                        "end": 1308,
                        "sentence_offsets": [
                            {
                                "start": 197,
                                "end": 470
                            },
                            {
                                "start": 470,
                                "end": 659
                            },
                            {
                                "start": 661,
                                "end": 850
                            },
                            {
                                "start": 850,
                                "end": 968
                            },
                            {
                                "start": 970,
                                "end": 1161
                            },
                            {
                                "start": 1161,
                                "end": 1308
                            }
                        ],
                        "ref_mentions": [
                            "261276613"
                        ],
                        "quote": "Classifier-free diffusion guidance is commonly employed in class-conditional and text-conditional image generation tasks to enhance the visual quality of the generated images and to ensure that the sampled outputs better correspond to their respective conditioning factors.Typically, diffusion models use a guidance signal, where the model is conditioned on the desired concept or attribute to generate samples aligned with that concept (Gandikota et al., 2023).\n\nOur approach employs negative guidance to allow the diffusion model to gradually forget a specified concept in the original image while applying positive guidance for learning text prompts.Combining with Eq 1 and Eq 2, we compute the following score estimate using Classifier-Free Guidance during inference:\n\nwhere c p and c n are input prompt and forgetting concepts, w and \u03b7 are weights for controlling the balance between the positive and negative guidance signals in the final CFG score estimate.A higher value of eta will give more weight to the negative guidance, allowing tuning the level of concept forgetting during the diffusion process."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[270391454 | Chung et al. | 2024 | Citations: 35]",
                "snippets": "Classifier-free guidance (CFG) (Ho, 2022) forms the key basis of modern text-guided generation with diffusion models (Dhariwal et al., 2021)(Rombach et al., 2021). Nowadays, it is common practice to train a diffusion model with large-scale paired text-image data (Schuhmann et al., 2022), so that sampling (i.e. generating) a signal (e.g. image, video) from a diffusion model can either be done unconditionally from p \u03b8 (x|\u2205) \u2261 p \u03b8 (x), or conditionally from p \u03b8 (x|c), where c is the text conditioning. Once trained, it seems natural that one would acquire samples from the conditional distribution by simply solving the probability-flow ODE or SDE sampling (Song et al., 2020)a;(Karras et al., 2022) with the conditional score function. In practice, however, it is observed that the conditioning signal is insufficient when used naively. To emphasize the guidance, one uses the guidance scale \u03c9 > 1, where the direction can be defined by the direction from the unconditional score to the conditional score (Ho, 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
                    "[227209335 | Song et al. | 2020 | Citations: 6585]": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion",
                    "[245335280 | Rombach et al. | 2021 | Citations: 15768]": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
                    "[249240415 | Karras et al. | 2022 | Citations: 2032]": "We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.",
                    "[252917726 | Schuhmann et al. | 2022 | Citations: 3500]": "Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection. Announcement page https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/"
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1042,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 175
                            },
                            {
                                "start": 176,
                                "end": 323
                            },
                            {
                                "start": 324,
                                "end": 350
                            },
                            {
                                "start": 351,
                                "end": 515
                            },
                            {
                                "start": 516,
                                "end": 750
                            },
                            {
                                "start": 751,
                                "end": 851
                            },
                            {
                                "start": 852,
                                "end": 1042
                            }
                        ],
                        "ref_mentions": [
                            "249145348",
                            "234357997",
                            "245335280",
                            "252917726",
                            "227209335",
                            "249240415",
                            "249145348"
                        ],
                        "quote": "Classifier-free guidance (CFG) (Ho, 2022) forms the key basis of modern text-guided generation with diffusion models (Dhariwal et al., 2021)(Rombach et al., 2021). Nowadays, it is common practice to train a diffusion model with large-scale paired text-image data (Schuhmann et al., 2022), so that sampling (i.e. generating) a signal (e.g. image, video) from a diffusion model can either be done unconditionally from p \u03b8 (x|\u2205) \u2261 p \u03b8 (x), or conditionally from p \u03b8 (x|c), where c is the text conditioning. Once trained, it seems natural that one would acquire samples from the conditional distribution by simply solving the probability-flow ODE or SDE sampling (Song et al., 2020)a;(Karras et al., 2022) with the conditional score function. In practice, however, it is observed that the conditioning signal is insufficient when used naively. To emphasize the guidance, one uses the guidance scale \u03c9 > 1, where the direction can be defined by the direction from the unconditional score to the conditional score (Ho, 2022)."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[270923987 | Sadat et al. | 2024 | Citations: 14]",
                "snippets": "In this paper, we analyze the methodology behind classifier-free guidance and show theoretically that similar behavior can be achieved without additional training of an unconditional model. The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1599,
                        "end": 1950,
                        "sentence_offsets": [
                            {
                                "start": 1585,
                                "end": 1856
                            },
                            {
                                "start": 1856,
                                "end": 1989
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this paper, we analyze the methodology behind classifier-free guidance and show theoretically that similar behavior can be achieved without additional training of an unconditional model. The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[271051241 | Azarian et al. | 2024 | Citations: 0]",
                "snippets": "Our objective is to dynamically adjust the negative prompt for each image patch. We examine attention maps within the diffusion model, specifically where it interacts with the text prompt embeddings. For each patch of the attention map, we aim to find the object in the positive prompt with the highest correlation. Subsequently, this selected object is excluded from the negative prompt interacting with that specific patch. Accordingly, the forward pass of the diffusion model carries out as if each patch cross-attends dynamically with a different negative prompt. Furthermore, the corresponding attention weight is adjusted to account for self-attention interactions. Since this proposed method of guidance does not involve any segmentation network as a guidance function, we term this method as segmentationfree guidance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 660,
                        "end": 1486,
                        "sentence_offsets": [
                            {
                                "start": 577,
                                "end": 697
                            },
                            {
                                "start": 697,
                                "end": 809
                            },
                            {
                                "start": 811,
                                "end": 891
                            },
                            {
                                "start": 891,
                                "end": 1009
                            },
                            {
                                "start": 1009,
                                "end": 1124
                            },
                            {
                                "start": 1124,
                                "end": 1233
                            },
                            {
                                "start": 1233,
                                "end": 1374
                            },
                            {
                                "start": 1374,
                                "end": 1477
                            },
                            {
                                "start": 1477,
                                "end": 1631
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Our objective is to dynamically adjust the negative prompt for each image patch. We examine attention maps within the diffusion model, specifically where it interacts with the text prompt embeddings. For each patch of the attention map, we aim to find the object in the positive prompt with the highest correlation. Subsequently, this selected object is excluded from the negative prompt interacting with that specific patch. Accordingly, the forward pass of the diffusion model carries out as if each patch cross-attends dynamically with a different negative prompt. Furthermore, the corresponding attention weight is adjusted to account for self-attention interactions. Since this proposed method of guidance does not involve any segmentation network as a guidance function, we term this method as segmentationfree guidance."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[272690217 | Buchanan et al. | 2024 | Citations: 1]",
                "snippets": "Classifier guidance (Dhariwal et al., 2021) modifies the diffusion score, specifically the gradient of the log probability density function, which is more tractable to learn than directly modeling the data distribution. This approach employs a classifier to guide the generation process by increasing the probability of data that the classifier assigns a high likelihood to the correct label. As demonstrated by (Salimans et al., 2016), data that is well-classified tends to exhibit high perceptual quality, contributing to superior image generation outcomes. \n\nDespite its effectiveness in balancing precision and recall, classifier guidance depends on the gradients of an image classifier, limiting the variability of generated images. This dependency raises the question of whether it is possible to achieve comparable or superior guidance without relying on a classifier. \n\nClassifier-free guidance, introduced by [7], addresses this issue by eliminating the need for a dedicated classifier. Instead, it involves training an unconditional denoising diffusion model, parameterized by a score estimator, alongside a conditional denoising diffusion model, parameterized by a conditional score estimator. These are parameterized using a single neural network. For the unconditional model, a null token is used for the class identifier. The models are jointly trained, with the conditioning randomly set to a null token based on a hyperparameter probability p-uncond. This methodology not only simplifies the model architecture but also enhances guidance capabilities without the dependence on an external classifier.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[1687220 | Salimans et al. | 2016 | Citations: 9072]": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                },
                "metadata": [
                    {
                        "section_title": "D. Classifier-Free Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1577,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 199
                            },
                            {
                                "start": 200,
                                "end": 372
                            },
                            {
                                "start": 373,
                                "end": 520
                            },
                            {
                                "start": 523,
                                "end": 698
                            },
                            {
                                "start": 699,
                                "end": 836
                            },
                            {
                                "start": 839,
                                "end": 956
                            },
                            {
                                "start": 957,
                                "end": 1165
                            },
                            {
                                "start": 1166,
                                "end": 1220
                            },
                            {
                                "start": 1221,
                                "end": 1296
                            },
                            {
                                "start": 1297,
                                "end": 1427
                            },
                            {
                                "start": 1428,
                                "end": 1577
                            }
                        ],
                        "ref_mentions": [
                            "234357997",
                            "1687220"
                        ],
                        "quote": "Classifier guidance (Dhariwal et al., 2021) modifies the diffusion score, specifically the gradient of the log probability density function, which is more tractable to learn than directly modeling the data distribution. This approach employs a classifier to guide the generation process by increasing the probability of data that the classifier assigns a high likelihood to the correct label. As demonstrated by (Salimans et al., 2016), data that is well-classified tends to exhibit high perceptual quality, contributing to superior image generation outcomes. \n\nDespite its effectiveness in balancing precision and recall, classifier guidance depends on the gradients of an image classifier, limiting the variability of generated images. This dependency raises the question of whether it is possible to achieve comparable or superior guidance without relying on a classifier. \n\nClassifier-free guidance, introduced by [7], addresses this issue by eliminating the need for a dedicated classifier. Instead, it involves training an unconditional denoising diffusion model, parameterized by a score estimator, alongside a conditional denoising diffusion model, parameterized by a conditional score estimator. These are parameterized using a single neural network. For the unconditional model, a null token is used for the class identifier. The models are jointly trained, with the conditioning randomly set to a null token based on a hyperparameter probability p-uncond. This methodology not only simplifies the model architecture but also enhances guidance capabilities without the dependence on an external classifier."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[273811150 | Luo et al. | 2023 | Citations: 4]",
                "snippets": "Further research has shown that guidance can be derived from the generative model itself without a classifier, a method known as classifier-free guidance. This advancement unlocks various forms of conditional signals, leading to conditional generative models (Zhang et al., 2023), (Rombach et al., 2021) that control synthesized samples with multiple modalities, such as text, edge maps, human pose skeletons, segmentation maps, depth, and normals.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[245335280 | Rombach et al. | 2021 | Citations: 15768]": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",
                    "[256827727 | Zhang et al. | 2023 | Citations: 4175]": "We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models."
                },
                "metadata": [
                    {
                        "section_title": "Conditional Generative Models",
                        "pdf_hash": "",
                        "start": 1120,
                        "end": 1534,
                        "sentence_offsets": [
                            {
                                "start": 1120,
                                "end": 1274
                            },
                            {
                                "start": 1275,
                                "end": 1534
                            }
                        ],
                        "ref_mentions": [
                            "256827727",
                            "245335280"
                        ],
                        "quote": "Further research has shown that guidance can be derived from the generative model itself without a classifier, a method known as classifier-free guidance. This advancement unlocks various forms of conditional signals, leading to conditional generative models (Zhang et al., 2023), (Rombach et al., 2021) that control synthesized samples with multiple modalities, such as text, edge maps, human pose skeletons, segmentation maps, depth, and normals."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[274446026 | Nguyen et al. | 2024 | Citations: 2]",
                "snippets": "Classifier-free Guidance (CFG) [15] is an inference technique designed to enhance the quality of generated images by blending the predictions from both a conditional and an unconditional model. At each sampling step, CFG adjusts the denoiser's output using a control parameter \u03ba > 1, allowing for controlled guidance that aligns more closely with the desired conditions...Negative Prompts provide enhanced control by suppressing unwanted features in the generated content. Instead of producing an unconditional output, the model generates an output conditioned on the negative prompt y \u2212 , as follows:...This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Background",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 369,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 193
                            },
                            {
                                "start": 194,
                                "end": 370
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Classifier-free Guidance (CFG) [15] is an inference technique designed to enhance the quality of generated images by blending the predictions from both a conditional and an unconditional model. At each sampling step, CFG adjusts the denoiser's output using a control parameter \u03ba > 1, allowing for controlled guidance that aligns more closely with the desired conditions"
                    },
                    {
                        "section_title": "Background",
                        "pdf_hash": "",
                        "start": 373,
                        "end": 603,
                        "sentence_offsets": [
                            {
                                "start": 373,
                                "end": 473
                            },
                            {
                                "start": 474,
                                "end": 602
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Negative Prompts provide enhanced control by suppressing unwanted features in the generated content. Instead of producing an unconditional output, the model generates an output conditioned on the negative prompt y \u2212 , as follows:"
                    },
                    {
                        "quote": "This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[274823034 | Viola et al. | 2024 | Citations: 6]",
                "snippets": "To allow fine-grained control over the output, guidance-based diffusion [13] incorporates external supervision alongside the original conditioning, using a guidance function that measures whether certain criteria are met. In guided image generation, classifier guidance [14] enables class-conditional outputs from a pretrained, unconditional diffusion model, via gradients from a classifier trained on ImageNet [58] im-ages at different noise scales. Similarly, gradients from a CLIP model [51] trained on noisy images can guide generation toward a user-defined text caption [47]. An alternative, classifier-free guidance [26,47], achieves similar control without training a separate classifier, by parameterizing both conditional and unconditional diffusion models within the same network. The approach is further extended to handle general nonlinear inverse problems [9], using gradients calculated on the expected denoised images.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.",
                    "[252596252 | Chung et al. | 2022 | Citations: 861]": "Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring. Code available at https://github.com/DPS2022/diffusion-posterior-sampling",
                    "[2930547 | Russakovsky et al. | 2014 | Citations: 39603]": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements."
                },
                "metadata": [
                    {
                        "section_title": "Diffusion Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 933,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 220
                            },
                            {
                                "start": 221,
                                "end": 449
                            },
                            {
                                "start": 450,
                                "end": 579
                            },
                            {
                                "start": 580,
                                "end": 789
                            },
                            {
                                "start": 790,
                                "end": 932
                            }
                        ],
                        "ref_mentions": [
                            "2930547",
                            "249145348",
                            "252596252"
                        ],
                        "quote": "To allow fine-grained control over the output, guidance-based diffusion [13] incorporates external supervision alongside the original conditioning, using a guidance function that measures whether certain criteria are met. In guided image generation, classifier guidance [14] enables class-conditional outputs from a pretrained, unconditional diffusion model, via gradients from a classifier trained on ImageNet [58] im-ages at different noise scales. Similarly, gradients from a CLIP model [51] trained on noisy images can guide generation toward a user-defined text caption [47]. An alternative, classifier-free guidance [26,47], achieves similar control without training a separate classifier, by parameterizing both conditional and unconditional diffusion models within the same network. The approach is further extended to handle general nonlinear inverse problems [9], using gradients calculated on the expected denoised images."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[275787953 | Zhuang et al. | 2025 | Citations: 11]",
                "snippets": "Classifier-free guidance (CFG) significantly enhances the capability of generative diffusion models to produce samples of exceptionally high fidelity. This approach integrates conditional generative models with the distribution estimation of unconditional models trained concurrently, thereby improving the overall quality of generation. Inspired by DALL-E 2 [68], VAR [84] and VAR-CLIP [100], we employ Gaussian noise features as conditional input to simulate unconditional generation. Subsequently, we derive the final distribution of image token outputs by subtracting the probability of uncon- ditional generation from the logits distribution of conditional generation. Specifically, assuming that the image features obtained through LLMs are denoted as H i , we derive the translated visual generation features through a mapping layer as H g = Projector(H i ). Subsequently, we employ the Classifier-Free Guidance (CFG) strategy to obtain the final features of the visual generation tokens, which can be expressed as: \n\nwhere H e represents randomly initialized Gaussian noise, serving as the unconditional feature, and \u03bb denotes the scale hyperparameter for the CFG strategy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Classifier-free Guidance.",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1181,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 150
                            },
                            {
                                "start": 151,
                                "end": 337
                            },
                            {
                                "start": 338,
                                "end": 486
                            },
                            {
                                "start": 487,
                                "end": 673
                            },
                            {
                                "start": 674,
                                "end": 865
                            },
                            {
                                "start": 866,
                                "end": 1022
                            },
                            {
                                "start": 1025,
                                "end": 1181
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Classifier-free guidance (CFG) significantly enhances the capability of generative diffusion models to produce samples of exceptionally high fidelity. This approach integrates conditional generative models with the distribution estimation of unconditional models trained concurrently, thereby improving the overall quality of generation. Inspired by DALL-E 2 [68], VAR [84] and VAR-CLIP [100], we employ Gaussian noise features as conditional input to simulate unconditional generation. Subsequently, we derive the final distribution of image token outputs by subtracting the probability of uncon- ditional generation from the logits distribution of conditional generation. Specifically, assuming that the image features obtained through LLMs are denoted as H i , we derive the translated visual generation features through a mapping layer as H g = Projector(H i ). Subsequently, we employ the Classifier-Free Guidance (CFG) strategy to obtain the final features of the visual generation tokens, which can be expressed as: \n\nwhere H e represents randomly initialized Gaussian noise, serving as the unconditional feature, and \u03bb denotes the scale hyperparameter for the CFG strategy."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[276094842 | Ohayon et al. | 2025 | Citations: 0]",
                "snippets": "Here, we introduce a new CFG method that allows generating compressed conditional samples using any pair of conditional and unconditional diffusion models, while controlling the tradeoff between generation quality and the fidelity to the inputs...We coin our method Compressed CFG (CCFG).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C.6. Compressed Classifier-Free Guidance",
                        "pdf_hash": "",
                        "start": 970,
                        "end": 1214,
                        "sentence_offsets": [
                            {
                                "start": 970,
                                "end": 1215
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Here, we introduce a new CFG method that allows generating compressed conditional samples using any pair of conditional and unconditional diffusion models, while controlling the tradeoff between generation quality and the fidelity to the inputs"
                    },
                    {
                        "section_title": "C.6. Compressed Classifier-Free Guidance",
                        "pdf_hash": "",
                        "start": 1478,
                        "end": 1520,
                        "sentence_offsets": [
                            {
                                "start": 1478,
                                "end": 1519
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We coin our method Compressed CFG (CCFG)."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[276422090 | Zheng et al. | 2025 | Citations: 3]",
                "snippets": "A major challenge in text-to-image generation using diffusion models (Yu et al., 2024)(Liu et al., 2024)(Xu et al., 2024) is guiding the generative process to reflect the input text accurately. A widely adopted solution is classifier-free guidance (CFG, Ho & Salimans (2022)), which eliminates the need for external classifiers by training a unified model for both unconditional and conditional image generation. During inference, CFG achieves conditional generation by interpolating between the conditional and unconditional scores, effectively guiding the model to match the input text. This method has shown significant success in various text-to-image tasks (Balaji et al., 2022;Nichol et al., 2021;Ramesh et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[268819764 | Liu et al. | 2024 | Citations: 28]": "Point-based interactive editing serves as an essential tool to complement the controllability of existing generative mod-els. A concurrent work, DragD iffus ion, updates the diffusion latent map in response to user inputs, causing global latent map alterations. This results in imprecise preservation of the original content and unsuccessful editing due to gradient vanishing. In contrast, we present DragNoise, offering ro-bust and accelerated editing without retracing the latent map. The core rationale of DragNoise lies in utilizing the predicted noise output of each U-Net as a semantic editor. This approach is grounded in two critical observations: firstly, the bottleneck features of U-Net inherently possess semantically rich features ideal for interactive editing; secondly, high-level semantics, established early in the denoising process, show minimal variation in subsequent stages. Leveraging these insights, DragNoise edits diffusion semantics in a sin-gle denoising step and efficiently propagates these changes, ensuring stability and efficiency in diffusion editing. Compar-ative experiments reveal that DragNoise achieves superior control and semantic retention, reducing the optimization time by over 50% compared to DragDiffusion. Our codes are available at https://github.com/haofenglIDragNoise.",
                    "[269625321 | Xu et al. | 2024 | Citations: 7]": "Text-to-image generation models have significantly broadened the horizons of creative expression through the power of natural language. However, navigating these models to generate unique concepts, alter their appearance, or reimagine them in unfamiliar roles presents an intricate challenge. For instance, how can we exploit language-guided models to transpose an anime character into a different art style, or envision a beloved character in a radically different setting or role? This paper unveils a novel approach named DreamAnime, designed to provide this level of creative freedom. Using a minimal set of 2-3 images of a user-specified concept such as an anime character or an art style, we teach our model to encapsulate its essence through novel \"words\" in the embedding space of a pre-existing text-to-image model. Crucially, we disentangle the concepts of style and identity into two separate \"words\", thus providing the ability to manipulate them independently. These distinct \"words\" can then be pieced together into natural language sentences, promoting an intuitive and personalized creative process. Empirical results suggest that this disentanglement into separate word embeddings successfully captures a broad range of unique and complex concepts, with each word focusing on style or identity as appropriate. Comparisons with existing methods illustrate DreamAnime's superior capacity to accurately interpret and recreate the desired concepts across various applications and tasks. Code is available at https://github.com/chnshx/DreamAnime."
                },
                "metadata": [
                    {
                        "section_title": "A.1 DIFFUSION MODELS FOR TEXT-TO-IMAGE GENERATION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 722,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 191
                            },
                            {
                                "start": 192,
                                "end": 410
                            },
                            {
                                "start": 411,
                                "end": 586
                            },
                            {
                                "start": 587,
                                "end": 722
                            }
                        ],
                        "ref_mentions": [
                            "272722848",
                            "268819764",
                            "269625321"
                        ],
                        "quote": "A major challenge in text-to-image generation using diffusion models (Yu et al., 2024)(Liu et al., 2024)(Xu et al., 2024) is guiding the generative process to reflect the input text accurately. A widely adopted solution is classifier-free guidance (CFG, Ho & Salimans (2022)), which eliminates the need for external classifiers by training a unified model for both unconditional and conditional image generation. During inference, CFG achieves conditional generation by interpolating between the conditional and unconditional scores, effectively guiding the model to match the input text. This method has shown significant success in various text-to-image tasks (Balaji et al., 2022;Nichol et al., 2021;Ramesh et al., 2022)."
                    }
                ]
            },
            {
                "idx": 40,
                "key": "[276725462 | Sordo et al. | 2025 | Citations: 1]",
                "snippets": "Classifier-free guidance, proposed by Ho et al. [12], allows for enhanced control in diffusion models by eliminating the need for separate classifiers. Instead of relying on a separate classifier, which increases training complexity and introduces bias potential, classifier-free guidance trains the diffusion model to directly learn and combine conditional and unconditional distributions during inference, streamlining the process. In other words, the authors train a conditional diffusion model \u03f5 \u03b8 (x t |y) and an unconditional model \u03f5 \u03b8 (x t |y = 0) as a single neural network as follows:\n\nThis approach is advantageous compared to the previous one as it trains a single model to guide the diffusion process and can take different types of conditional data such as text embeddings. We will see that many models rely on classifier free-guidance especially when training on multi-modal data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[234357997 | Dhariwal et al. | 2021 | Citations: 7951]": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                },
                "metadata": [
                    {
                        "section_title": "Conditional Image Generation with Guided Diffusion Classifier Guidance",
                        "pdf_hash": "",
                        "start": 689,
                        "end": 1583,
                        "sentence_offsets": [
                            {
                                "start": 641,
                                "end": 826
                            },
                            {
                                "start": 829,
                                "end": 880
                            },
                            {
                                "start": 881,
                                "end": 917
                            },
                            {
                                "start": 920,
                                "end": 1071
                            },
                            {
                                "start": 1072,
                                "end": 1353
                            },
                            {
                                "start": 1354,
                                "end": 1513
                            },
                            {
                                "start": 1516,
                                "end": 1707
                            }
                        ],
                        "ref_mentions": [
                            "234357997"
                        ],
                        "quote": "Classifier-free guidance, proposed by Ho et al. [12], allows for enhanced control in diffusion models by eliminating the need for separate classifiers. Instead of relying on a separate classifier, which increases training complexity and introduces bias potential, classifier-free guidance trains the diffusion model to directly learn and combine conditional and unconditional distributions during inference, streamlining the process. In other words, the authors train a conditional diffusion model \u03f5 \u03b8 (x t |y) and an unconditional model \u03f5 \u03b8 (x t |y = 0) as a single neural network as follows:\n\nThis approach is advantageous compared to the previous one as it trains a single model to guide the diffusion process and can take different types of conditional data such as text embeddings. We will see that many models rely on classifier free-guidance especially when training on multi-modal data."
                    }
                ]
            },
            {
                "idx": 41,
                "key": "[276774646 | Jacobi et al. | 2025 | Citations: 0]",
                "snippets": "Inspired by the\"features as directions\"perspective and the Classifier-Free Guidance (CFG) approach from diffusion models, Superscopes amplifies weak but meaningful features, enabling the interpretation of internal representations that previous methods failed to explain-all without requiring additional training.\n\nIn diffusion models, Classifier-Free Guidance (CFG) (Ho et al. (2022)) enhances the alignment of generated outputs with a given condition (such as a text prompt). It is widely used in diffusion-based models (Chen et al. (2024)). \n\nCFG operates by amplifying the direction that represents the condition, where the difference between conditional and unconditional predictions defines this direction in the model's latent space. By amplifying this directional shift, the model is effectively steered toward outputs that better align with the given condition. Reapplying this shift further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance. \n\nThis technique is particularly useful in text-to-image generation models like Stable Diffusion and GLIDE, as it improves adherence to prompts without requiring an external classifier.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Inspired by the\"features as directions\"perspective and the Classifier-Free Guidance (CFG) approach from diffusion models, Superscopes amplifies weak but meaningful features, enabling the interpretation of internal representations that previous methods failed to explain-all without requiring additional training.\n\nIn diffusion models, Classifier-Free Guidance (CFG) (Ho et al. (2022)) enhances the alignment of generated outputs with a given condition (such as a text prompt). It is widely used in diffusion-based models (Chen et al. (2024)). \n\nCFG operates by amplifying the direction that represents the condition, where the difference between conditional and unconditional predictions defines this direction in the model's latent space. By amplifying this directional shift, the model is effectively steered toward outputs that better align with the given condition. Reapplying this shift further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance. \n\nThis technique is particularly useful in text-to-image generation models like Stable Diffusion and GLIDE, as it improves adherence to prompts without requiring an external classifier.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 42,
                "key": "[277271753 | Kwon et al. | 2025 | Citations: 0]",
                "snippets": "In this work, we introduce a novel approach that leverages a geometric perspective on the unconditional score to enhance CFG performance when conditional scores are available. Specifically, we propose a method that filters the singular vectors of both conditional and unconditional scores using singular value decomposition. This filtering process aligns the unconditional score with the conditional score, thereby refining the sampling trajectory to stay closer to the manifold.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 544,
                        "end": 1023,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In this work, we introduce a novel approach that leverages a geometric perspective on the unconditional score to enhance CFG performance when conditional scores are available. Specifically, we propose a method that filters the singular vectors of both conditional and unconditional scores using singular value decomposition. This filtering process aligns the unconditional score with the conditional score, thereby refining the sampling trajectory to stay closer to the manifold."
                    }
                ]
            },
            {
                "idx": 43,
                "key": "[277633776 | Jagpal et al. | 2025 | Citations: 0]",
                "snippets": "Classifier-free guidance (Ho, 2022) enables conditioning diffusion models on text without relying on an external classifier. \n\nDuring training, the model alternates between a specific condition y (e.g., text) and a null condition \u2205, learning both the conditional s \u03b8 (x t , t | y) and unconditional s \u03b8 (x t , t | \u2205) score functions. The final conditional score function at sampling is given by: \n\nwhere \u03b3 controls the influence of the text condition. Adjusting the guidance scale, \u03b3, allows the diffusion process to produce images that align closely with the text prompt.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249145348 | Ho | 2022 | Citations: 3970]": "Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                },
                "metadata": [
                    {
                        "section_title": "Classifier-Free Guidance",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 566,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 118
                            },
                            {
                                "start": 121,
                                "end": 327
                            },
                            {
                                "start": 328,
                                "end": 389
                            },
                            {
                                "start": 392,
                                "end": 445
                            },
                            {
                                "start": 446,
                                "end": 566
                            }
                        ],
                        "ref_mentions": [
                            "249145348"
                        ],
                        "quote": "Classifier-free guidance (Ho, 2022) enables conditioning diffusion models on text without relying on an external classifier. \n\nDuring training, the model alternates between a specific condition y (e.g., text) and a null condition \u2205, learning both the conditional s \u03b8 (x t , t | y) and unconditional s \u03b8 (x t , t | \u2205) score functions. The final conditional score function at sampling is given by: \n\nwhere \u03b3 controls the influence of the text condition. Adjusting the guidance scale, \u03b3, allows the diffusion process to produce images that align closely with the text prompt."
                    }
                ]
            },
            {
                "idx": 44,
                "key": "[277955619 | Ifriqi et al. | 2025 | Citations: 0]",
                "snippets": "Guidance techniques are commonly used in diffusion and flow models to improve image quality and consistency for conditional generative tasks such as class-conditional and text-to-image generation. In particular, classifier-free guidance (CFG) -- the most widely adopted guidance technique -- contrasts conditional and unconditional predictions to improve the generated images. This results, however, in trade-offs across quality, diversity and consistency, improving some at the expense of others.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 497,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Guidance techniques are commonly used in diffusion and flow models to improve image quality and consistency for conditional generative tasks such as class-conditional and text-to-image generation. In particular, classifier-free guidance (CFG) -- the most widely adopted guidance technique -- contrasts conditional and unconditional predictions to improve the generated images. This results, however, in trade-offs across quality, diversity and consistency, improving some at the expense of others."
                    }
                ]
            },
            {
                "idx": 45,
                "key": "[278910862 | Li et al. | 2025 | Citations: 1]",
                "snippets": "Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.040896,
        "cot": "For this query about \"classifier free guidance beyond conditional diffusion models in image generation NLP tasks,\" I need to create a structured set of dimensions that will help answer the query effectively.\n\nThe user wants to understand classifier-free guidance (CFG) not just in the context of conditional diffusion models for image generation, but also how it extends to other domains like NLP tasks. Based on the quotes provided, I'll structure the information into the following dimensions:\n\n1. \"Introduction to Classifier-Free Guidance\" - This will be a synthesis section explaining what CFG is, its foundational concepts, and its key advantages. This will provide necessary background for understanding the rest of the content.\n\n2. \"Classifier-Free Guidance in Diffusion Models\" - This will be a synthesis section focusing on how CFG is implemented and used within diffusion models, which represents the original context where CFG was introduced.\n\n3. \"Technical Implementation of Classifier-Free Guidance\" - This will be a synthesis section explaining the mathematical formulation and technical implementation details of CFG. This is important to understand how it works before exploring extensions.\n\n4. \"Extensions Beyond Conditional Diffusion Models\" - This will be a synthesis section exploring how CFG has been extended beyond its original applications to other model architectures and domains.\n\n5. \"Applications in NLP Tasks\" - This will be a synthesis section focusing specifically on how CFG has been applied to natural language processing tasks, which is a key part of the user's query.\n\n6. \"Recent Innovations and Advanced Techniques\" - This will be a list section highlighting recent innovations, improvements, and advanced techniques related to CFG, which shows how the field is evolving.\n\n7. \"Limitations and Future Directions\" - This will be a synthesis section discussing the current limitations of CFG and potential future research directions, which gives a complete picture of the topic.\n\nThis organization will provide a comprehensive overview that addresses the query thoroughly, starting with the fundamentals and then expanding to show how CFG has evolved beyond its original application domain.",
        "plan": {
            "Introduction to Classifier-Free Guidance (synthesis)": [
                3,
                15,
                33,
                41
            ],
            "Classifier-Free Guidance in Diffusion Models (synthesis)": [
                0,
                1,
                2,
                5,
                6,
                7,
                9,
                10,
                11,
                13,
                17,
                23,
                30
            ],
            "Technical Implementation of Classifier-Free Guidance (synthesis)": [
                4,
                8,
                12,
                14,
                18,
                19,
                20,
                22,
                27,
                28,
                31,
                43
            ],
            "Extensions Beyond Conditional Diffusion Models (synthesis)": [
                21,
                24,
                26,
                29,
                32,
                34,
                35,
                36,
                37,
                38,
                39,
                42,
                44,
                45
            ],
            "Applications in NLP Tasks (synthesis)": [
                4,
                25,
                34,
                39
            ],
            "Recent Innovations and Advanced Techniques (list)": [
                16,
                35,
                36,
                38,
                40,
                42
            ],
            "Limitations and Future Directions (synthesis)": [
                23,
                29,
                31,
                44,
                45
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Classifier-Free Guidance",
                "tldr": "Classifier-Free Guidance (CFG) is a technique that improves conditional generation in diffusion models without requiring a separate classifier. It works by simultaneously training conditional and unconditional models, then combining their outputs during sampling to balance quality and diversity. (6 sources)",
                "text": "\nClassifier-Free Guidance (CFG) represents a significant advancement in the field of generative modeling, particularly for diffusion models. Introduced by Ho et al., this technique provides a method to enhance the quality of generated outputs without relying on external classifiers <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>. Unlike its predecessor, classifier guidance, CFG achieves a similar trade-off between sample quality and diversity by jointly training conditional and unconditional diffusion models and combining their score estimates during the sampling process.\n\nThe development of CFG addressed limitations in previous approaches to guided generation. Classifier guidance, introduced by Dhariwal et al., improved sample quality by modifying the diffusion score using gradients from an external classifier <Paper corpusId=\"234357997\" paperTitle=\"(Dhariwal et al., 2021)\" isShortName></Paper>. While effective, this method was constrained by its dependency on classifier gradients, which limited the variability of generated images <Paper corpusId=\"272690217\" paperTitle=\"(Buchanan et al., 2024)\" isShortName></Paper>. Building on the insight from Salimans et al. that well-classified data tends to exhibit high perceptual quality <Paper corpusId=\"1687220\" paperTitle=\"(Salimans et al., 2016)\" isShortName></Paper>, CFG sought to achieve similar benefits without the need for a separate classifier.\n\nThe implementation of CFG involves training a single neural network that serves both as an unconditional and conditional model. During training, conditioning is randomly set to a null token based on a probability hyperparameter, allowing the model to learn both conditional and unconditional generations simultaneously <Paper corpusId=\"272690217\" paperTitle=\"(Buchanan et al., 2024)\" isShortName></Paper>. This approach has become widely adopted due to its simplicity and effectiveness compared to classifier guidance <Paper corpusId=\"258615416\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>.\n\nCFG operates by amplifying the directional difference between conditional and unconditional predictions in the model's latent space. By enhancing this directional shift, the model is effectively steered toward outputs that better align with given conditions <Paper corpusId=\"276774646\" paperTitle=\"(Jacobi et al., 2025)\" isShortName></Paper>. This technique has proven particularly valuable in text-to-image generation models, where it improves adherence to text prompts without requiring external classifiers, making it a foundational component in many state-of-the-art generative systems <Paper corpusId=\"258615416\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Ho, 2022)",
                        "snippets": [
                            "We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                        ],
                        "paper": {
                            "corpus_id": 249145348,
                            "title": "Classifier-Free Diffusion Guidance",
                            "authors": [
                                {
                                    "authorId": "2126278",
                                    "name": "Jonathan Ho"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 3970
                        },
                        "score": 0.9599609375
                    },
                    {
                        "id": "(Dhariwal et al., 2021)",
                        "snippets": [
                            "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                        ],
                        "paper": {
                            "corpus_id": 234357997,
                            "title": "Diffusion Models Beat GANs on Image Synthesis",
                            "authors": [
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "38967461",
                                    "name": "Alex Nichol"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 7951
                        },
                        "score": 0
                    },
                    {
                        "id": "(Buchanan et al., 2024)",
                        "snippets": [
                            "Classifier guidance (Dhariwal et al., 2021) modifies the diffusion score, specifically the gradient of the log probability density function, which is more tractable to learn than directly modeling the data distribution. This approach employs a classifier to guide the generation process by increasing the probability of data that the classifier assigns a high likelihood to the correct label. As demonstrated by (Salimans et al., 2016), data that is well-classified tends to exhibit high perceptual quality, contributing to superior image generation outcomes. \n\nDespite its effectiveness in balancing precision and recall, classifier guidance depends on the gradients of an image classifier, limiting the variability of generated images. This dependency raises the question of whether it is possible to achieve comparable or superior guidance without relying on a classifier. \n\nClassifier-free guidance, introduced by [7], addresses this issue by eliminating the need for a dedicated classifier. Instead, it involves training an unconditional denoising diffusion model, parameterized by a score estimator, alongside a conditional denoising diffusion model, parameterized by a conditional score estimator. These are parameterized using a single neural network. For the unconditional model, a null token is used for the class identifier. The models are jointly trained, with the conditioning randomly set to a null token based on a hyperparameter probability p-uncond. This methodology not only simplifies the model architecture but also enhances guidance capabilities without the dependence on an external classifier."
                        ],
                        "paper": {
                            "corpus_id": 272690217,
                            "title": "Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation",
                            "authors": [
                                {
                                    "authorId": "2321408469",
                                    "name": "Noah Buchanan"
                                },
                                {
                                    "authorId": "2268404815",
                                    "name": "Susan Gauch"
                                },
                                {
                                    "authorId": "2308097654",
                                    "name": "Quan Mai"
                                }
                            ],
                            "year": 2024,
                            "venue": "2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)",
                            "n_citations": 1
                        },
                        "score": 0.9296875
                    },
                    {
                        "id": "(Salimans et al., 2016)",
                        "snippets": [
                            "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes."
                        ],
                        "paper": {
                            "corpus_id": 1687220,
                            "title": "Improved Techniques for Training GANs",
                            "authors": [
                                {
                                    "authorId": "2887364",
                                    "name": "Tim Salimans"
                                },
                                {
                                    "authorId": "153440022",
                                    "name": "I. Goodfellow"
                                },
                                {
                                    "authorId": "2563432",
                                    "name": "Wojciech Zaremba"
                                },
                                {
                                    "authorId": "34415167",
                                    "name": "Vicki Cheung"
                                },
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "41192764",
                                    "name": "Xi Chen"
                                }
                            ],
                            "year": 2016,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 9072
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhao et al., 2023)",
                        "snippets": [
                            "Classifier-free guidance [13] is a powerful sampling technique as it directs the model towards text guidance and away from null-text guidance by introducing a null-text guidance term. Compared to the previous study, classifier guidance (Dhariwal et al., 2021), which utilizes a separate classifier to trade off Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) via truncation or low-temperature sampling, classifierfree guidance can be easily implemented and applied. Specifically, classifier-free guidance trains an unconditional denoising diffusion model together with the conditional model and updates the prediction of noise by increasing the distance between target noise and null-text noise. The utilization of classifier-free guidance has greatly enhanced the caliber of generated images and has become ubiquitous in subsequent studies [7] 8, 10, 11, 15-17, 20, 25, 26, 33, 39]."
                        ],
                        "paper": {
                            "corpus_id": 258615416,
                            "title": "Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator",
                            "authors": [
                                {
                                    "authorId": "46509200",
                                    "name": "Jing Zhao"
                                },
                                {
                                    "authorId": "28331771",
                                    "name": "Heliang Zheng"
                                },
                                {
                                    "authorId": "2518211",
                                    "name": "Chaoyue Wang"
                                },
                                {
                                    "authorId": "2156125124",
                                    "name": "Long Lan"
                                },
                                {
                                    "authorId": "3441469",
                                    "name": "Wanrong Huang"
                                },
                                {
                                    "authorId": "2120811655",
                                    "name": "Wenjing Yang"
                                }
                            ],
                            "year": 2023,
                            "venue": "ACM Multimedia",
                            "n_citations": 10
                        },
                        "score": 0.91796875
                    },
                    {
                        "id": "(Jacobi et al., 2025)",
                        "snippets": [
                            "Inspired by the\"features as directions\"perspective and the Classifier-Free Guidance (CFG) approach from diffusion models, Superscopes amplifies weak but meaningful features, enabling the interpretation of internal representations that previous methods failed to explain-all without requiring additional training.\n\nIn diffusion models, Classifier-Free Guidance (CFG) (Ho et al. (2022)) enhances the alignment of generated outputs with a given condition (such as a text prompt). It is widely used in diffusion-based models (Chen et al. (2024)). \n\nCFG operates by amplifying the direction that represents the condition, where the difference between conditional and unconditional predictions defines this direction in the model's latent space. By amplifying this directional shift, the model is effectively steered toward outputs that better align with the given condition. Reapplying this shift further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance. \n\nThis technique is particularly useful in text-to-image generation models like Stable Diffusion and GLIDE, as it improves adherence to prompts without requiring an external classifier."
                        ],
                        "paper": {
                            "corpus_id": 276774646,
                            "title": "Superscopes: Amplifying Internal Feature Representations for Language Model Interpretation",
                            "authors": [
                                {
                                    "authorId": "2348485713",
                                    "name": "Jonathan Jacobi"
                                },
                                {
                                    "authorId": "2333352",
                                    "name": "Gal Niv"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.92529296875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Classifier-Free Guidance in Diffusion Models",
                "tldr": "Classifier-Free Guidance (CFG) has become the dominant conditioning approach in diffusion models, particularly for text-to-image generation, as it eliminates the need for external classifiers while providing superior control over quality-diversity trade-offs. CFG works by combining conditional and unconditional score estimates during sampling with a guidance scale parameter that amplifies the directional shift toward the desired condition. (10 sources)",
                "text": "\nClassifier-Free Guidance (CFG) represents a significant advancement in diffusion models, specifically addressing limitations in earlier conditioning approaches. Prior to CFG, conditional diffusion models primarily relied on classifier guidance, introduced by Dhariwal et al., which used gradients from an external classifier to modify the diffusion score <Paper corpusId=\"234357997\" paperTitle=\"(Dhariwal et al., 2021)\" isShortName></Paper>. While effective, this approach was constrained by its dependency on separate classifier models and limited conditioning capabilities <Paper corpusId=\"252438737\" paperTitle=\"(Zbinden, 2022)\" isShortName></Paper>.\n\nHo and Salimans introduced CFG to overcome these limitations, demonstrating that guidance could be achieved with a pure generative model without requiring an external classifier <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>. The approach works by jointly training a conditional and unconditional diffusion model using a single neural network. During training, the conditioning information is randomly dropped with a fixed probability, enabling the model to learn both conditional and unconditional objectives simultaneously <Paper corpusId=\"253420366\" paperTitle=\"(Schramowski et al., 2022)\" isShortName></Paper>. This dual-purpose training creates what is effectively a unified model that can generate both conditional and unconditional outputs.\n\nThe core mechanism of CFG involves a linear interpolation between conditional and unconditional predictions during the sampling process. This is typically expressed as:\n\n\u03b8\u0302(xt, t, c) = (1 + \u03c4)\u03b8(xt, t, c) - \u03c4\u03b8(xt, t)\n\nwhere \u03c4 is the guidance scale parameter <Paper corpusId=\"258059755\" paperTitle=\"(Armandpour et al., 2023)\" isShortName></Paper>. Intuitively, this pushes the unconditional prediction in the direction of the conditional prediction, with the guidance scale determining the magnitude of this directional shift <Paper corpusId=\"253420366\" paperTitle=\"(Schramowski et al., 2022)\" isShortName></Paper>. This parameter allows for a controlled trade-off between sample quality and diversity, similar to what classifier guidance achieves <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>.\n\nThe impact of CFG has been particularly profound in text-to-image generation. Nichol et al. demonstrated through their GLIDE model that CFG can effectively generate text-conditional images with remarkable quality <Paper corpusId=\"245335086\" paperTitle=\"(Nichol et al., 2021)\" isShortName></Paper>. When evaluated by human judges, GLIDE's samples using classifier-free guidance were preferred over those from DALL-E for both photorealism and caption similarity <Paper corpusId=\"245335086\" paperTitle=\"(Nichol et al., 2021)\" isShortName></Paper>. This demonstrated that CFG works more favorably than other guidance methods like CLIP guidance for text-conditional image generation <Paper corpusId=\"248097655\" paperTitle=\"(Ramesh et al., 2022)\" isShortName></Paper>.\n\nBeyond its core functionality, CFG has enabled more versatile conditioning capabilities. Unlike classifier guidance, which is limited by a fixed set of classes, CFG can condition on complex information such as detailed text descriptions, allowing for more elaborate image compositions <Paper corpusId=\"252438737\" paperTitle=\"(Zbinden, 2022)\" isShortName></Paper>. This flexibility has made CFG the foundation for numerous advanced generative systems, including those incorporating multiple types of conditioning <Paper corpusId=\"253581213\" paperTitle=\"(Brooks et al., 2022)\" isShortName></Paper>.\n\nThe widespread adoption of CFG in modern diffusion models is evident in its implementation across numerous state-of-the-art systems. It forms the key basis of modern text-guided generation with diffusion models <Paper corpusId=\"270391454\" paperTitle=\"(Chung et al., 2024)\" isShortName></Paper> and has been incorporated into prominent models like Stable Diffusion <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper>. By enabling efficient and high-quality conditional generation without external classifiers, CFG has become a cornerstone technique in the evolution of diffusion-based image generation.",
                "citations": [
                    {
                        "id": "(Dhariwal et al., 2021)",
                        "snippets": [
                            "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                        ],
                        "paper": {
                            "corpus_id": 234357997,
                            "title": "Diffusion Models Beat GANs on Image Synthesis",
                            "authors": [
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "38967461",
                                    "name": "Alex Nichol"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 7951
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zbinden, 2022)",
                        "snippets": [
                            "Depending on a separate model is inconvenient and it complicates the training pipeline. Moreover, for classifier guidance, the number of classes is limited, preventing us from conditioning on complex information to generate more elaborated image compositions. Thus,  proposed classifier-free guidance, which only relies on a single diffusion model. Classifierfree guidance considers a conditional diffusion model \u03b8 (x t , t|y) that can be made unconditional by replacing occasionally during training the condition y by an empty condition \u2205, e.g., by setting the caption to an empty string. The model \u03b8 (x t , t|\u2205) can then be used to generate unconditional images. Classifier-free guidance therefore consists in updating the model output using a linear combination between \u03b8 (x t , t|\u2205) and \u03b8 (x t , t|y) in the following way:\n\nwith s \u2265 1 being the guidance scale. This update can be understood as an attempt to move further in the direction of the conditional model, while moving away from the unconditional model. It is performed at each diffusion step when sampling and Ho and Salimans (2021) have shown that it improves sample quality."
                        ],
                        "paper": {
                            "corpus_id": 252438737,
                            "title": "Implementing and Experimenting with Diffusion Models for Text-to-Image Generation",
                            "authors": [
                                {
                                    "authorId": "1431226552",
                                    "name": "Robin Zbinden"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.95166015625
                    },
                    {
                        "id": "(Ho, 2022)",
                        "snippets": [
                            "We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                        ],
                        "paper": {
                            "corpus_id": 249145348,
                            "title": "Classifier-Free Diffusion Guidance",
                            "authors": [
                                {
                                    "authorId": "2126278",
                                    "name": "Jonathan Ho"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 3970
                        },
                        "score": 0.9599609375
                    },
                    {
                        "id": "(Schramowski et al., 2022)",
                        "snippets": [
                            "Classifier-free guidance [17] is a conditioning method using a purely generational diffusion model, eliminating the need for an additional pre-trained classifier. The approach randomly drops the text conditioning c p with a fixed probability during training, resulting in a joint model for unconditional and conditional objectives. During inference the score estimates for the x-prediction are adjusted so that:\n\nwith guidance scale s g which is typically chosen as s g \u2208 (0, 20] and \u03b8 defining the noise estimate with parameters \u03b8. Intuitively, the unconditioned -prediction \u03b8 (z t ) is pushed in the direction of the conditioned \u03b8 (z t , c p ) to yield an image faithful to prompt p. Lastly, s g determines the magnitude of the influence"
                        ],
                        "paper": {
                            "corpus_id": 253420366,
                            "title": "Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "40896023",
                                    "name": "P. Schramowski"
                                },
                                {
                                    "authorId": "2166299958",
                                    "name": "Manuel Brack"
                                },
                                {
                                    "authorId": "2905059",
                                    "name": "Bjorn Deiseroth"
                                },
                                {
                                    "authorId": "2066493115",
                                    "name": "K. Kersting"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 309
                        },
                        "score": 0.94091796875
                    },
                    {
                        "id": "(Armandpour et al., 2023)",
                        "snippets": [
                            "To generate photo-realistic images given text prompts, the diffusion models can further take advantage of classifier guidance [9] or classifier-free guidance [15] to improve the image quality. Especially, in the context of text-to-image generation, classifier-free guidance is more widely used, which is usually expressed as a linear interpolation between the conditional and unconditional prediction \u02c6 \u03b8 (x t , t, c) = (1 + \u03c4 ) \u03b8 (x t , t, c) \u2212 \u03c4 \u03b8 (x t , t) at each timestep t with a guidance scale parameter \u03c4."
                        ],
                        "paper": {
                            "corpus_id": 258059755,
                            "title": "Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond",
                            "authors": [
                                {
                                    "authorId": "66139883",
                                    "name": "Mohammadreza Armandpour"
                                },
                                {
                                    "authorId": "145321315",
                                    "name": "A. Sadeghian"
                                },
                                {
                                    "authorId": "8158616",
                                    "name": "Huangjie Zheng"
                                },
                                {
                                    "authorId": "145759966",
                                    "name": "Amir Sadeghian"
                                },
                                {
                                    "authorId": "2152175923",
                                    "name": "Mingyuan Zhou"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 128
                        },
                        "score": 0.9296875
                    },
                    {
                        "id": "(Nichol et al., 2021)",
                        "snippets": [
                            "Ho & Salimans (2021) achieved similar results without a separately trained classifier through the use of classifier-free guidance, a form of guidance that interpolates between predictions from a diffusion model with and without labels.\n\nNext, we compare two techniques for guiding diffusion models towards text prompts: CLIP guidance and classifier-free guidance. Using human and automated evaluations, we find that classifier-free guidance yields higherquality images.\n\nWe find that samples from our model generated with classifier-free guidance are both photorealistic and reflect a wide breadth of world knowledge. When evaluated by human judges, our samples are preferred to those from DALL-E (Ramesh et al., 2021) 87% of the time when evaluated for photorealism, and 69% of the time when evaluated for caption similarity."
                        ],
                        "paper": {
                            "corpus_id": 245335086,
                            "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "38967461",
                                    "name": "Alex Nichol"
                                },
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "67311962",
                                    "name": "Pranav Shyam"
                                },
                                {
                                    "authorId": "2051714782",
                                    "name": "Pamela Mishkin"
                                },
                                {
                                    "authorId": "39593364",
                                    "name": "Bob McGrew"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                },
                                {
                                    "authorId": "2108828435",
                                    "name": "Mark Chen"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 3629
                        },
                        "score": 0.9560546875
                    },
                    {
                        "id": "(Ramesh et al., 2022)",
                        "snippets": [
                            "Ho and Salimans (Ho, 2022) introduced classifier-free guidance and showed that one can perform guidance implictly from the predictions of the model with and without the conditioning information, thus removing the need for a classifier. Nichol et al. [35] showed classifier-free guidance works more favorably than CLIP guidance for text conditional image generation."
                        ],
                        "paper": {
                            "corpus_id": 248097655,
                            "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
                            "authors": [
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "38967461",
                                    "name": "Alex Nichol"
                                },
                                {
                                    "authorId": "30414789",
                                    "name": "Casey Chu"
                                },
                                {
                                    "authorId": "2108828435",
                                    "name": "Mark Chen"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 6915
                        },
                        "score": 0.95263671875
                    },
                    {
                        "id": "(Brooks et al., 2022)",
                        "snippets": [
                            "Classifier-free diffusion guidance [20] is a method for trading off the quality and diversity of samples generated by a diffusion model. It is commonly used in class-conditional and text-conditional image generation to improve the visual quality of generated images and to make sampled images better correspond with their conditioning. Classifierfree guidance effectively shifts probability mass toward data where an implicit classifier p \u03b8 (c|z t ) assigns high likelihood to the conditioning c. Our model is therefore capable of conditional or unconditional denoising with respect to both or either conditional inputs. We introduce two guidance scales, s I and s T , which can be adjusted to trade off how strongly the generated samples correspond with the input image and how strongly they correspond with the edit instruction."
                        ],
                        "paper": {
                            "corpus_id": 253581213,
                            "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
                            "authors": [
                                {
                                    "authorId": "2679394",
                                    "name": "Tim Brooks"
                                },
                                {
                                    "authorId": "2248172435",
                                    "name": "Aleksander Holynski"
                                },
                                {
                                    "authorId": "1763086",
                                    "name": "Alexei A. Efros"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 1833
                        },
                        "score": 0.9296875
                    },
                    {
                        "id": "(Chung et al., 2024)",
                        "snippets": [
                            "Classifier-free guidance (CFG) (Ho, 2022) forms the key basis of modern text-guided generation with diffusion models (Dhariwal et al., 2021)(Rombach et al., 2021). Nowadays, it is common practice to train a diffusion model with large-scale paired text-image data (Schuhmann et al., 2022), so that sampling (i.e. generating) a signal (e.g. image, video) from a diffusion model can either be done unconditionally from p \u03b8 (x|\u2205) \u2261 p \u03b8 (x), or conditionally from p \u03b8 (x|c), where c is the text conditioning. Once trained, it seems natural that one would acquire samples from the conditional distribution by simply solving the probability-flow ODE or SDE sampling (Song et al., 2020)a;(Karras et al., 2022) with the conditional score function. In practice, however, it is observed that the conditioning signal is insufficient when used naively. To emphasize the guidance, one uses the guidance scale \u03c9 > 1, where the direction can be defined by the direction from the unconditional score to the conditional score (Ho, 2022)."
                        ],
                        "paper": {
                            "corpus_id": 270391454,
                            "title": "CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2110872233",
                                    "name": "Hyungjin Chung"
                                },
                                {
                                    "authorId": "2109216792",
                                    "name": "Jeongsol Kim"
                                },
                                {
                                    "authorId": "153118937",
                                    "name": "Geon Yeong Park"
                                },
                                {
                                    "authorId": "2268758810",
                                    "name": "Hyelin Nam"
                                },
                                {
                                    "authorId": "2254155658",
                                    "name": "Jong Chul Ye"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 35
                        },
                        "score": 0.92919921875
                    },
                    {
                        "id": "(Rombach et al., 2021)",
                        "snippets": [
                            "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
                        ],
                        "paper": {
                            "corpus_id": 245335280,
                            "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "1660819540",
                                    "name": "Robin Rombach"
                                },
                                {
                                    "authorId": "119843260",
                                    "name": "A. Blattmann"
                                },
                                {
                                    "authorId": "2053482699",
                                    "name": "Dominik Lorenz"
                                },
                                {
                                    "authorId": "35175531",
                                    "name": "Patrick Esser"
                                },
                                {
                                    "authorId": "1796707",
                                    "name": "B. Ommer"
                                }
                            ],
                            "year": 2021,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 15768
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Technical Implementation of Classifier-Free Guidance",
                "tldr": "Classifier-Free Guidance is implemented by training a single model that serves both conditional and unconditional purposes by randomly dropping conditioning information during training. During sampling, outputs from conditional and unconditional paths are combined using a guidance scale parameter that controls the quality-diversity trade-off. (14 sources)",
                "text": "\nClassifier-Free Guidance (CFG) offers an elegant implementation approach that eliminates the need for separate conditional and unconditional models. The technical implementation consists of two primary phases: the training phase, where the model learns both conditional and unconditional generation capabilities, and the sampling phase, where these capabilities are combined to achieve guided generation.\n\nDuring the training phase, a single model is trained to handle both conditional and unconditional generation simultaneously. This is achieved by randomly dropping out the conditioning information (such as text prompts or class labels) with a predetermined probability <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>. When the condition is dropped, it is typically replaced with a null token, padding tokens, or a learned embedding, effectively teaching the model to generate content without conditioning <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>. This approach enables the model to learn both conditional distribution p(x|c) and unconditional distribution p(x) within a unified architecture <Paper corpusId=\"270923987\" paperTitle=\"(Sadat et al., 2024)\" isShortName></Paper>.\n\nThe sampling phase is where the true power of CFG emerges. The model combines predictions from both conditional and unconditional paths using a linear interpolation formula. This formula is typically expressed as:\n\n\u03b5(zt, c, t) = \u03b5\u03b8(zt, c, t) + \u03b1(\u03b5\u03b8(zt, c, t) - \u03b5\u03b8(zt, t))\n\nwhere \u03b5\u03b8(zt, c, t) is the conditional score estimate, \u03b5\u03b8(zt, t) is the unconditional score estimate, and \u03b1 (also called \u03c9, \u03c4, \u03b3, or w in different implementations) is the guidance scale parameter <Paper corpusId=\"269502576\" paperTitle=\"(Basu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258556958\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper> <Paper corpusId=\"260886966\" paperTitle=\"(Ye et al., 2023)\" isShortName></Paper>. This formula effectively amplifies the directional difference between conditional and unconditional predictions, steering the generation toward outputs that better align with the given condition.\n\nFrom a theoretical perspective, CFG can be understood through the lens of Bayesian inference. Ho and Salimans demonstrated that the difference between conditional and unconditional score estimates implicitly represents the gradient of the classifier <Paper corpusId=\"259341599\" paperTitle=\"(Baykal et al., 2023)\" isShortName></Paper>. This insight reveals that CFG effectively approximates classifier guidance without requiring a separate classifier, as expressed in the equation:\n\n\u2207x log p(y|x) \u2248 \u2207x log p(x|y) - \u2207x log p(x)\n\nwhere the right side represents the difference between conditional and unconditional score functions <Paper corpusId=\"260886956\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"234357997\" paperTitle=\"(Dhariwal et al., 2021)\" isShortName></Paper>.\n\nThe guidance scale parameter (\u03b1) plays a crucial role in controlling the balance between sample quality and diversity. Higher values of \u03b1 increase fidelity to the conditioning information but may reduce diversity and introduce artifacts, while lower values preserve diversity at the cost of weaker conditioning <Paper corpusId=\"277633776\" paperTitle=\"(Jagpal et al., 2025)\" isShortName></Paper>. This parameter provides a convenient knob for adjusting the output characteristics without retraining the model.\n\nOne of the notable advantages of CFG is its flexibility in handling multiple types of conditions. The approach can be extended to compose multiple guidance signals by introducing separate guidance scales for each condition type <Paper corpusId=\"258564566\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. For example, in models that condition on both text and reference images, different guidance scales can be applied to each condition, allowing fine-grained control over how strongly each condition influences the generation <Paper corpusId=\"258564566\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"249375227\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.\n\nCFG has been successfully implemented across various model architectures. In latent diffusion models (LDMs), it is applied to the noise prediction in latent space <Paper corpusId=\"254408758\" paperTitle=\"(Zhang et al., 2022)\" isShortName></Paper>. For text-to-image generation, the technique has been shown to significantly improve image-text alignment, especially for challenging text prompts <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"260886966\" paperTitle=\"(Ye et al., 2023)\" isShortName></Paper>. Recent research has also shown that CFG-like behavior can be achieved without additional training of an unconditional model by using a conditioning vector independent of the input data <Paper corpusId=\"270923987\" paperTitle=\"(Sadat et al., 2024)\" isShortName></Paper>.\n\nIn practice, the sampling process typically employs CFG in conjunction with methods like DDIM (Denoising Diffusion Implicit Models) sampling to update the latent representation at each time step <Paper corpusId=\"269502576\" paperTitle=\"(Basu et al., 2024)\" isShortName></Paper>. This combination has become the standard approach in most state-of-the-art diffusion-based generative systems, thanks to its computational efficiency and effectiveness in improving conditional generation quality <Paper corpusId=\"268351323\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265466084\" paperTitle=\"(Sueyoshi et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Yu et al., 2022)",
                        "snippets": [
                            "Classifier-free guidance (Ho, 2022) (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability",
                            "Classifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts."
                        ],
                        "paper": {
                            "corpus_id": 249926846,
                            "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
                            "authors": [
                                {
                                    "authorId": "2338016295",
                                    "name": "Jiahui Yu"
                                },
                                {
                                    "authorId": "2145139570",
                                    "name": "Yuanzhong Xu"
                                },
                                {
                                    "authorId": "23978705",
                                    "name": "Jing Yu Koh"
                                },
                                {
                                    "authorId": "1821711",
                                    "name": "Thang Luong"
                                },
                                {
                                    "authorId": "1396954703",
                                    "name": "Gunjan Baid"
                                },
                                {
                                    "authorId": "2331539",
                                    "name": "Zirui Wang"
                                },
                                {
                                    "authorId": "2053781980",
                                    "name": "Vijay Vasudevan"
                                },
                                {
                                    "authorId": "31702389",
                                    "name": "Alexander Ku"
                                },
                                {
                                    "authorId": "2118771180",
                                    "name": "Yinfei Yang"
                                },
                                {
                                    "authorId": "143990191",
                                    "name": "Burcu Karagol Ayan"
                                },
                                {
                                    "authorId": "2044655623",
                                    "name": "Ben Hutchinson"
                                },
                                {
                                    "authorId": "143911112",
                                    "name": "Wei Han"
                                },
                                {
                                    "authorId": "27456119",
                                    "name": "Zarana Parekh"
                                },
                                {
                                    "authorId": "2158973314",
                                    "name": "Xin Li"
                                },
                                {
                                    "authorId": null,
                                    "name": "Han Zhang"
                                },
                                {
                                    "authorId": "1387994164",
                                    "name": "Jason Baldridge"
                                },
                                {
                                    "authorId": "48607963",
                                    "name": "Yonghui Wu"
                                }
                            ],
                            "year": 2022,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 1133
                        },
                        "score": 0.95263671875
                    },
                    {
                        "id": "(Sadat et al., 2024)",
                        "snippets": [
                            "In this paper, we analyze the methodology behind classifier-free guidance and show theoretically that similar behavior can be achieved without additional training of an unconditional model. The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score."
                        ],
                        "paper": {
                            "corpus_id": 270923987,
                            "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2261742393",
                                    "name": "Seyedmorteza Sadat"
                                },
                                {
                                    "authorId": "2204861903",
                                    "name": "Manuel Kansy"
                                },
                                {
                                    "authorId": "1466533438",
                                    "name": "Otmar Hilliges"
                                },
                                {
                                    "authorId": "145848224",
                                    "name": "Romann M. Weber"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 14
                        },
                        "score": 0.93798828125
                    },
                    {
                        "id": "(Basu et al., 2024)",
                        "snippets": [
                            "During the inference process, the regulation of image generation involves the utilization of classifier-free guidance, as outlined in (Ho, 2022) which incorporates scores from both the conditional and unconditional diffusion models at each time-step.Specifically, the classifier-free guidance is applied at each time-step to combine the conditional (\u03f5 \u03b8 (z t , c, t)) and unconditional score estimates (\u03f5 \u03b8 (z t , t)).\n\nThe result is a combined score denoted as \u03b5(z t , c, t).\n\n\u03b5(z t , c, t) = \u03f5 \u03b8 (z t , c, t) + \u03b1 (\u03f5 \u03b8 (z t , c, t) \u2212 \u03f5 \u03b8 (z t , t)) , \u2200t \u2208 [T, 1] .\n\n(1) This combined score is used to update the latent z t using DDIM sampling (Song et al., 2020) at each time-step to obtain the final latent code z 0 ."
                        ],
                        "paper": {
                            "corpus_id": 269502576,
                            "title": "On Mechanistic Knowledge Localization in Text-to-Image Generative Models",
                            "authors": [
                                {
                                    "authorId": "2114710333",
                                    "name": "Samyadeep Basu"
                                },
                                {
                                    "authorId": "2204576892",
                                    "name": "Keivan Rezaei"
                                },
                                {
                                    "authorId": "1962835975",
                                    "name": "Priyatham Kattakinda"
                                },
                                {
                                    "authorId": "2317012495",
                                    "name": "Ryan A. Rossi"
                                },
                                {
                                    "authorId": "2299942084",
                                    "name": "Cherry Zhao"
                                },
                                {
                                    "authorId": "2061209811",
                                    "name": "V. Morariu"
                                },
                                {
                                    "authorId": "1977256",
                                    "name": "Varun Manjunatha"
                                },
                                {
                                    "authorId": "34389431",
                                    "name": "S. Feizi"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 15
                        },
                        "score": 0.93798828125
                    },
                    {
                        "id": "(Dong et al., 2023)",
                        "snippets": [
                            "To this end, the classifier-free guidance technique is proposed, where the prediction for each step is a combination of conditional and unconditional predictions. Formally, let c = \u03c4 \u03b8 (P) be the conditional embedding vector and \u2205 = \u03c4 \u03b8 (\"\") be the unconditional one, the classifierfree guidance prediction is calculated by: \n\nwhere \u03c9 is the guidance scale parameter."
                        ],
                        "paper": {
                            "corpus_id": 258556958,
                            "title": "Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "23677993",
                                    "name": "Wenkai Dong"
                                },
                                {
                                    "authorId": "2216675591",
                                    "name": "Song Xue"
                                },
                                {
                                    "authorId": "2067781481",
                                    "name": "Xiaoyue Duan"
                                },
                                {
                                    "authorId": "1488666685",
                                    "name": "Shumin Han"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 62
                        },
                        "score": 0.93017578125
                    },
                    {
                        "id": "(Ye et al., 2023)",
                        "snippets": [
                            "For the conditional diffusion models, classifier guidance (Dhariwal et al., 2021) is a straightforward technique used to balance image fidelity and sample diversity by utilizing gradients from a separately trained classifier. To eliminate the need for training a classifier independently, classifier-free guidance [39] is often employed as an alternative method. In this approach, the conditional and unconditional diffusion models are jointly trained by randomly dropping c during training. In the sampling stage, the predicted noise is calculated based on the prediction of both the conditional model \u03f5 \u03b8 (x t , c, t) and unconditional model \u03f5 \u03b8 (x t , t): \n\nhere, w, often named guidance scale or guidance weight, is a scalar value that adjusts the alignment with condition c. \n\nFor text-to-image diffusion models, classifier-free guidance plays a crucial role in enhancing the image-text alignment of generated samples."
                        ],
                        "paper": {
                            "corpus_id": 260886966,
                            "title": "IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2145877255",
                                    "name": "Hu Ye"
                                },
                                {
                                    "authorId": "2157209270",
                                    "name": "Jun Zhang"
                                },
                                {
                                    "authorId": "150301258",
                                    "name": "Siyi Liu"
                                },
                                {
                                    "authorId": null,
                                    "name": "Xiao Han"
                                },
                                {
                                    "authorId": "2150081263",
                                    "name": "Wei Yang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 807
                        },
                        "score": 0.93212890625
                    },
                    {
                        "id": "(Baykal et al., 2023)",
                        "snippets": [
                            "Ho and Salimans (2021) propose classifier-free guidance method that does not require a separate classifier. The conditioning information y is periodically utilized while it is dropped out at the remaining time. Therefore, a single model can be used for both unconditional and conditional generation. Ho and Salimans (2021) derive that the unconditional \u03b8 (x t , t) and conditional \u03b8 (x t , t, y) estimations can be used to represent the gradients of the classifier as: \n\nEquation 7 implies that an implicit classifier can eliminate the need for an explicit classifier, and Ho and Salimans (2021) report better results with the classifier-free guidance compared to the explicit classifier guidance."
                        ],
                        "paper": {
                            "corpus_id": 259341599,
                            "title": "ProtoDiffusion: Classifier-Free Diffusion Guidance with Prototype Learning",
                            "authors": [
                                {
                                    "authorId": "51222946",
                                    "name": "Gulcin Baykal"
                                },
                                {
                                    "authorId": "2213299635",
                                    "name": "Halil Faruk Karagoz"
                                },
                                {
                                    "authorId": "2092548838",
                                    "name": "T. Binhuraib"
                                },
                                {
                                    "authorId": "2256988285",
                                    "name": "Gozde Unal"
                                }
                            ],
                            "year": 2023,
                            "venue": "Asian Conference on Machine Learning",
                            "n_citations": 3
                        },
                        "score": 0.95849609375
                    },
                    {
                        "id": "(Yang et al., 2023)",
                        "snippets": [
                            "Classifier-guidance (Dhariwal et al., 2021) provides a way for diffusion model to achieve conditional generation by using the gradient of a separately trained classifier p(y|x t ) during sampling. As a more efficient technique, classifier-free guidance [14, 24] replaces the noise estimator by a combination of conditional and unconditional model, without requirement of p(y|x t ): \n\nwhere y is the class label or text embedding from language model [24], \u03c9 \u2265 1 denotes the guidance scale and trivially increasing \u03c9 will amplify the effect of conditional input."
                        ],
                        "paper": {
                            "corpus_id": 260886956,
                            "title": "LAW-Diffusion: Complex Scene Generation by Diffusion with Layouts",
                            "authors": [
                                {
                                    "authorId": "2118583061",
                                    "name": "Binbin Yang"
                                },
                                {
                                    "authorId": "2157838841",
                                    "name": "Yinzheng Luo"
                                },
                                {
                                    "authorId": "49865638",
                                    "name": "Ziliang Chen"
                                },
                                {
                                    "authorId": "2749191",
                                    "name": "Guangrun Wang"
                                },
                                {
                                    "authorId": "40250403",
                                    "name": "Xiaodan Liang"
                                },
                                {
                                    "authorId": "2148303324",
                                    "name": "Liang Lin"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 15
                        },
                        "score": 0.951171875
                    },
                    {
                        "id": "(Dhariwal et al., 2021)",
                        "snippets": [
                            "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                        ],
                        "paper": {
                            "corpus_id": 234357997,
                            "title": "Diffusion Models Beat GANs on Image Synthesis",
                            "authors": [
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "38967461",
                                    "name": "Alex Nichol"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 7951
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jagpal et al., 2025)",
                        "snippets": [
                            "Classifier-free guidance (Ho, 2022) enables conditioning diffusion models on text without relying on an external classifier. \n\nDuring training, the model alternates between a specific condition y (e.g., text) and a null condition \u2205, learning both the conditional s \u03b8 (x t , t | y) and unconditional s \u03b8 (x t , t | \u2205) score functions. The final conditional score function at sampling is given by: \n\nwhere \u03b3 controls the influence of the text condition. Adjusting the guidance scale, \u03b3, allows the diffusion process to produce images that align closely with the text prompt."
                        ],
                        "paper": {
                            "corpus_id": 277633776,
                            "title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation",
                            "authors": [
                                {
                                    "authorId": "2354333652",
                                    "name": "Diljeet Jagpal"
                                },
                                {
                                    "authorId": "2355422362",
                                    "name": "Xi Chen"
                                },
                                {
                                    "authorId": "145460361",
                                    "name": "Vinay P. Namboodiri"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9326171875
                    },
                    {
                        "id": "(Huang et al., 2023)",
                        "snippets": [
                            "Classifier-free guidance [17] is a method for trading off the quality and diversity of samples generated by a diffusion model. It is commonly used in class-conditional and text-conditional image generation to improve the visual quality of generated images and to make sampled images better correspond with their conditioning. Classifier-free guidance effectively shifts probability mass toward",
                            "Guidance for conditions. For our task, the scoring network \u03b5  (  ,   ,  T ) has three conditions: the input image   , text prompt  T , and self-attention map   . We find it beneficial to leverage classifier-free guidance:",
                            "concerning each condition. Liu et al. (Liu et al., 2022) demonstrate that a conditional diffusion model can compose score estimates from multiple different conditioning values. We introduce three guidance scales,   ,  T , and   , which can be adjusted to trade off how strongly the generated samples correspond with the conditions."
                        ],
                        "paper": {
                            "corpus_id": 258564566,
                            "title": "Style-A-Video: Agile Diffusion for Arbitrary Text-Based Video Style Transfer",
                            "authors": [
                                {
                                    "authorId": "2186281333",
                                    "name": "Nisha Huang"
                                },
                                {
                                    "authorId": "2108078624",
                                    "name": "Yu-xin Zhang"
                                },
                                {
                                    "authorId": "40441149",
                                    "name": "Weiming Dong"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Signal Processing Letters",
                            "n_citations": 17
                        },
                        "score": 0.94091796875
                    },
                    {
                        "id": "(Liu et al., 2022)",
                        "snippets": [
                            "Large text-guided diffusion models, such as DALLE-2, are able to generate stunning photorealistic images given natural language descriptions. While such models are highly flexible, they struggle to understand the composition of certain concepts, such as confusing the attributes of different objects or relations between objects. In this paper, we propose an alternative structured approach for compositional generation using diffusion models. An image is generated by composing a set of diffusion models, with each of them modeling a certain component of the image. To do this, we interpret diffusion models as energy-based models in which the data distributions defined by the energy functions may be explicitly combined. The proposed method can generate scenes at test time that are substantially more complex than those seen in training, composing sentence descriptions, object relations, human facial attributes, and even generalizing to new combinations that are rarely seen in the real world. We further illustrate how our approach may be used to compose pre-trained text-guided diffusion models and generate photorealistic images containing all the details described in the input descriptions, including the binding of certain object attributes that have been shown difficult for DALLE-2. These results point to the effectiveness of the proposed method in promoting structured generalization for visual generation. Project page: https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/"
                        ],
                        "paper": {
                            "corpus_id": 249375227,
                            "title": "Compositional Visual Generation with Composable Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2087010550",
                                    "name": "Nan Liu"
                                },
                                {
                                    "authorId": "145015904",
                                    "name": "Shuang Li"
                                },
                                {
                                    "authorId": "15394275",
                                    "name": "Yilun Du"
                                },
                                {
                                    "authorId": "143805211",
                                    "name": "A. Torralba"
                                },
                                {
                                    "authorId": "1763295",
                                    "name": "J. Tenenbaum"
                                }
                            ],
                            "year": 2022,
                            "venue": "European Conference on Computer Vision",
                            "n_citations": 528
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2022)",
                        "snippets": [
                            "Model-Based Classifier-Free Guidance. With the above-presented LDMs, we introduce our approach, inspired by classifier-free guidance, to overcome overfitting when fine-tuning LDMs with one image. Classifier-free guidance [15] is a technique widely adopted by prior text-to-image diffusion models [32,35]. A single diffusion model is trained using conditional and unconditional objectives by randomly dropping the condition during training. When sampling, a linear combination of the conditional and unconditional score estimation is used: where \u03f5 \u03b8 (z t , c) and \u03f5 \u03b8 (z t ) are the conditional and unconditional \u03f5-predictions, c is the conditioning vector generated by \u03c4 \u03b8 , and w is the weight for the guidance. The predication is performed using the Tweedie's formula [7], namely, z t \u2212 \u221a 1 \u2212 \u1fb1t \u03b5\u03b8 / \u221a \u1fb1t , where \u1fb1t is a function of t that affects the sampling quality."
                        ],
                        "paper": {
                            "corpus_id": 254408758,
                            "title": "SINE: SINgle Image Editing with Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2128662401",
                                    "name": "Zhixing Zhang"
                                },
                                {
                                    "authorId": "3471102",
                                    "name": "Ligong Han"
                                },
                                {
                                    "authorId": "2461629",
                                    "name": "Arna Ghosh"
                                },
                                {
                                    "authorId": "1711560",
                                    "name": "Dimitris N. Metaxas"
                                },
                                {
                                    "authorId": "2111473627",
                                    "name": "Jian Ren"
                                }
                            ],
                            "year": 2022,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 160
                        },
                        "score": 0.9267578125
                    },
                    {
                        "id": "(Huang et al., 2024)",
                        "snippets": [
                            "Text-to-image diffusion models incorporate additional text conditional variables c in the noise prediction model to predict the conditional noise \u03f5 \u03b8 (x t , c, t) and guide the generation.A classifier-free guidance technique (Ho, 2022) is typically adopted, enabling the utilization of text-conditioned guidance during training without the need for a classifier.The denoising model is trained to handle both conditioned input, where a text prompt is provided, and unconditioned input, where the prompt is replaced with \u2205.This allows for the representation of the guidance direction as the translation from the conditioned input \u03f5 \u03b8 (x t , c, t) to the unconditioned input \u03f5 \u03b8 (x t , \u2205, t).The guidance is performed with a guidance scale s by"
                        ],
                        "paper": {
                            "corpus_id": 268351323,
                            "title": "Active Generation for Image Classification",
                            "authors": [
                                {
                                    "authorId": "2265957484",
                                    "name": "Tao Huang"
                                },
                                {
                                    "authorId": "2290839014",
                                    "name": "Jiaqi Liu"
                                },
                                {
                                    "authorId": "2111867716",
                                    "name": "Shan You"
                                },
                                {
                                    "authorId": "2155590441",
                                    "name": "Chang Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "European Conference on Computer Vision",
                            "n_citations": 5
                        },
                        "score": 0.93115234375
                    },
                    {
                        "id": "(Sueyoshi et al., 2023)",
                        "snippets": [
                            "Training-Free Guidance Even when a diffusion model is designed without condition c, it can reproduce the conditional probability p(x|c) without retraining. This is because, from a diffusion model p(x) and a separate classifier p(c|x) for class label c, one can obtain the gradient of the conditional log-probability, \u2207 x log p(x|c) = \u2207 x log p(c|x) + \u2207 x log p(x). Although grounded in probability theory, what it practically offers is additional guidance \u2207 x log p(c|x) for updating images, which is generalized as classifier guidance (Dhariwal et al., 2021)",
                            "When the diffusion model is conditioned on c, the difference between conditional and unconditional updates serves as classifier-free guidance, which can adjust the fidelity of the generated image to condition c (Ho, 2022)."
                        ],
                        "paper": {
                            "corpus_id": 265466084,
                            "title": "Predicated Diffusion: Predicate Logic-Based Attention Guidance for Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2268492302",
                                    "name": "Kota Sueyoshi"
                                },
                                {
                                    "authorId": "2268495650",
                                    "name": "Takashi Matsubara"
                                }
                            ],
                            "year": 2023,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 8
                        },
                        "score": 0.9189453125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Extensions Beyond Conditional Diffusion Models",
                "tldr": "Classifier-Free Guidance has expanded beyond diffusion models to enhance other generative frameworks including Flow Matching, language models, and one-step generators. These extensions introduce novel guidance mechanisms like contrastive guidance, negative guidance, and hierarchical category guidance that enable more precise control across diverse modalities. (16 sources)",
                "text": "\nWhile Classifier-Free Guidance (CFG) was originally developed for diffusion models, its utility has expanded significantly to other generative frameworks. Zheng et al. demonstrated that CFG can be integrated into Flow Matching (FM) models, which represent an alternative simulation-free approach using Continuous Normalizing Flows (CNFs) <Paper corpusId=\"265351587\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>. This expansion beyond diffusion models indicates the versatility of the CFG methodology as a general conditioning approach for generative tasks.\n\nRecent research has also introduced variations that extend the basic CFG concept in novel directions. Hierarchical CFG approaches have emerged, where Pan et al. proposed a fine-grained CFG sampling method that incorporates hierarchical category label information <Paper corpusId=\"268041325\" paperTitle=\"(Pan et al., 2024)\" isShortName></Paper>. This approach replaces the unconditional model with a superclass conditional model, allowing the system to leverage both specific category knowledge and broader class relationships, resulting in enhanced control over image generation.\n\nAnother significant extension is negative guidance, where Li et al. demonstrated that CFG can be adapted to selectively \"forget\" specific concepts during image editing while maintaining desired attributes <Paper corpusId=\"270123253\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This implementation combines positive guidance for learning text prompts with negative guidance that allows the model to gradually remove unwanted concepts from the original image <Paper corpusId=\"261276613\" paperTitle=\"(Gandikota et al., 2023)\" isShortName></Paper>. The balance between positive and negative guidance signals can be controlled through weighting parameters, offering fine-grained manipulation of the generation process.\n\nContrastive Guidance represents another innovative extension, where Wu et al. characterized intended factors using two carefully crafted prompts that differ in minimal tokens <Paper corpusId=\"267770589\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. This approach provides continuous, rig-like controls for text-to-image generation and has shown benefits in guiding domain-specific diffusion models and improving zero-shot image editors.\n\nFor one-step diffusion models, Nguyen et al. introduced SNOOPI, which enhances guidance during both training and inference <Paper corpusId=\"274446026\" paperTitle=\"(Nguyen et al., 2024)\" isShortName></Paper>. Their approach includes Proper Guidance-SwiftBrush (PG-SB), which employs random-scale classifier-free guidance, and Negative-Away Steer Attention (NASA), which integrates negative prompts via cross-attention to suppress undesired elements in generated images.\n\nThe concept of spatially varying guidance has also emerged, with Azarian et al. proposing \"segmentation-free guidance\" that dynamically adjusts negative prompts for each image patch based on attention maps <Paper corpusId=\"271051241\" paperTitle=\"(Azarian et al., 2024)\" isShortName></Paper>. This approach enables different parts of an image to receive different guidance without requiring explicit segmentation, resulting in more coherent and controlled generation.\n\nIn large language model (LLM) applications, Zhuang et al. adapted CFG for image token generation by subtracting the probability of unconditional generation from the logits distribution of conditional generation <Paper corpusId=\"275787953\" paperTitle=\"(Zhuang et al., 2025)\" isShortName></Paper>. This implementation uses Gaussian noise features as conditional input to simulate unconditional generation, demonstrating how CFG principles can transfer to different architectures.\n\nCFG has also been extended to handle various conditioning inputs beyond text, enabling models to control synthesized samples with multiple modalities such as edge maps, human pose skeletons, segmentation maps, depth, and normals <Paper corpusId=\"273811150\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper> <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper> <Paper corpusId=\"256827727\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>. This multimodal conditioning capability has greatly expanded the application scope of generative models.\n\nRecent work by Kwon et al. has introduced a geometric perspective on enhancing CFG performance by filtering the singular vectors of both conditional and unconditional scores using singular value decomposition <Paper corpusId=\"277271753\" paperTitle=\"(Kwon et al., 2025)\" isShortName></Paper>. This process aligns the unconditional score with the conditional score, refining the sampling trajectory to stay closer to the data manifold.\n\nFor scenarios requiring efficient generation, Ohayon et al. developed Compressed CFG (CCFG), which allows generating compressed conditional samples using any pair of conditional and unconditional diffusion models while controlling the trade-off between generation quality and input fidelity <Paper corpusId=\"276094842\" paperTitle=\"(Ohayon et al., 2025)\" isShortName></Paper>.\n\nAddressing limitations in standard CFG's static unconditional input, Li et al. proposed a dynamic approach that adapts to model uncertainty during iterative generation processes <Paper corpusId=\"278910862\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. This approach recognizes that model uncertainty varies throughout the generation process and adjusts guidance accordingly.\n\nThese extensions demonstrate that the fundamental principles of CFG have proven valuable beyond their original context, influencing a broad spectrum of generative modeling approaches across different architectures and tasks <Paper corpusId=\"277955619\" paperTitle=\"(Ifriqi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"276422090\" paperTitle=\"(Zheng et al., 2025)\" isShortName></Paper>. The continued evolution of CFG variants reflects its flexibility as a core technique for enhancing conditional generation quality and control across the generative AI landscape.",
                "citations": [
                    {
                        "id": "(Zheng et al., 2023)",
                        "snippets": [
                            "Classifier-free guidance is a key component for enhancing the performance of conditional generative models across diverse tasks. While it has previously demonstrated remarkable improvements for the sample quality, it has only been exclusively employed for diffusion models. In this paper, we integrate classifier-free guidance into Flow Matching (FM) models, an alternative simulation-free approach that trains Continuous Normalizing Flows (CNFs) based on regressing vector fields."
                        ],
                        "paper": {
                            "corpus_id": 265351587,
                            "title": "Guided Flows for Generative Modeling and Decision Making",
                            "authors": [
                                {
                                    "authorId": "2166847",
                                    "name": "Qinqing Zheng"
                                },
                                {
                                    "authorId": "2267723599",
                                    "name": "Matt Le"
                                },
                                {
                                    "authorId": "2219927868",
                                    "name": "Neta Shaul"
                                },
                                {
                                    "authorId": "3232072",
                                    "name": "Y. Lipman"
                                },
                                {
                                    "authorId": "2267723293",
                                    "name": "Aditya Grover"
                                },
                                {
                                    "authorId": "2253976277",
                                    "name": "Ricky T. Q. Chen"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 46
                        },
                        "score": 0.96533203125
                    },
                    {
                        "id": "(Pan et al., 2024)",
                        "snippets": [
                            "Classifier-free guidance has supplanted classifier guided sampling to achieve a trade-off between the diversity and quality of sampling. While reducing training complexity, this approach secures high-quality image generation. Currently, classifier-free guidance is widely utilized in various diffusion models. We attempt to introduce hierarchical category label information because knowing which superclass each category belongs to is helpful for capturing intricate details and enhancing the overall quality of the generated images. Therefore, we introduce a fine-grained classifier-free guidance sampling method, yielding improved results under specific conditions. Integrated with TieredEmbedder, this approach leverages superclass conditional information to enhance control over generated images. Conventional classifierfree guidance involves training both conditional and unconditional generative models. In our approach, a superclass conditional generative model is introduced to replace the unconditional model. During training, we use the same neural network to parameterize both the conditional model and the superclass conditional model, leveraging a score estimator \u03f5 \u03b8 for optimization guidance. Subclass labels c are probabilistically replaced with their corresponding superclass labels c p to achieve the learning of superclass embeddings. During sampling, trained superclass and subclass embeddings are input into the DiT network to obtain predictive noise. Our method combines conditional scores estimation \u03f5 \u03b8 (x t | c) and superclass conditional scores estimation \u03f5 \u03b8 (x t | c p ) to guide specific category image generation, with the specific formula provided below: \n\nwhere \u03c9 is the guidance scale, and \u03b5\u03b8 is the modified score. \n\nBy thoroughly exploiting the distributional characteristics of superclasses and subclasses, our sampling method achieves a finer control than classifier-free guidance. We can capture commonalities among subclasses within the same superclass and simultaneously capture distinct distributions between different superclasses, thereby making the learned distributions of each category more closely approximate real-world situations."
                        ],
                        "paper": {
                            "corpus_id": 268041325,
                            "title": "FineDiffusion: Scaling up Diffusion Models for Fine-grained Image Generation with 10, 000 Classes",
                            "authors": [
                                {
                                    "authorId": "2288865816",
                                    "name": "Ziying Pan"
                                },
                                {
                                    "authorId": "2288886271",
                                    "name": "Kun Wang"
                                },
                                {
                                    "authorId": "2243959855",
                                    "name": "Gang Li"
                                },
                                {
                                    "authorId": "2242676809",
                                    "name": "Feihong He"
                                },
                                {
                                    "authorId": "2288032594",
                                    "name": "Xiwang Li"
                                },
                                {
                                    "authorId": "2289803819",
                                    "name": "Yongxuan Lai"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied intelligence (Boston)",
                            "n_citations": 1
                        },
                        "score": 0.958984375
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Classifier-free diffusion guidance is commonly employed in class-conditional and text-conditional image generation tasks to enhance the visual quality of the generated images and to ensure that the sampled outputs better correspond to their respective conditioning factors.Typically, diffusion models use a guidance signal, where the model is conditioned on the desired concept or attribute to generate samples aligned with that concept (Gandikota et al., 2023).\n\nOur approach employs negative guidance to allow the diffusion model to gradually forget a specified concept in the original image while applying positive guidance for learning text prompts.Combining with Eq 1 and Eq 2, we compute the following score estimate using Classifier-Free Guidance during inference:\n\nwhere c p and c n are input prompt and forgetting concepts, w and \u03b7 are weights for controlling the balance between the positive and negative guidance signals in the final CFG score estimate.A higher value of eta will give more weight to the negative guidance, allowing tuning the level of concept forgetting during the diffusion process."
                        ],
                        "paper": {
                            "corpus_id": 270123253,
                            "title": "Text Guided Image Editing with Automatic Concept Locating and Forgetting",
                            "authors": [
                                {
                                    "authorId": "2268721096",
                                    "name": "Jia Li"
                                },
                                {
                                    "authorId": "2153121378",
                                    "name": "Lijie Hu"
                                },
                                {
                                    "authorId": "2304013931",
                                    "name": "Zhixian He"
                                },
                                {
                                    "authorId": "2253808698",
                                    "name": "Jingfeng Zhang"
                                },
                                {
                                    "authorId": "2268675026",
                                    "name": "Tianhang Zheng"
                                },
                                {
                                    "authorId": "2268815109",
                                    "name": "Di Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.93017578125
                    },
                    {
                        "id": "(Gandikota et al., 2023)",
                        "snippets": [
                            "Text-to-image models suffer from various safety issues that may limit their suitability for deployment. Previous methods have separately addressed individual issues of bias, copyright, and offensive content in text-to-image models. However, in the real world, all of these issues appear simultaneously in the same model. We present a method that tackles all issues with a single approach. Our method, Unified Concept Editing (UCE), edits the model without training using a closed-form solution, and scales seamlessly to concurrent edits on text-conditional diffusion models.We present scalable simultaneous debiasing, style erasure, and content moderation by editing text-to-image projections, and perform extensive experiments demonstrating improved efficacy and scalability over prior work. Our code is available at unified.baulab.info."
                        ],
                        "paper": {
                            "corpus_id": 261276613,
                            "title": "Unified Concept Editing in Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "52017367",
                                    "name": "Rohit Gandikota"
                                },
                                {
                                    "authorId": "1398583303",
                                    "name": "Hadas Orgad"
                                },
                                {
                                    "authorId": "2083259",
                                    "name": "Yonatan Belinkov"
                                },
                                {
                                    "authorId": "2235064038",
                                    "name": "Joanna Materzy'nska"
                                },
                                {
                                    "authorId": "144159726",
                                    "name": "David Bau"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                            "n_citations": 192
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wu et al., 2024)",
                        "snippets": [
                            "The key idea of our method, Contrastive Guidance, is to characterize an intended factor with two prompts that differ in minimal tokens: the positive prompt describes the image to be synthesized, and the baseline prompt serves as a\"baseline\"that disentangles other factors. Contrastive Guidance is a general method we illustrate whose benefits in three scenarios: (1) to guide domain-specific diffusion models trained on an object class, (2) to gain continuous, rig-like controls for text-to-image generation, and (3) to improve the performance of zero-shot image editors."
                        ],
                        "paper": {
                            "corpus_id": 267770589,
                            "title": "Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2270764731",
                                    "name": "C. Wu"
                                },
                                {
                                    "authorId": "2239102325",
                                    "name": "Fernando De la Torre"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.95654296875
                    },
                    {
                        "id": "(Nguyen et al., 2024)",
                        "snippets": [
                            "Classifier-free Guidance (CFG) [15] is an inference technique designed to enhance the quality of generated images by blending the predictions from both a conditional and an unconditional model. At each sampling step, CFG adjusts the denoiser's output using a control parameter \u03ba > 1, allowing for controlled guidance that aligns more closely with the desired conditions",
                            "Negative Prompts provide enhanced control by suppressing unwanted features in the generated content. Instead of producing an unconditional output, the model generates an output conditioned on the negative prompt y \u2212 , as follows:",
                            "This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images."
                        ],
                        "paper": {
                            "corpus_id": 274446026,
                            "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
                            "authors": [
                                {
                                    "authorId": "2275329323",
                                    "name": "Viet Nguyen"
                                },
                                {
                                    "authorId": "2333424276",
                                    "name": "Anh Aengus Nguyen"
                                },
                                {
                                    "authorId": "2276606039",
                                    "name": "T. Dao"
                                },
                                {
                                    "authorId": "2261741144",
                                    "name": "Khoi Nguyen"
                                },
                                {
                                    "authorId": "2269462407",
                                    "name": "Cuong Pham"
                                },
                                {
                                    "authorId": "2275127531",
                                    "name": "Toan Tran"
                                },
                                {
                                    "authorId": "2327046351",
                                    "name": "Anh Tran"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.94482421875
                    },
                    {
                        "id": "(Azarian et al., 2024)",
                        "snippets": [
                            "Our objective is to dynamically adjust the negative prompt for each image patch. We examine attention maps within the diffusion model, specifically where it interacts with the text prompt embeddings. For each patch of the attention map, we aim to find the object in the positive prompt with the highest correlation. Subsequently, this selected object is excluded from the negative prompt interacting with that specific patch. Accordingly, the forward pass of the diffusion model carries out as if each patch cross-attends dynamically with a different negative prompt. Furthermore, the corresponding attention weight is adjusted to account for self-attention interactions. Since this proposed method of guidance does not involve any segmentation network as a guidance function, we term this method as segmentationfree guidance."
                        ],
                        "paper": {
                            "corpus_id": 271051241,
                            "title": "Segmentation-Free Guidance for Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2075053",
                                    "name": "K. Azarian"
                                },
                                {
                                    "authorId": "49950690",
                                    "name": "Debasmit Das"
                                },
                                {
                                    "authorId": "2293594635",
                                    "name": "Qiqi Hou"
                                },
                                {
                                    "authorId": "2253777162",
                                    "name": "F. Porikli"
                                }
                            ],
                            "year": 2024,
                            "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
                            "n_citations": 0
                        },
                        "score": 0.97900390625
                    },
                    {
                        "id": "(Zhuang et al., 2025)",
                        "snippets": [
                            "Classifier-free guidance (CFG) significantly enhances the capability of generative diffusion models to produce samples of exceptionally high fidelity. This approach integrates conditional generative models with the distribution estimation of unconditional models trained concurrently, thereby improving the overall quality of generation. Inspired by DALL-E 2 [68], VAR [84] and VAR-CLIP [100], we employ Gaussian noise features as conditional input to simulate unconditional generation. Subsequently, we derive the final distribution of image token outputs by subtracting the probability of uncon- ditional generation from the logits distribution of conditional generation. Specifically, assuming that the image features obtained through LLMs are denoted as H i , we derive the translated visual generation features through a mapping layer as H g = Projector(H i ). Subsequently, we employ the Classifier-Free Guidance (CFG) strategy to obtain the final features of the visual generation tokens, which can be expressed as: \n\nwhere H e represents randomly initialized Gaussian noise, serving as the unconditional feature, and \u03bb denotes the scale hyperparameter for the CFG strategy."
                        ],
                        "paper": {
                            "corpus_id": 275787953,
                            "title": "VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model",
                            "authors": [
                                {
                                    "authorId": "2293439758",
                                    "name": "Xianwei Zhuang"
                                },
                                {
                                    "authorId": "2306871988",
                                    "name": "Yuxin Xie"
                                },
                                {
                                    "authorId": "2276775533",
                                    "name": "Yufan Deng"
                                },
                                {
                                    "authorId": "2307892931",
                                    "name": "Liming Liang"
                                },
                                {
                                    "authorId": "2341529875",
                                    "name": "Jinghan Ru"
                                },
                                {
                                    "authorId": "2342470685",
                                    "name": "Yuguo Yin"
                                },
                                {
                                    "authorId": "2260859476",
                                    "name": "Yuexian Zou"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 11
                        },
                        "score": 0.9375
                    },
                    {
                        "id": "(Luo et al., 2023)",
                        "snippets": [
                            "Further research has shown that guidance can be derived from the generative model itself without a classifier, a method known as classifier-free guidance. This advancement unlocks various forms of conditional signals, leading to conditional generative models (Zhang et al., 2023), (Rombach et al., 2021) that control synthesized samples with multiple modalities, such as text, edge maps, human pose skeletons, segmentation maps, depth, and normals."
                        ],
                        "paper": {
                            "corpus_id": 273811150,
                            "title": "ReactFace: Online Multiple Appropriate Facial Reaction Generation in Dyadic Interactions.",
                            "authors": [
                                {
                                    "authorId": "2153561173",
                                    "name": "Cheng Luo"
                                },
                                {
                                    "authorId": "2275025172",
                                    "name": "Siyang Song"
                                },
                                {
                                    "authorId": "34181727",
                                    "name": "Weicheng Xie"
                                },
                                {
                                    "authorId": "73772115",
                                    "name": "Micol Spitale"
                                },
                                {
                                    "authorId": "2325909107",
                                    "name": "Zongyuan Ge"
                                },
                                {
                                    "authorId": "2121272943",
                                    "name": "Linlin Shen"
                                },
                                {
                                    "authorId": "2256439823",
                                    "name": "Hatice Gunes"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Transactions on Visualization and Computer Graphics",
                            "n_citations": 4
                        },
                        "score": 0.92578125
                    },
                    {
                        "id": "(Rombach et al., 2021)",
                        "snippets": [
                            "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
                        ],
                        "paper": {
                            "corpus_id": 245335280,
                            "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "1660819540",
                                    "name": "Robin Rombach"
                                },
                                {
                                    "authorId": "119843260",
                                    "name": "A. Blattmann"
                                },
                                {
                                    "authorId": "2053482699",
                                    "name": "Dominik Lorenz"
                                },
                                {
                                    "authorId": "35175531",
                                    "name": "Patrick Esser"
                                },
                                {
                                    "authorId": "1796707",
                                    "name": "B. Ommer"
                                }
                            ],
                            "year": 2021,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 15768
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models."
                        ],
                        "paper": {
                            "corpus_id": 256827727,
                            "title": "Adding Conditional Control to Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "17744884",
                                    "name": "Lvmin Zhang"
                                },
                                {
                                    "authorId": "36290866",
                                    "name": "Anyi Rao"
                                },
                                {
                                    "authorId": "1820412",
                                    "name": "Maneesh Agrawala"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 4175
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kwon et al., 2025)",
                        "snippets": [
                            "In this work, we introduce a novel approach that leverages a geometric perspective on the unconditional score to enhance CFG performance when conditional scores are available. Specifically, we propose a method that filters the singular vectors of both conditional and unconditional scores using singular value decomposition. This filtering process aligns the unconditional score with the conditional score, thereby refining the sampling trajectory to stay closer to the manifold."
                        ],
                        "paper": {
                            "corpus_id": 277271753,
                            "title": "TCFG: Tangential Damping Classifier-free Guidance",
                            "authors": [
                                {
                                    "authorId": "2182293854",
                                    "name": "Mingi Kwon"
                                },
                                {
                                    "authorId": "2352073121",
                                    "name": "Shin seong Kim"
                                },
                                {
                                    "authorId": "2351806103",
                                    "name": "Jaeseok Jeong. Yi Ting Hsiao"
                                },
                                {
                                    "authorId": "2253393666",
                                    "name": "Youngjung Uh"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9677734375
                    },
                    {
                        "id": "(Ohayon et al., 2025)",
                        "snippets": [
                            "Here, we introduce a new CFG method that allows generating compressed conditional samples using any pair of conditional and unconditional diffusion models, while controlling the tradeoff between generation quality and the fidelity to the inputs",
                            "We coin our method Compressed CFG (CCFG)."
                        ],
                        "paper": {
                            "corpus_id": 276094842,
                            "title": "Compressed Image Generation with Denoising Diffusion Codebook Models",
                            "authors": [
                                {
                                    "authorId": "51228065",
                                    "name": "Guy Ohayon"
                                },
                                {
                                    "authorId": "2245392181",
                                    "name": "Hila Manor"
                                },
                                {
                                    "authorId": "1880407",
                                    "name": "T. Michaeli"
                                },
                                {
                                    "authorId": "2266840963",
                                    "name": "Michael Elad"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.94775390625
                    },
                    {
                        "id": "(Li et al., 2025)",
                        "snippets": [
                            "Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically."
                        ],
                        "paper": {
                            "corpus_id": 278910862,
                            "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking",
                            "authors": [
                                {
                                    "authorId": "2363590342",
                                    "name": "Pengxiang Li"
                                },
                                {
                                    "authorId": "2362879323",
                                    "name": "Shilin Yan"
                                },
                                {
                                    "authorId": "2362728158",
                                    "name": "Joey Tsai"
                                },
                                {
                                    "authorId": "2291314199",
                                    "name": "Renrui Zhang"
                                },
                                {
                                    "authorId": "2363570661",
                                    "name": "Ruichuan An"
                                },
                                {
                                    "authorId": "145490494",
                                    "name": "Ziyu Guo"
                                },
                                {
                                    "authorId": "2303650253",
                                    "name": "Xiaowei Gao"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.9267578125
                    },
                    {
                        "id": "(Ifriqi et al., 2025)",
                        "snippets": [
                            "Guidance techniques are commonly used in diffusion and flow models to improve image quality and consistency for conditional generative tasks such as class-conditional and text-to-image generation. In particular, classifier-free guidance (CFG) -- the most widely adopted guidance technique -- contrasts conditional and unconditional predictions to improve the generated images. This results, however, in trade-offs across quality, diversity and consistency, improving some at the expense of others."
                        ],
                        "paper": {
                            "corpus_id": 277955619,
                            "title": "Entropy Rectifying Guidance for Diffusion and Flow Models",
                            "authors": [
                                {
                                    "authorId": "2329186126",
                                    "name": "Tariq Berrada Ifriqi"
                                },
                                {
                                    "authorId": "1456285042",
                                    "name": "Adriana Romero-Soriano"
                                },
                                {
                                    "authorId": "3325894",
                                    "name": "M. Drozdzal"
                                },
                                {
                                    "authorId": "2281637540",
                                    "name": "Jakob Verbeek"
                                },
                                {
                                    "authorId": "72492981",
                                    "name": "Alahari Karteek"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.93359375
                    },
                    {
                        "id": "(Zheng et al., 2025)",
                        "snippets": [
                            "A major challenge in text-to-image generation using diffusion models (Yu et al., 2024)(Liu et al., 2024)(Xu et al., 2024) is guiding the generative process to reflect the input text accurately. A widely adopted solution is classifier-free guidance (CFG, Ho & Salimans (2022)), which eliminates the need for external classifiers by training a unified model for both unconditional and conditional image generation. During inference, CFG achieves conditional generation by interpolating between the conditional and unconditional scores, effectively guiding the model to match the input text. This method has shown significant success in various text-to-image tasks (Balaji et al., 2022;Nichol et al., 2021;Ramesh et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 276422090,
                            "title": "RecDreamer: Consistent Text-to-3D Generation via Uniform Score Distillation",
                            "authors": [
                                {
                                    "authorId": "2220800052",
                                    "name": "Chenxi Zheng"
                                },
                                {
                                    "authorId": "2294183727",
                                    "name": "Yihong Lin"
                                },
                                {
                                    "authorId": "2220596660",
                                    "name": "Bangzhen Liu"
                                },
                                {
                                    "authorId": "2281155649",
                                    "name": "Xuemiao Xu"
                                },
                                {
                                    "authorId": "2273558537",
                                    "name": "Yongwei Nie"
                                },
                                {
                                    "authorId": "2257314718",
                                    "name": "Shengfeng He"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.93359375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Applications in NLP Tasks",
                "tldr": "Classifier-Free Guidance has been successfully extended from image generation to various NLP tasks, enabling more flexible conditioning for text generation, language modeling, and multimodal applications. These implementations leverage the same core principles while adapting to the unique requirements of language processing tasks. (9 sources)",
                "text": "\nWhile Classifier-Free Guidance (CFG) was initially developed for image generation, its principles have been effectively adapted to Natural Language Processing (NLP) tasks, demonstrating its versatility across different modalities. In text-to-image generation systems like Parti, CFG has been implemented by randomly replacing text prompts with padded tokens during training, which significantly improves image-text alignment, especially for challenging text prompts <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>.\n\nThe adaptation of CFG to NLP tasks stems from its ability to handle more versatile conditioning signals beyond simple class labels. Unlike earlier approaches that relied on external classifiers for guidance <Paper corpusId=\"235619773\" paperTitle=\"(Ho et al., 2021)\" isShortName></Paper> <Paper corpusId=\"234357997\" paperTitle=\"(Dhariwal et al., 2021)\" isShortName></Paper>, CFG's architecture allows for more flexible conditioning with text and other modalities <Paper corpusId=\"268033671\" paperTitle=\"(Huang et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>. This flexibility has proven particularly valuable in multimodal applications where language and visual elements intersect.\n\nRecent advances in text-to-image diffusion models have faced the challenge of accurately reflecting input text in generated images. CFG has emerged as a widely adopted solution for guiding the generative process to better match text inputs, showing significant success across various text-to-image tasks <Paper corpusId=\"276422090\" paperTitle=\"(Zheng et al., 2025)\" isShortName></Paper>. By interpolating between conditional and unconditional scores during inference, these models achieve better alignment between generated content and textual descriptions.\n\nThe expansion of CFG to handle multiple modalities has enabled conditional generative models to control synthesized content through various input forms, including text, edge maps, human pose skeletons, segmentation maps, depth information, and normal maps <Paper corpusId=\"273811150\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper> <Paper corpusId=\"256827727\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper>. This multimodal conditioning capability has created new possibilities for integrating language guidance with other forms of structural and semantic information.\n\nIn autoregressive text generation models, CFG has been applied by combining logits from conditional and unconditional models during inference, similar to its implementation in diffusion models <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>. This approach allows language models to benefit from the same quality-diversity trade-off mechanisms that have proven effective in image generation.\n\nThe integration of CFG into NLP frameworks demonstrates that its fundamental principles\u2014combining conditional and unconditional generation paths to enhance alignment with desired conditions\u2014transcend specific model architectures or data modalities. By adapting the core CFG methodology to language-specific representations and generation processes, researchers have extended its benefits to a wide range of text generation and multimodal language tasks.",
                "citations": [
                    {
                        "id": "(Yu et al., 2022)",
                        "snippets": [
                            "Classifier-free guidance (Ho, 2022) (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability",
                            "Classifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts."
                        ],
                        "paper": {
                            "corpus_id": 249926846,
                            "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
                            "authors": [
                                {
                                    "authorId": "2338016295",
                                    "name": "Jiahui Yu"
                                },
                                {
                                    "authorId": "2145139570",
                                    "name": "Yuanzhong Xu"
                                },
                                {
                                    "authorId": "23978705",
                                    "name": "Jing Yu Koh"
                                },
                                {
                                    "authorId": "1821711",
                                    "name": "Thang Luong"
                                },
                                {
                                    "authorId": "1396954703",
                                    "name": "Gunjan Baid"
                                },
                                {
                                    "authorId": "2331539",
                                    "name": "Zirui Wang"
                                },
                                {
                                    "authorId": "2053781980",
                                    "name": "Vijay Vasudevan"
                                },
                                {
                                    "authorId": "31702389",
                                    "name": "Alexander Ku"
                                },
                                {
                                    "authorId": "2118771180",
                                    "name": "Yinfei Yang"
                                },
                                {
                                    "authorId": "143990191",
                                    "name": "Burcu Karagol Ayan"
                                },
                                {
                                    "authorId": "2044655623",
                                    "name": "Ben Hutchinson"
                                },
                                {
                                    "authorId": "143911112",
                                    "name": "Wei Han"
                                },
                                {
                                    "authorId": "27456119",
                                    "name": "Zarana Parekh"
                                },
                                {
                                    "authorId": "2158973314",
                                    "name": "Xin Li"
                                },
                                {
                                    "authorId": null,
                                    "name": "Han Zhang"
                                },
                                {
                                    "authorId": "1387994164",
                                    "name": "Jason Baldridge"
                                },
                                {
                                    "authorId": "48607963",
                                    "name": "Yonghui Wu"
                                }
                            ],
                            "year": 2022,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 1133
                        },
                        "score": 0.95263671875
                    },
                    {
                        "id": "(Ho et al., 2021)",
                        "snippets": [
                            "We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation benchmark, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256, outperforming VQ-VAE-2."
                        ],
                        "paper": {
                            "corpus_id": 235619773,
                            "title": "Cascaded Diffusion Models for High Fidelity Image Generation",
                            "authors": [
                                {
                                    "authorId": "2126278",
                                    "name": "Jonathan Ho"
                                },
                                {
                                    "authorId": "2314850972",
                                    "name": "Chitwan Saharia"
                                },
                                {
                                    "authorId": "144333684",
                                    "name": "William Chan"
                                },
                                {
                                    "authorId": "1793739",
                                    "name": "David J. Fleet"
                                },
                                {
                                    "authorId": "144739074",
                                    "name": "Mohammad Norouzi"
                                },
                                {
                                    "authorId": "2887364",
                                    "name": "Tim Salimans"
                                }
                            ],
                            "year": 2021,
                            "venue": "Journal of machine learning research",
                            "n_citations": 1235
                        },
                        "score": 0
                    },
                    {
                        "id": "(Dhariwal et al., 2021)",
                        "snippets": [
                            "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                        ],
                        "paper": {
                            "corpus_id": 234357997,
                            "title": "Diffusion Models Beat GANs on Image Synthesis",
                            "authors": [
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "38967461",
                                    "name": "Alex Nichol"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 7951
                        },
                        "score": 0
                    },
                    {
                        "id": "(Huang et al._1, 2024)",
                        "snippets": [
                            "Early efforts (Ho et al., 2021), (Dhariwal et al., 2021), (Chao et al., 2022)- (246442182) usually incorporate the class-induced gradients via an additional pretrained classifier during sampling. However, Ho et al. (Ho, 2022) introduce the classifier-free guidance, which does not rely on an external classifier and allows for more versatile conditions, e.g., text, as guidance."
                        ],
                        "paper": {
                            "corpus_id": 268033671,
                            "title": "Diffusion Model-Based Image Editing: A Survey",
                            "authors": [
                                {
                                    "authorId": "2249841594",
                                    "name": "Yi Huang"
                                },
                                {
                                    "authorId": "2194958029",
                                    "name": "Jiancheng Huang"
                                },
                                {
                                    "authorId": "2247959941",
                                    "name": "Yifan Liu"
                                },
                                {
                                    "authorId": "2267694301",
                                    "name": "Mingfu Yan"
                                },
                                {
                                    "authorId": "2154657214",
                                    "name": "Jiaxi Lv"
                                },
                                {
                                    "authorId": "2267504760",
                                    "name": "Jianzhuang Liu"
                                },
                                {
                                    "authorId": "2273646978",
                                    "name": "Wei Xiong"
                                },
                                {
                                    "authorId": "2274091009",
                                    "name": "He Zhang"
                                },
                                {
                                    "authorId": "2247480051",
                                    "name": "Shifeng Chen"
                                },
                                {
                                    "authorId": "2288206453",
                                    "name": "Liangliang Cao"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 102
                        },
                        "score": 0.95068359375
                    },
                    {
                        "id": "(Ho, 2022)",
                        "snippets": [
                            "We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                        ],
                        "paper": {
                            "corpus_id": 249145348,
                            "title": "Classifier-Free Diffusion Guidance",
                            "authors": [
                                {
                                    "authorId": "2126278",
                                    "name": "Jonathan Ho"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 3970
                        },
                        "score": 0.9599609375
                    },
                    {
                        "id": "(Zheng et al., 2025)",
                        "snippets": [
                            "A major challenge in text-to-image generation using diffusion models (Yu et al., 2024)(Liu et al., 2024)(Xu et al., 2024) is guiding the generative process to reflect the input text accurately. A widely adopted solution is classifier-free guidance (CFG, Ho & Salimans (2022)), which eliminates the need for external classifiers by training a unified model for both unconditional and conditional image generation. During inference, CFG achieves conditional generation by interpolating between the conditional and unconditional scores, effectively guiding the model to match the input text. This method has shown significant success in various text-to-image tasks (Balaji et al., 2022;Nichol et al., 2021;Ramesh et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 276422090,
                            "title": "RecDreamer: Consistent Text-to-3D Generation via Uniform Score Distillation",
                            "authors": [
                                {
                                    "authorId": "2220800052",
                                    "name": "Chenxi Zheng"
                                },
                                {
                                    "authorId": "2294183727",
                                    "name": "Yihong Lin"
                                },
                                {
                                    "authorId": "2220596660",
                                    "name": "Bangzhen Liu"
                                },
                                {
                                    "authorId": "2281155649",
                                    "name": "Xuemiao Xu"
                                },
                                {
                                    "authorId": "2273558537",
                                    "name": "Yongwei Nie"
                                },
                                {
                                    "authorId": "2257314718",
                                    "name": "Shengfeng He"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.93359375
                    },
                    {
                        "id": "(Luo et al., 2023)",
                        "snippets": [
                            "Further research has shown that guidance can be derived from the generative model itself without a classifier, a method known as classifier-free guidance. This advancement unlocks various forms of conditional signals, leading to conditional generative models (Zhang et al., 2023), (Rombach et al., 2021) that control synthesized samples with multiple modalities, such as text, edge maps, human pose skeletons, segmentation maps, depth, and normals."
                        ],
                        "paper": {
                            "corpus_id": 273811150,
                            "title": "ReactFace: Online Multiple Appropriate Facial Reaction Generation in Dyadic Interactions.",
                            "authors": [
                                {
                                    "authorId": "2153561173",
                                    "name": "Cheng Luo"
                                },
                                {
                                    "authorId": "2275025172",
                                    "name": "Siyang Song"
                                },
                                {
                                    "authorId": "34181727",
                                    "name": "Weicheng Xie"
                                },
                                {
                                    "authorId": "73772115",
                                    "name": "Micol Spitale"
                                },
                                {
                                    "authorId": "2325909107",
                                    "name": "Zongyuan Ge"
                                },
                                {
                                    "authorId": "2121272943",
                                    "name": "Linlin Shen"
                                },
                                {
                                    "authorId": "2256439823",
                                    "name": "Hatice Gunes"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Transactions on Visualization and Computer Graphics",
                            "n_citations": 4
                        },
                        "score": 0.92578125
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models."
                        ],
                        "paper": {
                            "corpus_id": 256827727,
                            "title": "Adding Conditional Control to Text-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "17744884",
                                    "name": "Lvmin Zhang"
                                },
                                {
                                    "authorId": "36290866",
                                    "name": "Anyi Rao"
                                },
                                {
                                    "authorId": "1820412",
                                    "name": "Maneesh Agrawala"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 4175
                        },
                        "score": 0
                    },
                    {
                        "id": "(Rombach et al., 2021)",
                        "snippets": [
                            "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
                        ],
                        "paper": {
                            "corpus_id": 245335280,
                            "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "1660819540",
                                    "name": "Robin Rombach"
                                },
                                {
                                    "authorId": "119843260",
                                    "name": "A. Blattmann"
                                },
                                {
                                    "authorId": "2053482699",
                                    "name": "Dominik Lorenz"
                                },
                                {
                                    "authorId": "35175531",
                                    "name": "Patrick Esser"
                                },
                                {
                                    "authorId": "1796707",
                                    "name": "B. Ommer"
                                }
                            ],
                            "year": 2021,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 15768
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Recent Innovations and Advanced Techniques",
                "tldr": "Recent innovations in Classifier-Free Guidance have expanded its capabilities through techniques like Compressed CFG, geometric filtering of singular vectors, and integration with negative prompts. These advancements address specific limitations in the original approach while enhancing control and quality across diverse applications. (10 sources)",
                "text": "\n- **Compressed Classifier-Free Guidance (CCFG)**: Ohayon et al. introduced Compressed CFG, a novel approach that enables generating compressed conditional samples using any pair of conditional and unconditional diffusion models. This technique allows precise control over the trade-off between generation quality and fidelity to inputs. <Paper corpusId=\"276094842\" paperTitle=\"(Ohayon et al., 2025)\" isShortName></Paper>\n\n- **Geometric Filtering**: Kwon et al. developed a geometric enhancement for CFG that filters the singular vectors of both conditional and unconditional scores using singular value decomposition. This filtering process better aligns the unconditional score with the conditional score, refining the sampling trajectory to stay closer to the data manifold and improving overall CFG performance. <Paper corpusId=\"277271753\" paperTitle=\"(Kwon et al., 2025)\" isShortName></Paper>\n\n- **SNOOPI Framework**: For one-step diffusion models, Nguyen et al. created SNOOPI, which enhances guidance during both training and inference. It implements Proper Guidance-SwiftBrush (PG-SB), using random-scale classifier-free guidance to improve training stability, and Negative-Away Steer Attention (NASA), which integrates negative prompts via cross-attention to suppress unwanted elements in generated images. <Paper corpusId=\"274446026\" paperTitle=\"(Nguyen et al., 2024)\" isShortName></Paper>\n\n- **Guidance-Based Diffusion**: Viola et al. highlighted how guidance-based diffusion incorporates external supervision alongside original conditioning, using a guidance function that measures whether certain criteria are met. This approach enables fine-grained control over outputs in various applications. <Paper corpusId=\"274823034\" paperTitle=\"(Viola et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>\n\n- **Extension to Inverse Problems**: The CFG approach has been extended to handle general noisy linear and non-linear inverse problems through approximation of posterior sampling. This creates a blended version of diffusion sampling with manifold constrained gradient, yielding more desirable generative paths in noisy settings. <Paper corpusId=\"274823034\" paperTitle=\"(Viola et al., 2024)\" isShortName></Paper> <Paper corpusId=\"252596252\" paperTitle=\"(Chung et al., 2022)\" isShortName></Paper>\n\n- **Unified Network Training**: Advancements in the training methodology now allow conditional diffusion models (\u03b5_\u03b8(x_t|y)) and unconditional models (\u03b5_\u03b8(x_t|y=0)) to be trained as a single neural network. This streamlined approach offers advantages over earlier methods by training a single model to guide the diffusion process and accommodating different types of conditional data such as text embeddings. <Paper corpusId=\"276725462\" paperTitle=\"(Sordo et al., 2025)\" isShortName></Paper> <Paper corpusId=\"234357997\" paperTitle=\"(Dhariwal et al., 2021)\" isShortName></Paper>\n\n- **Image-Conditional Diffusion Models**: Building on Ho et al.'s classifier-free diffusion guidance, Saharia et al. developed image-conditional diffusion models for super-resolution and image-to-image translation tasks. This work demonstrated how the guidance scale hyperparameter could be interpreted within the classifier-free diffusion model framework. <Paper corpusId=\"258714952\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"243938678\" paperTitle=\"(Saharia et al., 2021)\" isShortName></Paper>\n\n- **Multi-Modal Adaptation**: Recent innovations have enabled CFG to better handle multi-modal data, making it particularly effective when training on diverse data types. This has expanded the applicability of CFG beyond its original context to more complex generative tasks. <Paper corpusId=\"276725462\" paperTitle=\"(Sordo et al., 2025)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Ohayon et al., 2025)",
                        "snippets": [
                            "Here, we introduce a new CFG method that allows generating compressed conditional samples using any pair of conditional and unconditional diffusion models, while controlling the tradeoff between generation quality and the fidelity to the inputs",
                            "We coin our method Compressed CFG (CCFG)."
                        ],
                        "paper": {
                            "corpus_id": 276094842,
                            "title": "Compressed Image Generation with Denoising Diffusion Codebook Models",
                            "authors": [
                                {
                                    "authorId": "51228065",
                                    "name": "Guy Ohayon"
                                },
                                {
                                    "authorId": "2245392181",
                                    "name": "Hila Manor"
                                },
                                {
                                    "authorId": "1880407",
                                    "name": "T. Michaeli"
                                },
                                {
                                    "authorId": "2266840963",
                                    "name": "Michael Elad"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.94775390625
                    },
                    {
                        "id": "(Kwon et al., 2025)",
                        "snippets": [
                            "In this work, we introduce a novel approach that leverages a geometric perspective on the unconditional score to enhance CFG performance when conditional scores are available. Specifically, we propose a method that filters the singular vectors of both conditional and unconditional scores using singular value decomposition. This filtering process aligns the unconditional score with the conditional score, thereby refining the sampling trajectory to stay closer to the manifold."
                        ],
                        "paper": {
                            "corpus_id": 277271753,
                            "title": "TCFG: Tangential Damping Classifier-free Guidance",
                            "authors": [
                                {
                                    "authorId": "2182293854",
                                    "name": "Mingi Kwon"
                                },
                                {
                                    "authorId": "2352073121",
                                    "name": "Shin seong Kim"
                                },
                                {
                                    "authorId": "2351806103",
                                    "name": "Jaeseok Jeong. Yi Ting Hsiao"
                                },
                                {
                                    "authorId": "2253393666",
                                    "name": "Youngjung Uh"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9677734375
                    },
                    {
                        "id": "(Nguyen et al., 2024)",
                        "snippets": [
                            "Classifier-free Guidance (CFG) [15] is an inference technique designed to enhance the quality of generated images by blending the predictions from both a conditional and an unconditional model. At each sampling step, CFG adjusts the denoiser's output using a control parameter \u03ba > 1, allowing for controlled guidance that aligns more closely with the desired conditions",
                            "Negative Prompts provide enhanced control by suppressing unwanted features in the generated content. Instead of producing an unconditional output, the model generates an output conditioned on the negative prompt y \u2212 , as follows:",
                            "This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images."
                        ],
                        "paper": {
                            "corpus_id": 274446026,
                            "title": "SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance",
                            "authors": [
                                {
                                    "authorId": "2275329323",
                                    "name": "Viet Nguyen"
                                },
                                {
                                    "authorId": "2333424276",
                                    "name": "Anh Aengus Nguyen"
                                },
                                {
                                    "authorId": "2276606039",
                                    "name": "T. Dao"
                                },
                                {
                                    "authorId": "2261741144",
                                    "name": "Khoi Nguyen"
                                },
                                {
                                    "authorId": "2269462407",
                                    "name": "Cuong Pham"
                                },
                                {
                                    "authorId": "2275127531",
                                    "name": "Toan Tran"
                                },
                                {
                                    "authorId": "2327046351",
                                    "name": "Anh Tran"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.94482421875
                    },
                    {
                        "id": "(Viola et al., 2024)",
                        "snippets": [
                            "To allow fine-grained control over the output, guidance-based diffusion [13] incorporates external supervision alongside the original conditioning, using a guidance function that measures whether certain criteria are met. In guided image generation, classifier guidance [14] enables class-conditional outputs from a pretrained, unconditional diffusion model, via gradients from a classifier trained on ImageNet [58] im-ages at different noise scales. Similarly, gradients from a CLIP model [51] trained on noisy images can guide generation toward a user-defined text caption [47]. An alternative, classifier-free guidance [26,47], achieves similar control without training a separate classifier, by parameterizing both conditional and unconditional diffusion models within the same network. The approach is further extended to handle general nonlinear inverse problems [9], using gradients calculated on the expected denoised images."
                        ],
                        "paper": {
                            "corpus_id": 274823034,
                            "title": "Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion",
                            "authors": [
                                {
                                    "authorId": "2335870109",
                                    "name": "Massimiliano Viola"
                                },
                                {
                                    "authorId": "2335870504",
                                    "name": "Kevin Qu"
                                },
                                {
                                    "authorId": "2031912818",
                                    "name": "Nando Metzger"
                                },
                                {
                                    "authorId": "34926212",
                                    "name": "Bingxin Ke"
                                },
                                {
                                    "authorId": "2078701909",
                                    "name": "Alexander Becker"
                                },
                                {
                                    "authorId": "2243003715",
                                    "name": "Konrad Schindler"
                                },
                                {
                                    "authorId": "4366091",
                                    "name": "Anton Obukhov"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.93115234375
                    },
                    {
                        "id": "(Ho, 2022)",
                        "snippets": [
                            "We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."
                        ],
                        "paper": {
                            "corpus_id": 249145348,
                            "title": "Classifier-Free Diffusion Guidance",
                            "authors": [
                                {
                                    "authorId": "2126278",
                                    "name": "Jonathan Ho"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 3970
                        },
                        "score": 0.9599609375
                    },
                    {
                        "id": "(Chung et al., 2022)",
                        "snippets": [
                            "Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring. Code available at https://github.com/DPS2022/diffusion-posterior-sampling"
                        ],
                        "paper": {
                            "corpus_id": 252596252,
                            "title": "Diffusion Posterior Sampling for General Noisy Inverse Problems",
                            "authors": [
                                {
                                    "authorId": "2110872233",
                                    "name": "Hyungjin Chung"
                                },
                                {
                                    "authorId": "2109216792",
                                    "name": "Jeongsol Kim"
                                },
                                {
                                    "authorId": "2179304",
                                    "name": "Michael T. McCann"
                                },
                                {
                                    "authorId": "83695651",
                                    "name": "M. Klasky"
                                },
                                {
                                    "authorId": "2998762",
                                    "name": "J. C. Ye"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 861
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sordo et al., 2025)",
                        "snippets": [
                            "Classifier-free guidance, proposed by Ho et al. [12], allows for enhanced control in diffusion models by eliminating the need for separate classifiers. Instead of relying on a separate classifier, which increases training complexity and introduces bias potential, classifier-free guidance trains the diffusion model to directly learn and combine conditional and unconditional distributions during inference, streamlining the process. In other words, the authors train a conditional diffusion model \u03f5 \u03b8 (x t |y) and an unconditional model \u03f5 \u03b8 (x t |y = 0) as a single neural network as follows:\n\nThis approach is advantageous compared to the previous one as it trains a single model to guide the diffusion process and can take different types of conditional data such as text embeddings. We will see that many models rely on classifier free-guidance especially when training on multi-modal data."
                        ],
                        "paper": {
                            "corpus_id": 276725462,
                            "title": "A Review on Generative AI For Text-To-Image and Image-To-Image Generation and Implications To Scientific Images",
                            "authors": [
                                {
                                    "authorId": "2267907323",
                                    "name": "Zineb Sordo"
                                },
                                {
                                    "authorId": "2271183137",
                                    "name": "Eric Chagnon"
                                },
                                {
                                    "authorId": "2276224795",
                                    "name": "Daniela Ushizima"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9560546875
                    },
                    {
                        "id": "(Dhariwal et al., 2021)",
                        "snippets": [
                            "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
                        ],
                        "paper": {
                            "corpus_id": 234357997,
                            "title": "Diffusion Models Beat GANs on Image Synthesis",
                            "authors": [
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "38967461",
                                    "name": "Alex Nichol"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 7951
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhu et al., 2023)",
                        "snippets": [
                            "Ho et al. [25] introduced the classifier-free diffusion guidance with s \u03b8 (x, t, y) = \u2207 x log p t (x|y) the imageconditional diffusion models. With the same idea, Saharia et al. (Saharia et al., 2021)[49] trained image-conditional diffusion models for SR and image-to-image translation in concurrent work. Nichol et al. [40] proposed to use text-guided diffusion models to generate photo-realistic images with classifierfree guidance. The hyperparameter \u03bb in (1) can be interpreted as the guidance scale in classifier-free diffusion models."
                        ],
                        "paper": {
                            "corpus_id": 258714952,
                            "title": "Denoising Diffusion Models for Plug-and-Play Image Restoration",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "Yuanzhi Zhu"
                                },
                                {
                                    "authorId": "144110274",
                                    "name": "K. Zhang"
                                },
                                {
                                    "authorId": "145270228",
                                    "name": "Jingyun Liang"
                                },
                                {
                                    "authorId": "32879676",
                                    "name": "Jiezhang Cao"
                                },
                                {
                                    "authorId": "1766554",
                                    "name": "B. Wen"
                                },
                                {
                                    "authorId": "1732855",
                                    "name": "R. Timofte"
                                },
                                {
                                    "authorId": "1681236",
                                    "name": "L. Gool"
                                }
                            ],
                            "year": 2023,
                            "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
                            "n_citations": 219
                        },
                        "score": 0.9228515625
                    },
                    {
                        "id": "(Saharia et al., 2021)",
                        "snippets": [
                            "This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io/ for an overview of the results and code."
                        ],
                        "paper": {
                            "corpus_id": 243938678,
                            "title": "Palette: Image-to-Image Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2139922989",
                                    "name": "Chitwan Saharia"
                                },
                                {
                                    "authorId": "2150198218",
                                    "name": "William Chan"
                                },
                                {
                                    "authorId": "2140690517",
                                    "name": "Huiwen Chang"
                                },
                                {
                                    "authorId": "2143770178",
                                    "name": "Chris A. Lee"
                                },
                                {
                                    "authorId": "2112615253",
                                    "name": "Jonathan Ho"
                                },
                                {
                                    "authorId": "2117261512",
                                    "name": "Tim Salimans"
                                },
                                {
                                    "authorId": "1793739",
                                    "name": "David J. Fleet"
                                },
                                {
                                    "authorId": "2138892698",
                                    "name": "Mohammad Norouzi"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Computer Graphics and Interactive Techniques",
                            "n_citations": 1647
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Limitations and Future Directions",
                "tldr": "Despite its widespread adoption, Classifier-Free Guidance faces several limitations including quality-diversity trade-offs, computational inefficiencies, and challenges with complex multi-modal conditioning. Future research directions focus on dynamic guidance mechanisms, more efficient implementations, and better theoretical frameworks to overcome these limitations. (5 sources)",
                "text": "\nWhile Classifier-Free Guidance (CFG) has become a fundamental technique in conditional generative models, it presents several important limitations that current and future research aims to address. One significant limitation is the inherent trade-off between sample quality, diversity, and consistency with conditioning. As noted by Ifriqi et al., standard CFG often improves some of these aspects at the expense of others <Paper corpusId=\"277955619\" paperTitle=\"(Ifriqi et al., 2025)\" isShortName></Paper>. This trade-off remains a fundamental challenge that requires more sophisticated solutions beyond simple guidance scaling.\n\nThe static nature of conventional CFG implementation poses another limitation. Li et al. highlight that standard CFG employs a fixed unconditional input throughout the generation process, which can be suboptimal for iterative generation where model uncertainty varies dynamically <Paper corpusId=\"278910862\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. This static approach fails to adapt to the changing requirements at different stages of the generation process, potentially limiting the quality of the final output.\n\nWhen extending CFG to complex multi-modal conditioning scenarios, additional challenges emerge. Kant et al. observed that applying classifier-free guidance beyond text conditioning can lead to undesirable artifacts such as over-saturated generations <Paper corpusId=\"267547881\" paperTitle=\"(Kant et al., 2024)\" isShortName></Paper>. This suggests that while CFG works well with single modality conditioning, its effectiveness diminishes with multiple conditioning types, indicating a need for more sophisticated multi-modal guidance techniques.\n\nThe computational overhead of CFG also presents a practical limitation. The need to compute both conditional and unconditional predictions at each step increases inference time and resource requirements. This becomes particularly problematic for real-time applications or deployment on resource-constrained devices <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nCurrent research is exploring several promising directions to address these limitations. Sadat et al. demonstrated theoretically that CFG-like behavior can be achieved without additional training of an unconditional model by using a conditioning vector independent of the input data <Paper corpusId=\"270923987\" paperTitle=\"(Sadat et al., 2024)\" isShortName></Paper>. This approach could potentially reduce the computational overhead of CFG while maintaining its benefits.\n\nAnother promising direction involves dynamic guidance mechanisms that adapt to the model's uncertainty throughout the generation process. Li et al. proposed approaches that modify the guidance strength based on the model's confidence at different stages <Paper corpusId=\"278910862\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. Such adaptive techniques could potentially optimize the quality-diversity trade-off more effectively than static approaches.\n\nThe integration of negative guidance with positive guidance represents another avenue for enhancing CFG's capabilities. Li et al. demonstrated that combining positive guidance for learning text prompts with negative guidance allows diffusion models to selectively \"forget\" specified concepts while preserving desired attributes <Paper corpusId=\"270123253\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This balanced approach, controlled through weighting parameters, enables more nuanced control over the generation process.\n\nFuture research will likely focus on developing unified theoretical frameworks that better explain and predict the effects of different guidance techniques. Additionally, more efficient implementations of CFG that reduce computational requirements without sacrificing effectiveness will be crucial for broader adoption. As generative models continue to evolve, we can expect more sophisticated guidance mechanisms that dynamically balance multiple objectives and better handle complex multi-modal conditioning scenarios <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Ifriqi et al., 2025)",
                        "snippets": [
                            "Guidance techniques are commonly used in diffusion and flow models to improve image quality and consistency for conditional generative tasks such as class-conditional and text-to-image generation. In particular, classifier-free guidance (CFG) -- the most widely adopted guidance technique -- contrasts conditional and unconditional predictions to improve the generated images. This results, however, in trade-offs across quality, diversity and consistency, improving some at the expense of others."
                        ],
                        "paper": {
                            "corpus_id": 277955619,
                            "title": "Entropy Rectifying Guidance for Diffusion and Flow Models",
                            "authors": [
                                {
                                    "authorId": "2329186126",
                                    "name": "Tariq Berrada Ifriqi"
                                },
                                {
                                    "authorId": "1456285042",
                                    "name": "Adriana Romero-Soriano"
                                },
                                {
                                    "authorId": "3325894",
                                    "name": "M. Drozdzal"
                                },
                                {
                                    "authorId": "2281637540",
                                    "name": "Jakob Verbeek"
                                },
                                {
                                    "authorId": "72492981",
                                    "name": "Alahari Karteek"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.93359375
                    },
                    {
                        "id": "(Li et al., 2025)",
                        "snippets": [
                            "Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically."
                        ],
                        "paper": {
                            "corpus_id": 278910862,
                            "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking",
                            "authors": [
                                {
                                    "authorId": "2363590342",
                                    "name": "Pengxiang Li"
                                },
                                {
                                    "authorId": "2362879323",
                                    "name": "Shilin Yan"
                                },
                                {
                                    "authorId": "2362728158",
                                    "name": "Joey Tsai"
                                },
                                {
                                    "authorId": "2291314199",
                                    "name": "Renrui Zhang"
                                },
                                {
                                    "authorId": "2363570661",
                                    "name": "Ruichuan An"
                                },
                                {
                                    "authorId": "145490494",
                                    "name": "Ziyu Guo"
                                },
                                {
                                    "authorId": "2303650253",
                                    "name": "Xiaowei Gao"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.9267578125
                    },
                    {
                        "id": "(Kant et al., 2024)",
                        "snippets": [
                            "Classifier-free diffusion guidance (Ho, 2022) is a technique used to balance the quality and diversity of images produced by diffusion models. This method is particularly effective in class-conditional and text-conditional image generation, enhancing both the visual quality of images and their alignment with given conditions. Inspired by (Brooks et al., 2022) we explore the integration of classifier-free guidance with Epipolar Attention and Pl\u00fccker Embedding. Implementing classifierfree guidance involves simultaneous training of the diffusion model for both conditional and unconditional denoising tasks. During inference, these models' score estimates are merged. We have four different types of conditioning injected into our system: Outcome: As shown in Fig. 13, we find that classifier-free guidance beyond text conditioning does not provide additional benefits, and rather leads to over-saturated generations. This also aligns with our observations on MVDream."
                        ],
                        "paper": {
                            "corpus_id": 267547881,
                            "title": "SPAD: Spatially Aware Multi-View Diffusers",
                            "authors": [
                                {
                                    "authorId": "66536530",
                                    "name": "Yash Kant"
                                },
                                {
                                    "authorId": "2253894469",
                                    "name": "Ziyi Wu"
                                },
                                {
                                    "authorId": "2261673978",
                                    "name": "Michael Vasilkovsky"
                                },
                                {
                                    "authorId": "2279023722",
                                    "name": "Guocheng Qian"
                                },
                                {
                                    "authorId": "2258296012",
                                    "name": "Jian Ren"
                                },
                                {
                                    "authorId": "134642679",
                                    "name": "R. A. Guler"
                                },
                                {
                                    "authorId": "2279742224",
                                    "name": "Bernard Ghanem"
                                },
                                {
                                    "authorId": "145582202",
                                    "name": "S. Tulyakov"
                                },
                                {
                                    "authorId": "2262216913",
                                    "name": "Igor Gilitschenski"
                                },
                                {
                                    "authorId": "10753214",
                                    "name": "Aliaksandr Siarohin"
                                }
                            ],
                            "year": 2024,
                            "venue": "Computer Vision and Pattern Recognition",
                            "n_citations": 38
                        },
                        "score": 0.9384765625
                    },
                    {
                        "id": "(Sadat et al., 2024)",
                        "snippets": [
                            "In this paper, we analyze the methodology behind classifier-free guidance and show theoretically that similar behavior can be achieved without additional training of an unconditional model. The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score."
                        ],
                        "paper": {
                            "corpus_id": 270923987,
                            "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models",
                            "authors": [
                                {
                                    "authorId": "2261742393",
                                    "name": "Seyedmorteza Sadat"
                                },
                                {
                                    "authorId": "2204861903",
                                    "name": "Manuel Kansy"
                                },
                                {
                                    "authorId": "1466533438",
                                    "name": "Otmar Hilliges"
                                },
                                {
                                    "authorId": "145848224",
                                    "name": "Romann M. Weber"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 14
                        },
                        "score": 0.93798828125
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Classifier-free diffusion guidance is commonly employed in class-conditional and text-conditional image generation tasks to enhance the visual quality of the generated images and to ensure that the sampled outputs better correspond to their respective conditioning factors.Typically, diffusion models use a guidance signal, where the model is conditioned on the desired concept or attribute to generate samples aligned with that concept (Gandikota et al., 2023).\n\nOur approach employs negative guidance to allow the diffusion model to gradually forget a specified concept in the original image while applying positive guidance for learning text prompts.Combining with Eq 1 and Eq 2, we compute the following score estimate using Classifier-Free Guidance during inference:\n\nwhere c p and c n are input prompt and forgetting concepts, w and \u03b7 are weights for controlling the balance between the positive and negative guidance signals in the final CFG score estimate.A higher value of eta will give more weight to the negative guidance, allowing tuning the level of concept forgetting during the diffusion process."
                        ],
                        "paper": {
                            "corpus_id": 270123253,
                            "title": "Text Guided Image Editing with Automatic Concept Locating and Forgetting",
                            "authors": [
                                {
                                    "authorId": "2268721096",
                                    "name": "Jia Li"
                                },
                                {
                                    "authorId": "2153121378",
                                    "name": "Lijie Hu"
                                },
                                {
                                    "authorId": "2304013931",
                                    "name": "Zhixian He"
                                },
                                {
                                    "authorId": "2253808698",
                                    "name": "Jingfeng Zhang"
                                },
                                {
                                    "authorId": "2268675026",
                                    "name": "Tianhang Zheng"
                                },
                                {
                                    "authorId": "2268815109",
                                    "name": "Di Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.93017578125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.249567
    }
}