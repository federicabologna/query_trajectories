{
    "query": "What are the risks and mitigation strategies for egocentric bias when using GPT-4 to evaluate its own generated data?",
    "user_id": "lib_user",
    "task_id": "b3db50e7-05f1-488f-8451-fa89119802ef",
    "timestamp": "2025-06-23T21:12:18.674252",
    "n_retrieval": 256,
    "n_retrieved": 272,
    "n_candidates": 20,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.31400400000000006,
    "decomposed_query": {
        "rewritten_query": "Risks and mitigation strategies for egocentric bias when using GPT-4 to evaluate its own generated data.",
        "keyword_query": "egocentric bias GPT-4 evaluate generated data",
        "search_filters": {
            "fieldsOfStudy": "Computer Science,Psychology"
        },
        "cost": 0.009807,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 24,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.05229, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2126996290",
                    "name": "Islam Eldifrawi"
                },
                {
                    "authorId": "2311878157",
                    "name": "Shengrui Wang"
                },
                {
                    "authorId": "2311887008",
                    "name": "Amine Trabelsi"
                }
            ],
            "abstract": "The field of explainable Automatic Fact-Checking (AFC) aims to enhance the transparency and trustworthiness of automated fact-verification systems by providing clear and comprehensible explanations. However, the effectiveness of these explanations depends on their actionability --their ability to empower users to make informed decisions and mitigate misinformation. Despite actionability being a critical property of high-quality explanations, no prior research has proposed a dedicated method to evaluate it. This paper introduces FinGrAct, a fine-grained evaluation framework that can access the web, and it is designed to assess actionability in AFC explanations through well-defined criteria and an evaluation dataset. FinGrAct surpasses state-of-the-art (SOTA) evaluators, achieving the highest Pearson and Kendall correlation with human judgments while demonstrating the lowest ego-centric bias, making it a more robust evaluation approach for actionability evaluation in AFC.",
            "corpus_id": 277621852,
            "sentences": [
                {
                    "corpus_id": "277621852",
                    "title": "FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking",
                    "text": "In their study, Liu et al. (2023) identified a bias in evaluators, where they tend to favor their own model's generations over those from other models, even when the latter are objectively better. Ohi et al. (2024) introduced a method for detecting this bias, which they termed 'Likelihood-based Evaluation Bias.' However, this approach requires access to the probability distribution of the LLM's generations, which is often unavailable, especially when working with commercial LLMs. Ye et al. (2024) also addressed this issue, referring to it as 'egocentric Bias,' and we adopt this terminology in our work. Their research primarily focuses on understanding the effects of this bias on performance and strategies for mitigating it. The purpose of this study is to compare the effect of ego-centric bias on our fine-grained evaluation \"FinGrAct\" and on other SOTA evaluators. In this paper, we propose a simple yet effective method for identifying this bias within the context of actionability evaluation in explainable AFC when the probability distribution of LLM generations is not available. \n\nWe identify biased samples by observing that evaluators tend to assign significantly higher scores to explanations generated by their own underlying LLMs compared to human ratings. For instance, Geval exhibits a preference for GPT-4-generated explanations, even when alternative explanations may be superior. To quantify this bias, we implement a Likert-scale scoring system ranging from 0 to 5, allowing for a tolerance of a 1-point difference between human and LLM scores. If multiple human annotators rate an explanation as 2 and the LLM assigns a 3, the sample is excluded from bias analysis. However, if the LLM scores the same explanation as 4 or 5, it is classified as ego-centric bias. Thus, a discrepancy of 2 or more points higher than the human rating serves as a clear indicator of bias. \n\nSetup: To measure ego-centric bias, each evaluator is tasked with assessing explanations generated by its underlying LLM. For instance, Prometheus evaluates Mistral-generated explanations, while G-Eval and FinGrAct evaluate GPT-4-generated explanations.",
                    "score": 0.39425639707834165,
                    "section_title": "Experiment 3: Ego-Centric Bias Analysis",
                    "char_start_offset": 24773,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 196
                        },
                        {
                            "start": 197,
                            "end": 313
                        },
                        {
                            "start": 314,
                            "end": 484
                        },
                        {
                            "start": 485,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 733
                        },
                        {
                            "start": 734,
                            "end": 876
                        },
                        {
                            "start": 877,
                            "end": 1095
                        },
                        {
                            "start": 1098,
                            "end": 1278
                        },
                        {
                            "start": 1279,
                            "end": 1406
                        },
                        {
                            "start": 1407,
                            "end": 1572
                        },
                        {
                            "start": 1573,
                            "end": 1694
                        },
                        {
                            "start": 1695,
                            "end": 1791
                        },
                        {
                            "start": 1792,
                            "end": 1897
                        },
                        {
                            "start": 1900,
                            "end": 2021
                        },
                        {
                            "start": 2022,
                            "end": 2153
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 16,
                            "end": 33,
                            "matchedPaperCorpusId": "257804696"
                        },
                        {
                            "start": 197,
                            "end": 214,
                            "matchedPaperCorpusId": "267938572"
                        },
                        {
                            "start": 485,
                            "end": 501,
                            "matchedPaperCorpusId": "273098639"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85986328125
                },
                {
                    "corpus_id": "277621852",
                    "title": "FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking",
                    "text": "For instance, Prometheus evaluates Mistral-generated explanations, while G-Eval and FinGrAct evaluate GPT-4-generated explanations. Their evaluations are then compared against human annotations, and instances of bias are identified and counted. Specifically, cases where an evaluator overestimates actionability-assigning a score at least 2 units higher than human ratings-are classified as ego-centric bias. \n\nThe scores from the three human annotators were averaged and compared against the mean scores from three evaluation runs for each automatic evaluator. Geval exhibited the highest variance, with 113 biased samples in the first run, 101 in the second, and 84 in the third, averaging 99. Prometheus demonstrated more stability, with 55 biased samples in the first run, 50 in the second, and 53 in the third. FinGrAct showed the least variance, with 17 biased samples in the first run, 19 in the second, and 17 in the third. \n\nAdditionally, instances where the evaluators underestimate actionability relative to human judgments are also recorded. This analysis allows us to determine whether ego-centric bias or underestimation contributes more to the misalignment between automated evaluators and human assessments Results: Out of 203 samples, the results clearly indicate that ego-centric bias contributes signifi-cantly more to the misalignment between human annotations and LLM-based evaluations than underestimation does. G-Eval exhibits ego-centric bias in 99 out of 203 samples (48.7%), whereas underestimation occurs in only 12 samples (5.9%). Similarly, Prometheus demonstrates bias in 26% of cases, while underestimation accounts for just 5%. FinGrAct, which shows the least bias, has 8.4% biased samples and 2% underestimation. \n\nAnalysis: These findings suggest that LLMbased evaluators tend to overestimate actionability far more frequently than they underestimate it, highlighting a key limitation in their judgment alignment with human evaluations. It is worth noting that LLMs as evaluators tend to exhibit higher correlation with human annotations and lower egocentric bias when the evaluation criteria are more detailed.",
                    "score": 0.3237014097802757,
                    "section_title": "Experiment 3: Ego-Centric Bias Analysis",
                    "char_start_offset": 26795,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 244
                        },
                        {
                            "start": 245,
                            "end": 408
                        },
                        {
                            "start": 411,
                            "end": 561
                        },
                        {
                            "start": 562,
                            "end": 695
                        },
                        {
                            "start": 696,
                            "end": 815
                        },
                        {
                            "start": 816,
                            "end": 931
                        },
                        {
                            "start": 934,
                            "end": 1053
                        },
                        {
                            "start": 1054,
                            "end": 1433
                        },
                        {
                            "start": 1434,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1659
                        },
                        {
                            "start": 1660,
                            "end": 1745
                        },
                        {
                            "start": 1748,
                            "end": 1970
                        },
                        {
                            "start": 1971,
                            "end": 2145
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.791015625
                }
            ],
            "relevance_judgement": 0.85986328125,
            "relevance_judgment_input_expanded": "# Title: FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking\n# Venue: arXiv.org\n# Authors: Islam Eldifrawi, Shengrui Wang, Amine Trabelsi\n## Abstract\nThe field of explainable Automatic Fact-Checking (AFC) aims to enhance the transparency and trustworthiness of automated fact-verification systems by providing clear and comprehensible explanations. However, the effectiveness of these explanations depends on their actionability --their ability to empower users to make informed decisions and mitigate misinformation. Despite actionability being a critical property of high-quality explanations, no prior research has proposed a dedicated method to evaluate it. This paper introduces FinGrAct, a fine-grained evaluation framework that can access the web, and it is designed to assess actionability in AFC explanations through well-defined criteria and an evaluation dataset. FinGrAct surpasses state-of-the-art (SOTA) evaluators, achieving the highest Pearson and Kendall correlation with human judgments while demonstrating the lowest ego-centric bias, making it a more robust evaluation approach for actionability evaluation in AFC.\n## Experiment 3: Ego-Centric Bias Analysis\nIn their study, Liu et al. (2023) identified a bias in evaluators, where they tend to favor their own model's generations over those from other models, even when the latter are objectively better. Ohi et al. (2024) introduced a method for detecting this bias, which they termed 'Likelihood-based Evaluation Bias.' However, this approach requires access to the probability distribution of the LLM's generations, which is often unavailable, especially when working with commercial LLMs. Ye et al. (2024) also addressed this issue, referring to it as 'egocentric Bias,' and we adopt this terminology in our work. Their research primarily focuses on understanding the effects of this bias on performance and strategies for mitigating it. The purpose of this study is to compare the effect of ego-centric bias on our fine-grained evaluation \"FinGrAct\" and on other SOTA evaluators. In this paper, we propose a simple yet effective method for identifying this bias within the context of actionability evaluation in explainable AFC when the probability distribution of LLM generations is not available. \n\nWe identify biased samples by observing that evaluators tend to assign significantly higher scores to explanations generated by their own underlying LLMs compared to human ratings. For instance, Geval exhibits a preference for GPT-4-generated explanations, even when alternative explanations may be superior. To quantify this bias, we implement a Likert-scale scoring system ranging from 0 to 5, allowing for a tolerance of a 1-point difference between human and LLM scores. If multiple human annotators rate an explanation as 2 and the LLM assigns a 3, the sample is excluded from bias analysis. However, if the LLM scores the same explanation as 4 or 5, it is classified as ego-centric bias. Thus, a discrepancy of 2 or more points higher than the human rating serves as a clear indicator of bias. \n\nSetup: To measure ego-centric bias, each evaluator is tasked with assessing explanations generated by its underlying LLM. For instance, Prometheus evaluates Mistral-generated explanations, while G-Eval and FinGrAct evaluate GPT-4-generated explanations.\n...\nFor instance, Prometheus evaluates Mistral-generated explanations, while G-Eval and FinGrAct evaluate GPT-4-generated explanations. Their evaluations are then compared against human annotations, and instances of bias are identified and counted. Specifically, cases where an evaluator overestimates actionability-assigning a score at least 2 units higher than human ratings-are classified as ego-centric bias. \n\nThe scores from the three human annotators were averaged and compared against the mean scores from three evaluation runs for each automatic evaluator. Geval exhibited the highest variance, with 113 biased samples in the first run, 101 in the second, and 84 in the third, averaging 99. Prometheus demonstrated more stability, with 55 biased samples in the first run, 50 in the second, and 53 in the third. FinGrAct showed the least variance, with 17 biased samples in the first run, 19 in the second, and 17 in the third. \n\nAdditionally, instances where the evaluators underestimate actionability relative to human judgments are also recorded. This analysis allows us to determine whether ego-centric bias or underestimation contributes more to the misalignment between automated evaluators and human assessments Results: Out of 203 samples, the results clearly indicate that ego-centric bias contributes signifi-cantly more to the misalignment between human annotations and LLM-based evaluations than underestimation does. G-Eval exhibits ego-centric bias in 99 out of 203 samples (48.7%), whereas underestimation occurs in only 12 samples (5.9%). Similarly, Prometheus demonstrates bias in 26% of cases, while underestimation accounts for just 5%. FinGrAct, which shows the least bias, has 8.4% biased samples and 2% underestimation. \n\nAnalysis: These findings suggest that LLMbased evaluators tend to overestimate actionability far more frequently than they underestimate it, highlighting a key limitation in their judgment alignment with human evaluations. It is worth noting that LLMs as evaluators tend to exhibit higher correlation with human annotations and lower egocentric bias when the evaluation criteria are more detailed.",
            "reference_string": "[277621852 | Eldifrawi et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era",
            "venue": "Knowledge Discovery and Data Mining",
            "year": 2024,
            "reference_count": 181,
            "citation_count": 82,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3637528.3671458",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.11457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2155892801",
                    "name": "Sunhao Dai"
                },
                {
                    "authorId": "2153078929",
                    "name": "Chen Xu"
                },
                {
                    "authorId": "2202745",
                    "name": "Shicheng Xu"
                },
                {
                    "authorId": "2263589454",
                    "name": "Liang Pang"
                },
                {
                    "authorId": "2297820120",
                    "name": "Zhenhua Dong"
                },
                {
                    "authorId": "2266437969",
                    "name": "Jun Xu"
                }
            ],
            "abstract": "With the rapid advancements of large language models (LLMs), information retrieval (IR) systems, such as search engines and recommender systems, have undergone a significant paradigm shift. This evolution, while heralding new opportunities, introduces emerging challenges, particularly in terms of biases and unfairness, which may threaten the information ecosystem. In this paper, we present a comprehensive survey of existing works on emerging and pressing bias and unfairness issues in IR systems when the integration of LLMs. We first unify bias and unfairness issues as distribution mismatch problems, providing a groundwork for categorizing various mitigation strategies through distribution alignment. Subsequently, we systematically delve into the specific bias and unfairness issues arising from three critical stages of LLMs integration into IR systems: data collection, model development, and result evaluation. In doing so, we meticulously review and analyze recent literature, focusing on the definitions, characteristics, and corresponding mitigation strategies associated with these issues. Finally, we identify and highlight some open problems and challenges for future work, aiming to inspire researchers and stakeholders in the IR field and beyond to better understand and mitigate bias and unfairness issues of IR in this LLM era. We also consistently maintain a GitHub repository for the relevant papers and resources in this rising direction at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.",
            "corpus_id": 269188154,
            "sentences": [
                {
                    "corpus_id": "269188154",
                    "title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era",
                    "text": "With LLMs being extensively utilized in the development of IR models, egocentric bias has emerged as a new bias during the automated evaluation conducted by LLMs [71,91,92,178], which can be defined as follows: \n\n\u2022 Definition. LLM-based evaluators prefer the responses generated by themselves or LLMs from the same family. \n\nA recent work [92] has identified that language model-driven evaluation metrics, such as BARTScore [185], T5Score [122], and GPTScore [41], inherently favor texts produced by their underlying LMs, especially in summarization tasks. Liu et al. [91] and Zheng et al. [196] further highlighted that when acting as evaluators, LLMs demonstrate a clear bias towards outputs generated by themselves over those from other models or human contributors. This bias could stem from that the LLM may share the same for both the model development phase and result evaluation phase [91]. \n\nThe emergence of egocentric bias introduces the risk of selfreinforcement for LLMs, particularly when they are further trained using rewards from LLM-based evaluations. This scenario can lead to LLMs overfitting to their own evaluation criteria, intensifying self-preference in next-generation model [91]. Current strategies for mitigating egocentric bias primarily involve employing diverse LLMs as evaluators to foster peer discussions [79], thereby reducing the preference for any specific LLM and enhancing the robustness of evaluation outcomes. However, this strategy inevitably increases the evaluation costs. Future research must explore more efficient solutions to ensure fair and unbiased evaluation.",
                    "score": 0.4390897310546684,
                    "section_title": "Egocentric Bias.",
                    "char_start_offset": 27455,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 210
                        },
                        {
                            "start": 213,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 322
                        },
                        {
                            "start": 325,
                            "end": 556
                        },
                        {
                            "start": 557,
                            "end": 769
                        },
                        {
                            "start": 770,
                            "end": 898
                        },
                        {
                            "start": 901,
                            "end": 1069
                        },
                        {
                            "start": 1070,
                            "end": 1206
                        },
                        {
                            "start": 1207,
                            "end": 1450
                        },
                        {
                            "start": 1451,
                            "end": 1516
                        },
                        {
                            "start": 1517,
                            "end": 1610
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 166,
                            "end": 169,
                            "matchedPaperCorpusId": "257804696"
                        },
                        {
                            "start": 424,
                            "end": 429,
                            "matchedPaperCorpusId": "235593404"
                        },
                        {
                            "start": 568,
                            "end": 572,
                            "matchedPaperCorpusId": "257804696"
                        },
                        {
                            "start": 590,
                            "end": 595,
                            "matchedPaperCorpusId": "259129398"
                        },
                        {
                            "start": 893,
                            "end": 897,
                            "matchedPaperCorpusId": "257804696"
                        },
                        {
                            "start": 1201,
                            "end": 1205,
                            "matchedPaperCorpusId": "257804696"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84765625
                }
            ],
            "relevance_judgement": 0.84765625,
            "relevance_judgment_input_expanded": "# Title: Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era\n# Venue: Knowledge Discovery and Data Mining\n# Authors: Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, Jun Xu\n## Abstract\nWith the rapid advancements of large language models (LLMs), information retrieval (IR) systems, such as search engines and recommender systems, have undergone a significant paradigm shift. This evolution, while heralding new opportunities, introduces emerging challenges, particularly in terms of biases and unfairness, which may threaten the information ecosystem. In this paper, we present a comprehensive survey of existing works on emerging and pressing bias and unfairness issues in IR systems when the integration of LLMs. We first unify bias and unfairness issues as distribution mismatch problems, providing a groundwork for categorizing various mitigation strategies through distribution alignment. Subsequently, we systematically delve into the specific bias and unfairness issues arising from three critical stages of LLMs integration into IR systems: data collection, model development, and result evaluation. In doing so, we meticulously review and analyze recent literature, focusing on the definitions, characteristics, and corresponding mitigation strategies associated with these issues. Finally, we identify and highlight some open problems and challenges for future work, aiming to inspire researchers and stakeholders in the IR field and beyond to better understand and mitigate bias and unfairness issues of IR in this LLM era. We also consistently maintain a GitHub repository for the relevant papers and resources in this rising direction at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.\n## Egocentric Bias.\nWith LLMs being extensively utilized in the development of IR models, egocentric bias has emerged as a new bias during the automated evaluation conducted by LLMs [71,91,92,178], which can be defined as follows: \n\n\u2022 Definition. LLM-based evaluators prefer the responses generated by themselves or LLMs from the same family. \n\nA recent work [92] has identified that language model-driven evaluation metrics, such as BARTScore [185], T5Score [122], and GPTScore [41], inherently favor texts produced by their underlying LMs, especially in summarization tasks. Liu et al. [91] and Zheng et al. [196] further highlighted that when acting as evaluators, LLMs demonstrate a clear bias towards outputs generated by themselves over those from other models or human contributors. This bias could stem from that the LLM may share the same for both the model development phase and result evaluation phase [91]. \n\nThe emergence of egocentric bias introduces the risk of selfreinforcement for LLMs, particularly when they are further trained using rewards from LLM-based evaluations. This scenario can lead to LLMs overfitting to their own evaluation criteria, intensifying self-preference in next-generation model [91]. Current strategies for mitigating egocentric bias primarily involve employing diverse LLMs as evaluators to foster peer discussions [79], thereby reducing the preference for any specific LLM and enhancing the robustness of evaluation outcomes. However, this strategy inevitably increases the evaluation costs. Future research must explore more efficient solutions to ensure fair and unbiased evaluation.",
            "reference_string": "[269188154 | Dai et al. | 2024 | Citations: 82]"
        },
        {
            "title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 122,
            "influential_citation_count": 16,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.04788, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279219833",
                    "name": "Dongping Chen"
                },
                {
                    "authorId": "2283244952",
                    "name": "Ruoxi Chen"
                },
                {
                    "authorId": "2283311181",
                    "name": "Shilin Zhang"
                },
                {
                    "authorId": "2283150665",
                    "name": "Yinuo Liu"
                },
                {
                    "authorId": "2283314623",
                    "name": "Yaochen Wang"
                },
                {
                    "authorId": "2283313383",
                    "name": "Huichi Zhou"
                },
                {
                    "authorId": "46324457",
                    "name": "Qihui Zhang"
                },
                {
                    "authorId": "2221116622",
                    "name": "Pan Zhou"
                },
                {
                    "authorId": "2254266993",
                    "name": "Yao Wan"
                },
                {
                    "authorId": "2267508610",
                    "name": "Lichao Sun"
                }
            ],
            "abstract": "Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence of multimodal benchmarks that align with human preferences. Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparison, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking. Furthermore, a closer examination reveals persistent challenges in the judgment capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. The code and dataset are publicly available at our project homepage: \\url{https://mllm-judge.github.io/}.",
            "corpus_id": 267523079,
            "sentences": [
                {
                    "corpus_id": "267523079",
                    "title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                    "text": "Egocentric Bias. Models tend to assign higher scores to their own responses while scoring others lower (Zheng et al., 2023b;Li et al., 2024). In Figures 19 and 20, GPT-4V exhibits a slight degree of Egocentricity. Conversely, Gemini maintains a uniform scoring distribution across different sources, demonstrating a more equitable approach to judgment. In contrast, GPT-4V shows self-preference, aligning its judgments with its predefined ethical guidelines. For example, GPT-4V consistently emphasizes privacy preservation, leading to higher scores for privacy-related questions based on its own metrics. Despite efforts in prompt engineer-  ing to ensure neutrality, these models still rely on judgment criteria set during post-alignment training (Ouyang et al., 2022). This bias can result in judgments that deviate from human preferences, highlighting the complexity of aligning MLLM judgments with humans'. \n\nPosition Bias. Model consistently favor answers in specific positions, often influenced by training data that typically places correct responses at the beginning or end of prompts (Liu et al., 2023e). Figure 4 illustrates bias in LLaVA and CogVLM during Pair Comparison tasks, where they consistently prefer answers in a specific position. This bias likely arises from their limited ability to follow complex instructions, leading them to be influenced by prompt structure. For example, if a Batch Ranking prompt includes a sequence like 'ABCD', LLaVA replicates this sequence in 88.2% of responses, significantly more than other sequences. However, this bias can be reduced by introducing multiple examples, suggesting that prompts with more examples can better direct these models to follow instructions accurately. \n\nLength Bias. Models tend to prefer longer answers over concise but correct ones (Li et al., 2024), also known as verbosity bias (Zheng et al., 2023b). Figure 6 shows that both GPT-4V and Gemini assign higher scores to longer content. We conducted an expanded scoring experiment using GPT-4 (OpenAI, 2023) without vision, increasing the semantic length of answers without changing their original intent. In Figure 7, we observe noticeable score increases, with GPT-4V and Gemini showing average gains of 0.6 and 0.75 points, respectively.",
                    "score": 0.3701196133142095,
                    "section_title": "Bias and Hallucination",
                    "char_start_offset": 16774,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 16
                        },
                        {
                            "start": 17,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 213
                        },
                        {
                            "start": 214,
                            "end": 352
                        },
                        {
                            "start": 353,
                            "end": 458
                        },
                        {
                            "start": 459,
                            "end": 605
                        },
                        {
                            "start": 606,
                            "end": 771
                        },
                        {
                            "start": 772,
                            "end": 911
                        },
                        {
                            "start": 914,
                            "end": 928
                        },
                        {
                            "start": 929,
                            "end": 1114
                        },
                        {
                            "start": 1115,
                            "end": 1253
                        },
                        {
                            "start": 1254,
                            "end": 1387
                        },
                        {
                            "start": 1388,
                            "end": 1554
                        },
                        {
                            "start": 1555,
                            "end": 1731
                        },
                        {
                            "start": 1734,
                            "end": 1746
                        },
                        {
                            "start": 1747,
                            "end": 1884
                        },
                        {
                            "start": 1885,
                            "end": 1967
                        },
                        {
                            "start": 1968,
                            "end": 2136
                        },
                        {
                            "start": 2137,
                            "end": 2271
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 749,
                            "end": 770,
                            "matchedPaperCorpusId": "246426909"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.84375
                }
            ],
            "relevance_judgement": 0.84375,
            "relevance_judgment_input_expanded": "# Title: MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark\n# Venue: International Conference on Machine Learning\n# Authors: Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, Lichao Sun\n## Abstract\nMultimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence of multimodal benchmarks that align with human preferences. Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparison, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking. Furthermore, a closer examination reveals persistent challenges in the judgment capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. The code and dataset are publicly available at our project homepage: \\url{https://mllm-judge.github.io/}.\n## Bias and Hallucination\nEgocentric Bias. Models tend to assign higher scores to their own responses while scoring others lower (Zheng et al., 2023b;Li et al., 2024). In Figures 19 and 20, GPT-4V exhibits a slight degree of Egocentricity. Conversely, Gemini maintains a uniform scoring distribution across different sources, demonstrating a more equitable approach to judgment. In contrast, GPT-4V shows self-preference, aligning its judgments with its predefined ethical guidelines. For example, GPT-4V consistently emphasizes privacy preservation, leading to higher scores for privacy-related questions based on its own metrics. Despite efforts in prompt engineer-  ing to ensure neutrality, these models still rely on judgment criteria set during post-alignment training (Ouyang et al., 2022). This bias can result in judgments that deviate from human preferences, highlighting the complexity of aligning MLLM judgments with humans'. \n\nPosition Bias. Model consistently favor answers in specific positions, often influenced by training data that typically places correct responses at the beginning or end of prompts (Liu et al., 2023e). Figure 4 illustrates bias in LLaVA and CogVLM during Pair Comparison tasks, where they consistently prefer answers in a specific position. This bias likely arises from their limited ability to follow complex instructions, leading them to be influenced by prompt structure. For example, if a Batch Ranking prompt includes a sequence like 'ABCD', LLaVA replicates this sequence in 88.2% of responses, significantly more than other sequences. However, this bias can be reduced by introducing multiple examples, suggesting that prompts with more examples can better direct these models to follow instructions accurately. \n\nLength Bias. Models tend to prefer longer answers over concise but correct ones (Li et al., 2024), also known as verbosity bias (Zheng et al., 2023b). Figure 6 shows that both GPT-4V and Gemini assign higher scores to longer content. We conducted an expanded scoring experiment using GPT-4 (OpenAI, 2023) without vision, increasing the semantic length of answers without changing their original intent. In Figure 7, we observe noticeable score increases, with GPT-4V and Gemini showing average gains of 0.6 and 0.75 points, respectively.",
            "reference_string": "[267523079 | Chen et al. | 2024 | Citations: 122]"
        },
        {
            "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 76,
            "citation_count": 86,
            "influential_citation_count": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.17012",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.17012, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2213239540",
                    "name": "Ryan Koo"
                },
                {
                    "authorId": "2187932371",
                    "name": "Minhwa Lee"
                },
                {
                    "authorId": "2831377",
                    "name": "Vipul Raheja"
                },
                {
                    "authorId": "2294310015",
                    "name": "Jong Inn Park"
                },
                {
                    "authorId": "2894340",
                    "name": "Zae Myung Kim"
                },
                {
                    "authorId": "48493368",
                    "name": "Dongyeop Kang"
                }
            ],
            "abstract": "Large Language Models are cognitively biased judges. Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 49.6%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences. Our project page is at: https://minnesotanlp.github.io/cobbler.",
            "corpus_id": 263310448,
            "sentences": [
                {
                    "corpus_id": "263310448",
                    "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                    "text": "We particularly focus on decoupling EGOCENTRIC and Salience, which are the most prone to having large correlations with each other (i.e. longer generations may indeed have overall higher quality generated by much stronger models).We highlight two important aspects regarding the identification of these biases: \n\n\u2022 If multiple models have a large proportion of evaluations preferring their own responses (as the evaluated pool of pairwise instances is the same for each evaluator), we reason that this may suggest \"egocentric\" qualities within involved evaluators, regardless of the objective strength of the models. Moreover, we see this effect is especially demonstrated between the more powerful models as well (GPT4 & CHAT-GPT) that suggest the presence of EGOCEN-TRIC evaluations from their disagreement. \n\n\u2022 We employ various strategies to mitigate these confounding variables and isolate each analysis as much as possible. For example, we employ a \"hierarchical\" rubric, where some biases take priority in an evaluation. Specifically, if an evaluation shows signs of order bias by choosing A in (A first, then B) and B in (B first, then A), we do not evaluate it for SALIENCE or EGOCEN-TRIC bias. \n\nTo get further insight into decoupling them, we examine additional statistics in Table 11 displaying the proportion of EGOCENTRIC samples where the model's generation was longer/shorter than the other generation. In particular, since OLMO only won once, and LLAMA never won, their EGOCEN-TRIC ratios look weird. Otherwise, we view overall that most models (9/16) exhibit a self-preference for their own generations often when their own generations exhibit longer token length. \n\nAs above, we see that SALIENCE may be associated with higher quality generations, as we see that the strongest models (GPT4, ChatGPT) often prefer their own responses when their generations are longer. Nevertheless, even in smaller models (e.g., Cohere, Koala), preference for their own generations occurs more often when they are longer. However, as we previously emphasized, if multiple models observe a self-preference for their own generations, it is difficult to associate with SALIENCE as there is disagreement that is indicative of an EGOCENTRIC bias.",
                    "score": 0.36138445945346914,
                    "section_title": "B.4 Decoupling Confounding Factors",
                    "char_start_offset": 34264,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 136
                        },
                        {
                            "start": 137,
                            "end": 310
                        },
                        {
                            "start": 313,
                            "end": 616
                        },
                        {
                            "start": 617,
                            "end": 809
                        },
                        {
                            "start": 812,
                            "end": 929
                        },
                        {
                            "start": 930,
                            "end": 1027
                        },
                        {
                            "start": 1028,
                            "end": 1203
                        },
                        {
                            "start": 1206,
                            "end": 1418
                        },
                        {
                            "start": 1419,
                            "end": 1517
                        },
                        {
                            "start": 1518,
                            "end": 1682
                        },
                        {
                            "start": 1685,
                            "end": 1886
                        },
                        {
                            "start": 1887,
                            "end": 2023
                        },
                        {
                            "start": 2024,
                            "end": 2243
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79541015625
                },
                {
                    "corpus_id": "263310448",
                    "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                    "text": "We categorize biases as \"implicit\" if they can be witnessed without including any additional information other than instructing the model to judge the quality of two given generated texts. \n\nOrder Bias is an evaluation bias we observe when a model tends to favor the model based on the order of the responses rather than their content quality. Order bias has been extensively studied (Jung et al., 2019;Wang et al., 2023a;Zheng et al., 2023), and it is well-known that language models can be influenced by the ordering of the responses in their evaluations. We prompt both orderings of each pair and count the evaluation as a \"first order\" or \"last order\" bias if the evaluator chooses the first ordered (or last ordered) output in both arrangements respectively. \n\nCompassion Fade (Naming). (Butts et al., 2019;V\u00e4stfj\u00e4ll et al., 2014) is a cognitive bias that denotes a decrease in empathy as the number of identifiable individuals increases. To this phenomenon, we modify the definition for our use case to measure whether model evaluations are affected by real/identifiable names as opposed to evaluations with anonymous aliases (e.g. System A). Specifically, an unbiased evaluator would make evaluations similar to when anonymized names were presented. \n\nEgocentric Bias (Self-Preference). (Ross and Sicoly, 1979) is a cognitive bias that refers to the tendency to have a higher opinion of oneself or to more easily accept ideas if they match one's own. We define an evaluator to be egocentrically biased if, for each instance, the evaluator prefers its own response over others. We note that an unbiased evaluator would choose between themselves and other comparand models equally in proportion. However, we highlight that some models would naturally generate higher quality responses (e.g., GPT4 vs. KOALA), resulting in a stronger inclination for such evaluators to choose their own responses. \n\nSalience Bias (Length). (Schenk, 2010;Zheng et al., 2023) The evaluator tends to favor responses that are either shorter or longer in length.",
                    "score": 0.28091338872220295,
                    "section_title": "Implicit Biases",
                    "char_start_offset": 9228,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 188
                        },
                        {
                            "start": 191,
                            "end": 343
                        },
                        {
                            "start": 344,
                            "end": 557
                        },
                        {
                            "start": 558,
                            "end": 763
                        },
                        {
                            "start": 766,
                            "end": 791
                        },
                        {
                            "start": 792,
                            "end": 943
                        },
                        {
                            "start": 944,
                            "end": 1137
                        },
                        {
                            "start": 1138,
                            "end": 1148
                        },
                        {
                            "start": 1149,
                            "end": 1256
                        },
                        {
                            "start": 1259,
                            "end": 1293
                        },
                        {
                            "start": 1294,
                            "end": 1457
                        },
                        {
                            "start": 1458,
                            "end": 1583
                        },
                        {
                            "start": 1584,
                            "end": 1700
                        },
                        {
                            "start": 1701,
                            "end": 1900
                        },
                        {
                            "start": 1903,
                            "end": 1926
                        },
                        {
                            "start": 1927,
                            "end": 2044
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 812,
                            "end": 835,
                            "matchedPaperCorpusId": "9485688"
                        },
                        {
                            "start": 1294,
                            "end": 1316,
                            "matchedPaperCorpusId": "37562229"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56982421875
                }
            ],
            "relevance_judgement": 0.79541015625,
            "relevance_judgment_input_expanded": "# Title: Benchmarking Cognitive Biases in Large Language Models as Evaluators\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, Dongyeop Kang\n## Abstract\nLarge Language Models are cognitively biased judges. Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 49.6%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences. Our project page is at: https://minnesotanlp.github.io/cobbler.\n## Implicit Biases\nWe categorize biases as \"implicit\" if they can be witnessed without including any additional information other than instructing the model to judge the quality of two given generated texts. \n\nOrder Bias is an evaluation bias we observe when a model tends to favor the model based on the order of the responses rather than their content quality. Order bias has been extensively studied (Jung et al., 2019;Wang et al., 2023a;Zheng et al., 2023), and it is well-known that language models can be influenced by the ordering of the responses in their evaluations. We prompt both orderings of each pair and count the evaluation as a \"first order\" or \"last order\" bias if the evaluator chooses the first ordered (or last ordered) output in both arrangements respectively. \n\nCompassion Fade (Naming). (Butts et al., 2019;V\u00e4stfj\u00e4ll et al., 2014) is a cognitive bias that denotes a decrease in empathy as the number of identifiable individuals increases. To this phenomenon, we modify the definition for our use case to measure whether model evaluations are affected by real/identifiable names as opposed to evaluations with anonymous aliases (e.g. System A). Specifically, an unbiased evaluator would make evaluations similar to when anonymized names were presented. \n\nEgocentric Bias (Self-Preference). (Ross and Sicoly, 1979) is a cognitive bias that refers to the tendency to have a higher opinion of oneself or to more easily accept ideas if they match one's own. We define an evaluator to be egocentrically biased if, for each instance, the evaluator prefers its own response over others. We note that an unbiased evaluator would choose between themselves and other comparand models equally in proportion. However, we highlight that some models would naturally generate higher quality responses (e.g., GPT4 vs. KOALA), resulting in a stronger inclination for such evaluators to choose their own responses. \n\nSalience Bias (Length). (Schenk, 2010;Zheng et al., 2023) The evaluator tends to favor responses that are either shorter or longer in length.\n\n## B.4 Decoupling Confounding Factors\nWe particularly focus on decoupling EGOCENTRIC and Salience, which are the most prone to having large correlations with each other (i.e. longer generations may indeed have overall higher quality generated by much stronger models).We highlight two important aspects regarding the identification of these biases: \n\n\u2022 If multiple models have a large proportion of evaluations preferring their own responses (as the evaluated pool of pairwise instances is the same for each evaluator), we reason that this may suggest \"egocentric\" qualities within involved evaluators, regardless of the objective strength of the models. Moreover, we see this effect is especially demonstrated between the more powerful models as well (GPT4 & CHAT-GPT) that suggest the presence of EGOCEN-TRIC evaluations from their disagreement. \n\n\u2022 We employ various strategies to mitigate these confounding variables and isolate each analysis as much as possible. For example, we employ a \"hierarchical\" rubric, where some biases take priority in an evaluation. Specifically, if an evaluation shows signs of order bias by choosing A in (A first, then B) and B in (B first, then A), we do not evaluate it for SALIENCE or EGOCEN-TRIC bias. \n\nTo get further insight into decoupling them, we examine additional statistics in Table 11 displaying the proportion of EGOCENTRIC samples where the model's generation was longer/shorter than the other generation. In particular, since OLMO only won once, and LLAMA never won, their EGOCEN-TRIC ratios look weird. Otherwise, we view overall that most models (9/16) exhibit a self-preference for their own generations often when their own generations exhibit longer token length. \n\nAs above, we see that SALIENCE may be associated with higher quality generations, as we see that the strongest models (GPT4, ChatGPT) often prefer their own responses when their generations are longer. Nevertheless, even in smaller models (e.g., Cohere, Koala), preference for their own generations occurs more often when they are longer. However, as we previously emphasized, if multiple models observe a self-preference for their own generations, it is difficult to associate with SALIENCE as there is disagreement that is indicative of an EGOCENTRIC bias.",
            "reference_string": "[263310448 | Koo et al. | 2023 | Citations: 86]"
        },
        {
            "title": "Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments",
            "venue": "ACM Symposium on Applied Computing",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.00323, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333360231",
                    "name": "Yasuaki Sumita"
                },
                {
                    "authorId": "2243408877",
                    "name": "Koh Takeuchi"
                },
                {
                    "authorId": "2247886893",
                    "name": "Hisashi Kashima"
                }
            ],
            "abstract": "Large Language Models (LLMs) are trained on large corpora written by humans and demonstrate high performance on various tasks. However, as humans are susceptible to cognitive biases, which can result in irrational judgments, LLMs can also be influenced by these biases, leading to irrational decision-making. For example, changing the order of options in multiple-choice questions affects the performance of LLMs due to order bias. In our research, we first conducted an extensive survey of existing studies examining LLMs' cognitive biases and their mitigation. The mitigation techniques in LLMs have the disadvantage that they are limited in the type of biases they can apply or require lengthy inputs or outputs. We then examined the effectiveness of two mitigation methods for humans, SoPro and AwaRe, when applied to LLMs, inspired by studies in crowdsourcing. To test the effectiveness of these methods, we conducted experiments on GPT-3.5 and GPT-4 to evaluate the influence of six biases on the outputs before and after applying these methods. The results demonstrate that while SoPro has little effect, AwaRe enables LLMs to mitigate the effect of these biases and make more rational responses.",
            "corpus_id": 274437478,
            "sentences": [
                {
                    "corpus_id": "274437478",
                    "title": "Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments",
                    "text": "The results are presented in Table 1 and Table 2. In these tables, higher scores are colored red, and lower scores are colored blue, relative to the case where the responses are random. This color coding indicates that red scores signify less consistency and greater influence of bias, whereas blue scores suggest greater consistency and less influence of bias. Furthermore, the intensity of the color increases with the deviation from the random response case. GPT-4 is more robust against cognitive biases compared to GPT-3.5. At baseline, GPT-3.5 exhibits vulnerability to bandwagon effect, attentional bias, and verbosity bias. Conversely, GPT-4 shows reduced susceptibility to all biases. GPT-4 indicated higher resistance to bandwagon effect and verbosity bias than GPT-3.5. \n\nThe existing method proves effective for mitigating bandwagon effect and attentional bias in GPT-3.5, as well as bandwagon effect in GPT-4. This aligns with findings from previous studies. \n\nAlthough SoPro mitigates egocentric bias in GPT-3.5 and verbosity bias in GPT-4, the models' susceptibility to bandwagon effect increased. SoPro, which aligns LLMs with others' perspectives, may not be effective for cognitive biases. This result is inconsistent with the claim of the study for humans. AwaRe mitigates order bias, compassion fade, egocentric bias, bandwagon effect, and attentional bias in GPT-3.5, and bandwagon effect and verbosity bias in GPT-4. This result suggests that AwaRe prompts LLMs to make rational judgments.",
                    "score": 0.42733162258228885,
                    "section_title": "Results",
                    "char_start_offset": 7202,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 49
                        },
                        {
                            "start": 50,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 361
                        },
                        {
                            "start": 362,
                            "end": 461
                        },
                        {
                            "start": 462,
                            "end": 528
                        },
                        {
                            "start": 529,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 693
                        },
                        {
                            "start": 694,
                            "end": 780
                        },
                        {
                            "start": 783,
                            "end": 922
                        },
                        {
                            "start": 923,
                            "end": 971
                        },
                        {
                            "start": 974,
                            "end": 1112
                        },
                        {
                            "start": 1113,
                            "end": 1207
                        },
                        {
                            "start": 1208,
                            "end": 1275
                        },
                        {
                            "start": 1276,
                            "end": 1438
                        },
                        {
                            "start": 1439,
                            "end": 1511
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71044921875
                }
            ],
            "relevance_judgement": 0.71044921875,
            "relevance_judgment_input_expanded": "# Title: Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments\n# Venue: ACM Symposium on Applied Computing\n# Authors: Yasuaki Sumita, Koh Takeuchi, Hisashi Kashima\n## Abstract\nLarge Language Models (LLMs) are trained on large corpora written by humans and demonstrate high performance on various tasks. However, as humans are susceptible to cognitive biases, which can result in irrational judgments, LLMs can also be influenced by these biases, leading to irrational decision-making. For example, changing the order of options in multiple-choice questions affects the performance of LLMs due to order bias. In our research, we first conducted an extensive survey of existing studies examining LLMs' cognitive biases and their mitigation. The mitigation techniques in LLMs have the disadvantage that they are limited in the type of biases they can apply or require lengthy inputs or outputs. We then examined the effectiveness of two mitigation methods for humans, SoPro and AwaRe, when applied to LLMs, inspired by studies in crowdsourcing. To test the effectiveness of these methods, we conducted experiments on GPT-3.5 and GPT-4 to evaluate the influence of six biases on the outputs before and after applying these methods. The results demonstrate that while SoPro has little effect, AwaRe enables LLMs to mitigate the effect of these biases and make more rational responses.\n## Results\nThe results are presented in Table 1 and Table 2. In these tables, higher scores are colored red, and lower scores are colored blue, relative to the case where the responses are random. This color coding indicates that red scores signify less consistency and greater influence of bias, whereas blue scores suggest greater consistency and less influence of bias. Furthermore, the intensity of the color increases with the deviation from the random response case. GPT-4 is more robust against cognitive biases compared to GPT-3.5. At baseline, GPT-3.5 exhibits vulnerability to bandwagon effect, attentional bias, and verbosity bias. Conversely, GPT-4 shows reduced susceptibility to all biases. GPT-4 indicated higher resistance to bandwagon effect and verbosity bias than GPT-3.5. \n\nThe existing method proves effective for mitigating bandwagon effect and attentional bias in GPT-3.5, as well as bandwagon effect in GPT-4. This aligns with findings from previous studies. \n\nAlthough SoPro mitigates egocentric bias in GPT-3.5 and verbosity bias in GPT-4, the models' susceptibility to bandwagon effect increased. SoPro, which aligns LLMs with others' perspectives, may not be effective for cognitive biases. This result is inconsistent with the claim of the study for humans. AwaRe mitigates order bias, compassion fade, egocentric bias, bandwagon effect, and attentional bias in GPT-3.5, and bandwagon effect and verbosity bias in GPT-4. This result suggests that AwaRe prompts LLMs to make rational judgments.",
            "reference_string": "[274437478 | Sumita et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Self-Preference Bias in LLM-as-a-Judge",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 24,
            "citation_count": 25,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.21819, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2007365532",
                    "name": "Koki Wataoka"
                },
                {
                    "authorId": "2325815191",
                    "name": "Tsubasa Takahashi"
                },
                {
                    "authorId": "1466451143",
                    "name": "Ryokan Ri"
                }
            ],
            "abstract": "Automated evaluation leveraging large language models (LLMs), commonly referred to as LLM evaluators or LLM-as-a-judge, has been widely used in measuring the performance of dialogue systems. However, the self-preference bias in LLMs has posed significant risks, including promoting specific styles or policies intrinsic to the LLMs. Despite the importance of this issue, there is a lack of established methods to measure the self-preference bias quantitatively, and its underlying causes are poorly understood. In this paper, we introduce a novel quantitative metric to measure the self-preference bias. Our experimental results demonstrate that GPT-4 exhibits a significant degree of self-preference bias. To explore the causes, we hypothesize that LLMs may favor outputs that are more familiar to them, as indicated by lower perplexity. We analyze the relationship between LLM evaluations and the perplexities of outputs. Our findings reveal that LLMs assign significantly higher evaluations to outputs with lower perplexity than human evaluators, regardless of whether the outputs were self-generated. This suggests that the essence of the bias lies in perplexity and that the self-preference bias exists because LLMs prefer texts more familiar to them.",
            "corpus_id": 273661820,
            "sentences": [
                {
                    "corpus_id": "273661820",
                    "title": "Self-Preference Bias in LLM-as-a-Judge",
                    "text": "To reduce self-preference bias, one possible approach is ensemble evaluation using multiple models. This method is expected to provide a more equitable evaluation by avoiding reliance on a single model. Specifically, when a model Figure 4: vs other LLMs conditioned on perplexity. Winning judgment rates by LLMs on their own texts and texts generated by other models conditioned on perplexity are plotted. Across all models, except for dolly-v2-12b and stablelm-tuned-alpha-7b, no significant difference was observed between the judgment rates for their own texts and those generated by other models. This suggests that LLM evaluators assign higher ratings to texts with lower perplexity, regardless of whether the text was self-generated or produced by other models. \n\nexhibits low perplexity on a sample, decreasing the weight assigned to that model's evaluation for that sample may contribute to bias mitigation. To evaluate the effectiveness of bias reduction strategies, our proposed new metric can be utilized. Therefore, we believe that our research makes a significant contribution to the understanding of self-preference bias and will greatly facilitate the development of future research in this area. \n\nOur experimental results reveal that LLM evaluators tend to assign higher scores to texts with lower perplexity. We further discuss the reasons behind this phenomenon. First, LLMs are trained during the pretraining phase to reduce perplexity on large-scale text corpora. Moreover, when aligning with human preferences, the models are also trained to minimize perplexity on the given dialogue data. Therefore, high-perplexity texts are likely those that the LLM has not frequently encountered during training, suggesting that such texts may be related to domains that the LLM evaluators do not fully comprehend. \n\nThis observation may seem contradicted by the fact that GPT-4, which is well-versed across various domains due to a wide range of benchmarks, exhibits a high degree of self-preference bias. However, by investigating specific cases of self-preference bias, as shown in Figure 1a and Table 1, we found that the bias was often not related to clear factual errors but rather to differences in response styles, such as the handling of specialized domains or the description of premises before answering.",
                    "score": 0.31288310784562373,
                    "section_title": "Discussion",
                    "char_start_offset": 19914,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 99
                        },
                        {
                            "start": 100,
                            "end": 202
                        },
                        {
                            "start": 203,
                            "end": 280
                        },
                        {
                            "start": 281,
                            "end": 405
                        },
                        {
                            "start": 406,
                            "end": 600
                        },
                        {
                            "start": 601,
                            "end": 767
                        },
                        {
                            "start": 770,
                            "end": 915
                        },
                        {
                            "start": 916,
                            "end": 1016
                        },
                        {
                            "start": 1017,
                            "end": 1211
                        },
                        {
                            "start": 1214,
                            "end": 1326
                        },
                        {
                            "start": 1327,
                            "end": 1381
                        },
                        {
                            "start": 1382,
                            "end": 1484
                        },
                        {
                            "start": 1485,
                            "end": 1611
                        },
                        {
                            "start": 1612,
                            "end": 1824
                        },
                        {
                            "start": 1827,
                            "end": 2016
                        },
                        {
                            "start": 2017,
                            "end": 2325
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70166015625
                },
                {
                    "corpus_id": "273661820",
                    "title": "Self-Preference Bias in LLM-as-a-Judge",
                    "text": "(b) compares the self-preference bias scores using our proposed metric (Definition 4.1). These figures demonstrate that GPT-4 exhibits a stronger self-preference bias than other models, suggesting that it tends to rate its own outputs more favorably than human evaluations. For detailed experimental settings, refer to Section 4. \n\nscope to specific tasks, such as text summarization or machine translation, and relied on reference-based metrics like BLEURT [Sellam et al., 2020], which does not reflect the diversity of real-world use cases. \n\nBy contrast, a pairwise evaluation approach that involves direct comparison between two texts enables evaluators to recognize specific differences more readily, resulting in more consistent human judgments. Consequently, such pairwise evaluation methods are particularly suitable for analyzing biases related to discrepancies with human evaluations. \n\nIn this paper, we measure the self-preference biases of LLMs in the pairwise evaluation. To accomplish this, we propose a new metric to quantify self-preference bias on the basis of algorithmic fairness concepts, thereby enabling discussions within the existing frameworks of fairness. In our experiment, we measured self-preference bias in eight LLMs. The results indicated that GPT-4 exhibited a significant self-preference bias (Figure 1b). This finding suggests a potential concern: using GPT-4 as a judge may lead to excessive influence from GPT-4's unique styles and policies. \n\nFurthermore, we investigated the underlying causes of self-preference bias. Although LLM evaluators are not explicitly informed whether a given text is their own, they still exhibit self-preference bias. We hypothesized that LLM evaluators might be affected by the perplexity of the text, which tends to be lower perplexity when it is generated by themselves. \n\nTo test this hypothesis, we analyzed the relationship between the perplexities of the texts to be evaluated and their corresponding evaluations. Our analysis revealed that LLMs assign significantly higher evaluations to texts with lower perplexity than human evaluators, regardless of whether the texts were self-generated. This suggests that the fundamental cause of self-preference bias may be the familiarity of the texts to the LLM evaluators, specifically how likely they are to generate the same response.",
                    "score": 0.3488467975045156,
                    "section_title": "Introduction",
                    "char_start_offset": 2076,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 88
                        },
                        {
                            "start": 89,
                            "end": 273
                        },
                        {
                            "start": 274,
                            "end": 329
                        },
                        {
                            "start": 332,
                            "end": 542
                        },
                        {
                            "start": 545,
                            "end": 751
                        },
                        {
                            "start": 752,
                            "end": 894
                        },
                        {
                            "start": 897,
                            "end": 985
                        },
                        {
                            "start": 986,
                            "end": 1182
                        },
                        {
                            "start": 1183,
                            "end": 1249
                        },
                        {
                            "start": 1250,
                            "end": 1340
                        },
                        {
                            "start": 1341,
                            "end": 1479
                        },
                        {
                            "start": 1482,
                            "end": 1557
                        },
                        {
                            "start": 1558,
                            "end": 1685
                        },
                        {
                            "start": 1686,
                            "end": 1841
                        },
                        {
                            "start": 1844,
                            "end": 1988
                        },
                        {
                            "start": 1989,
                            "end": 2167
                        },
                        {
                            "start": 2168,
                            "end": 2355
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 458,
                            "end": 479,
                            "matchedPaperCorpusId": "215548699"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61376953125
                },
                {
                    "corpus_id": "273661820",
                    "title": "Self-Preference Bias in LLM-as-a-Judge",
                    "text": "In this study, we propose a metric to quantify the self-preference bias in LLM-as-a-judge and measured the selfpreference bias of eight LLMs. Experimental results confirmed that GPT-4, in particular, exhibits a high self-preference bias. This finding suggests a risk that GPT-4 as a judge may inadvertently reinforce its own style and policies. Furthermore, we hypothesized that the self-preference bias is related to the perplexity of the texts, and showed that, compared to human evaluators, LLM evaluators assigned higher evaluations to texts with lower perplexity, and this tendency was observed regardless of whether the text was generated by themselves or not. This suggests that the essence of the bias lies in perplexity and that the self-preference bias exists because LLMs prefer texts more familiar to them.",
                    "score": 0.33226129614872224,
                    "section_title": "Conclusion",
                    "char_start_offset": 23978,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 237
                        },
                        {
                            "start": 238,
                            "end": 344
                        },
                        {
                            "start": 345,
                            "end": 666
                        },
                        {
                            "start": 667,
                            "end": 818
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5224609375
                },
                {
                    "corpus_id": "273661820",
                    "title": "Self-Preference Bias in LLM-as-a-Judge",
                    "text": "The results of the bias measurement using Definition 4.1 are presented in Figure 1b. It was confirmed that GPT-4 exhibits the highest self-preference bias. Definition 4.1 focuses on the recall of the LLM evaluator concerning both high and low ratings by the human evaluator. Thus, it can be concluded that GPT-4 showed lower recall in cases where humans evaluated unfavorably compared to when higher evaluating. When examining the recall values in the confusion matrix shown in Figure 2, they are calculated as 0.945 \u2248 1852 108+1852 and 0.425 \u2248 118 160+118 . The difference between these values is 0.520, which corresponds to the value reported in Figure 1b. Following GPT-4, Vicuna-13b and Koala-13b also exhibited significant bias. In contrast, other LLMs displayed values relatively close to zero. Notably, oasst-pythia-12, dolly-v2-12b, and stablelm-tuned-alpha-7b showed negative values, indicating a reverse bias where the LLMs tend to underestimate their own outputs. \n\nTable 1 presents a randomly selected example of self-preference bias in GPT-4, where humans favored the alternative response. In this example, the user query is a straightforward request to list blue items. While GPT-4 states that it lacks physical recognition before listing, GPT-3.5-Turbo directly lists the blue items without any such explanation. Both responses are of high quality, and the final evaluation reflects the evaluator's policy and stylistic preferences. Although humans and GPT-3.5-Turbo preferred the response from GPT-3.5-Turbo, GPT-4 favored its own response, illustrating a typical case of self-preference bias. \n\nFigure 2: matrix for each LLM evaluator's assessment of pairs including its own output. It suggests that some LLMs, including GPT-4, have relatively high true positive rates than true negative rate, which means that they have self-preference bias. Table 1: An example of self-preference bias in GPT-4. The user query involves a simple request to list blue items. GPT-4 responds by first acknowledging its lack of physical cognition, and then proceeds to list blue items.",
                    "score": 0.4094292764179572,
                    "section_title": "Result",
                    "char_start_offset": 13303,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 84
                        },
                        {
                            "start": 85,
                            "end": 155
                        },
                        {
                            "start": 156,
                            "end": 274
                        },
                        {
                            "start": 275,
                            "end": 411
                        },
                        {
                            "start": 412,
                            "end": 558
                        },
                        {
                            "start": 559,
                            "end": 658
                        },
                        {
                            "start": 659,
                            "end": 733
                        },
                        {
                            "start": 734,
                            "end": 800
                        },
                        {
                            "start": 801,
                            "end": 974
                        },
                        {
                            "start": 977,
                            "end": 1102
                        },
                        {
                            "start": 1103,
                            "end": 1183
                        },
                        {
                            "start": 1184,
                            "end": 1267
                        },
                        {
                            "start": 1268,
                            "end": 1327
                        },
                        {
                            "start": 1328,
                            "end": 1447
                        },
                        {
                            "start": 1448,
                            "end": 1481
                        },
                        {
                            "start": 1482,
                            "end": 1524
                        },
                        {
                            "start": 1525,
                            "end": 1609
                        },
                        {
                            "start": 1612,
                            "end": 1699
                        },
                        {
                            "start": 1700,
                            "end": 1859
                        },
                        {
                            "start": 1860,
                            "end": 1913
                        },
                        {
                            "start": 1914,
                            "end": 1974
                        },
                        {
                            "start": 1975,
                            "end": 2082
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.38525390625
                },
                {
                    "corpus_id": "273661820",
                    "title": "Self-Preference Bias in LLM-as-a-Judge",
                    "text": "However, by investigating specific cases of self-preference bias, as shown in Figure 1a and Table 1, we found that the bias was often not related to clear factual errors but rather to differences in response styles, such as the handling of specialized domains or the description of premises before answering. This suggests that, advanced models like GPT-4, which thoroughly understand and adhere to their predefined policies, may use the degree of alignment with these policies as a deciding factor when evaluating responses of comparable quality. \n\nThe proposed metric in Definition 4.1 is based on the fairness definition known as Equal Opportunity. Demographic Parity Calders et al. [2009] is also a prominent definition of fairness. Demographic Parity requires that the predictive distribution of a classifier be consistent across sensitive groups, regardless of the ground truth. In the context of this study, the focus is on whether the evaluations by LLMs align with human preferences, which is why Demographic Parity was not the focal point. While the bias metric based on Demographic Parity cannot demonstrate the unfairness of an evaluation, it is useful for analyzing how highly each LLM evaluator rates its own outputs within the given experimental setup. Similar to the Definition 4.1, the self-preference bias based on Demographic Parity also can be quantified as follows: \n\nThe scores derived from this metric of eight LLMs are presented in Table 2. The results indicate that GPT-4 exhibited significant bias, followed by Vicuna-13b, which aligns closely with the results obtained using Definition 4.1. \n\nTable 2: bias scores based on Demographic Parity. GPT-4 assigns the highest scores to its own outputs, followed by Vicuna-13b. However, it is important to note that these scores do not take intrinsic quality into account. Therefore, this analysis reflects how highly each LLM evaluator rates its own outputs within the given experimental setup and should not be interpreted as an indication of unjust evaluation.",
                    "score": 0.30627748957349615,
                    "section_title": "Discussion",
                    "char_start_offset": 21931,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 308
                        },
                        {
                            "start": 309,
                            "end": 547
                        },
                        {
                            "start": 550,
                            "end": 651
                        },
                        {
                            "start": 652,
                            "end": 736
                        },
                        {
                            "start": 737,
                            "end": 884
                        },
                        {
                            "start": 885,
                            "end": 1049
                        },
                        {
                            "start": 1050,
                            "end": 1267
                        },
                        {
                            "start": 1268,
                            "end": 1386
                        },
                        {
                            "start": 1389,
                            "end": 1464
                        },
                        {
                            "start": 1465,
                            "end": 1617
                        },
                        {
                            "start": 1620,
                            "end": 1669
                        },
                        {
                            "start": 1670,
                            "end": 1746
                        },
                        {
                            "start": 1747,
                            "end": 1841
                        },
                        {
                            "start": 1842,
                            "end": 2032
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 671,
                            "end": 692,
                            "matchedPaperCorpusId": "3945595"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.384765625
                }
            ],
            "relevance_judgement": 0.70166015625,
            "relevance_judgment_input_expanded": "# Title: Self-Preference Bias in LLM-as-a-Judge\n# Venue: arXiv.org\n# Authors: Koki Wataoka, Tsubasa Takahashi, Ryokan Ri\n## Abstract\nAutomated evaluation leveraging large language models (LLMs), commonly referred to as LLM evaluators or LLM-as-a-judge, has been widely used in measuring the performance of dialogue systems. However, the self-preference bias in LLMs has posed significant risks, including promoting specific styles or policies intrinsic to the LLMs. Despite the importance of this issue, there is a lack of established methods to measure the self-preference bias quantitatively, and its underlying causes are poorly understood. In this paper, we introduce a novel quantitative metric to measure the self-preference bias. Our experimental results demonstrate that GPT-4 exhibits a significant degree of self-preference bias. To explore the causes, we hypothesize that LLMs may favor outputs that are more familiar to them, as indicated by lower perplexity. We analyze the relationship between LLM evaluations and the perplexities of outputs. Our findings reveal that LLMs assign significantly higher evaluations to outputs with lower perplexity than human evaluators, regardless of whether the outputs were self-generated. This suggests that the essence of the bias lies in perplexity and that the self-preference bias exists because LLMs prefer texts more familiar to them.\n## Introduction\n(b) compares the self-preference bias scores using our proposed metric (Definition 4.1). These figures demonstrate that GPT-4 exhibits a stronger self-preference bias than other models, suggesting that it tends to rate its own outputs more favorably than human evaluations. For detailed experimental settings, refer to Section 4. \n\nscope to specific tasks, such as text summarization or machine translation, and relied on reference-based metrics like BLEURT [Sellam et al., 2020], which does not reflect the diversity of real-world use cases. \n\nBy contrast, a pairwise evaluation approach that involves direct comparison between two texts enables evaluators to recognize specific differences more readily, resulting in more consistent human judgments. Consequently, such pairwise evaluation methods are particularly suitable for analyzing biases related to discrepancies with human evaluations. \n\nIn this paper, we measure the self-preference biases of LLMs in the pairwise evaluation. To accomplish this, we propose a new metric to quantify self-preference bias on the basis of algorithmic fairness concepts, thereby enabling discussions within the existing frameworks of fairness. In our experiment, we measured self-preference bias in eight LLMs. The results indicated that GPT-4 exhibited a significant self-preference bias (Figure 1b). This finding suggests a potential concern: using GPT-4 as a judge may lead to excessive influence from GPT-4's unique styles and policies. \n\nFurthermore, we investigated the underlying causes of self-preference bias. Although LLM evaluators are not explicitly informed whether a given text is their own, they still exhibit self-preference bias. We hypothesized that LLM evaluators might be affected by the perplexity of the text, which tends to be lower perplexity when it is generated by themselves. \n\nTo test this hypothesis, we analyzed the relationship between the perplexities of the texts to be evaluated and their corresponding evaluations. Our analysis revealed that LLMs assign significantly higher evaluations to texts with lower perplexity than human evaluators, regardless of whether the texts were self-generated. This suggests that the fundamental cause of self-preference bias may be the familiarity of the texts to the LLM evaluators, specifically how likely they are to generate the same response.\n\n## Result\nThe results of the bias measurement using Definition 4.1 are presented in Figure 1b. It was confirmed that GPT-4 exhibits the highest self-preference bias. Definition 4.1 focuses on the recall of the LLM evaluator concerning both high and low ratings by the human evaluator. Thus, it can be concluded that GPT-4 showed lower recall in cases where humans evaluated unfavorably compared to when higher evaluating. When examining the recall values in the confusion matrix shown in Figure 2, they are calculated as 0.945 \u2248 1852 108+1852 and 0.425 \u2248 118 160+118 . The difference between these values is 0.520, which corresponds to the value reported in Figure 1b. Following GPT-4, Vicuna-13b and Koala-13b also exhibited significant bias. In contrast, other LLMs displayed values relatively close to zero. Notably, oasst-pythia-12, dolly-v2-12b, and stablelm-tuned-alpha-7b showed negative values, indicating a reverse bias where the LLMs tend to underestimate their own outputs. \n\nTable 1 presents a randomly selected example of self-preference bias in GPT-4, where humans favored the alternative response. In this example, the user query is a straightforward request to list blue items. While GPT-4 states that it lacks physical recognition before listing, GPT-3.5-Turbo directly lists the blue items without any such explanation. Both responses are of high quality, and the final evaluation reflects the evaluator's policy and stylistic preferences. Although humans and GPT-3.5-Turbo preferred the response from GPT-3.5-Turbo, GPT-4 favored its own response, illustrating a typical case of self-preference bias. \n\nFigure 2: matrix for each LLM evaluator's assessment of pairs including its own output. It suggests that some LLMs, including GPT-4, have relatively high true positive rates than true negative rate, which means that they have self-preference bias. Table 1: An example of self-preference bias in GPT-4. The user query involves a simple request to list blue items. GPT-4 responds by first acknowledging its lack of physical cognition, and then proceeds to list blue items.\n\n## Discussion\nTo reduce self-preference bias, one possible approach is ensemble evaluation using multiple models. This method is expected to provide a more equitable evaluation by avoiding reliance on a single model. Specifically, when a model Figure 4: vs other LLMs conditioned on perplexity. Winning judgment rates by LLMs on their own texts and texts generated by other models conditioned on perplexity are plotted. Across all models, except for dolly-v2-12b and stablelm-tuned-alpha-7b, no significant difference was observed between the judgment rates for their own texts and those generated by other models. This suggests that LLM evaluators assign higher ratings to texts with lower perplexity, regardless of whether the text was self-generated or produced by other models. \n\nexhibits low perplexity on a sample, decreasing the weight assigned to that model's evaluation for that sample may contribute to bias mitigation. To evaluate the effectiveness of bias reduction strategies, our proposed new metric can be utilized. Therefore, we believe that our research makes a significant contribution to the understanding of self-preference bias and will greatly facilitate the development of future research in this area. \n\nOur experimental results reveal that LLM evaluators tend to assign higher scores to texts with lower perplexity. We further discuss the reasons behind this phenomenon. First, LLMs are trained during the pretraining phase to reduce perplexity on large-scale text corpora. Moreover, when aligning with human preferences, the models are also trained to minimize perplexity on the given dialogue data. Therefore, high-perplexity texts are likely those that the LLM has not frequently encountered during training, suggesting that such texts may be related to domains that the LLM evaluators do not fully comprehend. \n\nThis observation may seem contradicted by the fact that GPT-4, which is well-versed across various domains due to a wide range of benchmarks, exhibits a high degree of self-preference bias. However, by investigating specific cases of self-preference bias, as shown in Figure 1a and Table 1, we found that the bias was often not related to clear factual errors but rather to differences in response styles, such as the handling of specialized domains or the description of premises before answering.\n...\nHowever, by investigating specific cases of self-preference bias, as shown in Figure 1a and Table 1, we found that the bias was often not related to clear factual errors but rather to differences in response styles, such as the handling of specialized domains or the description of premises before answering. This suggests that, advanced models like GPT-4, which thoroughly understand and adhere to their predefined policies, may use the degree of alignment with these policies as a deciding factor when evaluating responses of comparable quality. \n\nThe proposed metric in Definition 4.1 is based on the fairness definition known as Equal Opportunity. Demographic Parity Calders et al. [2009] is also a prominent definition of fairness. Demographic Parity requires that the predictive distribution of a classifier be consistent across sensitive groups, regardless of the ground truth. In the context of this study, the focus is on whether the evaluations by LLMs align with human preferences, which is why Demographic Parity was not the focal point. While the bias metric based on Demographic Parity cannot demonstrate the unfairness of an evaluation, it is useful for analyzing how highly each LLM evaluator rates its own outputs within the given experimental setup. Similar to the Definition 4.1, the self-preference bias based on Demographic Parity also can be quantified as follows: \n\nThe scores derived from this metric of eight LLMs are presented in Table 2. The results indicate that GPT-4 exhibited significant bias, followed by Vicuna-13b, which aligns closely with the results obtained using Definition 4.1. \n\nTable 2: bias scores based on Demographic Parity. GPT-4 assigns the highest scores to its own outputs, followed by Vicuna-13b. However, it is important to note that these scores do not take intrinsic quality into account. Therefore, this analysis reflects how highly each LLM evaluator rates its own outputs within the given experimental setup and should not be interpreted as an indication of unjust evaluation.\n\n## Conclusion\nIn this study, we propose a metric to quantify the self-preference bias in LLM-as-a-judge and measured the selfpreference bias of eight LLMs. Experimental results confirmed that GPT-4, in particular, exhibits a high self-preference bias. This finding suggests a risk that GPT-4 as a judge may inadvertently reinforce its own style and policies. Furthermore, we hypothesized that the self-preference bias is related to the perplexity of the texts, and showed that, compared to human evaluators, LLM evaluators assigned higher evaluations to texts with lower perplexity, and this tendency was observed regardless of whether the text was generated by themselves or not. This suggests that the essence of the bias lies in perplexity and that the self-preference bias exists because LLMs prefer texts more familiar to them.",
            "reference_string": "[273661820 | Wataoka et al. | 2024 | Citations: 25]"
        },
        {
            "title": "Addressing cognitive bias in medical language models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 26,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.08113, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1741847434",
                    "name": "Samuel Schmidgall"
                },
                {
                    "authorId": "2283934409",
                    "name": "Carl Harris"
                },
                {
                    "authorId": "2283935007",
                    "name": "Ime Essien"
                },
                {
                    "authorId": "2283936098",
                    "name": "Daniel Olshvang"
                },
                {
                    "authorId": "2283934987",
                    "name": "Tawsifur Rahman"
                },
                {
                    "authorId": "2277454837",
                    "name": "Ji Woong Kim"
                },
                {
                    "authorId": "2212401663",
                    "name": "Rojin Ziaei"
                },
                {
                    "authorId": "2256989424",
                    "name": "Jason Eshraghian"
                },
                {
                    "authorId": "2283934452",
                    "name": "Peter M Abadir"
                },
                {
                    "authorId": "2281927234",
                    "name": "Rama Chellappa"
                }
            ],
            "abstract": "There is increasing interest in the application large language models (LLMs) to the medical field, in part because of their impressive performance on medical exam questions. While promising, exam questions do not reflect the complexity of real patient-doctor interactions. In reality, physicians' decisions are shaped by many complex factors, such as patient compliance, personal experience, ethical beliefs, and cognitive bias. Taking a step toward understanding this, our hypothesis posits that when LLMs are confronted with clinical questions containing cognitive biases, they will yield significantly less accurate responses compared to the same questions presented without such biases. In this study, we developed BiasMedQA, a benchmark for evaluating cognitive biases in LLMs applied to medical tasks. Using BiasMedQA we evaluated six LLMs, namely GPT-4, Mixtral-8x70B, GPT-3.5, PaLM-2, Llama 2 70B-chat, and the medically specialized PMC Llama 13B. We tested these models on 1,273 questions from the US Medical Licensing Exam (USMLE) Steps 1, 2, and 3, modified to replicate common clinically-relevant cognitive biases. Our analysis revealed varying effects for biases on these LLMs, with GPT-4 standing out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which were disproportionately affected by cognitive bias. Our findings highlight the critical need for bias mitigation in the development of medical LLMs, pointing towards safer and more reliable applications in healthcare.",
            "corpus_id": 267637243,
            "sentences": [
                {
                    "corpus_id": "267637243",
                    "title": "Addressing cognitive bias in medical language models",
                    "text": "We demonstrate the results of three mitigation strategies: (1) bias education, (2) one-shot bias demonstration, and (3) fewshot bias demonstration (see Appendix B for details). For bias education, the model is provided with a short warning educating the model about potential cognitive biases, such as the following text provided for recency bias: \"Keep in mind the importance of individualized patient evaluation. Each patient is unique, and recent cases should not overshadow individual assessment and evidence-based practice.\" \n\nOne-shot bias demonstration includes a sample question from the MedQA dataset accompanied by a bias-inducing prompt. It also presents an example response that incorrectly selects an answer based on the bias from the prompt, which we refer to as a negative example. Before this incorrect answer, the model is presented with: \"The following is an example of incorrectly classifying based on [cognitive bias].\" \n\nFor the few-shot bias demonstration strategy, both a negative and a positive example are provided as part of the prompt. The negative example is the same as was shown in the oneshot bias demonstration, and the positive example is presented as follows: \"The following is an example of correctly classifying based on [cognitive bias],\" together with a correct classification. \n\nThe results of each bias mitigation strategy are presented in Tables 2-4 and graphically depicted in Figure 3. In comparing these three strategies, it is evident that different models respond differently to various mitigation techniques. gpt-4 consistently shows the highest level of improvement across all strategies. The other models, while showing some level of improvement, do not match gpt-4. This suggests that the architecture and training of gpt-4 might be more robust to bias-related feedback. \n\nBias education: The strategy of educating models about cognitive biases yielded the most significant improvements in gpt-4. For instance, in the \"Frequency\" bias category, its accuracy improved from 0.627 to 0.720. However, other models like mixtral-8x7b and gpt-3.5 displayed only marginal improvements.",
                    "score": 0.32833687476345497,
                    "section_title": "Bias mitigation strategies",
                    "char_start_offset": 17033,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 177,
                            "end": 414
                        },
                        {
                            "start": 415,
                            "end": 529
                        },
                        {
                            "start": 532,
                            "end": 648
                        },
                        {
                            "start": 649,
                            "end": 796
                        },
                        {
                            "start": 797,
                            "end": 939
                        },
                        {
                            "start": 942,
                            "end": 1062
                        },
                        {
                            "start": 1063,
                            "end": 1315
                        },
                        {
                            "start": 1318,
                            "end": 1428
                        },
                        {
                            "start": 1429,
                            "end": 1555
                        },
                        {
                            "start": 1556,
                            "end": 1636
                        },
                        {
                            "start": 1637,
                            "end": 1715
                        },
                        {
                            "start": 1716,
                            "end": 1820
                        },
                        {
                            "start": 1823,
                            "end": 1946
                        },
                        {
                            "start": 1947,
                            "end": 2037
                        },
                        {
                            "start": 2038,
                            "end": 2127
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6650390625
                },
                {
                    "corpus_id": "267637243",
                    "title": "Addressing cognitive bias in medical language models",
                    "text": "To understand the effect of common cognitive biases on medical models, we first evaluate the accuracy of each model with and without bias prompts on questions from the MedQA dataset. We then introduce three novel strategies for bias mitigation. \n\nWithout bias, we report the mean accuracy of each model across the USMLE test questions in Table 1. We find gpt-4 has significantly higher performance than all other models at 72.7% accuracy, compared with the second and third best models, mixtral-8x7b and gpt-3.5, with 51.8% and 49.7% accuracy respectively. Interestingly, the most medically relevant model, pmc-llama-13b, has the lowest performance of all models with 33.4%. \n\nOnce the bias prompts are introduced, every model drops in accuracy, as shown in Figure 2. We find that gpt-4 demonstrates a worst-case accuracy drop in response to falseconsensus biases by 14.0%, but is very resilient to confirmation bias, dropping by only 0.2%. This can be compared to gpt-3.5, with an average drop in accuracy of 37.4% across all biases, and in the worst-case, only scored 23.9% on data with false consensus biases. Overall, gpt-4 and mixtral-8x7b demonstrated the lowest reductions in accuracy from bias prompts, whereas the other models showed significant drops of 50% or more from original performance. \n\nThe bias which had the largest impact on the models was overwhelmingly the false consensus bias with a 24.9% decrease in model performance averaged across models. Frequency and recency biases closely follow with an 18.2% and 12.9% decrease, respectively. The least impactful bias was confirmation, at an average 8.1% decrease.",
                    "score": 0.29836154101934076,
                    "section_title": "Model evaluation",
                    "char_start_offset": 15371,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 244
                        },
                        {
                            "start": 247,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 556
                        },
                        {
                            "start": 557,
                            "end": 674
                        },
                        {
                            "start": 677,
                            "end": 940
                        },
                        {
                            "start": 941,
                            "end": 1112
                        },
                        {
                            "start": 1113,
                            "end": 1302
                        },
                        {
                            "start": 1305,
                            "end": 1467
                        },
                        {
                            "start": 1468,
                            "end": 1559
                        },
                        {
                            "start": 1560,
                            "end": 1631
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.381591796875
                }
            ],
            "relevance_judgement": 0.6650390625,
            "relevance_judgment_input_expanded": "# Title: Addressing cognitive bias in medical language models\n# Venue: arXiv.org\n# Authors: Samuel Schmidgall, Carl Harris, Ime Essien, Daniel Olshvang, Tawsifur Rahman, Ji Woong Kim, Rojin Ziaei, Jason Eshraghian, Peter M Abadir, Rama Chellappa\n## Abstract\nThere is increasing interest in the application large language models (LLMs) to the medical field, in part because of their impressive performance on medical exam questions. While promising, exam questions do not reflect the complexity of real patient-doctor interactions. In reality, physicians' decisions are shaped by many complex factors, such as patient compliance, personal experience, ethical beliefs, and cognitive bias. Taking a step toward understanding this, our hypothesis posits that when LLMs are confronted with clinical questions containing cognitive biases, they will yield significantly less accurate responses compared to the same questions presented without such biases. In this study, we developed BiasMedQA, a benchmark for evaluating cognitive biases in LLMs applied to medical tasks. Using BiasMedQA we evaluated six LLMs, namely GPT-4, Mixtral-8x70B, GPT-3.5, PaLM-2, Llama 2 70B-chat, and the medically specialized PMC Llama 13B. We tested these models on 1,273 questions from the US Medical Licensing Exam (USMLE) Steps 1, 2, and 3, modified to replicate common clinically-relevant cognitive biases. Our analysis revealed varying effects for biases on these LLMs, with GPT-4 standing out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which were disproportionately affected by cognitive bias. Our findings highlight the critical need for bias mitigation in the development of medical LLMs, pointing towards safer and more reliable applications in healthcare.\n## Model evaluation\nTo understand the effect of common cognitive biases on medical models, we first evaluate the accuracy of each model with and without bias prompts on questions from the MedQA dataset. We then introduce three novel strategies for bias mitigation. \n\nWithout bias, we report the mean accuracy of each model across the USMLE test questions in Table 1. We find gpt-4 has significantly higher performance than all other models at 72.7% accuracy, compared with the second and third best models, mixtral-8x7b and gpt-3.5, with 51.8% and 49.7% accuracy respectively. Interestingly, the most medically relevant model, pmc-llama-13b, has the lowest performance of all models with 33.4%. \n\nOnce the bias prompts are introduced, every model drops in accuracy, as shown in Figure 2. We find that gpt-4 demonstrates a worst-case accuracy drop in response to falseconsensus biases by 14.0%, but is very resilient to confirmation bias, dropping by only 0.2%. This can be compared to gpt-3.5, with an average drop in accuracy of 37.4% across all biases, and in the worst-case, only scored 23.9% on data with false consensus biases. Overall, gpt-4 and mixtral-8x7b demonstrated the lowest reductions in accuracy from bias prompts, whereas the other models showed significant drops of 50% or more from original performance. \n\nThe bias which had the largest impact on the models was overwhelmingly the false consensus bias with a 24.9% decrease in model performance averaged across models. Frequency and recency biases closely follow with an 18.2% and 12.9% decrease, respectively. The least impactful bias was confirmation, at an average 8.1% decrease.\n\n## Bias mitigation strategies\nWe demonstrate the results of three mitigation strategies: (1) bias education, (2) one-shot bias demonstration, and (3) fewshot bias demonstration (see Appendix B for details). For bias education, the model is provided with a short warning educating the model about potential cognitive biases, such as the following text provided for recency bias: \"Keep in mind the importance of individualized patient evaluation. Each patient is unique, and recent cases should not overshadow individual assessment and evidence-based practice.\" \n\nOne-shot bias demonstration includes a sample question from the MedQA dataset accompanied by a bias-inducing prompt. It also presents an example response that incorrectly selects an answer based on the bias from the prompt, which we refer to as a negative example. Before this incorrect answer, the model is presented with: \"The following is an example of incorrectly classifying based on [cognitive bias].\" \n\nFor the few-shot bias demonstration strategy, both a negative and a positive example are provided as part of the prompt. The negative example is the same as was shown in the oneshot bias demonstration, and the positive example is presented as follows: \"The following is an example of correctly classifying based on [cognitive bias],\" together with a correct classification. \n\nThe results of each bias mitigation strategy are presented in Tables 2-4 and graphically depicted in Figure 3. In comparing these three strategies, it is evident that different models respond differently to various mitigation techniques. gpt-4 consistently shows the highest level of improvement across all strategies. The other models, while showing some level of improvement, do not match gpt-4. This suggests that the architecture and training of gpt-4 might be more robust to bias-related feedback. \n\nBias education: The strategy of educating models about cognitive biases yielded the most significant improvements in gpt-4. For instance, in the \"Frequency\" bias category, its accuracy improved from 0.627 to 0.720. However, other models like mixtral-8x7b and gpt-3.5 displayed only marginal improvements.",
            "reference_string": "[267637243 | Schmidgall et al. | 2024 | Citations: 26]"
        },
        {
            "title": "Coding Inequity: Assessing GPT-4's Potential for Perpetuating Racial and Gender Biases in Healthcare",
            "venue": "medRxiv",
            "year": 2023,
            "reference_count": 6,
            "citation_count": 27,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.medrxiv.org/content/medrxiv/early/2023/07/17/2023.07.13.23292577.full.pdf",
                "status": "GREEN",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2023.07.13.23292577?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2023.07.13.23292577, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2068890474",
                    "name": "T. Zack"
                },
                {
                    "authorId": "51172373",
                    "name": "Eric P. Lehman"
                },
                {
                    "authorId": "51903517",
                    "name": "Mirac Suzgun"
                },
                {
                    "authorId": "143603572",
                    "name": "J. A. Rodriguez"
                },
                {
                    "authorId": "143605744",
                    "name": "L. Celi"
                },
                {
                    "authorId": "2221827815",
                    "name": "J. Gichoya"
                },
                {
                    "authorId": "144138733",
                    "name": "D. Jurafsky"
                },
                {
                    "authorId": "47288457",
                    "name": "P. Szolovits"
                },
                {
                    "authorId": "1739447",
                    "name": "D. Bates"
                },
                {
                    "authorId": "2223546402",
                    "name": "E. Raja-Elie"
                },
                {
                    "authorId": "2223546413",
                    "name": "Abdulnour"
                },
                {
                    "authorId": "1716151",
                    "name": "A. Butte"
                },
                {
                    "authorId": "6003726",
                    "name": "Emily Alsentzer"
                }
            ],
            "abstract": "Background. Large language models (LLMs) such as GPT-4 hold great promise as transformative tools in healthcare, ranging from automating administrative tasks to augmenting clinical decision-making. However, these models also pose a serious danger of perpetuating biases and delivering incorrect medical diagnoses, which can have a direct, harmful impact on medical care. Methods. Using the Azure OpenAI API, we tested whether GPT-4 encodes racial and gender biases and examined the impact of such biases on four potential applications of LLMs in the clinical domain---namely, medical education, diagnostic reasoning, plan generation, and patient assessment. We conducted experiments with prompts designed to resemble typical use of GPT-4 within clinical and medical education applications. We used clinical vignettes from NEJM Healer and from published research on implicit bias in healthcare. GPT-4 estimates of the demographic distribution of medical conditions were compared to true U.S. prevalence estimates. Differential diagnosis and treatment planning were evaluated across demographic groups using standard statistical tests for significance between groups. Findings. We find that GPT-4 does not appropriately model the demographic diversity of medical conditions, consistently producing clinical vignettes that stereotype demographic presentations. The differential diagnoses created by GPT-4 for standardized clinical vignettes were more likely to include diagnoses that stereotype certain races, ethnicities, and gender identities. Assessment and plans created by the model showed significant association between demographic attributes and recommendations for more expensive procedures as well as differences in patient perception. Interpretation. Our findings highlight the urgent need for comprehensive and transparent bias assessments of LLM tools like GPT-4 for every intended use case before they are integrated into clinical care. We discuss the potential sources of these biases and potential mitigation strategies prior to clinical implementation.",
            "corpus_id": 259923494,
            "sentences": [
                {
                    "corpus_id": "259923494",
                    "title": "Coding Inequity: Assessing GPT-4's Potential for Perpetuating Racial and Gender Biases in Healthcare",
                    "text": "Background. Large language models (LLMs) such as GPT-4 hold great promise as transformative tools in healthcare, ranging from automating administrative tasks to augmenting clinical decision-making. However, these models also pose a serious danger of perpetuating biases and delivering incorrect medical diagnoses, which can have a direct, harmful impact on medical care. Methods. Using the Azure OpenAI API, we tested whether GPT-4 encodes racial and gender biases and examined the impact of such biases on four potential applications of LLMs in the clinical domain---namely, medical education, diagnostic reasoning, plan generation, and patient assessment. We conducted experiments with prompts designed to resemble typical use of GPT-4 within clinical and medical education applications. We used clinical vignettes from NEJM Healer and from published research on implicit bias in healthcare. GPT-4 estimates of the demographic distribution of medical conditions were compared to true U.S. prevalence estimates. Differential diagnosis and treatment planning were evaluated across demographic groups using standard statistical tests for significance between groups. Findings. We find that GPT-4 does not appropriately model the demographic diversity of medical conditions, consistently producing clinical vignettes that stereotype demographic presentations. The differential diagnoses created by GPT-4 for standardized clinical vignettes were more likely to include diagnoses that stereotype certain races, ethnicities, and gender identities. Assessment and plans created by the model showed significant association between demographic attributes and recommendations for more expensive procedures as well as differences in patient perception. Interpretation. Our findings highlight the urgent need for comprehensive and transparent bias assessments of LLM tools like GPT-4 for every intended use case before they are integrated into clinical care. We discuss the potential sources of these biases and potential mitigation strategies prior to clinical implementation.",
                    "score": 0.3053920944449816,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.662109375
                }
            ],
            "relevance_judgement": 0.662109375,
            "relevance_judgment_input_expanded": "# Title: Coding Inequity: Assessing GPT-4's Potential for Perpetuating Racial and Gender Biases in Healthcare\n# Venue: medRxiv\n# Authors: T. Zack, Eric P. Lehman, Mirac Suzgun, J. A. Rodriguez, L. Celi, J. Gichoya, D. Jurafsky, P. Szolovits, D. Bates, E. Raja-Elie, Abdulnour, A. Butte, Emily Alsentzer\n## Abstract\nBackground. Large language models (LLMs) such as GPT-4 hold great promise as transformative tools in healthcare, ranging from automating administrative tasks to augmenting clinical decision-making. However, these models also pose a serious danger of perpetuating biases and delivering incorrect medical diagnoses, which can have a direct, harmful impact on medical care. Methods. Using the Azure OpenAI API, we tested whether GPT-4 encodes racial and gender biases and examined the impact of such biases on four potential applications of LLMs in the clinical domain---namely, medical education, diagnostic reasoning, plan generation, and patient assessment. We conducted experiments with prompts designed to resemble typical use of GPT-4 within clinical and medical education applications. We used clinical vignettes from NEJM Healer and from published research on implicit bias in healthcare. GPT-4 estimates of the demographic distribution of medical conditions were compared to true U.S. prevalence estimates. Differential diagnosis and treatment planning were evaluated across demographic groups using standard statistical tests for significance between groups. Findings. We find that GPT-4 does not appropriately model the demographic diversity of medical conditions, consistently producing clinical vignettes that stereotype demographic presentations. The differential diagnoses created by GPT-4 for standardized clinical vignettes were more likely to include diagnoses that stereotype certain races, ethnicities, and gender identities. Assessment and plans created by the model showed significant association between demographic attributes and recommendations for more expensive procedures as well as differences in patient perception. Interpretation. Our findings highlight the urgent need for comprehensive and transparent bias assessments of LLM tools like GPT-4 for every intended use case before they are integrated into clinical care. We discuss the potential sources of these biases and potential mitigation strategies prior to clinical implementation.\n",
            "reference_string": "[259923494 | Zack et al. | 2023 | Citations: 27]"
        },
        {
            "title": "Large language models in critical care",
            "venue": "Journal of Intensive Medicine",
            "year": 2024,
            "reference_count": 32,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11997603, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2244307473",
                    "name": "Laurens Biesheuvel"
                },
                {
                    "authorId": "15498846",
                    "name": "J. D. Workum"
                },
                {
                    "authorId": "15749622",
                    "name": "M. Reuland"
                },
                {
                    "authorId": "5917435",
                    "name": "M. V. van Genderen"
                },
                {
                    "authorId": "80032381",
                    "name": "P. Thoral"
                },
                {
                    "authorId": "2255202117",
                    "name": "Dave A Dongelmans"
                },
                {
                    "authorId": "2180549143",
                    "name": "Paul Elbers"
                }
            ],
            "abstract": null,
            "corpus_id": 275027597,
            "sentences": [
                {
                    "corpus_id": "275027597",
                    "title": "Large language models in critical care",
                    "text": "These are the result of imbalances or existing biases in the training data that the model learns from. [ 30 ] For models such as GPT-4 that have not fully disclosed model characteristics, the extent of imbalance is difficult to assess due to a lack of transparency about training data. [ 31 ] A study by Zack et al. [ 30 ] evaluated racial or gender biases in healthcare for the GPT-4 model. They found that the LLM propagated or even amplified societal biases. When GPT-4 was prompted to produce clinical vignettes, they found that the LLM consistently stereotyped demographic presentations for a multitude of diseases. Also, when generating differential diagnoses, it included diagnoses that reflected stereotypes associated with specific ethnicities and genders. Furthermore, their results showed that there was an association between demographic characteristics and recommendations for relatively expensive procedures. These results indicate that using these models for CDS could lead to inequities in care and potentially skew clinical judgment, ultimately posing significant risks to patient safety. For the clinician, caution and careful interpretation of outputs are advised. However, as these produced biases may not be immediately evident to the clinician, adequate oversight and mitigation strategies are crucial. These could include correcting for bias during the training process, fine-tuning models with representative clinical data, incorporating bias detection mechanisms, and conducting ongoing bias audits on model outputs. \n\nAdditionally, as a requirement in Article 4 of the EU Artificial Intelligence Act [ 32 ] for implementation in practice, it is mandated that those involved in the application and use of AI systems in healthcare -such as clinicians -are sufficiently trained in AI (AI literacy) to ensure safe and effective use. Training must be aligned with the technical expertise, experience, and clinical environments of healthcare professionals. Specific training programs in AI for healthcare providers could help them use LLMs responsibly, understand their limits, and follow best practices for safe use. \n\nThere are many other significant caveats to consider, including the potential generation of harmful content, [ 31 ] privacy concerns, EHR integration challenges, cost of resources, environmental footprint, and regulatory standards. Due to these challenges, a level of caution is required when utilizing LLMs.",
                    "score": 0.30019816601458094,
                    "section_title": "Limitations and Considerations",
                    "char_start_offset": 16692,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 109
                        },
                        {
                            "start": 110,
                            "end": 292
                        },
                        {
                            "start": 293,
                            "end": 391
                        },
                        {
                            "start": 392,
                            "end": 461
                        },
                        {
                            "start": 462,
                            "end": 620
                        },
                        {
                            "start": 621,
                            "end": 765
                        },
                        {
                            "start": 766,
                            "end": 922
                        },
                        {
                            "start": 923,
                            "end": 1105
                        },
                        {
                            "start": 1106,
                            "end": 1183
                        },
                        {
                            "start": 1184,
                            "end": 1324
                        },
                        {
                            "start": 1325,
                            "end": 1541
                        },
                        {
                            "start": 1544,
                            "end": 1854
                        },
                        {
                            "start": 1855,
                            "end": 1976
                        },
                        {
                            "start": 1977,
                            "end": 2137
                        },
                        {
                            "start": 2140,
                            "end": 2371
                        },
                        {
                            "start": 2372,
                            "end": 2448
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 103,
                            "end": 109,
                            "matchedPaperCorpusId": "266365557"
                        },
                        {
                            "start": 286,
                            "end": 292,
                            "matchedPaperCorpusId": "267039006"
                        },
                        {
                            "start": 316,
                            "end": 322,
                            "matchedPaperCorpusId": "266365557"
                        },
                        {
                            "start": 2249,
                            "end": 2255,
                            "matchedPaperCorpusId": "267039006"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6533203125
                }
            ],
            "relevance_judgement": 0.6533203125,
            "relevance_judgment_input_expanded": "# Title: Large language models in critical care\n# Venue: Journal of Intensive Medicine\n# Authors: Laurens Biesheuvel, J. D. Workum, M. Reuland, M. V. van Genderen, P. Thoral, Dave A Dongelmans, Paul Elbers\n## Abstract\nNone\n## Limitations and Considerations\nThese are the result of imbalances or existing biases in the training data that the model learns from. [ 30 ] For models such as GPT-4 that have not fully disclosed model characteristics, the extent of imbalance is difficult to assess due to a lack of transparency about training data. [ 31 ] A study by Zack et al. [ 30 ] evaluated racial or gender biases in healthcare for the GPT-4 model. They found that the LLM propagated or even amplified societal biases. When GPT-4 was prompted to produce clinical vignettes, they found that the LLM consistently stereotyped demographic presentations for a multitude of diseases. Also, when generating differential diagnoses, it included diagnoses that reflected stereotypes associated with specific ethnicities and genders. Furthermore, their results showed that there was an association between demographic characteristics and recommendations for relatively expensive procedures. These results indicate that using these models for CDS could lead to inequities in care and potentially skew clinical judgment, ultimately posing significant risks to patient safety. For the clinician, caution and careful interpretation of outputs are advised. However, as these produced biases may not be immediately evident to the clinician, adequate oversight and mitigation strategies are crucial. These could include correcting for bias during the training process, fine-tuning models with representative clinical data, incorporating bias detection mechanisms, and conducting ongoing bias audits on model outputs. \n\nAdditionally, as a requirement in Article 4 of the EU Artificial Intelligence Act [ 32 ] for implementation in practice, it is mandated that those involved in the application and use of AI systems in healthcare -such as clinicians -are sufficiently trained in AI (AI literacy) to ensure safe and effective use. Training must be aligned with the technical expertise, experience, and clinical environments of healthcare professionals. Specific training programs in AI for healthcare providers could help them use LLMs responsibly, understand their limits, and follow best practices for safe use. \n\nThere are many other significant caveats to consider, including the potential generation of harmful content, [ 31 ] privacy concerns, EHR integration challenges, cost of resources, environmental footprint, and regulatory standards. Due to these challenges, a level of caution is required when utilizing LLMs.",
            "reference_string": "[275027597 | Biesheuvel et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 14,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02584, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3391272",
                    "name": "Ziwei Ji"
                },
                {
                    "authorId": "2287917733",
                    "name": "Tiezheng Yu"
                },
                {
                    "authorId": "2285265406",
                    "name": "Yan Xu"
                },
                {
                    "authorId": "2314592345",
                    "name": "Nayeon Lee"
                },
                {
                    "authorId": "2278435713",
                    "name": "Albert Q. Jiang"
                },
                {
                    "authorId": "2256994781",
                    "name": "Alexandre Sablayrolles"
                },
                {
                    "authorId": "2319226386",
                    "name": "Arthur Men-655"
                },
                {
                    "authorId": "2256994975",
                    "name": "Chris Bamford"
                },
                {
                    "authorId": "2302815701",
                    "name": "Devendra Singh"
                },
                {
                    "authorId": "2302809975",
                    "name": "Diego Chaplot"
                },
                {
                    "authorId": "2318358390",
                    "name": "laume Lample"
                },
                {
                    "authorId": "2318350171",
                    "name": "L\u00e9lio Lucile Saulnier"
                },
                {
                    "authorId": "2318358843",
                    "name": "Renard Lavaud"
                },
                {
                    "authorId": "114952298",
                    "name": "M. Lachaux"
                },
                {
                    "authorId": "2256994779",
                    "name": "Pierre Stock"
                },
                {
                    "authorId": "1379806208",
                    "name": "Teven Le Scao"
                },
                {
                    "authorId": "2319259930",
                    "name": "Jerry Kang"
                },
                {
                    "authorId": "2319227308",
                    "name": "Mark W. Bennett"
                },
                {
                    "authorId": "2295905607",
                    "name": "Devon Carbado"
                },
                {
                    "authorId": "2319226400",
                    "name": "Pam Casey"
                },
                {
                    "authorId": "2310645453",
                    "name": "P. Liang"
                },
                {
                    "authorId": "2115397918",
                    "name": "Chiyu Wu"
                },
                {
                    "authorId": "49933077",
                    "name": "Louis-philippe Morency"
                },
                {
                    "authorId": "21626987",
                    "name": "Aman Madaan"
                },
                {
                    "authorId": "2261389843",
                    "name": "Niket Tandon"
                },
                {
                    "authorId": "2302821008",
                    "name": "Prakhar Gupta"
                },
                {
                    "authorId": null,
                    "name": "Skyler Hallinan"
                },
                {
                    "authorId": "2267242298",
                    "name": "Luyu Gao"
                },
                {
                    "authorId": "35823986",
                    "name": "Sarah Wiegreffe"
                },
                {
                    "authorId": "2268672727",
                    "name": "Uri Alon"
                },
                {
                    "authorId": "46217681",
                    "name": "Nouha Dziri"
                },
                {
                    "authorId": "9358910",
                    "name": "Shrimai Prabhumoye"
                },
                {
                    "authorId": "2315571964",
                    "name": "Yiming Yang"
                },
                {
                    "authorId": "2302819906",
                    "name": "Shashank Gupta"
                },
                {
                    "authorId": "3165738",
                    "name": "Bodhisattwa Prasad Majumder"
                },
                {
                    "authorId": "2273674137",
                    "name": "Katherine Hermann"
                },
                {
                    "authorId": "2129663",
                    "name": "S. Welleck"
                },
                {
                    "authorId": "80489277",
                    "name": "Amir Yazdan Bakhsh"
                },
                {
                    "authorId": "2319225091",
                    "name": "ing Bao"
                },
                {
                    "authorId": "2275251620",
                    "name": "Mo Bavarian"
                },
                {
                    "authorId": "2275245092",
                    "name": "J. Belgum"
                },
                {
                    "authorId": "2309476543",
                    "name": "Ir-wan Bello"
                },
                {
                    "authorId": "2275245414",
                    "name": "Jake Berdine"
                },
                {
                    "authorId": "2275245581",
                    "name": "Gabriel Bernadett-Shapiro"
                },
                {
                    "authorId": "133740015",
                    "name": "Christopher Berner"
                },
                {
                    "authorId": "2275251674",
                    "name": "Lenny Bogdonoff"
                },
                {
                    "authorId": "2275246071",
                    "name": "Oleg Boiko"
                },
                {
                    "authorId": "2275248137",
                    "name": "Made-laine Boyd"
                },
                {
                    "authorId": "2275245419",
                    "name": "Anna-Luisa Brakman"
                },
                {
                    "authorId": "2319225043",
                    "name": "Greg Brock-724 man"
                },
                {
                    "authorId": "2275219628",
                    "name": "Tim Brooks"
                },
                {
                    "authorId": "2265097787",
                    "name": "M. Brundage"
                },
                {
                    "authorId": "2146257251",
                    "name": "Kevin Button"
                },
                {
                    "authorId": "2275157286",
                    "name": "Trevor Cai"
                },
                {
                    "authorId": "2274782053",
                    "name": "Rosie Campbell"
                },
                {
                    "authorId": "2275245404",
                    "name": "Andrew Cann"
                },
                {
                    "authorId": "2275246368",
                    "name": "Brittany Carey"
                },
                {
                    "authorId": "2275120298",
                    "name": "Chelsea Carlson"
                },
                {
                    "authorId": "144114446",
                    "name": "Rory Carmichael"
                },
                {
                    "authorId": "1466431052",
                    "name": "Brooke Chan"
                },
                {
                    "authorId": "2275545855",
                    "name": "Che Chang"
                },
                {
                    "authorId": "2057091285",
                    "name": "Fotis Chantzis"
                },
                {
                    "authorId": "2253841704",
                    "name": "Derek Chen"
                },
                {
                    "authorId": "2256808607",
                    "name": "Su-Hong Chen"
                },
                {
                    "authorId": "2275179180",
                    "name": "Ruby Chen"
                },
                {
                    "authorId": "2275289833",
                    "name": "Jason Chen"
                },
                {
                    "authorId": "2108828435",
                    "name": "Mark Chen"
                },
                {
                    "authorId": "1490681878",
                    "name": "Benjamin Chess"
                },
                {
                    "authorId": "2275251158",
                    "name": "Chester Cho"
                },
                {
                    "authorId": "2309475703",
                    "name": "Hyung Casey Chu"
                },
                {
                    "authorId": "2282528643",
                    "name": "Won Chung"
                },
                {
                    "authorId": "2275231534",
                    "name": "Dave Cummings"
                },
                {
                    "authorId": "49645091",
                    "name": "Jeremiah Currier"
                },
                {
                    "authorId": "2276187456",
                    "name": "Yunxing Dai"
                },
                {
                    "authorId": "2309477435",
                    "name": "Tarun Goel"
                },
                {
                    "authorId": "2309477420",
                    "name": "Gabriel Gogineni"
                },
                {
                    "authorId": "2309475753",
                    "name": "Rapha Goh"
                },
                {
                    "authorId": "2319225953",
                    "name": "Jonathan Gontijo-738 Lopes"
                },
                {
                    "authorId": "2309478880",
                    "name": "Morgan Gordon"
                },
                {
                    "authorId": "2309480960",
                    "name": "Scott Grafstein"
                },
                {
                    "authorId": "2309478956",
                    "name": "Ryan Gray"
                },
                {
                    "authorId": "2309478103",
                    "name": "Joshua Greene"
                },
                {
                    "authorId": "2309475937",
                    "name": "Shixiang Shane Gross"
                },
                {
                    "authorId": "2309896895",
                    "name": "Yufei Gu"
                },
                {
                    "authorId": "2309804592",
                    "name": "Chris Guo"
                },
                {
                    "authorId": "2309477491",
                    "name": "Jesse Hallacy"
                },
                {
                    "authorId": "2309667621",
                    "name": "Jeff Han"
                },
                {
                    "authorId": "2309475604",
                    "name": "Harris Yuchen"
                },
                {
                    "authorId": "2310401628",
                    "name": "Mike He"
                },
                {
                    "authorId": "2309477353",
                    "name": "Johannes Heaton"
                },
                {
                    "authorId": "2309476458",
                    "name": "C. Heidecke"
                },
                {
                    "authorId": "2309475602",
                    "name": "Alan Hesse"
                },
                {
                    "authorId": "2275246148",
                    "name": "W. Hickey"
                },
                {
                    "authorId": "2309477265",
                    "name": "Peter Hickey"
                },
                {
                    "authorId": "2309477475",
                    "name": "Hoeschele Brandon"
                },
                {
                    "authorId": "2309480952",
                    "name": "Kenny Houghton"
                },
                {
                    "authorId": "2309479202",
                    "name": "Shengli Hsu"
                },
                {
                    "authorId": "2275777049",
                    "name": "Xin Hu"
                },
                {
                    "authorId": "2309663799",
                    "name": "Joost Hu"
                },
                {
                    "authorId": "2309477471",
                    "name": "Shantanu Huizinga"
                },
                {
                    "authorId": "2309900251",
                    "name": "Shawn Jain"
                },
                {
                    "authorId": "2309475571",
                    "name": "Jain Joanne"
                },
                {
                    "authorId": "2309477467",
                    "name": "Angela Jang"
                },
                {
                    "authorId": "2275172062",
                    "name": "Roger Jiang"
                },
                {
                    "authorId": "2309830920",
                    "name": "Haozhun Jiang"
                },
                {
                    "authorId": "2275203081",
                    "name": "Denny Jin"
                },
                {
                    "authorId": "2309901734",
                    "name": "Shino Jin"
                },
                {
                    "authorId": "2309481128",
                    "name": "Billie Jomoto"
                },
                {
                    "authorId": "2309480973",
                    "name": "Hee-woo Jonn"
                },
                {
                    "authorId": "2309475461",
                    "name": "Tomer Jun"
                },
                {
                    "authorId": "2309476661",
                    "name": "\u0141ukasz Kaftan"
                },
                {
                    "authorId": "2309476629",
                    "name": "Ali Kaiser"
                },
                {
                    "authorId": "2319225089",
                    "name": "Ingmar Ka-748 mali"
                },
                {
                    "authorId": "2102033721",
                    "name": "Kanitscheider"
                },
                {
                    "authorId": "2288125393",
                    "name": "Nitish Shirish"
                },
                {
                    "authorId": "2307452588",
                    "name": "Keskar Tabarak"
                },
                {
                    "authorId": "2307454011",
                    "name": "Logan Khan"
                },
                {
                    "authorId": "2307452560",
                    "name": "J. Kilpatrick"
                },
                {
                    "authorId": "2319227856",
                    "name": "Kim"
                },
                {
                    "authorId": "2149054292",
                    "name": "Christina Kim"
                },
                {
                    "authorId": "2275296777",
                    "name": "Yongjik Kim"
                },
                {
                    "authorId": "2319226271",
                    "name": "Jan Hendrik Kirch-751 ner"
                },
                {
                    "authorId": "51131802",
                    "name": "J. Kiros"
                },
                {
                    "authorId": "2146257375",
                    "name": "Matthew Knight"
                },
                {
                    "authorId": "1485556711",
                    "name": "Daniel Kokotajlo"
                },
                {
                    "authorId": "2319226462",
                    "name": "\u0141ukasz Kondraciuk"
                },
                {
                    "authorId": "1666171360",
                    "name": "Andrew Kondrich"
                },
                {
                    "authorId": "2319225191",
                    "name": "Aris Kon-753 stantinidis"
                },
                {
                    "authorId": "2275245594",
                    "name": "Kyle Kosic"
                },
                {
                    "authorId": "2064404342",
                    "name": "Gretchen Krueger"
                },
                {
                    "authorId": "2275229877",
                    "name": "Vishal Kuo"
                },
                {
                    "authorId": "2275247085",
                    "name": "Michael Lampe"
                },
                {
                    "authorId": "2275246287",
                    "name": "Ikai Lan"
                },
                {
                    "authorId": "2274915115",
                    "name": "Teddy Lee"
                },
                {
                    "authorId": "2990741",
                    "name": "Jan Leike"
                },
                {
                    "authorId": "52152632",
                    "name": "Jade Leung"
                },
                {
                    "authorId": "2319225189",
                    "name": "Chak Daniel Levy"
                },
                {
                    "authorId": "2319250141",
                    "name": "Ming Li"
                },
                {
                    "authorId": "2275176375",
                    "name": "Rachel Lim"
                },
                {
                    "authorId": "2275759230",
                    "name": "Molly Lin"
                },
                {
                    "authorId": "2253840098",
                    "name": "Stephanie Lin"
                },
                {
                    "authorId": "1380985420",
                    "name": "Ma-teusz Litwin"
                },
                {
                    "authorId": "2275248327",
                    "name": "Theresa Lopez"
                },
                {
                    "authorId": "2257272397",
                    "name": "Ryan Lowe"
                },
                {
                    "authorId": "2275245628",
                    "name": "Patricia Lue"
                },
                {
                    "authorId": "119341078",
                    "name": "A. Makanju"
                },
                {
                    "authorId": "2275245649",
                    "name": "Kim Malfacini"
                },
                {
                    "authorId": "46430291",
                    "name": "Sam Manning"
                },
                {
                    "authorId": "14113256",
                    "name": "Todor Markov"
                },
                {
                    "authorId": "2275245336",
                    "name": "Yaniv Markovski"
                },
                {
                    "authorId": "2114362965",
                    "name": "Bianca Martin"
                },
                {
                    "authorId": "2275231822",
                    "name": "Katie Mayer"
                },
                {
                    "authorId": "2275247045",
                    "name": "Andrew Mayne"
                },
                {
                    "authorId": "39593364",
                    "name": "Bob McGrew"
                },
                {
                    "authorId": "2047820455",
                    "name": "S. McKinney"
                },
                {
                    "authorId": "3028785",
                    "name": "Christine McLeavey"
                },
                {
                    "authorId": "2274772421",
                    "name": "Paul McMillan"
                },
                {
                    "authorId": "2275234856",
                    "name": "Jake McNeil"
                },
                {
                    "authorId": "2275210659",
                    "name": "David Medina"
                },
                {
                    "authorId": "2275132306",
                    "name": "Aalok Mehta"
                },
                {
                    "authorId": "10698483",
                    "name": "Jacob Menick"
                },
                {
                    "authorId": "2275246330",
                    "name": "Luke Metz"
                },
                {
                    "authorId": "2275252694",
                    "name": "An-drey Mishchenko"
                },
                {
                    "authorId": "2051714782",
                    "name": "Pamela Mishkin"
                },
                {
                    "authorId": "2275245453",
                    "name": "Vinnie Monaco"
                },
                {
                    "authorId": "1404556973",
                    "name": "Evan Morikawa"
                },
                {
                    "authorId": "3407880",
                    "name": "Daniel P. Mossing"
                },
                {
                    "authorId": "2319225702",
                    "name": "Tong Mu"
                },
                {
                    "authorId": "2117715631",
                    "name": "Mira Murati"
                },
                {
                    "authorId": "147746767",
                    "name": "O. Murk"
                },
                {
                    "authorId": "2319226404",
                    "name": "David M\u00e9ly"
                },
                {
                    "authorId": "2319226971",
                    "name": "Ashvin Nair"
                },
                {
                    "authorId": "7406311",
                    "name": "Reiichiro Nakano"
                },
                {
                    "authorId": "2057426488",
                    "name": "Rajeev Nayak"
                },
                {
                    "authorId": "2072676",
                    "name": "Arvind Neelakantan"
                },
                {
                    "authorId": "2273886618",
                    "name": "Richard Ngo"
                },
                {
                    "authorId": "2275115983",
                    "name": "Hyeonwoo Noh"
                },
                {
                    "authorId": "2228518120",
                    "name": "Ouyang Long"
                },
                {
                    "authorId": "1435765036",
                    "name": "Cullen O'Keefe"
                },
                {
                    "authorId": "2713380",
                    "name": "J. Pachocki"
                },
                {
                    "authorId": "34800652",
                    "name": "A. Paino"
                },
                {
                    "authorId": "2275244652",
                    "name": "Joe Palermo"
                },
                {
                    "authorId": "2275246178",
                    "name": "Ashley Pantuliano"
                },
                {
                    "authorId": "2275207240",
                    "name": "Carl Ross"
                },
                {
                    "authorId": "11150265",
                    "name": "Bob Rotsted"
                },
                {
                    "authorId": "2275250007",
                    "name": "Henri Roussez"
                },
                {
                    "authorId": "2319225618",
                    "name": "Nick Ry-779 der"
                },
                {
                    "authorId": "2252840300",
                    "name": "Mario D. Saltarelli"
                },
                {
                    "authorId": "2275246803",
                    "name": "Ted Sanders"
                },
                {
                    "authorId": "2852106",
                    "name": "Shibani Santurkar"
                },
                {
                    "authorId": "144864359",
                    "name": "Girish Sastry"
                },
                {
                    "authorId": "2275265666",
                    "name": "Heather Schmidt"
                },
                {
                    "authorId": "2252874293",
                    "name": "David Schnurr"
                },
                {
                    "authorId": "2297873691",
                    "name": "John Schulman"
                },
                {
                    "authorId": "2196579",
                    "name": "Daniel Selsam"
                },
                {
                    "authorId": "2275244711",
                    "name": "Kyla Sheppard"
                },
                {
                    "authorId": "102475503",
                    "name": "Toki Sherbakov"
                },
                {
                    "authorId": "2275246834",
                    "name": "Jessica Shieh"
                },
                {
                    "authorId": "118335789",
                    "name": "Sarah Shoker"
                },
                {
                    "authorId": "67311962",
                    "name": "Pranav Shyam"
                },
                {
                    "authorId": "2700360",
                    "name": "Szymon Sidor"
                },
                {
                    "authorId": "2064673055",
                    "name": "Eric Sigler"
                },
                {
                    "authorId": "2151735251",
                    "name": "Maddie Simens"
                },
                {
                    "authorId": "2275252299",
                    "name": "Jordan Sitkin"
                },
                {
                    "authorId": "2117680841",
                    "name": "Katarina Slama"
                },
                {
                    "authorId": "103422608",
                    "name": "Ian Sohl"
                },
                {
                    "authorId": "2901424",
                    "name": "Benjamin Sokolowsky"
                },
                {
                    "authorId": "2307592658",
                    "name": "Yang Song"
                },
                {
                    "authorId": "2275245668",
                    "name": "Natalie Staudacher"
                },
                {
                    "authorId": "2059411355",
                    "name": "Clemens Winter"
                },
                {
                    "authorId": "2275244177",
                    "name": "Samuel Wolrich"
                },
                {
                    "authorId": "2275225207",
                    "name": "Hannah Wong"
                },
                {
                    "authorId": "2275245771",
                    "name": "Lauren Workman"
                },
                {
                    "authorId": "2275299848",
                    "name": "Sherwin Wu"
                },
                {
                    "authorId": "2274911253",
                    "name": "Jeff Wu"
                },
                {
                    "authorId": "2307456650",
                    "name": "Michael Wu"
                },
                {
                    "authorId": "2307454769",
                    "name": "Kai Xiao"
                },
                {
                    "authorId": "2275452480",
                    "name": "Tao Xu"
                },
                {
                    "authorId": "2275310096",
                    "name": "Sarah Yoo"
                },
                {
                    "authorId": "2275593618",
                    "name": "Kevin Yu"
                },
                {
                    "authorId": "2275194186",
                    "name": "Qim-ing Yuan"
                },
                {
                    "authorId": "2307452791",
                    "name": "Wojciech Zaremba"
                },
                {
                    "authorId": "49629836",
                    "name": "Rowan Zellers"
                },
                {
                    "authorId": "2315024566",
                    "name": "Chong Zhang"
                },
                {
                    "authorId": "2281037751",
                    "name": "Marvin Zhang"
                },
                {
                    "authorId": "2307453667",
                    "name": "Tianhao Shengjia Zhao"
                },
                {
                    "authorId": "2298950344",
                    "name": "Xu Jiang"
                },
                {
                    "authorId": "2275252021",
                    "name": "Diogo Almeida"
                },
                {
                    "authorId": "2275245962",
                    "name": "Carroll L. Wainwright"
                },
                {
                    "authorId": "144517868",
                    "name": "Sandhini Agarwal"
                },
                {
                    "authorId": "2319227127",
                    "name": "Alex Gray"
                },
                {
                    "authorId": "2286540856",
                    "name": "Jacob Hilton"
                },
                {
                    "authorId": "2151735262",
                    "name": "Fraser Kelton"
                },
                {
                    "authorId": "2298421583",
                    "name": "Luke Miller"
                },
                {
                    "authorId": "2220750220",
                    "name": "Amanda Askell"
                },
                {
                    "authorId": "2930640",
                    "name": "P. Welinder"
                },
                {
                    "authorId": "2261980896",
                    "name": "Paul F. Christiano"
                },
                {
                    "authorId": "2197475360",
                    "name": "Joon Sung Park"
                },
                {
                    "authorId": "2213764034",
                    "name": "Joseph C. O\u2019Brien"
                },
                {
                    "authorId": "2276794641",
                    "name": "C. Cai"
                },
                {
                    "authorId": "2319287576",
                    "name": "Ringel Morris"
                },
                {
                    "authorId": "2256995425",
                    "name": "Percy Liang"
                },
                {
                    "authorId": "2319226111",
                    "name": "Michael S. Bern-814"
                },
                {
                    "authorId": "38909097",
                    "name": "Alec Radford"
                },
                {
                    "authorId": "2285784924",
                    "name": "Karthik Narasimhan"
                },
                {
                    "authorId": "2887364",
                    "name": "Tim Salimans"
                },
                {
                    "authorId": "2302559920",
                    "name": "Rachel Rudinger"
                },
                {
                    "authorId": "2300343",
                    "name": "Jason Naradowsky"
                },
                {
                    "authorId": "2319225916",
                    "name": "Brian Leonard"
                },
                {
                    "authorId": "1387983862",
                    "name": "Nisan Stiennon"
                },
                {
                    "authorId": "2319225774",
                    "name": "Ryan Ziegler"
                },
                {
                    "authorId": "2314165049",
                    "name": "Chelsea Lowe"
                },
                {
                    "authorId": "2314160468",
                    "name": "Alec Voss"
                },
                {
                    "authorId": "2289348718",
                    "name": "Radford"
                },
                {
                    "authorId": "2698777",
                    "name": "Dario Amodei"
                },
                {
                    "authorId": "2319225540",
                    "name": "Christiano. 2020. Learn-842"
                },
                {
                    "authorId": "2319580593",
                    "name": "Tony Sun"
                },
                {
                    "authorId": "146072982",
                    "name": "Andrew Gaut"
                },
                {
                    "authorId": "148149462",
                    "name": "Shirlyn Tang"
                },
                {
                    "authorId": "2154731574",
                    "name": "Yuxin Huang"
                },
                {
                    "authorId": "2288124187",
                    "name": "Mai ElSherief"
                },
                {
                    "authorId": "2311580116",
                    "name": "Jie Zhao"
                },
                {
                    "authorId": "2319225642",
                    "name": "Diba Mirza"
                },
                {
                    "authorId": "2319226119",
                    "name": "Kai-Wei Belding"
                },
                {
                    "authorId": "2319225109",
                    "name": "Chang William"
                },
                {
                    "authorId": "2319563352",
                    "name": "Yang Wang"
                },
                {
                    "authorId": "2165227666",
                    "name": "Yixin Wan"
                },
                {
                    "authorId": "2258548444",
                    "name": "George Pu"
                },
                {
                    "authorId": "2261454711",
                    "name": "Jiao Sun"
                },
                {
                    "authorId": "31099365",
                    "name": "Aparna Garimella"
                },
                {
                    "authorId": "2257127887",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "2285475879",
                    "name": "Nanyun Peng"
                },
                {
                    "authorId": "2319226454",
                    "name": "\u201ckelly"
                }
            ],
            "abstract": "As Large Language Models (LLMs) continue to evolve, they are increasingly being employed in numerous studies to simulate societies and execute diverse social tasks. However, LLMs are susceptible to societal biases due to their exposure to human-generated data. Given that LLMs are being used to gain insights into various societal aspects, it is essential to mitigate these biases. To that end, our study investigates the presence of implicit gender biases in multi-agent LLM interactions and proposes two strategies to mitigate these biases. We begin by creating a dataset of scenarios where implicit gender biases might arise, and subsequently develop a metric to assess the presence of biases. Our empirical analysis reveals that LLMs generate outputs characterized by strong implicit bias associations (>= 50\\% of the time). Furthermore, these biases tend to escalate following multi-agent interactions. To mitigate them, we propose two strategies: self-reflection with in-context examples (ICE); and supervised fine-tuning. Our research demonstrates that both methods effectively mitigate implicit biases, with the ensemble of fine-tuning and self-reflection proving to be the most successful.",
            "corpus_id": 272337179,
            "sentences": [
                {
                    "corpus_id": "272337179",
                    "title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions",
                    "text": "In order to assess the effectiveness of our bias mitigation strategies, we conduct evaluations in three comprehensive settings: \n\n1. Understanding the presence of implicit biases: We evaluate if models can correctly identify the presence/absence of implicit biases in task assignments on the dev set of the Fine-tune dataset. Results and analysis are provided in Appendix D.1. \n\n2. Generation8 in the 'no interaction' setting: \n\nWe use the Test Dataset, which contains scenarios from domains different than the finetune data and prompt LLMs to output task assignments. Results and analysis are provided in Appendix D.2. \n\n3. Generation in the 'interaction setting': Here, multi agents interact and utilize mitigation strategies to reduce implicit biases on the Test Dataset. We discuss this further below. \n\nFigure 6 illustrates the results of mitigation approaches on the multi-agent LLM interactions. It demonstrates that the ft-gpt-35-turbo with SR + ICE yields the lowest bias score of 0.01, indicating almost neutral or no bias. All our ensembles (fine-tuning + self-reflection) have the best performances for both gpt-35-turbo and mistral-7b-instruct. Among the two approaches, fine-tuning proves more effective than self-reflection in reducing implicit biases from the outset. This is visible right from the first responses, as well as reflected in lower bias scores overall across models. It is worth noting that the fine-tune data and test data have different domains, showing the effectiveness of fine-tuning in gen-eration. The changes in bias scores after interactions, however, are minimal, for fine-tuned agents because the first responses themselves are less biased. Additionally, half-ft is more effective in mitigating biases in mistral-7b-instruct. Similarly, self-reflection mitigation effects are more pronounced for mistral-7b-instruct. \n\nWe find that gpt-4 generates negative bias scores, i.e. anti-stereotypical assignments using mitigation strategies and does not present equally representative task assignments after self-reflection. These results imply that smaller models benefit more from our mitigation strategies. Fig 18 in Appendix D.3 shows the results for the 'goal' setting, which holds most of our results as discussed above.",
                    "score": 0.32257460751059086,
                    "section_title": "Experiments and Results: Bias Mitigation",
                    "char_start_offset": 22634,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 130,
                            "end": 325
                        },
                        {
                            "start": 326,
                            "end": 376
                        },
                        {
                            "start": 379,
                            "end": 426
                        },
                        {
                            "start": 429,
                            "end": 568
                        },
                        {
                            "start": 569,
                            "end": 619
                        },
                        {
                            "start": 622,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 805
                        },
                        {
                            "start": 808,
                            "end": 902
                        },
                        {
                            "start": 903,
                            "end": 1033
                        },
                        {
                            "start": 1034,
                            "end": 1157
                        },
                        {
                            "start": 1158,
                            "end": 1283
                        },
                        {
                            "start": 1284,
                            "end": 1396
                        },
                        {
                            "start": 1397,
                            "end": 1534
                        },
                        {
                            "start": 1535,
                            "end": 1681
                        },
                        {
                            "start": 1682,
                            "end": 1766
                        },
                        {
                            "start": 1767,
                            "end": 1857
                        },
                        {
                            "start": 1860,
                            "end": 2058
                        },
                        {
                            "start": 2059,
                            "end": 2143
                        },
                        {
                            "start": 2144,
                            "end": 2260
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6328125
                },
                {
                    "corpus_id": "272337179",
                    "title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions",
                    "text": "In our mitigation experiments, we find that gpt-4 leads to negative biases after mitigation, which require further analysis. Currently proposed mitigation approaches for reducing biases in gpt-4, specifically self-reflection, have not been found to effectively address the issue. Due to the limitation of not being able to fine-tune, our evaluation is limited to self-reflection only, further emphasizing this constraint. We also plan to analyze why gpt-4 has the highest biases as well. It is also important to note that most of our data are generated by gpt-4. Therefore, it is advisable to approach the results produced by GPT-4 with a certain level of skepticism. \n\nAdditionally, our dataset is limited to 111 scenarios, also because the number of implicit bias scenarios is scarce in the literature. In the future, we plan to create a larger dataset for implicit biases and extend the scope of biases to include factors beyond gender such as religion, race, and more.",
                    "score": 0.43062769847152615,
                    "section_title": "Limitations",
                    "char_start_offset": 28975,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 124
                        },
                        {
                            "start": 125,
                            "end": 279
                        },
                        {
                            "start": 280,
                            "end": 421
                        },
                        {
                            "start": 422,
                            "end": 487
                        },
                        {
                            "start": 488,
                            "end": 562
                        },
                        {
                            "start": 563,
                            "end": 667
                        },
                        {
                            "start": 670,
                            "end": 804
                        },
                        {
                            "start": 805,
                            "end": 972
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.54443359375
                }
            ],
            "relevance_judgement": 0.6328125,
            "relevance_judgment_input_expanded": "# Title: Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-655, Chris Bamford, Devendra Singh, Diego Chaplot, laume Lample, L\u00e9lio Lucile Saulnier, Renard Lavaud, M. Lachaux, Pierre Stock, Teven Le Scao, Jerry Kang, Mark W. Bennett, Devon Carbado, Pam Casey, P. Liang, Chiyu Wu, Louis-philippe Morency, Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, S. Welleck, Amir Yazdan Bakhsh, ing Bao, Mo Bavarian, J. Belgum, Ir-wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Made-laine Boyd, Anna-Luisa Brakman, Greg Brock-724 man, Tim Brooks, M. Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Su-Hong Chen, Ruby Chen, Jason Chen, Mark Chen, Benjamin Chess, Chester Cho, Hyung Casey Chu, Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Tarun Goel, Gabriel Gogineni, Rapha Goh, Jonathan Gontijo-738 Lopes, Morgan Gordon, Scott Grafstein, Ryan Gray, Joshua Greene, Shixiang Shane Gross, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Harris Yuchen, Mike He, Johannes Heaton, C. Heidecke, Alan Hesse, W. Hickey, Peter Hickey, Hoeschele Brandon, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Jain Joanne, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Hee-woo Jonn, Tomer Jun, \u0141ukasz Kaftan, Ali Kaiser, Ingmar Ka-748 mali, Kanitscheider, Nitish Shirish, Keskar Tabarak, Logan Khan, J. Kilpatrick, Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirch-751 ner, J. Kiros, Matthew Knight, Daniel Kokotajlo, \u0141ukasz Kondraciuk, Andrew Kondrich, Aris Kon-753 stantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Chak Daniel Levy, Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Ma-teusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, A. Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, S. McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, An-drey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel P. Mossing, Tong Mu, Mira Murati, O. Murk, David M\u00e9ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Ouyang Long, Cullen O'Keefe, J. Pachocki, A. Paino, Joe Palermo, Ashley Pantuliano, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-779 der, Mario D. Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Tianhao Shengjia Zhao, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Sandhini Agarwal, Alex Gray, Jacob Hilton, Fraser Kelton, Luke Miller, Amanda Askell, P. Welinder, Paul F. Christiano, Joon Sung Park, Joseph C. O\u2019Brien, C. Cai, Ringel Morris, Percy Liang, Michael S. Bern-814, Alec Radford, Karthik Narasimhan, Tim Salimans, Rachel Rudinger, Jason Naradowsky, Brian Leonard, Nisan Stiennon, Ryan Ziegler, Chelsea Lowe, Alec Voss, Radford, Dario Amodei, Christiano. 2020. Learn-842, Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jie Zhao, Diba Mirza, Kai-Wei Belding, Chang William, Yang Wang, Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei Chang, Nanyun Peng, \u201ckelly\n## Abstract\nAs Large Language Models (LLMs) continue to evolve, they are increasingly being employed in numerous studies to simulate societies and execute diverse social tasks. However, LLMs are susceptible to societal biases due to their exposure to human-generated data. Given that LLMs are being used to gain insights into various societal aspects, it is essential to mitigate these biases. To that end, our study investigates the presence of implicit gender biases in multi-agent LLM interactions and proposes two strategies to mitigate these biases. We begin by creating a dataset of scenarios where implicit gender biases might arise, and subsequently develop a metric to assess the presence of biases. Our empirical analysis reveals that LLMs generate outputs characterized by strong implicit bias associations (>= 50\\% of the time). Furthermore, these biases tend to escalate following multi-agent interactions. To mitigate them, we propose two strategies: self-reflection with in-context examples (ICE); and supervised fine-tuning. Our research demonstrates that both methods effectively mitigate implicit biases, with the ensemble of fine-tuning and self-reflection proving to be the most successful.\n## Experiments and Results: Bias Mitigation\nIn order to assess the effectiveness of our bias mitigation strategies, we conduct evaluations in three comprehensive settings: \n\n1. Understanding the presence of implicit biases: We evaluate if models can correctly identify the presence/absence of implicit biases in task assignments on the dev set of the Fine-tune dataset. Results and analysis are provided in Appendix D.1. \n\n2. Generation8 in the 'no interaction' setting: \n\nWe use the Test Dataset, which contains scenarios from domains different than the finetune data and prompt LLMs to output task assignments. Results and analysis are provided in Appendix D.2. \n\n3. Generation in the 'interaction setting': Here, multi agents interact and utilize mitigation strategies to reduce implicit biases on the Test Dataset. We discuss this further below. \n\nFigure 6 illustrates the results of mitigation approaches on the multi-agent LLM interactions. It demonstrates that the ft-gpt-35-turbo with SR + ICE yields the lowest bias score of 0.01, indicating almost neutral or no bias. All our ensembles (fine-tuning + self-reflection) have the best performances for both gpt-35-turbo and mistral-7b-instruct. Among the two approaches, fine-tuning proves more effective than self-reflection in reducing implicit biases from the outset. This is visible right from the first responses, as well as reflected in lower bias scores overall across models. It is worth noting that the fine-tune data and test data have different domains, showing the effectiveness of fine-tuning in gen-eration. The changes in bias scores after interactions, however, are minimal, for fine-tuned agents because the first responses themselves are less biased. Additionally, half-ft is more effective in mitigating biases in mistral-7b-instruct. Similarly, self-reflection mitigation effects are more pronounced for mistral-7b-instruct. \n\nWe find that gpt-4 generates negative bias scores, i.e. anti-stereotypical assignments using mitigation strategies and does not present equally representative task assignments after self-reflection. These results imply that smaller models benefit more from our mitigation strategies. Fig 18 in Appendix D.3 shows the results for the 'goal' setting, which holds most of our results as discussed above.\n\n## Limitations\nIn our mitigation experiments, we find that gpt-4 leads to negative biases after mitigation, which require further analysis. Currently proposed mitigation approaches for reducing biases in gpt-4, specifically self-reflection, have not been found to effectively address the issue. Due to the limitation of not being able to fine-tune, our evaluation is limited to self-reflection only, further emphasizing this constraint. We also plan to analyze why gpt-4 has the highest biases as well. It is also important to note that most of our data are generated by gpt-4. Therefore, it is advisable to approach the results produced by GPT-4 with a certain level of skepticism. \n\nAdditionally, our dataset is limited to 111 scenarios, also because the number of implicit bias scenarios is scarce in the literature. In the future, we plan to create a larger dataset for implicit biases and extend the scope of biases to include factors beyond gender such as religion, race, and more.",
            "reference_string": "[272337179 | Ji et al. | 2024 | Citations: 14]"
        },
        {
            "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 127,
            "citation_count": 15,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2401.07103",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.07103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2145256331",
                    "name": "Zhen Li"
                },
                {
                    "authorId": "2279658967",
                    "name": "Xiaohan Xu"
                },
                {
                    "authorId": "2279548827",
                    "name": "Tao Shen"
                },
                {
                    "authorId": "2284826718",
                    "name": "Can Xu"
                },
                {
                    "authorId": "2308241851",
                    "name": "Jia-Chen Gu"
                },
                {
                    "authorId": "2308073132",
                    "name": "Yuxuan Lai"
                },
                {
                    "authorId": "2287928517",
                    "name": "Chongyang Tao"
                },
                {
                    "authorId": "2307142498",
                    "name": "Shuai Ma"
                }
            ],
            "abstract": "In the rapidly evolving domain of Natural Language Generation (NLG) evaluation, introducing Large Language Models (LLMs) has opened new avenues for assessing generated content quality, e.g., coherence, creativity, and context relevance. This paper aims to provide a thorough overview of leveraging LLMs for NLG evaluation, a burgeoning area that lacks a systematic analysis. We propose a coherent taxonomy for organizing existing LLM-based evaluation metrics, offering a structured framework to understand and compare these methods. Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this paper seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques.",
            "corpus_id": 270391675,
            "sentences": [
                {
                    "corpus_id": "270391675",
                    "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
                    "text": "LLM-based evaluators frequently utilize GPT-4 (Liu et al., 2023c;Xu et al., 2023;Zheng et al., 2023), owing to its status as one of the most advanced LLM (OpenAI, 2023).However, relying on GPT-4 for evaluation might lead to biased or skewed results, especially when evaluating outputs generated by itself or an equally powerful model (Bai et al., 2023;Zheng et al., 2023).The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator).This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023).This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022;Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023;Ouyang et al., 2022) to ensure more balanced and comprehensive assessments.\n\nDomain-Specific Evaluation.LLMs have been prevalent across various domains, such as law (Cui et al., 2023a), medicine (Singhal et al., 2023), finance (Yang et al., 2023a), etc.However, most LLMs employed as evaluators are designed for general domains and are not specifically tailored to any particular field.This lack of specialization poses significant challenges.On one hand, these LLMs often lack the requisite domain-specific knowledge, making it difficult for them to accurately assess the correctness of content within specialized fields.On the other hand, the evaluation prompts need to be meticulously designed for different domains.This may involve tailoring the aspects of evaluation relevant to each field.",
                    "score": 0.3161033143509453,
                    "section_title": "Challenges and Open Problems",
                    "char_start_offset": 42121,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 169
                        },
                        {
                            "start": 169,
                            "end": 372
                        },
                        {
                            "start": 372,
                            "end": 543
                        },
                        {
                            "start": 543,
                            "end": 692
                        },
                        {
                            "start": 692,
                            "end": 877
                        },
                        {
                            "start": 877,
                            "end": 1185
                        },
                        {
                            "start": 1187,
                            "end": 1214
                        },
                        {
                            "start": 1214,
                            "end": 1363
                        },
                        {
                            "start": 1363,
                            "end": 1496
                        },
                        {
                            "start": 1496,
                            "end": 1553
                        },
                        {
                            "start": 1553,
                            "end": 1732
                        },
                        {
                            "start": 1732,
                            "end": 1829
                        },
                        {
                            "start": 1829,
                            "end": 1905
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1051,
                            "end": 1072,
                            "matchedPaperCorpusId": "215548699"
                        },
                        {
                            "start": 1110,
                            "end": 1130,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 1305,
                            "end": 1327,
                            "matchedPaperCorpusId": "255124952"
                        },
                        {
                            "start": 1337,
                            "end": 1357,
                            "matchedPaperCorpusId": "254877751"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6328125
                }
            ],
            "relevance_judgement": 0.6328125,
            "relevance_judgment_input_expanded": "# Title: Leveraging Large Language Models for NLG Evaluation: Advances and Challenges\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, Shuai Ma\n## Abstract\nIn the rapidly evolving domain of Natural Language Generation (NLG) evaluation, introducing Large Language Models (LLMs) has opened new avenues for assessing generated content quality, e.g., coherence, creativity, and context relevance. This paper aims to provide a thorough overview of leveraging LLMs for NLG evaluation, a burgeoning area that lacks a systematic analysis. We propose a coherent taxonomy for organizing existing LLM-based evaluation metrics, offering a structured framework to understand and compare these methods. Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this paper seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques.\n## Challenges and Open Problems\nLLM-based evaluators frequently utilize GPT-4 (Liu et al., 2023c;Xu et al., 2023;Zheng et al., 2023), owing to its status as one of the most advanced LLM (OpenAI, 2023).However, relying on GPT-4 for evaluation might lead to biased or skewed results, especially when evaluating outputs generated by itself or an equally powerful model (Bai et al., 2023;Zheng et al., 2023).The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator).This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023).This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022;Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023;Ouyang et al., 2022) to ensure more balanced and comprehensive assessments.\n\nDomain-Specific Evaluation.LLMs have been prevalent across various domains, such as law (Cui et al., 2023a), medicine (Singhal et al., 2023), finance (Yang et al., 2023a), etc.However, most LLMs employed as evaluators are designed for general domains and are not specifically tailored to any particular field.This lack of specialization poses significant challenges.On one hand, these LLMs often lack the requisite domain-specific knowledge, making it difficult for them to accurately assess the correctness of content within specialized fields.On the other hand, the evaluation prompts need to be meticulously designed for different domains.This may involve tailoring the aspects of evaluation relevant to each field.",
            "reference_string": "[270391675 | Li et al. | 2024 | Citations: 15]"
        },
        {
            "title": "Large Language Models are not Fair Evaluators",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 45,
            "citation_count": 573,
            "influential_citation_count": 51,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.17926",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.17926, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144202874",
                    "name": "Peiyi Wang"
                },
                {
                    "authorId": "49192881",
                    "name": "Lei Li"
                },
                {
                    "authorId": "2146034504",
                    "name": "Liang Chen"
                },
                {
                    "authorId": "2116276849",
                    "name": "Dawei Zhu"
                },
                {
                    "authorId": "3186130",
                    "name": "Binghuai Lin"
                },
                {
                    "authorId": "2154235",
                    "name": "Yunbo Cao"
                },
                {
                    "authorId": "2144831944",
                    "name": "Qi Liu"
                },
                {
                    "authorId": "1701889",
                    "name": "Tianyu Liu"
                },
                {
                    "authorId": "3335836",
                    "name": "Zhifang Sui"
                }
            ],
            "abstract": "In this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the\"win/tie/lose\"outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at \\url{https://github.com/i-Eval/FairEval} to facilitate future research.",
            "corpus_id": 258960339,
            "sentences": [
                {
                    "corpus_id": "258960339",
                    "title": "Large Language Models are not Fair Evaluators",
                    "text": "Specifically, we demonstrate that GPT-4 exhibits a preference for the first displayed candidate response by consistently assigning it higher scores, even when the order of candidates is subtly altered. As illustrated in Figure 1, merely swapping the presentation order can reverse evaluation outcomes. This bias is also present in Chat-GPT, which typically favors the second response. These findings highlight previously overlooked limitations in the current evaluation paradigm. \n\nTo address this issue, we propose three simple yet effective strategies to calibrate positional bias: 1) Multiple Evidence Calibration (MEC): We prompt the model to generate evaluation evidence before assigning scores, leveraging the inherent properties of causal language models for calibration. We also employ ensemble techniques to incorporate multiple evidence calibration results to further stabilize the evaluation. 2) Balanced Position Calibration (BPC): To further reduce positional bias, we evaluate each candidate in both positions across two runs and compute the final score as the average of the two runs. 3) Human In The Loop Calibration (HITLC): We also explore human-inthe-loop evaluation and consider a diversity-based method to get a cue to indicate biased candidates based on the evaluation results of MEC and BPC. \n\nTo assess the efficacy of our methods, we manually annotate the \"win/tie/lose\" outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna benchmark (Zheng et al., 2023), encompassing 80 questions spanning 9 distinct question categories. Our MEC and BPC enhance the evaluation alignment of GPT-4 and ChatGPT by 9.8% and 14.3% accuracy, respectively. Moreover, based on MEC and BPC, our HITLC can further effectively integrate human assistance into the evaluation process. Specifically, with only a 20% human annotation cost, GPT-4 and ChatGPT can achieve comparable or even better annotation alignment with the average human performance, reducing the annotation cost by up to 39%. \n\nIn summary, our key contributions are: 1) We reveal that LLMs exhibit severe positional bias, com- We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.",
                    "score": 0.29285570405463957,
                    "section_title": "GPT-4",
                    "char_start_offset": 2306,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 201
                        },
                        {
                            "start": 202,
                            "end": 301
                        },
                        {
                            "start": 302,
                            "end": 384
                        },
                        {
                            "start": 385,
                            "end": 479
                        },
                        {
                            "start": 482,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 903
                        },
                        {
                            "start": 904,
                            "end": 1099
                        },
                        {
                            "start": 1100,
                            "end": 1314
                        },
                        {
                            "start": 1317,
                            "end": 1558
                        },
                        {
                            "start": 1559,
                            "end": 1670
                        },
                        {
                            "start": 1671,
                            "end": 1792
                        },
                        {
                            "start": 1793,
                            "end": 2001
                        },
                        {
                            "start": 2004,
                            "end": 2231
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62353515625
                }
            ],
            "relevance_judgement": 0.62353515625,
            "relevance_judgment_input_expanded": "# Title: Large Language Models are not Fair Evaluators\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui\n## Abstract\nIn this paper, we uncover a systematic bias in the evaluation paradigm of adopting large language models~(LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. To address this issue, we propose a calibration framework with three simple yet effective strategies: 1) Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings; 2) Balanced Position Calibration, which aggregates results across various orders to determine the final score; 3) Human-in-the-Loop Calibration, which introduces a balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when needed. We also manually annotate the\"win/tie/lose\"outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and extensive experiments demonstrate that our approach successfully mitigates evaluation bias, resulting in closer alignment with human judgments. We release our code and human annotation at \\url{https://github.com/i-Eval/FairEval} to facilitate future research.\n## GPT-4\nSpecifically, we demonstrate that GPT-4 exhibits a preference for the first displayed candidate response by consistently assigning it higher scores, even when the order of candidates is subtly altered. As illustrated in Figure 1, merely swapping the presentation order can reverse evaluation outcomes. This bias is also present in Chat-GPT, which typically favors the second response. These findings highlight previously overlooked limitations in the current evaluation paradigm. \n\nTo address this issue, we propose three simple yet effective strategies to calibrate positional bias: 1) Multiple Evidence Calibration (MEC): We prompt the model to generate evaluation evidence before assigning scores, leveraging the inherent properties of causal language models for calibration. We also employ ensemble techniques to incorporate multiple evidence calibration results to further stabilize the evaluation. 2) Balanced Position Calibration (BPC): To further reduce positional bias, we evaluate each candidate in both positions across two runs and compute the final score as the average of the two runs. 3) Human In The Loop Calibration (HITLC): We also explore human-inthe-loop evaluation and consider a diversity-based method to get a cue to indicate biased candidates based on the evaluation results of MEC and BPC. \n\nTo assess the efficacy of our methods, we manually annotate the \"win/tie/lose\" outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna benchmark (Zheng et al., 2023), encompassing 80 questions spanning 9 distinct question categories. Our MEC and BPC enhance the evaluation alignment of GPT-4 and ChatGPT by 9.8% and 14.3% accuracy, respectively. Moreover, based on MEC and BPC, our HITLC can further effectively integrate human assistance into the evaluation process. Specifically, with only a 20% human annotation cost, GPT-4 and ChatGPT can achieve comparable or even better annotation alignment with the average human performance, reducing the annotation cost by up to 39%. \n\nIn summary, our key contributions are: 1) We reveal that LLMs exhibit severe positional bias, com- We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.",
            "reference_string": "[258960339 | Wang et al. | 2023 | Citations: 573]"
        },
        {
            "title": "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 37,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.05061, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "18171842",
                    "name": "Michael Krumdick"
                },
                {
                    "authorId": "2307472942",
                    "name": "Charles Lovering"
                },
                {
                    "authorId": "2266430123",
                    "name": "Varshini Reddy"
                },
                {
                    "authorId": "78150202",
                    "name": "Seth Ebner"
                },
                {
                    "authorId": "2266398345",
                    "name": "Chris Tanner"
                }
            ],
            "abstract": "LLM-as-a-Judge is a framework that uses an LLM (large language model) to evaluate the quality of natural language text - typically text that is also generated by an LLM. This framework holds great promise due to its relative low-cost, ease of use, and strong correlations with human stylistic preferences. However, LLM Judges have been shown to exhibit biases that can distort their judgments. We evaluate how well LLM Judges can grade whether a given response to a conversational question is correct, an ability crucial to soundly estimating the overall response quality. To do so, we create and publicly release a human-annotated dataset with labels of correctness for 1,200 LLM responses. We source questions from a combination of existing datasets and a novel, challenging benchmark (BFF-Bench) created for this analysis. We demonstrate a strong connection between an LLM's ability to correctly answer a question and grade responses to that question. Although aggregate level statistics might imply a judge has high agreement with human annotators, it will struggle on the subset of questions it could not answer. To address this issue, we recommend a simple solution: provide the judge with a correct, human-written reference answer. We perform an in-depth analysis on how reference quality can affect the performance of an LLM Judge. We show that providing a weaker judge (e.g. Qwen 2.5 7B) with higher quality references reaches better agreement with human annotators than a stronger judge (e.g. GPT-4o) with synthetic references.",
            "corpus_id": 276885275,
            "sentences": [
                {
                    "corpus_id": "276885275",
                    "title": "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding",
                    "text": "Different types of references-wrong, humangenerated, or self-generated-each lead to different profiles of judge behavior. One common judgment bias is self-preference, where a model tends to overrate its own responses. To quantify this bias we computed the false positive rate (FPR) for each judge when evaluating its own responses versus all other models. In this context, the FPR represents the rate at which the model erroneously labels incorrect responses as correct. We also evaluated the false negative rate (FNR), which is the rate at which the model erroneously labels correct responses as incorrect. A model with a strong selfpreference bias would exhibit a high FPR and a low FNR when grading its own references. Figure 4 displays the FPR and FNR aggregated over each judge with the \"Wrong\", \"Random\", \"Self\", \"None\", and \"Human\" reference types (Metrics per judge can be found in Figure 5). For every reference type, we see that on average models have a higher FPR when grading their own responses. The gap is particularly large when the model is provided with its own generated reference or no reference at all. Thus, providing a human reference reduces both the overall rate of error and the relative difference between a model's judgment of its own responses and those of others. \n\nWhen the model is given a \"Wrong\" reference, the FPR is low and the FNR is very high, which suggests that the incorrect information leads judges to erroneously classify correct responses as incorrect. However, this effect is not observed with the \"Random\" reference. This evidence supports the idea that judges are sensitive to the correctness of the reference, and providing a slightly incorrect reference is worse than providing an unrelated reference or no reference at all. \n\nEven when provided with a \"Wrong\" reference, Table 5: Cohen's Kappa comparison between different references, evaluated over the subset of total questions that GPT-4o gets correct. There is no longer a statistically significant difference in performance in between the human-written gold references and the verified GPT-4o (\u2713) references, suggesting that verifying the correctness of the LLM generated references can be enough. \n\nthe LLM Judges have a non-zero FPR. This means that they are still grading responses as correct.",
                    "score": 0.2878676045716214,
                    "section_title": "Error Analysis",
                    "char_start_offset": 22359,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 121
                        },
                        {
                            "start": 122,
                            "end": 217
                        },
                        {
                            "start": 218,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 470
                        },
                        {
                            "start": 471,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 721
                        },
                        {
                            "start": 722,
                            "end": 900
                        },
                        {
                            "start": 901,
                            "end": 1008
                        },
                        {
                            "start": 1009,
                            "end": 1122
                        },
                        {
                            "start": 1123,
                            "end": 1292
                        },
                        {
                            "start": 1295,
                            "end": 1495
                        },
                        {
                            "start": 1496,
                            "end": 1561
                        },
                        {
                            "start": 1562,
                            "end": 1772
                        },
                        {
                            "start": 1775,
                            "end": 1954
                        },
                        {
                            "start": 1955,
                            "end": 2201
                        },
                        {
                            "start": 2204,
                            "end": 2239
                        },
                        {
                            "start": 2240,
                            "end": 2300
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.599609375
                }
            ],
            "relevance_judgement": 0.599609375,
            "relevance_judgment_input_expanded": "# Title: No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding\n# Venue: arXiv.org\n# Authors: Michael Krumdick, Charles Lovering, Varshini Reddy, Seth Ebner, Chris Tanner\n## Abstract\nLLM-as-a-Judge is a framework that uses an LLM (large language model) to evaluate the quality of natural language text - typically text that is also generated by an LLM. This framework holds great promise due to its relative low-cost, ease of use, and strong correlations with human stylistic preferences. However, LLM Judges have been shown to exhibit biases that can distort their judgments. We evaluate how well LLM Judges can grade whether a given response to a conversational question is correct, an ability crucial to soundly estimating the overall response quality. To do so, we create and publicly release a human-annotated dataset with labels of correctness for 1,200 LLM responses. We source questions from a combination of existing datasets and a novel, challenging benchmark (BFF-Bench) created for this analysis. We demonstrate a strong connection between an LLM's ability to correctly answer a question and grade responses to that question. Although aggregate level statistics might imply a judge has high agreement with human annotators, it will struggle on the subset of questions it could not answer. To address this issue, we recommend a simple solution: provide the judge with a correct, human-written reference answer. We perform an in-depth analysis on how reference quality can affect the performance of an LLM Judge. We show that providing a weaker judge (e.g. Qwen 2.5 7B) with higher quality references reaches better agreement with human annotators than a stronger judge (e.g. GPT-4o) with synthetic references.\n## Error Analysis\nDifferent types of references-wrong, humangenerated, or self-generated-each lead to different profiles of judge behavior. One common judgment bias is self-preference, where a model tends to overrate its own responses. To quantify this bias we computed the false positive rate (FPR) for each judge when evaluating its own responses versus all other models. In this context, the FPR represents the rate at which the model erroneously labels incorrect responses as correct. We also evaluated the false negative rate (FNR), which is the rate at which the model erroneously labels correct responses as incorrect. A model with a strong selfpreference bias would exhibit a high FPR and a low FNR when grading its own references. Figure 4 displays the FPR and FNR aggregated over each judge with the \"Wrong\", \"Random\", \"Self\", \"None\", and \"Human\" reference types (Metrics per judge can be found in Figure 5). For every reference type, we see that on average models have a higher FPR when grading their own responses. The gap is particularly large when the model is provided with its own generated reference or no reference at all. Thus, providing a human reference reduces both the overall rate of error and the relative difference between a model's judgment of its own responses and those of others. \n\nWhen the model is given a \"Wrong\" reference, the FPR is low and the FNR is very high, which suggests that the incorrect information leads judges to erroneously classify correct responses as incorrect. However, this effect is not observed with the \"Random\" reference. This evidence supports the idea that judges are sensitive to the correctness of the reference, and providing a slightly incorrect reference is worse than providing an unrelated reference or no reference at all. \n\nEven when provided with a \"Wrong\" reference, Table 5: Cohen's Kappa comparison between different references, evaluated over the subset of total questions that GPT-4o gets correct. There is no longer a statistically significant difference in performance in between the human-written gold references and the verified GPT-4o (\u2713) references, suggesting that verifying the correctness of the LLM generated references can be enough. \n\nthe LLM Judges have a non-zero FPR. This means that they are still grading responses as correct.",
            "reference_string": "[276885275 | Krumdick et al. | 2025 | Citations: 6]"
        },
        {
            "title": "Style Over Substance: Evaluation Biases for Large Language Models",
            "venue": "International Conference on Computational Linguistics",
            "year": 2023,
            "reference_count": 51,
            "citation_count": 47,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.03025",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.03025, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2145209409",
                    "name": "Minghao Wu"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                }
            ],
            "abstract": "As large language models (LLMs) continue to advance, accurately and comprehensively evaluating their performance becomes increasingly challenging. Ranking the relative performance of LLMs based on Elo ratings, according to human judgment, is gaining more popularity. However, the extent to which humans and LLMs are capable evaluators remains uncertain. This study investigates the behavior of crowd-sourced and expert annotators, as well as LLMs, when comparing outputs from different models. To achieve this, we curate a dataset of intentionally flawed machine-generated answers. Our findings reveal a concerning bias in the evaluation process, as answers with factual errors are rated more favorably than answers that are too short or contained grammatical errors. To address this issue, we propose independently evaluating machine-generated text across multiple dimensions, rather than merging all the evaluation aspects into a single score. We instantiate this idea with the Elo rating system, resulting in the Multi-Elo Rating System (MERS). Empirical results from our study reveal that this proposed approach significantly enhances the quality of LLM-based evaluations, particularly in terms of factual accuracy. However, there is no significant improvement in crowd-sourced-based evaluations, indicating the need for further investigation.",
            "corpus_id": 259360998,
            "sentences": [
                {
                    "corpus_id": "259360998",
                    "title": "Style Over Substance: Evaluation Biases for Large Language Models",
                    "text": "As human evaluation can be costly and inefficient, there is an increase in the use of advanced LLMs, such as GPT-4, to evaluate model outputs. In our work, we also use LLMs as judges to assess answer quality. However, previous studies rely solely on GPT-4 as the LLM judge (Chiang et al., 2023;Li et al., 2023a;Zheng et al., 2023), which may not be appropriate for our work as our answers are refined by humans after being generated by GPT-4. This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4.5 By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study. \n\nWe utilize the evaluation prompt from Dettmers et al. ( 2023), as presented in Figure 1. The prompt assesses the answers based on their helpfulness, relevance, accuracy, and level of detail, while also aiming to avoid bias related to answer ordering.",
                    "score": 0.31713363161028885,
                    "section_title": "LLM Evaluation",
                    "char_start_offset": 13307,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 208
                        },
                        {
                            "start": 209,
                            "end": 442
                        },
                        {
                            "start": 443,
                            "end": 572
                        },
                        {
                            "start": 573,
                            "end": 658
                        },
                        {
                            "start": 659,
                            "end": 797
                        },
                        {
                            "start": 800,
                            "end": 888
                        },
                        {
                            "start": 889,
                            "end": 1050
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.552734375
                }
            ],
            "relevance_judgement": 0.552734375,
            "relevance_judgment_input_expanded": "# Title: Style Over Substance: Evaluation Biases for Large Language Models\n# Venue: International Conference on Computational Linguistics\n# Authors: Minghao Wu, Alham Fikri Aji\n## Abstract\nAs large language models (LLMs) continue to advance, accurately and comprehensively evaluating their performance becomes increasingly challenging. Ranking the relative performance of LLMs based on Elo ratings, according to human judgment, is gaining more popularity. However, the extent to which humans and LLMs are capable evaluators remains uncertain. This study investigates the behavior of crowd-sourced and expert annotators, as well as LLMs, when comparing outputs from different models. To achieve this, we curate a dataset of intentionally flawed machine-generated answers. Our findings reveal a concerning bias in the evaluation process, as answers with factual errors are rated more favorably than answers that are too short or contained grammatical errors. To address this issue, we propose independently evaluating machine-generated text across multiple dimensions, rather than merging all the evaluation aspects into a single score. We instantiate this idea with the Elo rating system, resulting in the Multi-Elo Rating System (MERS). Empirical results from our study reveal that this proposed approach significantly enhances the quality of LLM-based evaluations, particularly in terms of factual accuracy. However, there is no significant improvement in crowd-sourced-based evaluations, indicating the need for further investigation.\n## LLM Evaluation\nAs human evaluation can be costly and inefficient, there is an increase in the use of advanced LLMs, such as GPT-4, to evaluate model outputs. In our work, we also use LLMs as judges to assess answer quality. However, previous studies rely solely on GPT-4 as the LLM judge (Chiang et al., 2023;Li et al., 2023a;Zheng et al., 2023), which may not be appropriate for our work as our answers are refined by humans after being generated by GPT-4. This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4.5 By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study. \n\nWe utilize the evaluation prompt from Dettmers et al. ( 2023), as presented in Figure 1. The prompt assesses the answers based on their helpfulness, relevance, accuracy, and level of detail, while also aiming to avoid bias related to answer ordering.",
            "reference_string": "[259360998 | Wu et al. | 2023 | Citations: 47]"
        },
        {
            "title": "Toward Clinical Generative AI: Conceptual Framework",
            "venue": "JMIR AI",
            "year": 2023,
            "reference_count": 59,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://s3.ca-central-1.amazonaws.com/assets.jmir.org/assets/preprints/preprint-55957-accepted.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11193080, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2250832100",
                    "name": "N. Bragazzi"
                },
                {
                    "authorId": "2277106726",
                    "name": "Sergio Garbarino"
                }
            ],
            "abstract": "Clinical decision-making is a crucial aspect of health care, involving the balanced integration of scientific evidence, clinical judgment, ethical considerations, and patient involvement. This process is dynamic and multifaceted, relying on clinicians\u2019 knowledge, experience, and intuitive understanding to achieve optimal patient outcomes through informed, evidence-based choices. The advent of generative artificial intelligence (AI) presents a revolutionary opportunity in clinical decision-making. AI\u2019s advanced data analysis and pattern recognition capabilities can significantly enhance the diagnosis and treatment of diseases, processing vast medical data to identify patterns, tailor treatments, predict disease progression, and aid in proactive patient management. However, the incorporation of AI into clinical decision-making raises concerns regarding the reliability and accuracy of AI-generated insights. To address these concerns, 11 \u201cverification paradigms\u201d are proposed in this paper, with each paradigm being a unique method to verify the evidence-based nature of AI in clinical decision-making. This paper also frames the concept of \u201cclinically explainable, fair, and responsible, clinician-, expert-, and patient-in-the-loop AI.\u201d This model focuses on ensuring AI\u2019s comprehensibility, collaborative nature, and ethical grounding, advocating for AI to serve as an augmentative tool, with its decision-making processes being transparent and understandable to clinicians and patients. The integration of AI should enhance, not replace, the clinician\u2019s judgment and should involve continuous learning and adaptation based on real-world outcomes and ethical and legal compliance. In conclusion, while generative AI holds immense promise in enhancing clinical decision-making, it is essential to ensure that it produces evidence-based, reliable, and impactful knowledge. Using the outlined paradigms and approaches can help the medical and patient communities harness AI\u2019s potential while maintaining high patient care standards.",
            "corpus_id": 270614044,
            "sentences": [
                {
                    "corpus_id": "270614044",
                    "title": "Toward Clinical Generative AI: Conceptual Framework",
                    "text": "The resultant model, Med-PaLM, shows promise, yet it still does not match clinician performance even though the authors could observe that model scale and instruction prompt tuning significantly enhance comprehension, knowledge recall, and reasoning.\n\nA further risk is that LLMs might reinforce existing biases and provide inaccurate medical diagnoses, potentially leading to detrimental effects on health care.Zack et al [26] aimed to evaluate whether GPT-4 harbors biases that could influence its application in health care settings.Using the Azure OpenAI interface, the authors scrutinized GPT-4 for racial and gender biases and assessed the impact of such biases on four clinical applications of LLMs-(1) medical education, (2) diagnostic reasoning, (3) development and implementation of clinical plans, and (4) subjective patient evaluations-involving experiments using prompts mimicking typical GPT-4 use in clinical and medical educational settings and drawing from New England Journal of Medicine Healer clinical vignettes and research on implicit bias in health care.The study compared GPT-4's estimates of demographic distributions of medical conditions against actual US prevalence data.For differential diagnosis and treatment planning, the research analyzed variations across demographic groups using standard statistical methods to identify significant differences.The study revealed that GPT-4 inadequately represents demographic diversity in medical conditions, often resorting to stereotypical demographic portrayals in clinical vignettes.The differential diagnoses generated by GPT-4 for standardized clinical vignettes tended to reflect biases associated with race, ethnicity, and gender.Furthermore, the model's assessments and plans demonstrated a notable correlation between demographic characteristics and recommendations for costlier procedures, as well as varied perceptions of patients.\n\nAll this, taken together, suggests the potential role of LLMs in medicine, but human evaluations also highlight the current models' limitations, underscoring the importance of comprehensive evaluation frameworks and continued methodological advancements to develop safe, effective LLMs for clinical use.",
                    "score": 0.35076293115923396,
                    "section_title": "Toward Clinical LLMs: Necessity of Verifying Evidence-Based Knowledge",
                    "char_start_offset": 7783,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 250
                        },
                        {
                            "start": 252,
                            "end": 412
                        },
                        {
                            "start": 412,
                            "end": 536
                        },
                        {
                            "start": 536,
                            "end": 1077
                        },
                        {
                            "start": 1077,
                            "end": 1199
                        },
                        {
                            "start": 1199,
                            "end": 1380
                        },
                        {
                            "start": 1380,
                            "end": 1557
                        },
                        {
                            "start": 1557,
                            "end": 1708
                        },
                        {
                            "start": 1708,
                            "end": 1913
                        },
                        {
                            "start": 1915,
                            "end": 2218
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51806640625
                }
            ],
            "relevance_judgement": 0.51806640625,
            "relevance_judgment_input_expanded": "# Title: Toward Clinical Generative AI: Conceptual Framework\n# Venue: JMIR AI\n# Authors: N. Bragazzi, Sergio Garbarino\n## Abstract\nClinical decision-making is a crucial aspect of health care, involving the balanced integration of scientific evidence, clinical judgment, ethical considerations, and patient involvement. This process is dynamic and multifaceted, relying on clinicians\u2019 knowledge, experience, and intuitive understanding to achieve optimal patient outcomes through informed, evidence-based choices. The advent of generative artificial intelligence (AI) presents a revolutionary opportunity in clinical decision-making. AI\u2019s advanced data analysis and pattern recognition capabilities can significantly enhance the diagnosis and treatment of diseases, processing vast medical data to identify patterns, tailor treatments, predict disease progression, and aid in proactive patient management. However, the incorporation of AI into clinical decision-making raises concerns regarding the reliability and accuracy of AI-generated insights. To address these concerns, 11 \u201cverification paradigms\u201d are proposed in this paper, with each paradigm being a unique method to verify the evidence-based nature of AI in clinical decision-making. This paper also frames the concept of \u201cclinically explainable, fair, and responsible, clinician-, expert-, and patient-in-the-loop AI.\u201d This model focuses on ensuring AI\u2019s comprehensibility, collaborative nature, and ethical grounding, advocating for AI to serve as an augmentative tool, with its decision-making processes being transparent and understandable to clinicians and patients. The integration of AI should enhance, not replace, the clinician\u2019s judgment and should involve continuous learning and adaptation based on real-world outcomes and ethical and legal compliance. In conclusion, while generative AI holds immense promise in enhancing clinical decision-making, it is essential to ensure that it produces evidence-based, reliable, and impactful knowledge. Using the outlined paradigms and approaches can help the medical and patient communities harness AI\u2019s potential while maintaining high patient care standards.\n## Toward Clinical LLMs: Necessity of Verifying Evidence-Based Knowledge\nThe resultant model, Med-PaLM, shows promise, yet it still does not match clinician performance even though the authors could observe that model scale and instruction prompt tuning significantly enhance comprehension, knowledge recall, and reasoning.\n\nA further risk is that LLMs might reinforce existing biases and provide inaccurate medical diagnoses, potentially leading to detrimental effects on health care.Zack et al [26] aimed to evaluate whether GPT-4 harbors biases that could influence its application in health care settings.Using the Azure OpenAI interface, the authors scrutinized GPT-4 for racial and gender biases and assessed the impact of such biases on four clinical applications of LLMs-(1) medical education, (2) diagnostic reasoning, (3) development and implementation of clinical plans, and (4) subjective patient evaluations-involving experiments using prompts mimicking typical GPT-4 use in clinical and medical educational settings and drawing from New England Journal of Medicine Healer clinical vignettes and research on implicit bias in health care.The study compared GPT-4's estimates of demographic distributions of medical conditions against actual US prevalence data.For differential diagnosis and treatment planning, the research analyzed variations across demographic groups using standard statistical methods to identify significant differences.The study revealed that GPT-4 inadequately represents demographic diversity in medical conditions, often resorting to stereotypical demographic portrayals in clinical vignettes.The differential diagnoses generated by GPT-4 for standardized clinical vignettes tended to reflect biases associated with race, ethnicity, and gender.Furthermore, the model's assessments and plans demonstrated a notable correlation between demographic characteristics and recommendations for costlier procedures, as well as varied perceptions of patients.\n\nAll this, taken together, suggests the potential role of LLMs in medicine, but human evaluations also highlight the current models' limitations, underscoring the importance of comprehensive evaluation frameworks and continued methodological advancements to develop safe, effective LLMs for clinical use.",
            "reference_string": "[270614044 | Bragazzi et al. | 2023 | Citations: 13]"
        },
        {
            "title": "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 56,
            "citation_count": 77,
            "influential_citation_count": 12,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.15123, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35106509",
                    "name": "Swarnadeep Saha"
                },
                {
                    "authorId": "2253752918",
                    "name": "Omer Levy"
                },
                {
                    "authorId": "1709797",
                    "name": "Asli Celikyilmaz"
                },
                {
                    "authorId": "2253762115",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "2243265350",
                    "name": "Jason Weston"
                },
                {
                    "authorId": "2243015223",
                    "name": "Xian Li"
                }
            ],
            "abstract": "Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that involve satisfying intricate user constraints or taking into account multiple aspects and criteria. However, their performance can fall short, due to the model\u2019s lack of coherence and inability to plan and decompose the problem. We propose Branch-Solve-Merge (BSM), a Large Language Model program (Schlag et al., 2023) for tackling such challenging natural language tasks. It consists of branch, solve, and merge modules that are parameterized with specific prompts to the base LLM. These three modules plan a decomposition of the task into multiple parallel sub-tasks, independently solve them, and fuse the solutions to the sub-tasks. We apply our method to the tasks of LLM response evaluation and constrained text generation and evaluate its effectiveness with multiple LLMs, including Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and consistency for each LLM by enhancing human-LLM agreement by up to 26%, reducing length and pairwise position biases by up to 50%, and allowing LLaMA-2-chat to match or outperform GPT-4 on most domains. On a constraint story generation task, BSM improves the coherence of stories while also improving constraint satisfaction by 12%.",
            "corpus_id": 264591429,
            "sentences": [
                {
                    "corpus_id": "264591429",
                    "title": "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation",
                    "text": "\u2022 Self-enhancement Bias Reduction.Table 2 evaluates self-enhancement bias by comparing BSM (with zero-shot GPT-4) for the samples where one of the responses is also generated by GPT-4.We observe a 3% better correlation with humans, suggesting that BSM improves evaluation even when the LLM judges its own outputs.\n\nBSM not only leads to an improvement in overall LLM-human agreement (as per the 'Ag' metric) but also on the fraction of samples where one response is generated by the same evaluator LLM (as per the 'SB' metric), thus pointing to its robustness as an evaluation method.In summary, BSM improves both correctness and consistency of LLM-based evaluators.\n\nBSM improves upon all zero-shot base LLMs.We demonstrate the generalizability of BSM as an LLM program by implementing it on top of four dif- We compare LLaMA-2-70B-chat BSM with the baseline zero-shot method, and also report GPT-4 results.BSM obtains significant improvements over the LLaMA baseline, and matches or is close to GPT-4 agreement in three of the four domains, while sometimes outperforming GPT-4 in reducing biases.\n\nferent base LLMs, ranging from LLaMA-2-7B to GPT-4.As shown in Table 3, BSM improves agreement with humans for all base LLMs, compared to a zero-shot baseline.Even though zero-shot GPT-4 is the state-of-the-art LLM-based evaluator, applying BSM obtains a further improvement of 3%.Moreover, applying BSM to LLaMA-2-70Bchat makes it competitive with GPT-4 for turn-1 questions.BSM also significantly reduces position and length biases for all models except for GPT-4.",
                    "score": 0.3344188913081778,
                    "section_title": "Main Results",
                    "char_start_offset": 22039,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 34
                        },
                        {
                            "start": 34,
                            "end": 184
                        },
                        {
                            "start": 184,
                            "end": 313
                        },
                        {
                            "start": 315,
                            "end": 584
                        },
                        {
                            "start": 584,
                            "end": 666
                        },
                        {
                            "start": 668,
                            "end": 710
                        },
                        {
                            "start": 710,
                            "end": 908
                        },
                        {
                            "start": 908,
                            "end": 1098
                        },
                        {
                            "start": 1100,
                            "end": 1151
                        },
                        {
                            "start": 1151,
                            "end": 1259
                        },
                        {
                            "start": 1259,
                            "end": 1381
                        },
                        {
                            "start": 1381,
                            "end": 1476
                        },
                        {
                            "start": 1476,
                            "end": 1566
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.513671875
                }
            ],
            "relevance_judgement": 0.513671875,
            "relevance_judgment_input_expanded": "# Title: Branch-Solve-Merge Improves Large Language Model Evaluation and Generation\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, Xian Li\n## Abstract\nLarge Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that involve satisfying intricate user constraints or taking into account multiple aspects and criteria. However, their performance can fall short, due to the model\u2019s lack of coherence and inability to plan and decompose the problem. We propose Branch-Solve-Merge (BSM), a Large Language Model program (Schlag et al., 2023) for tackling such challenging natural language tasks. It consists of branch, solve, and merge modules that are parameterized with specific prompts to the base LLM. These three modules plan a decomposition of the task into multiple parallel sub-tasks, independently solve them, and fuse the solutions to the sub-tasks. We apply our method to the tasks of LLM response evaluation and constrained text generation and evaluate its effectiveness with multiple LLMs, including Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and consistency for each LLM by enhancing human-LLM agreement by up to 26%, reducing length and pairwise position biases by up to 50%, and allowing LLaMA-2-chat to match or outperform GPT-4 on most domains. On a constraint story generation task, BSM improves the coherence of stories while also improving constraint satisfaction by 12%.\n## Main Results\n\u2022 Self-enhancement Bias Reduction.Table 2 evaluates self-enhancement bias by comparing BSM (with zero-shot GPT-4) for the samples where one of the responses is also generated by GPT-4.We observe a 3% better correlation with humans, suggesting that BSM improves evaluation even when the LLM judges its own outputs.\n\nBSM not only leads to an improvement in overall LLM-human agreement (as per the 'Ag' metric) but also on the fraction of samples where one response is generated by the same evaluator LLM (as per the 'SB' metric), thus pointing to its robustness as an evaluation method.In summary, BSM improves both correctness and consistency of LLM-based evaluators.\n\nBSM improves upon all zero-shot base LLMs.We demonstrate the generalizability of BSM as an LLM program by implementing it on top of four dif- We compare LLaMA-2-70B-chat BSM with the baseline zero-shot method, and also report GPT-4 results.BSM obtains significant improvements over the LLaMA baseline, and matches or is close to GPT-4 agreement in three of the four domains, while sometimes outperforming GPT-4 in reducing biases.\n\nferent base LLMs, ranging from LLaMA-2-7B to GPT-4.As shown in Table 3, BSM improves agreement with humans for all base LLMs, compared to a zero-shot baseline.Even though zero-shot GPT-4 is the state-of-the-art LLM-based evaluator, applying BSM obtains a further improvement of 3%.Moreover, applying BSM to LLaMA-2-70Bchat makes it competitive with GPT-4 for turn-1 questions.BSM also significantly reduces position and length biases for all models except for GPT-4.",
            "reference_string": "[264591429 | Saha et al. | 2023 | Citations: 77]"
        },
        {
            "title": "Bias Testing and Mitigation in LLM-based Code Generation",
            "venue": "ACM Transactions on Software Engineering and Methodology",
            "year": 2023,
            "reference_count": 141,
            "citation_count": 27,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.14345",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.14345, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145252513",
                    "name": "Dong Huang"
                },
                {
                    "authorId": "2290184536",
                    "name": "Qingwen Bu"
                },
                {
                    "authorId": "51250527",
                    "name": "J Zhang"
                },
                {
                    "authorId": "2265090245",
                    "name": "Xiaofei Xie"
                },
                {
                    "authorId": "123878420",
                    "name": "Junjie Chen"
                },
                {
                    "authorId": "2944075",
                    "name": "Heming Cui"
                }
            ],
            "abstract": "As the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social bias and unfairness, such as those related to age, gender, and race? This issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models but are underexplored in the literature. This paper presents a novel bias testing framework that is specifically designed for code generation tasks. Based on this framework, we conduct an extensive empirical study on the biases in code generated by five widely studied LLMs (i.e., PALM-2-CodeChat-bison, Claude-instant-1, GPT-3.5-turbo, GPT-4-turbo, and GPT-4). Our findings reveal that biases are prevalent. For example, 13.47% to 49.10% of the codes generated by these LLMs have biased behaviors towards gender. Moreover, we study five bias mitigation prompt strategies that are commonly used in current code generation scenarios, i.e., zero-shot, one-shot, few-shot, and two Chain-of-Thought (CoT) prompts, with and without provided feedback-driven refinement. Our evaluation results illustrate that using direct prompt engineering strategies has limited effectiveness in mitigating bias, but our test execution feedback can help to reduce the ratio of code biases to a large extent (e.g., from 59.88% to 4.79% for GPT-4)1.",
            "corpus_id": 262824773,
            "sentences": [
                {
                    "corpus_id": "262824773",
                    "title": "Bias Testing and Mitigation in LLM-based Code Generation",
                    "text": "Our manual analysis confirms that the bias testing procedure we designed is reliable in detecting bias from the code snippets, e.g., the precision of automated bias testing is 100%. \n\nInspired by the recent works [23][24][25][26][27][28][29][30][31] that uses few-shot learning and Chain-of-Thought to tackle complex challenges, we also conduct an empirical study of five bias mitigation strategies (i.e., zero-shot, one-shot, few-shot learning, and two Chain-of-Though) to mitigate bias from the code generation procedure and mitigate bias from already generated code snippets. Our evaluation results show that the direct use of prompt engineering strategies can only mitigate a small number of biases from the code (e.g., the overall CBS of GPT-4 decreases from 59.88% to 36.23% for zero-shot prompting). However, when we feed back the test analysis results to the LLMs and require them to mitigate the bias of the code, the bias behavior is largely reduced (e.g., the overall CBS of GPT-4 decreases from 59.88% to 10.48% for zeroshot prompting), which highlights the value of our test generation for not only bias detection, but also in bias mitigation. \n\nIn summary, this paper makes the following contributions: \n\n\u2022 We propose a novel code bias evaluation framework (as shown in Fig. 3) specifically designed for code generation models. This framework incorporates three code bias metrics (i.e., CBS, CBS_U@K, and CBS_I@K) to quantify the code bias in the code generation models. \u2022 Using our evaluation framework, we conduct an empirical study to comprehensively investigate and analyze the fairness of five state-of-the-art LLMs in code generation. Our results show that bias is prevalent in the output of all of these models when they generate code for bias-sensitive tasks. \n\n\u2022 We evaluate a series of widely studied prompt engineering strategies to check whether these strategies can reduce bias from the code. Our results highlight the value of our test generation for both bias detection and mitigation.",
                    "score": 0.2790467494726986,
                    "section_title": "Previous Ours",
                    "char_start_offset": 5572,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 181
                        },
                        {
                            "start": 184,
                            "end": 578
                        },
                        {
                            "start": 579,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 1156
                        },
                        {
                            "start": 1159,
                            "end": 1216
                        },
                        {
                            "start": 1219,
                            "end": 1341
                        },
                        {
                            "start": 1342,
                            "end": 1484
                        },
                        {
                            "start": 1485,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1781
                        },
                        {
                            "start": 1784,
                            "end": 1919
                        },
                        {
                            "start": 1920,
                            "end": 2014
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 213,
                            "end": 217,
                            "matchedPaperCorpusId": "248476411"
                        },
                        {
                            "start": 225,
                            "end": 229,
                            "matchedPaperCorpusId": "246411621"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51318359375
                }
            ],
            "relevance_judgement": 0.51318359375,
            "relevance_judgment_input_expanded": "# Title: Bias Testing and Mitigation in LLM-based Code Generation\n# Venue: ACM Transactions on Software Engineering and Methodology\n# Authors: Dong Huang, Qingwen Bu, J Zhang, Xiaofei Xie, Junjie Chen, Heming Cui\n## Abstract\nAs the adoption of LLMs becomes more widespread in software coding ecosystems, a pressing issue has emerged: does the generated code contain social bias and unfairness, such as those related to age, gender, and race? This issue concerns the integrity, fairness, and ethical foundation of software applications that depend on the code generated by these models but are underexplored in the literature. This paper presents a novel bias testing framework that is specifically designed for code generation tasks. Based on this framework, we conduct an extensive empirical study on the biases in code generated by five widely studied LLMs (i.e., PALM-2-CodeChat-bison, Claude-instant-1, GPT-3.5-turbo, GPT-4-turbo, and GPT-4). Our findings reveal that biases are prevalent. For example, 13.47% to 49.10% of the codes generated by these LLMs have biased behaviors towards gender. Moreover, we study five bias mitigation prompt strategies that are commonly used in current code generation scenarios, i.e., zero-shot, one-shot, few-shot, and two Chain-of-Thought (CoT) prompts, with and without provided feedback-driven refinement. Our evaluation results illustrate that using direct prompt engineering strategies has limited effectiveness in mitigating bias, but our test execution feedback can help to reduce the ratio of code biases to a large extent (e.g., from 59.88% to 4.79% for GPT-4)1.\n## Previous Ours\nOur manual analysis confirms that the bias testing procedure we designed is reliable in detecting bias from the code snippets, e.g., the precision of automated bias testing is 100%. \n\nInspired by the recent works [23][24][25][26][27][28][29][30][31] that uses few-shot learning and Chain-of-Thought to tackle complex challenges, we also conduct an empirical study of five bias mitigation strategies (i.e., zero-shot, one-shot, few-shot learning, and two Chain-of-Though) to mitigate bias from the code generation procedure and mitigate bias from already generated code snippets. Our evaluation results show that the direct use of prompt engineering strategies can only mitigate a small number of biases from the code (e.g., the overall CBS of GPT-4 decreases from 59.88% to 36.23% for zero-shot prompting). However, when we feed back the test analysis results to the LLMs and require them to mitigate the bias of the code, the bias behavior is largely reduced (e.g., the overall CBS of GPT-4 decreases from 59.88% to 10.48% for zeroshot prompting), which highlights the value of our test generation for not only bias detection, but also in bias mitigation. \n\nIn summary, this paper makes the following contributions: \n\n\u2022 We propose a novel code bias evaluation framework (as shown in Fig. 3) specifically designed for code generation models. This framework incorporates three code bias metrics (i.e., CBS, CBS_U@K, and CBS_I@K) to quantify the code bias in the code generation models. \u2022 Using our evaluation framework, we conduct an empirical study to comprehensively investigate and analyze the fairness of five state-of-the-art LLMs in code generation. Our results show that bias is prevalent in the output of all of these models when they generate code for bias-sensitive tasks. \n\n\u2022 We evaluate a series of widely studied prompt engineering strategies to check whether these strategies can reduce bias from the code. Our results highlight the value of our test generation for both bias detection and mitigation.",
            "reference_string": "[262824773 | Huang et al. | 2023 | Citations: 27]"
        },
        {
            "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13125, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261896751",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "2257016293",
                    "name": "Yunshi Lan"
                },
                {
                    "authorId": "2268678836",
                    "name": "Chao Yang"
                }
            ],
            "abstract": "Recently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge. However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process. To address this issue, we introduce TreeEval, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage. Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process. We evaluate 6 models of different parameter sizes, including 7B, 13B, and 34B, and ultimately achieved the highest correlation coefficient with AlpacaEval2.0 using only around 45 questions. We also conduct more analysis to show the robustness and reliability of TreeEval.",
            "corpus_id": 267760188,
            "sentences": [
                {
                    "corpus_id": "267760188",
                    "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
                    "text": "Using LLMs like GPT-4 as judges introduces potential data leakage risks due to biases in their pre-training data. This can be mitigated by selecting neutral evaluators independent of the assessed models' training data or randomly rotating evaluators to reduce bias. While GPT-4 is a powerful examiner, it has limitations, particularly in areas outside its expertise. This can be addressed by providing more contextual guidance during evaluations. In the future, training specialized evaluators to extract questions from document repositories and assess comprehension could ensure more accurate, domain-specific evaluations.",
                    "score": 0.3906504774286632,
                    "section_title": "Limitations",
                    "char_start_offset": 24218,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 114,
                            "end": 265
                        },
                        {
                            "start": 266,
                            "end": 366
                        },
                        {
                            "start": 367,
                            "end": 446
                        },
                        {
                            "start": 447,
                            "end": 623
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51025390625
                }
            ],
            "relevance_judgement": 0.51025390625,
            "relevance_judgment_input_expanded": "# Title: TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Xiang Li, Yunshi Lan, Chao Yang\n## Abstract\nRecently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge. However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process. To address this issue, we introduce TreeEval, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage. Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process. We evaluate 6 models of different parameter sizes, including 7B, 13B, and 34B, and ultimately achieved the highest correlation coefficient with AlpacaEval2.0 using only around 45 questions. We also conduct more analysis to show the robustness and reliability of TreeEval.\n## Limitations\nUsing LLMs like GPT-4 as judges introduces potential data leakage risks due to biases in their pre-training data. This can be mitigated by selecting neutral evaluators independent of the assessed models' training data or randomly rotating evaluators to reduce bias. While GPT-4 is a powerful examiner, it has limitations, particularly in areas outside its expertise. This can be addressed by providing more contextual guidance during evaluations. In the future, training specialized evaluators to extract questions from document repositories and assess comprehension could ensure more accurate, domain-specific evaluations.",
            "reference_string": "[267760188 | Li et al. | 2024 | Citations: 11]"
        },
        {
            "title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 88,
            "citation_count": 29,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.01534, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2161635474",
                    "name": "Dawei Li"
                },
                {
                    "authorId": "2344419674",
                    "name": "Renliang Sun"
                },
                {
                    "authorId": "2324070910",
                    "name": "Yue Huang"
                },
                {
                    "authorId": "2316709408",
                    "name": "Ming Zhong"
                },
                {
                    "authorId": "2036355404",
                    "name": "Bohan Jiang"
                },
                {
                    "authorId": "2343853966",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "2307963162",
                    "name": "Xiangliang Zhang"
                },
                {
                    "authorId": "2343828587",
                    "name": "Wei Wang"
                },
                {
                    "authorId": "2287545693",
                    "name": "Huan Liu"
                }
            ],
            "abstract": "Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between the data generator LLM and the judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive and real-world problem that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge. We release all codes and data at: https://github.com/David-Li0406/Preference-Leakage.",
            "corpus_id": 276106991,
            "sentences": [
                {
                    "corpus_id": "276106991",
                    "title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge",
                    "text": "In our main experiment, we aim to provide insights into RQ1. Mistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA-3.3), the judge LLMs exhibit a strong preference toward their related student models, leading to large positive values in the preference leakage scores. This finding suggests that preference leakage, along with the resulting bias, is widespread in SFT when the data generator and evaluator are the same. Evaluators' bias towards certain LLMs can be inherited by its student models. From Figure 2 and Figure 3, we find an obvious preference of GPT-4o towards Mistral/ Qwen-LLaMA-3.3 and this leads to the low preference leakage score in the Mistral-GPT-4o vs Mistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA-3.3 pairs. To investigate the source of this preference, we examine whether the GPT-4 evaluator has a bias toward LLaMA series models. Using the MTBench (Zheng et al., 2023) dataset, which includes pairwise comparison judgments from both humans and GPT-4, we compare GPT-4's and human evaluators' judgments on LLaMA-2 vs other models (including Vicuna, Alpaca, GPT-3.5, and GPT-4, which are preferred by GPT-4 due to preference leakage or egocentric bias) and LLaMA-2 vs Claude. The results, shown in Figure 4, reveal a clear preference for LLaMA-2 by GPT-4. Consequently, we conclude that evaluators' bias can be inherited. In this case, GPT-4's bias toward LLaMA has been passed on to LLaMA's student models. This inheritance, combined with the opaque training data of LLMs, makes preference leakage a more complex and challenging problem. \n\nModel pairs with similar performance tend to have more  obvious preference leakage.",
                    "score": 0.3594802977685757,
                    "section_title": "Main Results",
                    "char_start_offset": 14023,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 60
                        },
                        {
                            "start": 61,
                            "end": 114
                        },
                        {
                            "start": 115,
                            "end": 261
                        },
                        {
                            "start": 262,
                            "end": 412
                        },
                        {
                            "start": 413,
                            "end": 490
                        },
                        {
                            "start": 491,
                            "end": 590
                        },
                        {
                            "start": 591,
                            "end": 718
                        },
                        {
                            "start": 719,
                            "end": 725
                        },
                        {
                            "start": 726,
                            "end": 849
                        },
                        {
                            "start": 850,
                            "end": 1193
                        },
                        {
                            "start": 1194,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1425
                        },
                        {
                            "start": 1426,
                            "end": 1556
                        },
                        {
                            "start": 1559,
                            "end": 1642
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 868,
                            "end": 888,
                            "matchedPaperCorpusId": "259129398"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.50439453125
                }
            ],
            "relevance_judgement": 0.50439453125,
            "relevance_judgment_input_expanded": "# Title: Preference Leakage: A Contamination Problem in LLM-as-a-judge\n# Venue: arXiv.org\n# Authors: Dawei Li, Renliang Sun, Yue Huang, Ming Zhong, Bohan Jiang, Jiawei Han, Xiangliang Zhang, Wei Wang, Huan Liu\n## Abstract\nLarge Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between the data generator LLM and the judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive and real-world problem that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge. We release all codes and data at: https://github.com/David-Li0406/Preference-Leakage.\n## Main Results\nIn our main experiment, we aim to provide insights into RQ1. Mistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA-3.3), the judge LLMs exhibit a strong preference toward their related student models, leading to large positive values in the preference leakage scores. This finding suggests that preference leakage, along with the resulting bias, is widespread in SFT when the data generator and evaluator are the same. Evaluators' bias towards certain LLMs can be inherited by its student models. From Figure 2 and Figure 3, we find an obvious preference of GPT-4o towards Mistral/ Qwen-LLaMA-3.3 and this leads to the low preference leakage score in the Mistral-GPT-4o vs Mistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA-3.3 pairs. To investigate the source of this preference, we examine whether the GPT-4 evaluator has a bias toward LLaMA series models. Using the MTBench (Zheng et al., 2023) dataset, which includes pairwise comparison judgments from both humans and GPT-4, we compare GPT-4's and human evaluators' judgments on LLaMA-2 vs other models (including Vicuna, Alpaca, GPT-3.5, and GPT-4, which are preferred by GPT-4 due to preference leakage or egocentric bias) and LLaMA-2 vs Claude. The results, shown in Figure 4, reveal a clear preference for LLaMA-2 by GPT-4. Consequently, we conclude that evaluators' bias can be inherited. In this case, GPT-4's bias toward LLaMA has been passed on to LLaMA's student models. This inheritance, combined with the opaque training data of LLMs, makes preference leakage a more complex and challenging problem. \n\nModel pairs with similar performance tend to have more  obvious preference leakage.",
            "reference_string": "[276106991 | Li et al. | 2025 | Citations: 29]"
        },
        {
            "title": "On The Persona-based Summarization of Domain-Specific Documents",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.03986, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "21724468",
                    "name": "Ankan Mullick"
                },
                {
                    "authorId": "2238668264",
                    "name": "Sombit Bose"
                },
                {
                    "authorId": "2304954421",
                    "name": "Rounak Saha"
                },
                {
                    "authorId": "19181085",
                    "name": "Ayan Kumar Bhowmick"
                },
                {
                    "authorId": "2261284157",
                    "name": "Pawan Goyal"
                },
                {
                    "authorId": "2261284171",
                    "name": "Niloy Ganguly"
                },
                {
                    "authorId": "2287821944",
                    "name": "Prasenjit Dey"
                },
                {
                    "authorId": "2247701385",
                    "name": "Ravi Kokku"
                }
            ],
            "abstract": "In an ever-expanding world of domain-specific knowledge, the increasing complexity of consuming, and storing information necessitates the generation of summaries from large information repositories. However, every persona of a domain has different requirements of information and hence their summarization. For example, in the healthcare domain, a persona-based (such as Doctor, Nurse, Patient etc.) approach is imperative to deliver targeted medical information efficiently. Persona-based summarization of domain-specific information by humans is a high cognitive load task and is generally not preferred. The summaries generated by two different humans have high variability and do not scale in cost and subject matter expertise as domains and personas grow. Further, AI-generated summaries using generic Large Language Models (LLMs) may not necessarily offer satisfactory accuracy for different domains unless they have been specifically trained on domain-specific data and can also be very expensive to use in day-to-day operations. Our contribution in this paper is two-fold: 1) We present an approach to efficiently fine-tune a domain-specific small foundation LLM using a healthcare corpus and also show that we can effectively evaluate the summarization quality using AI-based critiquing. 2) We further show that AI-based critiquing has good concordance with Human-based critiquing of the summaries. Hence, such AI-based pipelines to generate domain-specific persona-based summaries can be easily scaled to other domains such as legal, enterprise documents, education etc. in a very efficient and cost-effective manner.",
            "corpus_id": 270286247,
            "sentences": [
                {
                    "corpus_id": "270286247",
                    "title": "On The Persona-based Summarization of Domain-Specific Documents",
                    "text": "It is acknowledged that using summaries generated exclusively by GPT-4 could introduce biases inherent in its summarization capabilities, it may also be noted that alternatives, such as human evaluation, also carry their own biases.Despite the potential for bias, leveraging GPT-4 for summarization may still be a pragmatic choice, especially in scenarios access to diverse datasets or sophisticated validation methods is limited.However, in this work, we remain vigilant, recognizing the limitations inherent in both automated and humangenerated summaries, and take proactive steps such as human intervention to validate and contextualise the results to mitigate biases to the best extent possible within the given constraints.",
                    "score": 0.38202013279120905,
                    "section_title": "C GPT-4 Bias",
                    "char_start_offset": 22088,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 232
                        },
                        {
                            "start": 232,
                            "end": 430
                        },
                        {
                            "start": 430,
                            "end": 728
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.501953125
                }
            ],
            "relevance_judgement": 0.501953125,
            "relevance_judgment_input_expanded": "# Title: On The Persona-based Summarization of Domain-Specific Documents\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Ankan Mullick, Sombit Bose, Rounak Saha, Ayan Kumar Bhowmick, Pawan Goyal, Niloy Ganguly, Prasenjit Dey, Ravi Kokku\n## Abstract\nIn an ever-expanding world of domain-specific knowledge, the increasing complexity of consuming, and storing information necessitates the generation of summaries from large information repositories. However, every persona of a domain has different requirements of information and hence their summarization. For example, in the healthcare domain, a persona-based (such as Doctor, Nurse, Patient etc.) approach is imperative to deliver targeted medical information efficiently. Persona-based summarization of domain-specific information by humans is a high cognitive load task and is generally not preferred. The summaries generated by two different humans have high variability and do not scale in cost and subject matter expertise as domains and personas grow. Further, AI-generated summaries using generic Large Language Models (LLMs) may not necessarily offer satisfactory accuracy for different domains unless they have been specifically trained on domain-specific data and can also be very expensive to use in day-to-day operations. Our contribution in this paper is two-fold: 1) We present an approach to efficiently fine-tune a domain-specific small foundation LLM using a healthcare corpus and also show that we can effectively evaluate the summarization quality using AI-based critiquing. 2) We further show that AI-based critiquing has good concordance with Human-based critiquing of the summaries. Hence, such AI-based pipelines to generate domain-specific persona-based summaries can be easily scaled to other domains such as legal, enterprise documents, education etc. in a very efficient and cost-effective manner.\n## C GPT-4 Bias\nIt is acknowledged that using summaries generated exclusively by GPT-4 could introduce biases inherent in its summarization capabilities, it may also be noted that alternatives, such as human evaluation, also carry their own biases.Despite the potential for bias, leveraging GPT-4 for summarization may still be a pragmatic choice, especially in scenarios access to diverse datasets or sophisticated validation methods is limited.However, in this work, we remain vigilant, recognizing the limitations inherent in both automated and humangenerated summaries, and take proactive steps such as human intervention to validate and contextualise the results to mitigate biases to the best extent possible within the given constraints.",
            "reference_string": "[270286247 | Mullick et al. | 2024 | Citations: 3]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "269188154",
            "title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era",
            "text": "With LLMs being extensively utilized in the development of IR models, egocentric bias has emerged as a new bias during the automated evaluation conducted by LLMs [71,91,92,178], which can be defined as follows: \n\n\u2022 Definition. LLM-based evaluators prefer the responses generated by themselves or LLMs from the same family. \n\nA recent work [92] has identified that language model-driven evaluation metrics, such as BARTScore [185], T5Score [122], and GPTScore [41], inherently favor texts produced by their underlying LMs, especially in summarization tasks. Liu et al. [91] and Zheng et al. [196] further highlighted that when acting as evaluators, LLMs demonstrate a clear bias towards outputs generated by themselves over those from other models or human contributors. This bias could stem from that the LLM may share the same for both the model development phase and result evaluation phase [91]. \n\nThe emergence of egocentric bias introduces the risk of selfreinforcement for LLMs, particularly when they are further trained using rewards from LLM-based evaluations. This scenario can lead to LLMs overfitting to their own evaluation criteria, intensifying self-preference in next-generation model [91]. Current strategies for mitigating egocentric bias primarily involve employing diverse LLMs as evaluators to foster peer discussions [79], thereby reducing the preference for any specific LLM and enhancing the robustness of evaluation outcomes. However, this strategy inevitably increases the evaluation costs. Future research must explore more efficient solutions to ensure fair and unbiased evaluation.",
            "score": 0.4390897310546684,
            "section_title": "Egocentric Bias.",
            "char_start_offset": 27455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 213,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 322
                },
                {
                    "start": 325,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 898
                },
                {
                    "start": 901,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1610
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 169,
                    "matchedPaperCorpusId": "257804696"
                },
                {
                    "start": 424,
                    "end": 429,
                    "matchedPaperCorpusId": "235593404"
                },
                {
                    "start": 568,
                    "end": 572,
                    "matchedPaperCorpusId": "257804696"
                },
                {
                    "start": 590,
                    "end": 595,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "257804696"
                },
                {
                    "start": 1201,
                    "end": 1205,
                    "matchedPaperCorpusId": "257804696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84765625
        },
        {
            "corpus_id": "270878599",
            "title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models",
            "text": "To assess whether the evaluation results from GPT-4 are biased, we conduct additional human evaluations for scoring. We randomly selected 25 samples from each configuration (i.e., a column Table 4: Results of LLMs on existing bias evaluation datasets under the recognition and selection task settings. We use WB, SS, RB, and CP to represent WinoBias (Zhao et al., 2018), StereoSet (Nadeem et al., 2021), RedditBias (Barikeri et al., 2021), and CrowS-Pairs (Nangia et al., 2020), respectively. We use the micro F1 score in % as the evaluation metric, along with the RtA (Refuse to Answer) rate shown in the brackets. Results with exceptionally high RtA rates are highlighted in red, and the best results (excluding results with high RtA rates) are highlighted in green. in the table). We recruit 20 volunteers and asked each of them to assess the bias of 100 samples. In this setup, each sample is evaluated by 5 volunteers. Results are provided in Table 3, and we have the following observations: (1) Human-GPT-4 Alignment. Humans are generally aligned with GPT-4 in terms of evaluation performance in most cases. This suggests that GPT-4 could serve as a viable and reliable tool for evaluating bias in generated content. This is a significant insight, as it validates GPT-4's potential use as a scalable alternative to human evaluation, particularly when manual evaluation is costly or infeasible at large scales. \n\n(2) Lower Bias Scores in Human Evaluations. \n\nInterestingly, the bias scores from human evaluators are slightly lower than those generated by GPT-4 itself. This observation implies that GPT-4's superior performance as an evaluator does not stem from an inherent bias in favor of its own generated outputs. Instead, the slight difference between human and GPT-4 ratings could be attributed to subtle factors such as individual perspectives on bias or cultural influences, Nevertheless, the gap is small enough to indicate that GPT-4 is generally unbiased in its assessments of its own content.",
            "score": 0.4316887668867849,
            "section_title": "HUMAN EVALUATION",
            "char_start_offset": 20202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1415
                },
                {
                    "start": 1418,
                    "end": 1461
                },
                {
                    "start": 1464,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 2010
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 369,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 381,
                    "end": 402,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 415,
                    "end": 438,
                    "matchedPaperCorpusId": "235358955"
                },
                {
                    "start": 456,
                    "end": 477,
                    "matchedPaperCorpusId": "222090785"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302001953125
        },
        {
            "corpus_id": "272337179",
            "title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions",
            "text": "In our mitigation experiments, we find that gpt-4 leads to negative biases after mitigation, which require further analysis. Currently proposed mitigation approaches for reducing biases in gpt-4, specifically self-reflection, have not been found to effectively address the issue. Due to the limitation of not being able to fine-tune, our evaluation is limited to self-reflection only, further emphasizing this constraint. We also plan to analyze why gpt-4 has the highest biases as well. It is also important to note that most of our data are generated by gpt-4. Therefore, it is advisable to approach the results produced by GPT-4 with a certain level of skepticism. \n\nAdditionally, our dataset is limited to 111 scenarios, also because the number of implicit bias scenarios is scarce in the literature. In the future, we plan to create a larger dataset for implicit biases and extend the scope of biases to include factors beyond gender such as religion, race, and more.",
            "score": 0.43062769847152615,
            "section_title": "Limitations",
            "char_start_offset": 28975,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 972
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54443359375
        },
        {
            "corpus_id": "274437478",
            "title": "Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments",
            "text": "The results are presented in Table 1 and Table 2. In these tables, higher scores are colored red, and lower scores are colored blue, relative to the case where the responses are random. This color coding indicates that red scores signify less consistency and greater influence of bias, whereas blue scores suggest greater consistency and less influence of bias. Furthermore, the intensity of the color increases with the deviation from the random response case. GPT-4 is more robust against cognitive biases compared to GPT-3.5. At baseline, GPT-3.5 exhibits vulnerability to bandwagon effect, attentional bias, and verbosity bias. Conversely, GPT-4 shows reduced susceptibility to all biases. GPT-4 indicated higher resistance to bandwagon effect and verbosity bias than GPT-3.5. \n\nThe existing method proves effective for mitigating bandwagon effect and attentional bias in GPT-3.5, as well as bandwagon effect in GPT-4. This aligns with findings from previous studies. \n\nAlthough SoPro mitigates egocentric bias in GPT-3.5 and verbosity bias in GPT-4, the models' susceptibility to bandwagon effect increased. SoPro, which aligns LLMs with others' perspectives, may not be effective for cognitive biases. This result is inconsistent with the claim of the study for humans. AwaRe mitigates order bias, compassion fade, egocentric bias, bandwagon effect, and attentional bias in GPT-3.5, and bandwagon effect and verbosity bias in GPT-4. This result suggests that AwaRe prompts LLMs to make rational judgments.",
            "score": 0.42733162258228885,
            "section_title": "Results",
            "char_start_offset": 7202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 50,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 971
                },
                {
                    "start": 974,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1511
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71044921875
        },
        {
            "corpus_id": "271088509",
            "title": "Probability of Differentiation Reveals Brittleness of Homogeneity Bias in Large Language Models",
            "text": "Previous work by Lee et al. ( 2024) used gpt-3.5turbo for data collection, whereas our study used gpt-4-0125-preview. Newer models like GPT-4 often incorporate enhanced safety features and mitigation strategies to reduce bias, following advancements in algorithmic fairness and more diverse training data. To examine if these improvements contributed to diminished homogeneity bias, we conducted an ablation study using gpt-3.5-turbo. Finding evidence of bias in the ablation study would indicate that improvements in GPT-4 may explain the variations in our findings. We refer to this study as the GPT-3.5 Study.",
            "score": 0.4194128420434893,
            "section_title": "GPT-4 or GPT-3.5",
            "char_start_offset": 16083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 612
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2130126953125
        },
        {
            "corpus_id": "268296690",
            "title": "Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation",
            "text": "While GPT-4 is commonly used as a proxy for human evaluation of generation quality [47,39], we observed significant position bias issues in its output. In some cases, when flipping the position of two responses for the same pair of generations, GPT-4 yielded contradictory evaluation results. Therefore, to get a fair assessment of the responses we use a combination of GPT-4 evaluation and human labelling as follows: For each paired response comparison, we query GPT-4 twice by swapping their positions. If GPT-4 consistently judges one answer as preferred to the other, we adopt GPT-4's judgment. In cases where GPT-4 provides inconsistent judgments or declares a tie, we engage three individuals for annotations, and the majority vote among the manual annotations is considered the final evaluation. Given the expense and time-consuming nature of obtaining GPT4 and human annotations, we randomly select 200 prompts from the validation data of the Anthropic HH dataset and 100 prompts from the TL;DR dataset for evaluation. \n\nThe GPT-4 prompts and human annotation instructions used for evaluating the TL;DR summarization and Anthropic HH datasets are as follows. There was no risk to annotators.",
            "score": 0.41014941167518754,
            "section_title": "B The GPT-4 evaluation prompt and human annotation instructions.",
            "char_start_offset": 41799,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 1027
                },
                {
                    "start": 1030,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1200
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34228515625
        },
        {
            "corpus_id": "273661820",
            "title": "Self-Preference Bias in LLM-as-a-Judge",
            "text": "The results of the bias measurement using Definition 4.1 are presented in Figure 1b. It was confirmed that GPT-4 exhibits the highest self-preference bias. Definition 4.1 focuses on the recall of the LLM evaluator concerning both high and low ratings by the human evaluator. Thus, it can be concluded that GPT-4 showed lower recall in cases where humans evaluated unfavorably compared to when higher evaluating. When examining the recall values in the confusion matrix shown in Figure 2, they are calculated as 0.945 \u2248 1852 108+1852 and 0.425 \u2248 118 160+118 . The difference between these values is 0.520, which corresponds to the value reported in Figure 1b. Following GPT-4, Vicuna-13b and Koala-13b also exhibited significant bias. In contrast, other LLMs displayed values relatively close to zero. Notably, oasst-pythia-12, dolly-v2-12b, and stablelm-tuned-alpha-7b showed negative values, indicating a reverse bias where the LLMs tend to underestimate their own outputs. \n\nTable 1 presents a randomly selected example of self-preference bias in GPT-4, where humans favored the alternative response. In this example, the user query is a straightforward request to list blue items. While GPT-4 states that it lacks physical recognition before listing, GPT-3.5-Turbo directly lists the blue items without any such explanation. Both responses are of high quality, and the final evaluation reflects the evaluator's policy and stylistic preferences. Although humans and GPT-3.5-Turbo preferred the response from GPT-3.5-Turbo, GPT-4 favored its own response, illustrating a typical case of self-preference bias. \n\nFigure 2: matrix for each LLM evaluator's assessment of pairs including its own output. It suggests that some LLMs, including GPT-4, have relatively high true positive rates than true negative rate, which means that they have self-preference bias. Table 1: An example of self-preference bias in GPT-4. The user query involves a simple request to list blue items. GPT-4 responds by first acknowledging its lack of physical cognition, and then proceeds to list blue items.",
            "score": 0.4094292764179572,
            "section_title": "Result",
            "char_start_offset": 13303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 974
                },
                {
                    "start": 977,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2082
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38525390625
        },
        {
            "corpus_id": "259671923",
            "title": "How robust are egocentric and altercentric interference effects in social cognition? a test with explicit and implicit versions of a continuous false belief task",
            "text": "Study 1 revealed almost no differences between experimental and control trials (except the collapsed analyses, where deviations were negative and were not biased toward the second location); hence no evidence for egocentric or altercentric biases and did not reveal any cross-cultural differences in egocentric and altercentric biases either. \n\nThe null results found in Study 1 are difficult to interpret, and they should be evaluated with caution for two reasons. First, there was a very high dropout rate (almost 50%). Even though some of these dropouts occurred due to technical issues or timeout, many participants intentionally stopped participating without completing the study simply because they were bored due to the dull materials. We suspect that the not-so-engaging task materials might have caused our remaining participants to fail to pay enough attention to the task, which could have made the task less reliable. Secondly, the altercentric bias measure may have not been spontaneous enough to tap automatic interference effects. Possibly, with too much time, participants begin Average object location deviations in experimental and control trials across conditions and participants in Study 1. Here (and in the following studies) the binned data is presented for ease of depiction, but the analyses were run with the continous data. to reflect on and evaluate their own perspective and correct any potential spontaneous biases. Therefore, although, in theory, we expected the Sandbox task to tap altercentric biases as well as egocentric biases, this task may not be suitable to detect altercentric interferences. These two issues were addressed in the following studies.",
            "score": 0.3947514754339767,
            "section_title": "Discussion",
            "char_start_offset": 28202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1689
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0321044921875
        },
        {
            "corpus_id": "277621852",
            "title": "FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking",
            "text": "In their study, Liu et al. (2023) identified a bias in evaluators, where they tend to favor their own model's generations over those from other models, even when the latter are objectively better. Ohi et al. (2024) introduced a method for detecting this bias, which they termed 'Likelihood-based Evaluation Bias.' However, this approach requires access to the probability distribution of the LLM's generations, which is often unavailable, especially when working with commercial LLMs. Ye et al. (2024) also addressed this issue, referring to it as 'egocentric Bias,' and we adopt this terminology in our work. Their research primarily focuses on understanding the effects of this bias on performance and strategies for mitigating it. The purpose of this study is to compare the effect of ego-centric bias on our fine-grained evaluation \"FinGrAct\" and on other SOTA evaluators. In this paper, we propose a simple yet effective method for identifying this bias within the context of actionability evaluation in explainable AFC when the probability distribution of LLM generations is not available. \n\nWe identify biased samples by observing that evaluators tend to assign significantly higher scores to explanations generated by their own underlying LLMs compared to human ratings. For instance, Geval exhibits a preference for GPT-4-generated explanations, even when alternative explanations may be superior. To quantify this bias, we implement a Likert-scale scoring system ranging from 0 to 5, allowing for a tolerance of a 1-point difference between human and LLM scores. If multiple human annotators rate an explanation as 2 and the LLM assigns a 3, the sample is excluded from bias analysis. However, if the LLM scores the same explanation as 4 or 5, it is classified as ego-centric bias. Thus, a discrepancy of 2 or more points higher than the human rating serves as a clear indicator of bias. \n\nSetup: To measure ego-centric bias, each evaluator is tasked with assessing explanations generated by its underlying LLM. For instance, Prometheus evaluates Mistral-generated explanations, while G-Eval and FinGrAct evaluate GPT-4-generated explanations.",
            "score": 0.39425639707834165,
            "section_title": "Experiment 3: Ego-Centric Bias Analysis",
            "char_start_offset": 24773,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1897
                },
                {
                    "start": 1900,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2153
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 33,
                    "matchedPaperCorpusId": "257804696"
                },
                {
                    "start": 197,
                    "end": 214,
                    "matchedPaperCorpusId": "267938572"
                },
                {
                    "start": 485,
                    "end": 501,
                    "matchedPaperCorpusId": "273098639"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85986328125
        },
        {
            "corpus_id": "267760188",
            "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
            "text": "Using LLMs like GPT-4 as judges introduces potential data leakage risks due to biases in their pre-training data. This can be mitigated by selecting neutral evaluators independent of the assessed models' training data or randomly rotating evaluators to reduce bias. While GPT-4 is a powerful examiner, it has limitations, particularly in areas outside its expertise. This can be addressed by providing more contextual guidance during evaluations. In the future, training specialized evaluators to extract questions from document repositories and assess comprehension could ensure more accurate, domain-specific evaluations.",
            "score": 0.3906504774286632,
            "section_title": "Limitations",
            "char_start_offset": 24218,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 623
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51025390625
        },
        {
            "corpus_id": "270286247",
            "title": "On The Persona-based Summarization of Domain-Specific Documents",
            "text": "It is acknowledged that using summaries generated exclusively by GPT-4 could introduce biases inherent in its summarization capabilities, it may also be noted that alternatives, such as human evaluation, also carry their own biases.Despite the potential for bias, leveraging GPT-4 for summarization may still be a pragmatic choice, especially in scenarios access to diverse datasets or sophisticated validation methods is limited.However, in this work, we remain vigilant, recognizing the limitations inherent in both automated and humangenerated summaries, and take proactive steps such as human intervention to validate and contextualise the results to mitigate biases to the best extent possible within the given constraints.",
            "score": 0.38202013279120905,
            "section_title": "C GPT-4 Bias",
            "char_start_offset": 22088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 232,
                    "end": 430
                },
                {
                    "start": 430,
                    "end": 728
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.501953125
        },
        {
            "corpus_id": "270878599",
            "title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models",
            "text": "Here are several potential negative societal impacts of this work: \n\n\u2022 Reinforcement of Biases: While our CEB benchmark aims to evaluate inherent biases in LLMs to promote bias mitigation, there is a risk that the datasets might be used to inadvertently reinforce existing biases if not properly monitored. \n\n\u2022 Misinterpretation of Results: The results from our CEB benchmark could be misinterpreted or misused, leading to incorrect conclusions about the fairness and bias levels of LLMs. For example, if bias is not thoroughly detected in LLMs, it could result in misguided policy or business decisions. \n\n\u2022 Ethical Considerations in Data Construction: Using LLMs like GPT-4 to generate new evaluation datasets could raise ethical concerns, especially if the inherent bias of GPT-4 is incorporated into the generation process, inadvertently creating harmful or offensive content.",
            "score": 0.37902357098831496,
            "section_title": "E.2 SOCIETAL IMPACTS",
            "char_start_offset": 66062,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 69,
                    "end": 306
                },
                {
                    "start": 309,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 880
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.381103515625
        },
        {
            "corpus_id": "259671923",
            "title": "How robust are egocentric and altercentric interference effects in social cognition? a test with explicit and implicit versions of a continuous false belief task",
            "text": "So far, original studies of the Sandbox task have repeatedly revealed significant egocentric interference effects for both children and adults (e.g., Bernstein et al., 2011;Begeer et al., 2012;Sommerville et al., 2013;Coburn et al., 2015;Mahy et al., 2017). These positive findings have been challenged by more recent replication attempts (Samuel et al., 2018a,b), where the egocentric interference effects were either absent or may have occurred due to a general difficulty with reasoning about false representations rather than false beliefs. As an example for the latter, Samuel et al. (2018a) have found equivalent levels of egocentric bias when participants were asked to indicate where a false film would depict an object as when they were asked about a protagonist's false belief regarding the object's location. The results of the current study add to the unsuccessful replication attempts and null results. It should be noted that the current study constitutes a conceptual, rather than a direct, replication attempt. Following Machery (2020), we do not argue that one form of replication is more valuable than the other. We simply emphasize that the current study was different than the original studies in terms of the task format and visual materials (starting from Study 2); and it aimed to extend the original studies to various samples by using additional measures. \n\nBut why do some studies succeed in finding evidence for egocentric biases whereas others do not? Are there any deep and systematic differences that can explain this pattern of positive versus null findings? One such potential difference may lie in the format of the studies: These differences between in-person versus online tasks could occur due to various reasons such as videodeficit effect, which has been shown to influence children's performance on FB tasks (e.g., Rei\u00df et al., 2019) or decreased attention and motivation during online testing (see for their possible hindering effects in memory tasks, Finley and Penningroth, 2015). The two pilot studies we have conducted speak against these possibilities and extend the null results to an in-person (paperpencil) version of the Sandbox task (see Supplementary Documents). However, those pilot studies were not direct and systematic comparisons of the online versions we used, therefore they should be approached with caution.",
            "score": 0.37301557397849544,
            "section_title": "Absence of evidence or evidence of absence for egocentric bias?",
            "char_start_offset": 50725,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2213
                },
                {
                    "start": 2214,
                    "end": 2367
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 173,
                    "matchedPaperCorpusId": "777338"
                },
                {
                    "start": 173,
                    "end": 193,
                    "matchedPaperCorpusId": "26158466"
                },
                {
                    "start": 193,
                    "end": 218,
                    "matchedPaperCorpusId": "8563845"
                },
                {
                    "start": 218,
                    "end": 238,
                    "matchedPaperCorpusId": "25540555"
                },
                {
                    "start": 238,
                    "end": 256,
                    "matchedPaperCorpusId": "9301213"
                },
                {
                    "start": 575,
                    "end": 596,
                    "matchedPaperCorpusId": "53106461"
                },
                {
                    "start": 1854,
                    "end": 1872,
                    "matchedPaperCorpusId": "149127479"
                },
                {
                    "start": 1992,
                    "end": 2021,
                    "matchedPaperCorpusId": "6019107"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.029083251953125
        },
        {
            "corpus_id": "259316877",
            "title": "Situated Cameras, Situated Knowledges: Towards an Egocentric Epistemology for Computer Vision",
            "text": "Like Haraway believes there is a way to criticize bias in science without rejecting scientific knowledge, we believe there is a way to criticize bias in CV without rejecting knowledge about technical performance. Egocentric CV is naturally suited towards this reconciliation because it avoids the first god trick: egocentric images are messy: the cameras shake and scenes are often partially obscured. The images usually contain hands [2] and sometimes include other observers who have their own cameras [10]. In contrast, images taken by human photographers usually come from outside the scene they depict. They are well-framed, with objects un-occluded. The photographer can control exposure time and focal length to best represent the scene [17]. Counter-intuitively, egocentric images are often more objective, less authored, views of a scene because they avoid the god trick of the photograph. They depict the world more like it appears to a particular human, not the way a photographer believes it should be depicted. \n\nBut egocentric vision still takes part in the second god trick. We treat evaluations using quantitative metrics on benchmark datasets as true, a view from above which provides objective evaluation of the relative strengths and weaknesses of our models. However, our performance metrics are more like a photographers' camera: they are designed by CV researchers, sometimes the same researchers designing the models under evaluation. Those researchers make numerous decisions regarding the collection and curation of the data, and define what good performance means, with their own external goals and applications in mind. This approach is not objective, but that is not a bad thing! Just as there is no digital image without a camera or sensor to capture it, there is no problem statement or dataset without a human author and underlying motive. To some extent that is good -our motives for proposing vision problems ground them in reality and make their solutions valuable. But the task definition overwrites the normative perspective of the person wearing the camera with that of the researchers.",
            "score": 0.37014439913862635,
            "section_title": "Egocentric Vision Avoids the First God Trick",
            "char_start_offset": 6613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 435,
                    "end": 438,
                    "matchedPaperCorpusId": "12306448"
                },
                {
                    "start": 504,
                    "end": 508,
                    "matchedPaperCorpusId": "327191"
                },
                {
                    "start": 744,
                    "end": 748,
                    "matchedPaperCorpusId": "252915800"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1485595703125
        },
        {
            "corpus_id": "267523079",
            "title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
            "text": "Egocentric Bias. Models tend to assign higher scores to their own responses while scoring others lower (Zheng et al., 2023b;Li et al., 2024). In Figures 19 and 20, GPT-4V exhibits a slight degree of Egocentricity. Conversely, Gemini maintains a uniform scoring distribution across different sources, demonstrating a more equitable approach to judgment. In contrast, GPT-4V shows self-preference, aligning its judgments with its predefined ethical guidelines. For example, GPT-4V consistently emphasizes privacy preservation, leading to higher scores for privacy-related questions based on its own metrics. Despite efforts in prompt engineer-  ing to ensure neutrality, these models still rely on judgment criteria set during post-alignment training (Ouyang et al., 2022). This bias can result in judgments that deviate from human preferences, highlighting the complexity of aligning MLLM judgments with humans'. \n\nPosition Bias. Model consistently favor answers in specific positions, often influenced by training data that typically places correct responses at the beginning or end of prompts (Liu et al., 2023e). Figure 4 illustrates bias in LLaVA and CogVLM during Pair Comparison tasks, where they consistently prefer answers in a specific position. This bias likely arises from their limited ability to follow complex instructions, leading them to be influenced by prompt structure. For example, if a Batch Ranking prompt includes a sequence like 'ABCD', LLaVA replicates this sequence in 88.2% of responses, significantly more than other sequences. However, this bias can be reduced by introducing multiple examples, suggesting that prompts with more examples can better direct these models to follow instructions accurately. \n\nLength Bias. Models tend to prefer longer answers over concise but correct ones (Li et al., 2024), also known as verbosity bias (Zheng et al., 2023b). Figure 6 shows that both GPT-4V and Gemini assign higher scores to longer content. We conducted an expanded scoring experiment using GPT-4 (OpenAI, 2023) without vision, increasing the semantic length of answers without changing their original intent. In Figure 7, we observe noticeable score increases, with GPT-4V and Gemini showing average gains of 0.6 and 0.75 points, respectively.",
            "score": 0.3701196133142095,
            "section_title": "Bias and Hallucination",
            "char_start_offset": 16774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1731
                },
                {
                    "start": 1734,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2136
                },
                {
                    "start": 2137,
                    "end": 2271
                }
            ],
            "ref_mentions": [
                {
                    "start": 749,
                    "end": 770,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84375
        },
        {
            "corpus_id": "272826919",
            "title": "Behavioral Bias of Vision-Language Models: A Behavioral Finance View",
            "text": "Accuracy(%) Bias Index(%) Accuracy(%) Bias Index(%) Our tasks require strong visual understanding and reasoning to resist the biases human shows. We suspect that GPT4-o's larger model size, strong ability to handle high-resolution images and bettercurated training data contribute to its superior contextual understanding and mitigation of potential bias, resulting in its strong performance. The reason LLaVA-NeXT shows performance closest to GPT-4o might be its carefully curated training data, particularly for multimodal documents and chart data, and its similar approach to handling high-resolution images by splitting and resizing. Conversely, MobileVLM-V2's poor performance might be attributed to its inability to handle highresolution images due to its lightweight design. Our results call for further research to investigate these types of interdisciplinary tasks. Longer window size mitigates recency bias. As shown in Figure 3, GPT-4o maintains a bias index below 5% for all window sizes, indicating that it is almost unaffected by recency bias. Open-source models, however, are evidently influenced by recency bias to some extent. Nonetheless, our results suggest that this bias can be mitigated by using a larger window size. In general, for open-source models, an increase in window size correlates with a reduction in the bias index. Notably, LLaVA-NeXT's bias index decreases to the level of GPT-4o when window size exceeds 12. It is important to note that while the bias index decreases with larger window sizes, the accuracy does not necessarily increase. Although GPT-4o is unbiased to recency bias, its accuracy still varies with window size. We hypothesize that this phenomenon may be due to data distribution shifts in the data retrieved at different window sizes. Belief in authority during pretraining may contribute. In terms of authority bias, GPT-4o is still nearly unimpacted regardless of the window size (Figure 4). Open-source models, however, exhibit a noticeable influence of authority bias. However, there is no clear relationship among the bias index, accuracy and window size.",
            "score": 0.3640156326820973,
            "section_title": "Model Name Recency Bias Authority Bias",
            "char_start_offset": 15081,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2113
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16162109375
        },
        {
            "corpus_id": "263310448",
            "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
            "text": "We particularly focus on decoupling EGOCENTRIC and Salience, which are the most prone to having large correlations with each other (i.e. longer generations may indeed have overall higher quality generated by much stronger models).We highlight two important aspects regarding the identification of these biases: \n\n\u2022 If multiple models have a large proportion of evaluations preferring their own responses (as the evaluated pool of pairwise instances is the same for each evaluator), we reason that this may suggest \"egocentric\" qualities within involved evaluators, regardless of the objective strength of the models. Moreover, we see this effect is especially demonstrated between the more powerful models as well (GPT4 & CHAT-GPT) that suggest the presence of EGOCEN-TRIC evaluations from their disagreement. \n\n\u2022 We employ various strategies to mitigate these confounding variables and isolate each analysis as much as possible. For example, we employ a \"hierarchical\" rubric, where some biases take priority in an evaluation. Specifically, if an evaluation shows signs of order bias by choosing A in (A first, then B) and B in (B first, then A), we do not evaluate it for SALIENCE or EGOCEN-TRIC bias. \n\nTo get further insight into decoupling them, we examine additional statistics in Table 11 displaying the proportion of EGOCENTRIC samples where the model's generation was longer/shorter than the other generation. In particular, since OLMO only won once, and LLAMA never won, their EGOCEN-TRIC ratios look weird. Otherwise, we view overall that most models (9/16) exhibit a self-preference for their own generations often when their own generations exhibit longer token length. \n\nAs above, we see that SALIENCE may be associated with higher quality generations, as we see that the strongest models (GPT4, ChatGPT) often prefer their own responses when their generations are longer. Nevertheless, even in smaller models (e.g., Cohere, Koala), preference for their own generations occurs more often when they are longer. However, as we previously emphasized, if multiple models observe a self-preference for their own generations, it is difficult to associate with SALIENCE as there is disagreement that is indicative of an EGOCENTRIC bias.",
            "score": 0.36138445945346914,
            "section_title": "B.4 Decoupling Confounding Factors",
            "char_start_offset": 34264,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 310
                },
                {
                    "start": 313,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1682
                },
                {
                    "start": 1685,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2243
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79541015625
        },
        {
            "corpus_id": "276106991",
            "title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge",
            "text": "In our main experiment, we aim to provide insights into RQ1. Mistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA-3.3), the judge LLMs exhibit a strong preference toward their related student models, leading to large positive values in the preference leakage scores. This finding suggests that preference leakage, along with the resulting bias, is widespread in SFT when the data generator and evaluator are the same. Evaluators' bias towards certain LLMs can be inherited by its student models. From Figure 2 and Figure 3, we find an obvious preference of GPT-4o towards Mistral/ Qwen-LLaMA-3.3 and this leads to the low preference leakage score in the Mistral-GPT-4o vs Mistral-LLaMA-3.3 and Qwen-GPT-4o vs Qwen-LLaMA-3.3 pairs. To investigate the source of this preference, we examine whether the GPT-4 evaluator has a bias toward LLaMA series models. Using the MTBench (Zheng et al., 2023) dataset, which includes pairwise comparison judgments from both humans and GPT-4, we compare GPT-4's and human evaluators' judgments on LLaMA-2 vs other models (including Vicuna, Alpaca, GPT-3.5, and GPT-4, which are preferred by GPT-4 due to preference leakage or egocentric bias) and LLaMA-2 vs Claude. The results, shown in Figure 4, reveal a clear preference for LLaMA-2 by GPT-4. Consequently, we conclude that evaluators' bias can be inherited. In this case, GPT-4's bias toward LLaMA has been passed on to LLaMA's student models. This inheritance, combined with the opaque training data of LLMs, makes preference leakage a more complex and challenging problem. \n\nModel pairs with similar performance tend to have more  obvious preference leakage.",
            "score": 0.3594802977685757,
            "section_title": "Main Results",
            "char_start_offset": 14023,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1556
                },
                {
                    "start": 1559,
                    "end": 1642
                }
            ],
            "ref_mentions": [
                {
                    "start": 868,
                    "end": 888,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50439453125
        },
        {
            "corpus_id": "267626820",
            "title": "AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension",
            "text": "In our evaluation framework, we adopt a strategy of scoring twice by interchanging the positions of the hypothesis and reference and calculating the average of the two scores. This approach helps mitigate the bias that may arise from the positional placement. The outcomes of these two evaluations are presented in Figure 4 (c). We observe that the GPT-4 evaluator exhibits a clear bias in scoring when the hypothesis is placed before the reference. This highlights the importance of conducting a second scoring to account for addressing this bias.",
            "score": 0.3570379393967871,
            "section_title": "Ablation Study of Positional Bias",
            "char_start_offset": 26654,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 548
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34375
        },
        {
            "corpus_id": "259671923",
            "title": "How robust are egocentric and altercentric interference effects in social cognition? a test with explicit and implicit versions of a continuous false belief task",
            "text": "The present study developed a new task to test egocentric and altercentric biases, as potential indicators of explicit and implicit ToM, within the same task format. To this end, building on existing continuous explicit False Belief (Sandbox) tasks, closely matched altercentric and egocentric versions of an online task were devised. Across three studies and two different measures, we found no evidence for any bias. More formal investigation of the null results via Bayes Factors analyses yielded mostly anecdotal to moderate evidence for the null hypotheses across all studies and conditions (with minor exceptions). 10 Even though the experimental and control trials differed from each other in the collapsed analyses in Study 1, the deviations were not in the expected direction and thus do not reveal a true bias of interest. In addition, there was no evidence for cross-cultural differences (Study 1) or the effect of order of task versions administered (Study 3). In the following, 10 Following the suggestion of one anonymous reviewer, we also conducted separate one-sample Bayesian t-tests for each comparison we did in this study. \n\nIn these Bayesian t-tests, we investigated if the data supported the null hypothesis (i.e., no difference exists between experimental and control trials). \n\nFollowing Dienes (2014), we accepted the BF 10 value of 0.33 or smaller as a benchmark of a null result of sufficient sensitivity. BF 10 values of 0.33 or below suggest that the data are at least three times as likely under the null hypothesis than under the alternative. In all one-sample Bayesian t-tests analyses, we used the difference between the experimental and control trials. This strategy was adopted from Samuel et al. (2018b). Almost all of these analyses revealed anecdotal to moderate evidence for null hypothesis, with Bayes factors ranging between 0.85 and 0.12 (indicating that the data were 1.12 to 8.68 times more likely under the null hypothesis). Only two analyses (Study 1 Turkish participants and Study 2 English-speaking participants, egocentric bias condition, when all answers were included in the compared data) provided anecdotal evidence for a difference between experimental and control trials, with a Bayes Factor of 2.53 in both cases.",
            "score": 0.3567719131155451,
            "section_title": "General discussion",
            "char_start_offset": 48282,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1299
                },
                {
                    "start": 1302,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2269
                }
            ],
            "ref_mentions": [
                {
                    "start": 1312,
                    "end": 1325,
                    "matchedPaperCorpusId": "2376271"
                },
                {
                    "start": 1718,
                    "end": 1739,
                    "matchedPaperCorpusId": "56176147"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.055816650390625
        },
        {
            "corpus_id": "273185774",
            "title": "MindScope: Exploring Cognitive Biases in Large Language Models Through Multi-Agent Systems",
            "text": "Experimental Design. We sampled 10% of the data for each bias type from the static dataset and recruited three psychology graduate and PhD students for manual annotation. We ensure reliable correlation between annotators. The detailed annotation strategy can be viewed in Appendix C. \n\nEvaluation Method. We use accuracy, Pearson's coefficient, and the Kappa statistic to calculate the correlation between the evaluation results of GPT-4 and human evaluators. GPT-4 conducted assessments via interpretable zero-shot prompts, judging the presence of specific cognitive biases based on current scenarios, evaluation criteria, and the names and descriptions of biases. To ensure consistency, the temperature parameter was set to 0, and GPT-4's evaluation was repeated three times. \n\nResult analysis. The average results from three evaluations reveal a significant correlation between GPT-4 and humans in the annotation task. Notably, the average kappa statistic is 0.7180, the Pearson correlation coefficient is 0.7230, and the average accuracy is 88.08%. Specifically, the Kappa statistics for the three evaluations of GPT-4 are 0.9395, 0.9546, and 0.9402, respectively. These highly consistent statistics underscore the robustness and reliability of its assessment process. more details in Appendix C.",
            "score": 0.3546638384941569,
            "section_title": "Proficiency testing of GPT-4 as an evaluator",
            "char_start_offset": 23960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1300
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0894775390625
        },
        {
            "corpus_id": "267783118",
            "title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
            "text": "When evaluating the ground truth, GPT-4 consistently assigns marginally lower ratings across all three categories. Intriguingly, GPT-4 shows a preference for the Disliked group over the Liked group when considering the ground truth, a tendency that diverges from human inclinations. This suggests that when assessing human-composed text, such as ground truth survey articles, GPT-4 might not yet be an impeccable substitute for human discernment. Thus, in response to RQ3, we found that GPT-4 exhibits a notable preference for machine-generated texts with specific biases. Furthermore, we contend that the complete replacement of human experts by GPT-4 is a challenging prospect. For instance, human expertise remains indispensable for manual content fact checking.",
            "score": 0.35425819085813093,
            "section_title": "LLM and Human Preference",
            "char_start_offset": 13604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 765
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1396484375
        },
        {
            "corpus_id": "269293311",
            "title": "LLM Evaluators Recognize and Favor Their Own Generations",
            "text": "All three evaluators models display ordering bias.GPT-4, GPT-3.5, and Llama reverse their pairwise preferences when the ordering of options is reversed at rates of 25%, 58%, and 89% respectively, averaged across tasks and datasets (Figure 5).We account for this bias by averaging the logit-based confidence scores across the two orderings.\n\nAn alternative interpretation of the data is, for each evaluator, to discard all the results with the label \"ambiguous\" where its preference displayed ordering-based reversal, reporting an evaluator's self-recognition ability and self-preference tendency as its frequency of recognizing or preferring its own summary in \"unambiguous\" cases (Figure 5).This method exposes differences in evaluator results between the two datasets, but supports the presence of out-of-the-box self-recognition and self-preference.",
            "score": 0.35220406765437334,
            "section_title": "Alternative Adjustment for Ordering Bias",
            "char_start_offset": 10013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 50,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 339
                },
                {
                    "start": 341,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 852
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1436767578125
        },
        {
            "corpus_id": "267740522",
            "title": "Humans or LLMs as the Judge? A Study on Judgement Bias",
            "text": "Answer and Perturbation Generator GPT-4 Claude-3 \n\nGPT-4 0.07 0.08 Claude-3 0.10 0.08 As shown in Table 2, GPT-4 performs excellently in evaluating its own generated responses and those generated by Claude-3. Claude-3 also performs stably well during the evaluation process. Meanwhile, the ASR of GPT-4 on evaluating answers generated by itself on this subset is 0.07, and the corresponding result in Table 1 is 0.08. This suggests the representativeness of the sampled subset. \n\nTake-away 5. The excellence of GPT-4 and Claude-3 in factual error detection does not stem from their self-enhancement bias.",
            "score": 0.352099227543891,
            "section_title": "Judges",
            "char_start_offset": 22562,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 51,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 604
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056243896484375
        },
        {
            "corpus_id": "270614044",
            "title": "Toward Clinical Generative AI: Conceptual Framework",
            "text": "The resultant model, Med-PaLM, shows promise, yet it still does not match clinician performance even though the authors could observe that model scale and instruction prompt tuning significantly enhance comprehension, knowledge recall, and reasoning.\n\nA further risk is that LLMs might reinforce existing biases and provide inaccurate medical diagnoses, potentially leading to detrimental effects on health care.Zack et al [26] aimed to evaluate whether GPT-4 harbors biases that could influence its application in health care settings.Using the Azure OpenAI interface, the authors scrutinized GPT-4 for racial and gender biases and assessed the impact of such biases on four clinical applications of LLMs-(1) medical education, (2) diagnostic reasoning, (3) development and implementation of clinical plans, and (4) subjective patient evaluations-involving experiments using prompts mimicking typical GPT-4 use in clinical and medical educational settings and drawing from New England Journal of Medicine Healer clinical vignettes and research on implicit bias in health care.The study compared GPT-4's estimates of demographic distributions of medical conditions against actual US prevalence data.For differential diagnosis and treatment planning, the research analyzed variations across demographic groups using standard statistical methods to identify significant differences.The study revealed that GPT-4 inadequately represents demographic diversity in medical conditions, often resorting to stereotypical demographic portrayals in clinical vignettes.The differential diagnoses generated by GPT-4 for standardized clinical vignettes tended to reflect biases associated with race, ethnicity, and gender.Furthermore, the model's assessments and plans demonstrated a notable correlation between demographic characteristics and recommendations for costlier procedures, as well as varied perceptions of patients.\n\nAll this, taken together, suggests the potential role of LLMs in medicine, but human evaluations also highlight the current models' limitations, underscoring the importance of comprehensive evaluation frameworks and continued methodological advancements to develop safe, effective LLMs for clinical use.",
            "score": 0.35076293115923396,
            "section_title": "Toward Clinical LLMs: Necessity of Verifying Evidence-Based Knowledge",
            "char_start_offset": 7783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 252,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 536
                },
                {
                    "start": 536,
                    "end": 1077
                },
                {
                    "start": 1077,
                    "end": 1199
                },
                {
                    "start": 1199,
                    "end": 1380
                },
                {
                    "start": 1380,
                    "end": 1557
                },
                {
                    "start": 1557,
                    "end": 1708
                },
                {
                    "start": 1708,
                    "end": 1913
                },
                {
                    "start": 1915,
                    "end": 2218
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51806640625
        },
        {
            "corpus_id": "273661820",
            "title": "Self-Preference Bias in LLM-as-a-Judge",
            "text": "(b) compares the self-preference bias scores using our proposed metric (Definition 4.1). These figures demonstrate that GPT-4 exhibits a stronger self-preference bias than other models, suggesting that it tends to rate its own outputs more favorably than human evaluations. For detailed experimental settings, refer to Section 4. \n\nscope to specific tasks, such as text summarization or machine translation, and relied on reference-based metrics like BLEURT [Sellam et al., 2020], which does not reflect the diversity of real-world use cases. \n\nBy contrast, a pairwise evaluation approach that involves direct comparison between two texts enables evaluators to recognize specific differences more readily, resulting in more consistent human judgments. Consequently, such pairwise evaluation methods are particularly suitable for analyzing biases related to discrepancies with human evaluations. \n\nIn this paper, we measure the self-preference biases of LLMs in the pairwise evaluation. To accomplish this, we propose a new metric to quantify self-preference bias on the basis of algorithmic fairness concepts, thereby enabling discussions within the existing frameworks of fairness. In our experiment, we measured self-preference bias in eight LLMs. The results indicated that GPT-4 exhibited a significant self-preference bias (Figure 1b). This finding suggests a potential concern: using GPT-4 as a judge may lead to excessive influence from GPT-4's unique styles and policies. \n\nFurthermore, we investigated the underlying causes of self-preference bias. Although LLM evaluators are not explicitly informed whether a given text is their own, they still exhibit self-preference bias. We hypothesized that LLM evaluators might be affected by the perplexity of the text, which tends to be lower perplexity when it is generated by themselves. \n\nTo test this hypothesis, we analyzed the relationship between the perplexities of the texts to be evaluated and their corresponding evaluations. Our analysis revealed that LLMs assign significantly higher evaluations to texts with lower perplexity than human evaluators, regardless of whether the texts were self-generated. This suggests that the fundamental cause of self-preference bias may be the familiarity of the texts to the LLM evaluators, specifically how likely they are to generate the same response.",
            "score": 0.3488467975045156,
            "section_title": "Introduction",
            "char_start_offset": 2076,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1841
                },
                {
                    "start": 1844,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2167
                },
                {
                    "start": 2168,
                    "end": 2355
                }
            ],
            "ref_mentions": [
                {
                    "start": 458,
                    "end": 479,
                    "matchedPaperCorpusId": "215548699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61376953125
        },
        {
            "corpus_id": "263671602",
            "title": "Evaluating Hallucinations in Chinese Large Language Models",
            "text": "Judge once Judge 5 times Consistency rate 93.33% 93.50% \n\nTable 4: The average consistency rate between human evaluations and GPT-4 evaluations across six models. \"Juage 5 times\" refers to instructing GPT-4 to generate judgments five times, and adopting the answer that appears most frequently as the final decision. \n\nDetermining whether the answer to a question contains hallucinations poses a significant challenge for human evaluators. Relying on human evaluation as a fair and scalable automated assessment method is not feasible, which in turn limits the usability of datasets. In recent, many work adopt AI feedback from some powerful instructionfollowing large language model like GPT-3.5 and GPT-4 for training and evaluation (Cheng et al., 2023;Zheng et al., 2023a;Li et al., 2023b;Fu et al., 2023). Besides, Wang et al. (2023b) found that using LLM-based evaluator for open-domain QA evaluation is better than other methods. The evaluation of TruthfulQA also employed models as scorers, which were achieved by fine-tuning two 6.7B GPT-3 models on data collected by the authors. We believe that we can use LLM-based evaluators to replace such fine-tuning methods. In our benchmark, we use GPT-4 (gpt-4-0613) as the evaluator. \n\nDuring evaluation, we put our criteria into the instruction for GPT-4. And we give GPT-4 correct answer examples for reference. The specific format of the evaluation prompt is in Appendix D.2. Due to the inability of GPT-4 to access top logits and to produce deterministic outputs, we employ GPT-4 to generate five judgments for voting and use the result with the highest number of votes as the final judgment and we set the temperature to 0 and top p to 0.5. \n\nWe conducted experiments to assess the consistency between GPT-4's evaluation results and human evaluation results, and evaluated the impact of GPT-4's randomness on the consistency rate. In particular,we sampled two questions from each domain of the three parts, totaling 100 questions.",
            "score": 0.34759914057730246,
            "section_title": "EVALUATION METHOD",
            "char_start_offset": 17369,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 58,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 316
                },
                {
                    "start": 319,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1235
                },
                {
                    "start": 1238,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1697
                },
                {
                    "start": 1700,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1987
                }
            ],
            "ref_mentions": [
                {
                    "start": 735,
                    "end": 755,
                    "matchedPaperCorpusId": "258460987"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03192138671875
        },
        {
            "corpus_id": "267094801",
            "title": "Red Teaming Visual Language Models",
            "text": "This section primarily tests the safety of responses generated by the model, refraining from producing content that poses ethical risks and social harm. We construct 4 sub-tasks. \n\nPolitics & Race Selected from the Crowd Activity Dataset (Wang et al., 2022)  background. Then, we generate similar annotations using the self-instruct method mentioned above. We assess the VLM's fairness in terms of gender and race by evaluating with GPT-4 for any underlying bias, such as lower educational qualifications and more junior job roles for certain races. We used GPT-4 instead of GPT-4V here to prevent the image input from introducing GPT-4V's own potential bias.",
            "score": 0.34121984178562226,
            "section_title": "Safety",
            "char_start_offset": 7760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 178
                },
                {
                    "start": 181,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 659
                }
            ],
            "ref_mentions": [
                {
                    "start": 238,
                    "end": 257,
                    "matchedPaperCorpusId": "247762347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1351318359375
        },
        {
            "corpus_id": "265609311",
            "title": "Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies",
            "text": "Each approach uncovered different stereotypes that were not found using the other. Another alternative is to use a mixture of bias metrics to evaluate LLMs instead of just one. \n\nThe most recent LLMs, such as GPT-4 and Llama 2, have shown incredible capabilities compared to the earlier models, with researchers speculating the possibility of these models becoming part of the solution to tackling the bias problem [25,199]. Initial experiments of GPT-4 are shown to be more trustworthy and not strongly biased for most stereotyped topics when compared to earlier GPT models [199], and GPT-4 could provide a text completion for prompts with commentary on the possible offensiveness of its generation [25]. Although it is unclear the extent to which these capabilities can be utilised to tackle the bias problem or self-correct biases, [199] warns that GPT-4 models' ability to follow instructions more precisely can be used maliciously to manipulate the outputs. There is a need for future research to identify the benefits and risks of the most recent huge LLMs before using them directly as a way to tackle the bias problem. \n\nThe role of governance and laws can also help shape notions of bias more broadly. The risk requires broader concerted action between policy-makers, civil society, and other stakeholders to be mitigated. Moreover, the importance of an inclusive, cross-disciplinary and cross-cultural community, including technical and socio-technical AI researchers, civil society organisations, policy-makers, product designers, affected societies and the wider public, is highlighted in several studies. \n\nBias detection and mitigation is an ongoing process, and it is essential to regularly monitor the model for any new sources of bias that may emerge. This can be achieved by developing automated monitoring systems that flag potential bias in real time and regular audits of the model's performance.",
            "score": 0.3398316753702882,
            "section_title": "DISCUSSIONS",
            "char_start_offset": 74393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 176
                },
                {
                    "start": 179,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1917
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2086181640625
        },
        {
            "corpus_id": "270123866",
            "title": "Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding",
            "text": "We utilize GPT-4 as a proxy for human evaluation.First, we sample a set of questions and corresponding answers generated by two methods.To mitigate positional bias, we then randomize the order of the answers within each pair.The question along with two answers is subsequently formatted according to the predefined GPT4 input template: Given the following question and two candidate answers, please choose which one is better, considering accuracy, coherence, logicality, factuality, relevance, and information completeness.\\n \\n",
            "score": 0.33913125236992264,
            "section_title": "G Evaluation of Generation Quality",
            "char_start_offset": 28066,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 49,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 524
                },
                {
                    "start": 524,
                    "end": 529
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1353759765625
        },
        {
            "corpus_id": "267587759",
            "title": "Generative Pre-Trained Transformer (GPT) in Research: A Systematic Review on Data Augmentation",
            "text": "Existing reviews on GPT [19][20][21] did not address how GPT could be useful for researchers in generating or augmenting research-related data and analyzing it. To mitigate this gap, this methodically crafted literature offers a strategic focus on data augmentation, backed by a meticulous examination of 412 scholarly works. In conclusion, the practical contributions of this comprehensive literature review are paramount in guiding researchers towards the judicious integration of GPT and associated technologies in their scholarly pursuits. By meticulously distilling 77 selected research contributions and developing a rigorous classification framework for \"GPT's use on research data\", this study provides a nuanced understanding of the multifaceted applications of GPT in data augmentation, critical analysis, and research design. Researchers can leverage the findings to inform their approach to generating and processing research data, analyzing complex datasets, and enhancing research design and problem-solving. Moreover, the systematic comparison of 54 extant literary works, evaluating diverse research domains, methodological approaches, and associated advantages and disadvantages, offers a practical roadmap for scientists seeking to seamlessly integrate GPT across various phases of their academic endeavors, thereby fostering innovation and efficiency in scholarly pursuits. \n\nThe deployment of GPT in research is not immune to inherent limitations, notably encompassing the issues of ethics [6], biases [7], hallucinations [83] and sycophantic behavior [73]. GPT, while proficient at generating human-like text, is susceptible to generating content that may be speculative or diverge from factual accuracy, leading to hallucinations within the generated information [83]. Furthermore, the model may exhibit sycophantic tendencies, showcasing an inclination to excessively praise or flatter, potentially compromising the objectivity and reliability of the generated output [73]. The manifestation of hallucinations and sycophantic behavior raises concerns about the model's capacity to maintain a rigorous and unbiased approach in generating content for research purposes, necessitating careful scrutiny and consideration of these limitations in the utilization of GPT within the academic realm. \n\nFuture studies could explore refining GPT through advanced training techniques to minimize bias, and hallucinations and enhance content accuracy.",
            "score": 0.3387906499878044,
            "section_title": "Conclusions",
            "char_start_offset": 50409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1392
                },
                {
                    "start": 1395,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2313
                },
                {
                    "start": 2316,
                    "end": 2461
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 28,
                    "matchedPaperCorpusId": "260209946"
                },
                {
                    "start": 1510,
                    "end": 1513,
                    "matchedPaperCorpusId": "258701329"
                },
                {
                    "start": 1522,
                    "end": 1525,
                    "matchedPaperCorpusId": "260713371"
                },
                {
                    "start": 1542,
                    "end": 1546,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 1785,
                    "end": 1789,
                    "matchedPaperCorpusId": "246652372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2274169921875
        },
        {
            "corpus_id": "274515216",
            "title": "EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios",
            "text": "GPT: Yes, they are matched. GPT-4 and InternVL-1.5 are then employed to verify whether each candidate frame is qualified. In this example, the selected candidate frame contains all objects necessary for the next action, fulfilling the second criterion. However, since InternVL-1.5 can correctly predict the upcoming action without historical task progress information, this frame fails to meet the first criterion and should therefore be discarded. \n\nscenarios reflective of real-world human life, b) an egocentric perspective, and c) a focus on evaluating planning tasks. \n\nOur methodology begins with the collection of a comprehensive set of egocentric videos that cover 4 fundamental domains of human life, contributing to the properties of egocentric perspective and diverse scenarios of EgoPlan-Bench2. In terms of the last principle, we design a semi-automatic dataset construction pipeline to generate high-quality QA pairs focusing on planning tasks. Finally, we provide the detailed data statistics of EgoPlan-Bench2.",
            "score": 0.33850218617580446,
            "section_title": "You: The next action is to give cashier money. Tell me whether the description of \"pay cashier\" and the next action match.",
            "char_start_offset": 15284,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05865478515625
        },
        {
            "corpus_id": "273234268",
            "title": "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users",
            "text": "Specifically, we compare the fairness degradation of GPT series LLMs on BBQ, contrasting the No-RAG baseline with RAG-based LLMs that utilize unfair data (unfairness rate of 1.0) as shown in Fig. 5. We observe a slight decrease in fairness regarding prominent biases, such as race-ethnicity and sexual orientation. However, for less prominent bias categories like religion and age, there is a more significant drop in fairness after applying RAG. This suggest that GPT series LLMs' alignment efforts focus more on widely recognized biases, with less attention given to underrepresented categories. This finding aligns with prior research [14]. Full results are provided in Appendix E. Remark 4.1. The fairness of LLMs can be significantly compromised through RAG when using uncensored datasets. \n\nAs the level of uncensorship increases, fairness decreases more sharply, posing serious risks to model alignment. This is especially concerning given the substantial asymmetry in alignment efforts: despite OpenAI's commitment to allocating 20% of its computational resources to alignment [54,14], fairness can still be easily undermined through RAG without any additional fine-tuning or retraining. Given the practical scenario discussed in Sec. 3.2, it is critical to assess whether mitigating bias in one specific category is sufficient on its own. More broadly, we explore whether bias in one category (RAG bias category, RC) affects fairness in another category (test bias category, TC) with RAG-based LLMs. To investigate this, we create partially censored datasets where unfair samples from one RC (with a 1.0 unfairness rate) are combined with fair samples from one TC (with a 0.0 unfairness rate). We then measure the impact of the biased RC on the TC by comparing RAG with partially biased data against RAG with fully censored data (clean RAG). The difference in fairness scores allows us to quantify how bias in the RC impacts fairness in TC.",
            "score": 0.33467090852586834,
            "section_title": "Generation",
            "char_start_offset": 23129,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1949
                }
            ],
            "ref_mentions": [
                {
                    "start": 638,
                    "end": 642,
                    "matchedPaperCorpusId": "263671523"
                },
                {
                    "start": 1089,
                    "end": 1092,
                    "matchedPaperCorpusId": "263671523"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.184326171875
        },
        {
            "corpus_id": "276079993",
            "title": "The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating Reward Hacking",
            "text": "Evaluation Data. \n\nFor general dialogue task, we use in-distribution data (test set of Anthropic-HH (Bai et al., 2022), including Anthropic-Helpful and Anthropic-Harmless datasets) and out-of-distribution data (test set of AlpacaFarm (Dubois et al., 2024)). For summarization task, the Reddit TL;DR test set (Stiennon et al., 2020) is used. \n\nGPT-4 Evaluation. We evaluate EPPO by comparing the win ratio of RLHF model responses under EPPO against baselines. GPT-4 is adopted for assessment due to its strong alignment with human evaluations (Chen et al., 2023;Zheng et al., 2024). To mitigate positional bias (Wang et al., 2018), each sample pair is evaluated twice with reversed order. We use the GPT-4 prompt with the highest human agreement in AlpacaEval (Li et al., 2023), as detailed in Appendix J.3. To ensure the reliability of our experiments, we also adopt Claude-3.5-Sonnet and human as evaluators in Appendix F. \n\nGPT-4 Identification of Hacking Samples. To explore the relationship between energy loss and reward hacking, we use AI feedback to identify hacking samples following Miao et al. (2024). Specifically, we first outline common hacking behaviors (Coste et al., 2024;Zhai et al., 2023) and then use GPT-4 to evaluate responses based on these criteria, with prompts detailed in Appendix J.3 and over 95% human agreement validated in Appendix I.",
            "score": 0.3345392941707101,
            "section_title": "Setup",
            "char_start_offset": 16107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 19,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1364
                }
            ],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 255,
                    "matchedPaperCorpusId": "258865545"
                },
                {
                    "start": 308,
                    "end": 331,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 542,
                    "end": 561,
                    "matchedPaperCorpusId": "261881796"
                },
                {
                    "start": 561,
                    "end": 580,
                    "matchedPaperCorpusId": "264289051"
                },
                {
                    "start": 610,
                    "end": 629,
                    "matchedPaperCorpusId": "21054674"
                },
                {
                    "start": 1092,
                    "end": 1110,
                    "matchedPaperCorpusId": "267657799"
                },
                {
                    "start": 1168,
                    "end": 1188,
                    "matchedPaperCorpusId": "263620686"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07476806640625
        },
        {
            "corpus_id": "264591429",
            "title": "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation",
            "text": "\u2022 Self-enhancement Bias Reduction.Table 2 evaluates self-enhancement bias by comparing BSM (with zero-shot GPT-4) for the samples where one of the responses is also generated by GPT-4.We observe a 3% better correlation with humans, suggesting that BSM improves evaluation even when the LLM judges its own outputs.\n\nBSM not only leads to an improvement in overall LLM-human agreement (as per the 'Ag' metric) but also on the fraction of samples where one response is generated by the same evaluator LLM (as per the 'SB' metric), thus pointing to its robustness as an evaluation method.In summary, BSM improves both correctness and consistency of LLM-based evaluators.\n\nBSM improves upon all zero-shot base LLMs.We demonstrate the generalizability of BSM as an LLM program by implementing it on top of four dif- We compare LLaMA-2-70B-chat BSM with the baseline zero-shot method, and also report GPT-4 results.BSM obtains significant improvements over the LLaMA baseline, and matches or is close to GPT-4 agreement in three of the four domains, while sometimes outperforming GPT-4 in reducing biases.\n\nferent base LLMs, ranging from LLaMA-2-7B to GPT-4.As shown in Table 3, BSM improves agreement with humans for all base LLMs, compared to a zero-shot baseline.Even though zero-shot GPT-4 is the state-of-the-art LLM-based evaluator, applying BSM obtains a further improvement of 3%.Moreover, applying BSM to LLaMA-2-70Bchat makes it competitive with GPT-4 for turn-1 questions.BSM also significantly reduces position and length biases for all models except for GPT-4.",
            "score": 0.3344188913081778,
            "section_title": "Main Results",
            "char_start_offset": 22039,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 34,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 313
                },
                {
                    "start": 315,
                    "end": 584
                },
                {
                    "start": 584,
                    "end": 666
                },
                {
                    "start": 668,
                    "end": 710
                },
                {
                    "start": 710,
                    "end": 908
                },
                {
                    "start": 908,
                    "end": 1098
                },
                {
                    "start": 1100,
                    "end": 1151
                },
                {
                    "start": 1151,
                    "end": 1259
                },
                {
                    "start": 1259,
                    "end": 1381
                },
                {
                    "start": 1381,
                    "end": 1476
                },
                {
                    "start": 1476,
                    "end": 1566
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.513671875
        },
        {
            "corpus_id": "269484364",
            "title": "MetaRM: Shifted Distributions Alignment via Meta-Learning",
            "text": "To evaluate the effectiveness of our method, we assess it by comparing its win rate with other baselines.Specifically, we randomly select 100 prompts from the test datasets and generate the responses from our method and baselines, respectively.We then provide these pairs of prompts and responses to human evaluators, asking them to determine which response is of higher quality, more useful, and harmless.During the entire evaluation process, the human evaluators are unaware of the responses' sources.Additionally, some studies indicate that GPT-4's evaluation of the responses aligns closely with that of human evaluators (Chang et al., 2023;Zheng et al., 2023a).Meanwhile, GPT-4 is noted for being more cost-effective and efficient compared to human evaluators, while also offering greater consistency in evaluation results (Zheng et al., 2023c).So we also utilize GPT-4 to evaluate the performance of MetaRM against other baselines.To mitigate the impact of irrelevant bias on GPT-4 evaluations such as response length and position, we randomly assign the order of the responses in GPT-4 evaluation prompts.The GPT-4 prompts for evaluation can be found in Appendix A.2.This indicates that our method can maintain the RM's ability to modeling human preferences in the gradient descent, while making it adapt to the new distribution by using the meta-process.",
            "score": 0.33302930790583324,
            "section_title": "Metrics & Evaluation",
            "char_start_offset": 17254,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 105,
                    "end": 244
                },
                {
                    "start": 244,
                    "end": 406
                },
                {
                    "start": 406,
                    "end": 503
                },
                {
                    "start": 503,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 937
                },
                {
                    "start": 937,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1174
                },
                {
                    "start": 1174,
                    "end": 1362
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2098388671875
        },
        {
            "corpus_id": "273661820",
            "title": "Self-Preference Bias in LLM-as-a-Judge",
            "text": "In this study, we propose a metric to quantify the self-preference bias in LLM-as-a-judge and measured the selfpreference bias of eight LLMs. Experimental results confirmed that GPT-4, in particular, exhibits a high self-preference bias. This finding suggests a risk that GPT-4 as a judge may inadvertently reinforce its own style and policies. Furthermore, we hypothesized that the self-preference bias is related to the perplexity of the texts, and showed that, compared to human evaluators, LLM evaluators assigned higher evaluations to texts with lower perplexity, and this tendency was observed regardless of whether the text was generated by themselves or not. This suggests that the essence of the bias lies in perplexity and that the self-preference bias exists because LLMs prefer texts more familiar to them.",
            "score": 0.33226129614872224,
            "section_title": "Conclusion",
            "char_start_offset": 23978,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 818
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5224609375
        },
        {
            "corpus_id": "265316675",
            "title": "AI USEFULNESS IN SYSTEMS MODELLING AND SIMULATION: GPT-4 APPLICATION",
            "text": "However, these biases represent the existing mental models about a complex problem in the available training data. The GPT algorithm cannot understand human prompts or its own generated responses. The AI and NLP algorithms derive an understanding from a vast amount of text data without considering or reflecting on its quality. This may lead to incorrect answers or biased responses [20,21]. \n\nGPT-4 may provide useful identification and explanation of the problem (system) elements and their relationships, including the underlying logic behind the responses. However, the 'Show me' performs slightly worse in constructing causal loop diagrams, but can be improved with 'Code interpreter'. Here, the main problem may be occasionally providing incorrect polarities [20]. GPT-4 is good at replying to factual questions, even with slight variances, by looking up the main data points used for training the system. Response quality and usefulness decrease when questions require more intelligent interpolation between data points [22].",
            "score": 0.3317501600377395,
            "section_title": "The potential of GPT-4 in system dynamics",
            "char_start_offset": 12101,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 392
                },
                {
                    "start": 395,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1033
                }
            ],
            "ref_mentions": [
                {
                    "start": 388,
                    "end": 391,
                    "matchedPaperCorpusId": "257799222"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1246337890625
        },
        {
            "corpus_id": "273653866",
            "title": "LLMs are Biased Evaluators But Not Biased for Retrieval Augmented Generation",
            "text": "To address our primary research question, we confined our analysis to truthful versions of passages, mirroring the setting in section 4.1.1. The results, illustrated in Figure 3, reveal two key findings: 1) Both GPT and LLaMA demonstrate approximately a neutral self-preference across NQ and MARCO datasets. 2) Two notable exceptions were observed: First, LLaMA tends to prefer humanwritten content over model-generated content for both datasets. Second, GPT shows a preference for LLaMA-generated content for the MARCO dataset. \n\nThese findings are noteworthy as they diverge from previous studies on self-evaluation biases in LLMs. In our generation settings within the RAG framework, the models exhibited a markedly lower degree of bias, with this effect being particularly pronounced for the GPT model. This suggests that the RAG framework may mitigate some of the selfpreference biases observed in other contexts.",
            "score": 0.33097469229350435,
            "section_title": "Self-Preference Tendency",
            "char_start_offset": 19652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 918
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1685791015625
        },
        {
            "corpus_id": "22918663",
            "title": "Error rate on the director's task is influenced by the need to take another's perspective but not the type of perspective",
            "text": "The number of egocentric errors made by participants in the ambiguous trials varied depending on the experimental condition; for these trial types the mean number of errors was 10.42% (s.d. = 20.78%) in the L1 task, 3.50% (s.d. = 13.27%) in the L2 task and 1.56% (s.d. = 7.65%) in the Rule task (Kruskal-Wallis test, \u03c7 2\n\n(2) = 6.49, p = 0.039). More egocentric errors were made in the L1 task than in the Rule task (Mann-Whitney U-Test: W = 441, p = 0.03). However, we found no significant difference between the error rates in the L2 task and the Rule task (Mann-Whitney U-Test: W = 311.5, p = 0.58). Moreover, error rates were similar across the two perspective-taking versions of the task (Mann-Whitney U-Test: W = 444, p = 0.083).",
            "score": 0.33092007672030155,
            "section_title": "Ambiguous trials",
            "char_start_offset": 18283,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 216,
                    "end": 217,
                    "matchedPaperCorpusId": "18087684"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0299224853515625
        },
        {
            "corpus_id": "259671923",
            "title": "How robust are egocentric and altercentric interference effects in social cognition? a test with explicit and implicit versions of a continuous false belief task",
            "text": "Theoretically, this bias could thus reflect more unambiguously implicit ToM processes than the standard implicit FB tasks (i.e., VoL, AL, or interactive tasks). And from a methodological perspective, altercentric bias tasks have several potential advantages over typical implicit FB tasks. For instance, they can provide more fine-grained, continuous measures of implicit ToM (participants can be more or less subject to altercentric interference). \n\nIn addition, altercentric bias measures are particularly interesting and promising from a methodological point of view: they allow researchers to construct structurally analogous tasks to tap implicit and explicit ToM within one task format such that the two types of tasks differ merely with regard to the critical test question. On the one hand, in implicit versions employing altercentric bias, participants are asked to make a factual judgment about the world (e.g., How many dots are there? / Where is an object?) in the presence of an irrelevant agent who does or does not share their perspective. If participants are slower or more error-prone in their own factual judgments when the other agent has a deviant perspective, this indicates altercentric bias. On the other hand, the explicit versions exploit the so-called egocentric bias, which refers to the influences of one's own knowledge when judging others' perspectives. In the explicit versions, participants are asked about the other agent's perspective or behavior (e.g., How many dots does the agent see? / Where will the agent look for the object?). If participants become slower or more error-prone in these perspective judgments when their own perspective is different from the agents' , this indicates egocentric bias. This bias could then be used to infer the explicit ToM ability of participants: more interference from one's own perspective -even if the task asks to take other's perspective-means poorer ToM. \n\nSo far, these two biases have been implemented together in the so-called Dot Perspective Task (Samson et al., 2010). In this task, adult participants were asked to judge the number of dots presented in a scene either from their own perspective (SELF condition) or as seen by an on-screen avatar (OTHER condition). Each condition featured two types of trials: consistent versus inconsistent.",
            "score": 0.33034720882131113,
            "section_title": "Introduction",
            "char_start_offset": 7795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1933
                },
                {
                    "start": 1936,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2249
                },
                {
                    "start": 2250,
                    "end": 2326
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1177978515625
        },
        {
            "corpus_id": "273821559",
            "title": "Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models",
            "text": "CoBBLEr Recent studies have found that LLM-as-a-Judge often exhibits cognitive biases, such as preferences for verbosity, egocentrism, bandwagon, and an overly authoritative tone (Wang et al., 2024a;Koo et al., 2024;Zheng et al., 2024;Chen et al., 2024). To investigate the biases of the compared models, we evaluate them on the CoB-BLEr benchmark (Cognitive Bias Benchmark for LLMs as EvaluatoRs) (Koo et al., 2024). This dataset is designed to evaluate the quality and reliability of LLMs when used as automated evaluators in a question-answering (QA) setting. \n\nIt assesses the presence of six cognitive biases, both implicit and induced, when LLMs are tasked with ranking responses generated by various other models. CoBBLEr's core objective is to identify the extent of bias in LLM evaluation outputs. \n\nThe CoBBLEr dataset includes 50 QA instructions, randomly selected from two well-established benchmarks: BIGbench (bench authors, 2023) and ELI5 (Fan et al., 2019). 16 LLMs, both open and closed-source models, generate responses to these instructions. The evaluations involve pairwise comparisons between the responses of two models, wherein each model also acts as an evaluator to rank its own and others' outputs. The biases tested are categorized into two groups: (1) Implicit biases, such as egocentric bias (where a model tends to prefer its own outputs), and \n\n(2) Induced biases, such as order bias, where the ranking of responses is influenced by their order in the evaluation. \n\nTable 6 presents the performance of the compared models on the CoBBLEr benchmark with metrics of order bias, bandwagon effect, compassion, selective bias, salience, distraction, and frequency. Note that lower scores indicates better performance (i.e., fewer biases) on CoBBLEr. Overall, our REC-70B model has the best average performance with a score of 0.2141, followed closely by GPT-4 (0.2279) and GPT-4o (0.2349).",
            "score": 0.33014697618592426,
            "section_title": "Bias Testing",
            "char_start_offset": 22776,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1494
                },
                {
                    "start": 1497,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 199,
                    "matchedPaperCorpusId": "258960339"
                },
                {
                    "start": 199,
                    "end": 216,
                    "matchedPaperCorpusId": "263310448"
                },
                {
                    "start": 216,
                    "end": 235,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 398,
                    "end": 416,
                    "matchedPaperCorpusId": "263310448"
                },
                {
                    "start": 954,
                    "end": 972,
                    "matchedPaperCorpusId": "196170479"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.294677734375
        },
        {
            "corpus_id": "272827130",
            "title": "Failures in Perspective-taking of Multimodal AI Systems",
            "text": "Level 1 GPT-4o performed with near-perfect accuracy on 6 out of the 8 image angles (Figure 2). Its poor performance on 0\u00b0images is likely due to an accidental viewpoint where the avatar blocked one of the shapes. However, poor performance on 315\u00b0image types is less interpretable, especially in contrast to GPT-4o's impressive performance on 45\u00b0images, which have the same angular perspective. Level 2 Spatial & Visual Judgments As previously mentioned, human response times increase on perspective-taking tasks as the angular difference between the target and observer increases [12]. We administered the task to a small number of human participants-part of a larger, IRB-approved study-and replicated this effect with both our stimuli types, finding a bell-shaped curve in the relationship between response time and angle. Response times peaked when the target required a full mental rotation (180\u00b0), as seen in the green line in Figure 3. Error bars were calculated by taking the standard error of all trials from each participant. As expected, GPT-4o struggled with the task when mental rotation was involved, beginning around a 90\u00b0angular difference. Interestingly, in both tasks, GPT-4o exhibited a response bias toward either \"left\" or \"6\" or \"W\" when the angular difference of the avatar is 90\u00b0or 135\u00b0in either direction. This likely reflects uncertainty from an egocentric perspective, and thus, a default to one response over another. Chain of Thought GPT-4o performance significantly improved with chain-of-thought prompting on 180\u00b0stimuli (Figure 4). However, this linguistic strategy did not improve the model's ability to handle intermediate rotations between 90\u00b0and 180\u00b0. This suggests that while language can convey some level of spatial information, it lacks the precision required for human-level spatial cognition. This demonstration of surface-level perspective-taking abilities can partially explain how multimodal models achieve high performance on certain spatial benchmarks. \n\nFigure 4: GPT-4o performance on Level 2 spatial-judgment stimuli with chain-of-thought prompting.",
            "score": 0.32879136944120274,
            "section_title": "Results",
            "char_start_offset": 7543,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1998
                },
                {
                    "start": 2001,
                    "end": 2098
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1810302734375
        },
        {
            "corpus_id": "267637243",
            "title": "Addressing cognitive bias in medical language models",
            "text": "We demonstrate the results of three mitigation strategies: (1) bias education, (2) one-shot bias demonstration, and (3) fewshot bias demonstration (see Appendix B for details). For bias education, the model is provided with a short warning educating the model about potential cognitive biases, such as the following text provided for recency bias: \"Keep in mind the importance of individualized patient evaluation. Each patient is unique, and recent cases should not overshadow individual assessment and evidence-based practice.\" \n\nOne-shot bias demonstration includes a sample question from the MedQA dataset accompanied by a bias-inducing prompt. It also presents an example response that incorrectly selects an answer based on the bias from the prompt, which we refer to as a negative example. Before this incorrect answer, the model is presented with: \"The following is an example of incorrectly classifying based on [cognitive bias].\" \n\nFor the few-shot bias demonstration strategy, both a negative and a positive example are provided as part of the prompt. The negative example is the same as was shown in the oneshot bias demonstration, and the positive example is presented as follows: \"The following is an example of correctly classifying based on [cognitive bias],\" together with a correct classification. \n\nThe results of each bias mitigation strategy are presented in Tables 2-4 and graphically depicted in Figure 3. In comparing these three strategies, it is evident that different models respond differently to various mitigation techniques. gpt-4 consistently shows the highest level of improvement across all strategies. The other models, while showing some level of improvement, do not match gpt-4. This suggests that the architecture and training of gpt-4 might be more robust to bias-related feedback. \n\nBias education: The strategy of educating models about cognitive biases yielded the most significant improvements in gpt-4. For instance, in the \"Frequency\" bias category, its accuracy improved from 0.627 to 0.720. However, other models like mixtral-8x7b and gpt-3.5 displayed only marginal improvements.",
            "score": 0.32833687476345497,
            "section_title": "Bias mitigation strategies",
            "char_start_offset": 17033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1820
                },
                {
                    "start": 1823,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2127
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6650390625
        },
        {
            "corpus_id": "271956682",
            "title": "Uncovering Biases with Reflective Large Language Models",
            "text": "Our experimental framework aims to assess the feasibility of both detecting biases in textual content and implementing effective mitigation strategies. The first experiment focuses on bias detection, while the second explores the generation of balanced textual outputs as a corrective measure, moving beyond the limitations of prior studies that primarily focused on identification (Section 2). \n\nTo establish a baseline, we used Claude and GPT-4 to generate initial results. For experimenting with EVINCE, we used two instances of GPT-4, as Claude appeared prone to easily shifting its predictions (discussed shortly). We utilized GPT-4 via OpenAI API on Microsoft Azure, setting the temperature to 0.1 with maximum token size. The cost is around US$1,000.",
            "score": 0.32820207266047285,
            "section_title": "Experiments",
            "char_start_offset": 17460,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 394
                },
                {
                    "start": 397,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 757
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328125
        },
        {
            "corpus_id": "267035049",
            "title": "GPT4Ego: Unleashing the Potential of Pre-Trained Models for Zero-Shot Egocentric Action Recognition",
            "text": "We conducted extensive quantitative and qualitative experiments on three public egocentric benchmarks, including EPIC-KITCHENS-100 [14], EGTEA [55], and EGTEA [55], to evaluate the effectiveness of the proposed method. In this section, we first illustrate the dataset and metrics in Section IV-A. Then, we will compare with state-of-the-art methods to illustrate the performance of GPT4Ego in Section IV-C. The implementation details, including Encoder, EgoTP\u2660, and EgoVP\u2663, are presented in Section IV-B. After that, we perform a set of ablation studies in Section IV-D, to discuss the effect of each component, each text prompt, and the number of visual concepts. Lastly, the qualitative analysis is demonstrated in Section IV-E.",
            "score": 0.3276843530888929,
            "section_title": "IV. EXPERIMENTS",
            "char_start_offset": 23010,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 730
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "52233948"
                },
                {
                    "start": 159,
                    "end": 163,
                    "matchedPaperCorpusId": "52233948"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0728759765625
        },
        {
            "corpus_id": "273097994",
            "title": "Collective Critics for Creative Story Generation",
            "text": "In the ablation study, the automatic evaluation employed GPT-4 with a temperature setting of 0, using the GPT-4 automatic evaluation prompt from (Wang et al., 2022a). It's important to note that the outcomes of GPT-4's automatic evaluation can be significantly affected by the order in which content is presented and may exhibit instability. To mitigate this, two-story plans is presented to GPT-4 in a random order for evaluation. The pairwise evaluation prompt is detailed in Table 19. \n\nTo verify the reliability of GPT-4's automatic evaluation and confirm its alignment with human judgments, we compare the human evaluation re- sults from Section 4 with the automatic evaluation results. In Section 4, three annotators evaluate 300 stories, and we calculate the Cohen's Kappa score between each annotator's evaluations and GPT-4's automatic evaluation of the same stories. As shown in Table 20, we measure each annotator's Cohen's kappa score and found a fair to moderate level of agreement, indicating the reliability of GPT-4's automatic evaluations. The results show that both human and GPT-4 evaluations exhibit similar trends with lower kappa scores for \"Relevant\" due to data bias, where most annotators rated options as 'good', suggesting alignment between GPT-4's automatic evaluations and human judgments.",
            "score": 0.3273077195638239,
            "section_title": "B.2 Automatic Evaluation Details.",
            "char_start_offset": 38730,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1318
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1754150390625
        },
        {
            "corpus_id": "259274988",
            "title": "Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting",
            "text": "Can GPT-4 be used in automatic evaluation? \n\nSince the food effect summarization would be used to assist PSG assessment, it is imperative for FDA professionals to evaluate the quality of the summary, though the evaluation by human is quite laborious and expensive. In this work, we find that GPT-4 evaluation provides a cheap and reasonable alternative to human evaluation. When using GPT-4 to rate the performance of different models against the reference summary, there are a number of issues. First, there exists an inconsistency problem whereby there is no guarantee that an LLM will produce the identical output for the same input every time. A good practice to fix the problem is to set temperature = 0; the temperature parameter of the model controls the randomness of the text generated, with 0 being deterministic. Second, we find a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt. To mitigate such recency bias, we recommend reporting the mean score over both orders. Third, we observe from Figure 4 that GPT-4 assigns significantly higher scores to its own outputs compared to human ratings (64% vs. 43%), which represents the propensity of the model to favor its own suggestions. Future work should investigate the existence of potential biases in GPT-4 evaluation as well as possible mitigation strategies. \n\nDespite the potential of GPT-4 for automating evaluation, it is not as flexible as humans, particularly in situations where specific definitions of summary quality are not provided. The performance of GPT-4 hence may vary due to the lack of clarity in evaluation standards. GPT-4 aligns with the human evaluation, concluding that it generates better summaries than ChatGPT. However, GPT-4 encounters difficulty distinguishing scenarios where both models produce equally good summaries. In cases where human assessors perceive the summaries as equally good, GPT-4 may favor a summary that includes relevant contextual information does not present in the other, such as \"explicitly linking it to a high-fat, high-calorie meal\" or \"mention the lack of clinically significant differences\". It also may penalize certain aspects based on subjective criteria, such as \"less concise\".",
            "score": 0.32698852090994557,
            "section_title": "Which model performs better in the study?",
            "char_start_offset": 33838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 45,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1374
                },
                {
                    "start": 1377,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2253
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.334228515625
        },
        {
            "corpus_id": "274280574",
            "title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
            "text": "One approach in this direction relies on the judge LLMs themselves to generate the synthetic feedback. For example, Wu et al. (2024) construct pairwise feedback for judgment enhancement by prompting the policy LLMs to evaluate their own judgments. Wang et al. (2024f) prompt the LLM to generate a \"noisy\" version of the original instruction and use the corresponding response to this corrupted instruction as the inferior response. Wang et al. (2024a) prompt GPT-4-Turbo to generate multiple pieces of evidence based on the original evidence for each instance, categorizing them into completely irrelevant evidence, partially irrelevant evidence and highly related evidence to train a hallucination judgment LLMs. Park et al. (2024) build OFFSETBIAS, a pairwise preference dataset that leverages GPT-4 to generate bad, off-topic and erroneous responses and perform difficulty filtering. For safety judging, Xie et al. (2024a) adopt GPT-4 as the classifier to map each data point to a predefined safety category to train an automated evaluator. Different from previous works, Li et al. (2024e) adopt GPT-4 to synthesize both pairwise and pointwise data to train a generative judge LLM. For pointwise data, they adopt a \"divide-andconquer\" strategy, where two critiques are collected from GPT-4 for a single response, combined into  2023) introduce JudgeLM and propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges.",
            "score": 0.3261531083182567,
            "section_title": "Data Source",
            "char_start_offset": 21067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1535
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 267,
                    "matchedPaperCorpusId": "271709606"
                },
                {
                    "start": 714,
                    "end": 732,
                    "matchedPaperCorpusId": "271064337"
                },
                {
                    "start": 1075,
                    "end": 1092,
                    "matchedPaperCorpusId": "263829791"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11102294921875
        },
        {
            "corpus_id": "72941074",
            "title": "Egocentric Bias and Doubt in Cognitive Agents",
            "text": "Egocentric bias is the tendency to rely too heavily on one's own perspective and/or to have a higher opinion of oneself than others. Ralph Barton Perry [50] coined the term egocentric predicament and described it as the problem of not being able to view reality outside of our own perceptions. Greenwald [26] described it as a phenomenon in which people skew their beliefs in agreement with their perceptions or what they recall from their memory. We are susceptible to this bias because information is better encoded when an agent produces information actively by being a participant in the interaction. \n\nResearch suggests that this skewed view of reality is a virtually universal trait and that it affects each person's life far more significantly than had been realized [45]. It has also been shown to be pervasive among people and groups in various contexts such as relationships, team sports, etc. [55]. It is closely connected to important traits such as self-esteem and confidence [38]. A high degree of egocentric bias hinders the ability to empathize with others' perspectives, and it has been shown that egocentricity tends to be lower in depressed individuals [24]. Egocentric bias also plays a key factor in a person's perception of fairness: people tend to believe that situations that favor them are fair whereas a similar favor to others is unjust [21,25]. Perceived fairness is a crucial element in several resource allocation problems. Most importantly, it has been shown to be ineradicable even after standard debiasing strategies such as feedback and education [37]. \n\nPrior work has been done to model confirmation bias, but the most used model has been the Bounded Confidence (BC) model. The BC model was first introduced by Krause in 2000 [36]. Later, Deffuant et al. [13] proposed a relative agreement model (RA) which extended the BC model. In the BC model, an agent considers only those opinions that are sufficiently close to its own, and shuns any opinion outside the confidence threshold. This model has been used to model confirmation bias in many papers [67,14,30,61,15].",
            "score": 0.32597490928715195,
            "section_title": "Egocentric Bias",
            "char_start_offset": 6157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1586
                },
                {
                    "start": 1589,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 156,
                    "matchedPaperCorpusId": "150236079"
                },
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "1350893"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "8508954"
                },
                {
                    "start": 904,
                    "end": 908,
                    "matchedPaperCorpusId": "37562229"
                },
                {
                    "start": 1364,
                    "end": 1368,
                    "matchedPaperCorpusId": "21715706"
                },
                {
                    "start": 1368,
                    "end": 1371,
                    "matchedPaperCorpusId": "147559741"
                },
                {
                    "start": 1581,
                    "end": 1585,
                    "matchedPaperCorpusId": "1016753"
                },
                {
                    "start": 1762,
                    "end": 1766,
                    "matchedPaperCorpusId": "8012334"
                },
                {
                    "start": 1791,
                    "end": 1795,
                    "matchedPaperCorpusId": "45337175"
                },
                {
                    "start": 2085,
                    "end": 2089,
                    "matchedPaperCorpusId": "15327759"
                },
                {
                    "start": 2092,
                    "end": 2095,
                    "matchedPaperCorpusId": "8130429"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.299560546875
        },
        {
            "corpus_id": "256502122",
            "title": "Mindful self-focus\u2013an interaction affecting Theory of Mind?",
            "text": "In real life, the ability to reason about other people's mental states requires the consideration of past, present, general, and occasion-specific information about people and social situations [21]. Additionally, one's own mental state regarding some context may provide valuable insight into other people's perspectives. Often, what is shared already explains quite a lot [22]. However, one can also be mistaken to project one's mental state onto other people or believe they are likeminded, that is egocentric bias and false consensus belief, respectively [4,23,24], the crux for successful ToM is appropriately differentiating between oneself and another person, that is self-other distinction [25]. \n\nNotably, bias and accuracy are not necessarily opposites; in the right circumstances, bias can facilitate accuracy because bias allows for robust predictions under uncertainty [26,27]. In other words, a person can be right for the \"wrong\" reason like when grounding inferences about other people's mental states in their own mental state rather than information about the other person. This is important because most ToM tasks measure either accuracy or bias but not both. Thus, they actually cannot answer whether in real life more egocentric participants will be less accurate or vice versa. Previous research on self-focus and ToM has mostly employed measures of egocentric bias, while here an accuracy measure was used.",
            "score": 0.3257162499761633,
            "section_title": "The outcome: Theory of Mind (ToM)",
            "char_start_offset": 1879,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 703
                },
                {
                    "start": 706,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1429
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "20173887"
                },
                {
                    "start": 374,
                    "end": 378,
                    "matchedPaperCorpusId": "1537018"
                },
                {
                    "start": 559,
                    "end": 562,
                    "matchedPaperCorpusId": "26637346"
                },
                {
                    "start": 562,
                    "end": 565,
                    "matchedPaperCorpusId": "9032175"
                },
                {
                    "start": 565,
                    "end": 568,
                    "matchedPaperCorpusId": "37562229"
                },
                {
                    "start": 698,
                    "end": 702,
                    "matchedPaperCorpusId": "2317841"
                },
                {
                    "start": 886,
                    "end": 889,
                    "matchedPaperCorpusId": "146648828"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.245849609375
        },
        {
            "corpus_id": "273993682",
            "title": "EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation",
            "text": "We adopt a set of metrics from AIGCBench [11] and VBench [28] to assess the quality of the generated egocentric videos. Specifically, our evaluation metrics utilize the CD-FVD [12] for spatial and temporal quality, the CLIP [49] for semantic consistency, the EgoVideo [45] for action consistency, the DOVER [63] for clarity score, frame interpolation model [33] for motion smoothness, and RAFT [56] for motion strength. Additionally, following [20,66], we assess kinematic control consistency using translation error and rotation error, which measures the difference between COLMAP poses and the ground truth poses in the canonical space [66]. The specific calculations for each metric are detailed in the supplement. \n\nNext, we verify the impact of different data cleaning strategies on egocentric video generation. Subsequently, we substantiate, quantitatively and qualitatively, that the proposed EgoVid can enhance various baselines' egocentric video generation capabilities. Finally, experiments are conducted to demonstrate that the proposed EgoDreamer can generate egocentric videos under the control of both action descriptions and kinematic signals.",
            "score": 0.3252729182108516,
            "section_title": "Experiment Details",
            "char_start_offset": 19687,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1158
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 45,
                    "matchedPaperCorpusId": "266741637"
                },
                {
                    "start": 57,
                    "end": 61,
                    "matchedPaperCorpusId": "265506207"
                },
                {
                    "start": 176,
                    "end": 180,
                    "matchedPaperCorpusId": "269214646"
                },
                {
                    "start": 224,
                    "end": 228,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 307,
                    "end": 311,
                    "matchedPaperCorpusId": "257378482"
                },
                {
                    "start": 394,
                    "end": 398,
                    "matchedPaperCorpusId": "214667893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03985595703125
        },
        {
            "corpus_id": "265154694",
            "title": "Evidence against implicit belief processing in a blindfold task",
            "text": "The outcome is incongruent with respect to ball location in half of the trials and thus surprising for the participant. The authors found an altercentric bias in participants' reaction times, i.e., participants were faster to detect an unexpected ball behind the occluder when the agent falsely believed the ball to be there, compared to when the agent also knew that the ball was gone. A critical question is whether participants will also show such altercentric bias when the belief of the agent is manipulated with a blindfold, rather than their presence or absence. This would allow for false belief and true belief conditions to be identical, except for participants' knowledge concerning the visibility through the blindfold. It would thus exclude the possibility that the altercentric bias resulted from the fact that the agent's presence or absence, during the location change, impacted the saliency of the event and, thus, the participants' memory of the actual object location. In the current study, we therefore tested whether adults show an altercentric bias in a blindfold false belief task, that is, in a setting where the agent's belief could only be deduced from the participant's own experience with the transparency of a blindfold. \n\nWhile another person's perspective can affect our own behavior, it can also be the other way around. That is, our own perspective can affect our ability to understand what other people think. For instance, participants were slower and less accurate in their conclusions about what another person could see or believed when their own perspective did not match that of the other person [8,25]. This bias towards one's own perspective has been referred to as 'egocentric bias' [8,25]. Since it impedes the ability to infer others' perspectives, the egocentric bias can be considered a negative measure of ToM reasoning. Thus, there might be a negative correlation between altercentric and egocentric biases. That is, people who are very receptive to the perspective of others are less likely to be impaired by their own perspective. Therefore, a second aim of the present study was to investigate the relation between altercentric and egocentric biases in response times within the same experimental setting.",
            "score": 0.32419685579994056,
            "section_title": "Introduction",
            "char_start_offset": 3882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 1636,
                    "end": 1639,
                    "matchedPaperCorpusId": "3660567"
                },
                {
                    "start": 1639,
                    "end": 1642,
                    "matchedPaperCorpusId": "8563845"
                },
                {
                    "start": 1726,
                    "end": 1729,
                    "matchedPaperCorpusId": "3660567"
                },
                {
                    "start": 1729,
                    "end": 1732,
                    "matchedPaperCorpusId": "8563845"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08416748046875
        },
        {
            "corpus_id": "268513204",
            "title": "Embracing the Generative AI Revolution: Advancing Tertiary Education in Cybersecurity with GPT",
            "text": "To facilitate the analysis of policy statements, we utilized GPT-4 to perform a first-pass review of the collected documents.We provided GPT-4 with specific prompts, such as \"summarize the key points regarding the use of generative AI technologies in tertiary education\" and \"identify the main concerns and benefits discussed in relation to AI technologies in educational settings\".This allowed GPT-4 to extract relevant information from the policy statements.After extracting the data, we cross-verified the results to ensure accuracy and comprehensiveness.This approach enabled us to summarize the responses and provide a comprehensive overview of the universities' stance on GPTs.\n\n7) Addressing Biases, Limitations, and Conflicts of Interest: In our data analysis, we acknowledge potential biases that may have affected the interpretation of our results.Biases could arise from the subjective nature of content and thematic analysis, limitations inherent to descriptive and inferential statistical methods, or potential conflicts of interest stemming from some authors' affiliations with the evaluated cybersecurity degrees.To mitigate these concerns, we implemented several strategies.For biases related to analytical methods, we employed a diverse set of techniques and cross-verified the findings.For potential conflicts of interest, we utilized a rigorous review process that included blind evaluation and independent review by experts with no direct affiliations to the authors or institutions under examination.This ensured that the evaluation of cybersecurity degrees remained unbiased and objective.Limitations of our analysis may include the potential lack of generalizability due to the specific sample of universities and industries considered.We have documented these biases, limitations, and conflicts of interest to provide transparency in our approach and interpretation of findings.Moreover, we have been conscious of biases that could be introduced by relying on GPT-4 for initial analysis, tied to the algorithm's training data or design.By combining these data collection methods and analysis techniques, our methodology offers a comprehensive and human-like approach to exploring the influence of GPT on tertiary education in cybersecurity.",
            "score": 0.32390930331542983,
            "section_title": "6) Human-guided GPT-4 analysis:",
            "char_start_offset": 24987,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 382
                },
                {
                    "start": 382,
                    "end": 460
                },
                {
                    "start": 460,
                    "end": 558
                },
                {
                    "start": 558,
                    "end": 683
                },
                {
                    "start": 685,
                    "end": 858
                },
                {
                    "start": 858,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1304
                },
                {
                    "start": 1304,
                    "end": 1521
                },
                {
                    "start": 1521,
                    "end": 1611
                },
                {
                    "start": 1611,
                    "end": 1759
                },
                {
                    "start": 1759,
                    "end": 1902
                },
                {
                    "start": 1902,
                    "end": 2060
                },
                {
                    "start": 2060,
                    "end": 2264
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1339111328125
        },
        {
            "corpus_id": "277621852",
            "title": "FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking",
            "text": "For instance, Prometheus evaluates Mistral-generated explanations, while G-Eval and FinGrAct evaluate GPT-4-generated explanations. Their evaluations are then compared against human annotations, and instances of bias are identified and counted. Specifically, cases where an evaluator overestimates actionability-assigning a score at least 2 units higher than human ratings-are classified as ego-centric bias. \n\nThe scores from the three human annotators were averaged and compared against the mean scores from three evaluation runs for each automatic evaluator. Geval exhibited the highest variance, with 113 biased samples in the first run, 101 in the second, and 84 in the third, averaging 99. Prometheus demonstrated more stability, with 55 biased samples in the first run, 50 in the second, and 53 in the third. FinGrAct showed the least variance, with 17 biased samples in the first run, 19 in the second, and 17 in the third. \n\nAdditionally, instances where the evaluators underestimate actionability relative to human judgments are also recorded. This analysis allows us to determine whether ego-centric bias or underestimation contributes more to the misalignment between automated evaluators and human assessments Results: Out of 203 samples, the results clearly indicate that ego-centric bias contributes signifi-cantly more to the misalignment between human annotations and LLM-based evaluations than underestimation does. G-Eval exhibits ego-centric bias in 99 out of 203 samples (48.7%), whereas underestimation occurs in only 12 samples (5.9%). Similarly, Prometheus demonstrates bias in 26% of cases, while underestimation accounts for just 5%. FinGrAct, which shows the least bias, has 8.4% biased samples and 2% underestimation. \n\nAnalysis: These findings suggest that LLMbased evaluators tend to overestimate actionability far more frequently than they underestimate it, highlighting a key limitation in their judgment alignment with human evaluations. It is worth noting that LLMs as evaluators tend to exhibit higher correlation with human annotations and lower egocentric bias when the evaluation criteria are more detailed.",
            "score": 0.3237014097802757,
            "section_title": "Experiment 3: Ego-Centric Bias Analysis",
            "char_start_offset": 26795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1745
                },
                {
                    "start": 1748,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2145
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.791015625
        },
        {
            "corpus_id": "272987805",
            "title": "Can Models Learn Skill Composition from Examples?",
            "text": "However, GPT-4 is heavily used during data generation, and one can argue the improvement might solely come from the fact that GPT-4 favors its own outputs. Although the possibility is low, to rigorously eliminate this confounding factor, we re-evaluate SKILL-MIX all (k) using Claude 3 Opus as the Grader, and report the results in Table 5. Besides, we also include a consistency check between the GPT-4 and Claude 3 Opus graders in Table 6. \n\nFrom Table 5, we observe the metrics graded by Claude 3 Opus have a similar trend as those graded by GPT-4: after fine-tuning on D SKILL-MIX (1, 2, 3), SKILL-MIX all (k) performance improves for all k = 2, 3, 4, 5, while fine-tuning only on D SKILL-MIX (1) has limited improvement over the original LLaMA-2-13B-Chat. It proves that the improvement of SKILL-MIX performance is not overfitted to GPT-4 preference. \n\nInterestingly, we find that Claude 3 Opus is more generous, assigning higher scores to both the LLaMA-2-13B-Chat and the fine-tuned version. Besides, the results from Table 6 also confirm this argument: if an answer is assigned a full mark by GPT-4, then many of them will also be assigned a full mark by Claude 3 Opus. Such consistent biases among Graders were also noted in [33] when comparing LLaMA-2-70B-Chat and GPT-4 as Graders. \n\nBesides switching to Claude 3 Opus, we also do human spot checks on the SKILL-MIX generations, making sure that the model is not generating something that does not make sense to human. Please refer to Appendix D for some of the examples of SKILL-MIX evaluations before and after the fine-tuning.",
            "score": 0.32316839791255714,
            "section_title": "Data requirement for inducing compositional generalization",
            "char_start_offset": 24336,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1292
                },
                {
                    "start": 1295,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1590
                }
            ],
            "ref_mentions": [
                {
                    "start": 1234,
                    "end": 1238,
                    "matchedPaperCorpusId": "264490642"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.332763671875
        },
        {
            "corpus_id": "272337179",
            "title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions",
            "text": "In order to assess the effectiveness of our bias mitigation strategies, we conduct evaluations in three comprehensive settings: \n\n1. Understanding the presence of implicit biases: We evaluate if models can correctly identify the presence/absence of implicit biases in task assignments on the dev set of the Fine-tune dataset. Results and analysis are provided in Appendix D.1. \n\n2. Generation8 in the 'no interaction' setting: \n\nWe use the Test Dataset, which contains scenarios from domains different than the finetune data and prompt LLMs to output task assignments. Results and analysis are provided in Appendix D.2. \n\n3. Generation in the 'interaction setting': Here, multi agents interact and utilize mitigation strategies to reduce implicit biases on the Test Dataset. We discuss this further below. \n\nFigure 6 illustrates the results of mitigation approaches on the multi-agent LLM interactions. It demonstrates that the ft-gpt-35-turbo with SR + ICE yields the lowest bias score of 0.01, indicating almost neutral or no bias. All our ensembles (fine-tuning + self-reflection) have the best performances for both gpt-35-turbo and mistral-7b-instruct. Among the two approaches, fine-tuning proves more effective than self-reflection in reducing implicit biases from the outset. This is visible right from the first responses, as well as reflected in lower bias scores overall across models. It is worth noting that the fine-tune data and test data have different domains, showing the effectiveness of fine-tuning in gen-eration. The changes in bias scores after interactions, however, are minimal, for fine-tuned agents because the first responses themselves are less biased. Additionally, half-ft is more effective in mitigating biases in mistral-7b-instruct. Similarly, self-reflection mitigation effects are more pronounced for mistral-7b-instruct. \n\nWe find that gpt-4 generates negative bias scores, i.e. anti-stereotypical assignments using mitigation strategies and does not present equally representative task assignments after self-reflection. These results imply that smaller models benefit more from our mitigation strategies. Fig 18 in Appendix D.3 shows the results for the 'goal' setting, which holds most of our results as discussed above.",
            "score": 0.32257460751059086,
            "section_title": "Experiments and Results: Bias Mitigation",
            "char_start_offset": 22634,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 130,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 376
                },
                {
                    "start": 379,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1857
                },
                {
                    "start": 1860,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2260
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6328125
        },
        {
            "corpus_id": "267500374",
            "title": "Measuring Implicit Bias in Explicitly Unbiased Large Language Models",
            "text": "We asked GPT-4 to moderate its own biases, using the moderation API. Using data from the last study, for each GPT-4 response on the implicit bias task, person profiles, final decisions, and their explanations, we asked GPT-4 to evaluate the response along the dimensions: harassment, harassment and threatening, hate, hate and threatening, self-harm, self-harm instructions, self-harm intent, sexual, sexual minors, violence, and violence graphics. We report average moderation scores across these dimensions, and how many responses are flagged as problematic. We find very few flagged reports.",
            "score": 0.32217878913268105,
            "section_title": "B GPT-4 Moderation on self-generated Implicit and Decision Responses",
            "char_start_offset": 31742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 594
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2274169921875
        },
        {
            "corpus_id": "273228938",
            "title": "Detecting Bias and Enhancing Diagnostic Accuracy in Large Language Models for Healthcare",
            "text": "Reducing the discrepancy distance would enable the model to better align with the real-world healthcare domain, allowing it to balance ethical concerns with diagnostic accuracy. In EthiClinician, we address this issue through fine-tuning, ensuring that the model is optimized for the healthcare-specific target domain, resulting in improved performance and ethical decision-making. \n\nGPT-4 Bias Mitigation and Diagnostic Gains-GPT-4 demonstrated significantly improved performance on the Disease-Matcher dataset compared to previous models. It provided correct answers in 82.84% of cases, showed indecisiveness in 11.00% of responses, and gave incorrect answers in only 5.29% of cases. The model withheld a diagnosis due to patient demographics in just 0.87% of responses, indicating a substantial improvement in handling patient diversity with minimal bias (Figure 5c). GPT-4's ability to mitigate distributional shift effectively explains its superior performance. The model reduces the discrepancy distance between the pretraining and target domain distributions: \n\nThis reduced discrepancy minimizes excess risk, allowing GPT-4 to generalize better across diverse patient demographics and medical conditions. However, while GPT-4 performs better than ChatDoctor and Llama2-7B, it still exhibits limitations when handling nuanced demographic biases, where its refusal to provide answers remains an issue, albeit at a much lower rate. \n\nEthiClinician Leading in Ethics and Accuracy-The finetuned EthiClinician model achieved an accuracy of 92.47% on the DiseaseMatcher dataset. This performance represents a 9.63% improvement over GPT-4, a 72.07% increase compared to the open-source Llama2-7B model, and a 41% enhancement over its base model, ChatDoctor (Figure 5d). EthiClinician successfully mitigates position bias through its balanced attention mechanism, which ensures that neither patient is unfairly favored in the prediction process. The model also minimizes the discrepancy distance ( d H (P pretrain , P target ) \u2192 0, allowing it to generalize more effectively across patient demographics and symptom profiles.",
            "score": 0.3216176100242384,
            "section_title": "DiseaseMatcher's Accuracy in Predicting Diseases from Symptoms",
            "char_start_offset": 17795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1436
                },
                {
                    "start": 1439,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2123
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42041015625
        },
        {
            "corpus_id": "259923494",
            "title": "Coding Inequity: Assessing GPT-4's Potential for Perpetuating Racial and Gender Biases in Healthcare",
            "text": "As LLM-based tools continue to be developed and deployed, it is essential to ensure that these 261 technologies do not perpetuate demographic or socioeconomic based health care inequities. Our 262 findings underscore the need for ongoing evaluation and mitigation strategies for biases that impact 263 GPT-4's clinical decision making capabilities. While LLM-based tools will likely be deployed with 264 a clinician in the loop, it is not clear that a provider would be necessarily able to identify biases in 265 LLMs when examining only individual patient cases (39). Targeted fairness evaluations are needed 266 for each intended use of LLMs. Furthermore, understanding the contributions of the training data 267 and the training methods (such as RLHF) will be important for limiting these biases in the future. 268 We must place a strong emphasis on refining the processes of model training and data sourcing CC-BY 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\n\nThe copyright holder for this preprint this version posted July 16, 2023. ;https://doi.org/10.1101https://doi.org/10. /2023 Limitations. Our study has several limitations. We focused our investigations on GPT-4 based 272 on its imminent integration within several electronic health systems. However, we believe similar 273 biases may be present more broadly within other LLMs, all of which warrant caution and careful 274 consideration of the potential for bias prior to deployment in a healthcare setting. Furthermore, we 275 performed our experiments with clinical vignettes rather than real patient data to limit potential 276 confounding variables. Further investigation is needed to assess GPT-4's biases using clinical notes.",
            "score": 0.3212735448319467,
            "section_title": "260",
            "char_start_offset": 13682,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2225341796875
        },
        {
            "corpus_id": "265610016",
            "title": "TIBET: Identifying and Evaluating Biases in Text-to-Image Generative Models",
            "text": "We propose TIBET, an approach to automatically detect and evaluate biases present in images generated by TTI models in an explainable manner. Our approach has the potential to address previously unexplored issues related to bias in TTI models, including reasoning about intersectionality of different bias axes and comprehensive and automated bias mitigation. Our hope is that TIBET can serve as the foundation for future research in the these directions. Limitations and Ethical Considerations. Although there are many benefits to our method, we acknowledge that incorrect bias detection can be harmful. In our work, we use LLMs (GPT-3) and VLMs (MiniGPT-v2, CLIP) that may have their own limitations and biases (see Appendix 7). Our sensitivity analysis improves the transparency of our pipeline, and can measure the effect of these biases. Ultimately, our approach is modular and not dependent on any specific versions of these models. We expect that as fairer and more capable LLMs and VLLMs emerge, they will replace the current models used in our method. Finally, we note that we conduct user studies in accordance with ethics guidelines. if c i j is synonym of c i k then 6: \n\nmerge \n\nremove \n\nend if 9: end for 10: Repeat loop above for C cf \n\nStep 2: Add missing concepts 11: For any concept that is present in but not in C cf , add the concept into C cf with a frequency of 0, and vice versa. \n\nStep 3: Compare Histograms 12: Re-order Cinit and C cf to the same order, as in the vocabulary, so that corresponding concept frequencies can be compared. 13: CAS = HistIoU (w i * , w cf * ) where w i * and w cf * are the frequencies in Cinit and C cf respectively. \n\nembed images into vectors. We then compare each image in the initial set with every image from the counterfactual set (pairwise) using cosine distance. \n\nWe compare CAS and CAS CLIP in Fig. 11.",
            "score": 0.3206558684282581,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 30327,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1189
                },
                {
                    "start": 1192,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1402
                },
                {
                    "start": 1405,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1670
                },
                {
                    "start": 1673,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1824
                },
                {
                    "start": 1827,
                    "end": 1866
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2109375
        },
        {
            "corpus_id": "270357475",
            "title": "Phased Instruction Fine-Tuning for Large Language Models",
            "text": "Figure 7 details the prompt used for scoring and comparing the quality of output generated by two models using GPT-4.To mitigate any potential positional bias (Wang et al., 2023a;Chen et al., 2024) from GPT-4, we conduct two scoring sessions.Specifically, we provide GPT-4 with inputs in two different orders: (instruction, input, output-1 of model 1, output-2 of model 2) and (instruction, input, output-2 of model 2, output-1 of model 1) for separate evaluations.",
            "score": 0.3200431259323868,
            "section_title": "A Appendix",
            "char_start_offset": 33150,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 465
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2454833984375
        },
        {
            "corpus_id": "258298283",
            "title": "AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation",
            "text": "To build hand pose estimators for egocentric views, we train models on egocentric images with keypoint annotations generated in Section 3. Training on egocentric images is necessary because existing exocentric datasets do not fully capture egocentric-specific biases in terms of the viewpoint, camera characteristics (egocentric cameras are typically fisheye), and blur from the head motion. Hence, the generalization of exocentric models to egocentric data tends to be limited: for example, in [26], the model trained on DexYCB [3] (exocentric) achieves 14% PCK on FPHA [7] (egocentric), compared to 63% when fine-tuned on FPHA. Problem setting. We conduct an evaluation of a 3D hand pose estimator trained by egocentric images. Given a single egocentric image, the model aims to predict the 3D coordinates of 21 joints in the wrist-relative space. We split both the manually annotated and the automatically annotated datasets (M/A) into training and evaluation. Manually annotated training and evaluation sets contain 19.2K and 3.0K images, respectively, which are sampled at 1 Hz  Table 4. Effect of automatic annotation for the training of SVEgoNet. We use egocentric image sets with manual (M), automatic (A), and manual and automatic (M + A) annotation for training and evaluation. We report MPJPE (mm) as the evaluation metric (lower is better).\n\nfrom 62 video sequences with 14 subjects. Automatically annotated sets include 405K and 63K images, respectively, which are sampled at 30 Hz from a disjoint set of 20 sequences with 20 subjects.\n\nSingle-view baseline. Following standard heatmap-based hand pose estimators [15,23], we build a single-view network (SVEgoNet) trained on monochrome egocentric images. The model consists of 2.5D heatmap optimization and hand identity classification. The 2.5D heatmaps represent 2D keypoint heatmaps in x-y axis and the wrist-relative distance from the camera in z axis. We use the ResNet-50 [13] backbone. The 3D joint coordinates are computed",
            "score": 0.31963541368502346,
            "section_title": "Egocentric 3D hand pose estimation",
            "char_start_offset": 20092,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 495,
                    "end": 499,
                    "matchedPaperCorpusId": "247476074"
                },
                {
                    "start": 529,
                    "end": 532,
                    "matchedPaperCorpusId": "233210016"
                },
                {
                    "start": 571,
                    "end": 574,
                    "matchedPaperCorpusId": "4721179"
                },
                {
                    "start": 1626,
                    "end": 1630,
                    "matchedPaperCorpusId": "13746398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0290374755859375
        },
        {
            "corpus_id": "212752168",
            "title": "Reduced egocentric bias when perspective-taking compared with working from rules",
            "text": "removing from view the very target participants had to think about, it was now impossible in the Target Unseen block to rely on one's recent perceptual experience.\n\nAs in Experiment 1, one half of the participants performed the task alone and were instructed to make judgements based on their own reasoning. The other half of the participants performed the Target Unseen block with another agent and were instructed to indicate the colour that agent saw through the filter. Note that this time the participant herself did not see the colour through the filter on these trials, only the other agent. This time, instead of the experimenter we recruited a second and na\u00efve participant to be the observer. Although the issue of observer naivety is usually considered important for tasks in which a confederate is engaged in language-use with a na\u00efve participant (Kuhlen & Brennan, 2013), some theories of egocentric biases posit that they might arise only when considering a more na\u00efve or ignorant other (Birch & Bloom, 2004). This manipulation eliminated the possibility that participants did not show greater bias in the social context simply because of the type of agent they were reasoning about. Finally, we preregistered our methods and analyses for Experiment 2: https://osf.io/65dsb/register/5,771ca429ad5 a1020de2872e. Our primary hypothesis was again in line with the social egocentricity hypothesis, namely that participants should indicate colours closer to the object's true colour when asked to take the perspective of a na\u00efve agent; in other words, a social context would promote more egocentric responses. We set as evidence for this hypothesis a statistically significant interaction between Group (Perspective Taking vs. No Perspective-Taking) 2 and Target (Target Seen vs. Target Unseen), favouring judgements closer to the true colour in the Target Unseen, Perspective-Taking condition than in the Target Unseen, No Perspective-Taking condition. If an interaction should not be found, then there should at least be a main effect of Target, such that participants should indicate colours closer to the true colour when the target was hidden by a barrier compared with when it was visible. This would indicate that the task was sensitive enough to elicit modulations in egocentricity, should no evidence of any effect of perspective-taking be found.",
            "score": 0.319268170211476,
            "section_title": "Discussion",
            "char_start_offset": 24259,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 858,
                    "end": 882,
                    "matchedPaperCorpusId": "178840"
                },
                {
                    "start": 1000,
                    "end": 1021,
                    "matchedPaperCorpusId": "320240"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11578369140625
        },
        {
            "corpus_id": "1919583",
            "title": "Network autocorrelation models with egocentric data",
            "text": "Network autocorrelation models are widely used to measure covariate and network effects on a response variable of interest. These models, however, necessitate data on all actors of the network. This is very often not feasible. Egocentric network data are very often dramatically more feasible to collect, but the current methods to estimate network effects on this type of data are ad hoc, and not founded on a data generating process that could explain the full network data and all the complex dependencies therein. This paper derives a model for egocentric data that is consistent with a data generating process that can account for the full network data. Specifically, if the true underlying generating process is a network autocorrelation model, the proposed conditional distribution used in this paper converges to the joint distribution of the data as n e \u2192 n. That is, when n e = n, (7) is equivalent to the distribution of y as given in (1), and ( 12) is equivalent to the distribution of y as given in (3). \n\nThe negative bias in the estimation of the network effect as quantified by the parameter \u03c1 is an important issue in network autocorrelation models. The simulation study has shown that it is present in our context of egocentric data, and is especially problematic when there is row-normalization. It is the author's hope that this is an area of future research that receives its due attention. \n\nAs mentioned earlier, a common ad hoc approach to estimating network effects with egocentric network data is to use as a covariate either network size or an average of some alter attribute. This can be viewed as using a spatial Durbin model, rather than a more sophisticated network autocorrelation model, only looking at a subset of the data. The  Durbin model for the full data is \n\nwhere X 1 and X 2 may share some, all, or none of their columns. There is no complicated dependence structure in this model (which seems unrealistic in the network context), and so it is straightforward to use this model for egocentric data. Using the network size as a covariate is equivalent to letting X 2 be the vector of 1's.",
            "score": 0.3177730902666114,
            "section_title": "Discussion",
            "char_start_offset": 15569,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1796
                },
                {
                    "start": 1799,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2129
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046722412109375
        },
        {
            "corpus_id": "259360998",
            "title": "Style Over Substance: Evaluation Biases for Large Language Models",
            "text": "As human evaluation can be costly and inefficient, there is an increase in the use of advanced LLMs, such as GPT-4, to evaluate model outputs. In our work, we also use LLMs as judges to assess answer quality. However, previous studies rely solely on GPT-4 as the LLM judge (Chiang et al., 2023;Li et al., 2023a;Zheng et al., 2023), which may not be appropriate for our work as our answers are refined by humans after being generated by GPT-4. This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4.5 By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study. \n\nWe utilize the evaluation prompt from Dettmers et al. ( 2023), as presented in Figure 1. The prompt assesses the answers based on their helpfulness, relevance, accuracy, and level of detail, while also aiming to avoid bias related to answer ordering.",
            "score": 0.31713363161028885,
            "section_title": "LLM Evaluation",
            "char_start_offset": 13307,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 797
                },
                {
                    "start": 800,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1050
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.552734375
        },
        {
            "corpus_id": "273507683",
            "title": "How Can We Diagnose and Treat Bias in Large Language Models for Clinical Decision-Making?",
            "text": "GPT-3 GPT-4o GPT-4 Turbo Gender CPV Intersectionality and prioritisation in bias mitigation Table 3 shows the results of the bias evaluation in our two CPV datasets, examining the impact of gender-only and gender-x-ethnicity CPV strategies on MCQ performance and explanation (XPL) quality. The introduction of ethnicity as a variable led to changes in gender-related disparities, with varying effects across models. For GPT-3.5, the gap between female and neutral cases narrowed from 1.00% to 0.60%, while the gap between male and neutral cases increased from 0.00% to 3.77%. Despite the reduction in gender-related disparities, gender terms remained among the top influential features for all models: \"man\" and \"woman\" appeared in the top 5 SHAP features for GPT-3 and GPT-4o in both experiments, as displayed in Figure 2. We also observed the introduction of ethnicity biases: GPT-3.5 and GPT-4 Turbo consistently underperformed on ethnicity-varied cases compared to the no ethnicity case, with Asian cases systematically showing the best performance (-0.46% for both models). The SHAP feature analysis revealed that ethnicity terms became highly influential when introduced. For instance, \"white\" became the most important feature for GPT-4o (0.74), while \"black\" became the most negatively influential feature for GPT-4 Turbo (-0.60). The introduction of ethnicity appeared to shift rather than eliminate bias patterns, as reflected in the changing importance and direction of influence for demographic terms. For example, \"white\" shifted from contributing to incorrect predictions (-0.45) to strongly favouring correct predictions (0.74) for GPT-4o. These findings underscore the need for comprehensive debiasing strategies that address both gender and ethnic dimensions in outcomes and reasoning processes. \n\nEffectiveness of Fine-Tuning in mitigating with CPV for bias mitigation Our fine-tuning experiments showed interesting results across MCQ (Table 4) and XPL (Figure 3) GPT-4o mini models.",
            "score": 0.3166242187158098,
            "section_title": "Metric",
            "char_start_offset": 16116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1812
                },
                {
                    "start": 1815,
                    "end": 2001
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.227294921875
        },
        {
            "corpus_id": "267039006",
            "title": "Peer review of GPT-4 technical report and systems card",
            "text": "Like many other computer science publications, the OpenAI report is publicly available at arXiv. However, the capabilities and scale of GPT-4 have resulted in novel safety challenges that must be safeguarded against [9]. Like any machine learning algorithm, GPT-4 may have inherent biases that reflect biases in training data. In addition, the safety risks listed by the authors include the following: Hallucinations, Harmful content, Harms of representation, allocation, and quality of service, Disinformation and influence operations, Proliferation of conventional and unconventional weapons, Privacy, Cybersecurity, Potential for risky emergent behaviors, Interactions with Other Systems, Economic impacts, Acceleration, and Overreliance. \n\nTraditionally, the artificial intelligence (AI) field has greatly benefitted from the Open-first approach, from ImageNet to MNIST and MIMIC. Therefore, for GPT-4 to go against this grain, at present, where the data used for training and the necessary compute for training remain a trade secret, raises old concerns. Initial hesitance in releasing LLMs due to potential societal impact was common, yet it needs to be clarified if these obstacles have been overcome or what has changed in this period [10,11]. Especially when we review the model for bias, we are now speculative-as many researchers will not have access to the resources and datasets needed to train their own LLMs. Without systems and standards safeguarding against potential risks, self-audit can produce selective sharing and expose humans and patients to risks. \n\nIn the context of GPT-4, the OpenAI report is highly selective in disclosing foundational elements, such as training data, necessary to scrutinize critical aspects, including representativeness, robustness, and quality of the datasets. To ensure fairness and equity, it is essential to identify and address such biases and ensure that the model is trained on diverse and representative data. This self-selection is further demonstrated by the number of redactions released from the final draft, including areas such as toxic language. While this also indicates that the TR was meant as less of a rigorous technical evaluation and more of a demonstration of current performance, it highlights the potential for omitting areas deemed damaging.",
            "score": 0.31641553629241437,
            "section_title": "AI has greatly benefitted from and necessitates transparency",
            "char_start_offset": 1281,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1573
                },
                {
                    "start": 1576,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2110
                },
                {
                    "start": 2111,
                    "end": 2317
                }
            ],
            "ref_mentions": [
                {
                    "start": 1243,
                    "end": 1247,
                    "matchedPaperCorpusId": "257578487"
                },
                {
                    "start": 1247,
                    "end": 1250,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07720947265625
        },
        {
            "corpus_id": "270391675",
            "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
            "text": "LLM-based evaluators frequently utilize GPT-4 (Liu et al., 2023c;Xu et al., 2023;Zheng et al., 2023), owing to its status as one of the most advanced LLM (OpenAI, 2023).However, relying on GPT-4 for evaluation might lead to biased or skewed results, especially when evaluating outputs generated by itself or an equally powerful model (Bai et al., 2023;Zheng et al., 2023).The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator).This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023).This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022;Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023;Ouyang et al., 2022) to ensure more balanced and comprehensive assessments.\n\nDomain-Specific Evaluation.LLMs have been prevalent across various domains, such as law (Cui et al., 2023a), medicine (Singhal et al., 2023), finance (Yang et al., 2023a), etc.However, most LLMs employed as evaluators are designed for general domains and are not specifically tailored to any particular field.This lack of specialization poses significant challenges.On one hand, these LLMs often lack the requisite domain-specific knowledge, making it difficult for them to accurately assess the correctness of content within specialized fields.On the other hand, the evaluation prompts need to be meticulously designed for different domains.This may involve tailoring the aspects of evaluation relevant to each field.",
            "score": 0.3161033143509453,
            "section_title": "Challenges and Open Problems",
            "char_start_offset": 42121,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 543
                },
                {
                    "start": 543,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 877
                },
                {
                    "start": 877,
                    "end": 1185
                },
                {
                    "start": 1187,
                    "end": 1214
                },
                {
                    "start": 1214,
                    "end": 1363
                },
                {
                    "start": 1363,
                    "end": 1496
                },
                {
                    "start": 1496,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1732
                },
                {
                    "start": 1732,
                    "end": 1829
                },
                {
                    "start": 1829,
                    "end": 1905
                }
            ],
            "ref_mentions": [
                {
                    "start": 1051,
                    "end": 1072,
                    "matchedPaperCorpusId": "215548699"
                },
                {
                    "start": 1110,
                    "end": 1130,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1305,
                    "end": 1327,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 1337,
                    "end": 1357,
                    "matchedPaperCorpusId": "254877751"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6328125
        },
        {
            "corpus_id": "257378488",
            "title": "\u201cIf I Had All the Time in the World\u201d: Ophthalmologists\u2019 Perceptions of Anchoring Bias Mitigation in Clinical AI Support",
            "text": "We acknowledge several limitations in our work. First, we merged three debiasing techniques into a single prototype to ensure a more realistic workflow. This limits our ability to assess the suitability of individual techniques, as participants' perception was inevitably affected by their encounters with the other strategies. Second, the bias mitigation strategies we presented can be integrated into the user interface in myriad ways. As such, the ophthalmologists' statements regarding our strategies' implementation may not directly transfer to individual bias mitigation strategies. However, to integrate bias mitigation strategies into CDSS, we must suggest and evaluate concrete ways of doing so. Third, a quantitative assessment of the impact of the bias mitigation strategies was out of the scope of our study as we focused on clinicians' perceptions of bias mitigation. While we are therefore unable to report on the impact of bias mitigation during actual real-world usage, we consider this as a critical step towards aligning with stakeholder needs. This aligns with Bertrand et al.'s recent call for an increased emphasis on cognitive biases in the context of explainable AI [6]. \n\nFuture work may explore the effect of specific debiasing strategies and eventually study its effect in clinical trials to understand its impact in practice. Reflecting on our role as researchers, we chose to point to the literature when discussing cases of cognitive bias in the clinic-rather than potentially upsetting participants by pointing to their own biases. Although several participants actively recognised the impact of their own cognitive biases on their work, not all expressed a similar sentiment. This points to the need to increase participant awareness of potential cognitive biases, and the challenges researchers may face when involving end-users in designing cognitive bias mitigation strategies. This study focused exclusively on ophthalmological examination, DR in particular. Although tedious and requiring a thorough investigation, the assessment of DR is not representative of all ophthalmological tasks or clinical image assessment more broadly. In particular, different tasks may evoke other notions of urgency, both in terms of time and clinical need. It would therefore be valuable to study perceptions towards bias mitigation in other clinical contexts.",
            "score": 0.31579040525012636,
            "section_title": "Limitations and Future Work",
            "char_start_offset": 59549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2166
                },
                {
                    "start": 2167,
                    "end": 2274
                },
                {
                    "start": 2275,
                    "end": 2378
                }
            ],
            "ref_mentions": [
                {
                    "start": 1189,
                    "end": 1192,
                    "matchedPaperCorpusId": "250331219"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0170440673828125
        },
        {
            "corpus_id": "133537715",
            "title": "Lifting the curse of knowing: How feedback improves perspective-taking",
            "text": "People are likely to use their own knowledge as a frame of reference when they try to assess another person\u2019s perspective. Due to this egocentric anchoring, people often overestimate the extent to which others share their point of view. This study investigated which type of feedback (if any) stimulates perceivers to make estimations of another person\u2019s perspective that are less biased by egocentric knowledge. We allocated participants to one of the three feedback conditions (no feedback, accuracy feedback, narrative feedback). Findings showed that participants who were given feedback adjusted their perspective-judgement more than those who did not receive feedback. They also showed less egocentric projection on future assessments. Participants adjusted their perspective within the same trial to the same degree for both feedback types. However, participants\u2019 egocentric bias was only reduced when they received narrative feedback and not when they received accuracy feedback about their performance. Implications of these findings for theories of perspective-taking are discussed.",
            "score": 0.31564557961059286,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3232421875
        },
        {
            "corpus_id": "274581820",
            "title": "ChatNVD: Advancing Cybersecurity Vulnerability Assessment with Large Language Models",
            "text": "This disparity highlights the importance of effective contextual encoding strategies for descriptive information processing, suggesting that further fine-tuning and improved training data for descriptive tasks could address these limitations in models like Gemini 1.5 Pro. \n\n3) Hallucinations and Numerical Precision: LLaMA 3 demonstrated hallucination issues, specifically when responding to impact score questions. From 25 queries, it provided incorrect numerical values in two cases and failed to generate meaningful responses in four instances. These hallucinations raise concerns about the model's numerical reasoning and factual accuracy when processing structured cybersecurity data, suggesting the need for robust validation mechanisms to mitigate such errors. \n\n4) Key Takeaways: Our evaluation yields several valuable insights for researchers and practitioners: \n\n\u2022 Reliability and Consistency: GPT-4o Mini's superior accuracy across all batches establishes it as highly reliable for cybersecurity tasks requiring consistent information retrieval. Its ability to handle diverse question types makes it particularly suitable for comprehensive vulnerability assessments.",
            "score": 0.3154799192611085,
            "section_title": "D. Discussion",
            "char_start_offset": 49540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 275,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 871
                },
                {
                    "start": 874,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1178
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04779052734375
        },
        {
            "corpus_id": "270559556",
            "title": "VideoVista: A Versatile Benchmark for Video Understanding and Reasoning",
            "text": "However, this process sometimes results in incorrect or biased answers because of GPT-4's instability, so we perform human filtering for task types with high error rates.Additionally, we also use a template-based construction method for Relation Reasoning and Anomaly Detection.For video-video relation task, the candidates contain four predefined relationships (before, after, in, and none) between two videos.When merging videos, we can determine the relationship between any two videos and effectively construct relation-inferring questions through templates with fixed questions and candidates.For anomaly detection, the candidates include four risk types (hate and fairness, sexual, violence, and self-harm) from the content safety of Azure OpenAI Service.\n\nHuman Filtering: Upon reviewing QAs generated by GPT-4, we discovered obvious shortcomings in Object Count questions.Specifically, GPT-4o tended to focus solely on individuals in the centre of the frame, disregarding those in the background.Consequently, we implemented human filtering for the Object Count task.Additionally, errors were detected in the Objects Temporal Relation task (e.g., changes in the first appearance time of individuals for merged videos) and Human Activity Analysis task.Therefore, we also conducted human filtering for these two tasks.\n\nOption Generation: To ensure fairness and accuracy during evaluation, we convert all open-ended QAs generated by GPT-4 into multiple-choice QA pairs.Given open-ended QAs and auxiliary information such as video titles and audio transcripts, we instruct GPT-4 to generate one correct answer option and three incorrect distractors for each input question.Ensuring the options have similar lengths helps mitigate the common issue where 'longer options tend to be correct'.All detailed instruction prompts are shown in the Appendix \u00a7A.3.",
            "score": 0.31532228450725663,
            "section_title": "Automatic QA Generation",
            "char_start_offset": 9169,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 170,
                    "end": 278
                },
                {
                    "start": 278,
                    "end": 411
                },
                {
                    "start": 411,
                    "end": 598
                },
                {
                    "start": 598,
                    "end": 761
                },
                {
                    "start": 763,
                    "end": 880
                },
                {
                    "start": 880,
                    "end": 1004
                },
                {
                    "start": 1004,
                    "end": 1075
                },
                {
                    "start": 1075,
                    "end": 1259
                },
                {
                    "start": 1259,
                    "end": 1324
                },
                {
                    "start": 1326,
                    "end": 1475
                },
                {
                    "start": 1475,
                    "end": 1678
                },
                {
                    "start": 1678,
                    "end": 1794
                },
                {
                    "start": 1794,
                    "end": 1858
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2152099609375
        },
        {
            "corpus_id": "271329029",
            "title": "MIBench: Evaluating Multimodal Large Language Models over Multiple Images",
            "text": "To mitigate data contamination, our construction of test data exclusively utilizes the validation or test sets from existing datasets. Furthermore, we combine automated filtering and manual verification to ensure the quality and reliability of the test data. \n\nSpecifically, after the data generation process, we perform two automated filtering strategies on the obtained data. 1) We remove images from the input samples, and test multiple advanced MLLMs on them. Then we discard samples which can still be answered correctly without visual input. This avoids the overestimation of model performance due to the textual bias of the questions and options. 2) For the Multimodal Knowledge-Seeking scenario, we eliminate external knowledge from the samples and test them using multiple MLLMs. Then we remove samples which the models can answer correctly without external knowledge. This mitigates the impact of internal knowledge of the model, and provides a more accurate assessment of the model's ability of utilizing external knowledge. \n\nAs stated in Section 3.2, for some tasks such as Visual Referring, we employ GPT-4 to generate distractors. To ensure the high quality of the generated samples, we apply manual verification after automated filtering. The process is conducted by three trained annotators who possess relevant professional backgrounds. Specifically, a sample is discarded if there are duplicate options or more than one correct option. What are the differences between image 1 and image 2? \n\nA. The blue shirt has been changed to a red shirt.",
            "score": 0.31492900301019083,
            "section_title": "Quality Control",
            "char_start_offset": 15045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 258
                },
                {
                    "start": 261,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1561
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2078857421875
        },
        {
            "corpus_id": "278166184",
            "title": "Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages",
            "text": "GPT-4o on Catalan, however, is an outlier, achieving the second-best score among all language-model pairs. Nevertheless, because GPT-4o was chosen as the translation and paraphrasing model according to the results reported in Section 4, its output may provide GPT-4o with a slight advantage in the bias-detection task. Further work is required to evaluate this potential effect. Given the variance observed in Figure 4 across different bias categories, it is also evident that choosing an LLM may require a case-by-case approach. Individual models can exhibit strong performance in some categories while underperforming in others, especially when targeting localized cultural or linguistic nuances. Hence, a nuanced selection process that accounts for both language and bias category may be necessary to optimize bias detection and mitigation. \n\nIn conclusion, and in direct response to RQ2, these findings suggest that LLMs exhibit higher social biases when data augmentation is performed for low-resource languages. Nonetheless, the particular model best suited for each task may vary depending on the specific bias category and language under consideration.",
            "score": 0.3148333070941407,
            "section_title": "Performance Evaluation",
            "char_start_offset": 32026,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1160
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.052703857421875
        },
        {
            "corpus_id": "270391749",
            "title": "A Sociotechnical Lens for Evaluating Computer Vision Models: A Case Study on Detecting and Reasoning about Gender and Emotion",
            "text": "Researchers attempting to use GPT to simulate specific personas may find the tools unwilling to adopt such roles, as these models are designed to protect marginalized groups (Schramowski et al., 2022). For example, when researchers prompt the model with a non-binary persona to classify an image, GPT may be hesitant to address sensitive questions related to gender and decline to provide assistance for a classification task. Thus, we propose the second type of discriminant bias, \"rejection bias,\" defined as CV models reject to provide outputs or hesitate to produce complete outputs to classification tasks for certain personas. \n\nImage analyses of the GPT-4 Vision model could be context-dependent when researchers attach a particular persona to the prompt in studying images (Ronanki et al., 2024). Specifically, the model might demonstrate social biases in real-world scenarios while tailoring image analyses to a person of a particular socio political status. For example, the GPT-4 Vision model could yield different outcomes in classifying the gender of images when prompted with a persona of a straight person versus a transgender individual. This is because the GPT-4 Vision model might adjust its coding results based on the persona provided in the prompts. While scholars acknowledge that different prompting strategies could yield diverse results when performing the same task in the GPT-4 Vision model (Argyle et al., 2023;Ronanki et al., 2024), there is little knowledge about how social biases may manifest differently based on distinct prompting strategies in generative AI models. As we acknowledge the potential variation in the representation of social biases across CV models, we propose a sociotechnical framework in the next section to lay out the criteria to be considered for evaluating CV models, including off-the-shelf ones and the generative ones. Thus, our paper proposes this socio-technical framework to diagnose various types of biases in studying gender and emotional expressions using off-the-shelf CV models and Generative CV models. For the Generative CV models, we used GPT-4 Vision model from OpenAI as an example because it is one of the most popular generative AI models and large multi-modal language datasets are increasingly applied by researchers to examine images.",
            "score": 0.31450131992257196,
            "section_title": "Criterion 3: Discriminatory Bias",
            "char_start_offset": 35634,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2312
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 200,
                    "matchedPaperCorpusId": "246824056"
                },
                {
                    "start": 781,
                    "end": 803,
                    "matchedPaperCorpusId": "265043266"
                },
                {
                    "start": 1418,
                    "end": 1439,
                    "matchedPaperCorpusId": "252280474"
                },
                {
                    "start": 1439,
                    "end": 1460,
                    "matchedPaperCorpusId": "265043266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3125
        },
        {
            "corpus_id": "272911106",
            "title": "EAGLE: Egocentric AGgregated Language-video Engine",
            "text": "Following the evaluation methods [50,100] for recent LLMs, we use GPT-4 to assess the quality of responses generated by models. Due to the time-consuming nature of evaluating all 7,700 samples across nine models with GPT-4, we adopt a square root sampling strategy, selecting approximately ( \u221a 7700 \u2248 88) 100 samples as a representative subset. To maintain consistency and ensure the reproducibility of findings from the initial 100 samples, we further analyzed 200 additional responses. This was done to evaluate the performance of the top four models, which we have designated as EAGLE-pool 2 , Shikra 2 , BLIP-2 2 , and EAGLE 2 . The results are presented in Table 5. The results from this extended dataset are presented in the subsequent table and are consistent with the findings from our initial sample of 100 responses. \n\nGiven the nature of the egocentric dataset, which offers only action labels, recipe steps, and corresponding timestamps, we need to develop ground truth sentences for evaluation purposes. Our empirical findings indicate that compared to using polished sentences of ground truth labels, template-based construction reduces the occurrence of hallucination errors. The evaluation prompt was refined iteratively through trial and error, aiming to improve the accuracy in identifying event boundaries and objects and to enhance clarity. The evaluation prompt will be included in the supplementary. \n\nThese selected responses will be scored by GPT-4 based on five key metrics, each rated on a scale from 1 to 10, with higher scores indicating superior performance. The evaluation metrics are as follows: Please note that the descriptions provided above are instruction prompts for GPT-4. Metrics such as accuracy and detail assess the alignment between the outputs and the established ground truth, including the accurate representation of objects. Subjective metrics like helpfulness and conciseness focus on the quality of the language, ensuring that the responses aid users in grasping the broader context and intent of scenarios.",
            "score": 0.3140969428565996,
            "section_title": "Experiments 5.1 Evaluation Metrics",
            "char_start_offset": 24112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1421
                },
                {
                    "start": 1424,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 2056
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1649169921875
        },
        {
            "corpus_id": "259923494",
            "title": "Coding Inequity: Assessing GPT-4's Potential for Perpetuating Racial and Gender Biases in Healthcare",
            "text": "providing diagnosis or treatment recommendations) rather than medical information summarization 282 (e.g. summarizing a patient's treatment history). It is likely that summarization tasks will be less 283 susceptible to biases within training data. We also note that more \"demographically-conscious\" 284 prompts (e.g. an explicit request for the avoidance of bias) may mitigate some of the issues we 285 presented (40); however, we note that such bias-free prompting is unlikely to be common practice  While GPT-4 has significant potential to improve healthcare delivery, its tendency to encode societal 292 biases raises serious concerns for its use in clinical decision support. Targeted bias evaluations, 293 mitigation strategies, and a strong emphasis on transparency in model training and data sourcing are 294 needed to ensure that LLM-based tools provide benefit for everyone.",
            "score": 0.3140679156853791,
            "section_title": "281",
            "char_start_offset": 15937,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302001953125
        },
        {
            "corpus_id": "269484621",
            "title": "RST-LoRA: A Discourse-Aware Low-Rank Adaptation for Long Document Abstractive Summarization",
            "text": "GPT-4 Evaluation Inspired by Liu et al. (2023b), we engage GPT-4 to assess our candidate models using the same guidelines as our human evaluators. To ensure experimental consistency, all experiments use the identical hyper-parameters settings detailed in Appendix D. To avoid potential biases from previous interactions, we reset the conversation history prior to each query and abstain from making any further modifications. In our initial investigation, we aim to explore the extent to which GPT-4 evaluations5 generally concur with human assessments in terms of both relative ranking and average scores within the same subset of 10 samples delineated in human evaluations. We then extend the evaluation to include all samples from the test sets6 . \n\nThe outcomes for these tests are shown in Table 6, as well as in Table 9, 10 in Appendix H. We find that in GPT-4 evaluation, GPT-4 tends to assign the lowest scores to its own answers compared to those generated by other fine-tuned models. Summaries written by humans receive the highest scores and are generally regarded as the highest quality. In line with human evaluation findings, GPT-4 also recognizes LoRA as yielding inferior outcomes. In addition, the RST p w -LoRA model scored higher",
            "score": 0.3135399877024086,
            "section_title": "Impact of Different",
            "char_start_offset": 25366,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 750
                },
                {
                    "start": 753,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1248
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 47,
                    "matchedPaperCorpusId": "257804696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1337890625
        },
        {
            "corpus_id": "273549315",
            "title": "ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent Dialogue Synthesis",
            "text": "In this research, GPT-4 was employed as an evaluator and generator in a manner consistent with ethical guidelines. Transparency about its usage, accountability for its outputs, and mitigation of potential biases were prioritized. Data privacy and security were strictly maintained, and the AI's limitations were acknowledged, ensuring it supplemented rather than replaced human judgment. This approach aimed to enhance the research quality while upholding academic integrity and ethical standards.",
            "score": 0.3132002715424182,
            "section_title": "Ethic Statement",
            "char_start_offset": 29241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 497
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.313720703125
        },
        {
            "corpus_id": "269983123",
            "title": "As an AI Language Model, \"Yes I Would Recommend Calling the Police\": Norm Inconsistency in LLM Decision-Making",
            "text": "The opacity of LLM's normative decision-making complicates the effectiveness of traditional bias mitigation strategies for two reasons. First, many de-biasing and bias quantification strategies generally require defining ex-ante scenarios where bias may occur. This chicken-and-egg problem of knowing what the biased scenarios are before mitigation is not a robust way of correcting for the complex societal biases that appear in high-risk contexts. This means that conventional bias detection measures risk testing only for our own stereotypes of how models may be biased. Our unexpected result that neighborhoods, but not subject skin-tone, impact models' normative judgments is evidence of this issue. We suspect that this is due to the extensive attention paid specifically to skin-tone bias in computer vision. Second, and related to the chicken-and-egg problem, common approaches to bias quantification and mitigation often involve ensuring that model outcomes are invariant when the demographic groups associated with inputs are changed. For example, fine-tuning can ensure that LLM predictions do not change when transposing gender references in an input prompt (Czarnowska, Vyas, and Shah 2021;Kotek, Dockum, and Sun 2023). This assumes that the source of the bias is clear, and that it can be manipulated independently from other factors by researchers and engineers. Applying this mitigation approach to the decision context presented in this paper would require manipulating the \"whiteness\" of a video's neighborhood independent of other factors. Qualities like \"whiteness\" statistically co-vary with other complex neighborhood characteristics like median income and home price. Even if we were able to infer what visual elements might imply \"whiteness\" to a model like GPT-4, it's unlikely that they would not also influence other important parts of the model's understanding. More robust transparency or explanation tools will be crucial for developing bias mitigation strategies in complex normative decision-making. We believe this is an important area for future work.",
            "score": 0.3130156210042775,
            "section_title": "The Problem for Bias Mitigation",
            "char_start_offset": 31470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2085
                }
            ],
            "ref_mentions": [
                {
                    "start": 1170,
                    "end": 1203,
                    "matchedPaperCorpusId": "235658325"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1199951171875
        },
        {
            "corpus_id": "273661820",
            "title": "Self-Preference Bias in LLM-as-a-Judge",
            "text": "To reduce self-preference bias, one possible approach is ensemble evaluation using multiple models. This method is expected to provide a more equitable evaluation by avoiding reliance on a single model. Specifically, when a model Figure 4: vs other LLMs conditioned on perplexity. Winning judgment rates by LLMs on their own texts and texts generated by other models conditioned on perplexity are plotted. Across all models, except for dolly-v2-12b and stablelm-tuned-alpha-7b, no significant difference was observed between the judgment rates for their own texts and those generated by other models. This suggests that LLM evaluators assign higher ratings to texts with lower perplexity, regardless of whether the text was self-generated or produced by other models. \n\nexhibits low perplexity on a sample, decreasing the weight assigned to that model's evaluation for that sample may contribute to bias mitigation. To evaluate the effectiveness of bias reduction strategies, our proposed new metric can be utilized. Therefore, we believe that our research makes a significant contribution to the understanding of self-preference bias and will greatly facilitate the development of future research in this area. \n\nOur experimental results reveal that LLM evaluators tend to assign higher scores to texts with lower perplexity. We further discuss the reasons behind this phenomenon. First, LLMs are trained during the pretraining phase to reduce perplexity on large-scale text corpora. Moreover, when aligning with human preferences, the models are also trained to minimize perplexity on the given dialogue data. Therefore, high-perplexity texts are likely those that the LLM has not frequently encountered during training, suggesting that such texts may be related to domains that the LLM evaluators do not fully comprehend. \n\nThis observation may seem contradicted by the fact that GPT-4, which is well-versed across various domains due to a wide range of benchmarks, exhibits a high degree of self-preference bias. However, by investigating specific cases of self-preference bias, as shown in Figure 1a and Table 1, we found that the bias was often not related to clear factual errors but rather to differences in response styles, such as the handling of specialized domains or the description of premises before answering.",
            "score": 0.31288310784562373,
            "section_title": "Discussion",
            "char_start_offset": 19914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 767
                },
                {
                    "start": 770,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1211
                },
                {
                    "start": 1214,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1824
                },
                {
                    "start": 1827,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2325
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70166015625
        },
        {
            "corpus_id": "265018981",
            "title": "Grounded Intuition of GPT-Vision's Abilities with Scientific Images",
            "text": "G4 (Figure 15b) is more complex and shows the accuracy of 1.3 billion-, 13 billion-, and 175 billionparameter versions of GPT-3 as the number of in-context examples grows from 0 to 32. Each model size is represented by two lines: a solid line for a \"Natural Language Prompt\" and a dashed line for \"No Prompt.\" The graph contains free-floating labels for prompt style and model size as opposed to a legend, like in G3. \n\nAxes GPT-Vision described the x-and y-axes of each line graph moderately well, except that it consistently underestimated the bounds of the axes depending on the labels. For example, the y-axis in G4 is labeled from 0 to 60 in increments of 10, but the line itself extends to 70 without a tick label for y = 70: \n\nThe y-axis, or vertical axis, is labeled \"Accuracy (%)\" and has a linear scale ranging from 0 to 60. (G4 desc) This seems to be a stylistic trend because the bar graph (G1) and both line graphs (G3, G4) omit the tick label for the greatest value on the y-axis. GPT-Vision mistook the bounds of an axis when describing all three of these graphs, which was particularly precarious when the data went beyond the printed bounds (G3, G4). GPT-Vision appeared to have a bias toward text in an image when it incorporated adversarial labels into its output (see Section 3.4). Future work in \"artificial cognition\" to expose what GPT-Vision pays attention to can help mitigate this weakness. \n\nGPT-Vision made a subtler text-based error when describing the x-axis in G4: \n\nThe x-axis... has a logarithmic scale, starting at 10\u02c60 and increasing to 10\u02c61. (G4 desc) \n\nThe x-axis is labeled with \"0,\" \"10 0 ,\" and \"10 1 ,\" reminiscent of a logarithmic scale, but \"10 1 \" is further from \"10 0 \" than \"10 0 \" is from \"0.\" These values would be equally spaced on a true logarithmic scale.",
            "score": 0.3126426021051211,
            "section_title": "Graphic Misinterpretations",
            "char_start_offset": 47422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 417
                },
                {
                    "start": 420,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 731
                },
                {
                    "start": 734,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1587
                },
                {
                    "start": 1590,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1807
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.028656005859375
        },
        {
            "corpus_id": "270737939",
            "title": "Can LLMs Generate Visualizations with Dataless Prompts?",
            "text": "This motivated us to conduct a more in-depth investigation of GPT-4s abilities.We generated 15 prompts based on popular data visuals on the web, aided by a crowd-sourced survey to gather queries of popular interest.We then queried GPT-4 using these prompts and evaluated the responses by comparing them to visualization cheat sheets created by visualization experts and humangenerated charts on the web.\n\nIn the following.Section 2 provides relevant background.Section 3 details the study we conducted.Section 4 presents a discussion and concludes our paper.",
            "score": 0.311939090775342,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2092,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 79,
                    "end": 215
                },
                {
                    "start": 215,
                    "end": 403
                },
                {
                    "start": 405,
                    "end": 422
                },
                {
                    "start": 422,
                    "end": 461
                },
                {
                    "start": 461,
                    "end": 502
                },
                {
                    "start": 502,
                    "end": 558
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0290374755859375
        },
        {
            "corpus_id": "214693333",
            "title": "Action Localization through Continual Predictive Learning",
            "text": "Finally, we evaluate the ability to generalize to egocentric videos by quantifying the performance of the model on the unsupervised gaze prediction task. Given that we do not need any annotations or other auxiliary data, we employ the same architecture and training strategy for this task. We evaluate on the GTEA gaze dataset and compare it with other unsupervised models in Table 3  seen, we obtain competitive results on the gaze prediction task, outperforming all baselines on both the AUC and AAE scores. It is to be noted that we outperform the center bias method on the AUC metric. Center bias exploits the spatial bias in egocentric images and always predicts the center of the video frame as the predicted gaze position. The significant improvement in the AUC metric indicates that our approach predicts gaze fixations that are more closely aligned with the ground truth than the center bias approach. Although we do not return a specific gaze position, we outperform all baselines except center bias on the AAE metric. Given that the model was not designed explicitly for this task, it is a remarkable performance, especially given the performance of fully supervised baselines such as DFG [50], which achieve 10.6 and 88.3 for AUC and AAE.",
            "score": 0.3119367982720124,
            "section_title": "Unsupervised Egocentric Gaze Prediction",
            "char_start_offset": 29424,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1250
                }
            ],
            "ref_mentions": [
                {
                    "start": 1200,
                    "end": 1204,
                    "matchedPaperCorpusId": "5911448"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.118408203125
        },
        {
            "corpus_id": "259904068",
            "title": "Towards Precision Medicine in Spinal Surgery: Leveraging AI Technologies",
            "text": "I am writing to provide a critique and analysis of the recent paper titled \"Will ChatGPT/GPT-4 be a Lighthouse to Guide Spinal Surgeons?\" by He et al. [1]. While the authors discuss potential applications of OpenAI's advanced large language model GPT-4 and its chatbot-style interface ChatGPT in spinal surgery, it is crucial to examine the broader implications of technological determinism, biases, challenges in surgical domains, access and equity issues, cost implications, and global disparities in technology adoption within the context of clinical medicine and the biomedical enterprise. \n\nFirstly, it is essential to address the issue of algorithmic bias in AI applications. Algorithms, including language models like GPT-4, can inherit and perpetuate biases present in the data they are trained on. These biases may disproportionately impact marginalized populations, leading to unequal healthcare outcomes [2,3]. It is crucial to thoroughly evaluate and mitigate biases to ensure fair and equitable treatment for all patients. For example, Ahsen et al.'s study on breast cancer diagnosis demonstrated that biases inherited by algorithms from data generated by humans can diminish performance [4]. However, a bias-aware algorithm integrated into a clinical decision support system was able to mitigate the adverse impact of bias, improving the accuracy of decisions based on mammography. Specific examples of algorithmic biases relevant to spinal surgery must be explored to assess their potential impact on diagnostic accuracy, treatment recommendations, and surgical decision-making. Once identified, mitigating algorithmic biases in AI applications, including language models like GPT-4, is crucial in spinal surgery to ensure equitable care. \n\nFurthermore, when considering the integration of AI technologies in surgical domains, unique challenges arise. Surgical procedures require real-time decision-making, precise manual dexterity, and adaptability to unforeseen circumstances. While ChatGPT/GPT-4 may assist with information and decision support, its ability to account for dynamic surgical situations and respond appropriately remains unverified. The limitations and challenges faced by AI in surgical domains should be acknowledged, including the need for rigorous validation, addressing technical limitations, and ensuring seamless integration with existing surgical workflows (Table 1).",
            "score": 0.3119255190849459,
            "section_title": "Dear Editor,",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2407
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 154,
                    "matchedPaperCorpusId": "258189488"
                },
                {
                    "start": 915,
                    "end": 918,
                    "matchedPaperCorpusId": "220047415"
                },
                {
                    "start": 918,
                    "end": 920,
                    "matchedPaperCorpusId": "59603476"
                },
                {
                    "start": 1201,
                    "end": 1204,
                    "matchedPaperCorpusId": "148575208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24169921875
        },
        {
            "corpus_id": "212752168",
            "title": "Reduced egocentric bias when perspective-taking compared with working from rules",
            "text": "For example, in the Director Task, the instruction to select the \"top cup\" will, on critical trials, require a different response depending on viewpoint. A self-perspective response when the task was to take the other perspective is a clear indication of a failure to take perspectives and a reliance instead on what the participant herself sees. In our study, the correct answer was always the same filtered colour regardless of perspective, and as a result we may not know which perspective participants made responses from, with consequences for our interpretations of egocentric bias. We agree with this characterisation of our study up to a pointthere was no conflict between self and other perspectives in our study, which is unorthodox. However, this does not mean that participants could not demonstrate egocentricity. Overall, our measure of egocentricity came not from a privileged perspective vis-\u00e0-vis another agent, but rather privileged knowledge about the object that was the focus of the task. The crucial question concerned whether such bias would vary as a function of the way the task was framed.\n\nAnother point raised by an anonymous reviewer concerned whether the mere presence of the other agent might have elicited a different strategy in participants, independently of the instruction to take that agent's perspective. There was no evidence, however, that responses changed as a function of either the presence of an agent or the instruction to take that agent's perspective. This is indicated by the absence of a main effect of or interaction with Group in Experiment 1, and the absence of an effect of condition in the Perspective-Taking group in Experiment 2. Moreover, given that we explicitly instructed participants to take the agent's perspective, not just to remain aware that there was another agent present in the room, and that the agent could not observe the participant's responses on the colour scale in any case, it is to our minds more likely that the attenuation of egocentric bias in Experiment 2 was due to perspective-taking specifically. Nevertheless, it is very difficult with our data to separate any effect of the agent's presence from the instruction to take that agent's perspective, as the two always co-occurred, and we cannot definitively rule this possibility out.\n\nThe contrast between performance while perspectivetaking relative to performance while not perspective-taking is also relatively uncommon in the literature, but was the principal focus of the present",
            "score": 0.31165732091176945,
            "section_title": "General discussion",
            "char_start_offset": 41605,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.226806640625
        },
        {
            "corpus_id": "267627509",
            "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF",
            "text": "Incorporating recent advances in automated evaluation [Dubois et al., 2023, Zheng et al., 2023a, Chiang et al., 2023], we use model-based metrics for large-scale studies. We use GPT-4 [OpenAI, 2023] as the judge to compare two responses for each prompt. We use the same prompt as Chen et al. [2023], where GPT-4 is asked to give a rating for each response when both responses are present in the input; see Appendix D for details. By comparing the two ratings, the result can be win, tie, or lose. To counter positional bias in GPT-4 ratings [Wang et al., 2023a], we collect two sets of ratings by alternating the order of test and baseline model responses. A winning response must receive at least one win and at most one tie. This protocol can mitigate the positional bias and improve the rating quality of GPT-4 as reported by Chiang et al. [2023]. After counting number of win, tie and lose for the test model, we use the Win Score as defined in Eq. 6 as the aggregated metric. To show the relative improvement each model obtained compared with the SFT baseline (Vicuna-7B), for each prompt, we use one response generated by Vicuna-7B, and collect the other one from the RL policy we want to evaluate in all our GPT-4 evaluations. Taking the length bias in the GPT-4 evaluations into account [Wang et al., 2023b], a real improvement is achieved with higher Win Score at a similar average length, therefore we use the Pareto front achieved by each method for the final judgement. To validate the results, we also select best models at different length scales and compare them with human studies. \n\nBenchmarks. For the GPT-4 evaluation and human studies, we use prompts from the LIMA [Zhou et al., 2023] test-set, which contains 300 open-ended prompts in total. We also evaluate the performance of our models on benchmarks on specific model capabilities.",
            "score": 0.3114219836475849,
            "section_title": "Models and",
            "char_start_offset": 21195,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1855
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.195556640625
        },
        {
            "corpus_id": "270123263",
            "title": "Encoding and Controlling Global Semantics for Long-form Video Question Answering",
            "text": "We inherit 3k hours of 8640 egocentric videos from the Ego4D dataset (Grauman et al., 2022). Each video is associated with about 280 dense captions of consecutive moments. Based on these captions, we create our dataset in 2 stages, i.e. questionanswer generation and data filtering. \n\nQuestion-answer generation. In this first stage, we concatenate a video's dense captions following the time order to construct its language description. We utilize GPT-4 (Achiam et al., 2023) to generate 20 questions per video. In our prompt, we encourage GPT-4 to avoid questions that are visually biased and can be answered by a short video moment. Then, we present the generated questions to GPT-4 to generate the correct answer along with 4 wrong answer choices. Data filtering. In the second stage, we filter out questions that include clue words, e.g. \"passage\", \"text\", and \"description\". Moreover, we also remove questions that GPT-4 can answer without looking at the concatenated narration or the question. Then, we adopt manual filtering by asking ten graduate students who are native English speakers to ensure the veracity and temporal certificate length for every question-answer sample. Particularly, annotators are instructed to verify that 1) questions are valid and the correct answer is indeed correct, 2) all distractor answers are incorrect, and 3) the video length to watch to determine the correct answer is at least 2 minutes. The filtering stage reduces the number of admissible questions by a factor of 4\u00d7 to 5\u00d7. We accomplish 18.8K questions for 992 videos, which we split into 80% train, 10% val, and 10% test.",
            "score": 0.31132065957984323,
            "section_title": "Ego-QA",
            "char_start_offset": 11834,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1622
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 91,
                    "matchedPaperCorpusId": "238856888"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1259765625
        },
        {
            "corpus_id": "266725603",
            "title": "Uncertainty Resolution in Misinformation Detection",
            "text": "In the era of digital content, both human-made and, more recently, AI-generated (Zhou et al., 2023), misinformation poses a significant societal challenge. The proliferation of misinformation presents various threats, including undermining public trust (Ognyanova et al., 2020), spreading health misinformation during pandemics (Li et al., 2022), and influencing political discourse (Bovet and Makse, 2019;Meel and Vishwakarma, 2020). As the landscape of information dissemination evolves, it becomes increasingly important to build reliable tools for identifying and mitigating misinformation. With the advent of large language models (LLMs), there is growing interest in utilizing these models, particularly the more advanced ones, as tools for detecting and mitigating misinformation. Previous work suggests (Pelrine et al., 2023) that models like GPT-4 can effectively evaluate the veracity of statements and thus could help reducing the spread of misinformation in the public sphere. \n\nThis paper aims to enhance misinformation mitigation tools using GPT-4, focusing on resolving uncertainties and accurately assessing the truthfulness of statements with ambiguous or incomplete context. While GPT-4 efficiently evaluates wellcontextualized statements, it struggles with statements lacking sufficient context. We identify two strategies for resolving uncertainty: querying users for missing information and web retrieval. Our work primarily centers on querying the user. Using the LIAR-New dataset, we explore various methods to improve uncertainty resolution. We formalize guidelines on when to query the user for missing information, how to formulate effective questions, and address whether supplementing missing details aids GPT-4 in resolving statement uncertainties. \n\nOur main contributions are: \n\n\u2022 Developing a comprehensive framework for classifying missing information in ambiguous statements by category, and publishing category labels for the entire LIAR-New dataset to facilitate future research in content-specific misinformation mitigation tools. \u2022 Demonstrating a 38 percentage point improvement in answerability compared to generic approaches, and a 15% Macro F1 improvement in veracity evaluation and 36% more uncertainty resolution with GPT-4 on LIAR-New when given missing information from the user.",
            "score": 0.3105165838215757,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 988
                },
                {
                    "start": 991,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1777
                },
                {
                    "start": 1780,
                    "end": 1807
                },
                {
                    "start": 1810,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2325
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 99,
                    "matchedPaperCorpusId": "257633591"
                },
                {
                    "start": 328,
                    "end": 345,
                    "matchedPaperCorpusId": "247127078"
                },
                {
                    "start": 406,
                    "end": 433,
                    "matchedPaperCorpusId": "208113547"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.100341796875
        },
        {
            "corpus_id": "267412514",
            "title": "Identifying and Improving Disability Bias in GPT-Based Resume Screening",
            "text": "As Generative AI rises in adoption, its use has expanded to include domains such as hiring and recruiting. However, without examining the potential of bias, this may negatively impact marginalized populations, including people with disabilities. To address this important concern, we present a resume audit study, in which we ask ChatGPT (specifically, GPT-4) to rank a resume against the same resume enhanced with an additional leadership award, scholarship, panel presentation, and membership that are disability-related. We find that GPT-4 exhibits prejudice towards these enhanced CVs. Further, we show that this prejudice can be quantifiably reduced by training a custom GPTs on principles of DEI and disability justice. Our study also includes a unique qualitative analysis of the types of direct and indirect ableism GPT-4 uses to justify its biased decisions and suggest directions for additional bias mitigation work. Additionally, since these justifications are presumably drawn from training data containing real-world biased statements made by humans, our analysis suggests additional avenues for understanding and addressing human bias.",
            "score": 0.3096862608974865,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.451416015625
        },
        {
            "corpus_id": "269293024",
            "title": "A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models",
            "text": "Claude-3 (RQ2) \n\nIn the overall benchmark result, we adopt GPT-4's direct output to the question as a reference for score 8, and use GPT-4 as the evaluator base for final scoring, as detailed in Setion 4.1. Although this approach is widely adopted, it may cause favor to responses similar to GPT-4. To counteract this bias, we re-evaluated the performance of the top-2 LLMs, GPT-4o, and Claude-3-opus, employing cross-validation techniques. We run 5-fold evaluation settings on randomly selected 200 cases,  using GPT and Claude as the reference generator and base evaluator respectively, and the third performed LLM, Qwen, as both reference generation and evaluation for a third-party evaluation. \n\nAs illustrated in Figure 4, when GPT-generated responses are used as references, GPT-4 consistently achieves higher scores no matter the evaluator models. When Claude-generated responses are adopted in evaluations, the results are comparable. In thirdparty assessments, GPT-4 also maintained a superior ranking over Claude-3. This order is consistent with the overall benchmark results, indicating that the potential bias towards GPT does not affect the benchmark ranking.",
            "score": 0.3086613572135061,
            "section_title": "Cross Validation between GPT-4 and",
            "char_start_offset": 20397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 17,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1172
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2098388671875
        },
        {
            "corpus_id": "258762263",
            "title": "GPT (Generative Pre-Trained Transformer)\u2014 A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions",
            "text": "Data bias is an open issue concerned with the adoption of any advancements in AI, till GPT [196]. This is also a prominent challenge for GPT and other machine-learning models. It refers to patterns or relationships in the data that do not accurately reflect the true distribution of the target population or domain. GPT models are trained on vast amounts of text data which may contain bias in language use or cultural assumptions. Still, the source of data remains undeclared, considering GPTs are trained using internet data which may have faulty, fake, and error data, GPTs may generate biased texts or information imitating the training data [197]. Such biases can be amplified in the model's output, resulting in false or unfair results. Data bias can arise from various sources, such as selection bias, labelling bias, concept drift, confounding variables, and changes in input data distribution over time. For example, suppose a dataset used to train a GPT model is dominated by a particular demographic group. In that case, the resulting model may be biased in its predictions towards that group, leading to inaccurate or unfair predictions when applied to new data. This bias can have serious consequences, especially in healthcare, finance, and law enforcement, where biased results can significantly impact human lives. To mitigate these issues, researchers have developed strategies such as diversifying the training data, debiasing the training data, modifying the model architecture, and using post-processing methods to normalize the data and create more fair and inclusive GPT models. The authors in [198] have made an in-depth analysis of the most downloaded text generation model GPT2. By examining the intersections of gender with religion, sexuality, ethnicity, political affiliation, and continental name origin, the authors evaluated prejudices associated with occupational associations among various protected categories. These biases may have inaccuracies in climatic data prediction or global warming [199]. Therefore, data bias must be of greater concern in GPT model development as the data quality of the internet is limited to avoiding producing disturbing content.",
            "score": 0.3083440659711103,
            "section_title": "D. Data Bias",
            "char_start_offset": 148329,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2194
                }
            ],
            "ref_mentions": [
                {
                    "start": 1616,
                    "end": 1621,
                    "matchedPaperCorpusId": "236950797"
                },
                {
                    "start": 2026,
                    "end": 2031,
                    "matchedPaperCorpusId": "257257492"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2120361328125
        },
        {
            "corpus_id": "258041203",
            "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models",
            "text": "training data. This can lead to the model generating outputs that favor certain political perspectives or ideologies, thereby amplifying existing biases [57,58,54,59].\n\nRecognizing and addressing these biases is crucial for ensuring that language models produce fair and equitable outcomes and do not inadvertently perpetuate harmful stereotypes or marginalize certain groups.\n\nThis paper aims to explore the question of whether language models like GPT-4 [9,26,27], its prior versions [3,4,5], or other commercial or open-source alternatives [28,29,60] that power applications like ChatGPT [8] (or similar) should be biased or unbiased, taking into account the implications and risks of both perspectives. By examining the ethical, practical, and societal consequences of each viewpoint, we hope to contribute to the ongoing discussion surrounding responsible language model development and use. Through this exploration, our goal is to provide insights that can help guide the future evolution of GPT-style and other generative language models toward more ethical, fair, and beneficial outcomes while minimizing potential harm.\n\n3. Why are generative language models prone to bias?",
            "score": 0.30787737366621404,
            "section_title": "Defining bias in generative language models",
            "char_start_offset": 6486,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 157,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 157,
                    "end": 160,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 489,
                    "end": 491,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 491,
                    "end": 493,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 543,
                    "end": 547,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.334228515625
        },
        {
            "corpus_id": "263334303",
            "title": "Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models",
            "text": "The bias analyze (RQ3). We explore the bias of three LLMs (ChatGPT, GPT-4, and our model CALM) in three datasets. Our results indicate that the inherent biases present in these datasets are relatively small. However, it is crucial to acknowledge that there is a notable risk of bias for LLMs. For potential biases within each dataset, we analyze whether the benchmark and instruction-tuning data themselves have any bias. We consider the impact of gender, age, and foreign status on German, the impact of gender on ccFraud, and the impact of age on Travel Insurance. The fundamental bias information of these datasets can be seen in Figure 3. We can find that except for the 'foreigner' in German, all of these DI values are near 1. This suggests that the majority of the original datasets are unbiased when it comes to these sensitive features. Additionally, instruction tuning itself is unbiased towards the model. \n\nTo evaluate the bias of LLMs, we calculate the Equal Opportunity Difference (EOD) and the Average Odds Difference (AOD) on these features with the predictions made by LLMs. The results are shown in Table 4. For ChatGPT and GPT-4, it indicates that they have a bias in some special cases. For example, GPT-4 is more likely to give females wrong predictions (AOD is -0.273) on the ccFraud dataset and prefer foreign workers on the German dataset (EOD is 0.289), even though the original test data is unbiased (DI close to 1); on the German dataset, ChatGPT prefers to lend money to older people (EOD is 0.137). It's also interesting to note that the potential biases that exist in both ChatGPT and GPT-4 are not completely consistent with each other ('gender' and 'age' in German, and 'gender' in ccFraud). This may be related to their training dataset and the alignment process of reinforcement learning human feedback [15,67]. For our CALM, the risk of bias is similar to ChatGPT and GPT-4, with the greatest bias found in the foreigner of German.",
            "score": 0.3076626594441731,
            "section_title": "6.2.3",
            "char_start_offset": 34871,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1966
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08197021484375
        },
        {
            "corpus_id": "276724668",
            "title": "EgoNormia: Benchmarking Physical Social Norm Understanding",
            "text": "To robustly test the utility of EGONORMIA on new data, we curate an out-of-domain test dataset based on egocentric robotic assistant footage (Zhu et al., 2024), selected as its context and embodiment are orthogonal to those seen in Ego4D. Actions and justifications are manually generated to be highly challenging, with baseline GPT-4o scoring 18.2%. 6  Using retrieval across EGONORMIA, we demonstrate improvement relative to the best non-RAG model and base GPT-4o on unseen in-domain tasks, obtaining an EGONORMIA bench 9.4% better than base GPT-4o, and 7.9% better than randomized retrieval across EGONORMIA, as shown in 6 Related Work",
            "score": 0.3074973012456882,
            "section_title": "EGONORMIA-Enhanced Results",
            "char_start_offset": 20002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 638
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07763671875
        },
        {
            "corpus_id": "273661820",
            "title": "Self-Preference Bias in LLM-as-a-Judge",
            "text": "However, by investigating specific cases of self-preference bias, as shown in Figure 1a and Table 1, we found that the bias was often not related to clear factual errors but rather to differences in response styles, such as the handling of specialized domains or the description of premises before answering. This suggests that, advanced models like GPT-4, which thoroughly understand and adhere to their predefined policies, may use the degree of alignment with these policies as a deciding factor when evaluating responses of comparable quality. \n\nThe proposed metric in Definition 4.1 is based on the fairness definition known as Equal Opportunity. Demographic Parity Calders et al. [2009] is also a prominent definition of fairness. Demographic Parity requires that the predictive distribution of a classifier be consistent across sensitive groups, regardless of the ground truth. In the context of this study, the focus is on whether the evaluations by LLMs align with human preferences, which is why Demographic Parity was not the focal point. While the bias metric based on Demographic Parity cannot demonstrate the unfairness of an evaluation, it is useful for analyzing how highly each LLM evaluator rates its own outputs within the given experimental setup. Similar to the Definition 4.1, the self-preference bias based on Demographic Parity also can be quantified as follows: \n\nThe scores derived from this metric of eight LLMs are presented in Table 2. The results indicate that GPT-4 exhibited significant bias, followed by Vicuna-13b, which aligns closely with the results obtained using Definition 4.1. \n\nTable 2: bias scores based on Demographic Parity. GPT-4 assigns the highest scores to its own outputs, followed by Vicuna-13b. However, it is important to note that these scores do not take intrinsic quality into account. Therefore, this analysis reflects how highly each LLM evaluator rates its own outputs within the given experimental setup and should not be interpreted as an indication of unjust evaluation.",
            "score": 0.30627748957349615,
            "section_title": "Discussion",
            "char_start_offset": 21931,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 2032
                }
            ],
            "ref_mentions": [
                {
                    "start": 671,
                    "end": 692,
                    "matchedPaperCorpusId": "3945595"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.384765625
        },
        {
            "corpus_id": "267751238",
            "title": "A Chinese Dataset for Evaluating the Safeguards in Large Language Models",
            "text": "The current evaluation strategy is based on pre-defined question sets that evaluate risks in the response. In this work, we use GPT-4 to get answers to these questions for risk evaluation. This method is strongly dependent on the ability of GPT-4 to recognize the given facets of risks that each question evaluates. Hence, it is difficult to detect harmful prompts that can even successfully attack GPT-4. Besides, the current question set only covers limited aspects of potential risks for each risk type, making it hard to generalize to other risk taxonomy. For future work, we will extend the evaluation questions set and adopt prompt engineering techniques such as prompt chaining and self-verification (Weng et al., 2023) to detect risks that might be missed in the evaluation process.",
            "score": 0.306105723478794,
            "section_title": "Evaluation Strategy",
            "char_start_offset": 26691,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 790
                }
            ],
            "ref_mentions": [
                {
                    "start": 707,
                    "end": 726,
                    "matchedPaperCorpusId": "258840837"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.140869140625
        },
        {
            "corpus_id": "273374964",
            "title": "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation",
            "text": "To demonstrate that Auto-PRE effectively reduces the bias in single evaluator LLM (like GPT-4), we conducted the following experiments: Previous research (Chu et al., 2024;Zeng et al., 2023) has shown that one of the biases present in GPT-4 is that it may prefer answers generated by LLMs in the GPT series, regardless of the actual quality of the answers. To clearly demonstrate the negative impact of this bias on experimental outcomes, we used GPT-3.5-turbo and ChatGLM2-6B as two LLMs generating the answers to be tested and conducted experiments on the Xsum and NF_CATS datasets in pairwise evaluation format. rate method = p method \u2212 p human p human \u00d7 100% (1) \n\nTable 4 presents the experimental results. Rate is calculated using Equation 1, where p denotes the proportion of evaluation results that favor GPT-3.5-turbo. The rate value refers to the percentage change in the method's preference for GPT-3.5turbo compared to human preference. It is noteworthy that the rate does not necessarily correlate with accuracy, as it does not consider the order of answers. However, it can, to some extent, reflect a method's preference bias for specific answers. \n\nFrom the results, we can observe a significant performance gap between GPT-4 and Auto-PRE, with GPT-4's rate much higher than that of other methods. This suggests that GPT-4's preference for answers from GPT-3.5-turbo compromises its performance and reliability. Conversely, Auto-PRE enhances overall performance and reliability by collaborating with various types of LLMs.",
            "score": 0.30575219205994175,
            "section_title": "Bias Analysis",
            "char_start_offset": 21326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 666
                },
                {
                    "start": 669,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1537
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.404052734375
        },
        {
            "corpus_id": "259765944",
            "title": "Generative Pretraining in Multimodality",
            "text": "It is crucial to reiterate that Emu is designed exclusively for preliminary academic research and should not be deployed in specific applications without rigorous risk analyses and mitigation strategy exploration. Deployment in production environments warrants a more thorough investigation into model behavior and potential biases. \n\nGiven the extensive size of pre-training datasets and the associated training costs, curating datasets and developing models for widespread use exceeds the scope of a single research paper. However, we are open to discussing mitigation strategies to help address ethical concerns. \n\nShort-term approaches include: 1) relying on prompting to mitigate any biases and harmful outputs, 2) implementing rule-based filtering, human oversight, and evaluation to identify and block harmful information, 3) employing a discriminator model capable of classifying harmful information for enhanced blocking, 4) Emu itself can be finetuned to become a multimodal discriminator. \n\nIn the long term, strategies involve: 1) social or public policy interventions, such as regulatory frameworks and guidelines; 2) thoughtful product design, especially regarding user interface decisions; 3) advancements in AI Ethics of powerful large models, including the development of better benchmarks and improved mitigation strategies. \n\nAdditionally, to address privacy concerns, methods exist for obfuscating or generating personal human attributes like faces (Yang et al., 2022a;Maximov et al., 2020), ensuring anonymity without compromising the quality of learned representations. While this avenue is worth exploring, it is currently beyond the scope of this paper. \n\nIn conclusion, Emu is presently a model intended for preliminary research purposes only, and deployment should be deferred until the aforementioned issues are thoroughly considered and addressed. Caution must be exercised before transitioning to production environments.",
            "score": 0.30560311431905746,
            "section_title": "MITIGATION STRATEGIES",
            "char_start_offset": 22048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1342
                },
                {
                    "start": 1345,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 1469,
                    "end": 1489,
                    "matchedPaperCorpusId": "232170418"
                },
                {
                    "start": 1489,
                    "end": 1510,
                    "matchedPaperCorpusId": "218684764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4462890625
        },
        {
            "corpus_id": "266348592",
            "title": "Do LVLMs Understand Charts? Analyzing and Correcting Factual Errors in Chart Captioning",
            "text": "Our human assessments focus on comparing C2TFEC with GPT-4V by using the same annotation tasks detailed in \u00a72 for factual error identification, with the same annotators evaluating. We sampled 30 charts from each split of LVLM, LLM, and FT. For each chart, human judges are presented with a caption generated by one of the models. \n\nFigure 3 demonstrates C2TFECs superiority in multiple error categories, especially with a substantial decrease in Value Errors, over 20% better in the LVLM and LLM splits, and halving the overall error rate compared to GPT-4V. C2TFEC virtually eliminated Trend Errors, highlighting its strong error correction ability, particularly for axes-related errors like Label, Value, and Trend errors. A representative comparison is shown in Figure 4. GPT-4V's shortcomings seem to stem from its failure to accurately infer data point values from charts as evidenced in Figure 7. \n\nIn contrast, GPT-4V is better in addressing Outof-context Errors, involving information out of the chart's scope. However, GPT-4V seemed challenged in rectifying errors within captions generated by itself, particularly within the LVLM split. This observation echoes recent findings on LLMs' inability to self-correct (Huang et al., 2023a;Valmeekam et al., 2023), we find that LVLMs also cannot perform self-correction. More importantly, our human evaluation results, combined with our findings in Table 4 and Table 12, reflect that GPT-4V is subject to serious self-enhancement bias. Consequently, although GPT-4V's capabilities are formidable, we recommend not using them to assess their own outputs. \n\n7 Related Work",
            "score": 0.30544793091706735,
            "section_title": "Human Evaluation",
            "char_start_offset": 22535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1606
                },
                {
                    "start": 1609,
                    "end": 1623
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.187255859375
        },
        {
            "corpus_id": "262824773",
            "title": "Bias Testing and Mitigation in LLM-based Code Generation",
            "text": "The evaluation results are illustrated in Tab. 7, where we can observe that directly applying prompt engineering strategies (e.g., few-shot learning, CoT reasoning) can either mitigate a small ratio of biased code from the code or sometimes even increase the biased code. For example, for GPT-4, the overall CBS decreases from 59.88% to 36.23% for the zero shot learning prompt but increases to 68.56% for the few shot learning prompt. We suspect that the unexpected increase of bias is due to the lengthy extended prompt containing more frequencies of sensitive attributes, which may bring more confusion to LLMs. Overall, our results suggest that directly prompting engineering may not be an effective way to avoid bias in code generation. and Tab. 8, we observe that in Scenario 1, only a small portion of the bias has been removed from the LLM-generated code. \n\nIn contrast, most of the biases have been removed in Scenario 2. For example, when using the zero-shot prompt to guide GPT-3.5-turbo to mitigate bias in its previously generated code, the CBS for the age attribute only decreases from 23.35% to 20.36% in Scenario 1. However, in Scenario 2, the CBS of GPT-3.5-turbo generated code decreases from 23.35% to 5.39%, indicating a significant reduction in bias behavior compared to its initially generated code. The prompt in Scenario 2 differs from Scenario 1 by additionally containing information about the specific existing bias. Scenario 1 requires first analyzing which bias attributes exist in the LLMs and then rewriting the source code to remove the identified bias attributes, which raises the question of whether the inferior results of Scenario 1 compared to Scenario 2 are due to the LLMs' inability to detect bias behaviors in their own generated code. To investigate this, we fed the GPT-3.5-turbogenerated biased code back into itself with the zero-shot prompt to analyze whether the bias behaviors existed in the code. \n\nAs shown in Tab. 9, the evaluation results reveal that GPT-3.5-turbo can only detect a small percentage of the bias behaviors in its previously generated code. For instance, GPT-3.5-turbo",
            "score": 0.30539365042677,
            "section_title": "Effectiveness of prompt engineering in bias mitigation.",
            "char_start_offset": 34123,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1945
                },
                {
                    "start": 1948,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2135
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30615234375
        },
        {
            "corpus_id": "259923494",
            "title": "Coding Inequity: Assessing GPT-4's Potential for Perpetuating Racial and Gender Biases in Healthcare",
            "text": "Background. Large language models (LLMs) such as GPT-4 hold great promise as transformative tools in healthcare, ranging from automating administrative tasks to augmenting clinical decision-making. However, these models also pose a serious danger of perpetuating biases and delivering incorrect medical diagnoses, which can have a direct, harmful impact on medical care. Methods. Using the Azure OpenAI API, we tested whether GPT-4 encodes racial and gender biases and examined the impact of such biases on four potential applications of LLMs in the clinical domain---namely, medical education, diagnostic reasoning, plan generation, and patient assessment. We conducted experiments with prompts designed to resemble typical use of GPT-4 within clinical and medical education applications. We used clinical vignettes from NEJM Healer and from published research on implicit bias in healthcare. GPT-4 estimates of the demographic distribution of medical conditions were compared to true U.S. prevalence estimates. Differential diagnosis and treatment planning were evaluated across demographic groups using standard statistical tests for significance between groups. Findings. We find that GPT-4 does not appropriately model the demographic diversity of medical conditions, consistently producing clinical vignettes that stereotype demographic presentations. The differential diagnoses created by GPT-4 for standardized clinical vignettes were more likely to include diagnoses that stereotype certain races, ethnicities, and gender identities. Assessment and plans created by the model showed significant association between demographic attributes and recommendations for more expensive procedures as well as differences in patient perception. Interpretation. Our findings highlight the urgent need for comprehensive and transparent bias assessments of LLM tools like GPT-4 for every intended use case before they are integrated into clinical care. We discuss the potential sources of these biases and potential mitigation strategies prior to clinical implementation.",
            "score": 0.3053920944449816,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.662109375
        },
        {
            "corpus_id": "264832808",
            "title": "Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias",
            "text": "However, this disparity is considerably reduced when LLMs are conditioned with additional issue-related covariates. GPT-4, when conditioned with demographics and covariates, provides more accurate predictions regarding public perceptions of global warming and displays response distributions more closely aligned with survey results. This suggests that the extended \"training\" and \"alignment\" procedures employed by GPT-4 may impact algorithmic fidelity both positively and negatively, Our data reveal algorithmic bias regarding specific sub-populations. Prior research has also highlighted such bias, particularly in LLMs refined by human feedback (Santurkar et al., 2023). The estimated opinions from these LLMs tend to align more with individuals who are liberal, have higher incomes, higher education, and those who identify as non-religious or follow religious faiths other than Buddhism, Islam, and Hinduism (Santurkar et al., 2023). In our study, LLMs struggle to accurately predict the voting behaviors and beliefs about global warming of Black Americans. This finding cannot be solely attributed to the sample size, as evaluation metrics are still adequate for other racial and multi-racial groups with even smaller sample sizes. \n\nSuch inaccuracies can lead to skewed predictions and analyses, potentially marginalizing these groups and disregarding their specific concerns, particularly in discussions on critical social issues like climate justice. A comprehensive examination of algorithmic bias in LLMs regarding marginalized groups is crucial, not only in the context of algorithmic fidelity but also in various other applications of LLMs in social science research. for the researcher's target variable. Second, it is advisable to utilize advanced models like GPT-4. When LLMs are conditioned with relevant covariates, the algorithmic fidelity of  surpasses that of GPT-3.5 in all scenarios. Third, researchers should consider minimizing the number of answer options, especially for indeterminate answers like \"Don't Know,\" \"Refused,\" and \"Other.\" As with other prediction models, the predictability of LLMs diminishes as the number of answer options increases. Inclusion of indeterminate answers reduces the algorithmic fidelity of LLMs more significantly.",
            "score": 0.30538394811405173,
            "section_title": "Discussion",
            "char_start_offset": 31544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1238
                },
                {
                    "start": 1241,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2177
                },
                {
                    "start": 2178,
                    "end": 2273
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2264404296875
        },
        {
            "corpus_id": "270703657",
            "title": "Chain-of-Probe: Examing the Necessity and Accuracy of CoT Step-by-Step",
            "text": "In this research, GPT-4 was employed as an evaluator in a manner consistent with ethical guidelines. Transparency about its usage, accountability for its outputs, and mitigation of potential biases were prioritized. Data privacy and security were strictly maintained, and the AI's limitations were acknowledged, ensuring it supplemented rather than replaced human judgment. This approach aimed to enhance the research quality while upholding academic integrity and ethical standards. Refer to Appendix A.4 for the detailed implementation of the GPT-4 evaluation. \n\nBelow is a question paired with an answer provided by an AI assistant. With reference to the ground truth, you are tasked with examining the assistant's response. The primary focus of your evaluation should be to discern whether there are any inconsistencies or assertions within the reasoning that contradict established facts, rather than merely assessing the correctness of the answer itself. If no issues are found in the response, reply with",
            "score": 0.30477617524112954,
            "section_title": "Ethic Statement",
            "char_start_offset": 28096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1011
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.337890625
        },
        {
            "corpus_id": "269009821",
            "title": "SmurfCat at SemEval-2024 Task 6: Leveraging Synthetic Data for Hallucination Detection",
            "text": "The next approach by performance is our SFT E5-Mistral.The accuracy for different configurations in our synthetic data experiments can be found in Table 8.The combination of PG and DM synthetic data achieves the best results.Unexpectedly, the use of synthetic data from GPT-4 does not yield as good outcomes.This suggests that GPT-4's synthetic data may contain some inherent biases.We carried out a detailed evaluation of a particular subset and identified probable causes for bias:\n\n\u2022 For texts generated without hallucinations, they tend to be overly formal and intricate.\u2022 In cases with hallucinations, numerous instances are exceedingly convoluted, sometimes to the extent that the sentences convey the opposite meaning.Our investigation revealed that such hallucinations might not be readily detectable.It is also clear that relying solely on DM synthetic data does not sufficiently address other tasks.By contrast, a model checkpoint trained with PG synthetic data shows promising performance.Just like the MIS approach, it appears that having PG data is sufficient to address hallucinations in other tasks, provided that the target is accessible.",
            "score": 0.3046382297876877,
            "section_title": "SFT E5-Mistral",
            "char_start_offset": 17652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 55,
                    "end": 155
                },
                {
                    "start": 155,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 308
                },
                {
                    "start": 308,
                    "end": 383
                },
                {
                    "start": 383,
                    "end": 483
                },
                {
                    "start": 485,
                    "end": 575
                },
                {
                    "start": 575,
                    "end": 725
                },
                {
                    "start": 725,
                    "end": 809
                },
                {
                    "start": 809,
                    "end": 909
                },
                {
                    "start": 909,
                    "end": 1000
                },
                {
                    "start": 1000,
                    "end": 1154
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0743408203125
        },
        {
            "corpus_id": "244714897",
            "title": "Proceedings of the CSCW 2021 Workshop - Investigating and Mitigating Biases in Crowdsourced Data",
            "text": "As a first step towards mitigating biases in crowdsourced data, established cognitive debiasing strategies can be adapted into the crowdsourcing contexts so that their effectiveness can be empirically evaluated. Based on when these strategies will be applied during a data annotator's annotating process, a design space can be defined as the following: \n\n\u2022 Pre-annotation debiasing: Debiasing elements can be designed before an annotator starts the annotating process. These elements could serve two main goals: First, increase annotators' awareness of the existence and risks of their own biases, and promote their initiation in combating these biases (e.g., [18]). Second, help annotators to establish a physical and mental condition that is less vulnerable to biases (e.g., via short breaks and meditation [16,26]). \n\n\u2022 In-annotation debiasing: Debiasing elements can be designed to influence the annotators while they are determining the annotation. The main goal of these elements is to nudge annotators to consciously adopt Type 2 processing by, for example, formalizing their thinking process (e.g., as a checklist of actions or if-then rules) and grounding their annotations on sound data [25,28]. \n\n\u2022 Post-annotation debiasing: Finally, debiasing elements can be designed after the annotator provides an annotation in a task to help annotators reflect upon and critique their own annotations. These interventions aim to both enable annotators to identify any potential biases that they have been subject to in their annotations, and allow annotators to re-examine their annotations comprehensively and systematically (e.g., via examinations of competing hypothesis, feedback, interactions between annotators, etc. [8,9,33]). \n\nWe highlight a few steps to take in order to establish a comprehensive understanding of the effectiveness of various debiasing strategies on reducing biases in the crowdsourced annotations: (1) identify a few \"model annotation tasks, \"",
            "score": 0.30444795389498414,
            "section_title": "DESIGNING COGNITIVE DEBIASING STRATEGIES FOR CROWDSOURCING ANNOTATION",
            "char_start_offset": 39759,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 352
                },
                {
                    "start": 355,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1205
                },
                {
                    "start": 1208,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1733
                },
                {
                    "start": 1736,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 1197,
                    "end": 1201,
                    "matchedPaperCorpusId": "4776599"
                },
                {
                    "start": 1201,
                    "end": 1204,
                    "matchedPaperCorpusId": "70797127"
                },
                {
                    "start": 1728,
                    "end": 1731,
                    "matchedPaperCorpusId": "86725257"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1763916015625
        },
        {
            "corpus_id": "274436223",
            "title": "Towards Resource Efficient and Interpretable Bias Mitigation in Large Language Models",
            "text": "In this subsection, we analyze how the proposed bias mitigation framework performs with respect to the evaluation metrics. We adopted the default parameters of Hugging Face's transformers library [36] using the decoding strategy of nucleus sampling with top-p=0.9, max-new-tokens=15, and a temperature of 1.0. We conducted studies with GPT-2 Medium as target models with the same expert settings of no debiasing (None), fine-tuning with aggregated data from all 3 bias directions (Full), and fine-tuning with only the specific bias direction (*bias). Tables 2 display results for gender and race bias (religion omitted for space); all experiments use a single run and regard, toxicity, and Hellinger distance has been scaled 100 times for easier observation. \n\nWe repeated experiments with GPT-2 Small (omitted for space) and Medium as target models and found that there is a strong correlation between the results after bias mitigation due to both models incorporating the same debiasing signal. Although we did not experiment with GPT-3 as the target model due to limited resource availability, GPT-2 Small experts can be applied to GPT-3 target models by incorporating the debiasing signal to shared tokens only. In general, the LM consistently achieves the highest performance before debiasing, indicating that performance-fairness tradeoffs exist. Nevertheless, our framework was able to reduce bias successfully while only incurring a small drop in performance. \n\nInterpreting results for local and global bias metrics remains tricky in some cases as some models performed very well on one metric but very poorly on another. Indeed, from Section 4.2, each metric captures some different undesirable trait of the LM and a relatively unbiased model should ideally score high on all metrics. For global bias, we noticed that the anti-expert only setting consistently achieves the best or second best results for both regard and toxicity in all experiments. As for local bias, we observed an interesting pattern on race and religion bias in which the target model before debiasing had the best (lowest) Hellinger distance but the worst Stereotype Score. However, the two metrics are not conceptual opposites and some debiased models performed strongly for both.",
            "score": 0.3039955714923993,
            "section_title": "Performance on Bias Mitigation",
            "char_start_offset": 14711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1467
                },
                {
                    "start": 1470,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2263
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259521484375
        },
        {
            "corpus_id": "277621543",
            "title": "Cognitive Debiasing Large Language Models for Decision-Making",
            "text": "As patterns of deviation from norm or rationality in judgment, cognitive biases may lead to the production of inaccurate or skewed outputs [47,49,109,110]. Previous work in the IR community has focused on understanding and mitigating the influence of cognitive biases on human decision-making in interactive information-seeking scenarios, including retrieval systems [3, 7, 17-19, 28, 33, 51, 60, 62, 76, 77, 85, 105, 113, 121], recommender systems [4,15,34,61,90,101,115,128,142], and conversational assistants [2,8,21,44,51,54]. \n\nAlthough LLMs do not have cognitive structures, recent studies have demonstrated that LLMs exhibit emergent behavior that mimics human cognitive biases across various decision-making tasks [59,65,67,71,75,78,84,91,102,103,106,114]. For instance, Jones and Steinhardt [46] identify that error patterns of GPT-3 [13] and Codex [16] resemble human cognitive biases in programming tasks. Similarly, Agrawal et al. [1] discover the framing effect bias of GPT-3 [13] in clinical information extraction task. Schmidgall et al. [92] observe that the performance of LLMs significantly degrades when clinical questions contain cognitive biases, in clinical question-answering tasks. \n\nAdditionally, Koo et al. [53] find that LLMs exhibit biases as text quality evaluators. Itzhak et al. [42] suggest that LLMs develop emergent cognitive biases after training on extensive human-generated data. To mitigate the influence of cognitive bias in LLMs, Echterhoff et al. [31] propose a so-called self-help method to directly use LLMs to rewrite their own prompts. \n\nIn real-world decision-making scenarios, prompts may not contain cognitive biases or may contain multiple cognitive biases, as a result of which the self-help method fails to perform well in more realistic settings.",
            "score": 0.30351196831473404,
            "section_title": "Cognitive biases in decision-making",
            "char_start_offset": 10725,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1205
                },
                {
                    "start": 1208,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1580
                },
                {
                    "start": 1583,
                    "end": 1798
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 146,
                    "matchedPaperCorpusId": "222357440"
                },
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "6196452"
                },
                {
                    "start": 150,
                    "end": 154,
                    "matchedPaperCorpusId": "5643902"
                },
                {
                    "start": 452,
                    "end": 455,
                    "matchedPaperCorpusId": "222177417"
                },
                {
                    "start": 455,
                    "end": 458,
                    "matchedPaperCorpusId": "8224268"
                },
                {
                    "start": 458,
                    "end": 461,
                    "matchedPaperCorpusId": "257632875"
                },
                {
                    "start": 461,
                    "end": 464,
                    "matchedPaperCorpusId": "274281683"
                },
                {
                    "start": 468,
                    "end": 472,
                    "matchedPaperCorpusId": "264452037"
                },
                {
                    "start": 472,
                    "end": 476,
                    "matchedPaperCorpusId": "271711765"
                },
                {
                    "start": 476,
                    "end": 480,
                    "matchedPaperCorpusId": "254974048"
                },
                {
                    "start": 512,
                    "end": 515,
                    "matchedPaperCorpusId": "250340331"
                },
                {
                    "start": 515,
                    "end": 517,
                    "matchedPaperCorpusId": "274596669"
                },
                {
                    "start": 517,
                    "end": 520,
                    "matchedPaperCorpusId": "272366617"
                },
                {
                    "start": 520,
                    "end": 523,
                    "matchedPaperCorpusId": "269930364"
                },
                {
                    "start": 523,
                    "end": 526,
                    "matchedPaperCorpusId": "236203094"
                },
                {
                    "start": 526,
                    "end": 529,
                    "matchedPaperCorpusId": "273662122"
                },
                {
                    "start": 722,
                    "end": 726,
                    "matchedPaperCorpusId": "259859179"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "254563792"
                },
                {
                    "start": 738,
                    "end": 741,
                    "matchedPaperCorpusId": "267334679"
                },
                {
                    "start": 744,
                    "end": 747,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 747,
                    "end": 751,
                    "matchedPaperCorpusId": "258556944"
                },
                {
                    "start": 755,
                    "end": 759,
                    "matchedPaperCorpusId": "265043172"
                },
                {
                    "start": 800,
                    "end": 804,
                    "matchedPaperCorpusId": "247084098"
                },
                {
                    "start": 843,
                    "end": 847,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 989,
                    "end": 993,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1233,
                    "end": 1237,
                    "matchedPaperCorpusId": "263310448"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1114501953125
        },
        {
            "corpus_id": "265212888",
            "title": "Safer-Instruct: Aligning Language Models with Automated Preference Data",
            "text": "Table 4 shows models' harmlessness on Beaver Tail, HH, and SI datasets, evaluated by GPT-4 and Claude 3. The findings indicate that GPT-4 and Claude 3 consistently assess harmlessness, with Claude 3 exhibiting slightly more stringent evaluations. All models outperform the vanilla Alpaca model. Our model (Alpaca + SI) significantly outperforms Beaver, which is an Alpaca model of the same size but fine-tuned entirely on humanannotated data. Our model also outperforms the Alpaca model fine-tuned on the HH dataset, demonstrating the effectiveness of our SAFER-INSTRUCT pipeline. Notably, the model trained with the templated SI dataset surpassed the performance of its counterpart trained on the GPT-4 generated SI dataset. This discrepancy might be due to the occasional generation of harmful content by GPT-4, which could reduce the quality of the SI dataset. It may also be due to GPT-4's tendency to mitigate malicious prompts by generating safe responses. This behavior could inadvertently lead models trained on these responses to perceive answering such prompts as acceptable, thereby skewing the training outcome. Table 4: Models' harmlessness on the Anthropic HH dataset, the Beaver Tail dataset, and the SI dataset (ours) using GPT-4 and Claude 3 as the evaluator. The numbers denote the percentage of safe generations by the models. Alpaca + SI (GPT-4) denotes the model trained on GPT-4 generated data, while Alpaca + SI (Template) denotes the model trained on templated response with no GPT-4 involved. Our model (Alpaca + SI) significantly outperforms all Alpaca-based models at a 5% significance level. \n\nto safety from GPT-4 and Claude 3. A more finegrained analysis can be found in Appendix A.5. We also conduct an ablation study on SFT and DPO training, which can be found in Appendix A.6.",
            "score": 0.30321813967629413,
            "section_title": "Evaluation on Harmlessness",
            "char_start_offset": 24017,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1619
                },
                {
                    "start": 1622,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1809
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2333984375
        },
        {
            "corpus_id": "260351059",
            "title": "Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias",
            "text": "This observation highlights the complex interplay between reinforcement learning methodologies and The larger Flan-T5-XXL exhibits higher bias scores in decoy cheaper, certainty, and belief valid biases while demonstrating lower bias scores in decoy expensive and belief invalid biases compared to the smaller Flan-T5-XL. The decoy expensive bias discrepancy may stem from Flan-T5-XXL's preference for higher-priced products, while the belief invalid bias reduction can be attributed to the model's enhanced accuracy with neutral arguments. \n\nthe manifestation of biases. \n\nGPT4 is also biased. The results comparing GPT4 to its predecessor in the GPT series are presented in Table 4. Across our experiments, GPT4 demonstrates the highest bias score in the decoy expensive and decoy cheaper biases. Although the bias scores are lower in the certainty, belief valid, and belief invalid biases, GPT4 still exhibits significant bias levels. The decreased bias scores observed in belief biases might be attributed to the model training, at least partly aimed at enhancing logical reasoning. Part of the GPT4 training data was designed to improve reasoning skills using data from MATH (Hendrycks et al., 2021) and GSM-8K (Cobbe et al., 2021). However, since GPT4 might be different in many other ways from DaVinci-003, we cannot attribute the decreased bias scores to this specific change. Beyond that, even with possibly improved reasoning, the model had less success mitigating bias in the decoy effect, which exhibited the most pronounced bias. Furthermore, we encountered instances in the zero-shot setting where GPT4 refrained from providing explicit choices, so we report one-shot results in the fewshot format as explained later in Section 6.1 (the zero-shot results when GPT4 did answer are similar to the one-shot results). While GPT4 shows some mitigation of biases, the prominence of the decoy effect has increased, and all biases remain pronounced.",
            "score": 0.3029782402413315,
            "section_title": "Results",
            "char_start_offset": 23499,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1955
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11676025390625
        },
        {
            "corpus_id": "272826919",
            "title": "Behavioral Bias of Vision-Language Models: A Behavioral Finance View",
            "text": "Our work introduces a framework to evaluate LVLMs' behavioral bias in finance by carefully curating DynoStock, designing prompts and then evaluating on the most recent LVLMs on recency bias and authority bias. Our results show that open-source LVLMs such as LLaVA-NeXT, MobileVLM-V2, Mini-Gemini, MiniCPM-Llama3-V and Phi-3-vision are largely affected by these two biases, while the proprietary GPT-4o stands out by a significant margin. In other words, GPT-4o may exhibit superhuman performance as it is almost uninfluenced by the two human cognitive biases we study. Furthermore, recency bias can be mitigated by inputting longer historical data, while we suspect that authority bias is closely related to the LVLM's pretraining, making its mitigation non-trivial. Our results lead us to conjecture that models with larger size and trained with wellcurated data, like GPT-4o, can resist human-like biases and produce more powerful models. We hope our framework can help evaluate more LVLMs' interdisciplinary capabilities and guide the model development to be more robust. We discuss limitations in the Appendix and leave a more thorough analysis of human financial biases on LVLMs and a principled mitigation method for future work.",
            "score": 0.3028282367591733,
            "section_title": "Conclusion",
            "char_start_offset": 17699,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1235
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1776123046875
        },
        {
            "corpus_id": "275213467",
            "title": "Evidence that altercentric biases in a continuous false belief task depend on highlighting the agent's belief",
            "text": "As in experiment 1, we tested for a positive egocentric bias in the explicit task with and without including errors. This yielded evidence against a bias in both samples (Wilcoxon signed rank test with errors: A Bayesian Mann Whitney U test provided no evidence for an effect of gender (BF 10 = 0.94, W = 2178, R = 1.03, N male = 55, N female = 99, M male = \u2212 33.34, M female = \u2212 14.24, SD male = 66.87, SD female = 67.03). Finally, as in experiment 1, we analyzed the effects of the different hiding scenarios. These yielded moderate evidence against an effect of location (BF 10 = 0.23, error = 0.57 %) and very strong evidence for an effect of agent (BF 10 = 59.48, error = 0.89 %) replicating the effect observed in experiment 1 (see Appendix I, Tables I.1 and I.2 for post-hoc analyses of the different agents).",
            "score": 0.3028248728546346,
            "section_title": "Explicit task",
            "char_start_offset": 41158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 816
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.035552978515625
        },
        {
            "corpus_id": "270764854",
            "title": "The global landscape of academic guidelines for generative AI and Large Language Models",
            "text": "Transparent and accountable AI models are also explored to enable users to understand the reasoning behind outputs and assess their validity [49], [65], [90]. Additionally, strategies such as diversifying training data and implementing bias detection and mitigation are crucial in reducing the potential for biased AI-generated content [16], [47], [62]. Specific challenges like hallucination and the risk of spreading misinformation are central to the discussion. Hallucination occurs when AI systems provide answers that are statistically likely but not factually accurate, highlighting the need for post-processing, model fine-tuning, and improved data quality to mitigate this issue. Misinformation, closely linked to hallucination, underscores the importance of user responsibility in verifying accuracy, monitoring reliability, and addressing educational implications related to AI outputs [16], [47], [62], [62]. Despite the capabilities of Generative AI and LLMs, addressing these challenges requires advancements in technology, user education, and ethical considerations to ensure responsible and accurate use of these powerful tools.",
            "score": 0.3027406963109327,
            "section_title": "Balancing Innovation and Integrity: Strategies for Ethical AI/LLM Integration",
            "char_start_offset": 22929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1143
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.222900390625
        },
        {
            "corpus_id": "270688323",
            "title": "PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data",
            "text": "Lastly, we check for self-bias by the GPT evaluator towards its own outputs across both types of evaluation. We calculate the average rank of GPT-4 in the Elo leaderboard given by both the evaluators as well as the rank of all other models which were evaluated for atleast 8-10 languages. We find that of the 11 selected models, the average rank of GPT-4 increases by the highest amount (1.4 places) for evaluations performed by the GPT evaluator (Appendix H). The rationale behind using the Elo leaderboard is that simply checking the win-rate would not account for biases due to GPT evaluator giving lesser ties than humans. This indicates selfbias in GPT evaluator similar to Panickssery et al. (2024).",
            "score": 0.30221564149755775,
            "section_title": "Self-Bias",
            "char_start_offset": 24516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 705
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1236572265625
        },
        {
            "corpus_id": "270688534",
            "title": "ChatGPT as Research Scientist: Probing GPT\u2019s capabilities as a Research Librarian, Research Ethicist, Data Generator, and Data Predictor",
            "text": "Table 1 depicts the real WEAT D-scores for each construct examined, drawn from Charlesworth et al.'s (2021) metaanalytic estimates across adult corpora (39), compared to those calculated using responses from GPT-3.5 and GPT-4.Positive WEAT D-scores reflect effects in the stereotype-congruent direction based on prior research.Main results replicated prior findings: GPT's estimates based on its training data reflected a cultural preference for Female over Male, and a stronger association of Female (relative to Male) with Art vs. Science, Home vs. Work, and Reading vs. Math.\u2020 Note: We refer to GPT \"exploring its own corpus,\" which was the task asked of it.However, it should be noted that we lack insight into how other elements of GPT's training -e.g.reinforcement learning, fine-tuning -impact its responses.The effects gathered from GPT were often somewhat stronger than those reported in prior research, though this pattern is inconsistent.This may reflect the troubling tendency for AI systems to amplify biases in their training data (43)(44)(45).It is interesting to note that these effects are not generally smaller for GPT-4 versus GPT-3.5, despite efforts OpenAI has made to debias the model ( 46).This aligns with prior research showing that more powerful models tend to intrinsically learn human biases more precisely (47).\n\nThese results are promising in terms of GPT's ability to generate estimates of word embedding results, suggesting a use case in piloting this research.However, they come with some caveats.First, the inter-item correlations between GPT-3.5 and GPT-4's responses to the same word dyads were variable but modest: r = 0.382 for the Math-Reading task, r = 0.568 for the Preference task, r = 0.666 for the Work-Home task, and r = 0.554 for the Art-Science task (all Ps < .0001).These moderate correlations might indicate differences in how GPT-3.5 and GPT-4 approached the task.",
            "score": 0.30160935535230776,
            "section_title": "Results",
            "char_start_offset": 14226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 226,
                    "end": 327
                },
                {
                    "start": 327,
                    "end": 578
                },
                {
                    "start": 578,
                    "end": 661
                },
                {
                    "start": 661,
                    "end": 757
                },
                {
                    "start": 757,
                    "end": 815
                },
                {
                    "start": 815,
                    "end": 949
                },
                {
                    "start": 949,
                    "end": 1058
                },
                {
                    "start": 1058,
                    "end": 1213
                },
                {
                    "start": 1213,
                    "end": 1340
                },
                {
                    "start": 1342,
                    "end": 1493
                },
                {
                    "start": 1493,
                    "end": 1530
                },
                {
                    "start": 1530,
                    "end": 1814
                },
                {
                    "start": 1814,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 1045,
                    "end": 1049,
                    "matchedPaperCorpusId": "1389483"
                },
                {
                    "start": 1053,
                    "end": 1057,
                    "matchedPaperCorpusId": "232046102"
                },
                {
                    "start": 1335,
                    "end": 1339,
                    "matchedPaperCorpusId": "215828184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08831787109375
        },
        {
            "corpus_id": "270062858",
            "title": "Automatically Generating Numerous Context-Driven SFT Data for LLMs across Diverse Granularity",
            "text": "To conduct a more comprehensive evaluation, we utilized GPT-4 [2] (we use gpt-4-0125-preview) to judge as the referee on the baseline method and our approach.By using GPT-4 to compare the outputs of the model fine-tuned on our generated data with the baseline methods, we can better understand the advantages of our model and avoid biases stemming from manual preferences.We provide the query in the DailyM test set as inputs and get the outputs of the model fine-tuned using our method and baseline method on the DailyM dataset.To facilitate a nuanced evaluation, we categorized the queries into three levels of granularity: detail question (e.g., how deep does the abortion needle penetrate?),concept question (e.g., what is the difference between emergencies and crises?), and macro question (e.g., what impact does faith have on us?).We present the detailed guidance for categorizing in Table 11.\n\nFor the reliability of the results, we extract relevant references to the questions in the dataset corpus to assist GPT-4 in making decisions.Then, we ask GPT-4 to compare the outputs generated by the two models, with the template shown in below text box.To mitigate the potential impact of position bias of LLMs, we implement a robust evaluation strategy that for each pair of outputs, we swap their positions and queried GPT-4 twice.In cases where the two responses were not consistent, we continued to inquire until we obtained a unanimous answer.The comparison results are shown in Figure 4.",
            "score": 0.3012427919913546,
            "section_title": "G.2 GPT-4 Judge",
            "char_start_offset": 66420,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 529
                },
                {
                    "start": 529,
                    "end": 695
                },
                {
                    "start": 695,
                    "end": 838
                },
                {
                    "start": 838,
                    "end": 900
                },
                {
                    "start": 902,
                    "end": 1044
                },
                {
                    "start": 1044,
                    "end": 1157
                },
                {
                    "start": 1157,
                    "end": 1337
                },
                {
                    "start": 1337,
                    "end": 1452
                },
                {
                    "start": 1452,
                    "end": 1497
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15185546875
        },
        {
            "corpus_id": "275794644",
            "title": "Construction of Prompt Verbalizer Based on Dynamic Search Tree for Text Classification",
            "text": "Ethical issues have always been a major concern in the field of artificial intelligence, including gender bias, political stance, and other forms of discrimination. In the case of this method, the ethical concerns and potential biases primarily stem from the PLMs used. This is because the models may be influenced by biases present in their training data. As a result, when the classification task involves topics that are particularly susceptible to these biases, our method may yield biased results. \n\nAlthough there is currently no strategy that can eliminate ethical implications and potential biases-an issue that even GPT-4 cannot fully resolve-these concerns can be mitigated through post-processing techniques. For instance, rule-based filtering strategies can be established, or weighting can be applied to the results to reduce bias.",
            "score": 0.3007960318510105,
            "section_title": "H. ETHICAL IMPLICATIONS AND POTENTIAL BIASES",
            "char_start_offset": 43894,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 844
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.299072265625
        },
        {
            "corpus_id": "267039006",
            "title": "Peer review of GPT-4 technical report and systems card",
            "text": "The study provides a comprehensive review of OpenAI\u2019s Generative Pre-trained Transformer 4 (GPT-4) technical report, with an emphasis on applications in high-risk settings like healthcare. A diverse team, including experts in artificial intelligence (AI), natural language processing, public health, law, policy, social science, healthcare research, and bioethics, analyzed the report against established peer review guidelines. The GPT-4 report shows a significant commitment to transparent AI research, particularly in creating a systems card for risk assessment and mitigation. However, it reveals limitations such as restricted access to training data, inadequate confidence and uncertainty estimations, and concerns over privacy and intellectual property rights. Key strengths identified include the considerable time and economic investment in transparent AI research and the creation of a comprehensive systems card. On the other hand, the lack of clarity in training processes and data raises concerns about encoded biases and interests in GPT-4. The report also lacks confidence and uncertainty estimations, crucial in high-risk areas like healthcare, and fails to address potential privacy and intellectual property issues. Furthermore, this study emphasizes the need for diverse, global involvement in developing and evaluating large language models (LLMs) to ensure broad societal benefits and mitigate risks. The paper presents recommendations such as improving data transparency, developing accountability frameworks, establishing confidence standards for LLM outputs in high-risk settings, and enhancing industry research review processes. It concludes that while GPT-4\u2019s report is a step towards open discussions on LLMs, more extensive interdisciplinary reviews are essential for addressing bias, harm, and risk concerns, especially in high-risk domains. The review aims to expand the understanding of LLMs in general and highlights the need for new reflection forms on how LLMs are reviewed, the data required for effective evaluation, and addressing critical issues like bias and risk.",
            "score": 0.30058797113166036,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23193359375
        },
        {
            "corpus_id": "270257712",
            "title": "Language-guided Detection and Mitigation of Unknown Dataset Bias",
            "text": "In this study, we presented a language-guided framework for unknown bias detection that extracts potential dataset biases as keywords, making these biases more interpretable.We further proposed two debiasing methods: (a) leveraging Group-DRO through pseudo-annotation of detected bias, and (b) data augmentation via Stable Diffusion, leveraging acquired bias keywords as prompts.We conducted experiments on CMNIST, Waterbirds, and CelebA datasets.The experimental results show that our methods outperform existing methods without prior knowledge and are even competitive with Group-DRO that requires prior knowledge, despite being designed for scenarios where biases are unknown and bias labels are unavailable.Given that our detection framework presents biases as keywords, it is particularly suitable for real-world applications, offering high interpretability and effective bias mitigation performance.\n\nLimitations.While our framework identifies biases from captions generated by VLM, it cannot be applied to attributes that VLMs cannot represent.For instance, current VLMs seldom express hair length in captions, making it challenging to detect biases based on gender and hair length (e.g., short-haired women).Although it is possible to prompt the model to describe hair length through captions, this process requires some prior knowledge.Despite our experiments have demonstrated that our framework can detect and mitigate novel biases, it may not be capable of identifying all biases with the same effectiveness.\n\nBroader impact.Dataset bias is a well-documented concern in society, and utilizing models trained on biased datasets without acknowledging the presence of bias could yield controversial results in real-world applications, especially when the bias is related to sensitive attributes such as race or gender.In this paper, we proposed a framework to identify and mitigate potential dataset biases to reduce such risks, aiming to achieve a positive societal impact.It is conceivable that as the performance of VLMs improves, our framework will be able to mitigate an increasing range of biases.It is equally important, however, to recognize the ongoing debate regarding the potential risks of VLMs to amplify biases, as the issues and mitigation strategies addressed in [47,48].",
            "score": 0.3003551780989203,
            "section_title": "Conclusion",
            "char_start_offset": 26345,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 174,
                    "end": 379
                },
                {
                    "start": 379,
                    "end": 447
                },
                {
                    "start": 447,
                    "end": 711
                },
                {
                    "start": 711,
                    "end": 905
                },
                {
                    "start": 907,
                    "end": 919
                },
                {
                    "start": 919,
                    "end": 1051
                },
                {
                    "start": 1051,
                    "end": 1216
                },
                {
                    "start": 1216,
                    "end": 1345
                },
                {
                    "start": 1345,
                    "end": 1520
                },
                {
                    "start": 1522,
                    "end": 1537
                },
                {
                    "start": 1537,
                    "end": 1827
                },
                {
                    "start": 1827,
                    "end": 1983
                },
                {
                    "start": 1983,
                    "end": 2112
                },
                {
                    "start": 2112,
                    "end": 2296
                }
            ],
            "ref_mentions": [
                {
                    "start": 2288,
                    "end": 2292,
                    "matchedPaperCorpusId": "4384334"
                },
                {
                    "start": 2292,
                    "end": 2295,
                    "matchedPaperCorpusId": "258040970"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.254150390625
        },
        {
            "corpus_id": "275027597",
            "title": "Large language models in critical care",
            "text": "These are the result of imbalances or existing biases in the training data that the model learns from. [ 30 ] For models such as GPT-4 that have not fully disclosed model characteristics, the extent of imbalance is difficult to assess due to a lack of transparency about training data. [ 31 ] A study by Zack et al. [ 30 ] evaluated racial or gender biases in healthcare for the GPT-4 model. They found that the LLM propagated or even amplified societal biases. When GPT-4 was prompted to produce clinical vignettes, they found that the LLM consistently stereotyped demographic presentations for a multitude of diseases. Also, when generating differential diagnoses, it included diagnoses that reflected stereotypes associated with specific ethnicities and genders. Furthermore, their results showed that there was an association between demographic characteristics and recommendations for relatively expensive procedures. These results indicate that using these models for CDS could lead to inequities in care and potentially skew clinical judgment, ultimately posing significant risks to patient safety. For the clinician, caution and careful interpretation of outputs are advised. However, as these produced biases may not be immediately evident to the clinician, adequate oversight and mitigation strategies are crucial. These could include correcting for bias during the training process, fine-tuning models with representative clinical data, incorporating bias detection mechanisms, and conducting ongoing bias audits on model outputs. \n\nAdditionally, as a requirement in Article 4 of the EU Artificial Intelligence Act [ 32 ] for implementation in practice, it is mandated that those involved in the application and use of AI systems in healthcare -such as clinicians -are sufficiently trained in AI (AI literacy) to ensure safe and effective use. Training must be aligned with the technical expertise, experience, and clinical environments of healthcare professionals. Specific training programs in AI for healthcare providers could help them use LLMs responsibly, understand their limits, and follow best practices for safe use. \n\nThere are many other significant caveats to consider, including the potential generation of harmful content, [ 31 ] privacy concerns, EHR integration challenges, cost of resources, environmental footprint, and regulatory standards. Due to these challenges, a level of caution is required when utilizing LLMs.",
            "score": 0.30019816601458094,
            "section_title": "Limitations and Considerations",
            "char_start_offset": 16692,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1541
                },
                {
                    "start": 1544,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2137
                },
                {
                    "start": 2140,
                    "end": 2371
                },
                {
                    "start": 2372,
                    "end": 2448
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 109,
                    "matchedPaperCorpusId": "266365557"
                },
                {
                    "start": 286,
                    "end": 292,
                    "matchedPaperCorpusId": "267039006"
                },
                {
                    "start": 316,
                    "end": 322,
                    "matchedPaperCorpusId": "266365557"
                },
                {
                    "start": 2249,
                    "end": 2255,
                    "matchedPaperCorpusId": "267039006"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6533203125
        },
        {
            "corpus_id": "274060371",
            "title": "SedarEval: Automated Evaluation using Self-Adaptive Rubrics",
            "text": "Human annotators provide specific scores for each response without corresponding explanations, which is efficient but suboptimal for training evaluator LMs. To alleviate this issue, we use GPT-4 to generate detailed reasoning steps using Chainof-Thought. However, scoring preferences may differ between GPT-4 and human annotators, and both may make errors. To mitigate these errors and align the scoring process with human judgment, we introduce a Human-AI Consistency strategy to improve synthetic data quality. We extract final scores from the GPT-4 scoring process and compare them with human scores, retaining only the data where GPT-4 and human results are consistent, as shown in Equation 2, where H represents human scores, A represents AI scores, and I is an indicator function. \n\nThis approach only retains instances where human and AI scores are consistent and differs from rejection sampling, which uses human scores as a reward function to select the optimal output from multiple GPT-4 results.",
            "score": 0.2999517393936126,
            "section_title": "Human-AI Consistency",
            "char_start_offset": 14992,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 786
                },
                {
                    "start": 789,
                    "end": 1006
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.356689453125
        },
        {
            "corpus_id": "271903501",
            "title": "See What LLMs Cannot Answer: A Self-Challenge Framework for Uncovering LLM Weaknesses",
            "text": "To validate the effectiveness, we first compare the gold labels and model-evaluate labels of GPT-4 responses on the full 1,835 instances, where GPT-4 evaluation achieves 85.35 precision, 95.89 recall and 90.31 F-1. As GPT-4 evaluation can be biased to GPT-4 output (Liu et al., 2023), we further construct a small set that consists of 115 instances, where each gold response is completely annotated by human annotators without the assistance of GPT-4. The GPT-4 evaluation achieves 87.14 precision, 96.83 recall and 91.73 F-1 on this set, which we consider is good enough. Such a higher-recall and lower-precision evaluation indicates that GPT-4 is highly reliable in identifying correct answers but with a few errors in classifying incorrect responses as \"correct\". It also suggests that the model performance evaluated by GPT-4 can be slightly overestimated or higher than its real performance. \n\nWe also experiment with the evaluation method following Wang et al. (2023), i.e., asking LLMs to explain why the model-generated response is correct or not, and then give a label for the response. However, we do not observe significant improvement.",
            "score": 0.29991265831026526,
            "section_title": "I Evaluation Validation",
            "char_start_offset": 48405,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 896
                },
                {
                    "start": 899,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1147
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0948486328125
        },
        {
            "corpus_id": "273345229",
            "title": "LL-instruct: An Instruction-tuned model for English Language Proficiency Assessments",
            "text": "The risks and harms of language models are welldocumented. Bender et al. (2021) provides an overview, including: environmental and financial cost; unfathomable training data leading to encoded biases that reflect the dominant/hegemonic view; coherent output being mistaken as true knowledge. \n\nThis work uses GPT-4 (1.76 trillion parameters) for dataset generation. In addition to increased water consumption and carbon emissions (Strubell et al., 2020;George et al., 2023) when using a larger model, there is the risk of including harmful biases and misinformation in both the training data and in the fine-tuned models. The data was spot-checked and filtered using another LLM to remove factual data. To mitigate bias and fairness issues, we recommend adding additional checks, such as those implemented in Stowe et al. (2024), and involving human reviewers before rolling out any machine-generated content to learners. \n\nWe hope to show that a smaller model (i.e., a model that consumes less resources) can achieve the same performance as that of a larger model when training data is available. While smaller models are more accessible, they remain difficult to access in resource-limited environments where GPU compute is rare or expensive.",
            "score": 0.29984204692031124,
            "section_title": "Ethics",
            "char_start_offset": 24119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 59,
                    "end": 291
                },
                {
                    "start": 294,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 921
                },
                {
                    "start": 924,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1244
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 79,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 430,
                    "end": 453,
                    "matchedPaperCorpusId": "219182397"
                },
                {
                    "start": 809,
                    "end": 828,
                    "matchedPaperCorpusId": "269302555"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2310791015625
        },
        {
            "corpus_id": "248547763",
            "title": "Visual perspective taking is not automatic in a simplified Dot task: Evidence from newly sighted children, primary school children and adults",
            "text": "Block \u00d7 Age interaction for RTs (in milliseconds) in Experiment 2 (Block 1 = self-perspective task; Block 2 = perspective-taking task). Regression lines reflect the best fit of data. The shaded bands around the regression lines represent a 95% confidence region for the regression fit. Fig. 8. Block x Age \u00d7 Condition interaction for RTs (in milliseconds) in Experiment 2 (Block 1 = self-perspective task; Block 2 = perspective-taking task). Regression lines reflect the best fit of data. The shaded bands around the regression lines represent a 95% confidence region for the regression fit. agent's perspective.\n\nUnlike Surtees and Apperly (2012), who did not observe a reduction of egocentric interference across ages, our simplified Dot task revealed two developmental patterns. First, by the age of 10, children were able to perform the task from the agent's perspective as efficiently as they had performed the task from their own perspective in the first half of the task. Second, between the ages of 5 and 10, children went from suffering strong egocentric interference in the second half of the task, to performing comparably in congruent and incongruent trials in the two blocks. These results show that a sufficiently simplified version of the Dot task (simpler, at any rate, than the task design used by Surtees and Apperly (2012) with children of the same age) can reveal development of VPT in school children.\n\nThe question remains, however, as to whether adults would suffer altercentric interference in this version of the Dot taskperhaps because they have automatized VPT over years of social interaction, whereas primary school children may not have done so yet. Alternatively, the block design used in this task may reveal that adults do not necessarily suffer altercentric interference in the Dot task, challenging the automaticity view (see also Ferguson et al., 2017;del Sette et al., 2021). In addition, it would be interesting to see whether, in a very simple perspective-taking task, adults develop strategies during the course of the task to reduce both altercentric and egocentric biases.",
            "score": 0.2998323275753133,
            "section_title": "Fig. 7.",
            "char_start_offset": 34698,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 621,
                    "end": 647,
                    "matchedPaperCorpusId": "4982252"
                },
                {
                    "start": 1315,
                    "end": 1341,
                    "matchedPaperCorpusId": "4982252"
                },
                {
                    "start": 1866,
                    "end": 1888,
                    "matchedPaperCorpusId": "4949381"
                },
                {
                    "start": 1888,
                    "end": 1911,
                    "matchedPaperCorpusId": "238420796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05706787109375
        },
        {
            "corpus_id": "266875646",
            "title": "Generative AI in the Era of 'Alternative Facts'",
            "text": "When we present personalized GPT-4 disinformation to Amazon Mechanical Turk workers along with real news and human-written misinformation, we do not find that this disinformation is particularly deceptive overall compared to other false content. Workers achieve 82.32% accuracy at labeling real news, 32.30% accuracy at labeling human-written misinformation, and 35.84% \n\naccuracy at labeling the GPT-4 disinformation. \n\nHowever, we do find that worker labeling of personalized disinformation reliability is correlated using Pearson's R with alignment scores ( 8.9e-08). This indicates that workers are less able  = to discern the factuality of AI-generated disinformation if it specifically targets them, highlighting the risk of personalization being exploited by malicious actors. \n\nPresentation bias may play a role in how effectively deceptive GPT-4 generated disinformation is, especially the selection of imagery. In future work, we plan to explore this further and compare risks using more sophisticated generation strategies.",
            "score": 0.2997512487135613,
            "section_title": "Study Results",
            "char_start_offset": 37528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 418
                },
                {
                    "start": 421,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1034
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.327392578125
        },
        {
            "corpus_id": "276161500",
            "title": "FairT2I: Mitigating Social Bias in Text-to-Image Generation via Large Language Model-Assisted Detection and Attribute Rebalancing",
            "text": "Our comprehensive evaluation reveals both quantitative improvements and nuanced behavioral patterns in LLMassisted image generation methods. The human evaluation metrics demonstrate that both GPT-4o and DeepSeek-V3 maintain high image quality comparable to the baseline (scores around 4.0), while showing a slight decrease in prompt adherence (3.8-3.9 vs 4.0+). However, the most significant improvement appears in diversity scores, where both LLM-assisted methods substantially outperform the baseline (3.3-3.9 vs 2.7-2.8), indicating their effectiveness in reducing stereotypical patterns. This quantitative improvement in diversity is further supported by our non-parametric evaluation of demographic representations. While the baseline model exhibits strong stereotypical biases (e.g., 85.7% White male CEOs, 77.0% female nurses), GPT-4o achieves notably balanced distributions across professions, with no demographic group exceeding 7.1% in categories like computer programmers. However, the two LLM-assisted methods demonstrate distinct approaches to bias mitigation. GPT-4o's comprehensive attribute detection capability (identifying all gender categories in 109/131 cases) appears to contribute to its more nuanced and balanced representations. In contrast, DeepSeek-V3's tendency towards binary attribute distinctions (83 cases of binary gender detection) sometimes results in overcorrection, as evidenced by its treatment of the nurse profession where it heavily favors female representation (49.0% multiracial women) while minimizing male presence. These behavioral differences suggest that while both LLM-assisted methods effectively improve upon baseline diversity metrics, their underlying approaches to bias mitigation differ substantially. GPT-4o's more comprehensive attribute detection appears to facilitate truly balanced representations, while DeepSeek-V3's binary-focused approach, though effective at reducing traditional biases, may introduce new forms of demographic concentration. This tradeoff between diversity improvement and potential overcorrection presents an important consideration for future development of bias mitigation strategies in image generation systems.",
            "score": 0.2996741418580773,
            "section_title": "Analysis",
            "char_start_offset": 31793,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2196
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1715087890625
        },
        {
            "corpus_id": "273821836",
            "title": "On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback",
            "text": "In light of the above, it seems worth asking whether it is possible to obtain the benefits of optimizing user feedback while avoiding the risks. We try two main mitigation strategies, described below. Continued safety training. As a first approach, we try mixing in safety data throughout training, hoping that this would prevent the exploration of the most harmful behaviors during training. In particular, we mix the Anthropic HH-RLHF (Bai et al., 2022a) and PKU SafeRLHF (Ji et al., 2024) datasets into each iteration's KTO training, splitting their preference comparisons into positive and negative examples. Indeed, Ethayarajh et al. (2024) suggest that turning preference data into positive/negative examples in this manner when using KTO obtains comparable or better results than using DPO directly on preference data. However, we see that this is not particularly effective Figures 8 and 23, even when up to 75% of the training data used during KTO comes from such datasets. Moreover, in Section 4.4 we see how this mitigation technique can also backfire, making detection of harmful models harder. \n\nFiltering problematic training data. As another mitigation strategy, we attempt to leverage the fact that it should often be clear if a user is giving positive feedback to a problematic model output. For instance, in therapy-talk, it would be clear to any external judge that the conversation from Figure 2 should not be trained on as a positive example, regardless of whether it receives positive feedback. We use other LLMs as external judges rather than humans, following prior work using LLMs for judgement tasks (Zheng et al., 2023). To ensure higher judgement quality, we use GPT-4o-mini as our model that \"vetoes\" (filters) trajectories from training. The prompts used for our 'veto models' can be found in Appendix I. We tried different approaches:",
            "score": 0.29966769847104224,
            "section_title": "MITIGATION STRATEGIES ARE ONLY PARTIALLY EFFECTIVE, AND MAY GIVE A FALSE SENSE OF SAFETY",
            "char_start_offset": 25422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1865
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39208984375
        },
        {
            "corpus_id": "257952074",
            "title": "Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing",
            "text": "Recently, it has been proven that GPT-4 becomes more accurate when it adopts self-supervising learning with 30% of performance boost using the \"Reflexion\" technique. This means that the existing impressive capability of GPT-4 to carry out different tests is enhanced by a newly introduced framework that enables AI agents to imitate human-like self-reflection and self-evaluation [10,11]. Essentially, it involves additional steps in which GPT-4 creates tests to analyze its own answers, identifying mistakes and weaknesses, and subsequently revises its solutions accordingly. As a result, this framework enhances the ability of GPT-4 to perform various tests more effectively as shown in Figure 3. It is worth noting that GPT-3 and data-to-text are both technologies that belong to the field of NLG or \"Natural Language Generation\". This term refers to the process of generating text in natural language through automation. Although these two technologies may appear similar at first, they operate in distinct ways. Table 1 presents some of the differences between them.",
            "score": 0.2989812668524598,
            "section_title": "arXiv:2304.02017v6 [cs.CL] 15 Nov 2023",
            "char_start_offset": 5774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1071
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1114501953125
        },
        {
            "corpus_id": "236950692",
            "title": "Lumos: Increasing Awareness of Analytic Behavior during Visual Data Analysis",
            "text": "Encourage users to get lost in their analysis, but use awareness features to remind them. As described by the guidelines of \"fluid interaction\" [10], users may become less aware of their own process while performing in-depth analysis. Achieving this level of usability and utility in visualization tools is desirable, but raises the need for awareness functionality such as that in Lumos. Awareness features can help remind users that alternatives should be considered. Awareness of one's own activity is helpful, guidance towards best ways to mitigate may be better. Participants saw utility in interaction traces toward increasing awareness of their analysis processes. But, what's next? While users may be aware of potential biased analytic behaviors, there may not be a clear path forward to correct those. Thus it can be fruitful to explore guidance to help users actively mitigate biased analytic behaviors (e.g., by recommending data, visualizations, or filters that may draw a user's attention to overemphasized or underemphasized parts of the data). \n\nDifferent tasks call for different target distributions. \"Biased analytic behaviors\" (deviations from a baseline) in context of a movies dataset can likely be chalked up to relatively harmless preferences. However, given different analysis tasks, or different domain contexts (hiring, medical, etc), there may be a much more urgent need to ensure that the target baseline distribution is fit to the task. Lumos can provide the flexibility to specify custom target distributions accordingly. Promote awareness while maintaining user agency and control. \n\nWhile we maintain that user agency and control should be ensured, providing people with awareness of their analysis behavior has merit. At times, these goals may be at odds. For instance, if active strategies are employed for systems to automatically apply bias-mitigating measures, then agency may be compromised. These design decisions should be carefully considered when designing visualizations.",
            "score": 0.2986281093178548,
            "section_title": "Lessons Learned",
            "char_start_offset": 44437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 1058
                },
                {
                    "start": 1061,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "2334198"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.062103271484375
        },
        {
            "corpus_id": "277824504",
            "title": "The Digital Cybersecurity Expert: How Far Have We Come?",
            "text": "We observe that GPT-4-Turbo generates and answers its own questions, which can be seen as cyclical use. However, this does not impact the results in our paper. For other models, no cyclical use occurs, ensuring the validity of their results. Note that GPT-4-Turbo and GPT-4o are distinct models with different training data and methodologies. For GPT-4-Turbo, we believe there is no \"unfair cyclical use,\" as our carefully designed prompts ensure it solely relies on its summarization capabilities rather than its internal knowledge. To verify this, human experts manually examine 500 randomly selected questions to identify their corresponding source passages within the corpus. The process involves first identifying potential passages by searching for distinctive keywords in each question, followed by a thorough analysis to determine whether the passages contained all key concepts relevant to the question. A passage is considered the source if it fully encompasses these key concepts. In all cases, a corresponding passage is found, confirming that GPT-4-Turbo generates questions exclusively based on the provided material. Since the corpus is not available when answering the questions, no unfair cyclical use occurrs, ensuring the credibility of the results. \n\nAdditionally, considering the possibility that GPT-4-Turbo might introduce its own preferences when generating questions, potentially leading to bias, we conduct an evaluation to assess topic selection fairness. We randomly select three distinct knowledge points (Kerberos, Packet Sniffer, and Nikto) and asked GPT-4-Turbo, GPT-4o, Llama-3.1-70B, and Qwen-2.5-72B to extract topics from the corpus. These topics directly influence the question distribution, as five questions will be generated for each topic. Therefore, any skew in the topic distribution can reflect potential model bias. The topic distribution in semantic space (via BGE-M3) shows consistent results across the four models, with no bias observed (see Appendix C).",
            "score": 0.29860571449384665,
            "section_title": "Potential Cyclical Use and Model Bias",
            "char_start_offset": 54372,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2003
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1019287109375
        },
        {
            "corpus_id": "51964453",
            "title": "Exploring the causes of comparative optimism.",
            "text": "When evaluating vague, generalized targets (such as the average person), people have no case specific information and instead use distributional or the population base-rate. \n\nWe acknowledge overlap between egocentric thinking as an explanation for comparative optimism and Klar's idea that people use different judgment rules when evaluating singular versus generalize targets. According to Klar (Klar & Giladi, 1997;1999;Klar et al., 1996), when people make comparative judgments, they insufficiently attend to the generalized target, focusing instead on the singular target to form their judgments. The fact that people systematically conclude that the singular target is better off (e.g., less at risk) than the generalized target may arise from additional information people have (or perhaps create) when contemplating singular targets that is not present with generalized targets. \n\nEvidence that egocentric thinking and the use of different judgment mechanisms to evaluate single versus general targets can account for comparative optimism comes from a study by Weinstein (1980) in which one group of participants listed all factors that influenced their chances of experiencing a variety of events, and a second group read the list. Participants who read the list, and thus were induced to consider the behaviors of others, showed less comparative optimism in their own reports. Likewise, in a second study (Weinstein & Lachendro, 1982; see also Weinstein, 1983), participants who received detailed, personalized information about the risk status of five other students, displayed less comparative optimism in their risk judgments than did participants who did not receive this information. Indeed, participants who merely received instructions to imagine they were a typical same-sex student and then generated a list of personal risk factors as if they were that student displayed less comparative optimism than did participants who did not engage in this exercise. \n\nThe egocentric explanation would seem to suggest that comparative optimism stems primarily from a distortion in personal judgments rather than judgments for the average person because people consider actions and circumstances that will facilitate desired outcomes and fail to consider adequately impediments they are likely to encounter. It is important to note, however, there is also reason to expect that egocentric thinking may also lead people to distort their judgments of the average person's risk to the extent that they fail to adequately consider other people's intentions and goal directed behavior.",
            "score": 0.29842816430442204,
            "section_title": "Information About the Self versus the Target",
            "char_start_offset": 47889,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 886
                },
                {
                    "start": 889,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1975
                },
                {
                    "start": 1978,
                    "end": 2315
                },
                {
                    "start": 2316,
                    "end": 2588
                }
            ],
            "ref_mentions": [
                {
                    "start": 397,
                    "end": 418,
                    "matchedPaperCorpusId": "37317987"
                },
                {
                    "start": 423,
                    "end": 441,
                    "matchedPaperCorpusId": "145445921"
                },
                {
                    "start": 1069,
                    "end": 1085,
                    "matchedPaperCorpusId": "14051760"
                },
                {
                    "start": 1415,
                    "end": 1443,
                    "matchedPaperCorpusId": "145411934"
                },
                {
                    "start": 1454,
                    "end": 1470,
                    "matchedPaperCorpusId": "145420344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03985595703125
        },
        {
            "corpus_id": "259671923",
            "title": "How robust are egocentric and altercentric interference effects in social cognition? a test with explicit and implicit versions of a continuous false belief task",
            "text": "The mouse-tracking measure was only calculated for correct answers; therefore, only one average value was obtained per trial type (experimental versus control). \n\nFinally, similar to the first study, separate within-subject comparisons were conducted for egocentric and altercentric bias conditions and English-and German-speaking participants to see if experimental and control trials differ; hence a bias exists. Following Samuel et al. (2018b) and the first study of this paper, we used non-parametric tests (i.e., matched-pair Wilcoxon signed-rank Test) to conduct within-subject comparisons as the response data were not normally distributed.",
            "score": 0.2983861366601595,
            "section_title": "Bias calculation and analysis",
            "char_start_offset": 39598,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 163,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 647
                }
            ],
            "ref_mentions": [
                {
                    "start": 425,
                    "end": 446,
                    "matchedPaperCorpusId": "56176147"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0302734375
        },
        {
            "corpus_id": "267637243",
            "title": "Addressing cognitive bias in medical language models",
            "text": "To understand the effect of common cognitive biases on medical models, we first evaluate the accuracy of each model with and without bias prompts on questions from the MedQA dataset. We then introduce three novel strategies for bias mitigation. \n\nWithout bias, we report the mean accuracy of each model across the USMLE test questions in Table 1. We find gpt-4 has significantly higher performance than all other models at 72.7% accuracy, compared with the second and third best models, mixtral-8x7b and gpt-3.5, with 51.8% and 49.7% accuracy respectively. Interestingly, the most medically relevant model, pmc-llama-13b, has the lowest performance of all models with 33.4%. \n\nOnce the bias prompts are introduced, every model drops in accuracy, as shown in Figure 2. We find that gpt-4 demonstrates a worst-case accuracy drop in response to falseconsensus biases by 14.0%, but is very resilient to confirmation bias, dropping by only 0.2%. This can be compared to gpt-3.5, with an average drop in accuracy of 37.4% across all biases, and in the worst-case, only scored 23.9% on data with false consensus biases. Overall, gpt-4 and mixtral-8x7b demonstrated the lowest reductions in accuracy from bias prompts, whereas the other models showed significant drops of 50% or more from original performance. \n\nThe bias which had the largest impact on the models was overwhelmingly the false consensus bias with a 24.9% decrease in model performance averaged across models. Frequency and recency biases closely follow with an 18.2% and 12.9% decrease, respectively. The least impactful bias was confirmation, at an average 8.1% decrease.",
            "score": 0.29836154101934076,
            "section_title": "Model evaluation",
            "char_start_offset": 15371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 244
                },
                {
                    "start": 247,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1302
                },
                {
                    "start": 1305,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1631
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.381591796875
        },
        {
            "corpus_id": "268950458",
            "title": "The contribution of sensory information asymmetry and bias of attribution to egocentric tendencies in effort comparison tasks",
            "text": "When comparing themselves with others, people often evaluate their own behaviors more favorably. This egocentric tendency is often categorized as a bias of attribution, with favorable self-evaluation resulting from differing explanations of one\u2019s own behavior and that of others. However, studies on information availability in social contexts offer an alternative explanation, ascribing egocentric biases to the inherent informational asymmetries between performing an action and merely observing it. Since biases of attribution and availability often co-exist and interact with each other, it is not known whether they are both necessary for the egocentric biases to emerge. In this study, we used a design that allowed us to directly compare the contribution of these two distinct sources of bias to judgements about the difficulty of an effortful task. Participants exhibited no attribution bias as judgements made for themselves did not differ from those made for others. Importantly, however, participants perceived the tasks they actively performed to be harder than the tasks they observed, and this bias was magnified as the overall task difficulty increased. These findings suggest that information asymmetries inherent to the difference between actively performing a task and observing it can drive egocentric biases in effort evaluations on their own and without a contribution from biases of attribution.",
            "score": 0.2983264892026495,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07208251953125
        },
        {
            "corpus_id": "259176887",
            "title": "GPT-4: the future of artificial intelligence in medical school assessments",
            "text": "the various ways GPT-4 can be used to enhance assessment, but there are also many potential risks. There is currently limited information about the accuracy of GPT-4 to assess practical skills and clinical knowledge, especially when applied to real-life situations. Another risk is that GPT-4 may not be able to fully understand the nuances of medical language, which could lead to errors in grading and feedback. One of the greatest challenges of using GPT-4, and AI more generally, is that it is only as unbiased as the data put into it. With the increased understanding we currently have of inequities in medical education and healthcare, GPT-4 could potentially perpetuate these biases leading to greater gaps in health and educational outcomes.\n\nWith Generation Z growing up in the social media age, where technology affords them instant access and rapid responses, GPT-4 has the potential to enhance current assessments by providing instant feedback, personalising learning experiences, improving objectivity and consistency of assessments, and reducing the workload of educators. However, in order to effectively address pre-existing concerns, and use assessment to truly enhance students' learning, AI must be considered in the broader context of the constructive alignment of learning outcomes, teaching and assessment. 5 Great attention must also be given to the possible unintended consequences that may only become apparent as AI finds its way into education practices. The use of AI in medical education is rapidly growing, and to enable us to take full advantage of all of its possible benefits, further research into how we can embrace and safely utilise to enhance the learning experience of students is needed.",
            "score": 0.29802540969115277,
            "section_title": "body",
            "char_start_offset": 4320,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1329,
                    "end": 1330,
                    "matchedPaperCorpusId": "250146297"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1632080078125
        },
        {
            "corpus_id": "235442391",
            "title": "Letter to the editor: \u201cNot all biases are bad: equitable and inequitable biases in machine learning and radiology\u201d",
            "text": "Insights into Imaging *Correspondence: Hubert.beaumont@mediantechnologies.com 2 Median Technologies, 1800 route des cr\u00eates, 06560 Valbonne, France Full list of author information is available at the end of the article based on the standards of one's own culture). This bias is discriminative because it is associated with partiality to a sub-group value. Cognitive biases regroup under different names and have been previously described in radiology as attribution biases, as mentioned by the authors. \n\nWe agree with the authors that AI systems for radiology are not free of bias, which can be deleterious or useful, and that engineers, radiologists and politicians should be aware of bias when developing/using AI algorithms. \n\nHowever, our opinions differ on how to consider and manage bias. We are aware of the emergence of equally discriminatory strategies in the fight against cultural bias, which we believe are not the best approach to tackling the issue. \n\nIndeed, debias strategies discussed by the authors include introducing another bias to compensate for the one identified as being at risk of contaminating the AI algorithm in radiology. They suggest that a 'better' ratio of ethnicity, level of wealth or patient gender must be enforced in the dataset considered, to balance a 'socially inequitable' distribution. To handle qualitative cognitive biases, they suggest that one could positively discriminate algorithm developers or involved radiologists in order to promote a diversity of opinion. \n\nThis approach can be criticized for three major reasons: \n\n\u2022 First, voluntarily injecting a correction inside the ML algorithm in radiology is driven by political motives. \n\n\u2022 Second, if these corrections are not transparent to the end user, an additional bias would be introduced, which is not consistent with the initial objective. Indeed, unlike the systematic error controlled by the developer when training the algorithm, these biases are not perceived by the radiologist and are therefore very difficult to avoid without awareness. \u2022 Third, enforcing homogeneity across identified subpopulations in the training data can lead to risky and uncontrolled situations. Unless evidence can be collected to the contrary, this runs the risk of jeopardizing the performance of the ML algorithm for other/ unidentified subpopulations, when applied to the general population.",
            "score": 0.29799742269605245,
            "section_title": "Open Access",
            "char_start_offset": 1544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 963
                },
                {
                    "start": 966,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1684
                },
                {
                    "start": 1687,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2383
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0162811279296875
        },
        {
            "corpus_id": "259937187",
            "title": "Large Language Models as Superpositions of Cultural Perspectives",
            "text": "For instance, GPT-4 in the perspective of an AI expert in gospel and classical music expressed higher tradition, while for hip-hop higher power and achievement. Furthermore, GPT-4 for a car racer and a gambler expressed higher stimulation and hedonism, while for a singer in a church choir and a volunteer in a homeless shelter higher tradition and benevolence. As different values were expressed for perspectives which did not obviously imply them, these results demonstrate the context-dependence of GPT-4's values. These results also demonstrate the presence of various stereotypes. Furthermore, there seems to be stereotypes that may not necessarily be the ones one would expect to find in many groups of humans. For instance, we observe that the perspective of an AI expert in reggae expressed higher tradition and conformity compared to that of an expert in jazz music. This could imply that a model might not only be learning common stereotypes, but also potentially creating less common stereotypes, which might not be present in the training data. We investigate various methods for inducing perspectives to determine their effectiveness across different models. We explicitly define the target values to study which models are more controllable by the User message as compared to the System message, and which by setting the perspective via the second compared to the third person. The perspective is induced with extremely more perspective intensity as defined in section 4 (see appendix 7.1 for examples). In appendix 7.4.2, we study how the effect of RLHF fine-tuning on the models' controllability. \n\nWe study the controllability of models in terms of personal values, cultural values, and personality traits on the three questionnaires discussed in section 3. To increase the robustness of our results, we administer each questionnaire 50 times with different permutations in the order of answer choices, as discussed in section 4. Due to budgetary limitations, our evaluation of GPT-4 was restricted to five permutations. However, for comparative purposes, we assessed GPT-3.5 using both 50 and five permutations. \n\nResults Table 1 compares the correspondence of various models under different settings. High correspondence indicates controllability. It is evident that GPT-3.5 and GPT-4 exhibit considerably higher controllability than other models on all the questionnaires.",
            "score": 0.2978151802575261,
            "section_title": "5.3",
            "char_start_offset": 23626,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2129
                },
                {
                    "start": 2132,
                    "end": 2219
                },
                {
                    "start": 2220,
                    "end": 2266
                },
                {
                    "start": 2267,
                    "end": 2392
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039794921875
        },
        {
            "corpus_id": "269791266",
            "title": "When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models",
            "text": "They may also inherit societal biases present in training data, disproportionately disadvantaging certain groups when making predictions in real-world 3D scenes.It is crucial to approach the use of LLMs in 3D contexts with caution, employing strategies to create more inclusive datasets, robust evaluation frameworks for bias detection and correction, and mechanisms to minimize hallucinations, ensuring responsible and equitable outcomes.",
            "score": 0.2974495196882717,
            "section_title": "CHALLENGES AND OPPORTUNITIES",
            "char_start_offset": 96099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 439
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.053009033203125
        },
        {
            "corpus_id": "277501717",
            "title": "GPT Adoption and the Impact of Disclosure Policies",
            "text": "We have so far identified a general trend of misaligned incentives regarding the adoption of GPT among the manager-agent dyad, which points to a broader dilemma in GPT adoption with and without a disclosure policy. However, our empirical investigation does not fully explore whether some managers share a more aligned preference with an analyst in utilizing GPT. This could offer a preliminary solution and recommendation for a corporate AI policy, which we will discuss in this section. \n\n4.1 Impact of Managers' Acknowledgment on Human Effort in the Alignment of GPT Adoption Among the Manager-Analyst Dyad \n\nThe findings from the previous section indicate that misalignment incentives between managers and analysts regarding the use of GPT occur in two ways: first, in situations lacking a disclosure policy, managers are reluctant to embrace GPT due to risk apprehensions. Second, with a disclosure policy in place, managers often underestimate analysts' performance and the effort required to prepare research briefs involving GPT. These situations highlight that for GPT to be effectively integrated into the manager-analyst dynamic, managers should recognize the human effort in content creation and allow the use of GPT, even amid significant risk concerns. Consequently, we focus on a specific group of managers who value analysts' efforts when utilizing GPT. This group usually possesses a more realistic view of their employees' abilities, which aids in maintaining analysts' hourly wages by controlling the number of tasks assigned in a given period. While safeguarding analysts' income, we also evaluate whether these managers have a favorable opinion of their subordinates' performance. \n\nAdditionally, we explore if implementing a disclosure policy could help mitigate the negative impacts of risk on allowing analysts' GPT use. \n\nWe consider managers who would maintain their subordinates' workload when utilizing GPT if the relative estimated number of hours used to generate the Human-GPT is not less than that of the No-GPT deck for the same RFP case. Furthermore, we do not view managers as realistic if they consider a deck that requires less than four hours of work since the actual time spent generating a deck is approximately four hours without GPT. Table 3 Column (1) shows the subsample results related to analysts' incentive to adopt GPT by using managers' deck evaluation using the exact specification as shown in Table 2 Column (1).2",
            "score": 0.29707313261647933,
            "section_title": "in GPT Adoption",
            "char_start_offset": 47494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1843
                },
                {
                    "start": 1846,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2274
                },
                {
                    "start": 2275,
                    "end": 2463
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.021697998046875
        },
        {
            "corpus_id": "254865148",
            "title": "Neuropsychology of Environmental Navigation in Humans: Review and Meta-Analysis of fMRI Studies in Healthy Participants",
            "text": "In the egocentric strategy category, we included studies in which participants had to access route knowledge of the environment and in which tasks tapped offline egocentric knowledge of an environment, by means of a landmark-based or a route-following strategy (e.g., participants had to judge the relative distance between landmarks and their own position, Rosenbaum et al. 2007). As the authors never stated whether the tasks were egocentric (ego) or allocentric (allo), two experimenters (F.N. and M.B.) independently classified the studies. They classified all but one study (in Xu et al. 2010) in the same category. The data from this study were included in the general analysis and in the individual ALE analysis of the paradigm (RL vs. F environment) but not in the analysis of the neural substrate of navigational strategies. A total of 30 experiments were defined as allocentric and 34 as egocentric (see Tables 1 and 2 for more details). \n\nAfter carrying out separate ALE analyses on the categories of studies [paradigm (recently learned vs. familiar environment) and spatial strategies (egocentric vs. allocentric strategies)], we performed two contrast analyses to directly compare the effects of the paradigms [(F > RL) and (RL > F)] and strategies [(allo > ego) and (ego > allo)]. These contrast analyses allowed highlighting voxels whose signal was greater in the first than the second condition. We also carried out a conjunction analysis of paradigms [(RL)^(F)] and strategies [(allo)^(ego)] to identify voxels that subtended both paradigm and strategy conditions. \n\nThe ALE meta-analysis was performed using GingerALE 2.1.1 (brainmap.org) with MNI coordinates (Talairach coordinates were automatically converted into MNI coordinates by GingerALE.). According to Eickhoff et al.'s (2009) modified procedure, the ALE values of each voxel in the brain were computed and a test was performed to determine the null distribution of the ALE statistic of each voxel. The FWHM value was automatically computed, because this parameter is empirically determined (Eickhoff et al. 2009).",
            "score": 0.2969964658500718,
            "section_title": "Activation Likelihood Estimation",
            "char_start_offset": 25993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 947
                },
                {
                    "start": 950,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1581
                },
                {
                    "start": 1584,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 379,
                    "matchedPaperCorpusId": "12722930"
                },
                {
                    "start": 583,
                    "end": 598,
                    "matchedPaperCorpusId": "18493836"
                },
                {
                    "start": 1780,
                    "end": 1804,
                    "matchedPaperCorpusId": "17076648"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.007724761962890625
        },
        {
            "corpus_id": "257719288",
            "title": "Evidence of a cognitive bias in the quantification of COVID-19 with CT: an artificial intelligence randomised clinical trial",
            "text": "radiology have been studied 12 . The Fleischner Society 13 briefly warned of the cognition-related risks involved in lung nodule growth estimation. More recently, Patel et al. 14 showed evidence of an anchoring cognitive bias during the pandemic concerning the coexistence of other misdiagnosed respiratory syndromes. There is still a clear gap between the well-studied, comprehensive acknowledgement of general human cognitive biases 15 and the narrower little studied effects of cognition problems in radiology. \n\nDespite the wide adoption of lung involvement scores, the area judgement cognitive bias remains unaddressed in radiology. For the first time, radiologists are required to geometrically compare such irregular and disparate shapes, on such a large scale and all the available methods have one common denominator: the reliance on the reader's volume perception. This study analyses the perception risks of these measures and clinically tests an AI-based mitigation strategy. \n\nTo statistically confirm if this problem exists, we first conducted two experiments that analyse the two-step thinking process of a radiologist analysing CTs for COVID-19. The first experiment isolated and investigated the estimation step in simulated data, then the second examined the bias's effect over the whole process. The primary hypothesis (H1) is that this geometric ratio assessment is prone to an overestimation bias. Next, a randomised clinical trial was conducted to study whether the bias could be mitigated by using a commercial AI clinical support system. The secondary hypothesis (H2) is that the reader's objectiveness can be improved with the use of computer-aided diagnosis (CAD). As the study does not propose to address the development process of a new AI CAD system, it employs Rayscape 16 , an existing commercial medical device.",
            "score": 0.29608419671530267,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1843
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 30,
                    "matchedPaperCorpusId": "3378735"
                },
                {
                    "start": 56,
                    "end": 58,
                    "matchedPaperCorpusId": "4768442"
                },
                {
                    "start": 176,
                    "end": 178,
                    "matchedPaperCorpusId": "222278276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01024627685546875
        },
        {
            "corpus_id": "272826949",
            "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios",
            "text": "Numerous methods have been developed to evaluate and mitigate these biases, across various dimensions including political bias, gender bias, and racial bias which can significantly impact the fairness and reliability of AI systems. For instance, [7] presented a methodology to proactively assess and address discriminatory potential in LLMs by analyzing prompts with demographic variations. [3] explored gender biases and highlighted Debias Tuning as an effective strategy for bias mitigation, while [7] focused on reducing political bias using a reinforcement learning approach, balancing fairness with the quality of generated text. Additionally, [4] introduced \"GPTBIAS,\" a comprehensive framework designed to rigorously evaluate biases in LLMs like GPT-4, using advanced prompts called Bias Attack Instructions. This framework provides detailed assessments of various bias types and their underlying causes, though it may not fully capture subtle or context-specific biases due to its reliance on the LLM's learned patterns and heuristics. These studies collectively underscore the ongoing efforts to refine LLMs and minimize bias in their outputs.",
            "score": 0.2959989843729314,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1152
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 249,
                    "matchedPaperCorpusId": "233476528"
                },
                {
                    "start": 500,
                    "end": 503,
                    "matchedPaperCorpusId": "233476528"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2841796875
        },
        {
            "corpus_id": "266933337",
            "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",
            "text": "However, it is demonstrated that the incorporation of differential privacy inevitably degrades the model's performance. Therefore, researchers have employed a series of techniques to augment the model's utility and make a better privacy-utility trade-off [227], [239]- [241]. Recently, with the emergence of LLMs, a growing number of studies [227], [242]- [246] are applying the DP techniques during the pre-training and fine-tuning of LLMs. Detoxifying and Debiasing. To reduce the toxicity and bias of LLMs, prior efforts mainly focus on enhancing the quality of training data and conducting safety training. \n\n\u2022 Toxic and Biased Data Interventions. Similar to the idea of privacy data intervention, toxic/biased data intervention aims to filter undesired content within large-scale web-collected datasets to derive higher-quality training data. For toxicity detection, previous work [247], [248] usually uses labeled datasets to train toxicity classifiers [249]. Some of them have developed advanced automated tools to detect the toxic data in the training corpora, such as Perspective API [250] and Azure AI Content Safety [251]. For data debiasing, the majority of studies [252]- [255] focus on removing or altering bias-related words in the corpora, such as generating a revised dataset by replacing bias-related words (e.g., gendered words) with their opposites [253] or replacing biased texts in the dataset with neutral texts [254]. However, recent work [96] finds that a simple data intervention method may increase LM loss and carry the risk of accidentally filtering out some demographic groups. As a consequence, researchers in LLMs employ varied strategies when addressing toxic and biased data. For example, GPT-4 took a proactive approach to data filtering, whereas LLaMA refrained from such interventions [2], [4]. \n\n\u2022 Safety Training. Different from the data intervention-based methods of detoxifying and debiasing, safety training is a training-based method to mitigate toxicity and bias issues.",
            "score": 0.29598629135636095,
            "section_title": "B. Mitigation in Language Models",
            "char_start_offset": 58155,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1831
                },
                {
                    "start": 1834,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 255,
                    "end": 260,
                    "matchedPaperCorpusId": "258588141"
                },
                {
                    "start": 262,
                    "end": 267,
                    "matchedPaperCorpusId": "14651570"
                },
                {
                    "start": 269,
                    "end": 274,
                    "matchedPaperCorpusId": "237353275"
                },
                {
                    "start": 342,
                    "end": 347,
                    "matchedPaperCorpusId": "258588141"
                },
                {
                    "start": 356,
                    "end": 361,
                    "matchedPaperCorpusId": "263872623"
                },
                {
                    "start": 886,
                    "end": 891,
                    "matchedPaperCorpusId": "2944650"
                },
                {
                    "start": 893,
                    "end": 898,
                    "matchedPaperCorpusId": "3573703"
                },
                {
                    "start": 1178,
                    "end": 1183,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1185,
                    "end": 1190,
                    "matchedPaperCorpusId": "259095603"
                },
                {
                    "start": 1369,
                    "end": 1374,
                    "matchedPaperCorpusId": "102352962"
                },
                {
                    "start": 1435,
                    "end": 1440,
                    "matchedPaperCorpusId": "202541569"
                },
                {
                    "start": 1463,
                    "end": 1467,
                    "matchedPaperCorpusId": "237513578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.083740234375
        },
        {
            "corpus_id": "276317810",
            "title": "SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models",
            "text": "The model associates Buddhism with compassion and altruism, stereotyping the Buddhist as the likely donor and failing to uphold fairness in its reasoning. This highlights GPT-4o-mini's inherent biases across various categories, demonstrating a gap in stereotype mitigation and the need for more robust strategies to ensure fairness and reduce reliance on oversimplified assumptions. For a quantitative assessment, we present a correlation plot between MCQ accuracy and open-ended responses in Fig. 5, highlighting the overall consistency of 92.4% between multiple-choice selections and the models' underlying reasoning. \n\nAssessing bias in LMMs across modalities: To determine where bias emerges within LMMs, we conducted an experiment evaluating only the base LLMs of the models included in our evaluation. Specifically, we evaluated all nine categories in our proposed SB-bench dataset by prompting the LLMs with textual questions from the BBQ dataset (Huang & Xiong, 2023), without any visual input. The results of this analysis are summarized in Fig. 8, revealing several important findings: (a) Despite being the most robust open-source LMM, InternVL2 demonstrates a significant gap compared to its base model, InternLM2, with an overall 22% higher bias when visual input is included. In particular, the bias scores for InternVL2 increase by 36% on Nationality, 29% on Religion, and 26% on Socio-Economic Status (SES) when compared to its LLM counterpart.",
            "score": 0.2956124491711515,
            "section_title": "Benchmarking LMMs on SB-Bench",
            "char_start_offset": 21712,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1460
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09637451171875
        },
        {
            "corpus_id": "269214354",
            "title": "Ethical-Lens: Curbing malicious usages of open-source text-to-image models",
            "text": "In our study, we delve into the specifics of a user study designed to evaluate the representation of toxicity and bias in image generation. Drawing from two datasets, Tox100 and HumanBias, we carefully selected 40 prompts from each, yielding a diverse set of 80 prompts with an even distribution of toxicity and bias issues. To rigorously assess the treatment of toxicity, each model generated one image per prompt. For bias-related prompts, to expose potential model biases towards generating specific stereotypes, we had each model produce four images; see Figure S1-2 for details. To mitigate the impact of individual subjective judgment and minimize randomness, we recruited 80 volunteers. Each volunteer was tasked with reviewing 80 sets of images, providing us with a broad spectrum of evaluations to ensure a comprehensive analysis. Table S1 and Table S2 present the relevant information about the surveyed users. The gender distribution is well-balanced, and while the age group of 23-27 stands out, the distribution across other age groups is relatively even. The user study was conducted via a straightforward online webpage, comprising a homepage and an evaluation page. On the homepage, illustrated in Figure S5, each volunteer was instructed to read through the Ethical Principles and relevant guidelines. They were reminded to appraise the value alignment of the generated images with as much objectivity as possible, disregarding variations in image quality and clarity arising from the use of different models. Within the evaluation page in Figure S6, volunteers were presented with images generated by three anonymous models and tasked to rank the images from most appropriate to least appropriate as outputs from the text-to-image models. In calculating the percentage of votes for each model based on user rankings, we allocated 2 for the model ranked first, 1 for the second, and 0 for the last. the Yi-34B 2 model to generate related responses. Since Yi-34B 2 's responses to bias-related prompts were not satisfactory, GPT-4 3 was utilized to generate bias-related corpora. For the data related to bias, we first used GPT-4 3 to generate a considerable number of prompts by using the same way as constructing the HumanBias dataset. The process was divided into three steps.",
            "score": 0.29546411688917373,
            "section_title": "Note S2. User Study Details",
            "char_start_offset": 1041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2095
                },
                {
                    "start": 2096,
                    "end": 2253
                },
                {
                    "start": 2254,
                    "end": 2295
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.171630859375
        },
        {
            "corpus_id": "268973475",
            "title": "Generative AI for Customizable Learning Experiences",
            "text": "The debate on the ethical usage of generative AI has been huge since it became widely accessible. There are already lawsuits regarding AI using someone else's work, especially when generating images and video [66]. Besides the legal aspect, among the most crucial ethical considerations of using generative AI tools are biases. Overcoming these is particularly important when using AI to generate learning materials. ChatGPT, which uses the GPT-3.5 and GPT-4 models, just like the current study, has been linked to having many gender, racial, cultural, language, ideological, and cognitive biases, as well as many more [67]. Mitigation strategies include the resampling and pre-processing of data used for training the models [68], using diverse and representative datasets [69], feedback loops [70], etc. \n\nOther ethical implications for the current state of generative AI models is their capability to generate inaccurate or misleading information. LLMs have been accused of so-called hallucinations [71], when these models generate content that is coherent and grammatically correct, but factually incorrect and nonsensical [72]. There are ways to mitigate such behavior, including self-reflection methodologies [73], answer weighting techniques [74], double checks [75], among others.",
            "score": 0.2953494769968257,
            "section_title": "Ethical Implications of Generative AI",
            "char_start_offset": 15125,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1288
                }
            ],
            "ref_mentions": [
                {
                    "start": 726,
                    "end": 730,
                    "matchedPaperCorpusId": "258180322"
                },
                {
                    "start": 774,
                    "end": 778,
                    "matchedPaperCorpusId": "261696348"
                },
                {
                    "start": 795,
                    "end": 799,
                    "matchedPaperCorpusId": "145027458"
                },
                {
                    "start": 1215,
                    "end": 1219,
                    "matchedPaperCorpusId": "266176951"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494873046875
        },
        {
            "corpus_id": "259360619",
            "title": "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations",
            "text": "As indicated by recent work of Wang et al. (2023a), LLMs are prone to position bias, describing that LLMs tend to show a preference for specific positions, even when prompted not to do so (Table 1 in Appendix). In Table 11, the win rate of GPT-3 is highly affected by its position when models generate initial reviews. GPT-3.5 highly prefers the answer in the first position compared to Claude and GPT-4. The GPT-3 win rate calculated by GPT-3.5 is 15.79% higher than the win rate based on human-annotated pairwise comparisons when GPT-3 appears first (73.68 vs 57.89). After peer discussion, all LLM reviewers have closer preferences to humans. Second, all LLMs' scores for GPT-3 answers of both positions are closer as well, indicating that the position bias is mitigated after peer discussions. \n\nFrom another perspective, Figure 6 shows the global preference of selecting answers at the first or second positions across different LLM reviewers. Overall, GPT-3.5 prefers answers at the first position. The other two models favor answers in the second position, similar to the position bias shown in Table 11. After peer discussion, it shows the same trend of mitigating position bias as well.",
            "score": 0.2948985258931115,
            "section_title": "Peer discussions help mitigate position bias",
            "char_start_offset": 31253,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 797
                },
                {
                    "start": 800,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1195
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2086181640625
        },
        {
            "corpus_id": "275920895",
            "title": "Do LLMs Have Visualization Literacy? An Evaluation on Modified Visualizations to Test Generalization in Data Interpretation",
            "text": "Using the visualizations outlined in Section 4.1.1, we then evaluated the visualization literacy of GPT-4 (model gpt-4-vision-preview) and Gemini (model gemini-pro-vision). Since we modified the data for all the charts, we updated the corresponding answers accordingly based on the experiment's questions. Initially, we tested GPT-4 following the options structure outlined in the VLAT paper, which includes an Omit choice. However, as GPT-4 tended to answer Omit when it was uncertain, we excluded the Omit choice and prompted LLMs to make an educated guess if uncertain about the answer. To prevent LLMs from generating lengthy responses to questions, which can complicate the analysis process, we constrained its output to provide only the option letter, enabling a more focused analysis. In our pilot study, we experimented with various prompts and selected the most effective one for LLMs as follows: \"You are a helpful assistant for analyzing data visualizations. Please answer with the letter corresponding to the best option, or make a random guess if unsure. For example, if option (a) is correct, only reply with (a).\" A total of 53 questions were investigated across various types of visualizations and tasks. Each question featured 2 (True/False questions) to 4 answer options (multiplechoice questions). As we found in our pilot study, the order of choices can affect the answer from LLMs. For each question, we thus conducted 120 tests with a counterbalanced order for all choices, meaning each combination of option orders was repeated between 120 / 4 = 30 and 120 / 2 = 60 times out of the total 120 tests. We shuffled all questions with a random seed to mitigate the effects of question order for the model. This results in a total of 53 \u00d7 120 = 6,360 trials. We recorded the time taken by LLMs to respond for further analysis. Given that the answer options for later questions may influence LLMs' responses to earlier questions, we conducted each question in a separate session to ensure the independence of LLMs' answers.",
            "score": 0.2943660945054324,
            "section_title": "Experiment 1: Evaluate LLMs' Visualization Literacy",
            "char_start_offset": 19051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2040
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06951904296875
        },
        {
            "corpus_id": "273374964",
            "title": "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation",
            "text": "The final evaluation results are aggregated from the original evaluation results from all evaluators. Its experiments indicate PRE can outperform various baselines, including the method that uses a single GPT-4 as an evaluator. It also effectively mitigates systematic biases associated with using a single type of LLMs as evaluators.",
            "score": 0.29408630709930167,
            "section_title": "Evaluation Methods For LLMs",
            "char_start_offset": 7579,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 334
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12939453125
        },
        {
            "corpus_id": "270372048",
            "title": "Aligning Large Language Models with Representation Editing: A Control Perspective",
            "text": "Following [12,31], we use GPT-4 as the judge, having it review and score two responses to the same prompt on a scale from 1 to 10. We provide explicit instructions to assess the responses based on criteria such as helpfulness, harmlessness, relevance, accuracy, depth, creativity, and level of detail. The detailed prompt is provided in 11. Existing works [73] have shown that GPT-4's judgments align with human evaluations over 80% of the time. We randomly sample 300 prompts from the test set of RLHF. To mitigate position bias, we randomize the order in which we present the generated responses to GPT-4, as in [73].",
            "score": 0.29403324042863355,
            "section_title": "D GPT-4 Evaluation",
            "char_start_offset": 40583,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 619
                }
            ],
            "ref_mentions": [
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "256274676"
                },
                {
                    "start": 614,
                    "end": 618,
                    "matchedPaperCorpusId": "256274676"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275146484375
        },
        {
            "corpus_id": "258822867",
            "title": "Enhancing Transformer Backbone for Egocentric Video Action Segmentation",
            "text": "We utilize two challenging egocentric video datasets to assess the performance of the proposed approach. The first dataset, Georgia Tech Egocentric Activities (GTEA) [5], comprises 28 instructional videos recorded from an egocentric perspective. It includes 11 distinct action classes representing daily kitchen activities. The second dataset, HOI4D Office Tools, is a subset of the larger HOI4D dataset [13] and consists of 553 egocentric videos depicting hand interactions with office tools such as scissors, pliers, and staplers. It covers 12 action classes related to office tool usage. To evaluate the performance of our approach, we conducted a four-fold cross-subject validation for both datasets. This evaluation technique ensures robustness and mitigates the influence of subject-specific biases on the results. \n\nFor our experiments, we closely follow the settings employed by the ASFormer model [17], our baseline. However, for the large HOI4D dataset, we make specific adjustments for efficient training. We utilized a batch size of 8, a learning rate of 0.001, and incorporated 7 attention blocks in each encoder and decoder.",
            "score": 0.2939612668719194,
            "section_title": "Datasets and Experiment Setup",
            "char_start_offset": 7741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1138
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 169,
                    "matchedPaperCorpusId": "1641563"
                },
                {
                    "start": 404,
                    "end": 408,
                    "matchedPaperCorpusId": "247222786"
                },
                {
                    "start": 906,
                    "end": 910,
                    "matchedPaperCorpusId": "239016587"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1455078125
        },
        {
            "corpus_id": "259671923",
            "title": "How robust are egocentric and altercentric interference effects in social cognition? a test with explicit and implicit versions of a continuous false belief task",
            "text": "In a mixed design, participants were tested on both egocentric and altercentric biases. They were randomly assigned either to the egocentric-first or the altercentric-first condition. Both egocentric and altercentric bias measures included two types of critical trials: experimental and control. Each participant completed two experimental and two control trials presented in blocks per bias (the order of the blocks counterbalanced). This resulted in eight trials in total. Apart from the within-subject testing of the biases, the procedure of Study 3 was the same as in Study 2. The study took approximately 15 min.",
            "score": 0.2939484171405543,
            "section_title": "Design and procedure",
            "char_start_offset": 44834,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 617
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043365478515625
        },
        {
            "corpus_id": "274763207",
            "title": "Generative AI in Medicine",
            "text": "Abundant previous work has demonstrated how biases in medical datasets can propagate into AI models . It is thus unsurprising that the use of generative models in medicine creates several equity-related challenges (243). Research indicates that larger models do not, on their own, necessarily resolve equity concerns; indeed, larger models have been shown to exhibit more covert forms of bias (i.e. prejudice against certain dialects) compared to smaller counterparts (244). ( 243) perform the largest-scale health equity evaluation of large language models to date, highlighting the complexity of equity challenges and the necessity of careful, multi-dimensional evaluations to identify and mitigate them. \n\nA first challenge is mitigating stereotypes and bias in generated text. Like human clinicians (245), LLM-generated text has been shown to display medical stereotypes (246,247). For example, (246) finds that when GPT-4 is asked to provide clinical vignettes about sarcoidosis, it generates vignettes about Black patients 97% of the time, exaggerating the true population skew. Due to these embedded biases, if patients specify their demographics when asking questions to LLM chatbots, there is a risk that the LLM will overestimate the impact of race, gender, etc. in its response. Similarly, when generating new clinical vignettes (e.g., for use in medical education), LLMs may over-index on demographic correlations (246), which would skew the knowledge of medical trainees if generated vignettes are widely used (248). It is likely that these issues can be mitigated through improved prompting and careful auditing of generated text. However, LLM stereotypes remain a key risk, both because they can be hard to detect, and because even small effect sizes can cause significant harm if the models are used at scale. Further work is necessary to better document such patterns, to properly inform users about them, and to develop mitigating strategies. \n\nA second equity-related challenge is disparities in patient awareness of, and willingness to use, generative interfaces. Recent surveys show that awareness of LLMs positively correlates with formal education level and household income (249).",
            "score": 0.2938954758856418,
            "section_title": "Centering equity",
            "char_start_offset": 44634,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 706
                },
                {
                    "start": 709,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1960
                },
                {
                    "start": 1963,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 219,
                    "matchedPaperCorpusId": "268531269"
                },
                {
                    "start": 468,
                    "end": 473,
                    "matchedPaperCorpusId": "272214842"
                },
                {
                    "start": 803,
                    "end": 808,
                    "matchedPaperCorpusId": "12394314"
                },
                {
                    "start": 875,
                    "end": 880,
                    "matchedPaperCorpusId": "266365557"
                },
                {
                    "start": 899,
                    "end": 904,
                    "matchedPaperCorpusId": "266365557"
                },
                {
                    "start": 1426,
                    "end": 1431,
                    "matchedPaperCorpusId": "266365557"
                },
                {
                    "start": 1523,
                    "end": 1528,
                    "matchedPaperCorpusId": "21855336"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3544921875
        },
        {
            "corpus_id": "270123617",
            "title": "Uncovering Bias in Large Vision-Language Models at Scale with Counterfactuals",
            "text": "We also evaluated 78k generations produced by GPT-4o in response to each of our prompts on a sub-sample of counterfactual sets. While GPT-4o has lower MaxToxicity scores than open LVLMs (Table 1), we found that this can be at least partially attributed to the model's refusal to answer when images depicting certain social groups are provided. Table 14 (Appendix C.7) provides the percentage of queries which GPT-4o refused to answer for the Characteristics prompt, broken down by the gender and physical characteristics of the individual depicted in the input image. GPT-4o refuses to answer the prompt 4-6% of the time when presented with an image depicting obese individuals, which is approximately 5x higher than its refusal percentage for other Physical-Gender groups. While the proprietary nature of GPT-4o prevents us from determining the exact cause for this behavior, one possible explanation could be guardrails preventing the API from returning toxic content that is generated by GPT-4o. This raises questions regarding fairness, as the ability to use the model for various tasks is conditional on the social attributes depicted in input images. \n\nToxicity Evaluation with a Dataset of Real Images Our use of the synthetic images for bias evaluations may raise the question of the extent to which similar biases are observed when LVLMs are presented with real images. Unfortunately, there are no natural counterfactual image datasets that cover intersectional social attributes at the scale of SocialCounterfactuals. However, the Protected-Attribute Tag Association (PATA) dataset (Seth et al., 2023) contains 4,934 images organized in 24 scenes with binary gender annotations, five ethnicracial labels, and two age group labels (young, old). \n\nWe aligned the attributes from PATA to those in    SocialCounterfactuals and evaluated LVLMs using our five main prompts, varying the random seed 15 times to produce a comparable number of model responses. While we cannot calculate MaxToxicity because PATA lacks counterfactual sets, we report the 90th percentile of Perspective API toxicity scores for each model.",
            "score": 0.29344546428837437,
            "section_title": "Evaluation of GPT-4o",
            "char_start_offset": 18490,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2120
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31982421875
        },
        {
            "corpus_id": "268856493",
            "title": "METAL: Towards Multilingual Meta-Evaluation",
            "text": "Table 4 shows the values for evaluation of overlapping datapoints between SEAHORSE and METAL for RU and TR.The values are averaged over all datapoints.For Russian, we can observe that palm_1shot does the best as it is rated highly across all metrics by both the models.Interestingly, palm_1shot is rated better than palm_finetuned.\n\nGPT-4 good summaries received very bad evaluations across all metrics by both models.On further investigation, we found that GPT-4 good category had only 2 datapoints out of the overlapping 10 datapoints, and co-incidentally these 2 generated summaries were of bad quality and have been rated poorly by human annotators as well.This indicates that GPT-4 might not always be biased towards its own generations as compared to generations from other models.We can also observe that in almost all cases GPT-4 provides a higher rating as compared to PaLM2.For Turkish, we can observe that GPT-4 good receives the highest ratings by both models.Similar to Russian, palm_1shot receives better ratings than palm_finetuned.As expected, mt5_small_250 receives the lowest ratings since it is an under-trained model.We can also notice a clear difference in ratings for GPT-4 good vs GPT-4 bad generations.Overall, from this experiment we can conclude that using our metrics and prompting methods we can compare generations from different models.",
            "score": 0.2933349983599205,
            "section_title": "Comparison of evaluation between SEAHORSE and METAL",
            "char_start_offset": 19497,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 269
                },
                {
                    "start": 269,
                    "end": 331
                },
                {
                    "start": 333,
                    "end": 418
                },
                {
                    "start": 418,
                    "end": 661
                },
                {
                    "start": 661,
                    "end": 787
                },
                {
                    "start": 787,
                    "end": 884
                },
                {
                    "start": 884,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1047
                },
                {
                    "start": 1047,
                    "end": 1137
                },
                {
                    "start": 1137,
                    "end": 1226
                },
                {
                    "start": 1226,
                    "end": 1366
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.017578125
        },
        {
            "corpus_id": "46997852",
            "title": "Does social distance modulate adults\u2019 egocentric biases when reasoning about false beliefs?",
            "text": "As well as being ethnically heterogenous, their sample was likely more culturally heterogeneous than our own. Conversely, it is perhaps unlikely that a single factor such as culture is sufficient to explain all of the difference between our results and Ryskin and Brown-Schmidt's, as the two studies differed in more than just the culture and ethnicity of the participants. For example, participants' experience with psychological tests, or proficiency in English, may have differed between our white UK/US sample and Ryskin and Brown-Schmidt's sample of either undergraduates in the United States, or Amazon Mechanical Turk workers from the United States, and these in turn could have influenced the magnitude of any egocentric bias. Also warranting further investigation is which factors contribute to making an out-group 'salient', particularly with regards to modulating egocentric intrusion in Theory of Mind. A host of correlated features, such as likeability, similarity, familiarity and group-membership have been proposed as potential modulators of egocentric intrusion (see [13] for discussion). These features are difficult to disentangle using human out-groups, although recent studies suggest inter-personal dissimilarity may account for reductions in social projection [13,17]. However, the presence of an egocentric bias in the heterospecific condition in the current study suggests that inter-agent dissimilarity is not on its own sufficient to reduce egocentric biases when reasoning about false beliefs. Furthermore, such heterospecific protagonists may offer a unique method to test and validate theories of perspective taking. As examples, heterospecific protagonists may, i) facilitate the dissociation of the impacts of likeability, similarity, familiarity and group-membership on egocentric biases in Theory of Mind, ii) offer a window to minimise the effect of socio-cultural factors on participants while still using an animate protagonist, and iii) provide an interesting contrast between inanimate non-anthropomorphised stimuli, such as arrows (e.g. [38]), inanimate anthropomorphised stimuli, such as robots (e.g. [33]), and animate human protagonists.",
            "score": 0.2931669754299239,
            "section_title": "General discussion",
            "char_start_offset": 39717,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2180
                }
            ],
            "ref_mentions": [
                {
                    "start": 1084,
                    "end": 1088,
                    "matchedPaperCorpusId": "152025863"
                },
                {
                    "start": 1283,
                    "end": 1287,
                    "matchedPaperCorpusId": "152025863"
                },
                {
                    "start": 2142,
                    "end": 2146,
                    "matchedPaperCorpusId": "23840273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06536865234375
        },
        {
            "corpus_id": "255941663",
            "title": "A Comprehensive Empirical Study of Bias Mitigation Methods for Machine Learning Classifiers",
            "text": "models, (c) bias mitigation tasks, and (d) fairness-performance metric pairs. We observe that a notable proportion (25%) of mitigation cases fall into a lose-lose tradeoff region, and that the effectiveness of bias mitigation methods depends on models, tasks, the selection of protected attributes, and the set of metrics used to assess fairness and ML performance. DL4 achieve a relatively higher percentage of the good tradeoff (a lower percentage of the inverted tradeoff) than RF. For example, the difference between LR and RF in the proportion of the good tradeoff is 12% (i.e., 39% vs. 27%). Therefore, we suggest that researchers consider different models when evaluating the effectiveness of bias mitigation methods. Figure 8(c), we find that the region distribution is task-dependent. Existing methods achieve worse fairness-performance tradeoff on imbalanced datasets than on balanced datasets. For example, the Compas dataset is the most balanced among all the datasets, and thus the classification on it is easier compared to other tasks. Therefore, as shown in Figure  8(c), we can observe that existing bias mitigation methods can retain ML performance well while mitigating bias (i.e., having a good fairness-performance tradeoff). In contrast, existing methods achieve the worst tradeoff in the most imbalanced dataset (i.e., the Bank dataset), where the major label accounts for 82.8%. Furthermore, we observe that even for the same dataset, the selection of different protected attributes also affects the region distribution. For example, the proportion of the poor tradeoff in the Adult-Sex is 15%, nearly twice the corresponding proportion in Adult-Race task (8%). This finding suggests that (1) researchers evaluate bias mitigation methods in diverse tasks to improve the generalizability of the results, (2) practitioners need to be careful when choosing bias mitigation methods for their own tasks based on existing evaluation results on other tasks, and (3) practitioners may need to choose different bias mitigation methods for different protected attributes of the same dataset.",
            "score": 0.2928686518424987,
            "section_title": "RQ3.1: Effectiveness Region Distribution.",
            "char_start_offset": 58861,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04510498046875
        },
        {
            "corpus_id": "258960339",
            "title": "Large Language Models are not Fair Evaluators",
            "text": "Specifically, we demonstrate that GPT-4 exhibits a preference for the first displayed candidate response by consistently assigning it higher scores, even when the order of candidates is subtly altered. As illustrated in Figure 1, merely swapping the presentation order can reverse evaluation outcomes. This bias is also present in Chat-GPT, which typically favors the second response. These findings highlight previously overlooked limitations in the current evaluation paradigm. \n\nTo address this issue, we propose three simple yet effective strategies to calibrate positional bias: 1) Multiple Evidence Calibration (MEC): We prompt the model to generate evaluation evidence before assigning scores, leveraging the inherent properties of causal language models for calibration. We also employ ensemble techniques to incorporate multiple evidence calibration results to further stabilize the evaluation. 2) Balanced Position Calibration (BPC): To further reduce positional bias, we evaluate each candidate in both positions across two runs and compute the final score as the average of the two runs. 3) Human In The Loop Calibration (HITLC): We also explore human-inthe-loop evaluation and consider a diversity-based method to get a cue to indicate biased candidates based on the evaluation results of MEC and BPC. \n\nTo assess the efficacy of our methods, we manually annotate the \"win/tie/lose\" outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna benchmark (Zheng et al., 2023), encompassing 80 questions spanning 9 distinct question categories. Our MEC and BPC enhance the evaluation alignment of GPT-4 and ChatGPT by 9.8% and 14.3% accuracy, respectively. Moreover, based on MEC and BPC, our HITLC can further effectively integrate human assistance into the evaluation process. Specifically, with only a 20% human annotation cost, GPT-4 and ChatGPT can achieve comparable or even better annotation alignment with the average human performance, reducing the annotation cost by up to 39%. \n\nIn summary, our key contributions are: 1) We reveal that LLMs exhibit severe positional bias, com- We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above.",
            "score": 0.29285570405463957,
            "section_title": "GPT-4",
            "char_start_offset": 2306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 479
                },
                {
                    "start": 482,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1314
                },
                {
                    "start": 1317,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 2001
                },
                {
                    "start": 2004,
                    "end": 2231
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62353515625
        },
        {
            "corpus_id": "267616740",
            "title": "Rethinking Data Selection for Supervised Fine-Tuning",
            "text": "Another crucial potential bias is verbosity bias, meaning LLM judges usually prefer longer responses. However, GPT-4 as judge mitigates this bias to large extent: Zheng et al. (2023) shows that GPT-4 is robust to repetitive attack with failure rate of only 8%; whereas other LLM judges such as ChatGPT and Claude (Anthropic, 2023) fail more than 90% of the time. Following Chen et al. (2023b), we use the evaluation prompt shown in Appendix Figure 4) and winning score of #Win\u2212#Lose #Testset + 1 where #Testset = #Win + #Tie + #Lose as evaluation metric.",
            "score": 0.2925454838827054,
            "section_title": "Baselines.",
            "char_start_offset": 6887,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 554
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 182,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11102294921875
        },
        {
            "corpus_id": "269293311",
            "title": "LLM Evaluators Recognize and Favor Their Own Generations",
            "text": "Figure 1.The strength of self-preference bias is linearly correlated with the LLM's self-recognition capability.Each point represents a model evaluated on the two properties using the CNN/Dailymail dataset.We fine-tune GPT-3.5 and Llama 2 for self-recognition or control tasks using both CNN/Dailymail (in-domain) and XSUM (out-of-domain).The scores represented by both axes can be interpreted as measures of the LLM's confidence on these properties.\n\nIn self-evaluation, as the name suggests, the same underlying LLM acts as both the evaluator and the evaluatee.As a result, the neutrality of the evaluator is in question, and the evaluation can suffer from biases where the LLM evaluators diverge from humans in systematic ways (Zheng et al., 2024;Bai et al., 2024).One such bias is self-preference, where an LLM rates its own outputs higher than texts written by other LLMs or humans, while human annotators judge them as equal quality.Self-preference has been observed in GPT-4based dialogue benchmarks (Bitton et al., 2023b;Koo et al., 2023), as well as for text summarization (Liu et al., 2023).\n\nTowards understanding and mitigating self-preference, we study self-recognition-an LLM's capability of recognizing its own outputs.We ask: Is self-preference truly selfpreference, in the sense that the LLM prefers a text because it was generated by itself?\n\nWe measure their correlation while using prompting and fine-tuning to alter the LLM's self-recognition capability.In order to provide signals for the causal link between selfrecognition and self-preference, we also fine-tune the LLM on a comprehensive set of potential confounding properties.\n\nOur main findings are as follows:\n\n1. Frontier LLMs exhibit self-preference in selfevaluation.On two summarization tasks, LLMs (GPT-3.5 Turbo, GPT-4, and Llama 2) disproportionately favor summaries written by themselves over those by other LLMs and from humans.",
            "score": 0.2922795664982518,
            "section_title": "Fine-tuning source",
            "char_start_offset": 861,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 9,
                    "end": 112
                },
                {
                    "start": 112,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 450
                },
                {
                    "start": 452,
                    "end": 563
                },
                {
                    "start": 563,
                    "end": 768
                },
                {
                    "start": 768,
                    "end": 939
                },
                {
                    "start": 939,
                    "end": 1101
                },
                {
                    "start": 1103,
                    "end": 1234
                },
                {
                    "start": 1234,
                    "end": 1359
                },
                {
                    "start": 1361,
                    "end": 1475
                },
                {
                    "start": 1475,
                    "end": 1653
                },
                {
                    "start": 1655,
                    "end": 1688
                },
                {
                    "start": 1690,
                    "end": 1749
                },
                {
                    "start": 1749,
                    "end": 1916
                }
            ],
            "ref_mentions": [
                {
                    "start": 730,
                    "end": 750,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 750,
                    "end": 767,
                    "matchedPaperCorpusId": "259095491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35009765625
        },
        {
            "corpus_id": "265466111",
            "title": "POSTER: Identifying and Mitigating Vulnerabilities in LLM-Integrated Applications",
            "text": "However, it is more likely for LLMs to generate responses that contain biases when we increase the temperature hyperparameter T . For instance, when T = 0.75, we observe that 1.40% of responses from GPT-3.5 are flagged with biases by using GPT-auto. This indicates that the randomness of LLM at higher temperature yields undesired biases. The origins of such biases are subject to our future work. \n\nHigh Temperature is Insufficient to Mitigate Our Identified Threats. From Table 8, we observe that the TSRs of privacy and disinformation risks slightly decrease as we increase the temperature hyperparameter from T = 0 to T = 1. The reason is that higher temperature hyperparameters yield LLMs to be less likely follow the pattern desired by the threat initiator, and thus lowers the TSRs. However, simply tuning the temperature hyperparameter is not sufficient to mitigate our identified threats. In the best case, tuning the temperature hyperparameter can only decrease the TSR of disinformation to 86.60% when GPT-4 is used. Furthermore, the optimal temperature hyperparameter for mitigating one risk can be suboptimal for another risk. For instance, T = 0.5 yields lowest bias risk for GPT-4 under Pertb-System (72.70%), whereas the same temperature hyperparameter renders 100% TSR of toxic content generation. To summarize, we note that overall the TSRs are still very high after tuning the temperature hyperparameter (e.g., more than 90% for insider threat and more than 70% for outsider threat). Therefore, a more effective defense needs to be designed and deployed to mitigate both threats to LLM-integrated applications.",
            "score": 0.2920573528848618,
            "section_title": "Prompt to GPT-auto to Evaluate Disinformation Risk",
            "char_start_offset": 57016,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1629
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12420654296875
        },
        {
            "corpus_id": "270219020",
            "title": "An Empirical Analysis on Large Language Models in Debate Evaluation",
            "text": "We further investigate potential biases in GPT-3.5 and GPT-4 within the context of debate evaluation.While previous research has identified various biases in LLMs, such as persona bias (Wan et al., 2023), political bias (Feng et al., 2023), and positional bias (Wang et al., 2023b), our investigation uniquely concentrates on biases affecting debate evaluation performance, a relatively unexplored arXiv:2406.00050v2[cs.CL] 4 Jun 2024 domain.\n\nSpecifically, upon comparing outcomes between scenarios where the positions of candidate responses are switched, persistent bias has been observed in both GPT-3.5 and GPT-4 toward the second candidate response presented, a positional bias induced by the prompt design.Beyond this, both models also display significant lexical biases, particularly when label sets carry connotations such as sequential or magnitude, underscoring the importance of careful selection of label verbalizers in prompt design to mitigate unintended biases (Liu et al., 2023).Moreover, our study reveals that both GPT-3.5 and GPT-4 exhibit a tendency to favor the concluding side of a debate as the winner, pointing to a potential end-of-discussion order bias.Interestingly, after all the identified biases are eliminated, GPT-3.5 still demonstrates a consistent bias, while this residual bias is less obvious for GPT-4.These insights highlight the nuanced nature of biases in LLMs and the complexity of designing fair and unbiased evaluation methodologies for debate evaluation.",
            "score": 0.2915394644893191,
            "section_title": "Introduction",
            "char_start_offset": 1633,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 101,
                    "end": 416
                },
                {
                    "start": 416,
                    "end": 442
                },
                {
                    "start": 444,
                    "end": 712
                },
                {
                    "start": 712,
                    "end": 995
                },
                {
                    "start": 995,
                    "end": 1179
                },
                {
                    "start": 1179,
                    "end": 1339
                },
                {
                    "start": 1339,
                    "end": 1498
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20166015625
        },
        {
            "corpus_id": "257038929",
            "title": "Towards Safer Generative Language Models: A Survey on Safety Risks, Evaluations, and Improvements",
            "text": "Recent instruction-following models like Chat-GPT have demonstrated the ability to act as automated agents, performing practical tasks and using tools (Xu et al., 2023b;Liu et al., 2023c). However, this has raised new safety concerns, illustrated by instances like ChaosGPT generating plans for human annihilation and GPT-4 manipulating humans to assist in CAPTCHA tests. Studies indicate that GPT-4 exhibits power-seeking behaviors such as autonomous replication and shutdown evasion (OpenAI, 2023a). Regarding such safety concerns, current safety evaluations mainly depend on manual observation. Given these models' real-world interactions, it is crucial to invest more effort in developing automated risk detectors for a more thorough monitoring of potential risks. \n\nIn the pre-training phase, language models learn from a vast array of data, which is often sourced from the Internet. While this enables the models to master complex language patterns and acquire a broad knowledge base, it also poses inherent risks. Specifically, the models may inadvertently learn and propagate biases or harmful content in the training data. As such, careful handling of data during the pre-training phase plays a critical role in mitigating models' safety risks. \n\nFiltering out undesired content from the training data is among the most commonly used approaches. This can be accomplished via heuristic rule-based methods, such as keyword matching, or by employing safety detectors with confidence scores. Safety issue detectors like BBF (Dinan et al., 2019) and Detoxify (Hanu and Unitary team, 2020), discussed in Section 3.3, can be selectively applied to identify and eliminate undesired content in the training data. Given that much of the data used for pre-training is gleaned from social media platforms, some researchers have explored author-based filtering methods. For instance, if certain authors are known for frequently posting harmful material, removing all posts from these authors can serve as an effective strategy to discard both explicit and implicit unsafe content (Dinan et al., 2019;Wang et al., 2020;Gu et al., 2022).",
            "score": 0.2911563081456343,
            "section_title": "Advanced Safety Evaluation",
            "char_start_offset": 26526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 1529,
                    "end": 1549,
                    "matchedPaperCorpusId": "201070022"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35791015625
        },
        {
            "corpus_id": "274422922",
            "title": "Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds",
            "text": "These results suggest a distinct source of enhancement and highlights the importance of image data with more reliable object arrangements. \n\nHuman Evaluation. To validate the reliability of GPT-4o for this task, we conducted a human evaluation study presented in Table 3. The results demonstrate strong correlation between GPT-4o assessments and human judgments across all metrics, and both evaluation results consistently demonstrate substantial improvements in our best fine-tuned models.",
            "score": 0.2910408335228108,
            "section_title": "QUANTITATIVE RESULTS",
            "char_start_offset": 22542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 141,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 490
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.029083251953125
        },
        {
            "corpus_id": "268032097",
            "title": "OSCaR: Object State Captioning and State Change Representation",
            "text": "We acknowledge that bias could be present in the process of collecting data for our paper. To minimize this issue, we have taken several measures. Firstly, we have collected videos from two highly diverse data sources: EPIC-KITCHENS and Ego4D. Secondly, when labeling the data using GPT-4V, we are aware that bias could occur from the behavior of GPT-4V. To address this, we regularly take test samples during the data generation process. If we detect any significant issues, we are prepared to stop the process and conduct an inspection. On the human side, we use the Amazon Mechanical Turk platform to hire people to label data for both the GPT4 zero-shot and few-shot quality assessment steps and the user study. Our data collection was classified as an approved exempt protocol by the IRB. The conversational data have been generated from this prompt: <Caption here> From this caption, give me a conversation with 10 pairs of question-answer focusing on the object's state. The answer needs to be found in the caption, and the questions can also serve as a suggestion to generate this caption and not mention the caption in the question and answer. At least 3 questions and answers require reasoning ability.",
            "score": 0.2910408335228108,
            "section_title": "Ethics Statement",
            "char_start_offset": 29308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1212
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45263671875
        },
        {
            "corpus_id": "213018627",
            "title": "An Adaptive Cue Selection Model of Allocentric Spatial Reorientation",
            "text": ". In practice, this means that they learn where targets tend to be and bias their responses toward the places they tend to be most often. Finally, adults tend to adjust their search strategy in a visual search task when there is an uneven distribution of targets in an egocentric sense (Jiang & Swallow, 2013Smith et al., 2010). This can be viewed as using egocentric prior distributions to affect decision-making. Importantly, one could view self-motion and landmark information in a homing task as egocentric information. These are the cues and the task used in a series of studies where adults were fit well by a Bayesian model (Bates & Wolbers, 2014;Chen et al., 2017;Nardini et al., 2008;Sjolund et al., 2018;Zhao & Warren, 2015). The selfmotion information could be viewed as an egocentric vector to the goal that is updated by perception of own movement. The landmark information, in this case, could be like an egocentric \"snapshot\" of how the landmarks looked at the target (home) location St\u00fcrzl et al., 2008). In other words, while landmarks are usually thought of as allocentric information, the specific way that landmarks looked from a previous home location could be stored in an egocentric format. This makes this finding fit with the idea that adults use egocentric information in a Bayesian fashion.\n\nOf course, recent research has shown that this also faces some limits and suboptimalities (Rahnev & Denison, 2018)-many Bayesian processes are distorted under certain circumstances. For example, in one study, adults integrated multiple repeats of the same audio localization signal with lower than Bayesian efficiency (Jones, 2018). It is not clear exactly why this occurred, but adults' performance was much nearer to optimal Bayesian integration when the signals were not exact repeats of each other. We should emphasize that it may be typical for egocentric information to be processed in a Bayesian way, but it will not be universal.\n\nSecond, egocentric spatial information is not used in a Bayesian manner by young children. This would explain their difficulty in making egocentric spatial judgements with an audio and a visual cue (Gori et al., 2012), difficulty combining self-motion and landmark information during",
            "score": 0.2910136241288849,
            "section_title": "Toward a More General Theory",
            "char_start_offset": 71907,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 286,
                    "end": 308,
                    "matchedPaperCorpusId": "15032413"
                },
                {
                    "start": 308,
                    "end": 327,
                    "matchedPaperCorpusId": "16481349"
                },
                {
                    "start": 631,
                    "end": 654,
                    "matchedPaperCorpusId": "739708"
                },
                {
                    "start": 654,
                    "end": 672,
                    "matchedPaperCorpusId": "3747099"
                },
                {
                    "start": 672,
                    "end": 693,
                    "matchedPaperCorpusId": "318363"
                },
                {
                    "start": 714,
                    "end": 734,
                    "matchedPaperCorpusId": "2619837"
                },
                {
                    "start": 999,
                    "end": 1019,
                    "matchedPaperCorpusId": "17546584"
                },
                {
                    "start": 1409,
                    "end": 1432,
                    "matchedPaperCorpusId": "262070630"
                },
                {
                    "start": 1637,
                    "end": 1650,
                    "matchedPaperCorpusId": "51911036"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09844970703125
        },
        {
            "corpus_id": "271089054",
            "title": "Benchmarking LLMs for Environmental Review and Permitting",
            "text": "There is a concern that GPT-4 may inherently prefer the outputs generated by the same model over others in the factual correctness evaluation. This could lead to skewed evaluation results, where GPT-4's outputs are rated more favorably, not necessarily because they are superior, but because of the inherent biases in the evaluation model (GPT-4). \n\nTo address the potential bias in the answer correctness evaluation process, we assess both factual and semantic correctness in the evaluation. For semantic correctness, we utilize the BGE (Xiao et al., 2023) as the embedding model and we calculate the semantic similarity between the model's outputs and the ground-truth answers independently of GPT-4's own evaluation mechanisms. By combining both factual and semantic correctness, we aim to accurately reflect the true performance of various models, including GPT-4.",
            "score": 0.29093303405874577,
            "section_title": "Limitations",
            "char_start_offset": 29716,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 347
                },
                {
                    "start": 350,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 868
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32861328125
        },
        {
            "corpus_id": "270688666",
            "title": "V-RECS, a Low-Cost LLM4VIS Recommender with Explanations, Captioning and Suggestions",
            "text": "Based on human evaluation and subsequent analysis by the authors, we detected common issues in V-RECS and GPT-4, comparing their outputs. Visualization errors in both systems can be categorized as follows: \n\n1) Incorrect Scaling: Both systems sometimes struggle to scale the axes correctly, resulting in unusual and confusing data presentations. For example, in Figure 9b, GPT-4 erroneously encoded years as numeric quantities, resulting in a misleading x-axis. 2) Inverted Axes: Sometimes, the models map columns to the incorrect axis, resulting in inverted axes. For example, V-RECS in Figure 9a shows such an error. 3) Non-Optimal Zoom/Spacing: Both systems fail to optimize the visualization zoom, leading to charts where data is overlapped in a small section while a large portion remains empty and white. 4) Hallucinations: GPT-4 sometimes generates misleading visualizations. As shown in Figure 9a, GPT-4 created a new bar chart version with no meaningful visual representation, violating standard visualization principles. 5) Missing Data: Both models occasionally generate correct Vega-Lite specifications but fail to map the data, resulting in an empty grid and visualization. 6) Input errors For a limited number of (Q,D) pairs, we noted that either Q is hardly interpretable even by a human, or missing/wrong data formats are present in D. This is due to problems in the NvBench dataset, already highlighted in Section V-B. Surprisingly, although these problems lead to wrong visualizations, both systems are often still able to interpret the user's information need with substantially correct narratives. \n\nAll discussed errors are common to both models, except hallucination, which we detected only in GPT-4. We postulate that V-RECS is less prone to hallucinations due to its more straightforward grammar structure for generating the visualizations. VegaLite allows users to customize visualizations extensively, which can sometimes result in misleading visual representations. Conversely, V-RECS is built upon VegaZero, a streamlined version of VegaLite. VegaZero reduced the complexity by reducing the customization options, ensuring that the focus remains on the core properties of the visualization, thereby mitigating the risk of generating erroneous visual content.",
            "score": 0.29025833571204496,
            "section_title": "C. Error Analysis",
            "char_start_offset": 56328,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 205
                },
                {
                    "start": 208,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2286
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.030853271484375
        },
        {
            "corpus_id": "259095896",
            "title": "M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning",
            "text": "To further validate the quality of the generated response, we propose to utilize the powerful GPT-4 model as a proxy of human evaluators [38,12]. Specifically, following Vicuna [5], we use GPT-4 to rate the performance of different models against our Ying-VLM. \n\nConsidering the API cost of GPT-4, 300 examples are randomly sampled from OK-VQA, A-OKVQA and ViQuAE datasets as a subset for evaluation. For each sample, we construct a prompt consisting of the original question, its corresponding reference answer, the response generated by our Ying-VLM, and a baseline system output. GPT-4 is queried with the prompt to rate both responses on a scale of ten based on the given question and its reference answer. The ratings are primarily based on the accuracy, relevance, and naturalness of the response to meet the requirements when humans are interacting with multi-modal agents (see Appendix for the detailed evaluation template). We employ the strategy proposed by Wang et al. [51] to mitigate potential evaluation biases regarding the response order. 3 Figure 4 shows that our Ying-VLM outperforms all baseline models in most samples. Notably, Ying-VLM beat the strongest baseline MiniGPT4 on 167 over 300 tested samples. Consistent with the previous ROUGE-L evaluation, this result indicates that the model fine-tuned on our instruction dataset can produce more accurate and engaging responses on the challenging KVQA tasks.",
            "score": 0.29019574321318514,
            "section_title": "GPT-4 Evaluation Results",
            "char_start_offset": 19701,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 260
                },
                {
                    "start": 263,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1429
                }
            ],
            "ref_mentions": [
                {
                    "start": 980,
                    "end": 984,
                    "matchedPaperCorpusId": "258960339"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11065673828125
        },
        {
            "corpus_id": "271213241",
            "title": "Putting GPT-4o to the Sword: A Comprehensive Evaluation of Language, Vision, Speech, and Multimodal Proficiency",
            "text": "As illustrated in Figure 1, GPT-4o belongs to the class of technologies known as large language models (LLMs).These models are notable for their ability to mimic human language usage so closely that it can be difficult for a human observer to distinguish between text generated by a human and that generated by a machine (Thirunavukarasu et al., 2023;Hayawi et al., 2024).This innovation marks a significant advancement towards passing the Turing test and underscores the practicality of AI in writing and research (Aher et al., 2023;Mannuru et al., 2023).However, it also introduces significant risks, including potential invasions of privacy and the generation of inaccurate, misleading, biased, or harmful information (Lund et al., 2023).Therefore, it is crucial to carefully evaluate these LLMs and scrutinize their outputs.Failure to do so could lead to the proliferation of misinformation and malicious content on the Internet (Hu et al., 2024).\n\nGiven the serious issues associated with some LLMs, it is essential to critically examine each new model for its limitations.Recent versions of GPT have shown significant improvements over their predecessors in various areas.For example, Koubaa (2023) found substantial improvements in GPT-4 compared to GPT-3.5 on tests such as the Graduate Record Examination (GRE), SAT, and Bar exam, with GPT-4's performance placing it in the top tenth percentile on most of these exams.Similarly, Coyne et al. (2023) reported improvements in grammatical error correction for GPT-4 compared to GPT-3.5.However, having more parameters in a model does not inherently guarantee better performance on all tasks.Overfitting can occur when a model is extensively trained on a large dataset but fails to generalize well to real-world data (Salman & Liu, 2019).\n\nEvaluation of the GPT-4o model is currently very limited.",
            "score": 0.29006517245529023,
            "section_title": "Figure 1. Visualization of the Relationship Between General AI and GPT 4o",
            "char_start_offset": 3573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 110,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 556
                },
                {
                    "start": 556,
                    "end": 741
                },
                {
                    "start": 741,
                    "end": 828
                },
                {
                    "start": 828,
                    "end": 951
                },
                {
                    "start": 953,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1178
                },
                {
                    "start": 1178,
                    "end": 1427
                },
                {
                    "start": 1427,
                    "end": 1542
                },
                {
                    "start": 1542,
                    "end": 1647
                },
                {
                    "start": 1647,
                    "end": 1793
                },
                {
                    "start": 1795,
                    "end": 1852
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 351,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 515,
                    "end": 534,
                    "matchedPaperCorpusId": "251719353"
                },
                {
                    "start": 721,
                    "end": 740,
                    "matchedPaperCorpusId": "257463753"
                },
                {
                    "start": 933,
                    "end": 950,
                    "matchedPaperCorpusId": "262084042"
                },
                {
                    "start": 1438,
                    "end": 1457,
                    "matchedPaperCorpusId": "258967185"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06427001953125
        },
        {
            "corpus_id": "269005156",
            "title": "Reason from Fallacy: Enhancing Large Language Models' Logical Reasoning through Logical Fallacy Understanding",
            "text": "At first, all authors of this work abide by the provided Code of Ethics.The quality of manual proofreading for logical fallacy sentences is ensured through a double-check strategy outlined in Appendix C. We ensure that the privacy rights of all members for proofreading are respected in the process.Besides, synthetic data generated by LLMs may involve potential ethical risks regarding fairness and bias (Bommasani et al., 2021;Blodgett et al., 2020), which results in further consideration when they are employed in downstream tasks.Although our dataset LFUD was built for better understanding logical fallacies, which is not intended for safety-critical applications, we still asked our members for proofreading to refine the offensive and harmful data generated by GPT-4.Despite these considerations, there may still be some unsatisfactory data that goes unnoticed in our final dataset.",
            "score": 0.28973022433395923,
            "section_title": "Ethical Considerations",
            "char_start_offset": 25959,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 72,
                    "end": 299
                },
                {
                    "start": 299,
                    "end": 535
                },
                {
                    "start": 535,
                    "end": 775
                },
                {
                    "start": 775,
                    "end": 890
                }
            ],
            "ref_mentions": [
                {
                    "start": 429,
                    "end": 451,
                    "matchedPaperCorpusId": "218971825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0560302734375
        },
        {
            "corpus_id": "266174252",
            "title": "Steering Llama 2 via Contrastive Activation Addition",
            "text": "While GPT-4 provides a scalable way to evaluate open-ended generation, it has some limitations. GPT-4's scores can be sensitive to scoring prompt wording, potentially introducing noise in the evaluations. In addition, LLMs have their own biases that could cause systematic differences from human evaluators. To mitigate these limitations, we manually inspect a sample of GPT-4's ratings to check for surprising results that are inconsistent with our own manual scores, and find that these correspond well, in line with other work such as Hackl et al. (2023) which finds GPT-4 to be a consistent and reliable rater. We also sample many open-ended generations with the steering intervention and see a consistent noticeable change in the direction being steered, with preservation of subjective text quality, as demonstrated in the samples seen in Appendix G.",
            "score": 0.28946347582354504,
            "section_title": "Limitations GPT-4 eval",
            "char_start_offset": 25157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 856
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25
        },
        {
            "corpus_id": "266374496",
            "title": "Human-Centred Learning Analytics and AI in Education: a Systematic Literature Review",
            "text": "Fernandez-Nieto et al., 2022;Crain et al., 2022), arguing that excessive human control could lead to over-interpretation of data. The other three studies reported that insights formulated based on inaccurate visual analytics might be prone to interpretive bias (Pozdniakov et al., 2022;Alfredo et al., 2023;Fernandez Nieto et al., 2022). We also gathered several discussions about human bias in the design process, as examined by four studies (4%). Two studies explicitly discussed the risk of bias in the HCD process (i.e., Vinella et al., 2022;Di Mitri et al., 2022). For instance, Vinella et al. (2022) noted that the monetary incentive given to participants when employing HCD techniques (e.g., crowdsourcing in this study) could stir and bias stakeholders from the study's intended purpose. Participants who are more focused on immediate rewards may not necessarily invest the necessary time and effort to provide insights that contribute to the long-term reliability of the system. However, two studies discussed the importance of balancing researchers' bias with stakeholders' needs in the design process (i.e., Vannaprathip et al., 2022;Fernandez Nieto et al., 2022). For instance, Fernandez Nieto et al. ( 2022) highlighted that 'end-users' (teachers/students) should be actively involved in design and evaluation processes to perceive their real needs in order to minimise researchers' bias. When researchers or designers work in isolation, they may unintentionally introduce their own assumptions and biases into the system. Diverse perspectives are considered when involving end-users, leading to a better alignment with challenges found in authentic educational settings that can be more reliable for end-users use. \n\nFurthermore, another topic that emerged is related to the strategies to produce reliable data (5%). Three studies discussed how they ensured the reliability of the data from their studies (i.e., Martinez et al., 2020;Lee et al., 2023;Ocumpaugh et al., 2017).",
            "score": 0.28882297853091005,
            "section_title": "Reliability",
            "char_start_offset": 60105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1728
                },
                {
                    "start": 1731,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1989
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 48,
                    "matchedPaperCorpusId": "254855040"
                },
                {
                    "start": 261,
                    "end": 286,
                    "matchedPaperCorpusId": "247222385"
                },
                {
                    "start": 286,
                    "end": 307,
                    "matchedPaperCorpusId": "257051657"
                },
                {
                    "start": 525,
                    "end": 546,
                    "matchedPaperCorpusId": "250180015"
                },
                {
                    "start": 546,
                    "end": 568,
                    "matchedPaperCorpusId": "244419918"
                },
                {
                    "start": 584,
                    "end": 605,
                    "matchedPaperCorpusId": "250180015"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.036834716796875
        },
        {
            "corpus_id": "265154694",
            "title": "Evidence against implicit belief processing in a blindfold task",
            "text": "If participants engaged in ToM and predicted the agent's action based on her belief about the object's location, they should be slower in detecting such incongruent behavior. We predicted that when the agent held a true belief about the object's location, participants' responses should be faster if she looked for it in the correct location. In contrast, when the agent held a false belief, they should be faster to detect her action if she looked in the incorrect, but believed, location. If participants experienced an egocentric bias during this task, their response times should also be affected by their own belief about the object's location. That is, participants responses should be faster when the agent looked for the object in its actual location, regardless of the agent's belief. \n\nWe carried out two preregistered experiments using the same stimuli but different experimental setups. Experiment 1 was conducted online, experiment 2 in the laboratory.",
            "score": 0.28851895824226104,
            "section_title": "Introduction",
            "char_start_offset": 8140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 965
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.070068359375
        },
        {
            "corpus_id": "272986722",
            "title": "DCAST: Diverse Class-Aware Self-Training Mitigates Selection Bias for Fairer Learning",
            "text": "We performed experiments across 11 ML benchmark datasets with different characteristics to assess the effectiveness of (i) selection bias induction using the proposed hierarchy bias technique, and (ii) selection bias mitigation using the proposed (D)CAST strategies. Hierarchy bias was compared to other bias induction techniques concerning both the distribution shift produced by the data selection procedure and its effect on the performance of prediction models built using supervised learning. The (D)CAST semi-supervised bias mitigation strategies were evaluated against conventional semi-supervised self-training (ST), as well as a range of alternative domain adaptation methods, on their ability to build prediction models from biased data with better generalization than using supervised learning. \n\nData splits and bias induction. For each dataset, 20% of the samples were uniformly selected at random, stratified by class, and reserved as test data to evaluate prediction models (Fig. 6). The adult dataset already had its own separate test set, which we reserved. Additionally, we created 30 distinct train runs per dataset, each by randomly splitting the remaining 80% of the samples into two train sets, stratified by class: a labeled train set, containing 30% of the samples, from which we also generated biased labeled sets by applying different bias induction techniques; and an unlabeled train set, comprising the remaining 70% of the samples. The original and biased labeled train sets were later used to build prediction models with supervised learning or bias mitigation strategies, while the unlabeled train set was used to learn prediction models with the semi-supervised bias mitigation strategies (D)CAST and conventional ST (other bias mitigation methods used test data without labels). When necessary for model training, a validation set was further extracted from each biased train set, given that unbiased labeled data would not be available for this purpose in a realistic setting.",
            "score": 0.2884799708898627,
            "section_title": "Evaluation of bias induction and bias mitigation methods",
            "char_start_offset": 40676,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 2010
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.089111328125
        },
        {
            "corpus_id": "267412892",
            "title": "Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding",
            "text": "The biases we discussed earlier (like propositional iconicity or patterns reflecting UG) might contribute to GPT-4's effectiveness in these exchanges. These biases help GPT-4 generate linguistically coherent and contextually appropriate responses. However, they do not endow GPT-4 with true understanding or grounding. \n\nThese biases might explain GPT-4's surprising performance in producing meaningful-sounding language and its apparent capacity to 'understand' or respond relevantly to human input. The effectiveness of GPT-4 in such exchanges is largely due to its ability to replicate and work within the patterns and constraints inherent in its training data, shaped by human language use. \n\nIn conclusion, the exchange between a human and GPT-4 in language interaction is characterized by a significant asymmetry in grounding and understanding. While GPT-4 can simulate meaningful language output and seemingly appropriate responses, these are the result of advanced pattern recognition and generation capabilities, not genuine understanding or shared experiential grounding. The human participant in the exchange brings their own experiential grounding to the interpretation of GPT-4's outputs, often projecting meaning and intention where there is none. This asymmetry has important implications: GPT-4's Limitations in True Understanding: Despite its sophisticated language processing abilities, GPT-4 does not truly understand the content it generates or processes. Its responses are based on statistical correlations and learned patterns from its training data, lacking the experiential grounding that humans rely on for genuine understanding.",
            "score": 0.28823002711525125,
            "section_title": "Implications of Potential Biases:",
            "char_start_offset": 110442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 318
                },
                {
                    "start": 321,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1654
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1866455078125
        },
        {
            "corpus_id": "259671923",
            "title": "How robust are egocentric and altercentric interference effects in social cognition? a test with explicit and implicit versions of a continuous false belief task",
            "text": "As suggested by one of the anonymous reviewers, in order to increase power, we repeated the within-subject comparisons by collapsing the two groups since they did not differ. The analysis revealed a difference between experimental (Mdn = \u221241.0) and control (Mdn = \u221247.67) trials for egocentric bias (Z = -2.559, p = 0.01) only when the wrong answers were excluded from the analysis; but not for the whole sample (experimental: Mdn = \u221222.38, control: Mdn = \u221227.5; Z = -1.245, p = 0.21). No difference was observed in altercentric bias condition, regardless of the fact that the wrong answers were included (experimental: Mdn = \u22125.08, control: Mdn = \u22122.88; Z = -0.064, p = 0.95) or excluded (experimental: Mdn = 19.63, control: Mdn = 7.88; Z = -0.601, p = 0.55).",
            "score": 0.2881583152810824,
            "section_title": "Within-subject comparisons with collapsed datasets",
            "char_start_offset": 27427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 760
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0379638671875
        },
        {
            "corpus_id": "252718607",
            "title": "A new psychometric task measuring spatial perspective taking in ambulatory virtual reality",
            "text": "In SOT, previous research has shown that the egocentric strategy is the dominant strategy used by participants (Kozhevnikov and Hegarty, 2001;Hegarty and Waller, 2004;Gunalp et al., 2021). This means that although the object array was displayed from an overhead view, participants tend to imagine themselves being in the object array and then estimate the direction from a first-person-view. \n\n1) Perspective-taking hypothesis: If the common processes, including perspective shift and target direction estimation, are the dominant sources of individual differences in the iVTT, then a) An individual's absolute angular error in the iVTT should be correlated with their absolute angular error in the SOT; b) Angular errors on the iVTT will be similar to those on the SOT; c) An individual's absolute distance error in the iVTT should not be strongly correlated with angular error in the iVTT or in the SOT, because distance is not part of the perspective taking process. 2) Spatial-updating hypothesis: If the common processes are not the dominant sources of individual differences, and spatial updating processes unique to the iVTT are also a source of individual differences, then a) An individual's absolute angular error in the iVTT should not be highly correlated with their absolute angular error in the SOT; b) Angular errors on the iVTT will be greater than those on the SOT; c) An individual's absolute distance error in the iVTT should be correlated with their absolute angular error in iVTT. \n\nSecond, we analyzed the performance at the trial level (i.e., participants' average performance on different trials with different trial attributes) to test specific strategies that may account for the performance variance in the two tasks. The iVTT and SOT both require participants to shift their perspective (Perspective Shift) and then compute the relative direction to the target object (Pointing or Travel Direction). In previous research on the SOT, participants' angular error increased linearly as the magnitude of Perspective Shift increased from 0 to 180 \u00b0 (Kozhevnikov and Hegarty, 2001;Gunalp et al., 2021). This finding has been interpreted to indicate that the process of perspective shifting (Step 1) is an analog process (e.g.",
            "score": 0.2881583152810824,
            "section_title": "Introduction",
            "char_start_offset": 8023,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1501
                },
                {
                    "start": 1504,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 142,
                    "matchedPaperCorpusId": "14614215"
                },
                {
                    "start": 142,
                    "end": 167,
                    "matchedPaperCorpusId": "17528372"
                },
                {
                    "start": 167,
                    "end": 187,
                    "matchedPaperCorpusId": "232366861"
                },
                {
                    "start": 2072,
                    "end": 2103,
                    "matchedPaperCorpusId": "14614215"
                },
                {
                    "start": 2103,
                    "end": 2123,
                    "matchedPaperCorpusId": "232366861"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01287078857421875
        },
        {
            "corpus_id": "278311046",
            "title": "Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark",
            "text": "Biases in GPT-4o \n\nWe use the ESS dataset round 11 to validate the demographic biases in GPT-4o. For each demographic group, we calculate the average scores across all value dimensions. To determine relative differences, we subtract the average scores of all individuals from each demographic group's scores. These relative difference scores are compared with the effects of persona prompting. Similarly, for the GPT-4o model, we calculate the difference between value dimension scores with and without persona prompting.",
            "score": 0.2881583152810824,
            "section_title": "G Details in Measuring Demographic",
            "char_start_offset": 40190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 19,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 521
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049591064453125
        },
        {
            "corpus_id": "259376861",
            "title": "Automated evaluation of written discourse coherence using GPT-4",
            "text": "We believe that the method described in the paper should be generalizable to similar datasets that are publicly available. However, caution in the use of GPT-4 ratings is warranted due to limited reproducibility, the possibility of bias, and limited insight into the underlying processes that determine the ratings.",
            "score": 0.2881583152810824,
            "section_title": "Introduction",
            "char_start_offset": 1843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 315
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08990478515625
        },
        {
            "corpus_id": "260984952",
            "title": "Constructing the Future of Social Work Tech Habits of Mind With the Ethical OS",
            "text": "Risk Zone Four requires tech designers and users to examine the human bias in algorithms and deep data sets. It acknowledges that persons bring their biases with them when they construct technology. This zone also suggests accountability for algorithms that misidentify or unfairly assess someone.",
            "score": 0.2881583152810824,
            "section_title": "Zone 4: Machine Ethics and Algorithmic Biases",
            "char_start_offset": 20472,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 297
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.034423828125
        },
        {
            "corpus_id": "244728951",
            "title": "Comparable performance on a spatial memory task in data collected in the lab and online",
            "text": "In a previous study [28], a different analysis of the lab-based data revealed a systematic bias, in that participants were more likely to make errors in the direction that is congruent with the camera movement between encoding and test, i.e. when the camera moved left participants were more likely to make errors to the left. To investigate if this bias is replicated in the online data, we estimated the magnitude of participants' errors, similar to absolute error calculations. Additionally, we estimated the direction of the error, by coding errors to the left with a negative sign (i.e. -28cm) and errors to the right with a positive sign (i.e. 28cm). Next, we multiplied (folded) all of the errors where the camera movement was to the left by -1. Following this",
            "score": 0.2881583152810824,
            "section_title": "Signed error",
            "char_start_offset": 13698,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 767
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 24,
                    "matchedPaperCorpusId": "221403379"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.005664825439453125
        },
        {
            "corpus_id": "259671923",
            "title": "How robust are egocentric and altercentric interference effects in social cognition? a test with explicit and implicit versions of a continuous false belief task",
            "text": "The mean object location deviations in experimental and control trials are shown in Figure 7. We first compared these averages for each bias separately. For egocentric bias as measured by the Sandbox task, experimental (Mdn = 2.88) and control (Mdn = 3.27) trials did not differ from each other, Z = -0.030, p = 0.976; and excluding the wrong answers did not change this result, Z = -0.598, p = 0.550 (experimental trials: Mdn = 1.5; control trials: Mdn = 3.27). Mouse-tracking measures provided results along the same lines: no difference was found between experimental (Mdn = 1.47) and control (Mdn = 0.63) trials, Z = -1.536, p = 0.125. The altercentric bias version did not reveal any difference either. When all answers were considered, experimental (Mdn = 3.31) and control (Mdn = 7.9) trials did not differ from each other, Z = -0.697, p = 0.486. When the analysis was repeated with correct answers only, there was still no difference between experimental (Mdn = 2.71) and control (Mdn = 7.06) trials, Z = -1.54, p = 0.123. Mouse-tracking measures did not reveal any difference between trial types, Z = -1.102, p = 0.270 (experimental trials: Mdn = 2.56; control trials: Mdn = 3.19). We also compared the experimental and control trials separately in the altercentric-first and egocentric-first conditions. None of these comparisons revealed a difference (all ps > = 0.06). \n\nWhen investigating possible carry-over costs between biases, no difference was found as a function of the presentation order. For pure egocentric bias measured by the Sandbox task, a Mann-Whitney test revealed no difference between egocentric-first vs. altercentric-first conditions, U = 320.00, Z = \u22120.552, p = 0.581. Mouse-tracking measures revealed similar null results, U = 278.00, Z = \u22121.299,",
            "score": 0.2881583152810824,
            "section_title": "Results and discussion",
            "char_start_offset": 46000,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1780
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0330810546875
        },
        {
            "corpus_id": "222237730",
            "title": "SelfPose: 3D Egocentric Pose Estimation From a Headset Mounted Camera",
            "text": "Comparison on Human3.6M dataset: We show that our proposed approach is not specific for the egocentric case, but also provides excellent results in the more standard case of front-facing cameras. For this evaluation, we chose the Human3.6M dataset [7], [73]. We used two evaluation protocols. Protocol 1 has five subjects (S1, S5, S6, S7, S8) used in training, with subjects (S9, S11) used for evaluation. The MPJPE error is computed on every 64th frame. Protocol 2 contains six subjects (S1, S5, S6, S7, S8, S9) used for training, and the evaluation is performed on every 64th frame of Subject 11 (Procrustes aligned MPJPE is used for evaluation). The results are shown in Table 9 from where it can be seen that our approach is on par with state-of-the-art methods, scoring second overall within the non-temporal methods.",
            "score": 0.2881583152810824,
            "section_title": "Evaluation on Front-facing Cameras",
            "char_start_offset": 29727,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 822
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 251,
                    "matchedPaperCorpusId": "4244548"
                },
                {
                    "start": 253,
                    "end": 257,
                    "matchedPaperCorpusId": "8421376"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0162811279296875
        },
        {
            "corpus_id": "274280574",
            "title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
            "text": "The use of LLMs-as-a-judge inherently frames evaluation as a generation task, introducing significant challenges related to bias and vulnerability. These biases often stem from the models' training data, which frequently embeds societal stereotypes tied to demographic identities such as race, gender, religion, culture, and ideology (Sheng et al., 2021). Such biases can significantly compromise fairness and reliability when LLMs are deployed for diverse judging tasks. \n\nIn addition to these general biases, specific evaluative biases emerge when LLMs act as judges. Order Bias is a prominent issue where the sequence of candidates influences preferences (Zheng et al., 2023;Wang et al., 2023c;Koo et al., 2023a;LLMS). This bias can distort evaluation outcomes, particularly in pairwise comparisons, and is more pronounced when the quality gap between competing responses is small (LLMS; Wang et al., 2023c).Egocentric Bias arises when LLMs favor outputs generated by the same model, compromising objectivity (Liu et al., 2023c;Koo et al., 2023a;Wataoka et al., 2024). This issue is particularly pronounced when evaluation metrics are designed using the same model, leading to inflated scores for outputs from the originating model (Liu et al., 2023c). Length Bias, another prevalent challenge, skews evaluations by disproportionately favoring longer or shorter responses regardless of quality (Zheng et al., 2023;Koo et al., 2023a). Additional biases, such as Misinformation Oversight Bias, Authority Bias, and Beauty Bias, further complicate LLM evaluations. For instance, misinformation oversight bias reflects a tendency to overlook factual errors, authority bias favors statements from perceived authoritative sources, and beauty bias prioritizes visually appealing content over substantive quality (Chen et al., 2024b,d;Stephan et al., 2024). Similarly, Verbosity Bias shows a preference for lengthier explanations, often equating verbosity with quality, which can mislead the judgment process (Yuan et al., 2024a).",
            "score": 0.28805861607447086,
            "section_title": "Bias & Vulnerability",
            "char_start_offset": 60312,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 334,
                    "end": 354,
                    "matchedPaperCorpusId": "234337004"
                },
                {
                    "start": 658,
                    "end": 678,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 697,
                    "end": 715,
                    "matchedPaperCorpusId": "263310448"
                },
                {
                    "start": 1031,
                    "end": 1049,
                    "matchedPaperCorpusId": "263310448"
                },
                {
                    "start": 1049,
                    "end": 1070,
                    "matchedPaperCorpusId": "273661820"
                },
                {
                    "start": 1397,
                    "end": 1417,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 1417,
                    "end": 1435,
                    "matchedPaperCorpusId": "263310448"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17724609375
        },
        {
            "corpus_id": "271769620",
            "title": "SINAI at BioLaySumm: Self-Play Fine-Tuning of Large Language Models for Biomedical Lay Summarisation",
            "text": "The limitations of the presented approaches stem from the inherent characteristics and potential biases of the pre-trained models they are based on. Specifically, models like Llama-3-8B-Instruct, GPT-3.5, and GPT-4 were pre-trained on extensive text datasets, which were not thoroughly evaluated for existing biases. Consequently, these models may generate inappropriate content or replicate biases present in the underlying data. Therefore, it is crucial to conduct comprehensive evaluations of safety and fairness concerns before deploying these systems in any practical applications.",
            "score": 0.28800192192022694,
            "section_title": "Limitations",
            "char_start_offset": 12923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 586
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.052337646484375
        },
        {
            "corpus_id": "267679187",
            "title": "Can ChatGPT assist authors with abstract writing in medical journals? Evaluating the quality of scientific abstracts generated by ChatGPT and original abstracts",
            "text": "The underlying reason for these discrepancies remains unclear as OpenAI is yet to disclose explicit details to the parameters, architecture nor the hardware used for training GPT 4. However, our finding was reinforced in a study from Stanford University which evaluated both models of ChatGPT across diverse domains, encompassing code generation, visual reasoning, handling of sensitive questions and mathematical problem-solving. In particular, when prompted to answer maths problems, GPT 3.5 demonstrated 86.8% accuracy in comparison to 2.4% shown by GPT 4, and this was attributed to GPT 3.5's superiority in breaking down complex problems into smaller intermediate steps, a phenomenon known as the \"chain of thoughts\" effects [17]. Parallel to our study, it is plausible that GPT 3.5's aptitude for deconstructing complex commands resulted in superior abstract generation compared to its successor. In addition to this, the assumed broader training database and variance in fine-tuning approaches in GPT 4 could have further compromised its capacity for specialised tasks such as those required in this study. \n\nDespite the strengths of ChatGPT in scientific writing, it is imperative to recognise the associated risks and pitfalls. Firstly, LLMs draw from expansive dataset, which could unintentionally reflect biases related to sex, ethnicity, and language. Given that the training data for these LLMs predominantly originate from well-funded institutions in affluent, English-speaking countries, there exists a risk for the underrepresentation of minority groups [18]. Secondly, while the proficiency of LLMs to generate credible information is commending, it can sometimes be misleading. For instance, when prompted to generate a literature review, ChatGPT provided superficial summary that was far from the knowledge of an expert in the field and it faltered in providing correct references, meanwhile in our study, GPT 4 generated 4 abstracts which were entirely unrelated to the given topic [19,20]. This propensity to generate plausible yet fictitious content intertwines with issues of plagiarism. LLMs could inadvertently extract and apply content from its training data into their generated output which poses a real challenge to trust in medical research, especially as the difference between author written and LLM-generated texts gradually narrows.",
            "score": 0.28791714058674495,
            "section_title": "Discussion",
            "char_start_offset": 23118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1113
                },
                {
                    "start": 1116,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 2010
                },
                {
                    "start": 2011,
                    "end": 2110
                },
                {
                    "start": 2111,
                    "end": 2366
                }
            ],
            "ref_mentions": [
                {
                    "start": 2002,
                    "end": 2006,
                    "matchedPaperCorpusId": "258494319"
                },
                {
                    "start": 2006,
                    "end": 2009,
                    "matchedPaperCorpusId": "259241018"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10968017578125
        },
        {
            "corpus_id": "276885275",
            "title": "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding",
            "text": "Different types of references-wrong, humangenerated, or self-generated-each lead to different profiles of judge behavior. One common judgment bias is self-preference, where a model tends to overrate its own responses. To quantify this bias we computed the false positive rate (FPR) for each judge when evaluating its own responses versus all other models. In this context, the FPR represents the rate at which the model erroneously labels incorrect responses as correct. We also evaluated the false negative rate (FNR), which is the rate at which the model erroneously labels correct responses as incorrect. A model with a strong selfpreference bias would exhibit a high FPR and a low FNR when grading its own references. Figure 4 displays the FPR and FNR aggregated over each judge with the \"Wrong\", \"Random\", \"Self\", \"None\", and \"Human\" reference types (Metrics per judge can be found in Figure 5). For every reference type, we see that on average models have a higher FPR when grading their own responses. The gap is particularly large when the model is provided with its own generated reference or no reference at all. Thus, providing a human reference reduces both the overall rate of error and the relative difference between a model's judgment of its own responses and those of others. \n\nWhen the model is given a \"Wrong\" reference, the FPR is low and the FNR is very high, which suggests that the incorrect information leads judges to erroneously classify correct responses as incorrect. However, this effect is not observed with the \"Random\" reference. This evidence supports the idea that judges are sensitive to the correctness of the reference, and providing a slightly incorrect reference is worse than providing an unrelated reference or no reference at all. \n\nEven when provided with a \"Wrong\" reference, Table 5: Cohen's Kappa comparison between different references, evaluated over the subset of total questions that GPT-4o gets correct. There is no longer a statistically significant difference in performance in between the human-written gold references and the verified GPT-4o (\u2713) references, suggesting that verifying the correctness of the LLM generated references can be enough. \n\nthe LLM Judges have a non-zero FPR. This means that they are still grading responses as correct.",
            "score": 0.2878676045716214,
            "section_title": "Error Analysis",
            "char_start_offset": 22359,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1292
                },
                {
                    "start": 1295,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1772
                },
                {
                    "start": 1775,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2201
                },
                {
                    "start": 2204,
                    "end": 2239
                },
                {
                    "start": 2240,
                    "end": 2300
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.599609375
        },
        {
            "corpus_id": "267523079",
            "title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
            "text": "Recent research focuses on hallucination evaluation (Liu et al., 2023a), detection (Li et al., 2023e;Wang et al., 2023a), and mitigation (Yin et al., 2023;Gunjal et al., 2023;Zhou et al., 2023), noting that even GPT-4V suffer from these issues (Shi et al., 2023;Liu et al., 2023a;Cui et al., 2023). Besides, biases in MLLM-as-a-Judge, similar to those in human decision-making (Blunch, 1984;Raghubir & Valen-zuela, 2006) and other ML domains (Wang et al., 2018;Liu et al., 2023e), such as position (Zheng et al., 2023a), egocentric (Li et al., 2024), and verbosity biases (Saito et al., 2023), are compounded by the integration of visual perception, necessitating further investigation.",
            "score": 0.2878254283851144,
            "section_title": "Related Work",
            "char_start_offset": 21251,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 686
                }
            ],
            "ref_mentions": [
                {
                    "start": 377,
                    "end": 391,
                    "matchedPaperCorpusId": "147259862"
                },
                {
                    "start": 442,
                    "end": 461,
                    "matchedPaperCorpusId": "21054674"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.160888671875
        },
        {
            "corpus_id": "265445838",
            "title": "Cultural bias and cultural alignment of large language models",
            "text": "incorporated Reinforcement Learning with Human Feedback (RLHF) [49]. The cultural bias inherent in human feedback may have contributed to the substantial shift towards more secular values expressed by GPT-3.5-turbo. A Rule-Based Reward Model was introduced into the training process of GPT-4, which provides additional reward signals that may have mitigated cultural biases from the RLHF process [33]. The training process of models after GPT-4 has not been published at this time. We can only speculate that additional sources of human feedback and rule-based rewards account for the observed variation in tradition-secular cultural values. \n\nTo evaluate the effectiveness of the proposed control strategy to improve cultural alignment, cultural prompting, we examine how it changes the Euclidean distance on the map between each country's IVS-based values and its GPT-based values for each model. Figure 2 shows the distributions of cultural distances across countries for each model with and without cultural prompting. As expected based on the relative proximity of the GPT models in Figure 1, we find that the distribution of cultural bias without cultural prompting is similar across the five models (for GPT-4o/4/4-turbo, the difference is barely statistically significant; Kruskal-Wallis rank sum test: P = 0.036). Cultural prompting is effective at aligning GPT's expressed values more closely with the ground truth from the IVS data, especially for models released after GPT-3.5-turbo: it reduces the average cultural distance from 2.42 to 1.57 (Wilcoxon signed-rank test: P < 0.001) for GPT-4o, from 2.71 to 1.77 (P < 0.001) for GPT-4-turbo, and from 2.69 to 1.65 (P < 0.001) for GPT-4. Cultural prompting is less effective for GPT-3/3.5-turbo, consistent with prior evidence [9], though the improvement is still statistically significant from 2.39 to 2.11 (P < 0.001) for GPT-3 and from 3.35 to 2.83 (P < 0.001) for GPT-3.5-turbo.",
            "score": 0.287823853296773,
            "section_title": "Results",
            "char_start_offset": 11196,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 63,
                    "end": 67,
                    "matchedPaperCorpusId": "258447166"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07464599609375
        },
        {
            "corpus_id": "268194969",
            "title": "ChatGPT and generative AI are revolutionizing the scientific community: A Janus\u2010faced conundrum",
            "text": "While generative AI offers considerable advantages to the scientific community, its usage also presents potential drawbacks, encompassing not only the quality and reliability of research but also ethical and responsibility concerns.Diligent usage by scientists and editors is essential to mitigate these issues.\n\nA primary concern is the accuracy and reliability of AI-generated data.A survey revealed that 66% of scientists are wary of potential errors or inaccuracies infiltrating their research papers [7].AI models, trained on existing data sets, might propagate outdated, biased, or incomplete information, impacting the output's relevance and accuracy.For the sake of maintaining commercial advantages, the architecture, training data, and scale of most generative AI models are not publicly disclosed.When using data analysis plugins, inputting data can yield analysis results.However, researchers lacking proper data analysis methodologies may struggle to discern the correctness of these methods and determine if errors occurred during the analysis process.It is imperative for scientists to rigorously evaluate AI-generated content against authoritative sources, a process vital for maintaining accuracy in complex scientific contexts.Generative AI should function as an auxiliary tool augmenting research, rather than replacing human expertise.While beneficial for preliminary data processing, idea generation, or draft creation, the ultimate responsibility for the accuracy and integrity of the final output rests with human experts.\n\nData bias in ChatGPT and other generative AI models poses a significant challenge, especially in scientific contexts.These AI systems develop their text generation and interpretation capabilities based on their training datasets.Inherently biased data, characterized by stereotypes, prejudices, or skewed representations, can lead to AI perpetuating these biases in its outputs.For example, if training data consistently associates certain concepts or groups, the model may reinforce these biases, thereby deepening existing prejudices.It is crucial for scientists to critically assess generative AI outputs, particularly when they impact research findings or data interpretation.This necessitates vigilance against potential biases and cross-verification with impartial data sources.Furthermore, educating the scientific community about the propensity for bias in AI models is imperative.Heightened awareness promotes more prudent and informed utilization of AI in research.\n\nThe challenge of maintaining academic originality when employing generative AI in scientific research and publication is paramount.",
            "score": 0.2877035632388859,
            "section_title": "MISUSING GENERATIVE AI: RISKS TO SCIENTIFIC INTEGRITY",
            "char_start_offset": 9582,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 232,
                    "end": 311
                },
                {
                    "start": 313,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 509
                },
                {
                    "start": 509,
                    "end": 658
                },
                {
                    "start": 658,
                    "end": 808
                },
                {
                    "start": 808,
                    "end": 884
                },
                {
                    "start": 884,
                    "end": 1066
                },
                {
                    "start": 1066,
                    "end": 1245
                },
                {
                    "start": 1245,
                    "end": 1355
                },
                {
                    "start": 1355,
                    "end": 1545
                },
                {
                    "start": 1547,
                    "end": 1664
                },
                {
                    "start": 1664,
                    "end": 1776
                },
                {
                    "start": 1776,
                    "end": 1925
                },
                {
                    "start": 1925,
                    "end": 2083
                },
                {
                    "start": 2083,
                    "end": 2227
                },
                {
                    "start": 2227,
                    "end": 2331
                },
                {
                    "start": 2331,
                    "end": 2436
                },
                {
                    "start": 2436,
                    "end": 2522
                },
                {
                    "start": 2524,
                    "end": 2655
                }
            ],
            "ref_mentions": [
                {
                    "start": 505,
                    "end": 508,
                    "matchedPaperCorpusId": "263124615"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.258056640625
        },
        {
            "corpus_id": "268876074",
            "title": "Towards detecting unanticipated bias in Large Language Models",
            "text": "It is easy to see how this implicit bias in GPT-4 could lead to unfair decisions, e.g. in recruiting applications.\n\nmitigating components, particularly if used for purposes other than dialog systems.The complexity of neural networks, often seen as 'black boxes', exacerbates these problems.Addressing biases solely through training is inadequate, as it restricts the range of training data and does not account for the difficulty of defining 'unbiased' data.\n\nResearch in this area is mainly focused on quantifying and mitigating known biases.This paper addresses the challenge of detecting unanticipated biases in LLMs, particularly during the inference stage.We propose employing Uncertainty Quantification (UQ) and Explainable AI (XAI) as tools to unearth biases potentially influencing LLM outputs.\n\nThese techniques offer insights into whether LLMs are appropriately focusing on a task or if biases are swaying their outputs.Given the extensive computational and data requirements of LLMs, many users depend on pre-trained models with possible minor modifications.Therefore, our emphasis is on post-hoc, model-agnostic methods applicable to any LLM.\n\nThe paper is structured to provide an overview of current bias detection methods in LLMs, introduce technical aspects of UQ and XAI, and suggest applications of these techniques in bias detection.We also discuss challenges in evaluation, propose potential bias mitigation strategies, and conclude with the limitations and a summary of our findings.This exploration into Unanticipated Bias Detection (UBD) in LLMs is novel and holds significant promise in addressing a critical aspect of AI fairness and transparency.With this paper, we aim to bring this topic into greater focus and encourage further research in this area.",
            "score": 0.2869890678183292,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 116,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 290
                },
                {
                    "start": 290,
                    "end": 458
                },
                {
                    "start": 460,
                    "end": 543
                },
                {
                    "start": 543,
                    "end": 661
                },
                {
                    "start": 661,
                    "end": 802
                },
                {
                    "start": 804,
                    "end": 930
                },
                {
                    "start": 930,
                    "end": 1069
                },
                {
                    "start": 1069,
                    "end": 1154
                },
                {
                    "start": 1156,
                    "end": 1352
                },
                {
                    "start": 1352,
                    "end": 1504
                },
                {
                    "start": 1504,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1779
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280029296875
        },
        {
            "corpus_id": "271270236",
            "title": "AI AI Bias: Large Language Models Favor Their Own Generated Content",
            "text": "To complement our studies on LLM bias, we conducted an initial experiment to gauge human preferences in similar decision contexts (See Table 5 and Tabel 6 for results).It is important to note that these preferences were collected by research assistants rather than actual users.This study serves as a preliminary investigation with a small sample size and best-effort human baseline, and the findings are not definitive.We used the product details and scientific paper abstracts generated by both GPT-3.5 and GPT-4, and presented them to a group of human evaluators.These participants were asked to choose which descriptions they preferred without knowing whether they were written by a human or an LLM.They also had the option to state that they had no preference between the two presented texts.\n\nParticipants were presented with pairs of descriptions: one generated by an LLM and one written by a human.Each participant evaluated a randomized set of pairs to mitigate any order or preference bias.The results in Table 5 and Table 6 indicate that human evaluators generally preferred human-generated product descriptions over those generated by LLMs, with GPT-3.5 having a preference ratio of 0.29 and GPT-4 at 0.28.For scientific paper abstracts, the preferences leaned more towards LLM-generated texts, with GPT-4 abstracts being preferred at a ratio of 0.60 compared to GPT-3.5 at 0.46.",
            "score": 0.2866516403272328,
            "section_title": "Preferences of Humans",
            "char_start_offset": 14486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 278
                },
                {
                    "start": 278,
                    "end": 420
                },
                {
                    "start": 420,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 703
                },
                {
                    "start": 703,
                    "end": 797
                },
                {
                    "start": 799,
                    "end": 906
                },
                {
                    "start": 906,
                    "end": 1000
                },
                {
                    "start": 1000,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1391
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2132568359375
        },
        {
            "corpus_id": "267637243",
            "title": "Addressing cognitive bias in medical language models",
            "text": "However, other models like mixtral-8x7b and gpt-3.5 displayed only marginal improvements. \n\nOne-shot demonstration: When exposed to a negative example of bias, gpt-4 showed a remarkable ability to adjust its responses, particularly in the \"Recency\" bias category, with accuracy improving from 0.679 to 0.742. Other models also benefited from this strategy, but the degree of improvement was less pronounced compared to gpt-4, indicating a potential need for more nuanced or multiple examples for effective learning in these models. \n\nFew-shot demonstration: gpt-4 again exhibited the most significant improvements with this approach, especially in \"Status quo\" and \"Recency\" biases. The inclusion of both negative and positive examples provided a more comprehensive context for learning, resulting in higher accuracy improvements. The other models showed some degree of improvement with this method, but not as extensively as gpt-4. \n\nWe note that PaLM-2 refused to provide responses to a high proportion of one-and few-shot queries (non-response rates of 94.4% and 99.5%, respectively) due to safety filters triggered by our requests for medical advice, so we do not report performance metrics for these mitigation strategies (see Appendix C). We also note a significant increase in nonresponse and nonsensical answers for llama-2-70B and pmc-llama-13b following one-and few-shot mitigation. This behavior is likely due to the limited context length of these models compared with the higher performing models, such as gpt-4 and mixtral-8x7b.",
            "score": 0.2865565658190311,
            "section_title": "Bias mitigation strategies",
            "char_start_offset": 19071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 92,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 531
                },
                {
                    "start": 534,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 932
                },
                {
                    "start": 935,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1542
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1976318359375
        },
        {
            "corpus_id": "259360619",
            "title": "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations",
            "text": "They are tested on both standard datasets such as MMLU, and more importantly, on open-ended questions which are much more prevalent in real life (Nakano et al., 2021;Chiang et al., 2023). People mostly use GPT-4 (Liu et al., 2023;OpenAI, 2023) as an evaluator for either generating scores or pairwise comparisons (Wang et al., 2023b;Zhou et al., 2023). However, such a strategy has fundamental problems because of various biases, such as (1) positional bias (Dettmers et al., 2023;Wang et al., 2023a), where a model favors the first answer in pairwise comparisons; (2) verbosity and length bias (Wang et al., 2023b); (3) and most importantly, self-enhancement bias, where an LLM favors its own answers (Liu et al., 2023;Zheng et al., 2023). \n\nEfforts have been proposed to tackle them: (1) Using position switching (Wang et al., 2023a) for mitigating positional bias; (2) Zheng et al. (2023) proposes Chatbot Arena, where real users ask questions and provide Figure 1: The peer rank process (PR): each LLM model acts both as reviewers (A, B, C) and contestants (1,2,3). From the battles between contestants (pairwise comparisons), it induces a self-ranking. In this example, models A, B, and C represent GPT-4, Bard, and Claude, respectively. pairwise judgments of answers generated by two LLMs. But this is time-consuming and requires expert annotation to ensure fairness; (3) Bai et al. (2023) propose using each LLM as an examiner, where each generates questions to test other models. Different from PD, their \"exams\" are biased with randomly generated questions. Moreover, none of the above works support inducing self-rankings through peer ranking. \n\nOverall, our work, peer evaluation-oriented methods, undergo tests on two benchmarks, each covering a variety of tasks, and focuses more on the alignment between LLMs' evaluations and human judgments.",
            "score": 0.2864681456656777,
            "section_title": "Related Work",
            "char_start_offset": 5682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1856
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.413818359375
        },
        {
            "corpus_id": "265154694",
            "title": "Evidence against implicit belief processing in a blindfold task",
            "text": "Similarly, participants were less accurate in determining the agent's belief about an object's location when they themselves knew that the object was in a different location [25]. In the present task, the egocentric bias increased across trials reflecting the fact that, over time, the participant's own perspective became dominant over the agent's belief. This was likely due to the fact that the agent's belief was not a reliable indicator for her action, which participants might have realized in the course of the trials. In contrast, they did not ignore their own belief about the object's location, although it was not beneficial to the task either (namely, the agent also acted incongruent to the participants' belief in half of the trials). Thus, in case of unreliable information, individuals seem to favor their own perspective over that of someone else. \n\nThe fact that we were able to replicate two well-established cognitive phenomena, the reality congruency effect and the egocentric bias, suggests that our data are meaningful and strengthens our confidence in the absence of an altercentric bias in the current task. But what could be the reason for the lack of altercentric interference in the current experimental setting? Altercentric biases have been proposed as a measure of implicit belief processing. There are several other implicit ToM tasks that, for instance, use looking behavior as an indicator for action anticipation. Recently, replication difficulties have been reported for some of these tasks [e.g., 32,37,38]. Thus, in the context of these failed replications, the absence of an altercentric bias in the current task could be seen as an indicator that altercentric biases, just like implicit belief processing in general, are rather fragile phenomena that are difficult to tap. However, there were also important differences between our own and previous altercentric bias tasks [31,32]. Unlike the original paradigm [2,3,16], we manipulated the location of the object rather than its presence. This may require a richer belief attribution, as a particular location needs to be ascribed to the agent's belief instead of the mere presence or absence of the object. Also, our participants completed a two-alternative forced-choice task, which may have been more challenging than a simple go/no-go task.",
            "score": 0.28597133645119716,
            "section_title": "Results",
            "char_start_offset": 46971,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2197
                },
                {
                    "start": 2198,
                    "end": 2334
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 178,
                    "matchedPaperCorpusId": "8563845"
                },
                {
                    "start": 1534,
                    "end": 1537,
                    "matchedPaperCorpusId": "195837279"
                },
                {
                    "start": 1537,
                    "end": 1540,
                    "matchedPaperCorpusId": "11426290"
                },
                {
                    "start": 1540,
                    "end": 1543,
                    "matchedPaperCorpusId": "149831792"
                },
                {
                    "start": 1913,
                    "end": 1917,
                    "matchedPaperCorpusId": "4893575"
                },
                {
                    "start": 1917,
                    "end": 1920,
                    "matchedPaperCorpusId": "195837279"
                },
                {
                    "start": 1951,
                    "end": 1954,
                    "matchedPaperCorpusId": "109940562"
                },
                {
                    "start": 1954,
                    "end": 1956,
                    "matchedPaperCorpusId": "2908352"
                },
                {
                    "start": 1956,
                    "end": 1959,
                    "matchedPaperCorpusId": "28847533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0704345703125
        },
        {
            "corpus_id": "72941074",
            "title": "Egocentric Bias and Doubt in Cognitive Agents",
            "text": "Modeling social interactions based on individual behavior has always been an area of interest, but prior literature generally presumes rational behavior. Thus, such models may miss out on capturing the effects of biases humans are susceptible to. This work presents a method to model egocentric bias, the real-life tendency to emphasize one's own opinion heavily when presented with multiple opinions. We use a symmetric distribution centered at an agent's own opinion, as opposed to the Bounded Confidence (BC) model used in prior work. We consider a game of iterated interactions where an agent cooperates based on its opinion about an opponent. Our model also includes the concept of domain-based self-doubt, which varies as the interaction succeeds or not. An increase in doubt makes an agent reduce its egocentricity in subsequent interactions, thus enabling the agent to learn reactively. The agent system is modeled with factions not having a single leader, to overcome some of the issues associated with leader-follower factions. We find that agents belonging to factions perform better than individual agents. We observe that an intermediate level of egocentricity helps the agent perform at its best, which concurs with conventional wisdom that neither overconfidence nor low self-esteem brings benefits.",
            "score": 0.2859234829246833,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1884765625
        },
        {
            "corpus_id": "265497827",
            "title": "Measurement and Mitigation of Bias in Artificial Intelligence: A Narrative Literature Review for Regulatory Science",
            "text": "AI systems are designed by humans, and as described earlier, humans can be disposed to biases.It is when we recognize our biases and their impacts on the decisions we make, that we create better outcomes in our lives.With the emergence and widespread adoption of AI systems among many industry sectors, the need to understand, detect, and mitigate bias in AI applications has generated great interest.Undesired bias in AI presents many potential negative risks, both to the implementing institution and its clientele.Erroneous data results, loss of employee and customer trust, and negative effects on reputation and corporate bottom lines are just a few undesired impacts on organizations that do not address negative bias in their AI systems.It has been noted that bias may cause the AI to perform suboptimally in its application, for example, by discriminating against potential customers. 3This type of system behavior could lead to an uptick in lawsuits, increased oversight, and erosion of a company's market worth.In building a good defense to reduce these risks, it is important to first understand the sources of bias in AI.\n\n][5][6][7][8] Table 1 summarizes five common sources of bias in AI applications, including (i) research design, (ii) training data, (iii) input representations, (iv) model architecture, and (v) real-world usage.\n\n1. Biases from the research design (i.e., the strategy or plan for designing and developing the AI), sometimes referred to as human bias, reflect the biases of the AI system's developers, and as such, the AI's goal and implementation may negatively impact underrepresented groups. 3This kind of bias can be introduced during data collection and filtering, subjective feature selection, or during the model evaluation by using specifically designed measures and evaluation techniques.Navarro et al. assessed methodological quality and risk of bias for a collection of studies using AI prediction models and found that 87% of these studies had a high risk of bias.Some of the greatest contributing factors to the bias were poor handling of missing data and the failure to address overfitting. 4. Biases from the training data can arise from two conditions.",
            "score": 0.28564538347918955,
            "section_title": "SOURCES of AI BIAS",
            "char_start_offset": 3711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 94,
                    "end": 217
                },
                {
                    "start": 217,
                    "end": 401
                },
                {
                    "start": 401,
                    "end": 517
                },
                {
                    "start": 517,
                    "end": 744
                },
                {
                    "start": 744,
                    "end": 894
                },
                {
                    "start": 894,
                    "end": 1021
                },
                {
                    "start": 1021,
                    "end": 1133
                },
                {
                    "start": 1135,
                    "end": 1346
                },
                {
                    "start": 1348,
                    "end": 1630
                },
                {
                    "start": 1630,
                    "end": 1831
                },
                {
                    "start": 1831,
                    "end": 2010
                },
                {
                    "start": 2010,
                    "end": 2140
                },
                {
                    "start": 2140,
                    "end": 2202
                }
            ],
            "ref_mentions": [
                {
                    "start": 1136,
                    "end": 1139,
                    "matchedPaperCorpusId": "237298625"
                },
                {
                    "start": 1139,
                    "end": 1142,
                    "matchedPaperCorpusId": "208355156"
                },
                {
                    "start": 1142,
                    "end": 1145,
                    "matchedPaperCorpusId": "237503402"
                },
                {
                    "start": 1145,
                    "end": 1148,
                    "matchedPaperCorpusId": "220047415"
                },
                {
                    "start": 1629,
                    "end": 1630,
                    "matchedPaperCorpusId": "145027458"
                },
                {
                    "start": 2139,
                    "end": 2140,
                    "matchedPaperCorpusId": "239028865"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12890625
        },
        {
            "corpus_id": "1051357",
            "title": "Specific to Whose Body? Perspective-Taking and the Spatial Mapping of Valence",
            "text": "As in previous tests of body-specific space-valence associations, here participants' judgments appear to follow the\"dominant side is good\" mapping (whether they activate this association consciously or unconsciously). On the simplest interpretation of these data, when Bob shares their point of view, participants compute \"left\" and \"right\" from an egocentric spatial perspective, and when Bob has the opposite point of view, participants compute \"left\" and \"right\" from an allocentric spatial perspective. \n\nYet, there is an alternative to this conclusion. The data from the Opposite Perspective condition are consistent with participants computing \"left\" and \"right\" allocentrically, from Bob's 180rotated viewpoint, based on Bob's bodily characteristics -assuming (perhaps implicitly) that Bob is a right-hander, which is true of about 90% of the population (Coren, 1992). But the data are also consistent with the possibility that participants are not really considering Bob's bodily characteristics, at all, and are instead adopting what we will call a \"rotated egocentric\" perspective: maybe participants are projecting their own bodily characteristics onto Bob (perhaps because they cast themselves in the \"role\" of Bob). In which case, in the Opposite Perspective condition they would assign the \"good\" animal to the box on their left, not because they assume that Bob is a right-hander (based on the handedness statistics of the population), but rather because they themselves are right-handed, and they compute space-valence associations based on their own bodily characteristics even when asked to reason from another person's perspective. \n\nAdopting a\"rotated egocentric\"perspective would be consistent with other demonstrations of surprising egocentrism in adults, in which experimental participants project their own bodily characteristics onto another person. For example, in one set of experiments, when asked to recall the eye color of well-known celebrities, brown-eyed participants were biased to attribute brown-eyedness to most of the stars tested, but blue-eyed participants were biased to attribute blue-eyedness to the stars, despite the rarity of blueeyedness in the population (Casasanto and Staum Casasanto, 2011).",
            "score": 0.2855083831206183,
            "section_title": "Results and discussion",
            "char_start_offset": 24604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1650
                },
                {
                    "start": 1653,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2241
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07354736328125
        },
        {
            "corpus_id": "277955321",
            "title": "FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering",
            "text": "we use GPT-4 (Achiam et al., 2023) to assess the degree of stereotyping in the generated outputs, with scores ranging from 0 (nonbiased) to 100 (most biased). Specific prompts from Wang et al. ( 2024) are provided to GPT-4 to guide the scoring process, as detailed in Appendix D.2.",
            "score": 0.2853005396852057,
            "section_title": "Evaluation Tasks",
            "char_start_offset": 14151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 281
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09686279296875
        },
        {
            "corpus_id": "226043746",
            "title": "Utility and use of accuracy cues in social learning of crowd preferences",
            "text": "Thus, by \"effective use\" of the number cue for SP revision, we mean that people modulate the degree of accepting others' estimates in a relative sense, assigning a higher weight to others' estimates when the number is relatively larger despite the overall egocentric bias. In other words, if the egocentric bias can be overcome or mitigated, e.g., by taking other people's perspectives on board [39], people are potentially capable of regulating their SP revision based on the number cue in a near-optimal fashion in an absolute sense. This issue of distinguishing between the absolute optimality in weighting and the effective use of modulatory cues should also be applied to our interpretations about the human use of the other accuracy cues in the text below.",
            "score": 0.2851361951627651,
            "section_title": "Utility and use of the number-of-estimates cue",
            "char_start_offset": 38679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 762
                }
            ],
            "ref_mentions": [
                {
                    "start": 395,
                    "end": 399,
                    "matchedPaperCorpusId": "38041612"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.257080078125
        },
        {
            "corpus_id": "254877512",
            "title": "Protected Attributes Tell Us Who, Behavior Tells Us How: A Comparison of Demographic and Behavioral Oversampling for Fair Student Success Modeling",
            "text": "Our third and final research question was whether behavioral oversampling would also help to mitigate biases in our detectors, and if so how it would compare to the other demographic-based methods. We first grouped students by clustering them in behavior space, and then upsampled under-represented groups to encourage the model to learn relationships between these behaviors and outcomes of interest. This method consistently reduced biases across demographic groups, and these reductions were comparable to those obtained using demographic oversampling. \n\nThough behavioral oversampling does manage to sidestep some of the limitations of demographic oversampling while simultaneously reducing model biases, it also has its share of limitations. First, it relies on the ability to accurately cluster behavioral data into meaningful clusters. An important direction for future work is to identify methods which produce meaningful clusters regardless of the data structure. Second, when demographic information is unavailable, there is no way to confirm that this method has succeeded in reducing biases against specific demographic groups. Still, the ability to mitigate biases between groups exhibiting similar behaviors is likely better than doing nothing at all. \n\nOverall, our recommendations for those interested in mitigating algorithmic bias in the domain of early detection of at-risk students are as follows. First, when multiple demographic attributes are available, we recommend examining model fairness within subgroups to account for potential intersectional biases. Guided attribute oversampling can then be used in place of more common approaches to mitigate these biases. Second, if students can also be clustered based on their behaviors, we recommend using behavioral oversampling as well. In our experiments, behavioral oversampling worked as well as guided attribute oversampling and in general offers a privacy-friendly way to correct biases. \n\nFinally, we have identified intersectional oversampling as an important direction for future research. Intersectional oversampling would proceed similarly to guided demographic oversampling by grouping students based on combinations of demographic attributes and then oversampling these groups, but would differ in how bias is mitigated. Instead of evaluating bias reduction within individual demographic groups, reduction in bias would be evaluated across intersectional groups. This change would bring guided demographic oversampling more in-line with intersectional theory, and would continue the work started in this paper to make early detection models more fair and trustworthy.",
            "score": 0.2850869458925547,
            "section_title": "DISCUSSION",
            "char_start_offset": 52977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1265
                },
                {
                    "start": 1268,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1963
                },
                {
                    "start": 1966,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2303
                },
                {
                    "start": 2304,
                    "end": 2445
                },
                {
                    "start": 2446,
                    "end": 2650
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03314208984375
        },
        {
            "corpus_id": "270845672",
            "title": "SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs",
            "text": "Our dataset was generated from GPT-4 using the Azure OpenAI API, which includes a content filter for multiple risk categories (e.g., hate speech, fairness, sexual language, violence).As this filter automatically removes potentially offensive content that is generated by GPT-4, we believe that the likelihood of our dataset containing such content is relatively low.However, content filtering models are not infallible and we are unable to manually inspect our entire dataset for the presence of offensive content due to its large scale.It is also possible that potentially harmful biases possessed by GPT-4 which do not trigger content filters are reflected in our dataset.Users should carefully consider these risks relative to the benefits of our synthetic dataset before deploying systems which are built using it.",
            "score": 0.2849578526535243,
            "section_title": "Ethical Considerations",
            "char_start_offset": 28573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 183,
                    "end": 366
                },
                {
                    "start": 366,
                    "end": 537
                },
                {
                    "start": 537,
                    "end": 674
                },
                {
                    "start": 674,
                    "end": 818
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2294921875
        },
        {
            "corpus_id": "266690715",
            "title": "From Bytes to Biases: Investigating the Cultural Self-Perception of Large Language Models",
            "text": "The presence of ideological and political bias in the training data can lead to output that favors certain ideologies and political perspectives, thereby amplifying existing biases. \n\nNearly all prior studies indicate the presence of ideological and political bias to varying extents. Assessments of various versions of the GPT models reveal a tendency toward alignment with progressive and libertarian viewpoints (Fujimoto & Takemoto, 2023;Ghafouri et al., 2023;Hartmann et al., 2023;R. Liu, Jia, et al., 2022;Motoki et al., 2023). A study by Gupta et al. (2023) highlights performance differences based on allegiances to different political figureheads, with \"Trump-supporter\" and \"life-long Republican\" personas showing poorer performance than \"Obama supporter\" and \"life-long Democrat.\" While more recent versions, such as GPT-3.5-turbo, who reduced bias in certain socioeconomic factors, implicit political bias still persists (Ghafouri et al., 2023). \n\nMeanwhile, Bing AI, which runs on GPT-4, displays more centrist viewpoints than both human responses and its artificial counterparts. This suggests an evolution toward mitigating biases on socioeconomic and political topics with each iteration of the GPT family (Ghafouri et al., 2023). However, other LLMs yield varied results. For instance, LLMs like text-davinci-002 and text-davinci-003 exhibit inconsistencies, displaying conservative leanings on religious topics despite other left-leaning biases. Similarly, tests using j1-grande, j1-jumbo, j1-grande-v2-beta, and text-ada-001 reveal similarly inconsistent skews (Santurkar et al., 2023). \n\nStudies on LLMs' moral tendencies yield intriguing yet conflicting results. One paper suggests LLMs tend toward stricter, less flexible moral decisions compared to humans. While GPT-3.5 and GPT-4 align closely with human patterns, PaLM-2 and Llama 2 notably diverge (Fujimoto & Takemoto, 2023). Drawing definite conclusions about moral biases proves challenging given their subjective and hotly debated nature.",
            "score": 0.2847242337163945,
            "section_title": "Ideological and political biases",
            "char_start_offset": 17527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 184,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 956
                },
                {
                    "start": 959,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1604
                },
                {
                    "start": 1607,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 414,
                    "end": 441,
                    "matchedPaperCorpusId": "264400798"
                },
                {
                    "start": 441,
                    "end": 463,
                    "matchedPaperCorpusId": "261243997"
                },
                {
                    "start": 463,
                    "end": 485,
                    "matchedPaperCorpusId": "255440573"
                },
                {
                    "start": 485,
                    "end": 511,
                    "matchedPaperCorpusId": "245692342"
                },
                {
                    "start": 511,
                    "end": 531,
                    "matchedPaperCorpusId": "257372256"
                },
                {
                    "start": 932,
                    "end": 955,
                    "matchedPaperCorpusId": "261243997"
                },
                {
                    "start": 1221,
                    "end": 1244,
                    "matchedPaperCorpusId": "261243997"
                },
                {
                    "start": 1873,
                    "end": 1900,
                    "matchedPaperCorpusId": "264400798"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12841796875
        },
        {
            "corpus_id": "272987514",
            "title": "Rephrase and Contrast: Fine-Tuning Language Models for Enhanced Understanding of Communication and Computer Networks",
            "text": "This iterative process continued until the dataset met our stringent quality criteria, i.e., meticulously crafted questions, exact answers, and logically coherent, structurally sound explanations. \n\nAfter the GPT-assisted QA pair generation, we now explain how we augment the obtained dataset with ChoiceBoost. Analysis of the generated data reveals two significant observations: 1) While GPT-4's assistance facilitates the production of highquality data, the associated API costs would be prohibitive for large-scale data expansion. 2) The generated data exhibited potential bias in option selection, which could mislead the finetuned model into acquiring unfounded statistical bias. \n\nChoiceBoost is designed to mitigate the above two issues. In ChoiceBoost, each question in the dataset was duplicated four times, with each duplicate assigned a different correct answer (A, B, C, or D), while maintaining the original order of incorrect options. This approach substantially expands the dataset without additional API cost (which enhances the model's capacity by learning from diverse answer configurations) and mitigates biases inherent in the initial data generation process.",
            "score": 0.28452283746115986,
            "section_title": "A. Dataset Construction",
            "char_start_offset": 14160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 199,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1179
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3857421875
        },
        {
            "corpus_id": "265033415",
            "title": "Towards objective and systematic evaluation of bias in artificial intelligence for medical imaging",
            "text": "The same is true for studies that propose or benchmark bias mitigation methods 19,20 -while it is valuable to know that certain techniques are successful in reducing performance disparities between subgroups on certain clinical tasks, it remains unknown if such strategies are correcting for the true sources of bias in images, and if they are also effective when applied to data from populations not represented in the datasets used to evaluate these mitigation strategies. Therefore, to objectively and systematically study how biases manifest in deep learning models and confidently evaluate the robustness of bias mitigation strategies, it is imperative to understand and control the biases in a medical imaging dataset in the first place. \n\nIn other domains of computer vision-based AI research, \"toy\" datasets, such as MNIST 21 and its variations, 22,23 enable researchers to benchmark deep learning architectures and study simple, controlled scenarios to evaluate how trained models perform on data with, for example, intentional biases such as background color. However, there is a substantial gap between understanding how bias in these rather simple and small (28 \u00d7 28 pixels) 2-dimensional images impact deep learning models, compared to complex interactions between anatomical and scanner-induced biases in medical images that can comprise millions of voxels. In this work, we specifically aim to bridge this gap by introducing a method that integrates the customization and control of MNIST-like datasets with the scale and complexity of 3-dimensional medical images.",
            "score": 0.2842354615361336,
            "section_title": "Background and significance",
            "char_start_offset": 1854,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1580
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 82,
                    "matchedPaperCorpusId": "247619271"
                },
                {
                    "start": 82,
                    "end": 84,
                    "matchedPaperCorpusId": "252693109"
                },
                {
                    "start": 831,
                    "end": 833,
                    "matchedPaperCorpusId": "14542261"
                },
                {
                    "start": 854,
                    "end": 857,
                    "matchedPaperCorpusId": "203838141"
                },
                {
                    "start": 857,
                    "end": 859,
                    "matchedPaperCorpusId": "52894787"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.019683837890625
        },
        {
            "corpus_id": "259376840",
            "title": "Rating Short L2 Essays on the CEFR Scale with GPT-4",
            "text": "We showed that unlike GPT-3.5, GPT-4 is able to attain performance similar to conventional Automated Writing Evaluation (AWE) models when rating short L2 essays. GPT-4 only required one calibration example per rating category to achieve near optimal performance, but other prompt engineering techniques we tried were not very helpful. Furthermore, when assessing fairness with respect to the test-taker's gender or L1, we found that while GPT-4 did not show bias in favor of any one group, it showed significantly less agreement with human ratings for some L1s. It is unclear whether this is due to the reliability of GPT-4 or that of the human ratings themselves. More research is needed to understand this discrepancy and its implications for fairness. Future research may also explore other prompt engineering strategies for improving GPT-4's performance at this task, or potentially finetuning GPT-3.5, enabling one to leverage dramatically more training data than what can be provided in a prompt. Perhaps most excitingly, future work may explore GPT-4's potential for providing feedback aligned to essay scoring: a task for which GPT-4 seems particularly well suited.",
            "score": 0.28408882189906026,
            "section_title": "Conclusion",
            "char_start_offset": 13183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1173
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10931396484375
        },
        {
            "corpus_id": "265018956",
            "title": "DialogBench: Evaluating LLMs as Human-like Dialogue Systems",
            "text": "Since GPT-4 is trained on online data, GPT-4 may encode biases that perpetuate stereotypes, discrimination, or marginalization of specific languages or communities. This results in DialogBench potentially generating toxic and harmful instances Furthermore, we induce GPT-4 to generate a certain proportion of unfriendly dialogues for evaluating LLMs in unfriendly scenarios, which can reflect the true level of LLMs as human-like dialogue systems. Accordingly, this might lead to some unkind and harmful instances. In addition, we employ three experts to manually do these evaluation questions. We pay 0.2 to each expert for each instance.",
            "score": 0.2839774760499445,
            "section_title": "Ethics Statement",
            "char_start_offset": 26294,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 639
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20068359375
        },
        {
            "corpus_id": "257771531",
            "title": "Uncovering Bias in Personal Informatics",
            "text": "Guideline #1: To identify historical biases relevant to the use case at hand, consult prior literature and domain experts (e.g., oximeters are proven susceptible to biases against darker skin tones [46]) or conduct small-scale feasibility studies with relevant and diverse demographics. Guideline #2: If data are self-collected, aim for diverse user recruitment and collect and report relevant protected attributes (e.g., via datasheets for datasets and data statements). Otherwise, evaluate algorithms in generalizable cross-dataset benchmarks [108] and inclusive synthetic data [98], whenever possible. In either case, consider appropriate data manipulation actions to alleviate biases, e.g., re-sampling/rebalancing populations conditioned on demographic attributes. Guideline #3: When working with data originating from diverse devices, investigate device ownership differences conditioned on demographic attributes. Also, incorporate uncertainty estimation approaches and be transparent about possible measurement error effects in downstream tasks. \n\nSumming up, we believe that our findings shed light on the biases that can creep into the data generation and model building, and implementation streams of PI technologies. While our mitigation guidelines are by no means exhaustive, they provide a starting point for researchers and practitioners to incorporate bias assessments \"by design\" in the life cycle of their works to alleviate the potential negative effects of such biases. \n\nModel Building and Implementation Stream. Based on our results, digital biomarkers representation biases can be propagated or even amplified by learning models, regardless of the inclusion of protected attributes in the feature set. This is due to the existence of proxy variables in PI data that can be used by models to infer hidden protected attributes. Such aggregation biases are also prevalent in our use case for users with joint issues, diabetes, hypertension, and female users. These biases may (or may not) lead to discrimination depending on the context. Yet, mitigating them is the safest way to ensure fairness. Additionally, common learning choices in PI, such as personalization, can introduce learning biases, if trained on biased data. In our case, they perform worse -in terms of bias-across all attributes, while, in extreme cases (e.g., diabetic users), they can even introduce maximum bias.",
            "score": 0.2838337149935353,
            "section_title": "Findings & Implications",
            "char_start_offset": 47290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1053
                },
                {
                    "start": 1056,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2244
                },
                {
                    "start": 2245,
                    "end": 2403
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 202,
                    "matchedPaperCorpusId": "250422784"
                },
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "239768540"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1588134765625
        },
        {
            "corpus_id": "274141633",
            "title": "METEOR: Evolutionary Journey of Large Language Models from Guidance to Self-Growth",
            "text": "Additionally, we aim for the model to evolve its domain capabilities independently, without relying on strong models. To achieve this, we propose a method for the model to autonomously enhance its domain capabilities. Increasing FLOPs during inference has been  shown to effectively enhance model performance in downstream tasks (Snell et al., 2024). Inspired by this concept, we hypothesize that different inference strategies yield varying outcomes. We aim for the model's outputs to closely align with those generated by high-FLOPs strategies, while diverging from those produced by low-FLOPs strategies. Therefore, we employ beam search (Freitag and Al-Onaizan, 2017)  Metrics. Following the evaluated method proposed by Zheng et al. (2023), we use GPT-4 as a judge to evaluate both data quality and model performance. When comparing the distilled data quality with and without the use of guidelines, GPT-4 is used to score the data, where higher scores indicate better quality. In evaluating the domain-specific answers generated by the model, GPT-4 provides scores based on five criteria: accuracy, completeness, relevance, coherence, and reliability, allowing for a comprehensive assessment of the model's domain capabilities. The prompt for GPT-4's pairwise comparison across five evaluation criteria is presented in Table 8 of Appendix B. \n\nModel and parameters setting. We selected LLaMA3-8B-Chat (Dubey et al., 2024) and Qwen2-7B-Instruct (Yang et al., 2024a) as the baseline models for domain model training in this study. \n\nFor GPT-4, we utilized the publicly available API model, GPT-4o. During the iterative evolution of the model, we set the maximum number of iterations to N = 3, with a cumulative data amount of K = 10 during each iteration. The beam size for beam search was set to 10, and the maximum token length for the domain evolution model was configured to be 4096.",
            "score": 0.2837184126471757,
            "section_title": "Self-Evolution of Domain Capabilities through Inference Strategy Optimization",
            "char_start_offset": 13611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1347
                },
                {
                    "start": 1350,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1534
                },
                {
                    "start": 1537,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1891
                }
            ],
            "ref_mentions": [
                {
                    "start": 641,
                    "end": 671,
                    "matchedPaperCorpusId": "2229477"
                },
                {
                    "start": 725,
                    "end": 744,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 1450,
                    "end": 1470,
                    "matchedPaperCorpusId": "271923338"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03302001953125
        },
        {
            "corpus_id": "257687695",
            "title": "Capabilities of GPT-4 on Medical Challenge Problems",
            "text": "The results above demonstrate the capabilities of the publicly released version of GPT-4. As described by [Ope23], GPT-4 was developed in two major phases. The pretraining phase focused on maximizing the ability of the model to predict the next token in a document using a variety of data, both publicly available and privately licensed. In a subsequent post-training phase, the model was fine-tuned using RLHF [CLB + 17] to enhance its ability to follow instructions and its propensity for producing fair and safe outputs [OWJ + 22]. Several stages of refinement were aimed at minimizing undesirable behaviors such as gender bias, toxic language and stereotyping, dangerous recommendations, and harmful manipulation. OpenAI's experiments indicated that the RLHF-centric refinement did not adversely affect the model's capabilities on the exams they tested [Ope23]. We gained access to the base model, which we refer to as GPT-4-base, to study potential differences in performance attributable to the alignment process. The results of this evaluation are presented in Tables 4 and 5. While GPT-4-base and the release model both exhibit consistently strong performance across all 14 experimental datasets under study, we observe a notable increase of 3-5% when using the base versus the release model. \n\nThe experiments suggest that orienting the base model toward safety and instruction-following can influence performance on medical benchmarks. The observed diminishment of raw performance accompanying the alignment process frames directions for future research. Refinements to the fine-tuning procedures employed to shape GPT-4-base into the publicly released GPT-4 may be able to better navigate the tradeoff between safety and accuracy. Additionally, alternative fine-tuning techniques, such as incorporating expert domain knowledge or leveraging specialized medical datasets, may lead to further improvements in model performance without sacrificing safety and instruction-following capabilities.",
            "score": 0.2834369669318249,
            "section_title": "Influence of Alignment and Safety Tuning",
            "char_start_offset": 18531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 2002
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16162109375
        },
        {
            "corpus_id": "257687695",
            "title": "Capabilities of GPT-4 on Medical Challenge Problems",
            "text": "Studies have revealed biases in the delivery of healthcare, noting disparities in care received by people experiencing marginalization [CSM18, HCL + 15]. Such biases in healthcare delivery have been demonstrated to influence the systems and models that are developed to provide guidance to healthcare organizations and practitioners. Without study to address and mitigate bias in data and the systems constructed from that data, we risk fielding systems that propagate long-term disparities and inaccuracies [OPVM19]. Exploration of biases in healthcare and the potential reflection of these biases in AI systems come in the context of broader work on biases of AI systems. Several efforts have demonstrated that the output of machine-learned models can be unfair and harmful to specific groups of people, depending on backgrounds and demographics (e.g., [HZH17,BG18]). Progress has been made on technical methods for detecting and mitigating harmful biases in task-specific systems built via supervised machine learning (e.g., [ABD + 18, KS21]). However, engineers and organizations continue to face conceptual challenges with defining measures of fairness [JW21,BHN19]. \n\nWe have a poor understanding of the biases accrued and transmitted by large-scale language models, and how issues with fairness might arise for different types of healthcare-centric prompting and generations. In the absence of study, we must be wary of biases in both clinical practices and research [AGHR22] with regard to race, socioeconomic background, gender, and other factors, which are laced throughout the corpora used to train large-scale language models. Research is needed to understand the fairness of healthcare-centric recommendations generated by LLMs. \n\nInfluences on workflows, tasks, and specialties. The competencies on the USMLE examinations and other medical workloads suggest that GPT-4, properly harnessed with appropriate expert oversight, can contribute to enabling precision clinical medicine. GPT-4 and its successors could be leveraged to provide healthcare practitioners with analytics, reminders, and decision support, including assistance with the formulation and revision of differential diagnoses from patient history, physical findings and lab results, identification of relevant tests and their sequencing, and constructing therapy plans.",
            "score": 0.28334529336314596,
            "section_title": "Risks of Bias.",
            "char_start_offset": 51107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1171
                },
                {
                    "start": 1174,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1741
                },
                {
                    "start": 1744,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2347
                }
            ],
            "ref_mentions": [
                {
                    "start": 508,
                    "end": 516,
                    "matchedPaperCorpusId": "204881868"
                },
                {
                    "start": 855,
                    "end": 862,
                    "matchedPaperCorpusId": "221970190"
                },
                {
                    "start": 862,
                    "end": 867,
                    "matchedPaperCorpusId": "3298854"
                },
                {
                    "start": 1164,
                    "end": 1170,
                    "matchedPaperCorpusId": "248118878"
                },
                {
                    "start": 1474,
                    "end": 1482,
                    "matchedPaperCorpusId": "250954558"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1094970703125
        },
        {
            "corpus_id": "272551081",
            "title": "Identifying the sources of ideological bias in GPT models through linguistic variation in output",
            "text": "Extant work shows that generative AI models such as GPT-3.5 and 4 perpetuate social stereotypes and biases. One concerning but less explored source of bias is ideology. Do GPT models take ideological stances on politically sensitive topics? In this article, we provide an original approach to identifying ideological bias in generative models, showing that bias can stem from both the training data and the filtering algorithm. We leverage linguistic variation in countries with contrasting political attitudes to evaluate bias in average GPT responses to sensitive political topics in those languages. First, we find that GPT output is more conservative in languages that map well onto conservative societies (i.e., Polish), and more liberal in languages used uniquely in liberal societies (i.e., Swedish). This result provides strong evidence of training data bias in GPT models. Second, differences across languages observed in GPT-3.5 persist in GPT-4, even though GPT-4 is significantly more liberal due to OpenAI's filtering policy. Our main takeaway is that generative model training must focus on high-quality, curated datasets to reduce bias, even if it entails a compromise in training data size. Filtering responses after training only introduces new biases and does not remove the underlying training biases.",
            "score": 0.2833370454067593,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1898193359375
        },
        {
            "corpus_id": "18177826",
            "title": "Developmental changes in mental rotation ability and visual perspective-taking in children and adults with Williams syndrome",
            "text": "As in Experiment 1, a Two-Way ANOVA was applied to the correct responses and the proportion of egocentric-bias errors. In the analysis, Group (WS, VMA, and CA groups) was used as a between-subject factor, and Task (SM and SMI) was used as a within-subject factor. \n\nIn addition to the ANOVA, we used the same methods as in Experiment 1 to analyze correct and incorrect ToM task responses for the WS and VMA groups. For each ToM task, a three-way mixed-design repeated measures ANOVA was applied to the correct responses. Group (WS and VMA) and ToM performance (Pass group and Fail group of participants) were used as betweensubject factors, and Task (SM and SMI) was used as a withinsubject factor. If the sphericity assumption was violated as per Mauchly's sphericity test, then the Greenhouse-Geisser epsilon coefficient was used to correct the degrees of freedom. Both the F and P-values were then recalculated. A P-value of < 0.05 was considered statistically significant. \n\nIn addition to these analyses, we adopted a developmental trajectory approach to assess developmental changes in task performance for both the WS and VMA groups (Thomas et al., 2009). As in Experiment 1, we did not apply this analysis to the CA group because their performance scores reached a ceiling level, thus, preventing further developmental changes from being observed. For this analysis, we calculated coefficients and evaluated improvements in performance based on developmental changes in verbal mental age.",
            "score": 0.28287660442857926,
            "section_title": "DATA ANALYSIS",
            "char_start_offset": 25917,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 263
                },
                {
                    "start": 266,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 976
                },
                {
                    "start": 979,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1496
                }
            ],
            "ref_mentions": [
                {
                    "start": 1140,
                    "end": 1161,
                    "matchedPaperCorpusId": "14795581"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01262664794921875
        },
        {
            "corpus_id": "278502578",
            "title": "Do Language Model Agents Align with Humans in Rating Visualizations? An Empirical Study",
            "text": "Possible misuses. Novice evaluators may be influenced by the agent's human-like outputs as demonstrated in our study, despite their inherent unreliability. Moreover, verifying qualitative data proves more difficult than text data, possibly amplifying reliability concerns regarding crowdsourced data [21]. Currently, only closed-source large models like GPT-4V can rate visualizations in a human-like manner, with high usage costs serving as a temporary barrier. However, rapid advancements in open-source models may soon reduce these barriers, increasing associated risks. \n\nKeeping Pace with State-of-the-Art LLMs. More advanced LLMs are emerging that could produce ratings more aligned with human responses. We reproduced the agent study in Sec. 4.3.2 using GPT-4o and presented the results in the supplementary material. While GPT-4o differs from GPT-4V in rating range, its alignment with humans mirrors that of GPT-4V. We believe our study methods (Sec. 4.2) and open-source code can be directly applied to any LLM, and encourage others to use them to quickly verify the align-ment of the latest agent ratings with human responses. Data pollution. Since our studies predate GPT-4V's latest training, data pollution was anticipated and became evident in our test on Imputation for Uncertainty using text and general descriptions (Sec. 5.1). However, the agents' poor performance with specific image inputs made this issue less pronounced. Interestingly, data pollution offers an expected benefit. Though it hinders simulating new expert research, it aligns agent feedback with general evaluators for basic tasks, potentially making them useful for everyday visualization design and evaluation. \n\nKnowledge-assisted and reliable agent simulation. As discussed in Sec. 5.2, injecting external knowledge can enhance the alignment of agent-based feedback in certain scenarios but raises concerns about generalizability and reliability. Content searched from websites could introduce additional biases, especially for nonexpert evaluators unable to identify them. The challenge stems from the scattered and unsystematic nature of existing knowledge on human perception, with no comprehensive visual cognitive knowledge readily available.",
            "score": 0.28246718952177247,
            "section_title": "DISCUSSION",
            "char_start_offset": 46179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 573
                },
                {
                    "start": 576,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1698
                },
                {
                    "start": 1701,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2237
                }
            ],
            "ref_mentions": [
                {
                    "start": 300,
                    "end": 304,
                    "matchedPaperCorpusId": "258218228"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.307861328125
        },
        {
            "corpus_id": "16166122",
            "title": "Inferring global network properties from egocentric data with applications to epidemics",
            "text": "We now consider the case that we observe egocentric data, i.e. we observe the degree of a sample or all of the actors in the network. In case of a sample we neglect the uncertainty stemming from not knowing the exact degree distribution. We hence assume that we know the degree distribution {p k }, where p k is the probability that a randomly selected actor has degree k.",
            "score": 0.28246718952177247,
            "section_title": "Observing egocentric data: ego only",
            "char_start_offset": 14882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 372
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00945281982421875
        },
        {
            "corpus_id": "265154694",
            "title": "Evidence against implicit belief processing in a blindfold task",
            "text": "If, however, participants exhibited an egocentric bias, this measure would collapse to zero or even become positive. Since the egocentric bias can be interpreted as a negative measure of ToM and the altercentric bias as a positive indicator of ToM, we expected a negative correlation between the two. However, this analysis yielded moderate evidence against a negative correlation between the altercentric and the egocentric bias (Pearson's r = -0.032, BF 10 = 0.171).",
            "score": 0.28246718952177247,
            "section_title": "Results",
            "char_start_offset": 27832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 468
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057708740234375
        },
        {
            "corpus_id": "209462604",
            "title": "Intra- and inter-task reliability of spatial attention measures in healthy older adults",
            "text": "Once a spatial task was identified to have such outliers, the whole sample of that testing day (37 individual biases) was winsorized to adjust the spatial biases. The winsorized mean was calculated by replacing 20% of the 37 individual biases. The smallest and largest spatial biases were replaced with the values closest to them [35]. There were no outliers identified for the LM and MLB tasks so these remained untrimmed. For the GREY, GRA and LVD tasks 20% winsorized (modified) means were used. For the average bias across both testing days (for each of these 3 spatial tasks) the original (unmodified) means was calculated and then winsorized by 20%. In addition, one participant was excluded from the LVD task due to difficulty in detecting even the largest targets, despite passing the initial visual screening tests and the task modifications that were made (PSE = -22). Nonetheless this participant was not excluded from the other 4 tasks. This resulted in 37 participants for the LM, MLB, GREY and GRA tasks, and 36 participants for the LVD task, maintaining cross-task comparability as much as possible.",
            "score": 0.28246718952177247,
            "section_title": "Analyses",
            "char_start_offset": 21309,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1114
                }
            ],
            "ref_mentions": [
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "11784427"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0043487548828125
        },
        {
            "corpus_id": "275213467",
            "title": "Evidence that altercentric biases in a continuous false belief task depend on highlighting the agent's belief",
            "text": "To test whether there was a positive egocentric bias in the explicit task, we performed a directed Bayesian one sample Wilcoxon signed rank test >0. This test yielded moderate evidence against a positive egocentric bias (BF 10 = 0.17, W = 2280, R = 1, M = 5.08, SD = 63.20, see Fig. 4A). \n\nTo examine effects of task order and order of belief condition, we conducted an ANOVA with the factors task order (implicit first/explicit first) and belief order (false belief first/true belief first). This yielded moderate evidence against an effect of task order (BF 10 = 0.22, error = 2.74 %, model including belief order and task order against model including only belief order), inconclusive evidence for an effect of the order of belief condition (BF 10 = 0.42, error 2.04 %, model including belief order and task order against model including only task order), and moderate evidence against an interaction between the factors (BF 10 = 0.29, error = 2.89 %, full model against model including only the main effects). \n\nLooking at the data of participants who did the explicit task first and participants who did the implicit task first separately yielded moderate evidence against a positive egocentric bias for both groups (explicit first: As for the implicit task, in addition to our preregistration, we tested the subgroups depending on the order in which they had seen the conditions (implicit/explicit task first and false belief first/true belief first) with separate Bayesian Wilcoxon signed rank tests. This confirmed moderate evidence against an egocentric bias in both subgroups (implicit and explicit first) that started the task with the true belief condi- As before, we repeated our main analyses excluding trials in which participants searched on the wrong side of the screen (i.e., indicated that M.L. Speiger et al. Cognition 256 (2025) 106055 \n\nthe agent would search closer to the real object location than where the agent believed the object to be). This confirmed the evidence against an egocentric bias in the entire data set (BF 10 = 0.04, W = 1641, R = 1, M = \u2212 7.56, SD = 36.81)",
            "score": 0.28246718952177247,
            "section_title": "Explicit task",
            "char_start_offset": 24927,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 1013
                },
                {
                    "start": 1016,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1856
                },
                {
                    "start": 1859,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2099
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08447265625
        },
        {
            "corpus_id": "265033415",
            "title": "Towards objective and systematic evaluation of bias in artificial intelligence for medical imaging",
            "text": "Further work could be performed using this framework to analyze how mislabeling of bias attributes affects the result of this and other mitigation strategies. Moreover, the No Bias counterfactual datasets made it clear that performance disparities in the group models were not caused by the bias effects, but rather by the target class imbalance within each bias group. If this approach was used to debias a classification task on real data, the cause of these performance disparities may have been unclear since it could have been related to other subgroup attributes unaccounted for. Finally, we observed that the saliency scores align with performance of the model for each bias mitigation strategy. With knowledge of the exact regions that should be relevant for model decision-making, and of how a particular explainability method manifests in the absence of the bias effect, this framework facilitates a highly objective interpretation of explainable AI in medical imaging. Further work could use this setup for studying the extent to which different explainability methods highlight regions associated with predictive targets or bias effects, or developing new methods that are better at identifying salient regions of medical images. \n\nAlthough the SimBA tool enables the generation of increasingly complex dataset bias scenarios, real-world medical imaging data can be even more complex than the synthetic data we analyze in this controlled setup. This is because real datasets inherently contain numerous confounding and interacting biases, many of which may be imperceptible to humans. For instance, it may be necessary to account for disease prevalence and population shift when evaluating performance disparities in real data, since the underlying distribution of other subgroup-associated factors may confound analysis. Glocker et al 16 account for this by performing test set resampling to balance the representation of subgroups for a more accurate representation of algorithmic bias. Conversely, using the SimBA data generation tool in our framework, these underlying distribution shifts can be mitigated, as we do here with stratified sampling, or systematically simulated and evaluated by generating datasets with shifts in the distributions of subject or disease effects.",
            "score": 0.2823900913338509,
            "section_title": "Discussion",
            "char_start_offset": 21937,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2291
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0294189453125
        },
        {
            "corpus_id": "1051357",
            "title": "Specific to Whose Body? Perspective-Taking and the Spatial Mapping of Valence",
            "text": "For example, in one set of experiments, when asked to recall the eye color of well-known celebrities, brown-eyed participants were biased to attribute brown-eyedness to most of the stars tested, but blue-eyed participants were biased to attribute blue-eyedness to the stars, despite the rarity of blueeyedness in the population (Casasanto and Staum Casasanto, 2011). This effect persisted when analyses were controlled for how well participants knew the celebrities, how well they liked them, and how confident participants were in their judgments: participants still tended to project their own bodily characteristics onto other people. If such egocentric projection of one's own bodily traits onto others accounts for the results of the Opposite Perspective condition here, it would be inappropriate to conclude that switching points of view caused participants to spatialize valence from an allocentric perspective, based on Bob's (assumed) bodily characteristics. \n\nOne way to distinguish between the \"allocentric\" and \"rotated egocentric\" possibilities would be to repeat Experiment 1 in left-handers. If participants reason about Bob's choices from an allocentric perspective, then right-and left-handers should respond similarly in the Opposite Perspective condition, since both groups should assume that Bob is a right-hander, based on the statistics of the population. Alternatively, if participants reason from a rotated egocentric perspective, then right-and left-handers should show opposite patterns of responses in the Opposite Perspective condition: right-handers should impute right-handedness to Bob and choose the box on their left, but left-handers should impute left-handedness to Bob and choose the box on their right. Yet, there are practical and theoretical limitations to this proposed test. Practically speaking, a very large sample would be needed in order to recruit a sufficient number of left-handers from the general population. Theoretically, these imagined data would still be correlational, and therefore subject to speculations about other unexamined differences between right-and left-handers' judgments.",
            "score": 0.28238317196826546,
            "section_title": "Results and discussion",
            "char_start_offset": 26479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2139
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049591064453125
        },
        {
            "corpus_id": "268230909",
            "title": "Cognitive Bias in Decision-Making with LLMs",
            "text": "In general, we see small improvements when using zero-shot prompting. For Llama models, the awareness debiasing strategy shows better results for anchoring bias, whereas other (few-shot) methods lead to failure cases (Table 3). Awareness mitigation mitigates patterns of primacy bias to a certain extent (makes the distribution more uniform) for LLama 2 and GPT-4, but selfhelp leads to better results (Figure 3).",
            "score": 0.28236115125118577,
            "section_title": "Zero-Shot Debiasing helps to mitigate Bias",
            "char_start_offset": 23248,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 413
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.263427734375
        },
        {
            "corpus_id": "267312037",
            "title": "Beyond principlism: practical strategies for ethical AI use in research practices",
            "text": "Responsible and ethical use requires a nuanced understanding of the tool in question. LLMs are adept at generating text in a probabilistic manner-predicting the next token-based on their vast training datasets, often supplemented by reinforcement learning from human feedback (RLHF). Hence, the model output is probabilistic and intricately related to the input, training, customization, and deployment/application, raising major ethical concerns, such as truthfulness and bias. Regarding truthfulness, while methods have been developed to enhance output accuracy, such as retrieval-augmented generation (RAG)-a technique that sources relevant information from a given database to inform the model's response [34]-LLMs can still produce statements that sound confident but are incorrect or misleading (i.e., hallucinations). \n\nBias is insidious, infiltrating the multistage process of model training [35]. As Figure 1 and Box 2 detail, biases can emerge during data collection, preprocessing, model pre-training, customization/fine-tuning, and evaluation stages, with various mitigation strategies and measurement techniques available to address them. The consequences of biases primarily manifest in two forms: representational and allocational harms [36]. Representational harms occur when LLMs perpetuate stereotypes or misrepresent certain groups, reinforcing societal prejudices. For example, LLMs may generate text that reinforces stereotypes about gender, race [37], and geographic regions (e.g., associating them with negative attributes like low intelligence or unattractiveness, particularly for regions with lower socioeconomic status [38]). \n\nAllocational harms involve the unfair distribution of resources, opportunities, or other outcomes (e.g., education, jobs, loans). This occurs because of distribution shifts-the data distribution in real-world applications can differ substantially from the training data, known as out-of-distribution (OOD, as opposed to in-distribution) data [39]. Particularly when integrating AI within health and behavioral research, cultural sensitivity is crucial to accurately understand and respond to a range of cultural contexts, which may have different interpretations of behavior, language, and mental health.",
            "score": 0.28224430482125906,
            "section_title": "Goal one: understand model training, fine-tuning, and output",
            "char_start_offset": 15010,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 824
                },
                {
                    "start": 827,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1652
                },
                {
                    "start": 1655,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2259
                }
            ],
            "ref_mentions": [
                {
                    "start": 709,
                    "end": 713,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 900,
                    "end": 904,
                    "matchedPaperCorpusId": "271164699"
                },
                {
                    "start": 1252,
                    "end": 1256,
                    "matchedPaperCorpusId": "261530629"
                },
                {
                    "start": 1468,
                    "end": 1472,
                    "matchedPaperCorpusId": "261898112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1939697265625
        },
        {
            "corpus_id": "276647172",
            "title": "The erasure of intensive livestock farming in text-to-image generative AI",
            "text": "DALL-E 3's prompt revision was originally designed to mitigate bias [8]. The process involves using GPT-4 to \"upsample\" short user prompts into detailed, descriptive prompts. OpenAI has disclosed a system prompt they used to instruct GPT-4 to rewrite user prompts (see Appendix C in Betker et al. [8]) but the full guidelines governing prompt revision-particularly those concerning the removal of public figures and branded items, as well as protocols for animal depiction-are not publicly available. Although DALL-E 3's training data sources are also not disclosed, the evaluation dataset testing DALL-E 3's prompt following ability is publicly available. Among the 8000 evaluation prompts extracted from MSCOCO [8], 93 prompts involve cows/cattle, 58 depicted pastoral scenes such as cows on pasture with calves outdoors, while only 6 described housing a few cows indoors in pens. The remaining 29 prompts portrayed atypical scenarios like cows walking on streets and pigs only appeared in one prompt (in a cooking context). While OpenAI states they did not specifically use MSCOCO for training or optimization, they do acknowledge potential data leakage in the training process [8]. The model's ability to accurately depict the reality of intensive livestock farming was not evaluated. \n\nMore importantly, while this automatic prompt revision process is documented in API system cards available for programmers, the general public who mostly access these models through Chat-GPT's website or app interfaces are kept ignorant of this, raising transparency concerns. Without specialized prompt engineering techniques, typical ChatGPT users are unlikely to generate realistic representations of modal livestock farming. We recommend that ChatGPT transparently inform its website and app users about the prompt revision process, and provide more representative depictions of modern livestock farming, especially when it is explicitly requested to do so. It is important in the ongoing discussions between society and the animal industries that transparency exists regarding current farming practices, as failure to do so increases the risk of disconnect between producers and the consumers who purchase their products. E, F). The \"revise\" notation in the plot refers to images generated when DALL-E 3 by default revised user prompts.",
            "score": 0.28219333448022565,
            "section_title": "The biases in GPT-4 enabled prompt revision",
            "char_start_offset": 16534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2217
                },
                {
                    "start": 2218,
                    "end": 2224
                },
                {
                    "start": 2225,
                    "end": 2332
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0592041015625
        },
        {
            "corpus_id": "271719952",
            "title": "LLM economicus? Mapping the Behavioral Biases of LLMs via Utility Theory",
            "text": "As a competence test, we evaluate whether the LLM is monotonically consistent in choosing a switching point. We test GPT 4 and GPT 4 Turbo on this game, as these were the only  LLMs that passed the competency test. We use nonlinear regression to fit the value and weighting functions from the empirical certainty equivalents. As shown in Figure 2, GPT 4 and GPT 4 Turbo exhibit no probability distortion for gains. These LLMs are thus more economically rational than humans in assessing probabilities for gains. However, GPT 4 Turbo exhibits stronger probability distortion than humans for losses. Like humans, GPT 4 Turbo tends to overweight low probabilities and underweight high probabilities. Thus, GPT 4 Turbo is less economically rational than humans in assessing probabilities for losses. Both GPT 4 and GPT 4 Turbo exhibit lower risk aversion towards gains. Like humans, GPT 4 Turbo exhibits risk-seeking behavior towards losses. However, GPT 4 exhibits risk aversion towards losses. However, we find that GPT 4 Turbo is less reliable at evaluating losses than GPT 4, as shown in Figure 10 in the Appendix. In these cases, GPT 4 Turbo will act against its stated strategy and make riskier choices than usual.",
            "score": 0.2821739572096874,
            "section_title": "Risk and Loss Aversion: Gambling Games",
            "char_start_offset": 11636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1216
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03424072265625
        },
        {
            "corpus_id": "251468291",
            "title": "D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling Algorithmic Bias",
            "text": "This will compute and present a set of fairness metrics in the same fashion as the D-BIAS tool. Lastly, the user can click the 'Debias & Evaluate' button to debias the dataset and generate its evaluation metrics. A small lag is introduced before displaying evaluation metrics to mimic a real debiasing algorithm. \n\nStudy design. We conducted a within subject study where each participant was asked to use the baseline tool and D-BIAS in random order. The study was conducted remotely, i.e., each participant could access and interact with the tools via their own machine. For each tool, a small tutorial was given using the Synthetic Hiring dataset2 to demonstrate the workflow and features of the tool. Each participant was then given some time to explore and interact with each system. Next, the participants were asked to identify and mitigate bias for the Adult Income dataset. For the D-BIAS tool, participants were also asked to complete a set of 5 tasks to evaluate usability. Each task was carefully designed to cover our testing goals and had a verifiable correct solution. Tasks included: generate a causal network, direct undirected edges, identify if bias exists with respect to an attribute, identify proxy variables and finally debias the dataset. After using each tool, the participants were asked to answer a set of survey questions. Lastly, we collected subjective feedback from each participant regarding their overall experience with both the tools. Throughout the study, participants were in constant touch with the moderator for any assistance. Participants were encouraged to think aloud during the user study. \n\nSurvey Design. Each participant was asked to answer a set of 13 survey questions to quantitatively measure usability, interpretability, workload, accountability and trust. All of these questions can be answered on a 5-point Likert Scale. To capture cognitive workload, we selected two applicable questions from the NASA-LTX task load index [27], i.e., \"How mentally demanding was the task? and \"How hard did you have to work to accomplish your level of performance?\". Participants could choose between 1 = very low to 5 = very high. For capturing usability, we picked 3 questions from the System Usability Scale (SUS) [7].",
            "score": 0.28217255597331015,
            "section_title": "USER STUDY",
            "char_start_offset": 39714,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 312
                },
                {
                    "start": 315,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2102
                },
                {
                    "start": 2103,
                    "end": 2167
                },
                {
                    "start": 2168,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 1975,
                    "end": 1979,
                    "matchedPaperCorpusId": "15252590"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06829833984375
        },
        {
            "corpus_id": "267617105",
            "title": "The Generative AI Paradox in Evaluation: \u201cWhat It Can Solve, It May Not Evaluate\u201d",
            "text": "Models do not base their evaluation on how they solved the generation task. In cases where the Evaluators grade the SOLVED answers generated by themselves, GPT-4 marks approximately 7.7% of its own answers as non-correct (\"incorrect\", \"partially correct\", or \"I don't know\"). GPT-3.5 does so for 18% of its answers (including 141 instances of \"I don't know\") and PaLM-2 marks about 4% of its answers as non-correct. This result is consistent with the findings of West et al. (2023); generative models often face difficulties in responding to basic queries regarding the content they have produced themselves. \n\nTable 3 shows how Evaluators rate answers when the Evaluatees correctly SOLVED questions that the Evaluators have previously gotten wrong. The results indicate that even when the Evaluator model responded with an incorrect answer, it often evaluates the answers from Evaluatees as \"correct\" (Case 2 of Figure 1) (True Positives). PaLM-2 exhibits more paradoxical behavior, its recall being the highest among the three Evaluators. Furthermore, a qualitative analysis of cases where the Evaluator model has correctly SOLVED a problem but the Evaluatee provides a wrong answer reveals that all Evaluators sometimes grade the incorrect answers as correct, which seems unfaithful (Case1 of Figure 1). These three cases suggest that models do not necessarily apply their knowledge about their own answers to the question in a consistent manner during evaluation. The high rate at which Evaluator models deem different Evaluatee models' answers as correct, even when those answers differ from the Evaluator's own background knowledge, raises the possibility of a sycophantic grading bias.",
            "score": 0.282065003695532,
            "section_title": "Faithfulness Analysis",
            "char_start_offset": 14393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1692
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.258056640625
        },
        {
            "corpus_id": "270562788",
            "title": "Unveiling the Flaws: Exploring Imperfections in Synthetic Data and Mitigation Strategies for Large Language Models",
            "text": "Despite our substantial efforts, several limitations warrant further consideration.Firstly, while our unlearning-based strategy has shown promise in mitigating the negative effects of synthetic data, it may still cause degradation in specific model capabilities, such as mathematical reasoning.Moreover, its scalability to much larger models remains untested.\n\nAs LLMs continue to grow in size and complexity, the computational efficiency and practical applicability of this strategy require further validation.Additionally, this study primarily focuses on the flaws and mitigation strategies related to Q-A pair synthetic data.Although we have demonstrated the effectiveness of our unlearning strategy on the open-source synthetic dataset U33B, many other forms of synthetic data remain unexplored.Furthermore, the quality of synthetic data generated by GPT-4 used in this study may not fully represent the entire spectrum of synthetic data quality.Different synthetic data generation techniques and tools can produce data with varying degrees of imperfections, potentially impacting the effectiveness of our mitigation strategy.Further investigation into more advanced unlearning techniques is necessary to minimize these side effects.We will continue to refine and enhance our method in future work.",
            "score": 0.2819705556622603,
            "section_title": "Limitations",
            "char_start_offset": 24177,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 83,
                    "end": 294
                },
                {
                    "start": 294,
                    "end": 359
                },
                {
                    "start": 361,
                    "end": 511
                },
                {
                    "start": 511,
                    "end": 628
                },
                {
                    "start": 628,
                    "end": 799
                },
                {
                    "start": 799,
                    "end": 950
                },
                {
                    "start": 950,
                    "end": 1130
                },
                {
                    "start": 1130,
                    "end": 1237
                },
                {
                    "start": 1237,
                    "end": 1302
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1258544921875
        },
        {
            "corpus_id": "270560307",
            "title": "Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary Empirical Study",
            "text": "We investigate the consistency between different versions of evaluation results generated by the GPT-4o evaluator. Here, the single answer grading evaluation results can be used to compare the consistency between different versions. As shown in Figure 4, the bar corresponding to \"2 n \" on the x-axis represents the number of samples with consistent and inconsistent ratings in comparing evaluation results obtained twice using GPT-4o as an evaluator in MSwR. From the results, consistency improves as we increase the number of shots provided as incontext examples during inference. \n\nRecent studies (Wang et al., 2023b;Zheng et al., 2023;Chen et al., 2024) have shown that the performance of GPT-4 as an evaluator is highly in agreement with those of humans. However, both human and LLM evaluators are subject to potential biases (Chen et al., 2024). By analyzing prior studies, we suppose it unnecessary to obtain utterly accurate evaluation results (because this is difficult) by using LLMs as evaluators. It is only required to ensure that the evaluation results are highly consistent multiple times so that the single answer grading of GPT-4o as an evaluator may be effective. From all results in this work, we find that the many-shot ICL examples help the evaluation of LLMs more consistently, which is essential. We consider that the main reason may be that the many-shot in-context examples mitigate the potential biases of the GPT-4o evaluator. \n\nMeanwhile, we also implemented experiments via the prompt template MSoR, but the results show that this regime may be unsuitable in the scenario of acting as an evaluator because the problem of each sample is the same, but the scored questions and answers are different. From the results in Figure 5, it can be seen that the consistency corresponds to the number of appended in-context samples with model-generated rationales but nearly not to the number of in-context examples without model-generated rationales. \n\nIn addition, both the few-shot and many-shot regimes are sensitive to the selection and order of in-context examples. In the experimental setting of this paper, almost no test data will have the same in-context examples.",
            "score": 0.2818932635340198,
            "section_title": "Consistency Evaluation",
            "char_start_offset": 15171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1453
                },
                {
                    "start": 1456,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1969
                },
                {
                    "start": 1972,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2192
                }
            ],
            "ref_mentions": [
                {
                    "start": 620,
                    "end": 639,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251220703125
        },
        {
            "corpus_id": "266117209",
            "title": "Dynamic Debiasing Network for Visual Commonsense Generation",
            "text": "Table.2 presents the outcomes of applying various confounders to the validation and test sets of the VisualCOMET dataset. Initially, we applied a singular type of confounder to all data without using a modality-specific bias model. Subsequently, by incorporating DDNet, we dynamically mitigated bias across data. Across the board, our results indicate that the introduction of confounders signifiantly improves the performance of the base models. Moreover, the use of DDNet proves to be more effective in bias reduction compared to the application of a single type of confounder (e.g., only event, place, or image confounder). This underscores the benefit of customizing the choice of TABLE 2. Results of applying our method to the baseline model on validation and test set of VCG dataset. Evaluation metrics ''B@2'', ''M'', ''C'' denote BLUE2, METEOR, CIDEr, repectively. ''Event conf'', ''Place'', and ''Image conf'' refer to confounders in each respective modality. The results shown are from applying only one type of confounder to all samples in the dataset. For instance, '+Event conf' indicates the application of the event modality confounder to all data, without a modality-specific bias model. confounder based on the unique requirements of each data sample, as opposed to a uniform application of a single confounder across all data. \n\nIn Table 3, we compare DDNet with other state-of-theart methods [3], [4] on validation and test set of VCG dataset. Notably, we can observe that DDNet achieves the highest BLEU2, and CIDEr scores on both validation and test set. Even when DDNet simply fine-tunes the pre-existing GPT-2 architecture applying causal inference, it surpasses KM-BART in performance across both validation and test sets. It's noteworthy that KM-BART requires specialized pretraining for VCG using various external data, whereas DDNet does not.",
            "score": 0.28176360072464196,
            "section_title": "D. QUANTITATIVE RESULTS",
            "char_start_offset": 25360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1344
                },
                {
                    "start": 1347,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1869
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0537109375
        },
        {
            "corpus_id": "271404776",
            "title": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies",
            "text": "Our evaluation approach focuses on assessing models' ability to refuse risky requests across all 314 risk categories. We employ a category-specific judge prompt system, leveraging GPT-4o as both the generation model for the judge prompts of each category and the final judging model for evaluation. While we acknowledge the potential bias introduced by using this one model for generation and evaluation, we conducted a meticulous human alignment study to understand the accuracy of our evaluation results (Appendix B). \n\nIn particular, we implement a three-level scoring system (0, 0.5, and 1) for a refined evaluation compared to binary classifications. A score of 0 is assigned when the model generates detailed instructions or assistance for the risky activity, even with an ethical disclaimer. 0.5 is given for ambiguous responses or those not directly engaging with the prompt without explicitly refusing. \n\n1 is awarded for refusals to generate harmful content or assist with the requested task, regardless of the refusal form. This scoring system provides a more detailed assessment of model behavior, considering both the refusal to generate risky content and the context of each specific risk category. The judge prompts are initially created manually as the three in-context examples following the above structure, then we scaled the category-specific prompts to all the 314 risk categories using GPT-4o with category-specific knowledge and finally checked by humans, with approximately 70% of the final judge prompts being model-generated and 30% human-edited (more details in Appendix F). \n\nTo ensure the quality and reliability of our evaluation system (in particular, to understand the potential bias of dependent on one model as a judge), we validate judge results through human evaluation (Appendix B). Our approach demonstrates strong agreement with human judgments, achieving a Kappa score of 0.86, which significantly outperforms the 0.69 Kappa score reported for using a fixed judge prompt across all categories [53]. In our quantitative study, we use the refusal rate (percentage of scores that are 1) as the primary metric to assess model alignment with safety guidelines. \n\n4 Evaluation and Takeaways",
            "score": 0.28165116343915153,
            "section_title": "Category-Specific Evaluation: AIR-BENCH 2024 Three-level Scoring Autograder",
            "char_start_offset": 11740,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 519
                },
                {
                    "start": 522,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1601
                },
                {
                    "start": 1604,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2195
                },
                {
                    "start": 2198,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 2033,
                    "end": 2037,
                    "matchedPaperCorpusId": "263671523"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1446533203125
        },
        {
            "corpus_id": "267032873",
            "title": "Artificial intelligence model GPT4 narrowly fails simulated radiological protection exam",
            "text": "In conclusion, this evaluation of GPT-3.5 and GPT-4's performance reveals a multifaceted landscape.Both models fell short of the passing threshold, highlighting the ongoing challenges in deploying these AI systems for specific tasks like accurately answering specific radiological protection and health physics questions.However, promising avenues for improvement are evident.\n\nOne such avenue lies in the implementation of tailored strategies to enhance performance, encompassing the use of explicit instructions, in-context learning, and breaking down complex tasks.These strategies, if effectively applied, have the potential to elevate question accuracy, addressing the models' limitations.\n\nMoreover, the exploration of in-context learning presents an intriguing prospect.GPT models, while drawing from publicly available data sources, can benefit from in-context learning to adapt to new tasks more efficiently, potentially mitigating the limitations associated with their knowledge sources.\n\nThe discrepancy in performance between GPT-3.5 and GPT-4 underscores the advantages of technological advancements.GPT-4's superior performance, bolstered by increased parameters and enhanced processing capabilities, demonstrates its potential for tackling intricate scientific concepts and domain-specific challenges.This distinction becomes particularly significant when considering the inclusion of visual content in questions.\n\nParadoxically, the improved intelligence of the GPT4 model does not proportionately correspond to its ability or willingness to follow human directions in the input prompt.While GPT-4 achieves greater overall accuracy, GPT-3.5 excels in correctly formatting answers.This paradox highlights the need for an iterative approach to prompting the GPT4 model to maximise the value of its output.\n\nThe consistency in performance across knowledge domains provides valuable insights, revealing the particular challenges faced by GPT models in handling topics related to measurements, analytical techniques, and instrumentation.These topics demand real-world expertise and access to specialised databases, factors that GPT models inherently lack.In contrast, 'Fundamentals and Procedures' and 'Operations and Procedures' domains capitalise on the strengths of these models, allowing for enhanced accessibility and performance improvement with the transition from GPT-3.5 to GPT-4.",
            "score": 0.2816098362501194,
            "section_title": "Conclusions",
            "char_start_offset": 17546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 99,
                    "end": 321
                },
                {
                    "start": 321,
                    "end": 376
                },
                {
                    "start": 378,
                    "end": 568
                },
                {
                    "start": 568,
                    "end": 694
                },
                {
                    "start": 696,
                    "end": 777
                },
                {
                    "start": 777,
                    "end": 997
                },
                {
                    "start": 999,
                    "end": 1113
                },
                {
                    "start": 1113,
                    "end": 1316
                },
                {
                    "start": 1316,
                    "end": 1428
                },
                {
                    "start": 1430,
                    "end": 1602
                },
                {
                    "start": 1602,
                    "end": 1696
                },
                {
                    "start": 1696,
                    "end": 1819
                },
                {
                    "start": 1821,
                    "end": 2048
                },
                {
                    "start": 2048,
                    "end": 2166
                },
                {
                    "start": 2166,
                    "end": 2400
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037200927734375
        },
        {
            "corpus_id": "259088664",
            "title": "Evaluation of software impact designed for biomedical research: Are we measuring what\u2019s meaningful?",
            "text": "Collecting an exhaustive amount of user data, and then selecting metrics, can add complexity and increase the risk that metrics are selected in a biased manner. To mitigate this, hypothesis-driven metrics can be selected ahead of time based on a specific hypothesis to ultimately evaluate how well the software supports its intended goals [18].",
            "score": 0.28160493923083685,
            "section_title": "Metric selection should be hypothesis driven",
            "char_start_offset": 9032,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 344
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03662109375
        },
        {
            "corpus_id": "270688086",
            "title": "ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models",
            "text": "To facilitate subsequent research, based on InternLM2-7B-Chat (Cai et al., 2024) and using the manually annotated data in this article, we train ESC-RANK. ESC-RANK can score the results of multiple rounds of dialogues of different models to our well-designed dimension. \n\nWe randomly divide the annotated data into a training set, validation set, and test set according to 7:1:2. Compared with the base model and GPT-4, the results are shown in Table 4. \n\nFrom Table 4, it can be observed that ESC-RANK demonstrates the best scoring capability, surpassing GPT-4 by 35 points in terms of accuracy. As human scoring may not always have a clear-cut boundary, a tolerance of one-point error is allowed in scoring which denotes the result of ACC sof t . When considering ACC sof t , ESC-RANK achieves an accuracy rate of over 99%, providing a solution for subsequent automation processes. Interestingly, GPT-4 performs poorly in the dimension of humanoid and human preference scoring. The analysis suggests that GPT-4 assigns higher scores to its own generated content or content similar to its own, which can be easily judged during human evaluation, particularly in formatted outputs such as bullet-point suggestions, where it becomes apparent that the content is machine-generated, leading to a poor score of humanoid and human preference. InternLM2 also has the same problem in human preference behavior, but it performs better in humanoid scoring, which leads to higher performance than GPT-4 in ACC sof t . \n\n4 Related Work",
            "score": 0.28149428281041733,
            "section_title": "ESC-RANK",
            "char_start_offset": 17249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 269
                },
                {
                    "start": 272,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1524
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0823974609375
        },
        {
            "corpus_id": "270562306",
            "title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
            "text": "We are committed to ensuring that GLM-4 operates as a safe, responsible, and unbiased model. In addition to addressing common ethical and fairness concerns, we carefully assess and mitigate potential harms that the model may pose to users in real-world scenarios. Risk Mitigation. We carefully cleaned data in the pre-training stage by removing text containing sensitive keywords and web pages from a pre-defined blacklist. In the alignment phase, we evaluate each training sample for safety and remove any that pose potential risks. Harmlessness is also an important criteria for preference alignment when comparing multiple model outputs. \n\nWe have a red team that constantly challenges the model with tricky questions that tend to cause unsafe answers. We collect all harmful question-answer pairs from GLM-4 and improve them with human annotations for further model alignment. \n\nSafety Evaluation. We evaluate the GLM-4 model on the SafetyBench [56], which assesses each model from 7 dimensions: Ethics and Morality (unethical behaviors), Illegal Activities (basic knowledge of law), Mental Health (adverse impacts on mental health), Offensiveness (offensive behaviors), Physical Health (dangerous behaviors that can cause physical harms), Privacy and Property (privacy breach or property loss), Unfairness and Bias. We evaluate different models on the Chinese subset of SafetyBench, which is created by removing highly sensitive questions that tend to be censored, to mitigate interference from different API safety policies. \n\nTable 10 shows the safety results of GLM-4 and SOTA models. On most dimensions GLM-4 (0520) shows competitive safety performance, and overall it achieves comparable performance with Claude 3 Opus. GLM-4 slightly falls behind the GPT-4 family, especially on the Physical Health dimension, which demands robust common sense knowledge about the physical world to avoid potential risks. More efforts have been put into this direction to develop a more capable and safe GLM model.",
            "score": 0.28142598220411696,
            "section_title": "Safety and Risks",
            "char_start_offset": 27923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2008
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1729736328125
        },
        {
            "corpus_id": "255546116",
            "title": "EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset",
            "text": "We thus present EgoTracks: a large-scale longterm egocentric visual object tracking dataset for training and evaluating long-term trackers. Seeking a realistic challenge, we source videos from Ego4D [28], a large-scale dataset consisting of unscripted, in-the-wild egocentric videos of daily-life activities. The result is a large-scale dataset to evaluate the tracking and re-detection ability of SOT models, with more than 22,028 tracks from 5708 average 6-minute videos. This constitutes the first large-scale dataset for visual ob-ject tracking in egocentric videos in diverse settings, providing a new, significant challenge compared with previous datasets. \n\nWe perform a thorough analysis of our new dataset and its new characteristics relative to prior benchmarks, demonstrating its difficulty and the need for further research to develop trackers capable of handling long-term egocentric vision. Our experiments reveal remaining open problems and insights towards promising directions in egocentric tracking. Leveraging these intuitions, we propose multiple simple yet effective changes, such as adjustment of spatiotemporal priors, egocentric data finetuning, and combining multiple templates. We apply these proposed strategies on a state-of-the-art (SOTA) STARK tracker [79], training a strong tracker dedicated to long-term egocentric tracking: EgoSTARK. We hope Ego-STARK can serve as a strong baseline and facilitate future research. \n\nWe make the following contributions: \n\n1. We present EgoTracks, the first large-scale long-term object tracking dataset with diverse egocentric scenarios. We analyze its uniqueness in terms of evaluating the re-detection performance of trackers. 2. We conduct comprehensive experiments to understand the performance of many state-of-theart trackers on the EgoTracks validation set and observe that due to the biases and evaluation blindspots of existing third-person datasets, they tend to struggle. 3. We perform an analysis of what makes a good tracker for long-form egocentric video. \n\nApplying these learnings to the STARK tracker [79], we produce a strong baseline we call EgoSTARK, which achieves significant improvements (+15% F-score) on EgoTracks. \n\n2 Related work",
            "score": 0.28138301777698216,
            "section_title": "Introduction",
            "char_start_offset": 4256,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 662
                },
                {
                    "start": 665,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1448
                },
                {
                    "start": 1451,
                    "end": 1487
                },
                {
                    "start": 1490,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2037
                },
                {
                    "start": 2040,
                    "end": 2207
                },
                {
                    "start": 2210,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 203,
                    "matchedPaperCorpusId": "238856888"
                },
                {
                    "start": 1282,
                    "end": 1286,
                    "matchedPaperCorpusId": "232428140"
                },
                {
                    "start": 2086,
                    "end": 2090,
                    "matchedPaperCorpusId": "232428140"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03948974609375
        },
        {
            "corpus_id": "265609706",
            "title": "Competition-Level Problems are Effective LLM Evaluators",
            "text": "To further analyze the phenomenon, we compare GPT-4 with DeepSeek-Coder on D1 problems as shown in Figure 7 and Table 4. \n\nIt is noteworthy that while GPT-4 surpasses DeepSeek-Coder in terms of performance on problems that were released prior to September 2021, an unexpected observation is that DeepSeek-Coder exhibits a performance that is on par with GPT-4 when it comes to tackling problems that were released after September 2021. Considering the previous work (Yang et al., 2023;Zhou et al., 2023), although GPT-4 may perform particularly well on some previously seen problems due to its powerful capacity, it cannot be well generalized on unseen programming problems, and its performance is not significantly different from DeepSeek-Coder, which is specifically trained for code. This phenomenon merits attention, which is termed as \"evaluation hallucination\". \n\nHence, a more equitable evaluation strategy would be to select evaluation sets that all the models have not previously encountered. However, finding such data adhering to stringent conditions is challenging, as LLMs are typically pre-trained on extensive corpora containing diverse content, leading to the potential issue of data contamination. Therefore, if we could devote more attention to the data source and timeline of the evaluation sets, such  as the problems in Codeforces, it could potentially mitigate the effects of evaluation hallucination.",
            "score": 0.28114723638137407,
            "section_title": "Evaluation Hallucination of LLMs",
            "char_start_offset": 19328,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 123,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1423
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10247802734375
        },
        {
            "corpus_id": "274165887",
            "title": "Learning from \"Silly\" Questions Improves Large Language Models, But Only Slightly",
            "text": "Our findings indicate that none of the mixing strategies surpass the optimal results achieved by using a single rule-based augmentation method. \n\nOverall, using Ruozhiba's rules for data augmentation on the MMLU dataset has not resulted in significant improvements. We believe the reasons are mainly as follows: \n\n1. As shown in the task-level analysis results in the Appendix F.1, the tasks in MMLU vary greatly, encompassing different domains of knowledge (such as natural sciences and social sciences) and different types of tasks (such as knowledge-based questions and logical reasoning). Improvements in some tasks may be offset by declines in others. \n\n2. As shown in the in-context examples in Appendix D.1, Ruozhiba's data combines human knowledge with linguistic ingenuity, and not all texts can be rewritten to achieve such clever expression. In the samples generated using the rules provided in Appendix F, it is evident that GPT-4's rewrites primarily focus on stylistic changes. However, it remains challenging to mimic the nuanced expressions found in the in-context examples. \n\n3. Bai et al. [2024] used the BELLE-EVAL benchmark, where GPT serves as the judge. Within the COIG-CQIA dataset, Ruozhiba is unique for employing GPT in data filtering and response annotation. This is significant because LLM-as-a-Judge can exhibit self-enhancement bias [Zheng et al., 2023], potentially favoring its own outputs and inflating Ruozhiba's performance. To mitigate this, we calculated the accuracy of multiple-choice questions and had GPT-4 rewrite the questions while keeping the options and responses unchanged, effectively removing this bias. This might explain why the Ruozhiba dataset didn't show significant improvement in our experiments.",
            "score": 0.2809360961817481,
            "section_title": "General Results",
            "char_start_offset": 10935,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 146,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 311
                },
                {
                    "start": 314,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 656
                },
                {
                    "start": 659,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1090
                },
                {
                    "start": 1093,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1752
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.156494140625
        },
        {
            "corpus_id": "263310448",
            "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
            "text": "We categorize biases as \"implicit\" if they can be witnessed without including any additional information other than instructing the model to judge the quality of two given generated texts. \n\nOrder Bias is an evaluation bias we observe when a model tends to favor the model based on the order of the responses rather than their content quality. Order bias has been extensively studied (Jung et al., 2019;Wang et al., 2023a;Zheng et al., 2023), and it is well-known that language models can be influenced by the ordering of the responses in their evaluations. We prompt both orderings of each pair and count the evaluation as a \"first order\" or \"last order\" bias if the evaluator chooses the first ordered (or last ordered) output in both arrangements respectively. \n\nCompassion Fade (Naming). (Butts et al., 2019;V\u00e4stfj\u00e4ll et al., 2014) is a cognitive bias that denotes a decrease in empathy as the number of identifiable individuals increases. To this phenomenon, we modify the definition for our use case to measure whether model evaluations are affected by real/identifiable names as opposed to evaluations with anonymous aliases (e.g. System A). Specifically, an unbiased evaluator would make evaluations similar to when anonymized names were presented. \n\nEgocentric Bias (Self-Preference). (Ross and Sicoly, 1979) is a cognitive bias that refers to the tendency to have a higher opinion of oneself or to more easily accept ideas if they match one's own. We define an evaluator to be egocentrically biased if, for each instance, the evaluator prefers its own response over others. We note that an unbiased evaluator would choose between themselves and other comparand models equally in proportion. However, we highlight that some models would naturally generate higher quality responses (e.g., GPT4 vs. KOALA), resulting in a stronger inclination for such evaluators to choose their own responses. \n\nSalience Bias (Length). (Schenk, 2010;Zheng et al., 2023) The evaluator tends to favor responses that are either shorter or longer in length.",
            "score": 0.28091338872220295,
            "section_title": "Implicit Biases",
            "char_start_offset": 9228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 191,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1256
                },
                {
                    "start": 1259,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1900
                },
                {
                    "start": 1903,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2044
                }
            ],
            "ref_mentions": [
                {
                    "start": 812,
                    "end": 835,
                    "matchedPaperCorpusId": "9485688"
                },
                {
                    "start": 1294,
                    "end": 1316,
                    "matchedPaperCorpusId": "37562229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56982421875
        },
        {
            "corpus_id": "276482728",
            "title": "Investigating Non-Transitivity in LLM-as-a-Judge",
            "text": "We observe the same pattern from the table 3 as in the main text, which is with the threshold for ties. When GPT-4-Turbo serves as the judge, both PNT and SNTD increase as the performance gap between any pair of models, (m A , m B ) or (m B , m C ), decreases. In cases where all three models exhibit similar performance, such as in scenario MM, the incidence of non-transitivity rises significantly. We attribute this to the increased uncertainty judges face when assessing quality differences between similar outputs. When the comparisons between m A and m B , m B and m C , and m A and m C are all uncertain, non-transitivity reaches its highest level. Replicating our evaluation with GPT-3.5-Turbo as judge reveals an intriguing pattern: while the PNT remains minimal across scenarios, the consistently high SNTD values indicate substantial non-transitivity. This observation motivates us to define the tie threshold, as ties can serve as an indicator of model uncertainty. \n\nTo explain the low number of hard non-transitive cases when using GPT-3.5-Turbo as the judge with position switching in Figure 3, we hypothesize that GPT-3.5-Turbo is also affected by other biases, such as verbosity bias (Saito et al., 2023) and token bias (Alzahrani et al., 2024). Since GPT-3.5-Turbo struggles to accurately assess the quality of outputs, these combined biases influence the judge's preferences. As a result, even though position switching mitigates the position bias, the averaged preference is still not determined by the actual quality of the outputs but rather by other fixed biases in the prompt, leading to transitive preferences. This observation also motivates us to define the threshold, as it can be used to reduce the impact of other biases. Figure 7. Proportion of (non-)transitive instructions across all scenarios (without the threshold of ties), as evaluated by GPT-4-Turbo and GPT-3.5-Turbo. When evaluating model triplets with GPT-3.5-Turbo as judge, over 96% of instructions exhibit position bias effects.",
            "score": 0.2808897249563591,
            "section_title": "B.2. Results with Preferences without the Threshold of Ties",
            "char_start_offset": 29550,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 1237,
                    "end": 1261,
                    "matchedPaperCorpusId": "267412932"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1190185546875
        },
        {
            "corpus_id": "260682715",
            "title": "SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs",
            "text": "First, inconsistent results were observed when asking for a rating directly and when soliciting \"step-by-step\" thinking in the Chain-of-thought prompting style. Second, limited resources constrained us to run only 100 visual question-answering prompts for GPT-4 evaluations (for context, the authors of LLaVa used only 30 samples for their quantitative evaluation [9]. This hindered conclusive comparisons between the models. Third, the non-deterministic property of GPT-4 evaluation could render subsequent work on SciGraphQA incomparable with our published results. Therefore, we reported only the CIDEr, ROUGE, and BLEU-4 scores, which are freely accessible, open, and reproducible. We would like to stress that further exploration of GPT-4 as an evaluation tool is a worthwhile endeavor and could potentially be integrated as a reward model for reinforcement learning. \n\nAdapting MLLMs to handle out-of-distribution data, particularly in scientific graph question-answering is a significant challenge given the costly and complex nature of fine-tuning large foundation models, their sensitivity to hyperparameters and input data quality, and more importantly the risk of catastrophic forgetting. We explored the use of LoRA to mitigate the fine-tuning cost and further used prompt augmentation to alleviate the remaining issues.",
            "score": 0.2807758011107049,
            "section_title": "Discussion",
            "char_start_offset": 25206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1332
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.050811767578125
        },
        {
            "corpus_id": "267626820",
            "title": "AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension",
            "text": "Subsequently, we present questions and employ GPT-4 to evaluate the hypotheses produced by LALMs. To ensure consistency and fairness for evaluation, each model's answer is compared against the same reference answer for scoring. For the foundation benchmark, the reference answer is the golden choice, and we prompt the evaluator to determine whether the hypothesis is correct or not. For the chat benchmark, the reference answer is generated by GPT-4, and we prompt the evaluator to provide a score ranging from 1 to 10, based on the assessment of usefulness, relevance, accuracy, and comprehensiveness of the hypothesis. The prompts used in the evaluation process can be found in Appendix 5. Note that for the chat benchmark, the role of the reference is not to serve as the ground truth answer, but rather as a reference for scoring by GPT-4, in order to stabilize its scoring. \n\nAdditionally, to mitigate any potential position bias resulting from the order of hypothesis and reference, following Bai et al. (2023b), we perform a second scoring round by swapping their positions and then compute the average of the two scores. Unless otherwise specified, the GPT-4 evaluator is GPT-4 Turbo, the gpt-4-0125-preview version2 .",
            "score": 0.2801864008524675,
            "section_title": "Evaluation Strategy",
            "char_start_offset": 20137,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1227
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.255859375
        },
        {
            "corpus_id": "255372475",
            "title": "STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos",
            "text": "In Table 13, we present the effect of various components of our approach on four egocentric datasets. We see that our proposed losses brings an improvement to the final performance in most cases.",
            "score": 0.2796579529767513,
            "section_title": "E.4. Additional results on loss ablation",
            "char_start_offset": 43891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 195
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0567626953125
        },
        {
            "corpus_id": "267750144",
            "title": "Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation",
            "text": "In previous experiments, we used GPT-4 as the one of the evaluation tool. Although GPT-4 has been widely adopted as an evaluative instrument in numerous studies and its effectiveness has been thoroughly validated (Yang et al., 2023;Ji et al., 2023), we still conduct a meticulous examination of GPT-4's assessment accuracy. Specifically, we compare the evaluation results of GPT-4 with those of human annotated results. As shown in  . \n\nconsistency between GPT-4 and human-annotated results. For the original dataset, the inherent preference relations serve as the human annotations. For the self-generated dataset, we selected 1000 samples for human annotation, with detailed human annotation guidelines provided in Appendix J. \n\nAs shown in Table 4,GPT-4's evaluation results show 86.3% consistency on average with human annotations, significantly exceeding that of Llama2-70B. This indicates that GPT-4's evaluation accuracy is high. According to our case study (Appendix H), some cases are challenging to judge. However, GPT-4's evaluations are highly accurate in clear-cut cases. This underscores the reliability of GPT-4 as an evaluation tool.",
            "score": 0.2796579529767513,
            "section_title": "Human Evaluation",
            "char_start_offset": 24672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1149
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.107421875
        },
        {
            "corpus_id": "269502145",
            "title": "Fake Artificial Intelligence Generated Contents (FAIGC): A Survey of Theories, Detection Methods, and Opportunities",
            "text": "The influence of data on models predominantly manifests in the form of data biases, which impede the model's proficiency in comprehending visual information and executing tasks in diverse settings.For example, in the context of fact-checking Q&A pairs, a particular option may dominate the majority [139].LVLMs trained on such biased datasets tend to favor responses aligning with the predominant option.Additionally, given the substantial costs associated with manually annotated data, some studies have utilized Data Generation Models (DGMs) to produce images for model training.However, this approach exacerbates the tendency of LVLMs to generate erroneous interpretations or 'hallucinations' [69].Furthermore, research indicates [48] that GPT-4 variants, such as GPT-4V, demonstrate a stronger aptitude for interpreting images that are Western or contain English text.This disparity is indicative of the impact of data biases on model performance.These instances collectively highlight how training data imbalances can lead to misconceptions or 'hallucinations' in models.",
            "score": 0.2796579529767513,
            "section_title": "Data.",
            "char_start_offset": 49384,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 305
                },
                {
                    "start": 305,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 581
                },
                {
                    "start": 581,
                    "end": 701
                },
                {
                    "start": 701,
                    "end": 872
                },
                {
                    "start": 872,
                    "end": 951
                },
                {
                    "start": 951,
                    "end": 1076
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07598876953125
        },
        {
            "corpus_id": "257435216",
            "title": "IoT and Deep Learning-Based Farmer Safety System",
            "text": "We evaluated the approaches on both large-scale egocentric datasets and farming pack mocap datasets, and demonstrate that our proposed method is practical regarding computational efficiency and result.",
            "score": 0.2796579529767513,
            "section_title": "2.",
            "char_start_offset": 6408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00536346435546875
        },
        {
            "corpus_id": "265403156",
            "title": "Exploring the neural basis and modulating factors of implicit altercentric spatial perspective-taking with fNIRS",
            "text": "Participants who had proportionally more egocentric-labeled trials than altercentric-labeled trials overall were categorized into the egocentric group (n = 73), and vice versa for the altercentric group (n = 44). Trials with RTs deviating more than 3 standard deviations (SD) from a participant's overall mean were excluded from further analyses (2.17% of observations from the egocentric group, 1.52% from the altercentric group). The two groups did not differ in proportion of eliminated trials, \u03c7 2 (1) = 1.493, p = 0.222.",
            "score": 0.2796579529767513,
            "section_title": "Behavioral data",
            "char_start_offset": 21196,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 525
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0162811279296875
        },
        {
            "corpus_id": "275213467",
            "title": "Evidence that altercentric biases in a continuous false belief task depend on highlighting the agent's belief",
            "text": "This confirmed the evidence against an egocentric bias in the entire data set (BF 10 = 0.04, W = 1641, R = 1, M = \u2212 7.56, SD = 36.81) as well as in all groups (with inconclusive evidence in the implicit and true belief first group, details see Appendix D, Table D.2. There was moderate evidence against an effect of gender (Bayesian Mann Whitney U Test: BF 10 = 0.23, W = 950, R = 1, N male = 30, N female = 64, M male = 4.14, M female = 5.51, SD male = 56.87, SD female = 66.38). \n\nFinally, as for the implicit task, we analyzed the effects of the different hiding scenarios with two separate Bayesian repeated measures ANOVAs, one with the factor agent and one with the factor location. These yielded moderate evidence for an effect of agent (BF 10 = 3.51, error = 3.27 %) and moderate evidence against an effect of location (BF 10 = 0.32, error = 1.32 %). Pairwise post-hoc analyses of the different agents against each other showed evidence that the egocentric bias was smaller for the squirrel than for the seagull (BF 10 = 4.03, W = 1315, R = 1.02, M squirrel = \u2212 14.96, M seagull = 38.49, SD squirrel = 92.30, SD seagull = 152.83). The other pairwise comparisons between the agents yielded evidence against a difference or remained inconclusive (details see Appendix E, Table E.1). To follow-up on this effect, we additionally examined the agents in separate Wilcoxon signed rank tests. This revealed strong evidence against an egocentric bias in trials with the squirrel, the boy, or the girl as agent (squirrel: BF 10 = 0.04, W = 1475, R = 1, N = 94, M = \u2212 16.17",
            "score": 0.2796579529767513,
            "section_title": "Explicit task",
            "char_start_offset": 26893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1571
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.060211181640625
        },
        {
            "corpus_id": "271500318",
            "title": "Advancing radiology with GPT-4: Innovations in clinical applications, patient engagement, research, and learning",
            "text": "As discussed in this article Large Language Models like GPT-4 have a lot of promise in transforming radiology for the better. However, none of these models are currently approved as regulated medical devices. Their use will need close oversight, review and validation by radiologists for every use case. \n\nA significant technical limitation for GPT-4 is the phenomenon of \"hallucinations,\" where it generates convincing but inaccurate or fabricated information. One of the causes of hallucinations could be intrinsic bias or deficiencies in the data used for training these models [25]. The plausible sounding but inaccurate information generated by hallucinations may lead to inappropriate clinical decisions with serious adverse implications for the patients [26] Data privacy emerges as another paramount concern, especially given the sensitive nature of medical records; ensuring GPT-4's compliance with stringent data protection regulations like HIPAA is essential to safeguard patient confidentiality. Ethical issues also abound, particularly in the realm of patient consent and the transparency of AIdriven decisions. Patients must be adequately informed about the role of AI in their care, including the potential for errors and the measures taken to mitigate such risks.",
            "score": 0.27956812665320385,
            "section_title": "Pitfalls, drawbacks, limitations and ethical considerations",
            "char_start_offset": 13240,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 303
                },
                {
                    "start": 306,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1279
                }
            ],
            "ref_mentions": [
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "267225790"
                },
                {
                    "start": 761,
                    "end": 765,
                    "matchedPaperCorpusId": "259143161"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.228271484375
        },
        {
            "corpus_id": "247939617",
            "title": "In Rain or Shine: Understanding and Overcoming Dataset Bias for Improving Robustness Against Weather Corruptions for Autonomous Vehicles",
            "text": "One of the primary challenges posed to the implementation of ethical AI is bias. Bias is generally introduced in AI systems through data, people and model design. Data bias has been the reason for various issues with the implementation of ethical and fair AI (Torralba and Efros 2011; Khosla et al. 2013). It generates outcomes that are discriminatory and affect the opportunities and resource allocation for under-represented demographic groups. However, when it comes to high-risk applications such as autonomous driving, it is not just about fairness and ethics but can pose an even greater challenge specific to the safety of the surroundings and the AV itself. \n\nAs AVs deal with changing driving conditions and unpredictable scenarios, there is scope for many forms of bias to creep its way into the model predictions. Specifically, if the training data is only specific to certain conditions, then the model is incapable of making accurate decisions in the real world. In particular, several unique forms of bias have been identified in autonomous driving systems by previous works and novel methods to mitigate them were proposed as well (Jo et al. 2013;Bando et al. 2011;Danks and London 2017). One form of bias identified was the GPS error bias and was proposed to be corrected using additional information sources like camera vision systems (Jo et al. 2013). Kalman filters were shown to have good results in estimating sensor bias in AVs, which is useful for estimating altitude correctly (Bando et al. 2011). Multiple possible sources of algorithmic bias were discussed from the perspective of autonomous systems, for e.g., the performance of a model trained on data from a certain city when used in another city (Danks and London 2017). \n\nHowever, there is no study that specifically considers these benchmark datasets for OD to understand the bias in them. In this work, we propose the framework to identify, measure and mitigate such bias in these benchmark datasets.",
            "score": 0.2792977936457069,
            "section_title": "b. Bias",
            "char_start_offset": 6707,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1750
                },
                {
                    "start": 1753,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 1983
                }
            ],
            "ref_mentions": [
                {
                    "start": 1146,
                    "end": 1162,
                    "matchedPaperCorpusId": "16401685"
                },
                {
                    "start": 1162,
                    "end": 1180,
                    "matchedPaperCorpusId": "17357775"
                },
                {
                    "start": 1180,
                    "end": 1202,
                    "matchedPaperCorpusId": "33799296"
                },
                {
                    "start": 1352,
                    "end": 1368,
                    "matchedPaperCorpusId": "16401685"
                },
                {
                    "start": 1501,
                    "end": 1520,
                    "matchedPaperCorpusId": "17357775"
                },
                {
                    "start": 1726,
                    "end": 1749,
                    "matchedPaperCorpusId": "33799296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046539306640625
        },
        {
            "corpus_id": "262824773",
            "title": "Bias Testing and Mitigation in LLM-based Code Generation",
            "text": "Our manual analysis confirms that the bias testing procedure we designed is reliable in detecting bias from the code snippets, e.g., the precision of automated bias testing is 100%. \n\nInspired by the recent works [23][24][25][26][27][28][29][30][31] that uses few-shot learning and Chain-of-Thought to tackle complex challenges, we also conduct an empirical study of five bias mitigation strategies (i.e., zero-shot, one-shot, few-shot learning, and two Chain-of-Though) to mitigate bias from the code generation procedure and mitigate bias from already generated code snippets. Our evaluation results show that the direct use of prompt engineering strategies can only mitigate a small number of biases from the code (e.g., the overall CBS of GPT-4 decreases from 59.88% to 36.23% for zero-shot prompting). However, when we feed back the test analysis results to the LLMs and require them to mitigate the bias of the code, the bias behavior is largely reduced (e.g., the overall CBS of GPT-4 decreases from 59.88% to 10.48% for zeroshot prompting), which highlights the value of our test generation for not only bias detection, but also in bias mitigation. \n\nIn summary, this paper makes the following contributions: \n\n\u2022 We propose a novel code bias evaluation framework (as shown in Fig. 3) specifically designed for code generation models. This framework incorporates three code bias metrics (i.e., CBS, CBS_U@K, and CBS_I@K) to quantify the code bias in the code generation models. \u2022 Using our evaluation framework, we conduct an empirical study to comprehensively investigate and analyze the fairness of five state-of-the-art LLMs in code generation. Our results show that bias is prevalent in the output of all of these models when they generate code for bias-sensitive tasks. \n\n\u2022 We evaluate a series of widely studied prompt engineering strategies to check whether these strategies can reduce bias from the code. Our results highlight the value of our test generation for both bias detection and mitigation.",
            "score": 0.2790467494726986,
            "section_title": "Previous Ours",
            "char_start_offset": 5572,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 184,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1216
                },
                {
                    "start": 1219,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1781
                },
                {
                    "start": 1784,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 217,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51318359375
        },
        {
            "corpus_id": "270764411",
            "title": "DataGen: Unified Synthetic Dataset Generation via Large Language Models",
            "text": "The deployment of LLM-generated datasets, while beneficial in many contexts, carries potential negative societal impacts that warrant careful consideration. One significant concern is the propagation of biases present in the training data of the LLMs. If not adequately addressed, these biases can be reflected and even amplified in the generated datasets, leading to unfair or discriminatory outcomes in applications that utilize these datasets. Moreover, the use of synthetic data might reduce the diversity of perspectives if it over-relies on patterns learned from existing datasets, potentially overlooking minority viewpoints and underrepresented voices. To mitigate these risks, it is crucial to implement robust bias detection and correction mechanisms, enforce strict validation processes, and promote the ethical use of synthetic data in all applications.",
            "score": 0.2788838446551986,
            "section_title": "F Potential Negative Societal Impacts",
            "char_start_offset": 47665,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 865
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40380859375
        },
        {
            "corpus_id": "270559897",
            "title": "GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation",
            "text": "As mentioned in Section \u00a73, our pipeline will take original data D as input and output generated data D. We aim to generate easier data than the original one to facilitate model learning of basic geometric knowledge.This section demonstrates the efficacy of our pipeline by comparing the difficulty levels of D and D.\n\nWe initiate this by forming a data pair P 1 = {D, D} and utilize GPT-4V to assess the relative difficulty of the data points.To mitigate the bias that GPT-4V may have due to the presentation order, we also consider the pair P 2 = { D, D}, obtained by swapping the order of the data points.If GPT-4V produces different outputs based on P 1 and P 2 , we conclude that the difficulty of D and D is equal.A detailed prompt can be found in Appendix C.4.\n\nIn practice, we randomly sample 500 pairs of generated and corresponding original data points.The outcome, presented in Figure 2a, reveals that over 80% of the questions in the generated dataset are of equal or lesser difficulty compared to the original questions.This indicates that our pipeline is successful in generating data that is simpler than the original dataset.",
            "score": 0.27856933106419945,
            "section_title": "Difficulty Evaluation",
            "char_start_offset": 12852,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 317
                },
                {
                    "start": 319,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 608
                },
                {
                    "start": 608,
                    "end": 720
                },
                {
                    "start": 720,
                    "end": 767
                },
                {
                    "start": 769,
                    "end": 863
                },
                {
                    "start": 863,
                    "end": 1033
                },
                {
                    "start": 1033,
                    "end": 1141
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040985107421875
        },
        {
            "paperId": "4296f5e2c43930428d9a31dce9061a4ef488f674",
            "corpusId": 270717083,
            "title": "Artificial Intelligence Tools and Bias in Journalism-related Content Generation: Comparison Between Chat GPT-3.5, GPT-4 and Bing",
            "venue": "Tripodos",
            "year": 2024,
            "referenceCount": 46,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://tripodos.com/index.php/Facultat_Comunicacio_Blanquerna/article/download/1092/1303",
                "status": "GOLD",
                "license": "CCBYNCSA",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.51698/tripodos.2024.55.06?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.51698/tripodos.2024.55.06, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308148861",
                    "name": "Mar Castillo-Campos"
                },
                {
                    "authorId": "2308148211",
                    "name": "David Varona-Aramburu"
                },
                {
                    "authorId": "1402702795",
                    "name": "D. Becerra-Alonso"
                }
            ],
            "abstract": "This study explores the biases present in artificial intelligence (AI) tools, focusing on GPT-3.5, GPT-4, and Bing. The performance of the tools has been compared with a group of experts in linguistics, and journalists specialized in breaking news and international affairs. It reveals that GPT-3.5, widely accessible and free, exhibits a higher tendency rate in its word generation, suggesting an intrinsic bias within the tool itself rather than in the input data. Comparatively, GPT-4 and Bing demonstrate differing patterns in term generation and subjectivity, with GPT-4 aligning more closely with expert opinions and producing fewer opinative words. The research highlights the extensive use of generative AI in media and among the general populace, emphasizing the need for careful reliance on AI-generated content. The findings stress the risks of misinformation and biased reporting inherent in unexamined AI outputs. The challenge for journalists and information professionals is to ensure accuracy and ethical judgment in content creation to maintain the quality and diversity of content in journalistic practices.",
            "corpus_id": "270717083",
            "text": "This study explores the biases present in artificial intelligence (AI) tools, focusing on GPT-3.5, GPT-4, and Bing. The performance of the tools has been compared with a group of experts in linguistics, and journalists specialized in breaking news and international affairs. It reveals that GPT-3.5, widely accessible and free, exhibits a higher tendency rate in its word generation, suggesting an intrinsic bias within the tool itself rather than in the input data. Comparatively, GPT-4 and Bing demonstrate differing patterns in term generation and subjectivity, with GPT-4 aligning more closely with expert opinions and producing fewer opinative words. The research highlights the extensive use of generative AI in media and among the general populace, emphasizing the need for careful reliance on AI-generated content. The findings stress the risks of misinformation and biased reporting inherent in unexamined AI outputs. The challenge for journalists and information professionals is to ensure accuracy and ethical judgment in content creation to maintain the quality and diversity of content in journalistic practices.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.15087890625
        },
        {
            "paperId": "be0d3e87cdcca8424cce920330853ca6896b6b30",
            "corpusId": 275336149,
            "title": "Examining the Robustness of Homogeneity Bias to Hyperparameter Adjustments in GPT-4",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.02211, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2278586247",
                    "name": "Messi H.J. Lee"
                }
            ],
            "abstract": "Vision-Language Models trained on massive collections of human-generated data often reproduce and amplify societal stereotypes. One critical form of stereotyping reproduced by these models is homogeneity bias-the tendency to represent certain groups as more homogeneous than others. We investigate how this bias responds to hyperparameter adjustments in GPT-4, specifically examining sampling temperature and top p which control the randomness of model outputs. By generating stories about individuals from different racial and gender groups and comparing their similarities using vector representations, we assess both bias robustness and its relationship with hyperparameter values. We find that (1) homogeneity bias persists across most hyperparameter configurations, with Black Americans and women being represented more homogeneously than White Americans and men, (2) the relationship between hyperparameters and group representations shows unexpected non-linear patterns, particularly at extreme values, and (3) hyperparameter adjustments affect racial and gender homogeneity bias differently-while increasing temperature or decreasing top p can reduce racial homogeneity bias, these changes show different effects on gender homogeneity bias. Our findings suggest that while hyperparameter tuning may mitigate certain biases to some extent, it cannot serve as a universal solution for addressing homogeneity bias across different social group dimensions.",
            "corpus_id": "275336149",
            "text": "Vision-Language Models trained on massive collections of human-generated data often reproduce and amplify societal stereotypes. One critical form of stereotyping reproduced by these models is homogeneity bias-the tendency to represent certain groups as more homogeneous than others. We investigate how this bias responds to hyperparameter adjustments in GPT-4, specifically examining sampling temperature and top p which control the randomness of model outputs. By generating stories about individuals from different racial and gender groups and comparing their similarities using vector representations, we assess both bias robustness and its relationship with hyperparameter values. We find that (1) homogeneity bias persists across most hyperparameter configurations, with Black Americans and women being represented more homogeneously than White Americans and men, (2) the relationship between hyperparameters and group representations shows unexpected non-linear patterns, particularly at extreme values, and (3) hyperparameter adjustments affect racial and gender homogeneity bias differently-while increasing temperature or decreasing top p can reduce racial homogeneity bias, these changes show different effects on gender homogeneity bias. Our findings suggest that while hyperparameter tuning may mitigate certain biases to some extent, it cannot serve as a universal solution for addressing homogeneity bias across different social group dimensions.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.223876953125
        },
        {
            "paperId": "e5ffb49b472c8638acc885904c72dceb1643ec2c",
            "corpusId": 269626718,
            "title": "Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge",
            "venue": "Annual Conference on Innovation and Technology in Computer Science Education",
            "year": 2024,
            "referenceCount": 49,
            "citationCount": 23,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2405.05253",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.05253, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115139304",
                    "name": "Charles Koutcheme"
                },
                {
                    "authorId": "2278655212",
                    "name": "Nicola Dainese"
                },
                {
                    "authorId": "1571527353",
                    "name": "Sami Sarsa"
                },
                {
                    "authorId": "3446322",
                    "name": "Arto Hellas"
                },
                {
                    "authorId": "34690956",
                    "name": "Juho Leinonen"
                },
                {
                    "authorId": "2243041721",
                    "name": "Paul Denny"
                }
            ],
            "abstract": "Large language models (LLMs) have shown great potential for the automatic generation of feedback in a wide range of computing contexts. However, concerns have been voiced around the privacy and ethical implications of sending student work to proprietary models. This has sparked considerable interest in the use of open source LLMs in education, but the quality of the feedback that such open models can produce remains understudied. This is a concern as providing flawed or misleading generated feedback could be detrimental to student learning. Inspired by recent work that has utilised very powerful LLMs, such as GPT-4, to evaluate the outputs produced by less powerful models, we conduct an automated analysis of the quality of the feedback produced by several open source models using a dataset from an introductory programming course. First, we investigate the viability of employing GPT-4 as an automated evaluator by comparing its evaluations with those of a human expert. We observe that GPT-4 demonstrates a bias toward positively rating feedback while exhibiting moderate agreement with human raters, showcasing its potential as a feedback evaluator. Second, we explore the quality of feedback generated by several leading open-source LLMs by using GPT-4 to evaluate the feedback. We find that some models offer competitive performance with popular proprietary LLMs, such as ChatGPT, indicating opportunities for their responsible use in educational settings.",
            "corpus_id": "269626718",
            "text": "Large language models (LLMs) have shown great potential for the automatic generation of feedback in a wide range of computing contexts. However, concerns have been voiced around the privacy and ethical implications of sending student work to proprietary models. This has sparked considerable interest in the use of open source LLMs in education, but the quality of the feedback that such open models can produce remains understudied. This is a concern as providing flawed or misleading generated feedback could be detrimental to student learning. Inspired by recent work that has utilised very powerful LLMs, such as GPT-4, to evaluate the outputs produced by less powerful models, we conduct an automated analysis of the quality of the feedback produced by several open source models using a dataset from an introductory programming course. First, we investigate the viability of employing GPT-4 as an automated evaluator by comparing its evaluations with those of a human expert. We observe that GPT-4 demonstrates a bias toward positively rating feedback while exhibiting moderate agreement with human raters, showcasing its potential as a feedback evaluator. Second, we explore the quality of feedback generated by several leading open-source LLMs by using GPT-4 to evaluate the feedback. We find that some models offer competitive performance with popular proprietary LLMs, such as ChatGPT, indicating opportunities for their responsible use in educational settings.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.08447265625
        },
        {
            "paperId": "edccf36e3e90bc8e33dc178cfa83a0c9dd66c6af",
            "corpusId": 267759866,
            "title": "Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 14,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.12738, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275259829",
                    "name": "Michimasa Inaba"
                },
                {
                    "authorId": "2284761737",
                    "name": "Mariko Ukiyo"
                },
                {
                    "authorId": "2284761578",
                    "name": "Keiko Takamizo"
                }
            ],
            "abstract": "Mental health care poses an increasingly serious challenge to modern societies. In this context, there has been a surge in research that utilizes information technologies to address mental health problems, including those aiming to develop counseling dialogue systems. However, there is a need for more evaluations of the performance of counseling dialogue systems that use large language models. For this study, we collected counseling dialogue data via role-playing scenarios involving expert counselors, and the utterances were annotated with the intentions of the counselors. To determine the feasibility of a dialogue system in real-world counseling scenarios, third-party counselors evaluated the appropriateness of responses from human counselors and those generated by GPT-4 in identical contexts in role-play dialogue data. Analysis of the evaluation results showed that the responses generated by GPT-4 were competitive with those of human counselors.",
            "corpus_id": "267759866",
            "text": "Mental health care poses an increasingly serious challenge to modern societies. In this context, there has been a surge in research that utilizes information technologies to address mental health problems, including those aiming to develop counseling dialogue systems. However, there is a need for more evaluations of the performance of counseling dialogue systems that use large language models. For this study, we collected counseling dialogue data via role-playing scenarios involving expert counselors, and the utterances were annotated with the intentions of the counselors. To determine the feasibility of a dialogue system in real-world counseling scenarios, third-party counselors evaluated the appropriateness of responses from human counselors and those generated by GPT-4 in identical contexts in role-play dialogue data. Analysis of the evaluation results showed that the responses generated by GPT-4 were competitive with those of human counselors.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0738525390625
        },
        {
            "paperId": "56e583d66c7bbe3e64d2cec8f7ae47bbdd5c8daf",
            "corpusId": 277631115,
            "title": "Novel AI applications in systematic review: GPT-4 assisted data extraction, analysis, review of bias",
            "venue": "BMJ evidence-based medicine",
            "year": 2025,
            "referenceCount": 49,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1136/bmjebm-2024-113066?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1136/bmjebm-2024-113066, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152673440",
                    "name": "J. Kim"
                },
                {
                    "authorId": "2065640672",
                    "name": "M. Chua"
                },
                {
                    "authorId": "2354599432",
                    "name": "Tian Ge Li"
                },
                {
                    "authorId": "49663958",
                    "name": "M. Rickard"
                },
                {
                    "authorId": "2350001449",
                    "name": "Armando J. Lorenzo"
                }
            ],
            "abstract": "Objective To assess custom GPT-4 performance in extracting and evaluating data from medical literature to assist in the systematic review (SR) process. Design A proof-of-concept comparative study was conducted to assess the accuracy and precision of custom GPT-4 models against human-performed reviews of randomised controlled trials (RCTs). Setting Four custom GPT-4 models were developed, each specialising in one of the following areas: (1) extraction of study characteristics, (2) extraction of outcomes, (3) extraction of bias assessment domains and (4) evaluation of risk of bias using results from the third GPT-4 model. Model outputs were compared against data from four SRs conducted by human authors. The evaluation focused on accuracy in data extraction, precision in replicating outcomes and agreement levels in risk of bias assessments. Participants Among four SRs chosen, 43 studies were retrieved for data extraction evaluation. Additionally, 17 RCTs were selected for comparison of risk of bias assessments, where both human comparator SRs and an analogous SR provided assessments for comparison. Intervention Custom GPT-4 models were deployed to extract data and evaluate risk of bias from selected studies, and their outputs were compared to those generated by human reviewers. Main outcome measures Concordance rates between GPT-4 outputs and human-performed SRs in data extraction, effect size comparability and inter/intra-rater agreement in risk of bias assessments. Results When comparing the automatically extracted data to the first table of study characteristics from the published review, GPT-4 showed 88.6% concordance with the original review, with <5% discrepancies due to inaccuracies or omissions. It exceeded human accuracy in 2.5% of instances. Study outcomes were extracted and pooling of results showed comparable effect sizes to comparator SRs. A review of bias assessment using GPT-4 showed fair-moderate but significant intra-rater agreement (ICC=0.518, p<0.001) and inter-rater agreements between human comparator SR (weighted kappa=0.237) and the analogous SR (weighted kappa=0.296). In contrast, there was a poor agreement between the two human-performed SRs (weighted kappa=0.094). Conclusion Customized GPT-4 models perform well in extracting precise data from medical literature with potential for utilization in review of bias. While the evaluated tasks are simpler than the broader range of SR methodologies, they provide an important initial assessment of GPT-4's capabilities.",
            "corpus_id": "277631115",
            "text": "Objective To assess custom GPT-4 performance in extracting and evaluating data from medical literature to assist in the systematic review (SR) process. Design A proof-of-concept comparative study was conducted to assess the accuracy and precision of custom GPT-4 models against human-performed reviews of randomised controlled trials (RCTs). Setting Four custom GPT-4 models were developed, each specialising in one of the following areas: (1) extraction of study characteristics, (2) extraction of outcomes, (3) extraction of bias assessment domains and (4) evaluation of risk of bias using results from the third GPT-4 model. Model outputs were compared against data from four SRs conducted by human authors. The evaluation focused on accuracy in data extraction, precision in replicating outcomes and agreement levels in risk of bias assessments. Participants Among four SRs chosen, 43 studies were retrieved for data extraction evaluation. Additionally, 17 RCTs were selected for comparison of risk of bias assessments, where both human comparator SRs and an analogous SR provided assessments for comparison. Intervention Custom GPT-4 models were deployed to extract data and evaluate risk of bias from selected studies, and their outputs were compared to those generated by human reviewers. Main outcome measures Concordance rates between GPT-4 outputs and human-performed SRs in data extraction, effect size comparability and inter/intra-rater agreement in risk of bias assessments. Results When comparing the automatically extracted data to the first table of study characteristics from the published review, GPT-4 showed 88.6% concordance with the original review, with <5% discrepancies due to inaccuracies or omissions. It exceeded human accuracy in 2.5% of instances. Study outcomes were extracted and pooling of results showed comparable effect sizes to comparator SRs. A review of bias assessment using GPT-4 showed fair-moderate but significant intra-rater agreement (ICC=0.518, p<0.001) and inter-rater agreements between human comparator SR (weighted kappa=0.237) and the analogous SR (weighted kappa=0.296). In contrast, there was a poor agreement between the two human-performed SRs (weighted kappa=0.094). Conclusion Customized GPT-4 models perform well in extracting precise data from medical literature with potential for utilization in review of bias. While the evaluated tasks are simpler than the broader range of SR methodologies, they provide an important initial assessment of GPT-4's capabilities.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.11981201171875
        },
        {
            "paperId": "8869e032568e1e718d8ff27d229327236074c644",
            "corpusId": 274437486,
            "title": "The Promise and Peril of Generative AI: Evidence from GPT-4 as Sell-Side Analysts",
            "venue": "",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.01069, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284295687",
                    "name": "E. Li"
                },
                {
                    "authorId": "2222457097",
                    "name": "Zhiyuan Tu"
                },
                {
                    "authorId": "2333420754",
                    "name": "Dexin Zhou"
                }
            ],
            "abstract": "We investigate how advanced large language models (LLMs), specifically GPT-4, process corporate disclosures to forecast earnings. Using earnings press releases issued around GPT-4's knowledge cutoff date, we address two questions: (1) Do GPT-generated earnings forecasts outperform analysts in accuracy? (2) How is GPT's performance related to its processing of textual and quantitative information? Our findings suggest that GPT forecasts are significantly less accurate than those of analysts. This underperformance can be traced to GPT's distinct textual and quantitative approaches: its textual processing follows a consistent, generalized pattern across firms, highlighting its strengths in language tasks. In contrast, its quantitative processing capabilities vary significantly across firms, revealing limitations tied to the uneven availability of domain-specific training data. Additionally, there is some evidence that GPT's forecast accuracy diminishes beyond its knowledge cutoff, underscoring the need to evaluate LLMs under hindsight-free conditions. Overall, this study provides a novel exploration of the\"black box\"of GPT-4's information processing, offering insights into LLMs' potential and challenges in financial applications.",
            "corpus_id": "274437486",
            "text": "We investigate how advanced large language models (LLMs), specifically GPT-4, process corporate disclosures to forecast earnings. Using earnings press releases issued around GPT-4's knowledge cutoff date, we address two questions: (1) Do GPT-generated earnings forecasts outperform analysts in accuracy? (2) How is GPT's performance related to its processing of textual and quantitative information? Our findings suggest that GPT forecasts are significantly less accurate than those of analysts. This underperformance can be traced to GPT's distinct textual and quantitative approaches: its textual processing follows a consistent, generalized pattern across firms, highlighting its strengths in language tasks. In contrast, its quantitative processing capabilities vary significantly across firms, revealing limitations tied to the uneven availability of domain-specific training data. Additionally, there is some evidence that GPT's forecast accuracy diminishes beyond its knowledge cutoff, underscoring the need to evaluate LLMs under hindsight-free conditions. Overall, this study provides a novel exploration of the\"black box\"of GPT-4's information processing, offering insights into LLMs' potential and challenges in financial applications.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.09075927734375
        },
        {
            "paperId": "f1c3d945ada7be6d1d20f3e0a2b1d3ad7cb68329",
            "corpusId": 264975312,
            "title": "GUI-Based Software Testing: An Automated Approach Using GPT-4 and Selenium WebDriver",
            "venue": "2023 38th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW)",
            "year": 2023,
            "referenceCount": 16,
            "citationCount": 13,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ASEW60602.2023.00028?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ASEW60602.2023.00028, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2264965433",
                    "name": "Daniel Zimmermann"
                },
                {
                    "authorId": "1909157",
                    "name": "Anne Koziolek"
                }
            ],
            "abstract": "This paper presents a novel method for GUI testing in web applications that largely automates the process by integrating the advanced language model GPT-4 with Selenium, a popular web application testing framework. Unlike traditional deep learning approaches, which require extensive training data, GPT-4 is pre-trained on a large corpus, giving it significant generalisation and inference capabilities. These capabilities allow testing without the need for recorded data from human testers, significantly reducing the time and effort required for the testing process. We also compare the efficiency of our integrated GPT-4 approach with monkey testing, a widely used technique for automated GUI testing where user input is randomly generated. To evaluate our approach, we implemented a web calculator with an integrated code coverage system. The results show that our integrated GPT-4 approach provides significantly better branch coverage compared to monkey testing. These results highlight the significant potential of integrating specific AI models such as GPT-4 and automated testing tools to improve the accuracy and efficiency of GUI testing in web applications.",
            "corpus_id": "264975312",
            "text": "This paper presents a novel method for GUI testing in web applications that largely automates the process by integrating the advanced language model GPT-4 with Selenium, a popular web application testing framework. Unlike traditional deep learning approaches, which require extensive training data, GPT-4 is pre-trained on a large corpus, giving it significant generalisation and inference capabilities. These capabilities allow testing without the need for recorded data from human testers, significantly reducing the time and effort required for the testing process. We also compare the efficiency of our integrated GPT-4 approach with monkey testing, a widely used technique for automated GUI testing where user input is randomly generated. To evaluate our approach, we implemented a web calculator with an integrated code coverage system. The results show that our integrated GPT-4 approach provides significantly better branch coverage compared to monkey testing. These results highlight the significant potential of integrating specific AI models such as GPT-4 and automated testing tools to improve the accuracy and efficiency of GUI testing in web applications.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0287628173828125
        },
        {
            "paperId": "6da371352bf1a3b3ccaecda567404285459e8ef8",
            "corpusId": 259076029,
            "title": "Beyond Generating Code: Evaluating GPT on a Data Visualization Course",
            "venue": "2023 IEEE VIS Workshop on Visualization Education, Literacy, and Activities (EduVis)",
            "year": 2023,
            "referenceCount": 23,
            "citationCount": 31,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2306.02914",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.02914, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2874050",
                    "name": "Zhutian Chen"
                },
                {
                    "authorId": "2218943049",
                    "name": "Chenyang Zhang"
                },
                {
                    "authorId": "49110486",
                    "name": "Qianwen Wang"
                },
                {
                    "authorId": "1995573509",
                    "name": "J. Troidl"
                },
                {
                    "authorId": "2130925892",
                    "name": "Simon Warchol"
                },
                {
                    "authorId": "47273741",
                    "name": "Johanna Beyer"
                },
                {
                    "authorId": "1896974",
                    "name": "Nils Gehlenborg"
                },
                {
                    "authorId": "2062947008",
                    "name": "Hanspeter Pfister"
                }
            ],
            "abstract": "This paper presents an empirical evaluation of the performance of the Generative Pre-trained Transformer (GPT) model in Harvard\u2019s CS171 data visualization course. While previous studies have focused on GPT\u2019s ability to generate code for visualizations, this study goes beyond code generation to evaluate GPT\u2019s abilities in various visualization tasks, such as data interpretation, visualization design, visual data exploration, and insight communication. The evaluation utilized GPT-3.5 and GPT-4 through the APIs of OpenAI to complete assignments of CS171, and included a quantitative assessment based on the established course rubrics, a qualitative analysis informed by the feedback of three experienced graders, and an exploratory study of GPT\u2019s capabilities in completing border visualization tasks. Findings show that GPT-4 scored 80% on quizzes and homework, and Teaching Fellows could distinguish between GPT-and human-generated homework with 70% accuracy. The study also demonstrates GPT\u2019s potential in completing various visualization tasks, such as data cleanup, interaction with visualizations, and insight communication. The paper concludes by discussing the strengths and limitations of GPT in data visualization, potential avenues for incorporating GPT in broader visualization tasks, and the need to redesign visualization education.",
            "corpus_id": "259076029",
            "text": "This paper presents an empirical evaluation of the performance of the Generative Pre-trained Transformer (GPT) model in Harvard\u2019s CS171 data visualization course. While previous studies have focused on GPT\u2019s ability to generate code for visualizations, this study goes beyond code generation to evaluate GPT\u2019s abilities in various visualization tasks, such as data interpretation, visualization design, visual data exploration, and insight communication. The evaluation utilized GPT-3.5 and GPT-4 through the APIs of OpenAI to complete assignments of CS171, and included a quantitative assessment based on the established course rubrics, a qualitative analysis informed by the feedback of three experienced graders, and an exploratory study of GPT\u2019s capabilities in completing border visualization tasks. Findings show that GPT-4 scored 80% on quizzes and homework, and Teaching Fellows could distinguish between GPT-and human-generated homework with 70% accuracy. The study also demonstrates GPT\u2019s potential in completing various visualization tasks, such as data cleanup, interaction with visualizations, and insight communication. The paper concludes by discussing the strengths and limitations of GPT in data visualization, potential avenues for incorporating GPT in broader visualization tasks, and the need to redesign visualization education.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.07965087890625
        },
        {
            "paperId": "9e8cb8c91a0acb6e661b58ad724aa758490f2bea",
            "corpusId": 257985497,
            "title": "Instruction Tuning with GPT-4",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 27,
            "citationCount": 625,
            "influentialCitationCount": 59,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2304.03277",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2304.03277, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1780690",
                    "name": "Baolin Peng"
                },
                {
                    "authorId": "2109737569",
                    "name": "Chunyuan Li"
                },
                {
                    "authorId": "50462546",
                    "name": "Pengcheng He"
                },
                {
                    "authorId": "1947267",
                    "name": "Michel Galley"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                }
            ],
            "abstract": "Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.",
            "corpus_id": "257985497",
            "text": "Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0616455078125
        },
        {
            "paperId": "6d3122deede0cae6f3e46ccc6a8863748697ddae",
            "corpusId": 271878652,
            "title": "GPT-4 as an X data annotator: Unraveling its performance on a stance classification task",
            "venue": "PLoS ONE",
            "year": 2024,
            "referenceCount": 23,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1371/journal.pone.0307741",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11326574, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2202071684",
                    "name": "C. Liyanage"
                },
                {
                    "authorId": "2316179850",
                    "name": "Ravi Gokani"
                },
                {
                    "authorId": "33772005",
                    "name": "Vijay K. Mago"
                }
            ],
            "abstract": "Data annotation in NLP is a costly and time-consuming task, traditionally handled by human experts who require extensive training to enhance the task-related background knowledge. Besides, labeling social media texts is particularly challenging due to their brevity, informality, creativity, and varying human perceptions regarding the sociocultural context of the world. With the emergence of GPT models and their proficiency in various NLP tasks, this study aims to establish a performance baseline for GPT-4 as a social media text annotator. To achieve this, we employ our own dataset of tweets, expertly labeled for stance detection with full inter-rater agreement among three annotators. We experiment with three techniques: Zero-shot, Few-shot, and Zero-shot with Chain-of-Thoughts to create prompts for the labeling task. We utilize four training sets constructed with different label sets, including human labels, to fine-tune transformer-based large language models and various combinations of traditional machine learning models with embeddings for stance classification. Finally, all fine-tuned models undergo evaluation using a common testing set with human-generated labels. We use the results from models trained on human labels as the benchmark to assess GPT-4\u2019s potential as an annotator across the three prompting techniques. Based on the experimental findings, GPT-4 achieves comparable results through the Few-shot and Zero-shot Chain-of-Thoughts prompting methods. However, none of these labeling techniques surpass the top three models fine-tuned on human labels. Moreover, we introduce the Zero-shot Chain-of-Thoughts as an effective strategy for aspect-based social media text labeling, which performs better than the standard Zero-shot and yields results similar to the high-performing yet expensive Few-shot approach.",
            "corpus_id": "271878652",
            "text": "Data annotation in NLP is a costly and time-consuming task, traditionally handled by human experts who require extensive training to enhance the task-related background knowledge. Besides, labeling social media texts is particularly challenging due to their brevity, informality, creativity, and varying human perceptions regarding the sociocultural context of the world. With the emergence of GPT models and their proficiency in various NLP tasks, this study aims to establish a performance baseline for GPT-4 as a social media text annotator. To achieve this, we employ our own dataset of tweets, expertly labeled for stance detection with full inter-rater agreement among three annotators. We experiment with three techniques: Zero-shot, Few-shot, and Zero-shot with Chain-of-Thoughts to create prompts for the labeling task. We utilize four training sets constructed with different label sets, including human labels, to fine-tune transformer-based large language models and various combinations of traditional machine learning models with embeddings for stance classification. Finally, all fine-tuned models undergo evaluation using a common testing set with human-generated labels. We use the results from models trained on human labels as the benchmark to assess GPT-4\u2019s potential as an annotator across the three prompting techniques. Based on the experimental findings, GPT-4 achieves comparable results through the Few-shot and Zero-shot Chain-of-Thoughts prompting methods. However, none of these labeling techniques surpass the top three models fine-tuned on human labels. Moreover, we introduce the Zero-shot Chain-of-Thoughts as an effective strategy for aspect-based social media text labeling, which performs better than the standard Zero-shot and yields results similar to the high-performing yet expensive Few-shot approach.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.06561279296875
        },
        {
            "paperId": "3a9bbbb2038d55c3c7809454a2d0480fadcfe891",
            "corpusId": 273229757,
            "title": "Optimized biomedical entity relation extraction method with data augmentation and classification using GPT-4 and Gemini",
            "venue": "Database J. Biol. Databases Curation",
            "year": 2024,
            "referenceCount": 25,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1093/database/baae104",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11463225, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2315267736",
                    "name": "Cong-Phuoc Phan"
                },
                {
                    "authorId": "2325114330",
                    "name": "Ben Phan"
                },
                {
                    "authorId": "2315267566",
                    "name": "Jung-Hsien Chiang"
                }
            ],
            "abstract": "Abstract Despite numerous research efforts by teams participating in the BioCreative VIII Track 01 employing various techniques to achieve the high accuracy of biomedical relation tasks, the overall performance in this area still has substantial room for improvement. Large language models bring a new opportunity to improve the performance of existing techniques in natural language processing tasks. This paper presents our improved method for relation extraction, which involves integrating two renowned large language models: Gemini and GPT-4. Our new approach utilizes GPT-4 to generate augmented data for training, followed by an ensemble learning technique to combine the outputs of diverse models to create a more precise prediction. We then employ a method using Gemini responses as input to fine-tune the BioNLP\u2013PubMed\u2013Bert classification model, which leads to improved performance as measured by precision, recall, and F1 scores on the same test dataset used in the challenge evaluation. Database URL: https://biocreative.bioinformatics.udel.edu/tasks/biocreative-viii/track-1/",
            "corpus_id": "273229757",
            "text": "Abstract Despite numerous research efforts by teams participating in the BioCreative VIII Track 01 employing various techniques to achieve the high accuracy of biomedical relation tasks, the overall performance in this area still has substantial room for improvement. Large language models bring a new opportunity to improve the performance of existing techniques in natural language processing tasks. This paper presents our improved method for relation extraction, which involves integrating two renowned large language models: Gemini and GPT-4. Our new approach utilizes GPT-4 to generate augmented data for training, followed by an ensemble learning technique to combine the outputs of diverse models to create a more precise prediction. We then employ a method using Gemini responses as input to fine-tune the BioNLP\u2013PubMed\u2013Bert classification model, which leads to improved performance as measured by precision, recall, and F1 scores on the same test dataset used in the challenge evaluation. Database URL: https://biocreative.bioinformatics.udel.edu/tasks/biocreative-viii/track-1/",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.06231689453125
        },
        {
            "paperId": "f8e454ca836f80bfa2bea4671e97f60a7a76acd5",
            "corpusId": 268681244,
            "title": "A comparison of human, GPT-3.5, and GPT-4 performance in a university-level coding course",
            "venue": "Scientific Reports",
            "year": 2024,
            "referenceCount": 18,
            "citationCount": 11,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41598-024-73634-y.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.16977, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2293313677",
                    "name": "Will Yeadon"
                },
                {
                    "authorId": "2293313632",
                    "name": "Alex Peach"
                },
                {
                    "authorId": "50820901",
                    "name": "Craig P. Testrow"
                }
            ],
            "abstract": "This study evaluates the performance of ChatGPT variants, GPT-3.5 and GPT-4, both with and without prompt engineering, against solely student work and a mixed category containing both student and GPT-4 contributions in university-level physics coding assignments using the Python language. Comparing 50 student submissions to 50 AI-generated submissions across different categories, and marked blindly by three independent markers, we amassed \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$n = 300$$\\end{document}n=300 data points. Students averaged 91.9% (SE:0.4), surpassing the highest performing AI submission category, GPT-4 with prompt engineering, which scored 81.1% (SE:0.8)\u2014a statistically significant difference (p = \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$2.482 \\times 10^{-10}$$\\end{document}2.482\u00d710-10). Prompt engineering significantly improved scores for both GPT-4 (p = \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$1.661 \\times 10^{-4}$$\\end{document}1.661\u00d710-4) and GPT-3.5 (p = \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$4.967 \\times 10^{-9}$$\\end{document}4.967\u00d710-9). Additionally, the blinded markers were tasked with guessing the authorship of the submissions on a four-point Likert scale from \u2018Definitely AI\u2019 to \u2018Definitely Human\u2019. They accurately identified the authorship, with 92.1% of the work categorized as \u2018Definitely Human\u2019 being human-authored. Simplifying this to a binary \u2018AI\u2019 or \u2018Human\u2019 categorization resulted in an average accuracy rate of 85.3%. These findings suggest that while AI-generated work closely approaches the quality of university students\u2019 work, it often remains detectable by human evaluators.",
            "corpus_id": "268681244",
            "text": "This study evaluates the performance of ChatGPT variants, GPT-3.5 and GPT-4, both with and without prompt engineering, against solely student work and a mixed category containing both student and GPT-4 contributions in university-level physics coding assignments using the Python language. Comparing 50 student submissions to 50 AI-generated submissions across different categories, and marked blindly by three independent markers, we amassed \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$n = 300$$\\end{document}n=300 data points. Students averaged 91.9% (SE:0.4), surpassing the highest performing AI submission category, GPT-4 with prompt engineering, which scored 81.1% (SE:0.8)\u2014a statistically significant difference (p = \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$2.482 \\times 10^{-10}$$\\end{document}2.482\u00d710-10). Prompt engineering significantly improved scores for both GPT-4 (p = \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$1.661 \\times 10^{-4}$$\\end{document}1.661\u00d710-4) and GPT-3.5 (p = \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$4.967 \\times 10^{-9}$$\\end{document}4.967\u00d710-9). Additionally, the blinded markers were tasked with guessing the authorship of the submissions on a four-point Likert scale from \u2018Definitely AI\u2019 to \u2018Definitely Human\u2019. They accurately identified the authorship, with 92.1% of the work categorized as \u2018Definitely Human\u2019 being human-authored. Simplifying this to a binary \u2018AI\u2019 or \u2018Human\u2019 categorization resulted in an average accuracy rate of 85.3%. These findings suggest that while AI-generated work closely approaches the quality of university students\u2019 work, it often remains detectable by human evaluators.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0144500732421875
        },
        {
            "paperId": "0244aeb7c6927e2fb0c2e668687e160a00737dbe",
            "corpusId": 259075316,
            "title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 32,
            "citationCount": 276,
            "influentialCitationCount": 23,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.02707",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.02707, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153292652",
                    "name": "Subhabrata Mukherjee"
                },
                {
                    "authorId": "2146720788",
                    "name": "Arindam Mitra"
                },
                {
                    "authorId": "2065043351",
                    "name": "Ganesh Jawahar"
                },
                {
                    "authorId": "2211923024",
                    "name": "Sahaj Agarwal"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2072795428",
                    "name": "A. Awadallah"
                }
            ],
            "abstract": "Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMA's release policy to be published at https://aka.ms/orca-lm), a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.",
            "corpus_id": "259075316",
            "text": "Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMA's release policy to be published at https://aka.ms/orca-lm), a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.029754638671875
        },
        {
            "paperId": "1d1097d378555393b73b492995121ab880fff142",
            "corpusId": 276938001,
            "title": "Adaptive political surveys and GPT-4: Tackling the cold start problem with simulated user interactions",
            "venue": "PLoS ONE",
            "year": 2025,
            "referenceCount": 70,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.09311, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294571291",
                    "name": "Fynn Bachmann"
                },
                {
                    "authorId": "2191625924",
                    "name": "Daan van der Weijden"
                },
                {
                    "authorId": "121221604",
                    "name": "Lucien Heitz"
                },
                {
                    "authorId": "2313290",
                    "name": "Cristina Sarasua"
                },
                {
                    "authorId": "2191250173",
                    "name": "Abraham Bernstein"
                }
            ],
            "abstract": "Adaptive questionnaires dynamically select the next question for a survey participant based on their previous answers. Due to digitalisation, they have become a viable alternative to traditional surveys in application areas such as political science. One limitation, however, is their dependency on data to train the model for question selection. Often, such training data (i.e., user interactions) are unavailable a priori. To address this problem, we (i) test whether Large Language Models (LLM) can accurately generate such interaction data and (ii) explore if these synthetic data can be used to pre-train the statistical model of an adaptive political survey. To evaluate this approach, we utilise existing data from the Swiss Voting Advice Application (VAA) Smartvote in two ways: First, we compare the distribution of LLM-generated synthetic data to the real distribution to assess its similarity. Second, we compare the performance of an adaptive questionnaire that is randomly initialised with one pre-trained on synthetic data to assess their suitability for training. We benchmark these results against an \u201coracle\u201d questionnaire with perfect prior knowledge. We find that an off-the-shelf LLM (GPT-4) accurately generates answers to the Smartvote questionnaire from the perspective of different Swiss parties. Furthermore, we demonstrate that initialising the statistical model with synthetic data can (i) significantly reduce the error in predicting user responses and (ii) increase the candidate recommendation accuracy of the VAA. Our work emphasises the considerable potential of LLMs to create training data to improve the data collection process in adaptive questionnaires in LLM-affine areas such as political surveys.",
            "corpus_id": "276938001",
            "text": "Adaptive questionnaires dynamically select the next question for a survey participant based on their previous answers. Due to digitalisation, they have become a viable alternative to traditional surveys in application areas such as political science. One limitation, however, is their dependency on data to train the model for question selection. Often, such training data (i.e., user interactions) are unavailable a priori. To address this problem, we (i) test whether Large Language Models (LLM) can accurately generate such interaction data and (ii) explore if these synthetic data can be used to pre-train the statistical model of an adaptive political survey. To evaluate this approach, we utilise existing data from the Swiss Voting Advice Application (VAA) Smartvote in two ways: First, we compare the distribution of LLM-generated synthetic data to the real distribution to assess its similarity. Second, we compare the performance of an adaptive questionnaire that is randomly initialised with one pre-trained on synthetic data to assess their suitability for training. We benchmark these results against an \u201coracle\u201d questionnaire with perfect prior knowledge. We find that an off-the-shelf LLM (GPT-4) accurately generates answers to the Smartvote questionnaire from the perspective of different Swiss parties. Furthermore, we demonstrate that initialising the statistical model with synthetic data can (i) significantly reduce the error in predicting user responses and (ii) increase the candidate recommendation accuracy of the VAA. Our work emphasises the considerable potential of LLMs to create training data to improve the data collection process in adaptive questionnaires in LLM-affine areas such as political surveys.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.07421875
        },
        {
            "paperId": "381ab7a640f5b46b62f7e08d1af4a8e0d3eadd55",
            "corpusId": 257804696,
            "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "referenceCount": 40,
            "citationCount": 1211,
            "influentialCitationCount": 222,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2303.16634",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.16634, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "3310951",
                    "name": "Dan Iter"
                },
                {
                    "authorId": "2110197273",
                    "name": "Yichong Xu"
                },
                {
                    "authorId": "2146294891",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "8233965",
                    "name": "Ruochen Xu"
                },
                {
                    "authorId": "8652308",
                    "name": "Chenguang Zhu"
                }
            ],
            "abstract": "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval",
            "corpus_id": "257804696",
            "text": "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.193359375
        },
        {
            "paperId": "7f22311866adc42cc8884d5337eaaf291da1bcc8",
            "corpusId": 268477855,
            "title": "Unmasking Bias in Chat GPT Responses",
            "venue": "International Conference on Advances in Social Networks Analysis and Mining",
            "year": 2023,
            "referenceCount": 13,
            "citationCount": 6,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3625007.3627484",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3625007.3627484?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3625007.3627484, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2291531078",
                    "name": "Clay Duncan"
                },
                {
                    "authorId": "2638935",
                    "name": "I. Mcculloh"
                }
            ],
            "abstract": "Generative artificial intelligence (AI) has gained a great deal of recent attention with the release of Chat GPT 4. It has been praised for its ability to generate human-like responses but has perhaps faced even more criticism over potential concerns for biased responses, misinformation, and generation of harmful or inappropriate content. Chat GPT utilizes large sources of data to curate responses to all kinds of questions. The generative AI models are designed to be objective and avoid any sort of bias in their output. However, in the age of misinformation, social media, user-generated content and the 24-hour news cycle, biased information has never been more plentiful. This paper investigates the possibility of biased responses produced by Chat GPT 4 utilizing public data from biased media sources through Support Vector Machines. We find Chat GPT tends to have bias in its responses.",
            "corpus_id": "268477855",
            "text": "Generative artificial intelligence (AI) has gained a great deal of recent attention with the release of Chat GPT 4. It has been praised for its ability to generate human-like responses but has perhaps faced even more criticism over potential concerns for biased responses, misinformation, and generation of harmful or inappropriate content. Chat GPT utilizes large sources of data to curate responses to all kinds of questions. The generative AI models are designed to be objective and avoid any sort of bias in their output. However, in the age of misinformation, social media, user-generated content and the 24-hour news cycle, biased information has never been more plentiful. This paper investigates the possibility of biased responses produced by Chat GPT 4 utilizing public data from biased media sources through Support Vector Machines. We find Chat GPT tends to have bias in its responses.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1668701171875
        },
        {
            "paperId": "d36bbbe2eb83981c4e714f1d4688334f2aae6369",
            "corpusId": 268531430,
            "title": "GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 43,
            "citationCount": 6,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.11858, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292327296",
                    "name": "Shanglong Yang"
                },
                {
                    "authorId": "2196205875",
                    "name": "Zhipeng Yuan"
                },
                {
                    "authorId": "2162396286",
                    "name": "Shunbao Li"
                },
                {
                    "authorId": "2228249645",
                    "name": "Ruoling Peng"
                },
                {
                    "authorId": "2230710619",
                    "name": "Kang Liu"
                },
                {
                    "authorId": "2234027307",
                    "name": "Po Yang"
                }
            ],
            "abstract": "In the rapidly evolving field of artificial intelligence (AI), the application of large language models (LLMs) in agriculture, particularly in pest management, remains nascent. We aimed to prove the feasibility by evaluating the content of the pest management advice generated by LLMs, including the Generative Pre-trained Transformer (GPT) series from OpenAI and the FLAN series from Google. Considering the context-specific properties of agricultural advice, automatically measuring or quantifying the quality of text generated by LLMs becomes a significant challenge. We proposed an innovative approach, using GPT-4 as an evaluator, to score the generated content on Coherence, Logical Consistency, Fluency, Relevance, Comprehensibility, and Exhaustiveness. Additionally, we integrated an expert system based on crop threshold data as a baseline to obtain scores for Factual Accuracy on whether pests found in crop fields should take management action. Each model's score was weighted by percentage to obtain a final score. The results showed that GPT-3.4 and GPT-4 outperform the FLAN models in most evaluation categories. Furthermore, the use of instruction-based prompting containing domain-specific knowledge proved the feasibility of LLMs as an effective tool in agriculture, with an accuracy rate of 72%, demonstrating LLMs' effectiveness in providing pest management suggestions.",
            "corpus_id": "268531430",
            "text": "In the rapidly evolving field of artificial intelligence (AI), the application of large language models (LLMs) in agriculture, particularly in pest management, remains nascent. We aimed to prove the feasibility by evaluating the content of the pest management advice generated by LLMs, including the Generative Pre-trained Transformer (GPT) series from OpenAI and the FLAN series from Google. Considering the context-specific properties of agricultural advice, automatically measuring or quantifying the quality of text generated by LLMs becomes a significant challenge. We proposed an innovative approach, using GPT-4 as an evaluator, to score the generated content on Coherence, Logical Consistency, Fluency, Relevance, Comprehensibility, and Exhaustiveness. Additionally, we integrated an expert system based on crop threshold data as a baseline to obtain scores for Factual Accuracy on whether pests found in crop fields should take management action. Each model's score was weighted by percentage to obtain a final score. The results showed that GPT-3.4 and GPT-4 outperform the FLAN models in most evaluation categories. Furthermore, the use of instruction-based prompting containing domain-specific knowledge proved the feasibility of LLMs as an effective tool in agriculture, with an accuracy rate of 72%, demonstrating LLMs' effectiveness in providing pest management suggestions.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.044586181640625
        },
        {
            "paperId": "0bff9ce2c20f5374cbf6e7a90fd21e88980395f2",
            "corpusId": 272571020,
            "title": "Quantitative Analysis of GPT-4 model: Optimizing Patient Eligibility Classification for Clinical Trials and Reducing Expert Judgment Dependency",
            "venue": "International Conference on Medical and Health Informatics",
            "year": 2024,
            "referenceCount": 15,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3673971.3674014?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3673971.3674014, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308278238",
                    "name": "Arti Devi"
                },
                {
                    "authorId": "2153780597",
                    "name": "Shashank Uttrani"
                },
                {
                    "authorId": "2308278573",
                    "name": "Aryansh Singla"
                },
                {
                    "authorId": "2308342275",
                    "name": "Sarthak Jha"
                },
                {
                    "authorId": "46355474",
                    "name": "Nataraj Dasgupta"
                },
                {
                    "authorId": "40765840",
                    "name": "Sayee Natarajan"
                },
                {
                    "authorId": "2320631850",
                    "name": "Rajeshwari S. Punekar"
                },
                {
                    "authorId": "40769946",
                    "name": "Larry A. Pickett"
                },
                {
                    "authorId": "2257054685",
                    "name": "Varun Dutt"
                }
            ],
            "abstract": "Objective: Generative Pre-trained Transformer 4 (GPT-4) is a large multimodal language model created by OpenAI and the fourth in its series of GPT foundation models. Although GPT-4 has been utilized in several applications, its abilities are less known for patient categorization based on their eligibility for clinical trials. The primary objective of this work is to evaluate the accuracy and efficacy of GPT-4 for patient eligibility evaluation. Data: Ten US NSCLC drug-only interventional clinical trials were selected from clinicaltrials.gov. Ten patient profiles were manually created for each clinical trial using case presentations published in peer-reviewed medical journals by clinicians/epidemiologists. The dataset included two sets of adult patient profiles (50 eligible patients and 50 non-eligible patients for 100 patients) with a range of complexities, from complex to simple cases. The 100-case dataset was then analyzed, comparing the accuracy of the large language model\u2014GPT-4, against the human expert's evaluation. Analysis: Various data tuning scenarios (80% and 0%) were evaluated, explicitly examining the model's capacity to mimic the performance of human experts in classifying patient eligibility. Model evaluations were compared to human evaluations to ensure reliable accuracy results.To measure efficacy, age analysis, gender analysis, and sensitivity and specificity analyses were conducted, providing a comprehensive examination of the model's performance across various dimensions. Results: GPT-4 showed 100% test accuracy in scenarios involving tuning in 80% of cases and 95% test accuracy without tuning in 80% of cases. GPT-4 showed 86% test accuracy on all cases with 0% tuning. Furthermore, the GPT-4 model was compared to human patient categorization based on patient eligibility for clinical trials. The bias shown by GPT-4 was not significantly different from the bias shown by the human expert's evaluation both based on gender and age. Conclusion: The GPT-4 model has demonstrated a high level of accuracy and an unbiased approach to patient classification compared to human experts. However, further research with more extensive and diverse datasets is recommended to confirm these findings. Other LLMs may also be tested in clinical trial settings.",
            "corpus_id": "272571020",
            "text": "Objective: Generative Pre-trained Transformer 4 (GPT-4) is a large multimodal language model created by OpenAI and the fourth in its series of GPT foundation models. Although GPT-4 has been utilized in several applications, its abilities are less known for patient categorization based on their eligibility for clinical trials. The primary objective of this work is to evaluate the accuracy and efficacy of GPT-4 for patient eligibility evaluation. Data: Ten US NSCLC drug-only interventional clinical trials were selected from clinicaltrials.gov. Ten patient profiles were manually created for each clinical trial using case presentations published in peer-reviewed medical journals by clinicians/epidemiologists. The dataset included two sets of adult patient profiles (50 eligible patients and 50 non-eligible patients for 100 patients) with a range of complexities, from complex to simple cases. The 100-case dataset was then analyzed, comparing the accuracy of the large language model\u2014GPT-4, against the human expert's evaluation. Analysis: Various data tuning scenarios (80% and 0%) were evaluated, explicitly examining the model's capacity to mimic the performance of human experts in classifying patient eligibility. Model evaluations were compared to human evaluations to ensure reliable accuracy results.To measure efficacy, age analysis, gender analysis, and sensitivity and specificity analyses were conducted, providing a comprehensive examination of the model's performance across various dimensions. Results: GPT-4 showed 100% test accuracy in scenarios involving tuning in 80% of cases and 95% test accuracy without tuning in 80% of cases. GPT-4 showed 86% test accuracy on all cases with 0% tuning. Furthermore, the GPT-4 model was compared to human patient categorization based on patient eligibility for clinical trials. The bias shown by GPT-4 was not significantly different from the bias shown by the human expert's evaluation both based on gender and age. Conclusion: The GPT-4 model has demonstrated a high level of accuracy and an unbiased approach to patient classification compared to human experts. However, further research with more extensive and diverse datasets is recommended to confirm these findings. Other LLMs may also be tested in clinical trial settings.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1480712890625
        }
    ],
    "quotes": {
        "cost": 0.128475,
        "quotes": [
            {
                "idx": 0,
                "key": "[258960339 | Wang et al. | 2023 | Citations: 573]",
                "snippets": "Specifically, we demonstrate that GPT-4 exhibits a preference for the first displayed candidate response by consistently assigning it higher scores, even when the order of candidates is subtly altered. As illustrated in Figure 1, merely swapping the presentation order can reverse evaluation outcomes. This bias is also present in Chat-GPT, which typically favors the second response. These findings highlight previously overlooked limitations in the current evaluation paradigm.\n\nTo address this issue, we propose three simple yet effective strategies to calibrate positional bias: 1) Multiple Evidence Calibration (MEC): We prompt the model to generate evaluation evidence before assigning scores, leveraging the inherent properties of causal language models for calibration. We also employ ensemble techniques to incorporate multiple evidence calibration results to further stabilize the evaluation. 2) Balanced Position Calibration (BPC): To further reduce positional bias, we evaluate each candidate in both positions across two runs and compute the final score as the average of the two runs. 3) Human In The Loop Calibration (HITLC): We also explore human-inthe-loop evaluation and consider a diversity-based method to get a cue to indicate biased candidates based on the evaluation results of MEC and BPC.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "GPT-4",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1313,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 201
                            },
                            {
                                "start": 202,
                                "end": 301
                            },
                            {
                                "start": 302,
                                "end": 384
                            },
                            {
                                "start": 385,
                                "end": 479
                            },
                            {
                                "start": 482,
                                "end": 778
                            },
                            {
                                "start": 779,
                                "end": 903
                            },
                            {
                                "start": 904,
                                "end": 1099
                            },
                            {
                                "start": 1100,
                                "end": 1314
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Specifically, we demonstrate that GPT-4 exhibits a preference for the first displayed candidate response by consistently assigning it higher scores, even when the order of candidates is subtly altered. As illustrated in Figure 1, merely swapping the presentation order can reverse evaluation outcomes. This bias is also present in Chat-GPT, which typically favors the second response. These findings highlight previously overlooked limitations in the current evaluation paradigm.\n\nTo address this issue, we propose three simple yet effective strategies to calibrate positional bias: 1) Multiple Evidence Calibration (MEC): We prompt the model to generate evaluation evidence before assigning scores, leveraging the inherent properties of causal language models for calibration. We also employ ensemble techniques to incorporate multiple evidence calibration results to further stabilize the evaluation. 2) Balanced Position Calibration (BPC): To further reduce positional bias, we evaluate each candidate in both positions across two runs and compute the final score as the average of the two runs. 3) Human In The Loop Calibration (HITLC): We also explore human-inthe-loop evaluation and consider a diversity-based method to get a cue to indicate biased candidates based on the evaluation results of MEC and BPC."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[259360998 | Wu et al. | 2023 | Citations: 47]",
                "snippets": "This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4. By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "LLM Evaluation",
                        "pdf_hash": "",
                        "start": 327,
                        "end": 680,
                        "sentence_offsets": [
                            {
                                "start": 209,
                                "end": 442
                            },
                            {
                                "start": 443,
                                "end": 572
                            },
                            {
                                "start": 573,
                                "end": 658
                            },
                            {
                                "start": 659,
                                "end": 797
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4. By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[263310448 | Koo et al. | 2023 | Citations: 86]",
                "snippets": "Egocentric Bias (Self-Preference). (Ross and Sicoly, 1979) is a cognitive bias that refers to the tendency to have a higher opinion of oneself or to more easily accept ideas if they match one's own. We define an evaluator to be egocentrically biased if, for each instance, the evaluator prefers its own response over others. We note that an unbiased evaluator would choose between themselves and other comparand models equally in proportion. However, we highlight that some models would naturally generate higher quality responses (e.g., GPT4 vs. KOALA), resulting in a stronger inclination for such evaluators to choose their own responses.\n\nWe employ various strategies to mitigate these confounding variables and isolate each analysis as much as possible. For example, we employ a \"hierarchical\" rubric, where some biases take priority in an evaluation. Specifically, if an evaluation shows signs of order bias by choosing A in (A first, then B) and B in (B first, then A), we do not evaluate it for SALIENCE or EGOCEN-TRIC bias.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Egocentric Bias (Self-Preference). (Ross and Sicoly, 1979) is a cognitive bias that refers to the tendency to have a higher opinion of oneself or to more easily accept ideas if they match one's own. We define an evaluator to be egocentrically biased if, for each instance, the evaluator prefers its own response over others. We note that an unbiased evaluator would choose between themselves and other comparand models equally in proportion. However, we highlight that some models would naturally generate higher quality responses (e.g., GPT4 vs. KOALA), resulting in a stronger inclination for such evaluators to choose their own responses.\n\nWe employ various strategies to mitigate these confounding variables and isolate each analysis as much as possible. For example, we employ a \"hierarchical\" rubric, where some biases take priority in an evaluation. Specifically, if an evaluation shows signs of order bias by choosing A in (A first, then B) and B in (B first, then A), we do not evaluate it for SALIENCE or EGOCEN-TRIC bias.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[264591429 | Saha et al. | 2023 | Citations: 77]",
                "snippets": "Self-enhancement Bias Reduction.Table 2 evaluates self-enhancement bias by comparing BSM (with zero-shot GPT-4) for the samples where one of the responses is also generated by GPT-4.We observe a 3% better correlation with humans, suggesting that BSM improves evaluation even when the LLM judges its own outputs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Main Results",
                        "pdf_hash": "",
                        "start": 2,
                        "end": 313,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 34
                            },
                            {
                                "start": 34,
                                "end": 184
                            },
                            {
                                "start": 184,
                                "end": 313
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Self-enhancement Bias Reduction.Table 2 evaluates self-enhancement bias by comparing BSM (with zero-shot GPT-4) for the samples where one of the responses is also generated by GPT-4.We observe a 3% better correlation with humans, suggesting that BSM improves evaluation even when the LLM judges its own outputs."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[267523079 | Chen et al. | 2024 | Citations: 122]",
                "snippets": "Egocentric Bias. Models tend to assign higher scores to their own responses while scoring others lower (Zheng et al., 2023b;Li et al., 2024). In Figures 19 and 20, GPT-4V exhibits a slight degree of Egocentricity. Conversely, Gemini maintains a uniform scoring distribution across different sources, demonstrating a more equitable approach to judgment. In contrast, GPT-4V shows self-preference, aligning its judgments with its predefined ethical guidelines. For example, GPT-4V consistently emphasizes privacy preservation, leading to higher scores for privacy-related questions based on its own metrics. Despite efforts in prompt engineering to ensure neutrality, these models still rely on judgment criteria set during post-alignment training (Ouyang et al., 2022). This bias can result in judgments that deviate from human preferences, highlighting the complexity of aligning MLLM judgments with humans'.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[246426909 | Ouyang et al. | 2022 | Citations: 13203]": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
                },
                "metadata": [
                    {
                        "section_title": "Bias and Hallucination",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 908,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 16
                            },
                            {
                                "start": 17,
                                "end": 141
                            },
                            {
                                "start": 142,
                                "end": 213
                            },
                            {
                                "start": 214,
                                "end": 352
                            },
                            {
                                "start": 353,
                                "end": 458
                            },
                            {
                                "start": 459,
                                "end": 605
                            },
                            {
                                "start": 606,
                                "end": 771
                            },
                            {
                                "start": 772,
                                "end": 911
                            }
                        ],
                        "ref_mentions": [
                            "246426909"
                        ],
                        "quote": "Egocentric Bias. Models tend to assign higher scores to their own responses while scoring others lower (Zheng et al., 2023b;Li et al., 2024). In Figures 19 and 20, GPT-4V exhibits a slight degree of Egocentricity. Conversely, Gemini maintains a uniform scoring distribution across different sources, demonstrating a more equitable approach to judgment. In contrast, GPT-4V shows self-preference, aligning its judgments with its predefined ethical guidelines. For example, GPT-4V consistently emphasizes privacy preservation, leading to higher scores for privacy-related questions based on its own metrics. Despite efforts in prompt engineering to ensure neutrality, these models still rely on judgment criteria set during post-alignment training (Ouyang et al., 2022). This bias can result in judgments that deviate from human preferences, highlighting the complexity of aligning MLLM judgments with humans'."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[267760188 | Li et al. | 2024 | Citations: 11]",
                "snippets": "Using LLMs like GPT-4 as judges introduces potential data leakage risks due to biases in their pre-training data. This can be mitigated by selecting neutral evaluators independent of the assessed models' training data or randomly rotating evaluators to reduce bias.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Limitations",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 265,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 113
                            },
                            {
                                "start": 114,
                                "end": 265
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Using LLMs like GPT-4 as judges introduces potential data leakage risks due to biases in their pre-training data. This can be mitigated by selecting neutral evaluators independent of the assessed models' training data or randomly rotating evaluators to reduce bias."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[269188154 | Dai et al. | 2024 | Citations: 82]",
                "snippets": "The emergence of egocentric bias introduces the risk of selfreinforcement for LLMs, particularly when they are further trained using rewards from LLM-based evaluations. This scenario can lead to LLMs overfitting to their own evaluation criteria, intensifying self-preference in next-generation model (Liu et al., 2023). Current strategies for mitigating egocentric bias primarily involve employing diverse LLMs as evaluators to foster peer discussions [79], thereby reducing the preference for any specific LLM and enhancing the robustness of evaluation outcomes. However, this strategy inevitably increases the evaluation costs. Future research must explore more efficient solutions to ensure fair and unbiased evaluation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[257804696 | Liu et al. | 2023 | Citations: 1211]": "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval"
                },
                "metadata": [
                    {
                        "section_title": "Egocentric Bias.",
                        "pdf_hash": "",
                        "start": 901,
                        "end": 1610,
                        "sentence_offsets": [
                            {
                                "start": 901,
                                "end": 1069
                            },
                            {
                                "start": 1070,
                                "end": 1206
                            },
                            {
                                "start": 1207,
                                "end": 1450
                            },
                            {
                                "start": 1451,
                                "end": 1516
                            },
                            {
                                "start": 1517,
                                "end": 1610
                            }
                        ],
                        "ref_mentions": [
                            "257804696"
                        ],
                        "quote": "The emergence of egocentric bias introduces the risk of selfreinforcement for LLMs, particularly when they are further trained using rewards from LLM-based evaluations. This scenario can lead to LLMs overfitting to their own evaluation criteria, intensifying self-preference in next-generation model (Liu et al., 2023). Current strategies for mitigating egocentric bias primarily involve employing diverse LLMs as evaluators to foster peer discussions [79], thereby reducing the preference for any specific LLM and enhancing the robustness of evaluation outcomes. However, this strategy inevitably increases the evaluation costs. Future research must explore more efficient solutions to ensure fair and unbiased evaluation."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[270286247 | Mullick et al. | 2024 | Citations: 3]",
                "snippets": "It is acknowledged that using summaries generated exclusively by GPT-4 could introduce biases inherent in its summarization capabilities, it may also be noted that alternatives, such as human evaluation, also carry their own biases.Despite the potential for bias, leveraging GPT-4 for summarization may still be a pragmatic choice, especially in scenarios access to diverse datasets or sophisticated validation methods is limited.However, in this work, we remain vigilant, recognizing the limitations inherent in both automated and humangenerated summaries, and take proactive steps such as human intervention to validate and contextualise the results to mitigate biases to the best extent possible within the given constraints.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C GPT-4 Bias",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 728,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 232
                            },
                            {
                                "start": 232,
                                "end": 430
                            },
                            {
                                "start": 430,
                                "end": 728
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "It is acknowledged that using summaries generated exclusively by GPT-4 could introduce biases inherent in its summarization capabilities, it may also be noted that alternatives, such as human evaluation, also carry their own biases.Despite the potential for bias, leveraging GPT-4 for summarization may still be a pragmatic choice, especially in scenarios access to diverse datasets or sophisticated validation methods is limited.However, in this work, we remain vigilant, recognizing the limitations inherent in both automated and humangenerated summaries, and take proactive steps such as human intervention to validate and contextualise the results to mitigate biases to the best extent possible within the given constraints."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[270391675 | Li et al. | 2024 | Citations: 15]",
                "snippets": "The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator). This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023). This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator. To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022; Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023; Ouyang et al., 2022) to ensure more balanced and comprehensive assessments.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215548699 | Sellam et al. | 2020 | Citations: 1505]": "Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution."
                },
                "metadata": [
                    {
                        "section_title": "Challenges and Open Problems",
                        "pdf_hash": "",
                        "start": 260,
                        "end": 1078,
                        "sentence_offsets": [
                            {
                                "start": 169,
                                "end": 372
                            },
                            {
                                "start": 372,
                                "end": 543
                            },
                            {
                                "start": 543,
                                "end": 692
                            },
                            {
                                "start": 692,
                                "end": 877
                            },
                            {
                                "start": 877,
                                "end": 1185
                            }
                        ],
                        "ref_mentions": [
                            "215548699"
                        ],
                        "quote": "The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator). This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023). This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator. To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022; Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023; Ouyang et al., 2022) to ensure more balanced and comprehensive assessments."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[270614044 | Bragazzi et al. | 2023 | Citations: 13]",
                "snippets": "A further risk is that LLMs might reinforce existing biases and provide inaccurate medical diagnoses, potentially leading to detrimental effects on health care.Zack et al [26] aimed to evaluate whether GPT-4 harbors biases that could influence its application in health care settings.Using the Azure OpenAI interface, the authors scrutinized GPT-4 for racial and gender biases and assessed the impact of such biases on four clinical applications of LLMs-(1) medical education, (2) diagnostic reasoning, (3) development and implementation of clinical plans, and (4) subjective patient evaluations-involving experiments using prompts mimicking typical GPT-4 use in clinical and medical educational settings and drawing from New England Journal of Medicine Healer clinical vignettes and research on implicit bias in health care.The study compared GPT-4's estimates of demographic distributions of medical conditions against actual US prevalence data.For differential diagnosis and treatment planning, the research analyzed variations across demographic groups using standard statistical methods to identify significant differences.The study revealed that GPT-4 inadequately represents demographic diversity in medical conditions, often resorting to stereotypical demographic portrayals in clinical vignettes.The differential diagnoses generated by GPT-4 for standardized clinical vignettes tended to reflect biases associated with race, ethnicity, and gender.Furthermore, the model's assessments and plans demonstrated a notable correlation between demographic characteristics and recommendations for costlier procedures, as well as varied perceptions of patients.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Toward Clinical LLMs: Necessity of Verifying Evidence-Based Knowledge",
                        "pdf_hash": "",
                        "start": 252,
                        "end": 1913,
                        "sentence_offsets": [
                            {
                                "start": 252,
                                "end": 412
                            },
                            {
                                "start": 412,
                                "end": 536
                            },
                            {
                                "start": 536,
                                "end": 1077
                            },
                            {
                                "start": 1077,
                                "end": 1199
                            },
                            {
                                "start": 1199,
                                "end": 1380
                            },
                            {
                                "start": 1380,
                                "end": 1557
                            },
                            {
                                "start": 1557,
                                "end": 1708
                            },
                            {
                                "start": 1708,
                                "end": 1913
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "A further risk is that LLMs might reinforce existing biases and provide inaccurate medical diagnoses, potentially leading to detrimental effects on health care.Zack et al [26] aimed to evaluate whether GPT-4 harbors biases that could influence its application in health care settings.Using the Azure OpenAI interface, the authors scrutinized GPT-4 for racial and gender biases and assessed the impact of such biases on four clinical applications of LLMs-(1) medical education, (2) diagnostic reasoning, (3) development and implementation of clinical plans, and (4) subjective patient evaluations-involving experiments using prompts mimicking typical GPT-4 use in clinical and medical educational settings and drawing from New England Journal of Medicine Healer clinical vignettes and research on implicit bias in health care.The study compared GPT-4's estimates of demographic distributions of medical conditions against actual US prevalence data.For differential diagnosis and treatment planning, the research analyzed variations across demographic groups using standard statistical methods to identify significant differences.The study revealed that GPT-4 inadequately represents demographic diversity in medical conditions, often resorting to stereotypical demographic portrayals in clinical vignettes.The differential diagnoses generated by GPT-4 for standardized clinical vignettes tended to reflect biases associated with race, ethnicity, and gender.Furthermore, the model's assessments and plans demonstrated a notable correlation between demographic characteristics and recommendations for costlier procedures, as well as varied perceptions of patients."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[272337179 | Ji et al. | 2024 | Citations: 14]",
                "snippets": "It is also important to note that most of our data are generated by gpt-4. Therefore, it is advisable to approach the results produced by GPT-4 with a certain level of skepticism.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Limitations",
                        "pdf_hash": "",
                        "start": 488,
                        "end": 667,
                        "sentence_offsets": [
                            {
                                "start": 488,
                                "end": 562
                            },
                            {
                                "start": 563,
                                "end": 667
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "It is also important to note that most of our data are generated by gpt-4. Therefore, it is advisable to approach the results produced by GPT-4 with a certain level of skepticism."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[273661820 | Wataoka et al. | 2024 | Citations: 25]",
                "snippets": "This finding suggests a potential concern: using GPT-4 as a judge may lead to excessive influence from GPT-4's unique styles and policies.\n\nTo reduce self-preference bias, one possible approach is ensemble evaluation using multiple models. This method is expected to provide a more equitable evaluation by avoiding reliance on a single model. Specifically, when a model exhibits low perplexity on a sample, decreasing the weight assigned to that model's evaluation for that sample may contribute to bias mitigation. To evaluate the effectiveness of bias reduction strategies, our proposed new metric can be utilized.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "This finding suggests a potential concern: using GPT-4 as a judge may lead to excessive influence from GPT-4's unique styles and policies.\n\nTo reduce self-preference bias, one possible approach is ensemble evaluation using multiple models. This method is expected to provide a more equitable evaluation by avoiding reliance on a single model. Specifically, when a model exhibits low perplexity on a sample, decreasing the weight assigned to that model's evaluation for that sample may contribute to bias mitigation. To evaluate the effectiveness of bias reduction strategies, our proposed new metric can be utilized.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[274437478 | Sumita et al. | 2024 | Citations: 5]",
                "snippets": "Although SoPro mitigates egocentric bias in GPT-3.5 and verbosity bias in GPT-4, the models' susceptibility to bandwagon effect increased. SoPro, which aligns LLMs with others' perspectives, may not be effective for cognitive biases. This result is inconsistent with the claim of the study for humans. AwaRe mitigates order bias, compassion fade, egocentric bias, bandwagon effect, and attentional bias in GPT-3.5, and bandwagon effect and verbosity bias in GPT-4. This result suggests that AwaRe prompts LLMs to make rational judgments.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Results",
                        "pdf_hash": "",
                        "start": 974,
                        "end": 1511,
                        "sentence_offsets": [
                            {
                                "start": 974,
                                "end": 1112
                            },
                            {
                                "start": 1113,
                                "end": 1207
                            },
                            {
                                "start": 1208,
                                "end": 1275
                            },
                            {
                                "start": 1276,
                                "end": 1438
                            },
                            {
                                "start": 1439,
                                "end": 1511
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Although SoPro mitigates egocentric bias in GPT-3.5 and verbosity bias in GPT-4, the models' susceptibility to bandwagon effect increased. SoPro, which aligns LLMs with others' perspectives, may not be effective for cognitive biases. This result is inconsistent with the claim of the study for humans. AwaRe mitigates order bias, compassion fade, egocentric bias, bandwagon effect, and attentional bias in GPT-3.5, and bandwagon effect and verbosity bias in GPT-4. This result suggests that AwaRe prompts LLMs to make rational judgments."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[276106991 | Li et al. | 2025 | Citations: 29]",
                "snippets": "Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive and real-world problem that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios...Using the MTBench (Zheng et al., 2023) dataset, which includes pairwise comparison judgments from both humans and GPT-4, we compare GPT-4's and human evaluators' judgments on LLaMA-2 vs other models (including Vicuna, Alpaca, GPT-3.5, and GPT-4, which are preferred by GPT-4 due to preference leakage or egocentric bias) and LLaMA-2 vs Claude. The results, shown in Figure 4, reveal a clear preference for LLaMA-2 by GPT-4. Consequently, we conclude that evaluators' bias can be inherited. In this case, GPT-4's bias toward LLaMA has been passed on to LLaMA's student models. This inheritance, combined with the opaque training data of LLMs, makes preference leakage a more complex and challenging problem.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259129398 | Zheng et al. | 2023 | Citations: 4439]": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge."
                },
                "metadata": [
                    {
                        "quote": "Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive and real-world problem that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "Main Results",
                        "pdf_hash": "",
                        "start": 850,
                        "end": 1557,
                        "sentence_offsets": [
                            {
                                "start": 850,
                                "end": 1193
                            },
                            {
                                "start": 1194,
                                "end": 1273
                            },
                            {
                                "start": 1274,
                                "end": 1339
                            },
                            {
                                "start": 1340,
                                "end": 1425
                            },
                            {
                                "start": 1426,
                                "end": 1556
                            }
                        ],
                        "ref_mentions": [
                            "259129398"
                        ],
                        "quote": "Using the MTBench (Zheng et al., 2023) dataset, which includes pairwise comparison judgments from both humans and GPT-4, we compare GPT-4's and human evaluators' judgments on LLaMA-2 vs other models (including Vicuna, Alpaca, GPT-3.5, and GPT-4, which are preferred by GPT-4 due to preference leakage or egocentric bias) and LLaMA-2 vs Claude. The results, shown in Figure 4, reveal a clear preference for LLaMA-2 by GPT-4. Consequently, we conclude that evaluators' bias can be inherited. In this case, GPT-4's bias toward LLaMA has been passed on to LLaMA's student models. This inheritance, combined with the opaque training data of LLMs, makes preference leakage a more complex and challenging problem."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[276885275 | Krumdick et al. | 2025 | Citations: 6]",
                "snippets": "One common judgment bias is self-preference, where a model tends to overrate its own responses. To quantify this bias we computed the false positive rate (FPR) for each judge when evaluating its own responses versus all other models. In this context, the FPR represents the rate at which the model erroneously labels incorrect responses as correct. We also evaluated the false negative rate (FNR), which is the rate at which the model erroneously labels correct responses as incorrect. A model with a strong selfpreference bias would exhibit a high FPR and a low FNR when grading its own references. Figure 4 displays the FPR and FNR aggregated over each judge with the \"Wrong\", \"Random\", \"Self\", \"None\", and \"Human\" reference types (Metrics per judge can be found in Figure 5). For every reference type, we see that on average models have a higher FPR when grading their own responses. The gap is particularly large when the model is provided with its own generated reference or no reference at all. Thus, providing a human reference reduces both the overall rate of error and the relative difference between a model's judgment of its own responses and those of others.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Error Analysis",
                        "pdf_hash": "",
                        "start": 122,
                        "end": 1292,
                        "sentence_offsets": [
                            {
                                "start": 122,
                                "end": 217
                            },
                            {
                                "start": 218,
                                "end": 355
                            },
                            {
                                "start": 356,
                                "end": 470
                            },
                            {
                                "start": 471,
                                "end": 607
                            },
                            {
                                "start": 608,
                                "end": 721
                            },
                            {
                                "start": 722,
                                "end": 900
                            },
                            {
                                "start": 901,
                                "end": 1008
                            },
                            {
                                "start": 1009,
                                "end": 1122
                            },
                            {
                                "start": 1123,
                                "end": 1292
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "One common judgment bias is self-preference, where a model tends to overrate its own responses. To quantify this bias we computed the false positive rate (FPR) for each judge when evaluating its own responses versus all other models. In this context, the FPR represents the rate at which the model erroneously labels incorrect responses as correct. We also evaluated the false negative rate (FNR), which is the rate at which the model erroneously labels correct responses as incorrect. A model with a strong selfpreference bias would exhibit a high FPR and a low FNR when grading its own references. Figure 4 displays the FPR and FNR aggregated over each judge with the \"Wrong\", \"Random\", \"Self\", \"None\", and \"Human\" reference types (Metrics per judge can be found in Figure 5). For every reference type, we see that on average models have a higher FPR when grading their own responses. The gap is particularly large when the model is provided with its own generated reference or no reference at all. Thus, providing a human reference reduces both the overall rate of error and the relative difference between a model's judgment of its own responses and those of others."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[277621852 | Eldifrawi et al. | 2025 | Citations: 0]",
                "snippets": "In their study, (Liu et al., 2023) identified a bias in evaluators, where they tend to favor their own model's generations over those from other models, even when the latter are objectively better. (Ohi et al., 2024) introduced a method for detecting this bias, which they termed 'Likelihood-based Evaluation Bias.' However, this approach requires access to the probability distribution of the LLM's generations, which is often unavailable, especially when working with commercial LLMs. (Ye et al., 2024) also addressed this issue, referring to it as 'egocentric Bias,' and we adopt this terminology in our work. Their research primarily focuses on understanding the effects of this bias on performance and strategies for mitigating it...It is worth noting that LLMs as evaluators tend to exhibit higher correlation with human annotations and lower egocentric bias when the evaluation criteria are more detailed.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[257804696 | Liu et al. | 2023 | Citations: 1211]": "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval",
                    "[267938572 | Ohi et al. | 2024 | Citations: 4]": "Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics. However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods. In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators. We also propose a method to mitigate the likelihood bias. Our method utilizes highly biased instances as few-shot examples for in-context learning. Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias. Furthermore, our proposed method successfully mitigates this bias, also improving evaluation performance (in terms of correlation of models with human scores) significantly.",
                    "[273098639 | Ye et al. | 2024 | Citations: 78]": "LLM-as-a-Judge has been widely utilized as an evaluation method in various benchmarks and served as supervised rewards in model training. However, despite their excellence in many domains, potential issues are under-explored, undermining their reliability and the scope of their utility. Therefore, we identify 12 key potential biases and propose a new automated bias quantification framework-CALM-which systematically quantifies and analyzes each type of bias in LLM-as-a-Judge by using automated and principle-guided modification. Our experiments cover multiple popular language models, and the results indicate that while advanced models have achieved commendable overall performance, significant biases persist in certain specific tasks. Empirical results suggest that there remains room for improvement in the reliability of LLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence of these biases and give some suggestions for the reliable application of LLM-as-a-Judge. Our work highlights the need for stakeholders to address these issues and remind users to exercise caution in LLM-as-a-Judge applications."
                },
                "metadata": [
                    {
                        "section_title": "Experiment 3: Ego-Centric Bias Analysis",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 732,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 196
                            },
                            {
                                "start": 197,
                                "end": 313
                            },
                            {
                                "start": 314,
                                "end": 484
                            },
                            {
                                "start": 485,
                                "end": 609
                            },
                            {
                                "start": 610,
                                "end": 733
                            }
                        ],
                        "ref_mentions": [
                            "257804696",
                            "267938572",
                            "273098639"
                        ],
                        "quote": "In their study, (Liu et al., 2023) identified a bias in evaluators, where they tend to favor their own model's generations over those from other models, even when the latter are objectively better. (Ohi et al., 2024) introduced a method for detecting this bias, which they termed 'Likelihood-based Evaluation Bias.' However, this approach requires access to the probability distribution of the LLM's generations, which is often unavailable, especially when working with commercial LLMs. (Ye et al., 2024) also addressed this issue, referring to it as 'egocentric Bias,' and we adopt this terminology in our work. Their research primarily focuses on understanding the effects of this bias on performance and strategies for mitigating it"
                    },
                    {
                        "section_title": "Experiment 3: Ego-Centric Bias Analysis",
                        "pdf_hash": "",
                        "start": 1971,
                        "end": 2147,
                        "sentence_offsets": [
                            {
                                "start": 1971,
                                "end": 2145
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "It is worth noting that LLMs as evaluators tend to exhibit higher correlation with human annotations and lower egocentric bias when the evaluation criteria are more detailed."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.023601,
        "cot": "For this query about risks and mitigation strategies for egocentric bias when using GPT-4 to evaluate its own generated data, I need to organize the content into meaningful dimensions that will help answer the user's question comprehensively.\n\n1. \"Introduction to Egocentric Bias in LLMs\" - This will be a synthesis section providing background on what egocentric bias is in the context of GPT-4 and large language models. This is essential for setting the stage and ensuring the user understands the core concept being discussed.\n\n2. \"Manifestations of Egocentric Bias in GPT-4\" - This will be a synthesis section detailing how egocentric bias manifests specifically in GPT-4 when evaluating its own outputs. Several quotes discuss different ways this bias appears, so synthesizing these will give a comprehensive picture of the problem.\n\n3. \"Risks of Egocentric Bias\" - This will be a synthesis section that addresses the first part of the user's query about risks. The quotes discuss various dangers of egocentric bias, from skewed evaluations to self-reinforcement problems.\n\n4. \"Mitigation Strategies: Evaluation Design Approaches\" - This will be a list format section focusing on technical/methodological mitigation strategies mentioned in the papers. This format will make it easier for the user to identify specific techniques they might implement.\n\n5. \"Mitigation Strategies: Alternative Evaluator Approaches\" - This will be a list format section covering mitigation approaches that involve using different evaluators or combining evaluators. Again, a list format makes these distinct approaches more accessible.\n\n6. \"Limitations of Current Mitigation Approaches\" - This will be a synthesis section discussing the challenges and limitations of the mitigation strategies presented. This helps provide a balanced view of the solutions.",
        "plan": {
            "Introduction to Egocentric Bias in LLMs (synthesis)": [
                2,
                4,
                8
            ],
            "Manifestations of Egocentric Bias in GPT-4 (synthesis)": [
                0,
                4,
                7,
                9,
                10,
                13,
                14,
                15
            ],
            "Risks of Egocentric Bias (synthesis)": [
                1,
                6,
                7,
                8,
                9,
                11,
                13
            ],
            "Mitigation Strategies: Evaluation Design Approaches (list)": [
                0,
                2,
                3,
                5,
                12,
                14,
                15
            ],
            "Mitigation Strategies: Alternative Evaluator Approaches (list)": [
                1,
                5,
                6,
                8,
                11
            ],
            "Limitations of Current Mitigation Approaches (synthesis)": [
                6,
                7,
                8,
                12
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Egocentric Bias in LLMs",
                "tldr": "Egocentric bias in large language models refers to their tendency to prefer their own responses over others when evaluating outputs. This self-preference manifests when models like GPT-4 rate their own generations higher than comparable or even superior responses from other sources. (4 sources)",
                "text": "\nEgocentric bias, originating from cognitive psychology, refers to the tendency to have a higher opinion of oneself or to more readily accept ideas that align with one's own perspectives. In the context of large language models (LLMs), this bias manifests when models evaluate their own outputs more favorably than those generated by other systems. <Paper corpusId=\"263310448\" paperTitle=\"(Koo et al., 2023)\" isShortName></Paper>\n\nThis phenomenon creates significant challenges for LLM evaluation, particularly when using models to assess their own performance. As Li et al. note, \"The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator).\" <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> This creates a circular problem similar to a \"chicken-and-egg dilemma,\" where developing better models requires reliable evaluation, yet the evaluators themselves may be biased.\n\nRecent research has documented this behavior in prominent models. For instance, GPT-4V has been observed to exhibit \"a slight degree of Egocentricity\" when evaluating responses, showing a preference for its own outputs that align with its predefined ethical guidelines and training. <Paper corpusId=\"267523079\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper> This bias persists despite efforts to use prompt engineering for neutrality, as these models still rely on judgment criteria established during their post-alignment training. <Paper corpusId=\"267523079\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246426909\" paperTitle=\"(Ouyang et al., 2022)\" isShortName></Paper>\n\nIt's important to note that not all preference for one's own outputs constitutes bias. As researchers point out, \"some models would naturally generate higher quality responses (e.g., GPT4 vs. KOALA), resulting in a stronger inclination for such evaluators to choose their own responses.\" <Paper corpusId=\"263310448\" paperTitle=\"(Koo et al., 2023)\" isShortName></Paper> The challenge lies in distinguishing genuine quality differences from self-preferential bias.",
                "citations": [
                    {
                        "id": "(Koo et al., 2023)",
                        "snippets": [
                            "Egocentric Bias (Self-Preference). (Ross and Sicoly, 1979) is a cognitive bias that refers to the tendency to have a higher opinion of oneself or to more easily accept ideas if they match one's own. We define an evaluator to be egocentrically biased if, for each instance, the evaluator prefers its own response over others. We note that an unbiased evaluator would choose between themselves and other comparand models equally in proportion. However, we highlight that some models would naturally generate higher quality responses (e.g., GPT4 vs. KOALA), resulting in a stronger inclination for such evaluators to choose their own responses.\n\nWe employ various strategies to mitigate these confounding variables and isolate each analysis as much as possible. For example, we employ a \"hierarchical\" rubric, where some biases take priority in an evaluation. Specifically, if an evaluation shows signs of order bias by choosing A in (A first, then B) and B in (B first, then A), we do not evaluate it for SALIENCE or EGOCEN-TRIC bias."
                        ],
                        "paper": {
                            "corpus_id": 263310448,
                            "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                            "authors": [
                                {
                                    "authorId": "2213239540",
                                    "name": "Ryan Koo"
                                },
                                {
                                    "authorId": "2187932371",
                                    "name": "Minhwa Lee"
                                },
                                {
                                    "authorId": "2831377",
                                    "name": "Vipul Raheja"
                                },
                                {
                                    "authorId": "2294310015",
                                    "name": "Jong Inn Park"
                                },
                                {
                                    "authorId": "2894340",
                                    "name": "Zae Myung Kim"
                                },
                                {
                                    "authorId": "48493368",
                                    "name": "Dongyeop Kang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 86
                        },
                        "score": 0.79541015625
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator). This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023). This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator. To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022; Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023; Ouyang et al., 2022) to ensure more balanced and comprehensive assessments."
                        ],
                        "paper": {
                            "corpus_id": 270391675,
                            "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
                            "authors": [
                                {
                                    "authorId": "2145256331",
                                    "name": "Zhen Li"
                                },
                                {
                                    "authorId": "2279658967",
                                    "name": "Xiaohan Xu"
                                },
                                {
                                    "authorId": "2279548827",
                                    "name": "Tao Shen"
                                },
                                {
                                    "authorId": "2284826718",
                                    "name": "Can Xu"
                                },
                                {
                                    "authorId": "2308241851",
                                    "name": "Jia-Chen Gu"
                                },
                                {
                                    "authorId": "2308073132",
                                    "name": "Yuxuan Lai"
                                },
                                {
                                    "authorId": "2287928517",
                                    "name": "Chongyang Tao"
                                },
                                {
                                    "authorId": "2307142498",
                                    "name": "Shuai Ma"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 15
                        },
                        "score": 0.6328125
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "Egocentric Bias. Models tend to assign higher scores to their own responses while scoring others lower (Zheng et al., 2023b;Li et al., 2024). In Figures 19 and 20, GPT-4V exhibits a slight degree of Egocentricity. Conversely, Gemini maintains a uniform scoring distribution across different sources, demonstrating a more equitable approach to judgment. In contrast, GPT-4V shows self-preference, aligning its judgments with its predefined ethical guidelines. For example, GPT-4V consistently emphasizes privacy preservation, leading to higher scores for privacy-related questions based on its own metrics. Despite efforts in prompt engineering to ensure neutrality, these models still rely on judgment criteria set during post-alignment training (Ouyang et al., 2022). This bias can result in judgments that deviate from human preferences, highlighting the complexity of aligning MLLM judgments with humans'."
                        ],
                        "paper": {
                            "corpus_id": 267523079,
                            "title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                            "authors": [
                                {
                                    "authorId": "2279219833",
                                    "name": "Dongping Chen"
                                },
                                {
                                    "authorId": "2283244952",
                                    "name": "Ruoxi Chen"
                                },
                                {
                                    "authorId": "2283311181",
                                    "name": "Shilin Zhang"
                                },
                                {
                                    "authorId": "2283150665",
                                    "name": "Yinuo Liu"
                                },
                                {
                                    "authorId": "2283314623",
                                    "name": "Yaochen Wang"
                                },
                                {
                                    "authorId": "2283313383",
                                    "name": "Huichi Zhou"
                                },
                                {
                                    "authorId": "46324457",
                                    "name": "Qihui Zhang"
                                },
                                {
                                    "authorId": "2221116622",
                                    "name": "Pan Zhou"
                                },
                                {
                                    "authorId": "2254266993",
                                    "name": "Yao Wan"
                                },
                                {
                                    "authorId": "2267508610",
                                    "name": "Lichao Sun"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 122
                        },
                        "score": 0.84375
                    },
                    {
                        "id": "(Ouyang et al., 2022)",
                        "snippets": [
                            "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
                        ],
                        "paper": {
                            "corpus_id": 246426909,
                            "title": "Training language models to follow instructions with human feedback",
                            "authors": [
                                {
                                    "authorId": "31793034",
                                    "name": "Long Ouyang"
                                },
                                {
                                    "authorId": "49387725",
                                    "name": "Jeff Wu"
                                },
                                {
                                    "authorId": "2115903168",
                                    "name": "Xu Jiang"
                                },
                                {
                                    "authorId": "2061137049",
                                    "name": "Diogo Almeida"
                                },
                                {
                                    "authorId": "2064084601",
                                    "name": "Carroll L. Wainwright"
                                },
                                {
                                    "authorId": "2051714782",
                                    "name": "Pamela Mishkin"
                                },
                                {
                                    "authorId": null,
                                    "name": "Chong Zhang"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "2117680841",
                                    "name": "Katarina Slama"
                                },
                                {
                                    "authorId": "2064770039",
                                    "name": "Alex Ray"
                                },
                                {
                                    "authorId": "47971768",
                                    "name": "John Schulman"
                                },
                                {
                                    "authorId": "2052366271",
                                    "name": "Jacob Hilton"
                                },
                                {
                                    "authorId": "2151735262",
                                    "name": "Fraser Kelton"
                                },
                                {
                                    "authorId": "2142365973",
                                    "name": "Luke E. Miller"
                                },
                                {
                                    "authorId": "2151735251",
                                    "name": "Maddie Simens"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "2930640",
                                    "name": "P. Welinder"
                                },
                                {
                                    "authorId": "145791315",
                                    "name": "P. Christiano"
                                },
                                {
                                    "authorId": "2990741",
                                    "name": "Jan Leike"
                                },
                                {
                                    "authorId": "49407415",
                                    "name": "Ryan J. Lowe"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 13203
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Manifestations of Egocentric Bias in GPT-4",
                "tldr": "GPT-4 exhibits egocentric bias through several observable patterns including positional preferences, self-rating inflation, and a tendency to align evaluations with its own ethical guidelines and training. These manifestations can significantly impact the reliability of self-evaluation, especially in specialized domains like healthcare. (9 sources)",
                "text": "\nGPT-4's egocentric bias manifests in several distinct patterns. One prominent form is positional bias, where the model consistently assigns higher scores to responses presented in certain positions. Wang et al. demonstrated that \"GPT-4 exhibits a preference for the first displayed candidate response by consistently assigning it higher scores, even when the order of candidates is subtly altered,\" and showed that \"merely swapping the presentation order can reverse evaluation outcomes.\" <Paper corpusId=\"258960339\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>\n\nBeyond positional preferences, GPT-4 demonstrates self-preference by rating its own responses more favorably than comparable outputs from other models. This is particularly evident in evaluations involving ethical considerations, where \"GPT-4V shows self-preference, aligning its judgments with its predefined ethical guidelines.\" <Paper corpusId=\"267523079\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper> This alignment with its own training parameters occurs \"despite efforts in prompt engineering to ensure neutrality,\" as the model continues to rely on \"judgment criteria set during post-alignment training.\" <Paper corpusId=\"267523079\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246426909\" paperTitle=\"(Ouyang et al., 2022)\" isShortName></Paper>\n\nQuantitative analysis confirms this self-preference tendency. When evaluating its own responses versus those of other models, GPT-4 shows a higher false positive rate (FPR) \u2013 \"erroneously labeling incorrect responses as correct\" \u2013 particularly when \"provided with its own generated reference or no reference at all.\" <Paper corpusId=\"276885275\" paperTitle=\"(Krumdick et al., 2025)\" isShortName></Paper> This bias extends beyond individual model evaluations to a phenomenon termed \"preference leakage,\" where the model's bias can be \"inherited\" by student models trained on its outputs. <Paper corpusId=\"276106991\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>\n\nIn specialized domains like healthcare, this bias can have significant consequences. For example, when GPT-4 is used to generate medical content, it may \"inadequately represent demographic diversity in medical conditions\" and generate differential diagnoses that \"reflect biases associated with race, ethnicity, and gender.\" <Paper corpusId=\"270614044\" paperTitle=\"(Bragazzi et al., 2023)\" isShortName></Paper> This highlights how egocentric bias can compound existing societal biases embedded in the model's training data.\n\nResearch also suggests that the specificity of evaluation criteria influences the degree of bias. LLMs \"tend to exhibit higher correlation with human annotations and lower egocentric bias when the evaluation criteria are more detailed.\" <Paper corpusId=\"277621852\" paperTitle=\"(Eldifrawi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"273098639\" paperTitle=\"(Ye et al., 2024)\" isShortName></Paper> However, even with improved evaluation protocols, researchers still advise \"approaching results produced by GPT-4 with a certain level of skepticism,\" particularly when the model is generating and evaluating its own outputs. <Paper corpusId=\"272337179\" paperTitle=\"(Ji et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "Specifically, we demonstrate that GPT-4 exhibits a preference for the first displayed candidate response by consistently assigning it higher scores, even when the order of candidates is subtly altered. As illustrated in Figure 1, merely swapping the presentation order can reverse evaluation outcomes. This bias is also present in Chat-GPT, which typically favors the second response. These findings highlight previously overlooked limitations in the current evaluation paradigm.\n\nTo address this issue, we propose three simple yet effective strategies to calibrate positional bias: 1) Multiple Evidence Calibration (MEC): We prompt the model to generate evaluation evidence before assigning scores, leveraging the inherent properties of causal language models for calibration. We also employ ensemble techniques to incorporate multiple evidence calibration results to further stabilize the evaluation. 2) Balanced Position Calibration (BPC): To further reduce positional bias, we evaluate each candidate in both positions across two runs and compute the final score as the average of the two runs. 3) Human In The Loop Calibration (HITLC): We also explore human-inthe-loop evaluation and consider a diversity-based method to get a cue to indicate biased candidates based on the evaluation results of MEC and BPC."
                        ],
                        "paper": {
                            "corpus_id": 258960339,
                            "title": "Large Language Models are not Fair Evaluators",
                            "authors": [
                                {
                                    "authorId": "144202874",
                                    "name": "Peiyi Wang"
                                },
                                {
                                    "authorId": "49192881",
                                    "name": "Lei Li"
                                },
                                {
                                    "authorId": "2146034504",
                                    "name": "Liang Chen"
                                },
                                {
                                    "authorId": "2116276849",
                                    "name": "Dawei Zhu"
                                },
                                {
                                    "authorId": "3186130",
                                    "name": "Binghuai Lin"
                                },
                                {
                                    "authorId": "2154235",
                                    "name": "Yunbo Cao"
                                },
                                {
                                    "authorId": "2144831944",
                                    "name": "Qi Liu"
                                },
                                {
                                    "authorId": "1701889",
                                    "name": "Tianyu Liu"
                                },
                                {
                                    "authorId": "3335836",
                                    "name": "Zhifang Sui"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 573
                        },
                        "score": 0.62353515625
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "Egocentric Bias. Models tend to assign higher scores to their own responses while scoring others lower (Zheng et al., 2023b;Li et al., 2024). In Figures 19 and 20, GPT-4V exhibits a slight degree of Egocentricity. Conversely, Gemini maintains a uniform scoring distribution across different sources, demonstrating a more equitable approach to judgment. In contrast, GPT-4V shows self-preference, aligning its judgments with its predefined ethical guidelines. For example, GPT-4V consistently emphasizes privacy preservation, leading to higher scores for privacy-related questions based on its own metrics. Despite efforts in prompt engineering to ensure neutrality, these models still rely on judgment criteria set during post-alignment training (Ouyang et al., 2022). This bias can result in judgments that deviate from human preferences, highlighting the complexity of aligning MLLM judgments with humans'."
                        ],
                        "paper": {
                            "corpus_id": 267523079,
                            "title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark",
                            "authors": [
                                {
                                    "authorId": "2279219833",
                                    "name": "Dongping Chen"
                                },
                                {
                                    "authorId": "2283244952",
                                    "name": "Ruoxi Chen"
                                },
                                {
                                    "authorId": "2283311181",
                                    "name": "Shilin Zhang"
                                },
                                {
                                    "authorId": "2283150665",
                                    "name": "Yinuo Liu"
                                },
                                {
                                    "authorId": "2283314623",
                                    "name": "Yaochen Wang"
                                },
                                {
                                    "authorId": "2283313383",
                                    "name": "Huichi Zhou"
                                },
                                {
                                    "authorId": "46324457",
                                    "name": "Qihui Zhang"
                                },
                                {
                                    "authorId": "2221116622",
                                    "name": "Pan Zhou"
                                },
                                {
                                    "authorId": "2254266993",
                                    "name": "Yao Wan"
                                },
                                {
                                    "authorId": "2267508610",
                                    "name": "Lichao Sun"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 122
                        },
                        "score": 0.84375
                    },
                    {
                        "id": "(Ouyang et al., 2022)",
                        "snippets": [
                            "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
                        ],
                        "paper": {
                            "corpus_id": 246426909,
                            "title": "Training language models to follow instructions with human feedback",
                            "authors": [
                                {
                                    "authorId": "31793034",
                                    "name": "Long Ouyang"
                                },
                                {
                                    "authorId": "49387725",
                                    "name": "Jeff Wu"
                                },
                                {
                                    "authorId": "2115903168",
                                    "name": "Xu Jiang"
                                },
                                {
                                    "authorId": "2061137049",
                                    "name": "Diogo Almeida"
                                },
                                {
                                    "authorId": "2064084601",
                                    "name": "Carroll L. Wainwright"
                                },
                                {
                                    "authorId": "2051714782",
                                    "name": "Pamela Mishkin"
                                },
                                {
                                    "authorId": null,
                                    "name": "Chong Zhang"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "2117680841",
                                    "name": "Katarina Slama"
                                },
                                {
                                    "authorId": "2064770039",
                                    "name": "Alex Ray"
                                },
                                {
                                    "authorId": "47971768",
                                    "name": "John Schulman"
                                },
                                {
                                    "authorId": "2052366271",
                                    "name": "Jacob Hilton"
                                },
                                {
                                    "authorId": "2151735262",
                                    "name": "Fraser Kelton"
                                },
                                {
                                    "authorId": "2142365973",
                                    "name": "Luke E. Miller"
                                },
                                {
                                    "authorId": "2151735251",
                                    "name": "Maddie Simens"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "2930640",
                                    "name": "P. Welinder"
                                },
                                {
                                    "authorId": "145791315",
                                    "name": "P. Christiano"
                                },
                                {
                                    "authorId": "2990741",
                                    "name": "Jan Leike"
                                },
                                {
                                    "authorId": "49407415",
                                    "name": "Ryan J. Lowe"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 13203
                        },
                        "score": 0
                    },
                    {
                        "id": "(Krumdick et al., 2025)",
                        "snippets": [
                            "One common judgment bias is self-preference, where a model tends to overrate its own responses. To quantify this bias we computed the false positive rate (FPR) for each judge when evaluating its own responses versus all other models. In this context, the FPR represents the rate at which the model erroneously labels incorrect responses as correct. We also evaluated the false negative rate (FNR), which is the rate at which the model erroneously labels correct responses as incorrect. A model with a strong selfpreference bias would exhibit a high FPR and a low FNR when grading its own references. Figure 4 displays the FPR and FNR aggregated over each judge with the \"Wrong\", \"Random\", \"Self\", \"None\", and \"Human\" reference types (Metrics per judge can be found in Figure 5). For every reference type, we see that on average models have a higher FPR when grading their own responses. The gap is particularly large when the model is provided with its own generated reference or no reference at all. Thus, providing a human reference reduces both the overall rate of error and the relative difference between a model's judgment of its own responses and those of others."
                        ],
                        "paper": {
                            "corpus_id": 276885275,
                            "title": "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding",
                            "authors": [
                                {
                                    "authorId": "18171842",
                                    "name": "Michael Krumdick"
                                },
                                {
                                    "authorId": "2307472942",
                                    "name": "Charles Lovering"
                                },
                                {
                                    "authorId": "2266430123",
                                    "name": "Varshini Reddy"
                                },
                                {
                                    "authorId": "78150202",
                                    "name": "Seth Ebner"
                                },
                                {
                                    "authorId": "2266398345",
                                    "name": "Chris Tanner"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.599609375
                    },
                    {
                        "id": "(Li et al., 2025)",
                        "snippets": [
                            "Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive and real-world problem that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios",
                            "Using the MTBench (Zheng et al., 2023) dataset, which includes pairwise comparison judgments from both humans and GPT-4, we compare GPT-4's and human evaluators' judgments on LLaMA-2 vs other models (including Vicuna, Alpaca, GPT-3.5, and GPT-4, which are preferred by GPT-4 due to preference leakage or egocentric bias) and LLaMA-2 vs Claude. The results, shown in Figure 4, reveal a clear preference for LLaMA-2 by GPT-4. Consequently, we conclude that evaluators' bias can be inherited. In this case, GPT-4's bias toward LLaMA has been passed on to LLaMA's student models. This inheritance, combined with the opaque training data of LLMs, makes preference leakage a more complex and challenging problem."
                        ],
                        "paper": {
                            "corpus_id": 276106991,
                            "title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge",
                            "authors": [
                                {
                                    "authorId": "2161635474",
                                    "name": "Dawei Li"
                                },
                                {
                                    "authorId": "2344419674",
                                    "name": "Renliang Sun"
                                },
                                {
                                    "authorId": "2324070910",
                                    "name": "Yue Huang"
                                },
                                {
                                    "authorId": "2316709408",
                                    "name": "Ming Zhong"
                                },
                                {
                                    "authorId": "2036355404",
                                    "name": "Bohan Jiang"
                                },
                                {
                                    "authorId": "2343853966",
                                    "name": "Jiawei Han"
                                },
                                {
                                    "authorId": "2307963162",
                                    "name": "Xiangliang Zhang"
                                },
                                {
                                    "authorId": "2343828587",
                                    "name": "Wei Wang"
                                },
                                {
                                    "authorId": "2287545693",
                                    "name": "Huan Liu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 29
                        },
                        "score": 0.50439453125
                    },
                    {
                        "id": "(Bragazzi et al., 2023)",
                        "snippets": [
                            "A further risk is that LLMs might reinforce existing biases and provide inaccurate medical diagnoses, potentially leading to detrimental effects on health care.Zack et al [26] aimed to evaluate whether GPT-4 harbors biases that could influence its application in health care settings.Using the Azure OpenAI interface, the authors scrutinized GPT-4 for racial and gender biases and assessed the impact of such biases on four clinical applications of LLMs-(1) medical education, (2) diagnostic reasoning, (3) development and implementation of clinical plans, and (4) subjective patient evaluations-involving experiments using prompts mimicking typical GPT-4 use in clinical and medical educational settings and drawing from New England Journal of Medicine Healer clinical vignettes and research on implicit bias in health care.The study compared GPT-4's estimates of demographic distributions of medical conditions against actual US prevalence data.For differential diagnosis and treatment planning, the research analyzed variations across demographic groups using standard statistical methods to identify significant differences.The study revealed that GPT-4 inadequately represents demographic diversity in medical conditions, often resorting to stereotypical demographic portrayals in clinical vignettes.The differential diagnoses generated by GPT-4 for standardized clinical vignettes tended to reflect biases associated with race, ethnicity, and gender.Furthermore, the model's assessments and plans demonstrated a notable correlation between demographic characteristics and recommendations for costlier procedures, as well as varied perceptions of patients."
                        ],
                        "paper": {
                            "corpus_id": 270614044,
                            "title": "Toward Clinical Generative AI: Conceptual Framework",
                            "authors": [
                                {
                                    "authorId": "2250832100",
                                    "name": "N. Bragazzi"
                                },
                                {
                                    "authorId": "2277106726",
                                    "name": "Sergio Garbarino"
                                }
                            ],
                            "year": 2023,
                            "venue": "JMIR AI",
                            "n_citations": 13
                        },
                        "score": 0.51806640625
                    },
                    {
                        "id": "(Eldifrawi et al., 2025)",
                        "snippets": [
                            "In their study, (Liu et al., 2023) identified a bias in evaluators, where they tend to favor their own model's generations over those from other models, even when the latter are objectively better. (Ohi et al., 2024) introduced a method for detecting this bias, which they termed 'Likelihood-based Evaluation Bias.' However, this approach requires access to the probability distribution of the LLM's generations, which is often unavailable, especially when working with commercial LLMs. (Ye et al., 2024) also addressed this issue, referring to it as 'egocentric Bias,' and we adopt this terminology in our work. Their research primarily focuses on understanding the effects of this bias on performance and strategies for mitigating it",
                            "It is worth noting that LLMs as evaluators tend to exhibit higher correlation with human annotations and lower egocentric bias when the evaluation criteria are more detailed."
                        ],
                        "paper": {
                            "corpus_id": 277621852,
                            "title": "FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking",
                            "authors": [
                                {
                                    "authorId": "2126996290",
                                    "name": "Islam Eldifrawi"
                                },
                                {
                                    "authorId": "2311878157",
                                    "name": "Shengrui Wang"
                                },
                                {
                                    "authorId": "2311887008",
                                    "name": "Amine Trabelsi"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.85986328125
                    },
                    {
                        "id": "(Ye et al., 2024)",
                        "snippets": [
                            "LLM-as-a-Judge has been widely utilized as an evaluation method in various benchmarks and served as supervised rewards in model training. However, despite their excellence in many domains, potential issues are under-explored, undermining their reliability and the scope of their utility. Therefore, we identify 12 key potential biases and propose a new automated bias quantification framework-CALM-which systematically quantifies and analyzes each type of bias in LLM-as-a-Judge by using automated and principle-guided modification. Our experiments cover multiple popular language models, and the results indicate that while advanced models have achieved commendable overall performance, significant biases persist in certain specific tasks. Empirical results suggest that there remains room for improvement in the reliability of LLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence of these biases and give some suggestions for the reliable application of LLM-as-a-Judge. Our work highlights the need for stakeholders to address these issues and remind users to exercise caution in LLM-as-a-Judge applications."
                        ],
                        "paper": {
                            "corpus_id": 273098639,
                            "title": "Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge",
                            "authors": [
                                {
                                    "authorId": "2325239327",
                                    "name": "Jiayi Ye"
                                },
                                {
                                    "authorId": "2324066105",
                                    "name": "Yanbo Wang"
                                },
                                {
                                    "authorId": "2324070910",
                                    "name": "Yue Huang"
                                },
                                {
                                    "authorId": "2279219833",
                                    "name": "Dongping Chen"
                                },
                                {
                                    "authorId": "46324457",
                                    "name": "Qihui Zhang"
                                },
                                {
                                    "authorId": "2241357425",
                                    "name": "Nuno Moniz"
                                },
                                {
                                    "authorId": "2324218620",
                                    "name": "Tian Gao"
                                },
                                {
                                    "authorId": "2324052753",
                                    "name": "Werner Geyer"
                                },
                                {
                                    "authorId": "2324171997",
                                    "name": "Chao Huang"
                                },
                                {
                                    "authorId": "2279077171",
                                    "name": "Pin-Yu Chen"
                                },
                                {
                                    "authorId": "2286872295",
                                    "name": "Nitesh V. Chawla"
                                },
                                {
                                    "authorId": "2307963162",
                                    "name": "Xiangliang Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 78
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ji et al., 2024)",
                        "snippets": [
                            "It is also important to note that most of our data are generated by gpt-4. Therefore, it is advisable to approach the results produced by GPT-4 with a certain level of skepticism."
                        ],
                        "paper": {
                            "corpus_id": 272337179,
                            "title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions",
                            "authors": [
                                {
                                    "authorId": "3391272",
                                    "name": "Ziwei Ji"
                                },
                                {
                                    "authorId": "2287917733",
                                    "name": "Tiezheng Yu"
                                },
                                {
                                    "authorId": "2285265406",
                                    "name": "Yan Xu"
                                },
                                {
                                    "authorId": "2314592345",
                                    "name": "Nayeon Lee"
                                },
                                {
                                    "authorId": "2278435713",
                                    "name": "Albert Q. Jiang"
                                },
                                {
                                    "authorId": "2256994781",
                                    "name": "Alexandre Sablayrolles"
                                },
                                {
                                    "authorId": "2319226386",
                                    "name": "Arthur Men-655"
                                },
                                {
                                    "authorId": "2256994975",
                                    "name": "Chris Bamford"
                                },
                                {
                                    "authorId": "2302815701",
                                    "name": "Devendra Singh"
                                },
                                {
                                    "authorId": "2302809975",
                                    "name": "Diego Chaplot"
                                },
                                {
                                    "authorId": "2318358390",
                                    "name": "laume Lample"
                                },
                                {
                                    "authorId": "2318350171",
                                    "name": "L\u00e9lio Lucile Saulnier"
                                },
                                {
                                    "authorId": "2318358843",
                                    "name": "Renard Lavaud"
                                },
                                {
                                    "authorId": "114952298",
                                    "name": "M. Lachaux"
                                },
                                {
                                    "authorId": "2256994779",
                                    "name": "Pierre Stock"
                                },
                                {
                                    "authorId": "1379806208",
                                    "name": "Teven Le Scao"
                                },
                                {
                                    "authorId": "2319259930",
                                    "name": "Jerry Kang"
                                },
                                {
                                    "authorId": "2319227308",
                                    "name": "Mark W. Bennett"
                                },
                                {
                                    "authorId": "2295905607",
                                    "name": "Devon Carbado"
                                },
                                {
                                    "authorId": "2319226400",
                                    "name": "Pam Casey"
                                },
                                {
                                    "authorId": "2310645453",
                                    "name": "P. Liang"
                                },
                                {
                                    "authorId": "2115397918",
                                    "name": "Chiyu Wu"
                                },
                                {
                                    "authorId": "49933077",
                                    "name": "Louis-philippe Morency"
                                },
                                {
                                    "authorId": "21626987",
                                    "name": "Aman Madaan"
                                },
                                {
                                    "authorId": "2261389843",
                                    "name": "Niket Tandon"
                                },
                                {
                                    "authorId": "2302821008",
                                    "name": "Prakhar Gupta"
                                },
                                {
                                    "authorId": null,
                                    "name": "Skyler Hallinan"
                                },
                                {
                                    "authorId": "2267242298",
                                    "name": "Luyu Gao"
                                },
                                {
                                    "authorId": "35823986",
                                    "name": "Sarah Wiegreffe"
                                },
                                {
                                    "authorId": "2268672727",
                                    "name": "Uri Alon"
                                },
                                {
                                    "authorId": "46217681",
                                    "name": "Nouha Dziri"
                                },
                                {
                                    "authorId": "9358910",
                                    "name": "Shrimai Prabhumoye"
                                },
                                {
                                    "authorId": "2315571964",
                                    "name": "Yiming Yang"
                                },
                                {
                                    "authorId": "2302819906",
                                    "name": "Shashank Gupta"
                                },
                                {
                                    "authorId": "3165738",
                                    "name": "Bodhisattwa Prasad Majumder"
                                },
                                {
                                    "authorId": "2273674137",
                                    "name": "Katherine Hermann"
                                },
                                {
                                    "authorId": "2129663",
                                    "name": "S. Welleck"
                                },
                                {
                                    "authorId": "80489277",
                                    "name": "Amir Yazdan Bakhsh"
                                },
                                {
                                    "authorId": "2319225091",
                                    "name": "ing Bao"
                                },
                                {
                                    "authorId": "2275251620",
                                    "name": "Mo Bavarian"
                                },
                                {
                                    "authorId": "2275245092",
                                    "name": "J. Belgum"
                                },
                                {
                                    "authorId": "2309476543",
                                    "name": "Ir-wan Bello"
                                },
                                {
                                    "authorId": "2275245414",
                                    "name": "Jake Berdine"
                                },
                                {
                                    "authorId": "2275245581",
                                    "name": "Gabriel Bernadett-Shapiro"
                                },
                                {
                                    "authorId": "133740015",
                                    "name": "Christopher Berner"
                                },
                                {
                                    "authorId": "2275251674",
                                    "name": "Lenny Bogdonoff"
                                },
                                {
                                    "authorId": "2275246071",
                                    "name": "Oleg Boiko"
                                },
                                {
                                    "authorId": "2275248137",
                                    "name": "Made-laine Boyd"
                                },
                                {
                                    "authorId": "2275245419",
                                    "name": "Anna-Luisa Brakman"
                                },
                                {
                                    "authorId": "2319225043",
                                    "name": "Greg Brock-724 man"
                                },
                                {
                                    "authorId": "2275219628",
                                    "name": "Tim Brooks"
                                },
                                {
                                    "authorId": "2265097787",
                                    "name": "M. Brundage"
                                },
                                {
                                    "authorId": "2146257251",
                                    "name": "Kevin Button"
                                },
                                {
                                    "authorId": "2275157286",
                                    "name": "Trevor Cai"
                                },
                                {
                                    "authorId": "2274782053",
                                    "name": "Rosie Campbell"
                                },
                                {
                                    "authorId": "2275245404",
                                    "name": "Andrew Cann"
                                },
                                {
                                    "authorId": "2275246368",
                                    "name": "Brittany Carey"
                                },
                                {
                                    "authorId": "2275120298",
                                    "name": "Chelsea Carlson"
                                },
                                {
                                    "authorId": "144114446",
                                    "name": "Rory Carmichael"
                                },
                                {
                                    "authorId": "1466431052",
                                    "name": "Brooke Chan"
                                },
                                {
                                    "authorId": "2275545855",
                                    "name": "Che Chang"
                                },
                                {
                                    "authorId": "2057091285",
                                    "name": "Fotis Chantzis"
                                },
                                {
                                    "authorId": "2253841704",
                                    "name": "Derek Chen"
                                },
                                {
                                    "authorId": "2256808607",
                                    "name": "Su-Hong Chen"
                                },
                                {
                                    "authorId": "2275179180",
                                    "name": "Ruby Chen"
                                },
                                {
                                    "authorId": "2275289833",
                                    "name": "Jason Chen"
                                },
                                {
                                    "authorId": "2108828435",
                                    "name": "Mark Chen"
                                },
                                {
                                    "authorId": "1490681878",
                                    "name": "Benjamin Chess"
                                },
                                {
                                    "authorId": "2275251158",
                                    "name": "Chester Cho"
                                },
                                {
                                    "authorId": "2309475703",
                                    "name": "Hyung Casey Chu"
                                },
                                {
                                    "authorId": "2282528643",
                                    "name": "Won Chung"
                                },
                                {
                                    "authorId": "2275231534",
                                    "name": "Dave Cummings"
                                },
                                {
                                    "authorId": "49645091",
                                    "name": "Jeremiah Currier"
                                },
                                {
                                    "authorId": "2276187456",
                                    "name": "Yunxing Dai"
                                },
                                {
                                    "authorId": "2309477435",
                                    "name": "Tarun Goel"
                                },
                                {
                                    "authorId": "2309477420",
                                    "name": "Gabriel Gogineni"
                                },
                                {
                                    "authorId": "2309475753",
                                    "name": "Rapha Goh"
                                },
                                {
                                    "authorId": "2319225953",
                                    "name": "Jonathan Gontijo-738 Lopes"
                                },
                                {
                                    "authorId": "2309478880",
                                    "name": "Morgan Gordon"
                                },
                                {
                                    "authorId": "2309480960",
                                    "name": "Scott Grafstein"
                                },
                                {
                                    "authorId": "2309478956",
                                    "name": "Ryan Gray"
                                },
                                {
                                    "authorId": "2309478103",
                                    "name": "Joshua Greene"
                                },
                                {
                                    "authorId": "2309475937",
                                    "name": "Shixiang Shane Gross"
                                },
                                {
                                    "authorId": "2309896895",
                                    "name": "Yufei Gu"
                                },
                                {
                                    "authorId": "2309804592",
                                    "name": "Chris Guo"
                                },
                                {
                                    "authorId": "2309477491",
                                    "name": "Jesse Hallacy"
                                },
                                {
                                    "authorId": "2309667621",
                                    "name": "Jeff Han"
                                },
                                {
                                    "authorId": "2309475604",
                                    "name": "Harris Yuchen"
                                },
                                {
                                    "authorId": "2310401628",
                                    "name": "Mike He"
                                },
                                {
                                    "authorId": "2309477353",
                                    "name": "Johannes Heaton"
                                },
                                {
                                    "authorId": "2309476458",
                                    "name": "C. Heidecke"
                                },
                                {
                                    "authorId": "2309475602",
                                    "name": "Alan Hesse"
                                },
                                {
                                    "authorId": "2275246148",
                                    "name": "W. Hickey"
                                },
                                {
                                    "authorId": "2309477265",
                                    "name": "Peter Hickey"
                                },
                                {
                                    "authorId": "2309477475",
                                    "name": "Hoeschele Brandon"
                                },
                                {
                                    "authorId": "2309480952",
                                    "name": "Kenny Houghton"
                                },
                                {
                                    "authorId": "2309479202",
                                    "name": "Shengli Hsu"
                                },
                                {
                                    "authorId": "2275777049",
                                    "name": "Xin Hu"
                                },
                                {
                                    "authorId": "2309663799",
                                    "name": "Joost Hu"
                                },
                                {
                                    "authorId": "2309477471",
                                    "name": "Shantanu Huizinga"
                                },
                                {
                                    "authorId": "2309900251",
                                    "name": "Shawn Jain"
                                },
                                {
                                    "authorId": "2309475571",
                                    "name": "Jain Joanne"
                                },
                                {
                                    "authorId": "2309477467",
                                    "name": "Angela Jang"
                                },
                                {
                                    "authorId": "2275172062",
                                    "name": "Roger Jiang"
                                },
                                {
                                    "authorId": "2309830920",
                                    "name": "Haozhun Jiang"
                                },
                                {
                                    "authorId": "2275203081",
                                    "name": "Denny Jin"
                                },
                                {
                                    "authorId": "2309901734",
                                    "name": "Shino Jin"
                                },
                                {
                                    "authorId": "2309481128",
                                    "name": "Billie Jomoto"
                                },
                                {
                                    "authorId": "2309480973",
                                    "name": "Hee-woo Jonn"
                                },
                                {
                                    "authorId": "2309475461",
                                    "name": "Tomer Jun"
                                },
                                {
                                    "authorId": "2309476661",
                                    "name": "\u0141ukasz Kaftan"
                                },
                                {
                                    "authorId": "2309476629",
                                    "name": "Ali Kaiser"
                                },
                                {
                                    "authorId": "2319225089",
                                    "name": "Ingmar Ka-748 mali"
                                },
                                {
                                    "authorId": "2102033721",
                                    "name": "Kanitscheider"
                                },
                                {
                                    "authorId": "2288125393",
                                    "name": "Nitish Shirish"
                                },
                                {
                                    "authorId": "2307452588",
                                    "name": "Keskar Tabarak"
                                },
                                {
                                    "authorId": "2307454011",
                                    "name": "Logan Khan"
                                },
                                {
                                    "authorId": "2307452560",
                                    "name": "J. Kilpatrick"
                                },
                                {
                                    "authorId": "2319227856",
                                    "name": "Kim"
                                },
                                {
                                    "authorId": "2149054292",
                                    "name": "Christina Kim"
                                },
                                {
                                    "authorId": "2275296777",
                                    "name": "Yongjik Kim"
                                },
                                {
                                    "authorId": "2319226271",
                                    "name": "Jan Hendrik Kirch-751 ner"
                                },
                                {
                                    "authorId": "51131802",
                                    "name": "J. Kiros"
                                },
                                {
                                    "authorId": "2146257375",
                                    "name": "Matthew Knight"
                                },
                                {
                                    "authorId": "1485556711",
                                    "name": "Daniel Kokotajlo"
                                },
                                {
                                    "authorId": "2319226462",
                                    "name": "\u0141ukasz Kondraciuk"
                                },
                                {
                                    "authorId": "1666171360",
                                    "name": "Andrew Kondrich"
                                },
                                {
                                    "authorId": "2319225191",
                                    "name": "Aris Kon-753 stantinidis"
                                },
                                {
                                    "authorId": "2275245594",
                                    "name": "Kyle Kosic"
                                },
                                {
                                    "authorId": "2064404342",
                                    "name": "Gretchen Krueger"
                                },
                                {
                                    "authorId": "2275229877",
                                    "name": "Vishal Kuo"
                                },
                                {
                                    "authorId": "2275247085",
                                    "name": "Michael Lampe"
                                },
                                {
                                    "authorId": "2275246287",
                                    "name": "Ikai Lan"
                                },
                                {
                                    "authorId": "2274915115",
                                    "name": "Teddy Lee"
                                },
                                {
                                    "authorId": "2990741",
                                    "name": "Jan Leike"
                                },
                                {
                                    "authorId": "52152632",
                                    "name": "Jade Leung"
                                },
                                {
                                    "authorId": "2319225189",
                                    "name": "Chak Daniel Levy"
                                },
                                {
                                    "authorId": "2319250141",
                                    "name": "Ming Li"
                                },
                                {
                                    "authorId": "2275176375",
                                    "name": "Rachel Lim"
                                },
                                {
                                    "authorId": "2275759230",
                                    "name": "Molly Lin"
                                },
                                {
                                    "authorId": "2253840098",
                                    "name": "Stephanie Lin"
                                },
                                {
                                    "authorId": "1380985420",
                                    "name": "Ma-teusz Litwin"
                                },
                                {
                                    "authorId": "2275248327",
                                    "name": "Theresa Lopez"
                                },
                                {
                                    "authorId": "2257272397",
                                    "name": "Ryan Lowe"
                                },
                                {
                                    "authorId": "2275245628",
                                    "name": "Patricia Lue"
                                },
                                {
                                    "authorId": "119341078",
                                    "name": "A. Makanju"
                                },
                                {
                                    "authorId": "2275245649",
                                    "name": "Kim Malfacini"
                                },
                                {
                                    "authorId": "46430291",
                                    "name": "Sam Manning"
                                },
                                {
                                    "authorId": "14113256",
                                    "name": "Todor Markov"
                                },
                                {
                                    "authorId": "2275245336",
                                    "name": "Yaniv Markovski"
                                },
                                {
                                    "authorId": "2114362965",
                                    "name": "Bianca Martin"
                                },
                                {
                                    "authorId": "2275231822",
                                    "name": "Katie Mayer"
                                },
                                {
                                    "authorId": "2275247045",
                                    "name": "Andrew Mayne"
                                },
                                {
                                    "authorId": "39593364",
                                    "name": "Bob McGrew"
                                },
                                {
                                    "authorId": "2047820455",
                                    "name": "S. McKinney"
                                },
                                {
                                    "authorId": "3028785",
                                    "name": "Christine McLeavey"
                                },
                                {
                                    "authorId": "2274772421",
                                    "name": "Paul McMillan"
                                },
                                {
                                    "authorId": "2275234856",
                                    "name": "Jake McNeil"
                                },
                                {
                                    "authorId": "2275210659",
                                    "name": "David Medina"
                                },
                                {
                                    "authorId": "2275132306",
                                    "name": "Aalok Mehta"
                                },
                                {
                                    "authorId": "10698483",
                                    "name": "Jacob Menick"
                                },
                                {
                                    "authorId": "2275246330",
                                    "name": "Luke Metz"
                                },
                                {
                                    "authorId": "2275252694",
                                    "name": "An-drey Mishchenko"
                                },
                                {
                                    "authorId": "2051714782",
                                    "name": "Pamela Mishkin"
                                },
                                {
                                    "authorId": "2275245453",
                                    "name": "Vinnie Monaco"
                                },
                                {
                                    "authorId": "1404556973",
                                    "name": "Evan Morikawa"
                                },
                                {
                                    "authorId": "3407880",
                                    "name": "Daniel P. Mossing"
                                },
                                {
                                    "authorId": "2319225702",
                                    "name": "Tong Mu"
                                },
                                {
                                    "authorId": "2117715631",
                                    "name": "Mira Murati"
                                },
                                {
                                    "authorId": "147746767",
                                    "name": "O. Murk"
                                },
                                {
                                    "authorId": "2319226404",
                                    "name": "David M\u00e9ly"
                                },
                                {
                                    "authorId": "2319226971",
                                    "name": "Ashvin Nair"
                                },
                                {
                                    "authorId": "7406311",
                                    "name": "Reiichiro Nakano"
                                },
                                {
                                    "authorId": "2057426488",
                                    "name": "Rajeev Nayak"
                                },
                                {
                                    "authorId": "2072676",
                                    "name": "Arvind Neelakantan"
                                },
                                {
                                    "authorId": "2273886618",
                                    "name": "Richard Ngo"
                                },
                                {
                                    "authorId": "2275115983",
                                    "name": "Hyeonwoo Noh"
                                },
                                {
                                    "authorId": "2228518120",
                                    "name": "Ouyang Long"
                                },
                                {
                                    "authorId": "1435765036",
                                    "name": "Cullen O'Keefe"
                                },
                                {
                                    "authorId": "2713380",
                                    "name": "J. Pachocki"
                                },
                                {
                                    "authorId": "34800652",
                                    "name": "A. Paino"
                                },
                                {
                                    "authorId": "2275244652",
                                    "name": "Joe Palermo"
                                },
                                {
                                    "authorId": "2275246178",
                                    "name": "Ashley Pantuliano"
                                },
                                {
                                    "authorId": "2275207240",
                                    "name": "Carl Ross"
                                },
                                {
                                    "authorId": "11150265",
                                    "name": "Bob Rotsted"
                                },
                                {
                                    "authorId": "2275250007",
                                    "name": "Henri Roussez"
                                },
                                {
                                    "authorId": "2319225618",
                                    "name": "Nick Ry-779 der"
                                },
                                {
                                    "authorId": "2252840300",
                                    "name": "Mario D. Saltarelli"
                                },
                                {
                                    "authorId": "2275246803",
                                    "name": "Ted Sanders"
                                },
                                {
                                    "authorId": "2852106",
                                    "name": "Shibani Santurkar"
                                },
                                {
                                    "authorId": "144864359",
                                    "name": "Girish Sastry"
                                },
                                {
                                    "authorId": "2275265666",
                                    "name": "Heather Schmidt"
                                },
                                {
                                    "authorId": "2252874293",
                                    "name": "David Schnurr"
                                },
                                {
                                    "authorId": "2297873691",
                                    "name": "John Schulman"
                                },
                                {
                                    "authorId": "2196579",
                                    "name": "Daniel Selsam"
                                },
                                {
                                    "authorId": "2275244711",
                                    "name": "Kyla Sheppard"
                                },
                                {
                                    "authorId": "102475503",
                                    "name": "Toki Sherbakov"
                                },
                                {
                                    "authorId": "2275246834",
                                    "name": "Jessica Shieh"
                                },
                                {
                                    "authorId": "118335789",
                                    "name": "Sarah Shoker"
                                },
                                {
                                    "authorId": "67311962",
                                    "name": "Pranav Shyam"
                                },
                                {
                                    "authorId": "2700360",
                                    "name": "Szymon Sidor"
                                },
                                {
                                    "authorId": "2064673055",
                                    "name": "Eric Sigler"
                                },
                                {
                                    "authorId": "2151735251",
                                    "name": "Maddie Simens"
                                },
                                {
                                    "authorId": "2275252299",
                                    "name": "Jordan Sitkin"
                                },
                                {
                                    "authorId": "2117680841",
                                    "name": "Katarina Slama"
                                },
                                {
                                    "authorId": "103422608",
                                    "name": "Ian Sohl"
                                },
                                {
                                    "authorId": "2901424",
                                    "name": "Benjamin Sokolowsky"
                                },
                                {
                                    "authorId": "2307592658",
                                    "name": "Yang Song"
                                },
                                {
                                    "authorId": "2275245668",
                                    "name": "Natalie Staudacher"
                                },
                                {
                                    "authorId": "2059411355",
                                    "name": "Clemens Winter"
                                },
                                {
                                    "authorId": "2275244177",
                                    "name": "Samuel Wolrich"
                                },
                                {
                                    "authorId": "2275225207",
                                    "name": "Hannah Wong"
                                },
                                {
                                    "authorId": "2275245771",
                                    "name": "Lauren Workman"
                                },
                                {
                                    "authorId": "2275299848",
                                    "name": "Sherwin Wu"
                                },
                                {
                                    "authorId": "2274911253",
                                    "name": "Jeff Wu"
                                },
                                {
                                    "authorId": "2307456650",
                                    "name": "Michael Wu"
                                },
                                {
                                    "authorId": "2307454769",
                                    "name": "Kai Xiao"
                                },
                                {
                                    "authorId": "2275452480",
                                    "name": "Tao Xu"
                                },
                                {
                                    "authorId": "2275310096",
                                    "name": "Sarah Yoo"
                                },
                                {
                                    "authorId": "2275593618",
                                    "name": "Kevin Yu"
                                },
                                {
                                    "authorId": "2275194186",
                                    "name": "Qim-ing Yuan"
                                },
                                {
                                    "authorId": "2307452791",
                                    "name": "Wojciech Zaremba"
                                },
                                {
                                    "authorId": "49629836",
                                    "name": "Rowan Zellers"
                                },
                                {
                                    "authorId": "2315024566",
                                    "name": "Chong Zhang"
                                },
                                {
                                    "authorId": "2281037751",
                                    "name": "Marvin Zhang"
                                },
                                {
                                    "authorId": "2307453667",
                                    "name": "Tianhao Shengjia Zhao"
                                },
                                {
                                    "authorId": "2298950344",
                                    "name": "Xu Jiang"
                                },
                                {
                                    "authorId": "2275252021",
                                    "name": "Diogo Almeida"
                                },
                                {
                                    "authorId": "2275245962",
                                    "name": "Carroll L. Wainwright"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "2319227127",
                                    "name": "Alex Gray"
                                },
                                {
                                    "authorId": "2286540856",
                                    "name": "Jacob Hilton"
                                },
                                {
                                    "authorId": "2151735262",
                                    "name": "Fraser Kelton"
                                },
                                {
                                    "authorId": "2298421583",
                                    "name": "Luke Miller"
                                },
                                {
                                    "authorId": "2220750220",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "2930640",
                                    "name": "P. Welinder"
                                },
                                {
                                    "authorId": "2261980896",
                                    "name": "Paul F. Christiano"
                                },
                                {
                                    "authorId": "2197475360",
                                    "name": "Joon Sung Park"
                                },
                                {
                                    "authorId": "2213764034",
                                    "name": "Joseph C. O\u2019Brien"
                                },
                                {
                                    "authorId": "2276794641",
                                    "name": "C. Cai"
                                },
                                {
                                    "authorId": "2319287576",
                                    "name": "Ringel Morris"
                                },
                                {
                                    "authorId": "2256995425",
                                    "name": "Percy Liang"
                                },
                                {
                                    "authorId": "2319226111",
                                    "name": "Michael S. Bern-814"
                                },
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "2285784924",
                                    "name": "Karthik Narasimhan"
                                },
                                {
                                    "authorId": "2887364",
                                    "name": "Tim Salimans"
                                },
                                {
                                    "authorId": "2302559920",
                                    "name": "Rachel Rudinger"
                                },
                                {
                                    "authorId": "2300343",
                                    "name": "Jason Naradowsky"
                                },
                                {
                                    "authorId": "2319225916",
                                    "name": "Brian Leonard"
                                },
                                {
                                    "authorId": "1387983862",
                                    "name": "Nisan Stiennon"
                                },
                                {
                                    "authorId": "2319225774",
                                    "name": "Ryan Ziegler"
                                },
                                {
                                    "authorId": "2314165049",
                                    "name": "Chelsea Lowe"
                                },
                                {
                                    "authorId": "2314160468",
                                    "name": "Alec Voss"
                                },
                                {
                                    "authorId": "2289348718",
                                    "name": "Radford"
                                },
                                {
                                    "authorId": "2698777",
                                    "name": "Dario Amodei"
                                },
                                {
                                    "authorId": "2319225540",
                                    "name": "Christiano. 2020. Learn-842"
                                },
                                {
                                    "authorId": "2319580593",
                                    "name": "Tony Sun"
                                },
                                {
                                    "authorId": "146072982",
                                    "name": "Andrew Gaut"
                                },
                                {
                                    "authorId": "148149462",
                                    "name": "Shirlyn Tang"
                                },
                                {
                                    "authorId": "2154731574",
                                    "name": "Yuxin Huang"
                                },
                                {
                                    "authorId": "2288124187",
                                    "name": "Mai ElSherief"
                                },
                                {
                                    "authorId": "2311580116",
                                    "name": "Jie Zhao"
                                },
                                {
                                    "authorId": "2319225642",
                                    "name": "Diba Mirza"
                                },
                                {
                                    "authorId": "2319226119",
                                    "name": "Kai-Wei Belding"
                                },
                                {
                                    "authorId": "2319225109",
                                    "name": "Chang William"
                                },
                                {
                                    "authorId": "2319563352",
                                    "name": "Yang Wang"
                                },
                                {
                                    "authorId": "2165227666",
                                    "name": "Yixin Wan"
                                },
                                {
                                    "authorId": "2258548444",
                                    "name": "George Pu"
                                },
                                {
                                    "authorId": "2261454711",
                                    "name": "Jiao Sun"
                                },
                                {
                                    "authorId": "31099365",
                                    "name": "Aparna Garimella"
                                },
                                {
                                    "authorId": "2257127887",
                                    "name": "Kai-Wei Chang"
                                },
                                {
                                    "authorId": "2285475879",
                                    "name": "Nanyun Peng"
                                },
                                {
                                    "authorId": "2319226454",
                                    "name": "\u201ckelly"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 14
                        },
                        "score": 0.6328125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Risks of Egocentric Bias",
                "tldr": "Egocentric bias in GPT-4 creates significant risks including reinforcement of the model's own preferences through self-evaluation, potential amplification of medical and demographic biases, and the inheritance of biases by student models trained on its outputs. (10 sources)",
                "text": "\nWhen GPT-4 evaluates its own outputs, several substantial risks emerge. Perhaps most concerning is the potential for self-reinforcement, where the model's biases become amplified through continuous self-evaluation. As Dai et al. note, \"The emergence of egocentric bias introduces the risk of self-reinforcement for LLMs, particularly when they are further trained using rewards from LLM-based evaluations,\" which can lead to \"overfitting to their own evaluation criteria, intensifying self-preference in next-generation models.\" <Paper corpusId=\"269188154\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"257804696\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>\n\nThis self-reinforcement creates a problematic feedback loop that potentially undermines the reliability of evaluation processes. Wu et al. highlight \"concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process,\" suggesting the need for multiple evaluators to establish \"a more comprehensive and unbiased assessment.\" <Paper corpusId=\"259360998\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>\n\nIn specialized domains like healthcare, these biases can have particularly harmful consequences. Research by Bragazzi et al. warns that \"LLMs might reinforce existing biases and provide inaccurate medical diagnoses, potentially leading to detrimental effects on health care.\" Their analysis of GPT-4 revealed that it \"inadequately represents demographic diversity in medical conditions\" and generates differential diagnoses that \"reflect biases associated with race, ethnicity, and gender.\" <Paper corpusId=\"270614044\" paperTitle=\"(Bragazzi et al., 2023)\" isShortName></Paper>\n\nAnother significant risk is the propagation of bias through what researchers call \"preference leakage.\" Li et al. empirically confirmed \"the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks.\" This means that biases in GPT-4 can be inherited by models trained on its outputs, creating a compounding effect where \"GPT-4's bias toward LLaMA has been passed on to LLaMA's student models.\" <Paper corpusId=\"276106991\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper> <Paper corpusId=\"259129398\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>\n\nEven when researchers are aware of potential biases, mitigating them completely remains challenging. Mullick et al. acknowledge that \"using summaries generated exclusively by GPT-4 could introduce biases inherent in its summarization capabilities,\" while noting that \"alternatives, such as human evaluation, also carry their own biases.\" <Paper corpusId=\"270286247\" paperTitle=\"(Mullick et al., 2024)\" isShortName></Paper>\n\nThe fundamental challenge is what Li et al. describe as a \"chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.\" <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"215548699\" paperTitle=\"(Sellam et al., 2020)\" isShortName></Paper> Wataoka et al. further warn that \"using GPT-4 as a judge may lead to excessive influence from GPT-4's unique styles and policies,\" suggesting that over-reliance on a single model for evaluation can lead to a narrowing of the diversity of acceptable outputs. <Paper corpusId=\"273661820\" paperTitle=\"(Wataoka et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Dai et al., 2024)",
                        "snippets": [
                            "The emergence of egocentric bias introduces the risk of selfreinforcement for LLMs, particularly when they are further trained using rewards from LLM-based evaluations. This scenario can lead to LLMs overfitting to their own evaluation criteria, intensifying self-preference in next-generation model (Liu et al., 2023). Current strategies for mitigating egocentric bias primarily involve employing diverse LLMs as evaluators to foster peer discussions [79], thereby reducing the preference for any specific LLM and enhancing the robustness of evaluation outcomes. However, this strategy inevitably increases the evaluation costs. Future research must explore more efficient solutions to ensure fair and unbiased evaluation."
                        ],
                        "paper": {
                            "corpus_id": 269188154,
                            "title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era",
                            "authors": [
                                {
                                    "authorId": "2155892801",
                                    "name": "Sunhao Dai"
                                },
                                {
                                    "authorId": "2153078929",
                                    "name": "Chen Xu"
                                },
                                {
                                    "authorId": "2202745",
                                    "name": "Shicheng Xu"
                                },
                                {
                                    "authorId": "2263589454",
                                    "name": "Liang Pang"
                                },
                                {
                                    "authorId": "2297820120",
                                    "name": "Zhenhua Dong"
                                },
                                {
                                    "authorId": "2266437969",
                                    "name": "Jun Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Knowledge Discovery and Data Mining",
                            "n_citations": 82
                        },
                        "score": 0.84765625
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval"
                        ],
                        "paper": {
                            "corpus_id": 257804696,
                            "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                            "authors": [
                                {
                                    "authorId": "2152797401",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "3310951",
                                    "name": "Dan Iter"
                                },
                                {
                                    "authorId": "2110197273",
                                    "name": "Yichong Xu"
                                },
                                {
                                    "authorId": "2146294891",
                                    "name": "Shuo Wang"
                                },
                                {
                                    "authorId": "8233965",
                                    "name": "Ruochen Xu"
                                },
                                {
                                    "authorId": "8652308",
                                    "name": "Chenguang Zhu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1211
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wu et al., 2023)",
                        "snippets": [
                            "This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4. By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study."
                        ],
                        "paper": {
                            "corpus_id": 259360998,
                            "title": "Style Over Substance: Evaluation Biases for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2145209409",
                                    "name": "Minghao Wu"
                                },
                                {
                                    "authorId": "8129718",
                                    "name": "Alham Fikri Aji"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 47
                        },
                        "score": 0.552734375
                    },
                    {
                        "id": "(Bragazzi et al., 2023)",
                        "snippets": [
                            "A further risk is that LLMs might reinforce existing biases and provide inaccurate medical diagnoses, potentially leading to detrimental effects on health care.Zack et al [26] aimed to evaluate whether GPT-4 harbors biases that could influence its application in health care settings.Using the Azure OpenAI interface, the authors scrutinized GPT-4 for racial and gender biases and assessed the impact of such biases on four clinical applications of LLMs-(1) medical education, (2) diagnostic reasoning, (3) development and implementation of clinical plans, and (4) subjective patient evaluations-involving experiments using prompts mimicking typical GPT-4 use in clinical and medical educational settings and drawing from New England Journal of Medicine Healer clinical vignettes and research on implicit bias in health care.The study compared GPT-4's estimates of demographic distributions of medical conditions against actual US prevalence data.For differential diagnosis and treatment planning, the research analyzed variations across demographic groups using standard statistical methods to identify significant differences.The study revealed that GPT-4 inadequately represents demographic diversity in medical conditions, often resorting to stereotypical demographic portrayals in clinical vignettes.The differential diagnoses generated by GPT-4 for standardized clinical vignettes tended to reflect biases associated with race, ethnicity, and gender.Furthermore, the model's assessments and plans demonstrated a notable correlation between demographic characteristics and recommendations for costlier procedures, as well as varied perceptions of patients."
                        ],
                        "paper": {
                            "corpus_id": 270614044,
                            "title": "Toward Clinical Generative AI: Conceptual Framework",
                            "authors": [
                                {
                                    "authorId": "2250832100",
                                    "name": "N. Bragazzi"
                                },
                                {
                                    "authorId": "2277106726",
                                    "name": "Sergio Garbarino"
                                }
                            ],
                            "year": 2023,
                            "venue": "JMIR AI",
                            "n_citations": 13
                        },
                        "score": 0.51806640625
                    },
                    {
                        "id": "(Li et al., 2025)",
                        "snippets": [
                            "Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive and real-world problem that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios",
                            "Using the MTBench (Zheng et al., 2023) dataset, which includes pairwise comparison judgments from both humans and GPT-4, we compare GPT-4's and human evaluators' judgments on LLaMA-2 vs other models (including Vicuna, Alpaca, GPT-3.5, and GPT-4, which are preferred by GPT-4 due to preference leakage or egocentric bias) and LLaMA-2 vs Claude. The results, shown in Figure 4, reveal a clear preference for LLaMA-2 by GPT-4. Consequently, we conclude that evaluators' bias can be inherited. In this case, GPT-4's bias toward LLaMA has been passed on to LLaMA's student models. This inheritance, combined with the opaque training data of LLMs, makes preference leakage a more complex and challenging problem."
                        ],
                        "paper": {
                            "corpus_id": 276106991,
                            "title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge",
                            "authors": [
                                {
                                    "authorId": "2161635474",
                                    "name": "Dawei Li"
                                },
                                {
                                    "authorId": "2344419674",
                                    "name": "Renliang Sun"
                                },
                                {
                                    "authorId": "2324070910",
                                    "name": "Yue Huang"
                                },
                                {
                                    "authorId": "2316709408",
                                    "name": "Ming Zhong"
                                },
                                {
                                    "authorId": "2036355404",
                                    "name": "Bohan Jiang"
                                },
                                {
                                    "authorId": "2343853966",
                                    "name": "Jiawei Han"
                                },
                                {
                                    "authorId": "2307963162",
                                    "name": "Xiangliang Zhang"
                                },
                                {
                                    "authorId": "2343828587",
                                    "name": "Wei Wang"
                                },
                                {
                                    "authorId": "2287545693",
                                    "name": "Huan Liu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 29
                        },
                        "score": 0.50439453125
                    },
                    {
                        "id": "(Zheng et al., 2023)",
                        "snippets": [
                            "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge."
                        ],
                        "paper": {
                            "corpus_id": 259129398,
                            "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
                            "authors": [
                                {
                                    "authorId": "2149970173",
                                    "name": "Lianmin Zheng"
                                },
                                {
                                    "authorId": "2537924",
                                    "name": "Wei-Lin Chiang"
                                },
                                {
                                    "authorId": "2209360681",
                                    "name": "Ying Sheng"
                                },
                                {
                                    "authorId": "92721493",
                                    "name": "Siyuan Zhuang"
                                },
                                {
                                    "authorId": "1390573666",
                                    "name": "Zhanghao Wu"
                                },
                                {
                                    "authorId": "2152482391",
                                    "name": "Yonghao Zhuang"
                                },
                                {
                                    "authorId": "143872641",
                                    "name": "Zi Lin"
                                },
                                {
                                    "authorId": "2141335450",
                                    "name": "Zhuohan Li"
                                },
                                {
                                    "authorId": "2117961435",
                                    "name": "Dacheng Li"
                                },
                                {
                                    "authorId": "143977260",
                                    "name": "E. Xing"
                                },
                                {
                                    "authorId": "145140331",
                                    "name": "Haotong Zhang"
                                },
                                {
                                    "authorId": "49988044",
                                    "name": "Joseph E. Gonzalez"
                                },
                                {
                                    "authorId": "2055174324",
                                    "name": "Ion Stoica"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 4439
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mullick et al., 2024)",
                        "snippets": [
                            "It is acknowledged that using summaries generated exclusively by GPT-4 could introduce biases inherent in its summarization capabilities, it may also be noted that alternatives, such as human evaluation, also carry their own biases.Despite the potential for bias, leveraging GPT-4 for summarization may still be a pragmatic choice, especially in scenarios access to diverse datasets or sophisticated validation methods is limited.However, in this work, we remain vigilant, recognizing the limitations inherent in both automated and humangenerated summaries, and take proactive steps such as human intervention to validate and contextualise the results to mitigate biases to the best extent possible within the given constraints."
                        ],
                        "paper": {
                            "corpus_id": 270286247,
                            "title": "On The Persona-based Summarization of Domain-Specific Documents",
                            "authors": [
                                {
                                    "authorId": "21724468",
                                    "name": "Ankan Mullick"
                                },
                                {
                                    "authorId": "2238668264",
                                    "name": "Sombit Bose"
                                },
                                {
                                    "authorId": "2304954421",
                                    "name": "Rounak Saha"
                                },
                                {
                                    "authorId": "19181085",
                                    "name": "Ayan Kumar Bhowmick"
                                },
                                {
                                    "authorId": "2261284157",
                                    "name": "Pawan Goyal"
                                },
                                {
                                    "authorId": "2261284171",
                                    "name": "Niloy Ganguly"
                                },
                                {
                                    "authorId": "2287821944",
                                    "name": "Prasenjit Dey"
                                },
                                {
                                    "authorId": "2247701385",
                                    "name": "Ravi Kokku"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 3
                        },
                        "score": 0.501953125
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator). This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023). This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator. To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022; Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023; Ouyang et al., 2022) to ensure more balanced and comprehensive assessments."
                        ],
                        "paper": {
                            "corpus_id": 270391675,
                            "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
                            "authors": [
                                {
                                    "authorId": "2145256331",
                                    "name": "Zhen Li"
                                },
                                {
                                    "authorId": "2279658967",
                                    "name": "Xiaohan Xu"
                                },
                                {
                                    "authorId": "2279548827",
                                    "name": "Tao Shen"
                                },
                                {
                                    "authorId": "2284826718",
                                    "name": "Can Xu"
                                },
                                {
                                    "authorId": "2308241851",
                                    "name": "Jia-Chen Gu"
                                },
                                {
                                    "authorId": "2308073132",
                                    "name": "Yuxuan Lai"
                                },
                                {
                                    "authorId": "2287928517",
                                    "name": "Chongyang Tao"
                                },
                                {
                                    "authorId": "2307142498",
                                    "name": "Shuai Ma"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 15
                        },
                        "score": 0.6328125
                    },
                    {
                        "id": "(Sellam et al., 2020)",
                        "snippets": [
                            "Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution."
                        ],
                        "paper": {
                            "corpus_id": 215548699,
                            "title": "BLEURT: Learning Robust Metrics for Text Generation",
                            "authors": [
                                {
                                    "authorId": "145450400",
                                    "name": "Thibault Sellam"
                                },
                                {
                                    "authorId": "143790066",
                                    "name": "Dipanjan Das"
                                },
                                {
                                    "authorId": "144729897",
                                    "name": "Ankur P. Parikh"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 1505
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wataoka et al., 2024)",
                        "snippets": [
                            "This finding suggests a potential concern: using GPT-4 as a judge may lead to excessive influence from GPT-4's unique styles and policies.\n\nTo reduce self-preference bias, one possible approach is ensemble evaluation using multiple models. This method is expected to provide a more equitable evaluation by avoiding reliance on a single model. Specifically, when a model exhibits low perplexity on a sample, decreasing the weight assigned to that model's evaluation for that sample may contribute to bias mitigation. To evaluate the effectiveness of bias reduction strategies, our proposed new metric can be utilized."
                        ],
                        "paper": {
                            "corpus_id": 273661820,
                            "title": "Self-Preference Bias in LLM-as-a-Judge",
                            "authors": [
                                {
                                    "authorId": "2007365532",
                                    "name": "Koki Wataoka"
                                },
                                {
                                    "authorId": "2325815191",
                                    "name": "Tsubasa Takahashi"
                                },
                                {
                                    "authorId": "1466451143",
                                    "name": "Ryokan Ri"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 25
                        },
                        "score": 0.70166015625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Mitigation Strategies: Evaluation Design Approaches",
                "tldr": "Several evaluation design strategies have been developed to mitigate egocentric bias in GPT-4's self-evaluation, including position calibration techniques, hierarchical rubrics, and specialized prompting methods that encourage rational judgment and multiple evidence collection. (9 sources)",
                "text": "\n- **Multiple Evidence Calibration (MEC)**: This approach prompts the model to generate evaluation evidence before assigning scores, leveraging the inherent properties of causal language models for calibration. Combined with ensemble techniques, MEC can stabilize evaluation by incorporating multiple evidence calibration results. <Paper corpusId=\"258960339\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>\n\n- **Balanced Position Calibration (BPC)**: To reduce positional bias (which can influence egocentric bias), this technique evaluates each candidate in both positions across two runs and computes the final score as the average. This helps counteract GPT-4's tendency to favor responses in certain positions. <Paper corpusId=\"258960339\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>\n\n- **Human In The Loop Calibration (HITLC)**: This strategy incorporates human oversight into the evaluation process, using a diversity-based method to identify potentially biased candidates based on results from other calibration approaches. <Paper corpusId=\"258960339\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>\n\n- **Hierarchical Rubrics**: By prioritizing certain biases in the evaluation process, this approach helps isolate and address specific forms of bias. For example, if an evaluation shows signs of order bias, it would not be evaluated for other biases like salience or egocentric bias. <Paper corpusId=\"263310448\" paperTitle=\"(Koo et al., 2023)\" isShortName></Paper>\n\n- **Bias Scoring Methods (BSM)**: These methods can improve evaluation even when an LLM judges its own outputs. Research shows BSM can achieve a 3% better correlation with human judgments, suggesting it effectively reduces self-enhancement bias. <Paper corpusId=\"264591429\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>\n\n- **Neutral Evaluator Selection**: Using LLMs independent of the assessed models' training data can mitigate data leakage risks that contribute to egocentric bias. <Paper corpusId=\"267760188\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **Evaluator Rotation**: Randomly rotating different evaluators can help reduce the impact of bias from any single model's judgment. <Paper corpusId=\"267760188\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **AwaRe Prompting**: This technique has shown effectiveness in mitigating multiple biases including order bias, compassion fade, egocentric bias, bandwagon effect, and attentional bias in GPT-3.5, as well as bandwagon effect and verbosity bias in GPT-4. This approach prompts LLMs to make more rational judgments. <Paper corpusId=\"274437478\" paperTitle=\"(Sumita et al., 2024)\" isShortName></Paper>\n\n- **Human Reference Provision**: Providing human-generated references during evaluation reduces both the overall rate of error and the relative difference between a model's judgment of its own responses versus those of others. This is particularly effective in reducing false positive rates when models judge their own outputs. <Paper corpusId=\"276885275\" paperTitle=\"(Krumdick et al., 2025)\" isShortName></Paper>\n\n- **Detailed Evaluation Criteria**: LLMs tend to show higher correlation with human annotations and lower egocentric bias when evaluation criteria are more specific and detailed. <Paper corpusId=\"277621852\" paperTitle=\"(Eldifrawi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"257804696\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"273098639\" paperTitle=\"(Ye et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "Specifically, we demonstrate that GPT-4 exhibits a preference for the first displayed candidate response by consistently assigning it higher scores, even when the order of candidates is subtly altered. As illustrated in Figure 1, merely swapping the presentation order can reverse evaluation outcomes. This bias is also present in Chat-GPT, which typically favors the second response. These findings highlight previously overlooked limitations in the current evaluation paradigm.\n\nTo address this issue, we propose three simple yet effective strategies to calibrate positional bias: 1) Multiple Evidence Calibration (MEC): We prompt the model to generate evaluation evidence before assigning scores, leveraging the inherent properties of causal language models for calibration. We also employ ensemble techniques to incorporate multiple evidence calibration results to further stabilize the evaluation. 2) Balanced Position Calibration (BPC): To further reduce positional bias, we evaluate each candidate in both positions across two runs and compute the final score as the average of the two runs. 3) Human In The Loop Calibration (HITLC): We also explore human-inthe-loop evaluation and consider a diversity-based method to get a cue to indicate biased candidates based on the evaluation results of MEC and BPC."
                        ],
                        "paper": {
                            "corpus_id": 258960339,
                            "title": "Large Language Models are not Fair Evaluators",
                            "authors": [
                                {
                                    "authorId": "144202874",
                                    "name": "Peiyi Wang"
                                },
                                {
                                    "authorId": "49192881",
                                    "name": "Lei Li"
                                },
                                {
                                    "authorId": "2146034504",
                                    "name": "Liang Chen"
                                },
                                {
                                    "authorId": "2116276849",
                                    "name": "Dawei Zhu"
                                },
                                {
                                    "authorId": "3186130",
                                    "name": "Binghuai Lin"
                                },
                                {
                                    "authorId": "2154235",
                                    "name": "Yunbo Cao"
                                },
                                {
                                    "authorId": "2144831944",
                                    "name": "Qi Liu"
                                },
                                {
                                    "authorId": "1701889",
                                    "name": "Tianyu Liu"
                                },
                                {
                                    "authorId": "3335836",
                                    "name": "Zhifang Sui"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 573
                        },
                        "score": 0.62353515625
                    },
                    {
                        "id": "(Koo et al., 2023)",
                        "snippets": [
                            "Egocentric Bias (Self-Preference). (Ross and Sicoly, 1979) is a cognitive bias that refers to the tendency to have a higher opinion of oneself or to more easily accept ideas if they match one's own. We define an evaluator to be egocentrically biased if, for each instance, the evaluator prefers its own response over others. We note that an unbiased evaluator would choose between themselves and other comparand models equally in proportion. However, we highlight that some models would naturally generate higher quality responses (e.g., GPT4 vs. KOALA), resulting in a stronger inclination for such evaluators to choose their own responses.\n\nWe employ various strategies to mitigate these confounding variables and isolate each analysis as much as possible. For example, we employ a \"hierarchical\" rubric, where some biases take priority in an evaluation. Specifically, if an evaluation shows signs of order bias by choosing A in (A first, then B) and B in (B first, then A), we do not evaluate it for SALIENCE or EGOCEN-TRIC bias."
                        ],
                        "paper": {
                            "corpus_id": 263310448,
                            "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                            "authors": [
                                {
                                    "authorId": "2213239540",
                                    "name": "Ryan Koo"
                                },
                                {
                                    "authorId": "2187932371",
                                    "name": "Minhwa Lee"
                                },
                                {
                                    "authorId": "2831377",
                                    "name": "Vipul Raheja"
                                },
                                {
                                    "authorId": "2294310015",
                                    "name": "Jong Inn Park"
                                },
                                {
                                    "authorId": "2894340",
                                    "name": "Zae Myung Kim"
                                },
                                {
                                    "authorId": "48493368",
                                    "name": "Dongyeop Kang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 86
                        },
                        "score": 0.79541015625
                    },
                    {
                        "id": "(Saha et al., 2023)",
                        "snippets": [
                            "Self-enhancement Bias Reduction.Table 2 evaluates self-enhancement bias by comparing BSM (with zero-shot GPT-4) for the samples where one of the responses is also generated by GPT-4.We observe a 3% better correlation with humans, suggesting that BSM improves evaluation even when the LLM judges its own outputs."
                        ],
                        "paper": {
                            "corpus_id": 264591429,
                            "title": "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation",
                            "authors": [
                                {
                                    "authorId": "35106509",
                                    "name": "Swarnadeep Saha"
                                },
                                {
                                    "authorId": "2253752918",
                                    "name": "Omer Levy"
                                },
                                {
                                    "authorId": "1709797",
                                    "name": "Asli Celikyilmaz"
                                },
                                {
                                    "authorId": "2253762115",
                                    "name": "Mohit Bansal"
                                },
                                {
                                    "authorId": "2243265350",
                                    "name": "Jason Weston"
                                },
                                {
                                    "authorId": "2243015223",
                                    "name": "Xian Li"
                                }
                            ],
                            "year": 2023,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 77
                        },
                        "score": 0.513671875
                    },
                    {
                        "id": "(Li et al._1, 2024)",
                        "snippets": [
                            "Using LLMs like GPT-4 as judges introduces potential data leakage risks due to biases in their pre-training data. This can be mitigated by selecting neutral evaluators independent of the assessed models' training data or randomly rotating evaluators to reduce bias."
                        ],
                        "paper": {
                            "corpus_id": 267760188,
                            "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
                            "authors": [
                                {
                                    "authorId": "2261896751",
                                    "name": "Xiang Li"
                                },
                                {
                                    "authorId": "2257016293",
                                    "name": "Yunshi Lan"
                                },
                                {
                                    "authorId": "2268678836",
                                    "name": "Chao Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 11
                        },
                        "score": 0.51025390625
                    },
                    {
                        "id": "(Sumita et al., 2024)",
                        "snippets": [
                            "Although SoPro mitigates egocentric bias in GPT-3.5 and verbosity bias in GPT-4, the models' susceptibility to bandwagon effect increased. SoPro, which aligns LLMs with others' perspectives, may not be effective for cognitive biases. This result is inconsistent with the claim of the study for humans. AwaRe mitigates order bias, compassion fade, egocentric bias, bandwagon effect, and attentional bias in GPT-3.5, and bandwagon effect and verbosity bias in GPT-4. This result suggests that AwaRe prompts LLMs to make rational judgments."
                        ],
                        "paper": {
                            "corpus_id": 274437478,
                            "title": "Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments",
                            "authors": [
                                {
                                    "authorId": "2333360231",
                                    "name": "Yasuaki Sumita"
                                },
                                {
                                    "authorId": "2243408877",
                                    "name": "Koh Takeuchi"
                                },
                                {
                                    "authorId": "2247886893",
                                    "name": "Hisashi Kashima"
                                }
                            ],
                            "year": 2024,
                            "venue": "ACM Symposium on Applied Computing",
                            "n_citations": 5
                        },
                        "score": 0.71044921875
                    },
                    {
                        "id": "(Krumdick et al., 2025)",
                        "snippets": [
                            "One common judgment bias is self-preference, where a model tends to overrate its own responses. To quantify this bias we computed the false positive rate (FPR) for each judge when evaluating its own responses versus all other models. In this context, the FPR represents the rate at which the model erroneously labels incorrect responses as correct. We also evaluated the false negative rate (FNR), which is the rate at which the model erroneously labels correct responses as incorrect. A model with a strong selfpreference bias would exhibit a high FPR and a low FNR when grading its own references. Figure 4 displays the FPR and FNR aggregated over each judge with the \"Wrong\", \"Random\", \"Self\", \"None\", and \"Human\" reference types (Metrics per judge can be found in Figure 5). For every reference type, we see that on average models have a higher FPR when grading their own responses. The gap is particularly large when the model is provided with its own generated reference or no reference at all. Thus, providing a human reference reduces both the overall rate of error and the relative difference between a model's judgment of its own responses and those of others."
                        ],
                        "paper": {
                            "corpus_id": 276885275,
                            "title": "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding",
                            "authors": [
                                {
                                    "authorId": "18171842",
                                    "name": "Michael Krumdick"
                                },
                                {
                                    "authorId": "2307472942",
                                    "name": "Charles Lovering"
                                },
                                {
                                    "authorId": "2266430123",
                                    "name": "Varshini Reddy"
                                },
                                {
                                    "authorId": "78150202",
                                    "name": "Seth Ebner"
                                },
                                {
                                    "authorId": "2266398345",
                                    "name": "Chris Tanner"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.599609375
                    },
                    {
                        "id": "(Eldifrawi et al., 2025)",
                        "snippets": [
                            "In their study, (Liu et al., 2023) identified a bias in evaluators, where they tend to favor their own model's generations over those from other models, even when the latter are objectively better. (Ohi et al., 2024) introduced a method for detecting this bias, which they termed 'Likelihood-based Evaluation Bias.' However, this approach requires access to the probability distribution of the LLM's generations, which is often unavailable, especially when working with commercial LLMs. (Ye et al., 2024) also addressed this issue, referring to it as 'egocentric Bias,' and we adopt this terminology in our work. Their research primarily focuses on understanding the effects of this bias on performance and strategies for mitigating it",
                            "It is worth noting that LLMs as evaluators tend to exhibit higher correlation with human annotations and lower egocentric bias when the evaluation criteria are more detailed."
                        ],
                        "paper": {
                            "corpus_id": 277621852,
                            "title": "FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking",
                            "authors": [
                                {
                                    "authorId": "2126996290",
                                    "name": "Islam Eldifrawi"
                                },
                                {
                                    "authorId": "2311878157",
                                    "name": "Shengrui Wang"
                                },
                                {
                                    "authorId": "2311887008",
                                    "name": "Amine Trabelsi"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.85986328125
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval"
                        ],
                        "paper": {
                            "corpus_id": 257804696,
                            "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                            "authors": [
                                {
                                    "authorId": "2152797401",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "3310951",
                                    "name": "Dan Iter"
                                },
                                {
                                    "authorId": "2110197273",
                                    "name": "Yichong Xu"
                                },
                                {
                                    "authorId": "2146294891",
                                    "name": "Shuo Wang"
                                },
                                {
                                    "authorId": "8233965",
                                    "name": "Ruochen Xu"
                                },
                                {
                                    "authorId": "8652308",
                                    "name": "Chenguang Zhu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1211
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ye et al., 2024)",
                        "snippets": [
                            "LLM-as-a-Judge has been widely utilized as an evaluation method in various benchmarks and served as supervised rewards in model training. However, despite their excellence in many domains, potential issues are under-explored, undermining their reliability and the scope of their utility. Therefore, we identify 12 key potential biases and propose a new automated bias quantification framework-CALM-which systematically quantifies and analyzes each type of bias in LLM-as-a-Judge by using automated and principle-guided modification. Our experiments cover multiple popular language models, and the results indicate that while advanced models have achieved commendable overall performance, significant biases persist in certain specific tasks. Empirical results suggest that there remains room for improvement in the reliability of LLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence of these biases and give some suggestions for the reliable application of LLM-as-a-Judge. Our work highlights the need for stakeholders to address these issues and remind users to exercise caution in LLM-as-a-Judge applications."
                        ],
                        "paper": {
                            "corpus_id": 273098639,
                            "title": "Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge",
                            "authors": [
                                {
                                    "authorId": "2325239327",
                                    "name": "Jiayi Ye"
                                },
                                {
                                    "authorId": "2324066105",
                                    "name": "Yanbo Wang"
                                },
                                {
                                    "authorId": "2324070910",
                                    "name": "Yue Huang"
                                },
                                {
                                    "authorId": "2279219833",
                                    "name": "Dongping Chen"
                                },
                                {
                                    "authorId": "46324457",
                                    "name": "Qihui Zhang"
                                },
                                {
                                    "authorId": "2241357425",
                                    "name": "Nuno Moniz"
                                },
                                {
                                    "authorId": "2324218620",
                                    "name": "Tian Gao"
                                },
                                {
                                    "authorId": "2324052753",
                                    "name": "Werner Geyer"
                                },
                                {
                                    "authorId": "2324171997",
                                    "name": "Chao Huang"
                                },
                                {
                                    "authorId": "2279077171",
                                    "name": "Pin-Yu Chen"
                                },
                                {
                                    "authorId": "2286872295",
                                    "name": "Nitesh V. Chawla"
                                },
                                {
                                    "authorId": "2307963162",
                                    "name": "Xiangliang Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 78
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Mitigation Strategies: Alternative Evaluator Approaches",
                "tldr": "To counter egocentric bias, researchers recommend using diverse evaluator models rather than relying solely on GPT-4 to assess its own outputs. These approaches include multi-model ensembles, peer discussions, and weighted evaluation strategies that together provide more balanced and comprehensive assessments. (7 sources)",
                "text": "\n- **Multiple LLM Judges**: Incorporating different LLM evaluators beyond just GPT-4 helps establish \"a more comprehensive and unbiased assessment\" of generated content. This approach reduces the risk of any single model's biases dominating the evaluation process. <Paper corpusId=\"259360998\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>\n\n- **Neutral Evaluator Selection**: Using LLMs that are independent of the assessed models' training data can mitigate potential data leakage risks that contribute to egocentric bias. This separation helps ensure more objective evaluations. <Paper corpusId=\"267760188\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **Evaluator Rotation**: Randomly rotating different evaluators during the assessment process helps reduce the influence of any single model's biases, creating a more balanced overall evaluation. <Paper corpusId=\"267760188\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **Peer Discussion Models**: Employing diverse LLMs as evaluators to foster peer discussions reduces preference for any specific LLM and enhances the robustness of evaluation outcomes. This approach helps counterbalance the self-preference tendencies of individual models. <Paper corpusId=\"269188154\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"257804696\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>\n\n- **Weighted Ensemble Evaluation**: When using multiple models for evaluation, adjusting the weight assigned to each model based on its perplexity for a given sample can help mitigate bias. Specifically, when a model shows low perplexity (high confidence) on a sample, reducing that model's evaluation weight for that sample can contribute to bias mitigation. <Paper corpusId=\"273661820\" paperTitle=\"(Wataoka et al., 2024)\" isShortName></Paper>\n\n- **Broader Evaluation Spectrum**: Addressing the \"chicken-and-egg dilemma\" requires employing a wider range of evaluation methods beyond just LLM-based assessment. This includes diverse benchmarks, varied evaluation criteria, and incorporating human feedback to ensure more balanced and comprehensive assessments. <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"215548699\" paperTitle=\"(Sellam et al., 2020)\" isShortName></Paper>\n\n- **Cross-Model Evaluation**: Having models evaluate outputs from different models rather than their own can reduce the impact of egocentric bias, though this approach doesn't eliminate other forms of bias that might exist between different model families. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n- **Human-AI Collaborative Evaluation**: Combining human evaluators with AI systems creates a hybrid approach that leverages the strengths of both while mitigating the biases inherent to each system alone. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [
                    {
                        "id": "(Wu et al., 2023)",
                        "snippets": [
                            "This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4. By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study."
                        ],
                        "paper": {
                            "corpus_id": 259360998,
                            "title": "Style Over Substance: Evaluation Biases for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2145209409",
                                    "name": "Minghao Wu"
                                },
                                {
                                    "authorId": "8129718",
                                    "name": "Alham Fikri Aji"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 47
                        },
                        "score": 0.552734375
                    },
                    {
                        "id": "(Li et al._1, 2024)",
                        "snippets": [
                            "Using LLMs like GPT-4 as judges introduces potential data leakage risks due to biases in their pre-training data. This can be mitigated by selecting neutral evaluators independent of the assessed models' training data or randomly rotating evaluators to reduce bias."
                        ],
                        "paper": {
                            "corpus_id": 267760188,
                            "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
                            "authors": [
                                {
                                    "authorId": "2261896751",
                                    "name": "Xiang Li"
                                },
                                {
                                    "authorId": "2257016293",
                                    "name": "Yunshi Lan"
                                },
                                {
                                    "authorId": "2268678836",
                                    "name": "Chao Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 11
                        },
                        "score": 0.51025390625
                    },
                    {
                        "id": "(Dai et al., 2024)",
                        "snippets": [
                            "The emergence of egocentric bias introduces the risk of selfreinforcement for LLMs, particularly when they are further trained using rewards from LLM-based evaluations. This scenario can lead to LLMs overfitting to their own evaluation criteria, intensifying self-preference in next-generation model (Liu et al., 2023). Current strategies for mitigating egocentric bias primarily involve employing diverse LLMs as evaluators to foster peer discussions [79], thereby reducing the preference for any specific LLM and enhancing the robustness of evaluation outcomes. However, this strategy inevitably increases the evaluation costs. Future research must explore more efficient solutions to ensure fair and unbiased evaluation."
                        ],
                        "paper": {
                            "corpus_id": 269188154,
                            "title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era",
                            "authors": [
                                {
                                    "authorId": "2155892801",
                                    "name": "Sunhao Dai"
                                },
                                {
                                    "authorId": "2153078929",
                                    "name": "Chen Xu"
                                },
                                {
                                    "authorId": "2202745",
                                    "name": "Shicheng Xu"
                                },
                                {
                                    "authorId": "2263589454",
                                    "name": "Liang Pang"
                                },
                                {
                                    "authorId": "2297820120",
                                    "name": "Zhenhua Dong"
                                },
                                {
                                    "authorId": "2266437969",
                                    "name": "Jun Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Knowledge Discovery and Data Mining",
                            "n_citations": 82
                        },
                        "score": 0.84765625
                    },
                    {
                        "id": "(Liu et al., 2023)",
                        "snippets": [
                            "The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval"
                        ],
                        "paper": {
                            "corpus_id": 257804696,
                            "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
                            "authors": [
                                {
                                    "authorId": "2152797401",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "3310951",
                                    "name": "Dan Iter"
                                },
                                {
                                    "authorId": "2110197273",
                                    "name": "Yichong Xu"
                                },
                                {
                                    "authorId": "2146294891",
                                    "name": "Shuo Wang"
                                },
                                {
                                    "authorId": "8233965",
                                    "name": "Ruochen Xu"
                                },
                                {
                                    "authorId": "8652308",
                                    "name": "Chenguang Zhu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1211
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wataoka et al., 2024)",
                        "snippets": [
                            "This finding suggests a potential concern: using GPT-4 as a judge may lead to excessive influence from GPT-4's unique styles and policies.\n\nTo reduce self-preference bias, one possible approach is ensemble evaluation using multiple models. This method is expected to provide a more equitable evaluation by avoiding reliance on a single model. Specifically, when a model exhibits low perplexity on a sample, decreasing the weight assigned to that model's evaluation for that sample may contribute to bias mitigation. To evaluate the effectiveness of bias reduction strategies, our proposed new metric can be utilized."
                        ],
                        "paper": {
                            "corpus_id": 273661820,
                            "title": "Self-Preference Bias in LLM-as-a-Judge",
                            "authors": [
                                {
                                    "authorId": "2007365532",
                                    "name": "Koki Wataoka"
                                },
                                {
                                    "authorId": "2325815191",
                                    "name": "Tsubasa Takahashi"
                                },
                                {
                                    "authorId": "1466451143",
                                    "name": "Ryokan Ri"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 25
                        },
                        "score": 0.70166015625
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator). This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023). This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator. To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022; Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023; Ouyang et al., 2022) to ensure more balanced and comprehensive assessments."
                        ],
                        "paper": {
                            "corpus_id": 270391675,
                            "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
                            "authors": [
                                {
                                    "authorId": "2145256331",
                                    "name": "Zhen Li"
                                },
                                {
                                    "authorId": "2279658967",
                                    "name": "Xiaohan Xu"
                                },
                                {
                                    "authorId": "2279548827",
                                    "name": "Tao Shen"
                                },
                                {
                                    "authorId": "2284826718",
                                    "name": "Can Xu"
                                },
                                {
                                    "authorId": "2308241851",
                                    "name": "Jia-Chen Gu"
                                },
                                {
                                    "authorId": "2308073132",
                                    "name": "Yuxuan Lai"
                                },
                                {
                                    "authorId": "2287928517",
                                    "name": "Chongyang Tao"
                                },
                                {
                                    "authorId": "2307142498",
                                    "name": "Shuai Ma"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 15
                        },
                        "score": 0.6328125
                    },
                    {
                        "id": "(Sellam et al., 2020)",
                        "snippets": [
                            "Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution."
                        ],
                        "paper": {
                            "corpus_id": 215548699,
                            "title": "BLEURT: Learning Robust Metrics for Text Generation",
                            "authors": [
                                {
                                    "authorId": "145450400",
                                    "name": "Thibault Sellam"
                                },
                                {
                                    "authorId": "143790066",
                                    "name": "Dipanjan Das"
                                },
                                {
                                    "authorId": "144729897",
                                    "name": "Ankur P. Parikh"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 1505
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Limitations of Current Mitigation Approaches",
                "tldr": "Despite progress in mitigating egocentric bias, current approaches face significant limitations including increased computational costs, potential introduction of new biases, and the fundamental chicken-and-egg dilemma where evaluator development depends on more powerful models that themselves need evaluation. (5 sources)",
                "text": "\nWhile researchers have developed various strategies to address egocentric bias in GPT-4's self-evaluation, these approaches come with important limitations. One significant constraint is the computational and resource burden. As Dai et al. note, employing diverse LLMs as evaluators to foster peer discussions \"inevitably increases the evaluation costs,\" making widespread implementation challenging despite its effectiveness in reducing bias. <Paper corpusId=\"269188154\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper>\n\nAnother fundamental limitation is what Li et al. describe as the \"chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.\" This circular problem creates a structural challenge for any mitigation strategy that relies solely on LLM-based evaluation approaches. <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>\n\nSome mitigation techniques may inadvertently introduce new biases or exacerbate existing ones. For example, while the SoPro technique effectively reduces egocentric bias in GPT-3.5, researchers found that \"the models' susceptibility to bandwagon effect increased,\" suggesting that attempts to align LLMs with others' perspectives may create vulnerability to different cognitive biases. <Paper corpusId=\"274437478\" paperTitle=\"(Sumita et al., 2024)\" isShortName></Paper>\n\nEven the AwaRe prompting approach, which has shown promise in mitigating multiple biases including egocentric bias in GPT-3.5, demonstrates inconsistent performance across different models and bias types. It successfully addresses \"bandwagon effect and verbosity bias in GPT-4\" but doesn't uniformly mitigate all forms of bias across model generations. <Paper corpusId=\"274437478\" paperTitle=\"(Sumita et al., 2024)\" isShortName></Paper>\n\nResearchers also acknowledge that alternative approaches like human evaluation \"carry their own biases\" and may not represent a perfect solution. As Mullick et al. point out, \"using summaries generated exclusively by GPT-4 could introduce biases inherent in its summarization capabilities,\" but human alternatives may introduce different subjective biases. <Paper corpusId=\"270286247\" paperTitle=\"(Mullick et al., 2024)\" isShortName></Paper>\n\nThe limitations of current approaches underscore the need for what Li et al. describe as \"a broader spectrum of evaluation methods,\" involving various benchmarks, evaluation criteria, and human feedback \"to ensure more balanced and comprehensive assessments.\" <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"215548699\" paperTitle=\"(Sellam et al., 2020)\" isShortName></Paper> This suggests that no single mitigation strategy is sufficient, and that combinations of approaches tailored to specific contexts may be necessary to effectively address egocentric bias in GPT-4's self-evaluation.",
                "citations": [
                    {
                        "id": "(Dai et al., 2024)",
                        "snippets": [
                            "The emergence of egocentric bias introduces the risk of selfreinforcement for LLMs, particularly when they are further trained using rewards from LLM-based evaluations. This scenario can lead to LLMs overfitting to their own evaluation criteria, intensifying self-preference in next-generation model (Liu et al., 2023). Current strategies for mitigating egocentric bias primarily involve employing diverse LLMs as evaluators to foster peer discussions [79], thereby reducing the preference for any specific LLM and enhancing the robustness of evaluation outcomes. However, this strategy inevitably increases the evaluation costs. Future research must explore more efficient solutions to ensure fair and unbiased evaluation."
                        ],
                        "paper": {
                            "corpus_id": 269188154,
                            "title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era",
                            "authors": [
                                {
                                    "authorId": "2155892801",
                                    "name": "Sunhao Dai"
                                },
                                {
                                    "authorId": "2153078929",
                                    "name": "Chen Xu"
                                },
                                {
                                    "authorId": "2202745",
                                    "name": "Shicheng Xu"
                                },
                                {
                                    "authorId": "2263589454",
                                    "name": "Liang Pang"
                                },
                                {
                                    "authorId": "2297820120",
                                    "name": "Zhenhua Dong"
                                },
                                {
                                    "authorId": "2266437969",
                                    "name": "Jun Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Knowledge Discovery and Data Mining",
                            "n_citations": 82
                        },
                        "score": 0.84765625
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator). This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023). This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator. To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022; Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023; Ouyang et al., 2022) to ensure more balanced and comprehensive assessments."
                        ],
                        "paper": {
                            "corpus_id": 270391675,
                            "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges",
                            "authors": [
                                {
                                    "authorId": "2145256331",
                                    "name": "Zhen Li"
                                },
                                {
                                    "authorId": "2279658967",
                                    "name": "Xiaohan Xu"
                                },
                                {
                                    "authorId": "2279548827",
                                    "name": "Tao Shen"
                                },
                                {
                                    "authorId": "2284826718",
                                    "name": "Can Xu"
                                },
                                {
                                    "authorId": "2308241851",
                                    "name": "Jia-Chen Gu"
                                },
                                {
                                    "authorId": "2308073132",
                                    "name": "Yuxuan Lai"
                                },
                                {
                                    "authorId": "2287928517",
                                    "name": "Chongyang Tao"
                                },
                                {
                                    "authorId": "2307142498",
                                    "name": "Shuai Ma"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 15
                        },
                        "score": 0.6328125
                    },
                    {
                        "id": "(Sumita et al., 2024)",
                        "snippets": [
                            "Although SoPro mitigates egocentric bias in GPT-3.5 and verbosity bias in GPT-4, the models' susceptibility to bandwagon effect increased. SoPro, which aligns LLMs with others' perspectives, may not be effective for cognitive biases. This result is inconsistent with the claim of the study for humans. AwaRe mitigates order bias, compassion fade, egocentric bias, bandwagon effect, and attentional bias in GPT-3.5, and bandwagon effect and verbosity bias in GPT-4. This result suggests that AwaRe prompts LLMs to make rational judgments."
                        ],
                        "paper": {
                            "corpus_id": 274437478,
                            "title": "Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments",
                            "authors": [
                                {
                                    "authorId": "2333360231",
                                    "name": "Yasuaki Sumita"
                                },
                                {
                                    "authorId": "2243408877",
                                    "name": "Koh Takeuchi"
                                },
                                {
                                    "authorId": "2247886893",
                                    "name": "Hisashi Kashima"
                                }
                            ],
                            "year": 2024,
                            "venue": "ACM Symposium on Applied Computing",
                            "n_citations": 5
                        },
                        "score": 0.71044921875
                    },
                    {
                        "id": "(Mullick et al., 2024)",
                        "snippets": [
                            "It is acknowledged that using summaries generated exclusively by GPT-4 could introduce biases inherent in its summarization capabilities, it may also be noted that alternatives, such as human evaluation, also carry their own biases.Despite the potential for bias, leveraging GPT-4 for summarization may still be a pragmatic choice, especially in scenarios access to diverse datasets or sophisticated validation methods is limited.However, in this work, we remain vigilant, recognizing the limitations inherent in both automated and humangenerated summaries, and take proactive steps such as human intervention to validate and contextualise the results to mitigate biases to the best extent possible within the given constraints."
                        ],
                        "paper": {
                            "corpus_id": 270286247,
                            "title": "On The Persona-based Summarization of Domain-Specific Documents",
                            "authors": [
                                {
                                    "authorId": "21724468",
                                    "name": "Ankan Mullick"
                                },
                                {
                                    "authorId": "2238668264",
                                    "name": "Sombit Bose"
                                },
                                {
                                    "authorId": "2304954421",
                                    "name": "Rounak Saha"
                                },
                                {
                                    "authorId": "19181085",
                                    "name": "Ayan Kumar Bhowmick"
                                },
                                {
                                    "authorId": "2261284157",
                                    "name": "Pawan Goyal"
                                },
                                {
                                    "authorId": "2261284171",
                                    "name": "Niloy Ganguly"
                                },
                                {
                                    "authorId": "2287821944",
                                    "name": "Prasenjit Dey"
                                },
                                {
                                    "authorId": "2247701385",
                                    "name": "Ravi Kokku"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 3
                        },
                        "score": 0.501953125
                    },
                    {
                        "id": "(Sellam et al., 2020)",
                        "snippets": [
                            "Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution."
                        ],
                        "paper": {
                            "corpus_id": 215548699,
                            "title": "BLEURT: Learning Robust Metrics for Text Generation",
                            "authors": [
                                {
                                    "authorId": "145450400",
                                    "name": "Thibault Sellam"
                                },
                                {
                                    "authorId": "143790066",
                                    "name": "Dipanjan Das"
                                },
                                {
                                    "authorId": "144729897",
                                    "name": "Ankur P. Parikh"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 1505
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.152121
    }
}